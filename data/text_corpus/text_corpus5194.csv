index,text
25970,the impact of climate change on water resources is one of the major challenges of our society synthesis work is essential to tackle these challenges in a scientifically sound way synthesis assessments can benefit from interactive web applications that allow easy access managing and updating of its database we developed a web based tool for the synthesis assessment of the impacts of climate change on the water resources of brazil the tool called yara allows users to interact with the literature database and extract information of interest all in an updatable fashion yara was tested by 107 stakeholders and several improvements were suggested and implemented such as an interactive table and a form for registering new studies the survey confirmed the value of yara and we believe the tool is an important step to support robust adaptation strategies towards a more resilient society keywords climate change hydrology web application stakeholders sdg 13 ipcc 1 introduction climate change and water resources are one of the major challenges of our society ipcc 2018 ripple et al 2019 world economic forum 2020 several initiatives of the united nations such as the intergovernmental panel on climate change ipcc the united nations framework convention on climate change unfccc and the sustainable development goals sdg united nations 2016 address climate change and water resources issues since 1988 the ipcc synthesizes the evidence from thousands of scientific articles about climate change in the form of assessment reports those syntheses are an essential part of the scientific process they identify scientific gaps and help to establish an evidence based practice gurevitch et al 2018 at the global scale the ipcc informs policymakers on the current state of knowledge about climate change and its reports are used by the scientific community as the most comprehensive literature review on climate change ipcc 2020 synthesis efforts at the individual level are also common for example ciscar et al 2018 conducted a synthesis of the climate change impacts on agriculture kaczan and orgill meyer 2020 focused on climate change and migration and howes et al 2015 addressed impacts of climate change on the oceans at the national level for instance the brazilian panel on climate change painel brasileiro de mudanças climáticas pbmc gathers synthesizes and evaluates scientific information about climate change in brazil pbmc 2020 the number of studies addressing both climate change and water resources in brazil has grown substantially in the last two decades borges and chaffe 2019a based on the ipcc framework borges and chaffe 2019b developed a synthesis method and revealed the country s potential changes into a drier hydrological regime with the hydropower and the drinking water supply sectors under the highest concern another important point is that most of those studies have limitations regarding the use of a multi model ensemble the evaluation of models and the observational data despite the value of such efforts gulizia et al 2020 their databases are often neither accessible nor manageable as required by the end users while the scientific literature on climate change is ever expanding traditional reports are static and do not permit constant updates callaghan et al 2020 under those circumstances synthesis assessments can benefit from web applications that allow access the extraction of specific information and the updating of its literature database web based tools have been successfully implemented in several environmental and climate change adaptation contexts allowing greater usability of a database as well as constant updates of recently published information vitolo et al 2015 those tools have been applied in climate risk evaluation for water supply whateley et al 2015 mcdonald et al 2019 flood risk assessments knight et al 2015 to estimate the intensity duration frequency idf curves from climate projections and observational data simonovic et al 2016 pizarro et al 2018 to visualize and provide large climate data sets alder and hostetler 2015 mcdonald et al 2019 as decision support systems for the analysis of water soil waste nexus mannschatz et al 2016 and as an online meta database for reports on water resources management awad et al 2009 web based synthesis tools have been applied in biomedical sciences for handling visualizing and disclosing meta analyses for researchers and health care decision makers freeman et al 2019 owen et al 2019 scotti et al 2020 despite those efforts in biomedical science the application of web based tools for supporting synthesis assessments has not been explored in climate science yet the development of web based tools is a technical and a social process that requires the involvement of those who will make use of them the stakeholders participation not only increases the quality of the tool as new needs emerge but it also enhances the acceptance and use of the tool as a sense of ownership is evolved by the users díez and mcintosh 2009 mcintosh et al 2011 sandink et al 2016 in this paper we present yara a web based tool that allows easy access to the synthesis assessment of the impacts of climate change on brazilian water resources yara was tested by stakeholders and several of their suggestions were implemented the tool was developed to 1 inform decision and policymakers about the potential impacts of climate change in the water resources in brazil 2 facilitate the access to a topic specific and yet comprehensive updatable literature database 3 inform the scientific community about the current state of knowledge on the topic and research gaps 4 increase the visibility of scientific studies the first section of this paper explains the structure of yara the second part describes the implementation of the system in the form of a web based tool the third section details the stakeholders involvement process and its outcomes later we showcase the first version of the tool with the improvements recommended by the stakeholders in the end we highlight the best practices adopted in the implementation of yara and discuss alternatives to address those that were partially or not considered so far as well as the application prospect of the tool we hope that yara can support strengthen resilience and adaptive capacity to climate hazards contributing therefore to the sdg 13 climate action united nations 2016 2 materials and methods we call the system as yara the indigenous brazilian name i e tupi guarani language for the entity of water yara is hosted at the federal university of santa catarina s watershed hydrology lab s webpage www labhidro ufsc br yara the main goal of yara is to allow access to the literature database and the synthesis of the impacts of climate change on the brazilian water resources in a rapid intuitive and informative way the yara s database comprises a collection of 42 peer reviewed papers that were previously assessed by borges and chaffe 2019b during the evaluation phase of yara we asked the authors of the papers to review the description and information regarding their papers the yara s database was updated based on that request and we also included more details like the name of the models and watersheds the framework for displaying the results is also based on borges and chaffe 2019b where synthesis results are expressed in terms of heatmaps synthesis charts sorted by hydrographic regions and streamflow indices since the database has a geographical dimension we first made use of the keyhole markup language kml open geospatial consortium inc 2020 to be visualized in the google earth pro 1 1 https www google com earth versions earth pro however the access to kml files requires software installation to avoid low application mcintosh et al 2011 we developed and implemented a web based application as the involvement of stakeholders in the design process is crucial díez and mcintosh 2009 mcintosh et al 2011 sandink et al 2016 we tested the prototype with potential users to gather suggestions for improvement after the prioritization of the stakeholders suggestions we implemented the first version of the system fig 1 and the following sections detail all steps of the implementation of yara 1 0 2 1 system overview the website is divided into six tabs the main page database about help downloads and contact fig 2 the main page gives access to the interactive map in this map users can select one of the 12 hydrographic regions of brazil by selecting a hydrographic region a new window appears where users can access the list of all studies available for the selected region fig 3 by choosing a specific study from the list a new tab opens and shows all the methodological aspects of the study e g title of the article models used model evaluation procedure adopted and the general information e g year of publication authorship the scientific journal where the study was published the digital object identifier doi fig 4 in the interactive map users can access the synthesis for three indices i e minimum streamflow mean streamflow and maximum streamflow each tab includes a synthesis chart a short interpretation of the chart the list of papers and their conclusions fig 5 heatmaps illustrate the synthesis of evidence of change which is a function of consistency quality and quantity of studies the terminology and the synthesis method are based on the ipcc s guidance for the evaluation and communication of the degree of certainty in assessment findings see mastrandrea et al 2011 the grey colours show the number of studies per quadrant quantity the x axis represents the consistency of the conclusions of the studies and the direction of the sign of change the further to the left the greater the consistency that the streamflow index will decrease the further to the right the greater the consistency of increase in that streamflow index the y axis represents the quality of the studies each study has a quality score ranging from 1 to 5 depending on the comprehensiveness of the study in sampling all sources of uncertainty along the modelling chain the higher the score the higher the quality of the study and therefore the confidence of its conclusion more details about the calculations and interpretation of the synthesis charts can be found in borges and chaffe 2019b the information regarding the conclusions of the studies is provided in a table below the text of the synthesis chart to guarantee transparency and a traceable account on the main page the database tab provides access to the literature database the tab about is a short introduction about the yara and the tab help offers a tutorial on how to use the tool the tab downloads provides the entire database of yara as well as the code package used to calculate the synthesis charts users can contact us through the contact tab 2 2 technological aspects the main purpose of the yara web application is to give users access to the synthesis assessments for each individual hydrographic region arranged in minimum mean and maximum streamflow changes all data i e synthesis charts interpretation texts a summary table and the information regarding the studies are stored in a database and a web based interface allows the user to extract information of interest for example changes in minimum streamflow in the paraná hydrographic region this section describes the two major system components of the yara 1 the database and 2 the user interface 2 2 1 database the database comprises 1 a table storing the attributes of the studies 2 a table storing the conclusions of the studies sorted by hydrographic regions 3 thirty six figures illustrating the synthesis chart i e 3 streamflow indices for each of the 12 hydrographic regions and 4 texts with the interpretation of the thirty six synthesis charts the synthesis charts are calculated separately using a code written in exelis idl 8 5 2 2 https www harrisgeospatial com the variable quality is calculated based on methodological aspects of the studies which are described in the table of attributes the consistency is a result of the interpretation of the main conclusions of the studies which are described in the table of conclusions see more details about the calculation of the synthesis charts and assumptions in borges and chaffe 2019b 2 2 2 user interface the tool runs on a web server and can be accessed via standard web browsers for the construction of the website we used the hypertext markup language html and for the interactive map we applied extensible markup language xml files and geographic javascript object notation geojson butler et al 2016 fig 6 shows the flowchart of the user interface the interactive map was first constructed in the kml file and later translated into the geojson format the description element is stored in the geojson format as a property of each geometric object when the user clicks on the hydrographic region a window with the content stored in the geojson property description opens each hydrographic region is a placemark with polygon geometry that stores a set of hypertexts in the description element the hypertexts embrace the list of studies and synthesis assessments for each streamflow index see fig 3 the content is structured in a hierarchical tree by streamflow index element and hydrographic region subelement the program also generates individual pages with information about the methodological aspects of each study that are stored in xml files see fig 4 with this structure it is possible to keep the information about the methodological aspects of each study and the synthesis assessments updated as new studies are incorporated into the database 2 3 testing and feedback the testing of the tool was done in five steps 1 preparation of the questionnaire 2 listing of the stakeholders 3 call for participation 4 collection and processing of the questionnaire responses 5 prioritization of the improvements fig 1 the main goal of the questionnaire was to assess the relevance of the yara in terms of access to the database the information about the scientific articles synthesis charts and explanation as well as its graphic user interface and navigability we also asked the participants about their profile and experience with similar systems in the context of climate change impacts to avoid overburdening the participants we followed the recommendation by crawford et al 2001 and limited the number of questions to a 10 minute response time we used google form 3 3 https www google com forms about to collect the information and the questionnaire is available in the supplementary material the list of participants was based on the authors and co authors of the articles included in the database of borges and chaffe 2019a through personal contacts and internet survey we identified and included in the list several relevant stakeholders from the brazilian national water agency agência nacional de águas ana civil servants from two brazilian ministries i e ministry of environment and ministry of science technology and innovation research institutions hydrometeorological services universities and private companies all stakeholders of the list were dealing with water resources and climate change and preferably but not only based in brazil we ended up with a list of 212 stakeholders and sent personal invitation letters by email we launched the call on september 5th 2019 and closed it on october 9th 2019 3 results 3 1 profile of the survey participants we gathered 107 responses response rate of 50 most of the participants of the survey were from universities 57 followed by research centres 17 and public management bodies responsible for the implementation of water related policies like ana and ministries 17 fig 7 hydrometeorological offices private companies and other institutions are represented by three people each ninety three percent of the participants make use of information about the impacts of climate change fig s1 in supplementary material the vast majority of them get that information from scientific articles synthesis reports and databases fig 8 meanwhile 50 respondents said that their institutions perform their studies forty five respondents conduct literature reviews while 21 said that they hire studies from other institutions the diversity of the survey sample is representative despite the dominance of academics we gathered a considerable sample size of decision makers the vast majority of the participants match the profile of a potential user of the yara tool which is a person interested in scientific evidence for scientific research decision making and standards and legislation fig s2 in supplementary material 3 2 perception of the stakeholders about yara thirty nine percent of the respondents said that yara is highly applicable in the context of their work and 48 considered it somehow useful fig s3 in supplementary material we also explored the participants perception of the relevance of the functions provided by the tool fig 9 yara has four main functions that provide 1 access to the literature database 2 information about the methodological aspects of the articles 3 synthesis charts and 4 interpretation of the charts the majority of the survey participants considered all functions as highly relevant only a few respondents considered the relevance of a function as low or very low e g the worst case was seven respondents for interpretation of the charts function apparently the navigability of the system is satisfactory the vast majority of the stakeholders pointed out that the time consumed to access the information provide in yara was either sufficient 68 or very quick 28 fig s4 in supplementary material 3 3 improvements the survey participants suggested several improvements due to time technical constraints and in some cases the need for a deeper involvement of the users see mcintosh et al 2011 we applied a prioritization procedure for improvements considering two criteria 1 the relevance of the improvement according to the purpose of yara and 2 the feasibility of its implementation considering the limited human resources and or the need for users participation we adopted a scoring scheme using a scale from 1 low to 3 high although subjective the scoring was a consensus among the authors of this study we implemented all improvements that reached a total score of 4 5 and 6 with feasibility 2 or above table 1 shows the improvements suggested by the survey s participants and the score of the prioritization procedure in terms of the information about the articles we included the title of the paper the objective the full name of the streamflow indices other variables and indices analysed e g rainfall evapotranspiration the description of the results of the validation procedure the main conclusions and the suggestions for further studies we have not included the source of the data used nor the spatial scale considered in the downscaling in its current form the quality scoring of the articles is not influenced by those attributes see borges and chaffe 2019b and thus we considered both suggestions of low relevance some participants suggested the inclusion of the main figures of the articles we believe that is highly relevant however copyright issues need formal requests that make its implementation very time consuming alternatively yara provides the doi link of the original article the major improvement in new functions and navigability resources was the inclusion of an interactive table in the database tab that lists the whole literature database fig 10 in this table users can filter the studies using specific keywords like the name of a hydrological model or a watershed in the same tab we implemented a registration form for new studies eighty one percent of the respondents are willing to register their studies in the yara system fig s5 in supplementary material the motivations are vast but several mentioned the opportunity to make their studies more visible once a new study is registered we will first check the consistency of the information with the original paper before updating the database one participant recommended an english version of the yara tool indeed considering the disclosure of this tool to international audiences an english version is highly relevant and was therefore implemented the user tutorial is a fundamental part of any tool mcintosh et al 2011 sandink et al 2016 users should be able to use the yara correctly and yet in a rapid and simple manner ninety three percent of the participants considered the tutorial satisfactory fig s6 in supplementary material but some participants suggested prompt explanations about the interpretation of the charts and for specific terms we therefore implemented pop up explanation windows fig 11 represented by question mark symbols near charts and dashed underlines under specific terms like consistency and quality additionally we produced a tutorial video explaining all steps and functions of the tool see borges et al 2020a as well as a promotional video see borges et al 2020b we considered as highly relevant to illustrate in the main map the limits of the watersheds adopted in the studies and to permit the calculation of the synthesis charts by selecting specific keywords and or geographic coordinates however specific needs may lead to a negligible amount of studies rendering the feasibility of its implementation to low before its implementation more details about the need for this kind of function is required mcintosh et al 2011 4 discussion web technologies for climate change adaptation issues are extremely valuable e g alder and hostetler 2015 awad et al 2009 knight et al 2015 mannschatz et al 2016 mcdonald et al 2019 pizarro et al 2018 simonovic et al 2016 whateley et al 2015 web based tools like yara permit the interaction with users allowing them to extract specific information according to their interests and needs these tools offer easy access wide availability and constant updates mcdonald et al 2019 simonovic et al 2016 whateley et al 2015 one of the challenges is that computer based tools are justified only if users make use of them mcintosh et al 2011 sandink et al 2016 the low application of some computer based systems is mainly a result of not properly involving users in the implementation phases díez and mcintosh 2009 mcintosh et al 2011 sandink et al 2016 the participation of users in a pre implementation stage is likely to improve the design of the functions of the system and the way the results are communicated díez and mcintosh 2009 sandink et al 2016 the absence of users in the pre implementation phase could be seen as a weakness of the yara an issue also reported in sandink et al 2016 another challenge is to ensure longevity as many web based initiatives struggle with a lack of resources for operation maintenance and updating díez and mcintosh 2009 mcintosh et al 2011 sandink et al 2016 there are several recommendations of best practices to achieve the intended outcomes of environmental web based systems we sorted out the recommendations from the literature that apply to the context of yara table 2 we highlight the best practices adopted in the implementation of yara and discuss alternatives to address those that were partially or not considered so far a user friendly interface should be based on the user s needs and capabilities awad et al 2009 freeman et al 2019 mcintosh et al 2011 sandink et al 2016 in the case of yara we first proposed a user interface based on simonovic et al 2016 for the wireframe and based on borges and chaffe 2019b for the way results are displayed we validated that with users through a survey it is recommended to involve the users in the pre implementation phase díez and mcintosh 2009 mcintosh et al 2011 sandink et al 2016 but that requires substantial efforts to engage users mcintosh et al 2011 that is more feasible in the context of large projects like in sandink et al 2016 and that was not the case of yara despite involving the users only in the post implementation phase the survey shows that the user interface of yara was satisfactory see fig 9 and fig s4 it is important to identify the end users and stakeholders mcintosh et al 2011 sandink et al 2016 as the use of a computer based system is more likely when the user foresees benefits díez and mcintosh 2009 sandink et al 2016 we mapped more than two hundred stakeholders and almost 40 percent of the survey respondents said the yara is highly applicable in the context of their work fig s3 another best practice is to develop a system that is relatively easy and inexpensive to use alder and hostetler 2015 díez and mcintosh 2009 mcintosh et al 2011 sandink et al 2016 in this aspect the yara was perceived as relatively easy to navigate fig s4 and it is free for use mcintosh et al 2011 recommend starting simple and small by developing a tool based on known technology the yara is hosted on the web allowing easy access through internet browsers avoiding therefore extra efforts with software installation the technology used in yara is mainly an interactive map and a table filtering the survey participants highlighted that the system is very easy to navigate and we assume that this technology is not a barrier for the intended users yara offers a broad option of help which includes a video a help tab and pop up windows as recommended by mcintosh et al 2011 and sandink et al 2016 as endorsed by mcintosh et al 2011 we dedicated time and resources for the survey and more than a hundred stakeholders responded to it establishing trust and credibility is essential for that mcintosh et al 2011 and sandink et al 2016 emphasize the importance of being transparent about weaknesses of the system and areas that leave room for improvement like model uncertainties and assumptions in the case of yara uncertainties are intrinsically expressed in the synthesis charts where the y axis illustrates the quality of the studies and the x axis expresses the consistency of the conclusions of the studies all synthesis charts are interpreted and described in detail including possible bias due to interdependency among the studies additionally the limitations of the synthesis method are documented in the help tab tools are developed to be used after implementation and guaranteeing their longevity is dependent on stakeholders built capacity díez and mcintosh 2009 mcintosh et al 2011 sandink et al 2016 although the participants of our survey mentioned that synthesis charts are useful it is not clear if they interpreted the charts properly apart from the tutorial material we believe that extra attention is needed to instruct users to interpret the synthesis charts correctly one option is to offer webinars to train users sandink et al 2016 at the same time user support should be continuous díez and mcintosh 2009 mcintosh et al 2011 sandink et al 2016 we provided contact information for user support but since the tool addresses a very specific issue we do not expect a large number of users to use yara regularly another aspect of longevity is to ensure that the database can be easily updated díez and mcintosh 2009 knight et al 2015 mcintosh et al 2011 sandink et al 2016 the yara was made to be updated when new studies are registered in the database at this stage we calculate the synthesis charts separately in idl and then upload all figures and texts in the web interface this step requires time and is justified when a reasonable number of new studies are available therefore the updating process leaves room for improvement a solution for that is to use a software that combines data processing with web interactivity like the shiny r package chang et al 2017 which has been used in many web tools e g freeman et al 2019 owen et al 2019 whateley et al 2015 new functions should avoid unnecessary complexity and should be included when demanded by the stakeholders knight et al 2015 mannschatz et al 2016 mcintosh et al 2011 sandink et al 2016 three participants of the survey requested a function that calculates the synthesis charts according to specific parameters but each of them mentioned distinct parameters and there is a need for better shaping that demand similar synthesis initiatives in the biomedical field freeman et al 2019 owen et al 2019 recognize the value of enabling the users to explore different datasets assess the impact of modelling assumptions and scrutinise any concerns they may have however to avoid low application of a new function it is strongly recommended a deep involvement of the stakeholders through a participatory process freeman et al 2019 knight et al 2015 mcintosh et al 2011 sandink et al 2016 we believe that extensions of yara should include the provision of downloadable summary reports alder and hostetler 2015 the exploration of new visualization schemes harold et al 2020 owen et al 2019 and even the application of a bayesian framework freeman et al 2019 gurevitch et al 2018 owen et al 2019 additionally the inclusion of studies that address the current changing state of the water resources is foreseen as a means to obtain more robust evidence borges and chaffe 2019b future improvements of the yara should consider the participation of stakeholders beyond online surveys for instance through workshops freeman et al 2019 knight et al 2015 sandink et al 2016 that may require a business plan to define costs outcomes and necessary financial support mcintosh et al 2011 we believe that the application prospect of yara is vast at the individual level the tool provides information for literature reviews on the topic reducing the workload needed by researchers and practitioners at the national level yara can support upcoming climate change assessments like the pbmc 2014 cgee and ana 2014 as well as to the implementation of the brazilian national adaptation plan to climate change brazil 2016 since the method is based on ipcc s guidance the results can contribute to both the freshwater resources and central and south america chapters of future assessment reports of the ipcc s working group ii see jiménez cisneros et al 2014 magrin et al 2014 5 conclusions in this paper we developed and implemented a web based tool that allows easy access to the synthesis assessment and the literature database of the impacts of climate change on the brazilian territory the yara was launched in november 2019 is freely available on the website www labhidro ufsc br yara and discloses the results of a broad literature database i e borges and chaffe 2019b in a manageable and user friendly way the website was constructed in html language data processing is done in idl and for the interactive application we used xml files and geojson a prototype of the tool was tested by 107 stakeholders and an online survey collected relevant recommendations for improving the tool most of the survey participants were from universities with a significant number of stakeholders from research centres and public management bodies responsible for the management of water resources in brazil the majority of the participants make use of scientific articles and synthesis assessment as sources of information in their daily duties in general the perception of the participants about the yara was very positive the tool provides four functions the literature database the information about the methodological aspects of the studies the synthesis charts and the interpretation of the results all four functions were rated as highly relevant and navigability of the system was deemed as satisfactory both by the majority of the stakeholders at the same time more than a third of the respondents said the yara is highly applicable in the context of their work facts that confirms the potential of use of the tool we collected several recommendations for improving the tool and selected the most relevant suggestions by using a prioritization scheme the most significant improvements were 1 an interactive table that permits the filtering of the list of studies 2 the inclusion of help popup windows 3 the possibility of registering new studies and 4 an english version of the tool in future versions we will include a function that permits the calculation of the synthesis charts by selecting specific parameters like geographic coordinates and keywords there is still work to do on advertising engaging more users monitoring the usefulness and updating the database the yara may support literature reviews national assesments and expert judgements to inform decision makers the approach is flexible and can be easily replicated to other countries regions and contexts we believe yara represents an important step to achieve a more accessible manageable and yet easy to update synthesis assessments about the impacts of climate change supporting therefore robust adaptation strategies towards a more resilient society declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we thank the contributions of all 107 stakeholders that took part in the survey as well as the laboratory colleagues that tested the prototype and the survey questionnaire we acknowledge the brazilian national council for scientific and technological development conselho nacional de desenvolvimento científico e tecnológico cnpq for funding this study grant number 159528 2018 6 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104848 
25970,the impact of climate change on water resources is one of the major challenges of our society synthesis work is essential to tackle these challenges in a scientifically sound way synthesis assessments can benefit from interactive web applications that allow easy access managing and updating of its database we developed a web based tool for the synthesis assessment of the impacts of climate change on the water resources of brazil the tool called yara allows users to interact with the literature database and extract information of interest all in an updatable fashion yara was tested by 107 stakeholders and several improvements were suggested and implemented such as an interactive table and a form for registering new studies the survey confirmed the value of yara and we believe the tool is an important step to support robust adaptation strategies towards a more resilient society keywords climate change hydrology web application stakeholders sdg 13 ipcc 1 introduction climate change and water resources are one of the major challenges of our society ipcc 2018 ripple et al 2019 world economic forum 2020 several initiatives of the united nations such as the intergovernmental panel on climate change ipcc the united nations framework convention on climate change unfccc and the sustainable development goals sdg united nations 2016 address climate change and water resources issues since 1988 the ipcc synthesizes the evidence from thousands of scientific articles about climate change in the form of assessment reports those syntheses are an essential part of the scientific process they identify scientific gaps and help to establish an evidence based practice gurevitch et al 2018 at the global scale the ipcc informs policymakers on the current state of knowledge about climate change and its reports are used by the scientific community as the most comprehensive literature review on climate change ipcc 2020 synthesis efforts at the individual level are also common for example ciscar et al 2018 conducted a synthesis of the climate change impacts on agriculture kaczan and orgill meyer 2020 focused on climate change and migration and howes et al 2015 addressed impacts of climate change on the oceans at the national level for instance the brazilian panel on climate change painel brasileiro de mudanças climáticas pbmc gathers synthesizes and evaluates scientific information about climate change in brazil pbmc 2020 the number of studies addressing both climate change and water resources in brazil has grown substantially in the last two decades borges and chaffe 2019a based on the ipcc framework borges and chaffe 2019b developed a synthesis method and revealed the country s potential changes into a drier hydrological regime with the hydropower and the drinking water supply sectors under the highest concern another important point is that most of those studies have limitations regarding the use of a multi model ensemble the evaluation of models and the observational data despite the value of such efforts gulizia et al 2020 their databases are often neither accessible nor manageable as required by the end users while the scientific literature on climate change is ever expanding traditional reports are static and do not permit constant updates callaghan et al 2020 under those circumstances synthesis assessments can benefit from web applications that allow access the extraction of specific information and the updating of its literature database web based tools have been successfully implemented in several environmental and climate change adaptation contexts allowing greater usability of a database as well as constant updates of recently published information vitolo et al 2015 those tools have been applied in climate risk evaluation for water supply whateley et al 2015 mcdonald et al 2019 flood risk assessments knight et al 2015 to estimate the intensity duration frequency idf curves from climate projections and observational data simonovic et al 2016 pizarro et al 2018 to visualize and provide large climate data sets alder and hostetler 2015 mcdonald et al 2019 as decision support systems for the analysis of water soil waste nexus mannschatz et al 2016 and as an online meta database for reports on water resources management awad et al 2009 web based synthesis tools have been applied in biomedical sciences for handling visualizing and disclosing meta analyses for researchers and health care decision makers freeman et al 2019 owen et al 2019 scotti et al 2020 despite those efforts in biomedical science the application of web based tools for supporting synthesis assessments has not been explored in climate science yet the development of web based tools is a technical and a social process that requires the involvement of those who will make use of them the stakeholders participation not only increases the quality of the tool as new needs emerge but it also enhances the acceptance and use of the tool as a sense of ownership is evolved by the users díez and mcintosh 2009 mcintosh et al 2011 sandink et al 2016 in this paper we present yara a web based tool that allows easy access to the synthesis assessment of the impacts of climate change on brazilian water resources yara was tested by stakeholders and several of their suggestions were implemented the tool was developed to 1 inform decision and policymakers about the potential impacts of climate change in the water resources in brazil 2 facilitate the access to a topic specific and yet comprehensive updatable literature database 3 inform the scientific community about the current state of knowledge on the topic and research gaps 4 increase the visibility of scientific studies the first section of this paper explains the structure of yara the second part describes the implementation of the system in the form of a web based tool the third section details the stakeholders involvement process and its outcomes later we showcase the first version of the tool with the improvements recommended by the stakeholders in the end we highlight the best practices adopted in the implementation of yara and discuss alternatives to address those that were partially or not considered so far as well as the application prospect of the tool we hope that yara can support strengthen resilience and adaptive capacity to climate hazards contributing therefore to the sdg 13 climate action united nations 2016 2 materials and methods we call the system as yara the indigenous brazilian name i e tupi guarani language for the entity of water yara is hosted at the federal university of santa catarina s watershed hydrology lab s webpage www labhidro ufsc br yara the main goal of yara is to allow access to the literature database and the synthesis of the impacts of climate change on the brazilian water resources in a rapid intuitive and informative way the yara s database comprises a collection of 42 peer reviewed papers that were previously assessed by borges and chaffe 2019b during the evaluation phase of yara we asked the authors of the papers to review the description and information regarding their papers the yara s database was updated based on that request and we also included more details like the name of the models and watersheds the framework for displaying the results is also based on borges and chaffe 2019b where synthesis results are expressed in terms of heatmaps synthesis charts sorted by hydrographic regions and streamflow indices since the database has a geographical dimension we first made use of the keyhole markup language kml open geospatial consortium inc 2020 to be visualized in the google earth pro 1 1 https www google com earth versions earth pro however the access to kml files requires software installation to avoid low application mcintosh et al 2011 we developed and implemented a web based application as the involvement of stakeholders in the design process is crucial díez and mcintosh 2009 mcintosh et al 2011 sandink et al 2016 we tested the prototype with potential users to gather suggestions for improvement after the prioritization of the stakeholders suggestions we implemented the first version of the system fig 1 and the following sections detail all steps of the implementation of yara 1 0 2 1 system overview the website is divided into six tabs the main page database about help downloads and contact fig 2 the main page gives access to the interactive map in this map users can select one of the 12 hydrographic regions of brazil by selecting a hydrographic region a new window appears where users can access the list of all studies available for the selected region fig 3 by choosing a specific study from the list a new tab opens and shows all the methodological aspects of the study e g title of the article models used model evaluation procedure adopted and the general information e g year of publication authorship the scientific journal where the study was published the digital object identifier doi fig 4 in the interactive map users can access the synthesis for three indices i e minimum streamflow mean streamflow and maximum streamflow each tab includes a synthesis chart a short interpretation of the chart the list of papers and their conclusions fig 5 heatmaps illustrate the synthesis of evidence of change which is a function of consistency quality and quantity of studies the terminology and the synthesis method are based on the ipcc s guidance for the evaluation and communication of the degree of certainty in assessment findings see mastrandrea et al 2011 the grey colours show the number of studies per quadrant quantity the x axis represents the consistency of the conclusions of the studies and the direction of the sign of change the further to the left the greater the consistency that the streamflow index will decrease the further to the right the greater the consistency of increase in that streamflow index the y axis represents the quality of the studies each study has a quality score ranging from 1 to 5 depending on the comprehensiveness of the study in sampling all sources of uncertainty along the modelling chain the higher the score the higher the quality of the study and therefore the confidence of its conclusion more details about the calculations and interpretation of the synthesis charts can be found in borges and chaffe 2019b the information regarding the conclusions of the studies is provided in a table below the text of the synthesis chart to guarantee transparency and a traceable account on the main page the database tab provides access to the literature database the tab about is a short introduction about the yara and the tab help offers a tutorial on how to use the tool the tab downloads provides the entire database of yara as well as the code package used to calculate the synthesis charts users can contact us through the contact tab 2 2 technological aspects the main purpose of the yara web application is to give users access to the synthesis assessments for each individual hydrographic region arranged in minimum mean and maximum streamflow changes all data i e synthesis charts interpretation texts a summary table and the information regarding the studies are stored in a database and a web based interface allows the user to extract information of interest for example changes in minimum streamflow in the paraná hydrographic region this section describes the two major system components of the yara 1 the database and 2 the user interface 2 2 1 database the database comprises 1 a table storing the attributes of the studies 2 a table storing the conclusions of the studies sorted by hydrographic regions 3 thirty six figures illustrating the synthesis chart i e 3 streamflow indices for each of the 12 hydrographic regions and 4 texts with the interpretation of the thirty six synthesis charts the synthesis charts are calculated separately using a code written in exelis idl 8 5 2 2 https www harrisgeospatial com the variable quality is calculated based on methodological aspects of the studies which are described in the table of attributes the consistency is a result of the interpretation of the main conclusions of the studies which are described in the table of conclusions see more details about the calculation of the synthesis charts and assumptions in borges and chaffe 2019b 2 2 2 user interface the tool runs on a web server and can be accessed via standard web browsers for the construction of the website we used the hypertext markup language html and for the interactive map we applied extensible markup language xml files and geographic javascript object notation geojson butler et al 2016 fig 6 shows the flowchart of the user interface the interactive map was first constructed in the kml file and later translated into the geojson format the description element is stored in the geojson format as a property of each geometric object when the user clicks on the hydrographic region a window with the content stored in the geojson property description opens each hydrographic region is a placemark with polygon geometry that stores a set of hypertexts in the description element the hypertexts embrace the list of studies and synthesis assessments for each streamflow index see fig 3 the content is structured in a hierarchical tree by streamflow index element and hydrographic region subelement the program also generates individual pages with information about the methodological aspects of each study that are stored in xml files see fig 4 with this structure it is possible to keep the information about the methodological aspects of each study and the synthesis assessments updated as new studies are incorporated into the database 2 3 testing and feedback the testing of the tool was done in five steps 1 preparation of the questionnaire 2 listing of the stakeholders 3 call for participation 4 collection and processing of the questionnaire responses 5 prioritization of the improvements fig 1 the main goal of the questionnaire was to assess the relevance of the yara in terms of access to the database the information about the scientific articles synthesis charts and explanation as well as its graphic user interface and navigability we also asked the participants about their profile and experience with similar systems in the context of climate change impacts to avoid overburdening the participants we followed the recommendation by crawford et al 2001 and limited the number of questions to a 10 minute response time we used google form 3 3 https www google com forms about to collect the information and the questionnaire is available in the supplementary material the list of participants was based on the authors and co authors of the articles included in the database of borges and chaffe 2019a through personal contacts and internet survey we identified and included in the list several relevant stakeholders from the brazilian national water agency agência nacional de águas ana civil servants from two brazilian ministries i e ministry of environment and ministry of science technology and innovation research institutions hydrometeorological services universities and private companies all stakeholders of the list were dealing with water resources and climate change and preferably but not only based in brazil we ended up with a list of 212 stakeholders and sent personal invitation letters by email we launched the call on september 5th 2019 and closed it on october 9th 2019 3 results 3 1 profile of the survey participants we gathered 107 responses response rate of 50 most of the participants of the survey were from universities 57 followed by research centres 17 and public management bodies responsible for the implementation of water related policies like ana and ministries 17 fig 7 hydrometeorological offices private companies and other institutions are represented by three people each ninety three percent of the participants make use of information about the impacts of climate change fig s1 in supplementary material the vast majority of them get that information from scientific articles synthesis reports and databases fig 8 meanwhile 50 respondents said that their institutions perform their studies forty five respondents conduct literature reviews while 21 said that they hire studies from other institutions the diversity of the survey sample is representative despite the dominance of academics we gathered a considerable sample size of decision makers the vast majority of the participants match the profile of a potential user of the yara tool which is a person interested in scientific evidence for scientific research decision making and standards and legislation fig s2 in supplementary material 3 2 perception of the stakeholders about yara thirty nine percent of the respondents said that yara is highly applicable in the context of their work and 48 considered it somehow useful fig s3 in supplementary material we also explored the participants perception of the relevance of the functions provided by the tool fig 9 yara has four main functions that provide 1 access to the literature database 2 information about the methodological aspects of the articles 3 synthesis charts and 4 interpretation of the charts the majority of the survey participants considered all functions as highly relevant only a few respondents considered the relevance of a function as low or very low e g the worst case was seven respondents for interpretation of the charts function apparently the navigability of the system is satisfactory the vast majority of the stakeholders pointed out that the time consumed to access the information provide in yara was either sufficient 68 or very quick 28 fig s4 in supplementary material 3 3 improvements the survey participants suggested several improvements due to time technical constraints and in some cases the need for a deeper involvement of the users see mcintosh et al 2011 we applied a prioritization procedure for improvements considering two criteria 1 the relevance of the improvement according to the purpose of yara and 2 the feasibility of its implementation considering the limited human resources and or the need for users participation we adopted a scoring scheme using a scale from 1 low to 3 high although subjective the scoring was a consensus among the authors of this study we implemented all improvements that reached a total score of 4 5 and 6 with feasibility 2 or above table 1 shows the improvements suggested by the survey s participants and the score of the prioritization procedure in terms of the information about the articles we included the title of the paper the objective the full name of the streamflow indices other variables and indices analysed e g rainfall evapotranspiration the description of the results of the validation procedure the main conclusions and the suggestions for further studies we have not included the source of the data used nor the spatial scale considered in the downscaling in its current form the quality scoring of the articles is not influenced by those attributes see borges and chaffe 2019b and thus we considered both suggestions of low relevance some participants suggested the inclusion of the main figures of the articles we believe that is highly relevant however copyright issues need formal requests that make its implementation very time consuming alternatively yara provides the doi link of the original article the major improvement in new functions and navigability resources was the inclusion of an interactive table in the database tab that lists the whole literature database fig 10 in this table users can filter the studies using specific keywords like the name of a hydrological model or a watershed in the same tab we implemented a registration form for new studies eighty one percent of the respondents are willing to register their studies in the yara system fig s5 in supplementary material the motivations are vast but several mentioned the opportunity to make their studies more visible once a new study is registered we will first check the consistency of the information with the original paper before updating the database one participant recommended an english version of the yara tool indeed considering the disclosure of this tool to international audiences an english version is highly relevant and was therefore implemented the user tutorial is a fundamental part of any tool mcintosh et al 2011 sandink et al 2016 users should be able to use the yara correctly and yet in a rapid and simple manner ninety three percent of the participants considered the tutorial satisfactory fig s6 in supplementary material but some participants suggested prompt explanations about the interpretation of the charts and for specific terms we therefore implemented pop up explanation windows fig 11 represented by question mark symbols near charts and dashed underlines under specific terms like consistency and quality additionally we produced a tutorial video explaining all steps and functions of the tool see borges et al 2020a as well as a promotional video see borges et al 2020b we considered as highly relevant to illustrate in the main map the limits of the watersheds adopted in the studies and to permit the calculation of the synthesis charts by selecting specific keywords and or geographic coordinates however specific needs may lead to a negligible amount of studies rendering the feasibility of its implementation to low before its implementation more details about the need for this kind of function is required mcintosh et al 2011 4 discussion web technologies for climate change adaptation issues are extremely valuable e g alder and hostetler 2015 awad et al 2009 knight et al 2015 mannschatz et al 2016 mcdonald et al 2019 pizarro et al 2018 simonovic et al 2016 whateley et al 2015 web based tools like yara permit the interaction with users allowing them to extract specific information according to their interests and needs these tools offer easy access wide availability and constant updates mcdonald et al 2019 simonovic et al 2016 whateley et al 2015 one of the challenges is that computer based tools are justified only if users make use of them mcintosh et al 2011 sandink et al 2016 the low application of some computer based systems is mainly a result of not properly involving users in the implementation phases díez and mcintosh 2009 mcintosh et al 2011 sandink et al 2016 the participation of users in a pre implementation stage is likely to improve the design of the functions of the system and the way the results are communicated díez and mcintosh 2009 sandink et al 2016 the absence of users in the pre implementation phase could be seen as a weakness of the yara an issue also reported in sandink et al 2016 another challenge is to ensure longevity as many web based initiatives struggle with a lack of resources for operation maintenance and updating díez and mcintosh 2009 mcintosh et al 2011 sandink et al 2016 there are several recommendations of best practices to achieve the intended outcomes of environmental web based systems we sorted out the recommendations from the literature that apply to the context of yara table 2 we highlight the best practices adopted in the implementation of yara and discuss alternatives to address those that were partially or not considered so far a user friendly interface should be based on the user s needs and capabilities awad et al 2009 freeman et al 2019 mcintosh et al 2011 sandink et al 2016 in the case of yara we first proposed a user interface based on simonovic et al 2016 for the wireframe and based on borges and chaffe 2019b for the way results are displayed we validated that with users through a survey it is recommended to involve the users in the pre implementation phase díez and mcintosh 2009 mcintosh et al 2011 sandink et al 2016 but that requires substantial efforts to engage users mcintosh et al 2011 that is more feasible in the context of large projects like in sandink et al 2016 and that was not the case of yara despite involving the users only in the post implementation phase the survey shows that the user interface of yara was satisfactory see fig 9 and fig s4 it is important to identify the end users and stakeholders mcintosh et al 2011 sandink et al 2016 as the use of a computer based system is more likely when the user foresees benefits díez and mcintosh 2009 sandink et al 2016 we mapped more than two hundred stakeholders and almost 40 percent of the survey respondents said the yara is highly applicable in the context of their work fig s3 another best practice is to develop a system that is relatively easy and inexpensive to use alder and hostetler 2015 díez and mcintosh 2009 mcintosh et al 2011 sandink et al 2016 in this aspect the yara was perceived as relatively easy to navigate fig s4 and it is free for use mcintosh et al 2011 recommend starting simple and small by developing a tool based on known technology the yara is hosted on the web allowing easy access through internet browsers avoiding therefore extra efforts with software installation the technology used in yara is mainly an interactive map and a table filtering the survey participants highlighted that the system is very easy to navigate and we assume that this technology is not a barrier for the intended users yara offers a broad option of help which includes a video a help tab and pop up windows as recommended by mcintosh et al 2011 and sandink et al 2016 as endorsed by mcintosh et al 2011 we dedicated time and resources for the survey and more than a hundred stakeholders responded to it establishing trust and credibility is essential for that mcintosh et al 2011 and sandink et al 2016 emphasize the importance of being transparent about weaknesses of the system and areas that leave room for improvement like model uncertainties and assumptions in the case of yara uncertainties are intrinsically expressed in the synthesis charts where the y axis illustrates the quality of the studies and the x axis expresses the consistency of the conclusions of the studies all synthesis charts are interpreted and described in detail including possible bias due to interdependency among the studies additionally the limitations of the synthesis method are documented in the help tab tools are developed to be used after implementation and guaranteeing their longevity is dependent on stakeholders built capacity díez and mcintosh 2009 mcintosh et al 2011 sandink et al 2016 although the participants of our survey mentioned that synthesis charts are useful it is not clear if they interpreted the charts properly apart from the tutorial material we believe that extra attention is needed to instruct users to interpret the synthesis charts correctly one option is to offer webinars to train users sandink et al 2016 at the same time user support should be continuous díez and mcintosh 2009 mcintosh et al 2011 sandink et al 2016 we provided contact information for user support but since the tool addresses a very specific issue we do not expect a large number of users to use yara regularly another aspect of longevity is to ensure that the database can be easily updated díez and mcintosh 2009 knight et al 2015 mcintosh et al 2011 sandink et al 2016 the yara was made to be updated when new studies are registered in the database at this stage we calculate the synthesis charts separately in idl and then upload all figures and texts in the web interface this step requires time and is justified when a reasonable number of new studies are available therefore the updating process leaves room for improvement a solution for that is to use a software that combines data processing with web interactivity like the shiny r package chang et al 2017 which has been used in many web tools e g freeman et al 2019 owen et al 2019 whateley et al 2015 new functions should avoid unnecessary complexity and should be included when demanded by the stakeholders knight et al 2015 mannschatz et al 2016 mcintosh et al 2011 sandink et al 2016 three participants of the survey requested a function that calculates the synthesis charts according to specific parameters but each of them mentioned distinct parameters and there is a need for better shaping that demand similar synthesis initiatives in the biomedical field freeman et al 2019 owen et al 2019 recognize the value of enabling the users to explore different datasets assess the impact of modelling assumptions and scrutinise any concerns they may have however to avoid low application of a new function it is strongly recommended a deep involvement of the stakeholders through a participatory process freeman et al 2019 knight et al 2015 mcintosh et al 2011 sandink et al 2016 we believe that extensions of yara should include the provision of downloadable summary reports alder and hostetler 2015 the exploration of new visualization schemes harold et al 2020 owen et al 2019 and even the application of a bayesian framework freeman et al 2019 gurevitch et al 2018 owen et al 2019 additionally the inclusion of studies that address the current changing state of the water resources is foreseen as a means to obtain more robust evidence borges and chaffe 2019b future improvements of the yara should consider the participation of stakeholders beyond online surveys for instance through workshops freeman et al 2019 knight et al 2015 sandink et al 2016 that may require a business plan to define costs outcomes and necessary financial support mcintosh et al 2011 we believe that the application prospect of yara is vast at the individual level the tool provides information for literature reviews on the topic reducing the workload needed by researchers and practitioners at the national level yara can support upcoming climate change assessments like the pbmc 2014 cgee and ana 2014 as well as to the implementation of the brazilian national adaptation plan to climate change brazil 2016 since the method is based on ipcc s guidance the results can contribute to both the freshwater resources and central and south america chapters of future assessment reports of the ipcc s working group ii see jiménez cisneros et al 2014 magrin et al 2014 5 conclusions in this paper we developed and implemented a web based tool that allows easy access to the synthesis assessment and the literature database of the impacts of climate change on the brazilian territory the yara was launched in november 2019 is freely available on the website www labhidro ufsc br yara and discloses the results of a broad literature database i e borges and chaffe 2019b in a manageable and user friendly way the website was constructed in html language data processing is done in idl and for the interactive application we used xml files and geojson a prototype of the tool was tested by 107 stakeholders and an online survey collected relevant recommendations for improving the tool most of the survey participants were from universities with a significant number of stakeholders from research centres and public management bodies responsible for the management of water resources in brazil the majority of the participants make use of scientific articles and synthesis assessment as sources of information in their daily duties in general the perception of the participants about the yara was very positive the tool provides four functions the literature database the information about the methodological aspects of the studies the synthesis charts and the interpretation of the results all four functions were rated as highly relevant and navigability of the system was deemed as satisfactory both by the majority of the stakeholders at the same time more than a third of the respondents said the yara is highly applicable in the context of their work facts that confirms the potential of use of the tool we collected several recommendations for improving the tool and selected the most relevant suggestions by using a prioritization scheme the most significant improvements were 1 an interactive table that permits the filtering of the list of studies 2 the inclusion of help popup windows 3 the possibility of registering new studies and 4 an english version of the tool in future versions we will include a function that permits the calculation of the synthesis charts by selecting specific parameters like geographic coordinates and keywords there is still work to do on advertising engaging more users monitoring the usefulness and updating the database the yara may support literature reviews national assesments and expert judgements to inform decision makers the approach is flexible and can be easily replicated to other countries regions and contexts we believe yara represents an important step to achieve a more accessible manageable and yet easy to update synthesis assessments about the impacts of climate change supporting therefore robust adaptation strategies towards a more resilient society declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we thank the contributions of all 107 stakeholders that took part in the survey as well as the laboratory colleagues that tested the prototype and the survey questionnaire we acknowledge the brazilian national council for scientific and technological development conselho nacional de desenvolvimento científico e tecnológico cnpq for funding this study grant number 159528 2018 6 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104848 
25971,a multi reservoir simulation optimization model graps generalized multi reservoir analyses using probabilistic streamflow forecasts is developed in which reservoirs and users across the basin are represented using a node link representation unlike existing reservoir modeling software graps can handle probabilistic streamflow forecasts represented as ensembles for performing multi reservoir prognostic water allocation and evaluate the reliability of forecast based allocation with observed streamflow graps is applied to four linked reservoirs in the jaguaribe metropolitan hydro system jmh in ceará north east brazil results from the historical simulation and the zero inflow policy over the jmh system demonstrate the model s capability to support monthly water allocation and reproduce the observed monthly releases and storages additional analyses using streamflow forecast ensembles illustrate grap s abilities in developing storage reliability curves under inflow forecast uncertainty our analyses show that graps is versatile and can be applied for 1 short term operating policy studies 2 long term basin wide planning evaluations and 3 climate information based application studies keywords reservoir modeling software development simulation optimization water resources management 1 introduction water allocation among municipal industrial and agricultural sectors requires thorough integration of current supply and demand along with potential climate change population growth and ecological considerations over the river basin singh et al 2015 most large river systems typically have multiple reservoirs that are regulated to meet its design uses e g irrigation water supply hydropower considering several ecosystem and environmental constraints wang et al 2015a thus multi purpose multi reservoir operations encompass detailed analyses considering trade offs among conflicting uses kasprzyk et al 2009 for instance too little release may affect water quality and recreation while too much release may cause flooding singh et al 2015 the opposing nature of benefits associated with storing the water and profits associated with releasing the water contributes to the complexity of multi reservoir system operations yeh 1985 koustosyiannis et al 2003 li et al 2015 to understand the tradeoffs in multi sectoral water allocation over the river basin it is important the reservoir modeling framework should be capable of providing the tradeoffs under observed flows and forecasted flows which is typically represented in the form of ensembles sankarasubramanian et al 2009 the main intent of this study is to develop and validate a multi reservoir multi purpose reservoir modeling framework that can take probabilistic seasonal annual inflow forecasts for allocating water for multiple uses the operation of a reservoir system is likely to be subjected to both supply and demand variations which are typically provided as forecasts at subseasonal weekly to monthly to seasonal annual time scale it is important to analyze how these supply and demand variations impact the reliability of a given user and the probability of violating the target storage which are specified as rule curves sankarasubramanian et al 2009 golembesky et al 2009 currently reservoir modeling platforms typically use either deterministic forecasts provided as forecast mean median which ignores the probabilistic information and the forecast uncertainty on the mean a more rigorous approach is to analyze the multi reservoir system using probabilistic inflow forecasts specified as ensembles to support proactive and adaptive water management policies the utility of a multi reservoir modeling system is to support reservoir managers and operators for meeting different target demands and testing adaptive strategies including drought contingency plans based on the potential supply and demand maurer and lettenmaier 2004 several formulations of multi reservoir models have been in the literature which are well documented in several review papers yeh 1985 labadie 2004 ahmad et al 2014 commonly used mathematical programming techniques include linear programming models loucks et al 1981 belaineh et al 1999 network flow models hsu and cheng 2002 and interior point method seifi and hipel 2001 similarly non linear programming models have relied on sequential analyses to ensure convergence such as sequential linear programming barros et al 2003 sequential quadratic programming finardi et al 2005 and using generalized reduced gradient technique peng and buras 2000 studies have also used both dynamic programming and stochastic dynamic programming models alaya et al 2003 to reduce the dimensionality in the above mathematical programming models studies have suggested using simulation optimization models koustasyiannis and economou 2003 sankarasubramanian et al 2009 application of several novel heuristic techniques such as genetic programming tabu search and particle swarm optimization have also been used to solve multi reservoir models rani and moreira 2009 baltar and fontane 2008 reddy and kumar 2007 and references therein several agencies universities and private corporations have also developed multi reservoir modeling software packages hec ressim by u s army corps of engineers is used to simulate reservoir operations for flood risk management and real time decision support klipsch and hurst 2013 another popular software is modsim a generalized river basin software designed as a tool for making improved basin wide and regional strategies for management and water rights analysis labadie 2010 california department of water resources developed a general purpose reservoir river basin simulation model calsim that permits specifications of system description and operational constraints through a new water resources engineering simulation language draper et al 2004 a water resource planning tool weap developed by stockholm environment institute is capable of simulating water demand supply flows and storage and pollution generation treatment and discharge sieber and purkey 2015 waterware is a proprietary decision support river basin planning system that employs rule based concepts for developing operating criteria and policies jamieson and fedra 1996 another proprietary popular river basin modeling software is riverware developed by the center for advanced decision support for water and environmental systems cadswes zagona et al 2001 riverware is a river basin modeling tool that includes an extensible library of modeling algorithms solvers and a language for the expression of operating policy and is extensively used by many operating agencies such as bureau of reclamation and tennessee valley authority despite the availability of many multi reservoir models the implementation of these models under probabilistic inflow forecasts represented as ensembles requires running the model subsequently for each member or its deterministic form i e mean median of the forecast however detailed application of seasonal to interannual inflow forecasts for reservoir management shows the importance and incorporation of probabilistic constraints on target storage and release sankarasubramanian et al 2009 georgakakos and graham 2008 which cannot be handled by most of the above models the skill of seasonal climate forecasts over the last decade has improved considerably through a better understanding of teleconnection between the slowly evolving boundary conditions such as ssts in the tropical oceans and local hydroclimatology goddard et al 2003 devineni et al 2008 in grl wang et al 2015b low frequency climate variability such as el nino southern oscillation enso has been proven to influence streamflow in many parts of the world dettinger and diaz 2002 utilizing these climate forecasts with updated and corrected land surface conditions have resulted in improved streamflow and soil moisture forecasts wood et al 2002 sinha and sankarasubramanian 2013 mazrooei et al 2019 despite these advancements error propagation in downscaling and disaggregation of climate forecasts in developing streamflow forecasts wood et al 2005 sankarasubramanian et al 2009 mazrooei et al 2015 have caused the application of climate forecasts for real world water allocation to face challenges due to forecast uncertainty as well as due to institutional hierarchy pagano et al 2001 pagano et al 1999 broad et al 2007 these challenges necessitate the translation of uncertainty in climate forecasts into corresponding uncertainty in reservoir releases and storages li et al 2014 lu et al 2017 seasonal to interannual water allocation using a reservoir model based on climate information requires combining the initial storage conditions with the conditional distribution of streamflow specified as ensembles to develop with the forecasted probability of meeting the target storage for the user specified release sankarasubramanian et al 2009 georgakakos and graham 2008 considered a single reservoir to obtain an optimal solution for minimizing the squared deviation from the end of the season target storage under inflow forecast uncertainty maurer and lettenmaier 2004 evaluated the long lead hydrologic predictability represented as deterministic inflow forecast for improving hydropower generation from six reservoirs in the missouri river basin using an aggregated reservoir system representation probabilistic inflow forecasts developed from combining multiple gcms for a single reservoir falls lake in north carolina nc has been demonstrated to be valuable in invoking drought restrictions golembesky et al 2009 li et al 2014 considered inter basin transfer between two nc reservoirs falls lake and lake jordan using two separate single reservoirs for maintaining quality pool and water supply pool elevations under inflow forecast uncertainty wang et al 2015a used a single reservoir model to identify the trade offs between hydropower generation and ecological demands under inflow forecast uncertainty lu et al 2017 utilized multi time scale forecasts represented as ensembles in a single reservoir model for improving hydropower generation and reducing flood risk for a major hydropower reservoir in india thus most studies have used a single reservoir model or a simplified aggregated representation of a reservoir network for evaluating the utility of deterministic probabilistic inflow forecasts to improve water allocation to address this we propose a detailed multi reservoir simulation optimization model graps generalized reservoir analyses using probabilistic streamflow forecasts that considers the probabilistic inflow forecasts specified as ensembles along with probabilistic constraints on meeting the target storage i e rule curves to quantify the reliability of meeting the user specified releases the manuscript is organized as follows the generalized model formulation is presented in section two that can adapt to the complexity of interlinked reservoir systems by sequentially routing the flow from upstream to downstream graps is then applied to a system of reservoirs in ceará brazil to demonstrate graps capabilities in reservoir modeling and its abilities to accurately reproduce historical storage and flows results of the simulation are finally assessed under inflow forecasts and the performance of the forecast based application is validated with historical observations 2 graps formulation graps is extended from a water allocation framework as outlined in arumugam et al 2003 and sankarasubramanian et al 2009 that utilizes the benefits of ensemble forecasts of reservoir inflows to issue annual water contracts unlike many mainstream reservoir modeling tools graps is well suited to handle streamflow ensembles which translates forecast uncertainty into storage and release reliabilities fig 1 provides an overview of variables storages inflows and outflows for a given reservoir within the multi reservoir system the mathematical formulation for graps is outlined below assume there are n r reservoirs in a given basin with the index s s 1 n r denoting a particular reservoir the number of upstream reservoirs for the reservoir s is denoted by u s which includes reservoirs that contribute flows indirectly into reservoir s inflows into any reservoir could be grouped into two categories uncontrolled inflows and controlled flows uncontrolled flows are provided as ensembles denoting the conditional distribution two types of uncontrolled flows are considered in the model for the reservoir s natural inflows into the reservoir q t k s and spillage from the upstream reservoirs e x t k s controlled flows are of three types 1 releases and direct inflows from upstream reservoirs 2 return flows from command areas and wastewater from municipal and industrial use and 3 diversions and water from inter basin transfers or other sources controlled flows are expressed as functions of the decision variables of the multi reservoir water allocation model 2 1 reservoir variables 2 1 1 natural inflow natural inflows q t k s for time t 1 t k 1 k s 1 n r is the probabilistic streamflow forecasts with indices t k and s representing the time step monthly and above ensemble member and reservoirs respectively inflows both observed and forecasted ensembles are provided exogenously to the model by the user 2 1 2 spillage spillage is a result of uncontrolled spillway discharge net spillage from upstream reservoirs e x t k is the sum of the spill from all the upstream reservoirs after accounting conveyance losses which is estimated based on the past spill data at the reservoir and the reported streamflow in the downstream location i e reservoir node the term s p t k s from reservoir s is a derived conditional distribution of spill after accounting for releases diversions and evaporation 1 e x t k s 1 u s s p t k s 2 1 3 return flows from uses assuming there are n s uses in each reservoir and releases for each use r i t s return flows from the uses released from the upstream reservoirs u s could be calculated let n l be the number of lags the number of time steps it takes return flow to reach the reservoir then return flow in time step t into reservoir s r f t s can be estimated as 2 r f t s s 1 u s i 1 n s t t t n l f t i s s r i t s where f t i s s is the fraction of monthly releases from reservoir s that contribute to the current reservoir s with the contribution effective from previous releases n l months 2 1 4 direct release from upstream reservoirs direct outflows from upstream reservoirs d o t s results as part of instream requirement as well as excess water being released for hydropower generation and additional downstream needs here s denotes the upstream reservoir to the current reservoir s this can be expressed as 3 d i t s s 1 u s s s d o t s where s s is the fraction which quantifies the losses on the upstream reservoir s releases d o t s to the direct inflow of the downstream reservoir s s s is usually estimated based on historical upstream release and downstream recorded release between reservoirs s to s the optimization model treats d o t s as a decision variable which could be either considered as a hydropower plant if present or as a user with an ecological flow requirement to be met here we are treating the direct outflow d o t s from the release term r i t s for uses to indicate that they are not return flows and their consumptive use is very small 2 1 5 diversions and other transfers diversions for wildlife protection and other environmental inter basin transfers could contribute to additional inflows 4 d t s d 1 n d s η s d d t d η s d is the fraction representing the losses in the diverted quantity d t d d t d must be specified as part of the exogenous input to the model 2 1 6 net inflows net inflows q t k s is the sum of uncontrolled and controlled inflows into reservoir s fig 1 it is important to note that r f t s and d i t s are functions of decision variables of the allocation model whereas d t s must be specified as an input to the model this variable is not required but is used to simplify the equations 5 q t k s r f t s d i t s d t s e x t k s q t k s 2 2 reservoir simulation the minimum dead maximum and initial storages of reservoir s are represented by s min s s max s and s 0 s h s and s p max s are the elevation of spillway crest level and maximum spillway discharge respectively δ 1 s and δ 2 s are the storage area curve coefficients of the reservoir in line with the water contract specification see section 2 5 different restriction levels are imposed if the actual inflows are less than the forecasted inflows these restriction levels are defined as p r l s where l 1 n j s and r denotes the number of restriction level in a particular reservoir s ψ t s represents the monthly evaporation rates in each reservoir s to simulate reservoir operation the following mass balance equation is used to solve for the end of time step storage for each time step t for each ensemble member k 6 s t k s s t 1 k s q t k s e t k s i 1 n s r i t s d o t s d v t s s p t k s s d t k s for t 1 s t 1 k s s 0 s indicates the observed storage in each reservoir s equation 6 states that the end of time step storage is equal to the current storage plus the net inflow and any deficit s d t s minus spill s p t s all releases r i t s for uses direct downstream release from the reservoir d o t s diversions to other basins d v t s and evaporation e t k s the outflow term from the reservoir d o t s will provide the direct inflow to the downstream reservoir based on equation 3 after accounting the instream losses r i t s and d o t s are decision variables to the optimization model however diversion flows d v t s are specified exogenously equations 7 and 8 calculate the spill and deficit for each time step t 7 spill s p t k s s t k s s max s s t s s max s 0 o t h e r w i s e with s 1 n r 8 deficit s d t k s s min s s t k s s t s s min s 0 o t h e r w i s e with s 1 n r equation 9 requires that all reservoirs operate between their minimum and maximum storage levels 9 s t k s min s t k s s max s s t k s max s t k s s min s evaporation e t s is computed as a function of average storage for the current time step using the initial storage and the end of time step storage with the storage elevation relationship 10 e t k s ψ t s δ 1 s s t k s s t 1 k s 2 δ 2 s the depth of evaporation ψ t s is specified exogenously to the model because both s t s and e t s are unknown evaporation must be calculated implicitly this is done using the secant method for root finding press et al 1986 2 3 hydropower hydropower p is computed as a function of generator efficiency η the density of water ρ gravity g and the height difference between the reservoir level and the tailwater h t t w the reservoir level is given as a function of the storage elevation coefficients β 1 s and β 2 s and the average storage between time steps 11 p t k s η ρ g β 1 s s t k s s t 1 k s 2 β 2 s h t t w n r t s net release n r t s consists of release for hydropower as well as the releases for other uses that go through the turbines graps identifies hydropower as a separate use and also requests details on turbines along with details on whether the outlet for a particular use e g irrigation is available for hydropower generation although generator efficiency varies with elevation and flow rate the efficiency is considered constant for simplicity based on the above equation graps determines an ensemble of hydropower generation for conventional hydropower plants and pumped storage hydropower plants generation from run of river hydropower plants cannot be determined as they have little no storage 2 4 net benefit from the water allocation when graps is optimized the objective is to maximize the net utility of water allocations across all uses equation 12 describes the mathematical formulation of the objective function o denoting the basin wide net benefit from the allocation the revenue φ i s r i t s is the tariff paid for the release in each time period t allocated over the season for the ith water use from reservoir s tariffs in graps are expressed as functions of release to allow users to represent linear increasing block and decreasing block tariff structures compensations to users for not meeting the specified use r i t s is subtracted from the total revenue generated in the second term of the objective function these compensations are defined for each reservoir with γ i j s indicating the compensation to user i if restriction level j is imposed and w i j s is the demand deficit for the entire modeled period for user i and restriction level j ν i s is the compensation schedule for user i if the supply falls below the maximum allowed deficit the contract penalty ν i s is invoked if the difference between the total deficit for user i w i s and the maximum allowable deficit for user i w i max s equations 12 and 13 for further details of the above contract structure see sankarasubramanian et al 2009 12 o s 1 n s i 1 n t 1 t φ i s r i t s s 1 n s i 1 n j 1 n i γ i j s w i j s i 1 n ν i s w i s w i max s 13 where x 1 x 0 0 o t h e r w i s e 2 5 model constraints graps is subjected to both deterministic and probabilistic constraints the following deterministic constraints 14 17 are prescribed in the model for each reservoir for each time step t demand constraint 14 r i min s r i t s r i max s with i 1 n s s 1 n r inflow requirement between two reservoir segments ss 15 d i t min s n r t s s d i t max s with s 1 n r diversion demands 16 d t min s d t s d t max s with s 1 n r spillway constraint 17 0 s p t k s s p max s with s 1 n r target storage constraint 18 pr s t s s t r s n s t s s t r s n p s with s 1 n r user reliability constraint 19 pr w i s w i max s n w i s w i m a x s n p f i s with i 1 n s s 1 n r using ensemble input graps calculates the probabilistic constraints and also reports the probability of spill and deficit by counting how many times an event occurs e g meeting target storage represented as the operator n and dividing that by the number of members n in the ensemble in equation 18 s t s and s t r s represent the simulated end of time period storage and the target storage respectively for each reservoir s the model estimated prob s t s s t r s should be lesser than the target constraint p s in equation 19 w i s is the restriction level for user i at reservoir s and w i max s is the maximum allowable restriction level for a user at a reservoir based on this the model meets specified the user reliability 1 p f i s where p f i s denotes the model estimated failure probability for the current release patterns in addition to these probability performance measures graps also provides ensembles of hydropower spill and deficit for each reservoir at each time step graps can also operate under observed flows in which the probabilistic constraints in 18 and 19 are converted into deterministic constraints with the total number of ensembles k being equal to 1 the deterministic form of 18 and 19 ensures by forcing p s and p f i s being equal to zero 2 6 optimization simulation framework graps equations specified in equations 1 19 could be used in a stand alone simulation mode or in an optimization simulation mode under simulation mode graps performs the simulation across the cascade using equations 1 11 based on the user specified decision variables r i t s and d i t s and computes the model outputs such as ensembles of spill hydropower generation and storages along with the net benefit and probabilistic constraints 12 19 under the optimization simulation mode graps maximizes the net benefit in 12 based on the deterministic and probabilistic constraints in 14 19 to obtain the decision variables r i t s and d i t s using feasible sequential quadratic programming zhou and tits 1992 add a line on observed flows fsqp obtains the decision variables by solving both the deterministic and probabilistic constraints fsqp optimizes continuous functions and their derivatives and also considers finite differences when continuous derivatives are not available for the objective function and the constraints in this study we used both simulation mode and the optimization simulation mode to obtain the releases using finite difference approximation of constraints and objective function using both observed flows and climatological ensembles 2 7 graps model characteristics 2 7 1 node link representation similar to many other existing water resources systems modeling programs graps adopted a node link representation to characterize physical river basin networks even though the shapes of the rivers and reservoirs are arbitrary the underlying spatial configuration can be simplified and represented in the program by a two dimensional interconnected directed network of nodes and links each node in the model can represent one of six entities watersheds reservoirs inter basin transfers users junction nodes and sinks reservoirs watersheds and users are represented by system blocks and diversion locations and flow confluences are designated by junction nodes rivers streams and channels are designated as links and are defined with a direction and upper and lower bounds on flow capacity in such a node link representation a reservoir system begins from a watershed node and ends at a sink node a representation of this node link structure is shown in fig 2 such node link formulation provides an efficient and simple representation of the underlying reservoir system 2 7 2 ensemble input framework by using probabilistic streamflow ensembles graps can be used to investigate climate change effects on basin management and reservoir systems fig 3 illustrates how the simulation model is executed with a streamflow ensemble performing reservoir mass balance for all the traces in the ensemble as opposed to going over the entire cascade for each trace in the ensemble can also facilitate parallelization in computing which we are currently working on incorporating in graps the model simulates the most upstream reservoir time steps 1 to t and for ensembles 1 to k then goes to the next reservoir if a reservoir has multiple branches flowing into it such as the case with reservoir 3 in fig 3 graps will simulate all reservoirs on all of the branches upstream of that reservoir before simulating that reservoir 2 7 3 python interface using python 3 7 and pyqt5 a graphical user interface was developed to increase the usability of graps this interface streamlines the creation of the data files required to run graps and provides a method for visualizing the network cascade it is designed to allow a user to use intuitive keystrokes and mouse movements to create the network cascade and then use input dialogs for each system block to enter information an example network created with the interface is shown in fig 2 2 7 4 input like many other advanced water allocation models graps requires information on basin hydrology reservoir and users for a generalized reservoir model like graps input files are prepared and tailored to each reservoir system these input files specify the connectivity reservoir characteristics reservoir management and user demands of the modeled system of these files the most crucial input to the program is naturalized flow or streamflow that represents natural hydrology into every reservoir 2 7 5 output to model the reservoir system specific information describing individual reservoirs must be provided this information includes the area storage relationship current reservoir storage and water demands for disparate water uses the user is required to provide reservoir system specific information regarding network and diversion blocks specific details about users such as information about demands and water contracts must also be provided simulation results comprise of variables such as storage reservoir releases releases to users hydropower generation spill and deficit the main result of the ensemble simulations is the reliability of meeting end of period target storage for each reservoir 2 7 6 connectivity the reservoir network is represented as an acyclic directed graph with a single terminal node the sink a hierarchical tree structure in which nodes and users ordered from upstream to downstream is used to store the reservoir network in the model each network must have at least one watershed and exactly one sink to simulate a network with multiple sinks one can use junction nodes in place of the individual sinks and then connect the junctions to a single artificial sink fig 4 shows how individual elements of the reservoir network can be connected 3 graps application using ensemble forecast for a multi reservoir system 3 1 study area jaguaribe valley ceará brazil the purpose of this case study is to demonstrate graps modeling capability to accurately simulate historical operations the case is based on the jaguaribe river basin fig 5 a basin situated in the semiarid state of ceará in the northeastern part of brazil with a drainage basin covering an area of 75 961 07 km2 the jaguaribe river extends for about 610 km and its discharge can range from zero to 7000 m3 s campos et al 2000 the main water management challenge in the region is to retain water in reservoirs in rainy years and to manage it such that it will last for several years johnsson and kemper 2005 another challenge is the increasing dependence of the capital fortaleza one of the largest and fastest growing cities in brazil johnsson and kemper 2005 in a 2007 study broad et al 2007 pointed out that a third of ceará s population is rural and most of the population is in the agricultural sector persistent poverty and drought have created an ongoing vulnerability the reservoirs in the upper jaguaribe river basin provide water for agricultural uses including agribusiness and small family farming the downstream reservoirs provide water for municipal use for the city of fortaleza and other small towns in the region 3 2 modeled reservoirs the generalized model is applied to 4 reservoirs in the jaguaribe river basin the modeled reservoirs in this case study are orós banabuiú pacajus and pacoti water is diverted from the jaguaribe river basin to the pacajus reservoir via canal do trabalhador worker s canal the canal is a diversion medium that supplies water to any user along the way through a small canal via a pump station water is again delivered from pacajus reservoir to pacoti reservoir table 1 summarizes the four modeled reservoirs in this case study the largest reservoir in this study is orós with a maximum storage of 1940 hm3 pacajus reservoir on the other hand is the smallest reservoir with a maximum storage of 240 hm3 due to the aridity of the region the minimum storages for all the reservoirs are low additionally most of the annual inflow happens from january to june pacoti and pacajus have higher minimum storages due to their role in ensuring water supply to the nearby city of fortaleza 3 3 input historical streamflow and reservoir data were provided by dr de assis de souza filho at the federal university of ceará fortaleza brazil however historical inflow and reservoir levels were dated before the year 2000 castanhão reservoir a sizable reservoir in the region was constructed after 2000 consequently castanhão reservoir was not included in the model 3 4 streamflow ensemble considering the lag one correlation between the annual flows is close to zero an ensemble of climatological streamflow forecasts was developed from the historical inflows for the corresponding month from 1913 to 2000 to populate 100 ensemble forecasts arumugam et al 2003 this ensemble of forecasts was generated by bootstrapping a simplistic resampling technique that draws randomly from a set of data points and allows replacement 3 5 zero inflow policy since half of the year july december there is zero inflow into the orós reservoir the water management agency in ceará brazil cogerh assumes zero inflow for the upcoming twelve months for additional details on the zero inflow assumption and its merits in water allocation see sankarasubramanian et al 2009 this conservative approach allocates water based on the beginning of the year storage as a result when the reservoir is simulated with the observed inflow the reservoir may spill in some instances 3 6 schematic representation of the modeled system fig 6 illustrates how the multi reservoir system is schematically represented in the program the network contains reservoirs connected in series and parallel at the very top of the graph are orós and banabuiú reservoirs since the agriculture sector dominates the state and most of the users are agriculture users we simplify the modeling by assigning orós and banabuiú to have only one aggregated agriculture user each node 1 a junction node is used to represent a point of river confluence and to gather upstream reservoir and user releases from orós and banabuiú canal do trabalhador is represented in the network as a node and is modeled as a user that delivers water from node 1 to the pacajus reservoir due to its function as a small relay reservoir pacajus reservoir only has one user a pump that delivers water from the reservoir to pacoti reservoir finally pacoti reservoir supplies all the drinking water to the city of fortaleza which is represented as a municipality user node although there are two distinctive watersheds interbasin transfer is not needed as the two watersheds are represented within one system model denoted as a sink the atlantic ocean receives return flows from lower jaguaribe agriculture node 1 pacoti reservoir and the city of fortaleza 3 7 assumptions for simplicity and illustration several assumptions were made in modeling the ceará reservoir system both orós and banabuiú are linked to only agriculture users as the municipal demand in the rural area is very small given that the jaguaribe metropolitan system is primarily operated based on priority based allocation see sankarasurbamanian et al 2009 broad et al 2007 the application of graps was primarily implemented as a simulation model additionally the simulation period is chosen to be from january 1997 to december 1999 as it encompasses a wet 1997 normal 1998 and dry 1999 years lower jaguaribe agriculture is the only user in the lower jaguaribe river and represents all the water demands in that area to account for water loss due to consumptive use it is assumed that return flows from agricultural municipal and industrial users are 40 90 of the water allocation 4 results and discussion the primary objectives of the case study are to demonstrate graps capability to model a complex reservoir system and validate the program s ability to accurately compute flows and reservoir storages and to generate storage reliability curves from ensemble inputs the multi reservoir system is modeled for an entire calendar year and for a three year period with monthly time steps in this case travel time for return flows it is not useful so it is not considered simulation results are presented in the following subsections 4 1 model validation fig 7 shows simulated flow routing for different seasons in 1998 for this purpose graps was run in a simulation mode with observed flows i e total number of ensembles k 1 and the rhs of equations 18 and 19 are set to zero as shown in the network diagram fig 6 orós and banabuiú are the two uppermost reservoirs and receive natural inflow as a result inflow into orós and banabuiú is part of the flow routing demonstration proper routing is an important function of any multi reservoir simulation program the preceding result indicates that the program is routing various flows natural flows reservoir releases and user return flows correctly from upstream to downstream and through junction nodes graps allows for the specification of loss fractions to incorporate consumptive use because a junction node is a place for gathering releases and return flows from upstream and distributing the flow to downstream without using the water total inflow into node 1 is the same as the total outflow from node 1 to demonstrate graps ability to model and optimize reservoir systems over a multi year time horizon the ceará system is optimized using observed inflows i e total number of ensembles k 1 and the rhs of equations 18 and 19 are set to zero from january 1997 to december 1999 using fsqp fig 8 this period is chosen because 1997 is an abnormally wet year 1999 is a dry year for the region in 1998 the inflow into the system is near the long term averages the high inflow in 1997 results in three months of spill from orós and banabuiú and two months of spill from pacajus because spill from orós and banabuiú can flow directly to the atlantic ocean their spill flow does not impact the storages of pacajus and pacoti in april of 1999 releases for industrial use taper off and become zero this can be attributed to the critical need of municipal use during dry years 4 2 optimized reservoir system analysis using climatological inflow ensemble in fig 9 the impact of three different inflow scenarios climatological ensemble zero inflow forecast and perfect forecast i e observed flows on the estimated spill for orós and banabuiú in 1997 is analyzed by optimizing the releases using fsqp the optimized releases for three inflow scenarios were run with the observed flows to evaluate how the system would have performed under each inflow scenario the climatological ensemble k 100 was developed by bootstrapping the observed flows over the period 1913 2000 by assuming each year has the equal probability of occurrence for the zero inflow for t 12 months and observed inflow policies k was set to 1 and the deterministic form 18 and 19 was considered for optimization using fsqp because extended severe droughts are prone in the jaguaribe river basin a zero inflow policy was applied as a conservative measure by the local government for seasonal water allocation see sankarasubramanian et al 2009 this zero inflow policy is considered here as well as using the observed monthly inflows and the climatological ensemble the spill distribution for the optimized release under climatological ensemble is smoothened with a guassian kernel density estimator fig 9 the spill that would have occurred for the optimized release under three inflow scenarios is also shown three straight lines spill volume indicated in legend which is obtained by running graps in a simulation mode with the respective releases under each inflow scenario the zero inflow policy resulted in the most spill for orós followed by the climatological flows and then the observed flows this is along the expected lines as zero inflow policy being conservative has estimated lower release resulting in higher spill this is followed by lesser spill based on the release under climatological ensemble and the optimized release under perfect forecast i e observed flows result in the lowest spill for banabuiú all three policies result in similar yearly spill volumes due to its limited storage the results from the climatological spill indicate that orós banabuiú forecasted spill is 10 7 probability analyzing the spill density it is evident that most of the inflow ensemble members result in no or very little spill for both reservoirs at the beginning of 1997 orós and banabuiú each have approximately 900 hm3 of unused storage thereby the forecasted climatological spill distribution is small though the ensemble used in this analysis is simplistic with no forecast skill the ability of graps to effectively handle inflow ensembles and optimize the release under inflow uncertainty is effectively demonstrated fig 10 shows the ability of graps in estimating the optimal releases that meet various target storage reliabilities p s using climatological ensemble for the year 1998 target storage reliability is a critical constraint in ensuring enough water at the end of the planning period golembesky et al 2009 li et al 2014 for fig 10 the feasible sequential quadratic programming fsqp algorithm is used to maximize the system benefits given a bootstrapped climatological ensemble for the natural inflow into the two reservoirs in the jmh system the target storages for orós 1100 hm3 and banabuiú 400 hm3 are chosen to provide enough storage to meet demand from multiple sectors given two years of no inflow into the reservoirs sankarasubramanian et al 2009 the initial storage for both reservoirs is set as the observed storages at the beginning of january 1998 1425 4 hm3 for orós and 728 9 hm3 for banabuiú the end of the season target storage reliability is computed by counting the number of times end of period storage equal or above the target storage over the entire ensemble and the specified p s is given as a constraint in 18 the target storage reliability p s is increased in increments of 5 from 55 to 100 as a constraint in the optimization simulation model to obtain the optimal releases under each case using climatological ensemble as the target storage reliability increases the optimal yearly release decreases for banabuiú the relationship flattens for reliabilities greater than 70 i e small changes in release result in large changes in reliability this suggests that the low flow years for banabuiú are similar and because the jaguaribe river is known to go long periods of time with little to no flow likely nearly zero inflow years because the climatological inflow provided is bootstrapped from historical inflow data it shows the drier nature of flows occurring over the arid jmh basin such discrete and continuous nature of inflow ensemble can cause large discontinuities in the reliability constraints in 18 and 19 under these situations the optimal solutions from fsqp typically ends indicating that the new solution is numerically equivalent to the previous best solution which indicates that the objective function has flattened out in the search space even though this sounds logical under the discrete i e zero flows and continuous nature of the density of inflows evaluation of the optimized solutions with other optimization algorithms such as particle swarm optimization and iterative linear programming could reduce the uncertainty in the optimal solutions given graps is designed to work with any optimization algorithms as a stand alone optimization simulation model our future effort will evaluate various algorithms in providing optimal solutions with graps 5 discussion and concluding remarks graps a next generation multi reservoir simulation program is presented and detailed as an optimization simulation model the program uses simulation for reservoirs and junction nodes and optimizes the releases for multiple users that maximizes the net benefit from allocation by considering deterministic constraints and probabilistic constraints on target storage and user deficits in this study we demonstrated the optimization using fsqp but in principle graps can be called using any non linear programming solvers graps can also be run in a simulation mode to obtain the target storage reliability and reliability of obtaining user specified releases equations 19 and 19 graps can perform water allocation using observed inflows or using the seasonal annual inflow forecast ensemble graps uses a node link representation with reservoirs watersheds junction nodes and users represented as nodes and rivers streams and channels represented as links the program routes the flow of water from upstream to downstream both spatially and temporally through a connected network of different reservoir elements as opposed to existing reservoir models such as riverware and hec ressim graps can handle ensemble forecast to translate the inflow uncertainty into appropriate probabilistic information on the target storage equation 18 and the reliability of allocating the user specified amount equation 19 by optimizing simulating the releases unlike a site specific reservoir model graps can be applied to any reservoir systems and to help the process of setting up a python user interface is developed in contrast to commercial software graps is free to use for noncommercial and educational purposes and can be downloaded from https github com lcford2 graps git graps formulation is tested in a simulation mode fig 7 in routing the flow through the four reservoirs fig 7 and also evaluated in an optimization simulation mode by performing multi sectoral water allocation fig 8 using observed flows for the jmh basin in ceara north east brazil graps was also evaluated in estimating the spill by optimizing the releases under three inflow scenarios climatological ensembles zero inflow and perfect forecast i e observed flows during a wet year fig 9 analyses show zero inflow forecast estimate the highest spill amount followed by climatological ensembles with the perfect forecast providing the least amount of spill when the optimized release for the three scenarios were run as a simulation using observed flows graps was also evaluated in optimizing the releases in meeting different target storage reliability p s values under climatological ensembles analyses show clearly as p s increases total release for all uses decreases using fsqp but to get convergence on the optimized release it is important that the inflow forecast ensembles should be well calibrated otherwise the uncertainty in optimized releases should be analyzed with various optimization solvers graps maximizes the net benefit from multi sectoral water allocation considering seasonal annual inflow uncertainty challenges in estimating the target storage reliability particularly during dry years as the nature of the inflow distribution tends to be mixture i e discrete and continuous distribution thus estimation of target storage reliability and reliability of allocating the user demand depends on the skill of inflow forecasts and their ability to predict the observed frequencies of various events this implies that apart from the accuracy e g correlation of the inflow forecasts it is important that the forecast needs to be well calibrated between the forecast probability of wet dry years with their observed frequencies multimodel forecasts developed by combining multiple climate forecasts and hydrologic models to develop well calibrated inflow forecasts sinha and sankarasubramanian 2013 application of multimodel inflow forecasts have benefitted in improving the hydropower generation oludhe et al 2013 and in setting up restrictions during drier years golembesky et al 2009 thus providing an inflow forecast that is skillful and well calibrated could result in reliable estimation of the conditional probabilities related to management attributes p s and pf i which is critical if the forecast skill is significant only during a particular season e g winter spring given graps can also work with other solvers the optimized releases should also be tested with other solvers for ensuring global optima this is particularly important if the number of decision variables i e n s n r t increases for a large system that has multiple uses graps is designed to maximize the expected net benefit in considering the revenue and penalties in allocating water for multiple uses this study considered only three uses i e municipal industrial and irrigation but other uses such as hydropower flood control and recreation could also be considered formulation on hydropower is already included and it could be included in the revenue part of the net benefit flood control benefits could be considered explicitly if the simulated storages do not violate the flood control space previous deterministic reservoir modeling efforts have suggested approaches for incorporating flood control benefits simonovic and marino 1982 similarly recreation benefits can also be incorporated if the monthly simulated storages within the desired reservoir levels that support recreational benefits any violation of those storage spaces could also be considered as a penalty into the net benefit specified in equation 12 for additional details on estimating recreational benefits see cordell and bergstrom 1993 other ecological benefits such as instream flow maintenance could also be incorporated explicitly by considering ecosystem maintenance as a user incorporation of these additional benefits could be implemented by modifying the get net ben subroutine related to the net benefit function or by defining them as a user in the system graps is primarily designed to support multi sectoral water allocation considering seasonal annual inflow forecasts it has not yet been applied for daily streamflow forecasts in which the routing and loss coefficients i e coefficients in equations 2 4 could be quite significant depending on the inflow conditions as graps can estimate the monthly seasonal forecasted hydropower potential it can also be linked with power system model for supporting seasonal power generation planning and maintenance efforts are currently underway in linking graps with an energy system model temoa https temoacloud com for the tva system that includes 28 hydropower reservoirs 3 nuclear power plants and 23 fossil fuel coal and natural gas plants ford et al 2019 similarly graps can also be extended for long term planning studies considering climate change projections e g singh et al 2006 under such conditions the inflow forecast will become inflow projections developed multiple climate model s projections under such long term planning conditions the initial conditions of the reservoirs play a limited role but the future demand and inflow conditions play a critical role even though graps cannot explicitly estimate modified rule curves for potential capacity expansion it can estimate the probability of meeting the target storage p s and probability of meeting the user specified demand pf i under projected inflow and demand conditions by estimating the target storage for different p s and pf i for different storage values one could choose the target storage that will ensure desired target storage probability and reliability for different uses under projected climate and demand scenarios these are critical modeling efforts that link water system with both energy and food systems for analyzing their performance under changing climatic conditions software availability graps is written in fortran 90 and uses the fortran feasible sequential quadratic programming ffsqp for optimization package zhou and tits 1992 graps was developed by the authors of this article and is available as free and open source software on github at https github com lcford2 graps contained in the repository is the source code along with pre compiled executables for linux and windows compilation was performed using intel compilers for fortran this repository was made public in march 2019 the source code is less than 200 kb and the executable is slightly more than 1 mb depending on the operating system and the compiler used an example is included in the repository that is based on the reservoir system in ceará brazil along with explanations of the input files declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this material is based on work supported by the national science foundation under grant no 1442909 and grant no 1805293 any opinions findings and conclusions or recommendations expressed in this material are those of the author s and do not necessarily reflect the views of the national science foundation notations variables noted with a star are represented as ensembles in graps d i direct inflow from upstream reservoir d o direct outflow to downstream reservoir d v diversion releases from a reservoir n r net release through reservoir turbines loss fraction n r total number of reservoirs u s upstream reservoirs t number of time steps n l number of lags r f return flow n s number of uses for each reservoir n r number of restriction levels for each reservoir f the fraction of monthly releases from an upstream reservoir that contribute to the current reservoir with the contribution effective from previous releases nl months β monthly demand fraction n d number of diversions d diversion inflow into a reservoir η diversion loss fraction q natural inflow e x spill inflow from upstream reservoirs k ensemble number s p spill outflow from a reservoir s d deficit e x net spillage q net inflow r release for each use p f failure probability p r l restriction level probability s min minimum storage s max maximum storage s 0 initial storage s reservoir storage h elevation of spillway s p max maximum spillway discharge δ storage elevation curve coefficients e evaporation ψ evaporation rate lake evaporation depth p hydropower k generator efficiency ρ density of water g gravity constant h the height difference between headwater and tailwater elevations n total number of ensembles w user restriction level w user demand deficit appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104802 
25971,a multi reservoir simulation optimization model graps generalized multi reservoir analyses using probabilistic streamflow forecasts is developed in which reservoirs and users across the basin are represented using a node link representation unlike existing reservoir modeling software graps can handle probabilistic streamflow forecasts represented as ensembles for performing multi reservoir prognostic water allocation and evaluate the reliability of forecast based allocation with observed streamflow graps is applied to four linked reservoirs in the jaguaribe metropolitan hydro system jmh in ceará north east brazil results from the historical simulation and the zero inflow policy over the jmh system demonstrate the model s capability to support monthly water allocation and reproduce the observed monthly releases and storages additional analyses using streamflow forecast ensembles illustrate grap s abilities in developing storage reliability curves under inflow forecast uncertainty our analyses show that graps is versatile and can be applied for 1 short term operating policy studies 2 long term basin wide planning evaluations and 3 climate information based application studies keywords reservoir modeling software development simulation optimization water resources management 1 introduction water allocation among municipal industrial and agricultural sectors requires thorough integration of current supply and demand along with potential climate change population growth and ecological considerations over the river basin singh et al 2015 most large river systems typically have multiple reservoirs that are regulated to meet its design uses e g irrigation water supply hydropower considering several ecosystem and environmental constraints wang et al 2015a thus multi purpose multi reservoir operations encompass detailed analyses considering trade offs among conflicting uses kasprzyk et al 2009 for instance too little release may affect water quality and recreation while too much release may cause flooding singh et al 2015 the opposing nature of benefits associated with storing the water and profits associated with releasing the water contributes to the complexity of multi reservoir system operations yeh 1985 koustosyiannis et al 2003 li et al 2015 to understand the tradeoffs in multi sectoral water allocation over the river basin it is important the reservoir modeling framework should be capable of providing the tradeoffs under observed flows and forecasted flows which is typically represented in the form of ensembles sankarasubramanian et al 2009 the main intent of this study is to develop and validate a multi reservoir multi purpose reservoir modeling framework that can take probabilistic seasonal annual inflow forecasts for allocating water for multiple uses the operation of a reservoir system is likely to be subjected to both supply and demand variations which are typically provided as forecasts at subseasonal weekly to monthly to seasonal annual time scale it is important to analyze how these supply and demand variations impact the reliability of a given user and the probability of violating the target storage which are specified as rule curves sankarasubramanian et al 2009 golembesky et al 2009 currently reservoir modeling platforms typically use either deterministic forecasts provided as forecast mean median which ignores the probabilistic information and the forecast uncertainty on the mean a more rigorous approach is to analyze the multi reservoir system using probabilistic inflow forecasts specified as ensembles to support proactive and adaptive water management policies the utility of a multi reservoir modeling system is to support reservoir managers and operators for meeting different target demands and testing adaptive strategies including drought contingency plans based on the potential supply and demand maurer and lettenmaier 2004 several formulations of multi reservoir models have been in the literature which are well documented in several review papers yeh 1985 labadie 2004 ahmad et al 2014 commonly used mathematical programming techniques include linear programming models loucks et al 1981 belaineh et al 1999 network flow models hsu and cheng 2002 and interior point method seifi and hipel 2001 similarly non linear programming models have relied on sequential analyses to ensure convergence such as sequential linear programming barros et al 2003 sequential quadratic programming finardi et al 2005 and using generalized reduced gradient technique peng and buras 2000 studies have also used both dynamic programming and stochastic dynamic programming models alaya et al 2003 to reduce the dimensionality in the above mathematical programming models studies have suggested using simulation optimization models koustasyiannis and economou 2003 sankarasubramanian et al 2009 application of several novel heuristic techniques such as genetic programming tabu search and particle swarm optimization have also been used to solve multi reservoir models rani and moreira 2009 baltar and fontane 2008 reddy and kumar 2007 and references therein several agencies universities and private corporations have also developed multi reservoir modeling software packages hec ressim by u s army corps of engineers is used to simulate reservoir operations for flood risk management and real time decision support klipsch and hurst 2013 another popular software is modsim a generalized river basin software designed as a tool for making improved basin wide and regional strategies for management and water rights analysis labadie 2010 california department of water resources developed a general purpose reservoir river basin simulation model calsim that permits specifications of system description and operational constraints through a new water resources engineering simulation language draper et al 2004 a water resource planning tool weap developed by stockholm environment institute is capable of simulating water demand supply flows and storage and pollution generation treatment and discharge sieber and purkey 2015 waterware is a proprietary decision support river basin planning system that employs rule based concepts for developing operating criteria and policies jamieson and fedra 1996 another proprietary popular river basin modeling software is riverware developed by the center for advanced decision support for water and environmental systems cadswes zagona et al 2001 riverware is a river basin modeling tool that includes an extensible library of modeling algorithms solvers and a language for the expression of operating policy and is extensively used by many operating agencies such as bureau of reclamation and tennessee valley authority despite the availability of many multi reservoir models the implementation of these models under probabilistic inflow forecasts represented as ensembles requires running the model subsequently for each member or its deterministic form i e mean median of the forecast however detailed application of seasonal to interannual inflow forecasts for reservoir management shows the importance and incorporation of probabilistic constraints on target storage and release sankarasubramanian et al 2009 georgakakos and graham 2008 which cannot be handled by most of the above models the skill of seasonal climate forecasts over the last decade has improved considerably through a better understanding of teleconnection between the slowly evolving boundary conditions such as ssts in the tropical oceans and local hydroclimatology goddard et al 2003 devineni et al 2008 in grl wang et al 2015b low frequency climate variability such as el nino southern oscillation enso has been proven to influence streamflow in many parts of the world dettinger and diaz 2002 utilizing these climate forecasts with updated and corrected land surface conditions have resulted in improved streamflow and soil moisture forecasts wood et al 2002 sinha and sankarasubramanian 2013 mazrooei et al 2019 despite these advancements error propagation in downscaling and disaggregation of climate forecasts in developing streamflow forecasts wood et al 2005 sankarasubramanian et al 2009 mazrooei et al 2015 have caused the application of climate forecasts for real world water allocation to face challenges due to forecast uncertainty as well as due to institutional hierarchy pagano et al 2001 pagano et al 1999 broad et al 2007 these challenges necessitate the translation of uncertainty in climate forecasts into corresponding uncertainty in reservoir releases and storages li et al 2014 lu et al 2017 seasonal to interannual water allocation using a reservoir model based on climate information requires combining the initial storage conditions with the conditional distribution of streamflow specified as ensembles to develop with the forecasted probability of meeting the target storage for the user specified release sankarasubramanian et al 2009 georgakakos and graham 2008 considered a single reservoir to obtain an optimal solution for minimizing the squared deviation from the end of the season target storage under inflow forecast uncertainty maurer and lettenmaier 2004 evaluated the long lead hydrologic predictability represented as deterministic inflow forecast for improving hydropower generation from six reservoirs in the missouri river basin using an aggregated reservoir system representation probabilistic inflow forecasts developed from combining multiple gcms for a single reservoir falls lake in north carolina nc has been demonstrated to be valuable in invoking drought restrictions golembesky et al 2009 li et al 2014 considered inter basin transfer between two nc reservoirs falls lake and lake jordan using two separate single reservoirs for maintaining quality pool and water supply pool elevations under inflow forecast uncertainty wang et al 2015a used a single reservoir model to identify the trade offs between hydropower generation and ecological demands under inflow forecast uncertainty lu et al 2017 utilized multi time scale forecasts represented as ensembles in a single reservoir model for improving hydropower generation and reducing flood risk for a major hydropower reservoir in india thus most studies have used a single reservoir model or a simplified aggregated representation of a reservoir network for evaluating the utility of deterministic probabilistic inflow forecasts to improve water allocation to address this we propose a detailed multi reservoir simulation optimization model graps generalized reservoir analyses using probabilistic streamflow forecasts that considers the probabilistic inflow forecasts specified as ensembles along with probabilistic constraints on meeting the target storage i e rule curves to quantify the reliability of meeting the user specified releases the manuscript is organized as follows the generalized model formulation is presented in section two that can adapt to the complexity of interlinked reservoir systems by sequentially routing the flow from upstream to downstream graps is then applied to a system of reservoirs in ceará brazil to demonstrate graps capabilities in reservoir modeling and its abilities to accurately reproduce historical storage and flows results of the simulation are finally assessed under inflow forecasts and the performance of the forecast based application is validated with historical observations 2 graps formulation graps is extended from a water allocation framework as outlined in arumugam et al 2003 and sankarasubramanian et al 2009 that utilizes the benefits of ensemble forecasts of reservoir inflows to issue annual water contracts unlike many mainstream reservoir modeling tools graps is well suited to handle streamflow ensembles which translates forecast uncertainty into storage and release reliabilities fig 1 provides an overview of variables storages inflows and outflows for a given reservoir within the multi reservoir system the mathematical formulation for graps is outlined below assume there are n r reservoirs in a given basin with the index s s 1 n r denoting a particular reservoir the number of upstream reservoirs for the reservoir s is denoted by u s which includes reservoirs that contribute flows indirectly into reservoir s inflows into any reservoir could be grouped into two categories uncontrolled inflows and controlled flows uncontrolled flows are provided as ensembles denoting the conditional distribution two types of uncontrolled flows are considered in the model for the reservoir s natural inflows into the reservoir q t k s and spillage from the upstream reservoirs e x t k s controlled flows are of three types 1 releases and direct inflows from upstream reservoirs 2 return flows from command areas and wastewater from municipal and industrial use and 3 diversions and water from inter basin transfers or other sources controlled flows are expressed as functions of the decision variables of the multi reservoir water allocation model 2 1 reservoir variables 2 1 1 natural inflow natural inflows q t k s for time t 1 t k 1 k s 1 n r is the probabilistic streamflow forecasts with indices t k and s representing the time step monthly and above ensemble member and reservoirs respectively inflows both observed and forecasted ensembles are provided exogenously to the model by the user 2 1 2 spillage spillage is a result of uncontrolled spillway discharge net spillage from upstream reservoirs e x t k is the sum of the spill from all the upstream reservoirs after accounting conveyance losses which is estimated based on the past spill data at the reservoir and the reported streamflow in the downstream location i e reservoir node the term s p t k s from reservoir s is a derived conditional distribution of spill after accounting for releases diversions and evaporation 1 e x t k s 1 u s s p t k s 2 1 3 return flows from uses assuming there are n s uses in each reservoir and releases for each use r i t s return flows from the uses released from the upstream reservoirs u s could be calculated let n l be the number of lags the number of time steps it takes return flow to reach the reservoir then return flow in time step t into reservoir s r f t s can be estimated as 2 r f t s s 1 u s i 1 n s t t t n l f t i s s r i t s where f t i s s is the fraction of monthly releases from reservoir s that contribute to the current reservoir s with the contribution effective from previous releases n l months 2 1 4 direct release from upstream reservoirs direct outflows from upstream reservoirs d o t s results as part of instream requirement as well as excess water being released for hydropower generation and additional downstream needs here s denotes the upstream reservoir to the current reservoir s this can be expressed as 3 d i t s s 1 u s s s d o t s where s s is the fraction which quantifies the losses on the upstream reservoir s releases d o t s to the direct inflow of the downstream reservoir s s s is usually estimated based on historical upstream release and downstream recorded release between reservoirs s to s the optimization model treats d o t s as a decision variable which could be either considered as a hydropower plant if present or as a user with an ecological flow requirement to be met here we are treating the direct outflow d o t s from the release term r i t s for uses to indicate that they are not return flows and their consumptive use is very small 2 1 5 diversions and other transfers diversions for wildlife protection and other environmental inter basin transfers could contribute to additional inflows 4 d t s d 1 n d s η s d d t d η s d is the fraction representing the losses in the diverted quantity d t d d t d must be specified as part of the exogenous input to the model 2 1 6 net inflows net inflows q t k s is the sum of uncontrolled and controlled inflows into reservoir s fig 1 it is important to note that r f t s and d i t s are functions of decision variables of the allocation model whereas d t s must be specified as an input to the model this variable is not required but is used to simplify the equations 5 q t k s r f t s d i t s d t s e x t k s q t k s 2 2 reservoir simulation the minimum dead maximum and initial storages of reservoir s are represented by s min s s max s and s 0 s h s and s p max s are the elevation of spillway crest level and maximum spillway discharge respectively δ 1 s and δ 2 s are the storage area curve coefficients of the reservoir in line with the water contract specification see section 2 5 different restriction levels are imposed if the actual inflows are less than the forecasted inflows these restriction levels are defined as p r l s where l 1 n j s and r denotes the number of restriction level in a particular reservoir s ψ t s represents the monthly evaporation rates in each reservoir s to simulate reservoir operation the following mass balance equation is used to solve for the end of time step storage for each time step t for each ensemble member k 6 s t k s s t 1 k s q t k s e t k s i 1 n s r i t s d o t s d v t s s p t k s s d t k s for t 1 s t 1 k s s 0 s indicates the observed storage in each reservoir s equation 6 states that the end of time step storage is equal to the current storage plus the net inflow and any deficit s d t s minus spill s p t s all releases r i t s for uses direct downstream release from the reservoir d o t s diversions to other basins d v t s and evaporation e t k s the outflow term from the reservoir d o t s will provide the direct inflow to the downstream reservoir based on equation 3 after accounting the instream losses r i t s and d o t s are decision variables to the optimization model however diversion flows d v t s are specified exogenously equations 7 and 8 calculate the spill and deficit for each time step t 7 spill s p t k s s t k s s max s s t s s max s 0 o t h e r w i s e with s 1 n r 8 deficit s d t k s s min s s t k s s t s s min s 0 o t h e r w i s e with s 1 n r equation 9 requires that all reservoirs operate between their minimum and maximum storage levels 9 s t k s min s t k s s max s s t k s max s t k s s min s evaporation e t s is computed as a function of average storage for the current time step using the initial storage and the end of time step storage with the storage elevation relationship 10 e t k s ψ t s δ 1 s s t k s s t 1 k s 2 δ 2 s the depth of evaporation ψ t s is specified exogenously to the model because both s t s and e t s are unknown evaporation must be calculated implicitly this is done using the secant method for root finding press et al 1986 2 3 hydropower hydropower p is computed as a function of generator efficiency η the density of water ρ gravity g and the height difference between the reservoir level and the tailwater h t t w the reservoir level is given as a function of the storage elevation coefficients β 1 s and β 2 s and the average storage between time steps 11 p t k s η ρ g β 1 s s t k s s t 1 k s 2 β 2 s h t t w n r t s net release n r t s consists of release for hydropower as well as the releases for other uses that go through the turbines graps identifies hydropower as a separate use and also requests details on turbines along with details on whether the outlet for a particular use e g irrigation is available for hydropower generation although generator efficiency varies with elevation and flow rate the efficiency is considered constant for simplicity based on the above equation graps determines an ensemble of hydropower generation for conventional hydropower plants and pumped storage hydropower plants generation from run of river hydropower plants cannot be determined as they have little no storage 2 4 net benefit from the water allocation when graps is optimized the objective is to maximize the net utility of water allocations across all uses equation 12 describes the mathematical formulation of the objective function o denoting the basin wide net benefit from the allocation the revenue φ i s r i t s is the tariff paid for the release in each time period t allocated over the season for the ith water use from reservoir s tariffs in graps are expressed as functions of release to allow users to represent linear increasing block and decreasing block tariff structures compensations to users for not meeting the specified use r i t s is subtracted from the total revenue generated in the second term of the objective function these compensations are defined for each reservoir with γ i j s indicating the compensation to user i if restriction level j is imposed and w i j s is the demand deficit for the entire modeled period for user i and restriction level j ν i s is the compensation schedule for user i if the supply falls below the maximum allowed deficit the contract penalty ν i s is invoked if the difference between the total deficit for user i w i s and the maximum allowable deficit for user i w i max s equations 12 and 13 for further details of the above contract structure see sankarasubramanian et al 2009 12 o s 1 n s i 1 n t 1 t φ i s r i t s s 1 n s i 1 n j 1 n i γ i j s w i j s i 1 n ν i s w i s w i max s 13 where x 1 x 0 0 o t h e r w i s e 2 5 model constraints graps is subjected to both deterministic and probabilistic constraints the following deterministic constraints 14 17 are prescribed in the model for each reservoir for each time step t demand constraint 14 r i min s r i t s r i max s with i 1 n s s 1 n r inflow requirement between two reservoir segments ss 15 d i t min s n r t s s d i t max s with s 1 n r diversion demands 16 d t min s d t s d t max s with s 1 n r spillway constraint 17 0 s p t k s s p max s with s 1 n r target storage constraint 18 pr s t s s t r s n s t s s t r s n p s with s 1 n r user reliability constraint 19 pr w i s w i max s n w i s w i m a x s n p f i s with i 1 n s s 1 n r using ensemble input graps calculates the probabilistic constraints and also reports the probability of spill and deficit by counting how many times an event occurs e g meeting target storage represented as the operator n and dividing that by the number of members n in the ensemble in equation 18 s t s and s t r s represent the simulated end of time period storage and the target storage respectively for each reservoir s the model estimated prob s t s s t r s should be lesser than the target constraint p s in equation 19 w i s is the restriction level for user i at reservoir s and w i max s is the maximum allowable restriction level for a user at a reservoir based on this the model meets specified the user reliability 1 p f i s where p f i s denotes the model estimated failure probability for the current release patterns in addition to these probability performance measures graps also provides ensembles of hydropower spill and deficit for each reservoir at each time step graps can also operate under observed flows in which the probabilistic constraints in 18 and 19 are converted into deterministic constraints with the total number of ensembles k being equal to 1 the deterministic form of 18 and 19 ensures by forcing p s and p f i s being equal to zero 2 6 optimization simulation framework graps equations specified in equations 1 19 could be used in a stand alone simulation mode or in an optimization simulation mode under simulation mode graps performs the simulation across the cascade using equations 1 11 based on the user specified decision variables r i t s and d i t s and computes the model outputs such as ensembles of spill hydropower generation and storages along with the net benefit and probabilistic constraints 12 19 under the optimization simulation mode graps maximizes the net benefit in 12 based on the deterministic and probabilistic constraints in 14 19 to obtain the decision variables r i t s and d i t s using feasible sequential quadratic programming zhou and tits 1992 add a line on observed flows fsqp obtains the decision variables by solving both the deterministic and probabilistic constraints fsqp optimizes continuous functions and their derivatives and also considers finite differences when continuous derivatives are not available for the objective function and the constraints in this study we used both simulation mode and the optimization simulation mode to obtain the releases using finite difference approximation of constraints and objective function using both observed flows and climatological ensembles 2 7 graps model characteristics 2 7 1 node link representation similar to many other existing water resources systems modeling programs graps adopted a node link representation to characterize physical river basin networks even though the shapes of the rivers and reservoirs are arbitrary the underlying spatial configuration can be simplified and represented in the program by a two dimensional interconnected directed network of nodes and links each node in the model can represent one of six entities watersheds reservoirs inter basin transfers users junction nodes and sinks reservoirs watersheds and users are represented by system blocks and diversion locations and flow confluences are designated by junction nodes rivers streams and channels are designated as links and are defined with a direction and upper and lower bounds on flow capacity in such a node link representation a reservoir system begins from a watershed node and ends at a sink node a representation of this node link structure is shown in fig 2 such node link formulation provides an efficient and simple representation of the underlying reservoir system 2 7 2 ensemble input framework by using probabilistic streamflow ensembles graps can be used to investigate climate change effects on basin management and reservoir systems fig 3 illustrates how the simulation model is executed with a streamflow ensemble performing reservoir mass balance for all the traces in the ensemble as opposed to going over the entire cascade for each trace in the ensemble can also facilitate parallelization in computing which we are currently working on incorporating in graps the model simulates the most upstream reservoir time steps 1 to t and for ensembles 1 to k then goes to the next reservoir if a reservoir has multiple branches flowing into it such as the case with reservoir 3 in fig 3 graps will simulate all reservoirs on all of the branches upstream of that reservoir before simulating that reservoir 2 7 3 python interface using python 3 7 and pyqt5 a graphical user interface was developed to increase the usability of graps this interface streamlines the creation of the data files required to run graps and provides a method for visualizing the network cascade it is designed to allow a user to use intuitive keystrokes and mouse movements to create the network cascade and then use input dialogs for each system block to enter information an example network created with the interface is shown in fig 2 2 7 4 input like many other advanced water allocation models graps requires information on basin hydrology reservoir and users for a generalized reservoir model like graps input files are prepared and tailored to each reservoir system these input files specify the connectivity reservoir characteristics reservoir management and user demands of the modeled system of these files the most crucial input to the program is naturalized flow or streamflow that represents natural hydrology into every reservoir 2 7 5 output to model the reservoir system specific information describing individual reservoirs must be provided this information includes the area storage relationship current reservoir storage and water demands for disparate water uses the user is required to provide reservoir system specific information regarding network and diversion blocks specific details about users such as information about demands and water contracts must also be provided simulation results comprise of variables such as storage reservoir releases releases to users hydropower generation spill and deficit the main result of the ensemble simulations is the reliability of meeting end of period target storage for each reservoir 2 7 6 connectivity the reservoir network is represented as an acyclic directed graph with a single terminal node the sink a hierarchical tree structure in which nodes and users ordered from upstream to downstream is used to store the reservoir network in the model each network must have at least one watershed and exactly one sink to simulate a network with multiple sinks one can use junction nodes in place of the individual sinks and then connect the junctions to a single artificial sink fig 4 shows how individual elements of the reservoir network can be connected 3 graps application using ensemble forecast for a multi reservoir system 3 1 study area jaguaribe valley ceará brazil the purpose of this case study is to demonstrate graps modeling capability to accurately simulate historical operations the case is based on the jaguaribe river basin fig 5 a basin situated in the semiarid state of ceará in the northeastern part of brazil with a drainage basin covering an area of 75 961 07 km2 the jaguaribe river extends for about 610 km and its discharge can range from zero to 7000 m3 s campos et al 2000 the main water management challenge in the region is to retain water in reservoirs in rainy years and to manage it such that it will last for several years johnsson and kemper 2005 another challenge is the increasing dependence of the capital fortaleza one of the largest and fastest growing cities in brazil johnsson and kemper 2005 in a 2007 study broad et al 2007 pointed out that a third of ceará s population is rural and most of the population is in the agricultural sector persistent poverty and drought have created an ongoing vulnerability the reservoirs in the upper jaguaribe river basin provide water for agricultural uses including agribusiness and small family farming the downstream reservoirs provide water for municipal use for the city of fortaleza and other small towns in the region 3 2 modeled reservoirs the generalized model is applied to 4 reservoirs in the jaguaribe river basin the modeled reservoirs in this case study are orós banabuiú pacajus and pacoti water is diverted from the jaguaribe river basin to the pacajus reservoir via canal do trabalhador worker s canal the canal is a diversion medium that supplies water to any user along the way through a small canal via a pump station water is again delivered from pacajus reservoir to pacoti reservoir table 1 summarizes the four modeled reservoirs in this case study the largest reservoir in this study is orós with a maximum storage of 1940 hm3 pacajus reservoir on the other hand is the smallest reservoir with a maximum storage of 240 hm3 due to the aridity of the region the minimum storages for all the reservoirs are low additionally most of the annual inflow happens from january to june pacoti and pacajus have higher minimum storages due to their role in ensuring water supply to the nearby city of fortaleza 3 3 input historical streamflow and reservoir data were provided by dr de assis de souza filho at the federal university of ceará fortaleza brazil however historical inflow and reservoir levels were dated before the year 2000 castanhão reservoir a sizable reservoir in the region was constructed after 2000 consequently castanhão reservoir was not included in the model 3 4 streamflow ensemble considering the lag one correlation between the annual flows is close to zero an ensemble of climatological streamflow forecasts was developed from the historical inflows for the corresponding month from 1913 to 2000 to populate 100 ensemble forecasts arumugam et al 2003 this ensemble of forecasts was generated by bootstrapping a simplistic resampling technique that draws randomly from a set of data points and allows replacement 3 5 zero inflow policy since half of the year july december there is zero inflow into the orós reservoir the water management agency in ceará brazil cogerh assumes zero inflow for the upcoming twelve months for additional details on the zero inflow assumption and its merits in water allocation see sankarasubramanian et al 2009 this conservative approach allocates water based on the beginning of the year storage as a result when the reservoir is simulated with the observed inflow the reservoir may spill in some instances 3 6 schematic representation of the modeled system fig 6 illustrates how the multi reservoir system is schematically represented in the program the network contains reservoirs connected in series and parallel at the very top of the graph are orós and banabuiú reservoirs since the agriculture sector dominates the state and most of the users are agriculture users we simplify the modeling by assigning orós and banabuiú to have only one aggregated agriculture user each node 1 a junction node is used to represent a point of river confluence and to gather upstream reservoir and user releases from orós and banabuiú canal do trabalhador is represented in the network as a node and is modeled as a user that delivers water from node 1 to the pacajus reservoir due to its function as a small relay reservoir pacajus reservoir only has one user a pump that delivers water from the reservoir to pacoti reservoir finally pacoti reservoir supplies all the drinking water to the city of fortaleza which is represented as a municipality user node although there are two distinctive watersheds interbasin transfer is not needed as the two watersheds are represented within one system model denoted as a sink the atlantic ocean receives return flows from lower jaguaribe agriculture node 1 pacoti reservoir and the city of fortaleza 3 7 assumptions for simplicity and illustration several assumptions were made in modeling the ceará reservoir system both orós and banabuiú are linked to only agriculture users as the municipal demand in the rural area is very small given that the jaguaribe metropolitan system is primarily operated based on priority based allocation see sankarasurbamanian et al 2009 broad et al 2007 the application of graps was primarily implemented as a simulation model additionally the simulation period is chosen to be from january 1997 to december 1999 as it encompasses a wet 1997 normal 1998 and dry 1999 years lower jaguaribe agriculture is the only user in the lower jaguaribe river and represents all the water demands in that area to account for water loss due to consumptive use it is assumed that return flows from agricultural municipal and industrial users are 40 90 of the water allocation 4 results and discussion the primary objectives of the case study are to demonstrate graps capability to model a complex reservoir system and validate the program s ability to accurately compute flows and reservoir storages and to generate storage reliability curves from ensemble inputs the multi reservoir system is modeled for an entire calendar year and for a three year period with monthly time steps in this case travel time for return flows it is not useful so it is not considered simulation results are presented in the following subsections 4 1 model validation fig 7 shows simulated flow routing for different seasons in 1998 for this purpose graps was run in a simulation mode with observed flows i e total number of ensembles k 1 and the rhs of equations 18 and 19 are set to zero as shown in the network diagram fig 6 orós and banabuiú are the two uppermost reservoirs and receive natural inflow as a result inflow into orós and banabuiú is part of the flow routing demonstration proper routing is an important function of any multi reservoir simulation program the preceding result indicates that the program is routing various flows natural flows reservoir releases and user return flows correctly from upstream to downstream and through junction nodes graps allows for the specification of loss fractions to incorporate consumptive use because a junction node is a place for gathering releases and return flows from upstream and distributing the flow to downstream without using the water total inflow into node 1 is the same as the total outflow from node 1 to demonstrate graps ability to model and optimize reservoir systems over a multi year time horizon the ceará system is optimized using observed inflows i e total number of ensembles k 1 and the rhs of equations 18 and 19 are set to zero from january 1997 to december 1999 using fsqp fig 8 this period is chosen because 1997 is an abnormally wet year 1999 is a dry year for the region in 1998 the inflow into the system is near the long term averages the high inflow in 1997 results in three months of spill from orós and banabuiú and two months of spill from pacajus because spill from orós and banabuiú can flow directly to the atlantic ocean their spill flow does not impact the storages of pacajus and pacoti in april of 1999 releases for industrial use taper off and become zero this can be attributed to the critical need of municipal use during dry years 4 2 optimized reservoir system analysis using climatological inflow ensemble in fig 9 the impact of three different inflow scenarios climatological ensemble zero inflow forecast and perfect forecast i e observed flows on the estimated spill for orós and banabuiú in 1997 is analyzed by optimizing the releases using fsqp the optimized releases for three inflow scenarios were run with the observed flows to evaluate how the system would have performed under each inflow scenario the climatological ensemble k 100 was developed by bootstrapping the observed flows over the period 1913 2000 by assuming each year has the equal probability of occurrence for the zero inflow for t 12 months and observed inflow policies k was set to 1 and the deterministic form 18 and 19 was considered for optimization using fsqp because extended severe droughts are prone in the jaguaribe river basin a zero inflow policy was applied as a conservative measure by the local government for seasonal water allocation see sankarasubramanian et al 2009 this zero inflow policy is considered here as well as using the observed monthly inflows and the climatological ensemble the spill distribution for the optimized release under climatological ensemble is smoothened with a guassian kernel density estimator fig 9 the spill that would have occurred for the optimized release under three inflow scenarios is also shown three straight lines spill volume indicated in legend which is obtained by running graps in a simulation mode with the respective releases under each inflow scenario the zero inflow policy resulted in the most spill for orós followed by the climatological flows and then the observed flows this is along the expected lines as zero inflow policy being conservative has estimated lower release resulting in higher spill this is followed by lesser spill based on the release under climatological ensemble and the optimized release under perfect forecast i e observed flows result in the lowest spill for banabuiú all three policies result in similar yearly spill volumes due to its limited storage the results from the climatological spill indicate that orós banabuiú forecasted spill is 10 7 probability analyzing the spill density it is evident that most of the inflow ensemble members result in no or very little spill for both reservoirs at the beginning of 1997 orós and banabuiú each have approximately 900 hm3 of unused storage thereby the forecasted climatological spill distribution is small though the ensemble used in this analysis is simplistic with no forecast skill the ability of graps to effectively handle inflow ensembles and optimize the release under inflow uncertainty is effectively demonstrated fig 10 shows the ability of graps in estimating the optimal releases that meet various target storage reliabilities p s using climatological ensemble for the year 1998 target storage reliability is a critical constraint in ensuring enough water at the end of the planning period golembesky et al 2009 li et al 2014 for fig 10 the feasible sequential quadratic programming fsqp algorithm is used to maximize the system benefits given a bootstrapped climatological ensemble for the natural inflow into the two reservoirs in the jmh system the target storages for orós 1100 hm3 and banabuiú 400 hm3 are chosen to provide enough storage to meet demand from multiple sectors given two years of no inflow into the reservoirs sankarasubramanian et al 2009 the initial storage for both reservoirs is set as the observed storages at the beginning of january 1998 1425 4 hm3 for orós and 728 9 hm3 for banabuiú the end of the season target storage reliability is computed by counting the number of times end of period storage equal or above the target storage over the entire ensemble and the specified p s is given as a constraint in 18 the target storage reliability p s is increased in increments of 5 from 55 to 100 as a constraint in the optimization simulation model to obtain the optimal releases under each case using climatological ensemble as the target storage reliability increases the optimal yearly release decreases for banabuiú the relationship flattens for reliabilities greater than 70 i e small changes in release result in large changes in reliability this suggests that the low flow years for banabuiú are similar and because the jaguaribe river is known to go long periods of time with little to no flow likely nearly zero inflow years because the climatological inflow provided is bootstrapped from historical inflow data it shows the drier nature of flows occurring over the arid jmh basin such discrete and continuous nature of inflow ensemble can cause large discontinuities in the reliability constraints in 18 and 19 under these situations the optimal solutions from fsqp typically ends indicating that the new solution is numerically equivalent to the previous best solution which indicates that the objective function has flattened out in the search space even though this sounds logical under the discrete i e zero flows and continuous nature of the density of inflows evaluation of the optimized solutions with other optimization algorithms such as particle swarm optimization and iterative linear programming could reduce the uncertainty in the optimal solutions given graps is designed to work with any optimization algorithms as a stand alone optimization simulation model our future effort will evaluate various algorithms in providing optimal solutions with graps 5 discussion and concluding remarks graps a next generation multi reservoir simulation program is presented and detailed as an optimization simulation model the program uses simulation for reservoirs and junction nodes and optimizes the releases for multiple users that maximizes the net benefit from allocation by considering deterministic constraints and probabilistic constraints on target storage and user deficits in this study we demonstrated the optimization using fsqp but in principle graps can be called using any non linear programming solvers graps can also be run in a simulation mode to obtain the target storage reliability and reliability of obtaining user specified releases equations 19 and 19 graps can perform water allocation using observed inflows or using the seasonal annual inflow forecast ensemble graps uses a node link representation with reservoirs watersheds junction nodes and users represented as nodes and rivers streams and channels represented as links the program routes the flow of water from upstream to downstream both spatially and temporally through a connected network of different reservoir elements as opposed to existing reservoir models such as riverware and hec ressim graps can handle ensemble forecast to translate the inflow uncertainty into appropriate probabilistic information on the target storage equation 18 and the reliability of allocating the user specified amount equation 19 by optimizing simulating the releases unlike a site specific reservoir model graps can be applied to any reservoir systems and to help the process of setting up a python user interface is developed in contrast to commercial software graps is free to use for noncommercial and educational purposes and can be downloaded from https github com lcford2 graps git graps formulation is tested in a simulation mode fig 7 in routing the flow through the four reservoirs fig 7 and also evaluated in an optimization simulation mode by performing multi sectoral water allocation fig 8 using observed flows for the jmh basin in ceara north east brazil graps was also evaluated in estimating the spill by optimizing the releases under three inflow scenarios climatological ensembles zero inflow and perfect forecast i e observed flows during a wet year fig 9 analyses show zero inflow forecast estimate the highest spill amount followed by climatological ensembles with the perfect forecast providing the least amount of spill when the optimized release for the three scenarios were run as a simulation using observed flows graps was also evaluated in optimizing the releases in meeting different target storage reliability p s values under climatological ensembles analyses show clearly as p s increases total release for all uses decreases using fsqp but to get convergence on the optimized release it is important that the inflow forecast ensembles should be well calibrated otherwise the uncertainty in optimized releases should be analyzed with various optimization solvers graps maximizes the net benefit from multi sectoral water allocation considering seasonal annual inflow uncertainty challenges in estimating the target storage reliability particularly during dry years as the nature of the inflow distribution tends to be mixture i e discrete and continuous distribution thus estimation of target storage reliability and reliability of allocating the user demand depends on the skill of inflow forecasts and their ability to predict the observed frequencies of various events this implies that apart from the accuracy e g correlation of the inflow forecasts it is important that the forecast needs to be well calibrated between the forecast probability of wet dry years with their observed frequencies multimodel forecasts developed by combining multiple climate forecasts and hydrologic models to develop well calibrated inflow forecasts sinha and sankarasubramanian 2013 application of multimodel inflow forecasts have benefitted in improving the hydropower generation oludhe et al 2013 and in setting up restrictions during drier years golembesky et al 2009 thus providing an inflow forecast that is skillful and well calibrated could result in reliable estimation of the conditional probabilities related to management attributes p s and pf i which is critical if the forecast skill is significant only during a particular season e g winter spring given graps can also work with other solvers the optimized releases should also be tested with other solvers for ensuring global optima this is particularly important if the number of decision variables i e n s n r t increases for a large system that has multiple uses graps is designed to maximize the expected net benefit in considering the revenue and penalties in allocating water for multiple uses this study considered only three uses i e municipal industrial and irrigation but other uses such as hydropower flood control and recreation could also be considered formulation on hydropower is already included and it could be included in the revenue part of the net benefit flood control benefits could be considered explicitly if the simulated storages do not violate the flood control space previous deterministic reservoir modeling efforts have suggested approaches for incorporating flood control benefits simonovic and marino 1982 similarly recreation benefits can also be incorporated if the monthly simulated storages within the desired reservoir levels that support recreational benefits any violation of those storage spaces could also be considered as a penalty into the net benefit specified in equation 12 for additional details on estimating recreational benefits see cordell and bergstrom 1993 other ecological benefits such as instream flow maintenance could also be incorporated explicitly by considering ecosystem maintenance as a user incorporation of these additional benefits could be implemented by modifying the get net ben subroutine related to the net benefit function or by defining them as a user in the system graps is primarily designed to support multi sectoral water allocation considering seasonal annual inflow forecasts it has not yet been applied for daily streamflow forecasts in which the routing and loss coefficients i e coefficients in equations 2 4 could be quite significant depending on the inflow conditions as graps can estimate the monthly seasonal forecasted hydropower potential it can also be linked with power system model for supporting seasonal power generation planning and maintenance efforts are currently underway in linking graps with an energy system model temoa https temoacloud com for the tva system that includes 28 hydropower reservoirs 3 nuclear power plants and 23 fossil fuel coal and natural gas plants ford et al 2019 similarly graps can also be extended for long term planning studies considering climate change projections e g singh et al 2006 under such conditions the inflow forecast will become inflow projections developed multiple climate model s projections under such long term planning conditions the initial conditions of the reservoirs play a limited role but the future demand and inflow conditions play a critical role even though graps cannot explicitly estimate modified rule curves for potential capacity expansion it can estimate the probability of meeting the target storage p s and probability of meeting the user specified demand pf i under projected inflow and demand conditions by estimating the target storage for different p s and pf i for different storage values one could choose the target storage that will ensure desired target storage probability and reliability for different uses under projected climate and demand scenarios these are critical modeling efforts that link water system with both energy and food systems for analyzing their performance under changing climatic conditions software availability graps is written in fortran 90 and uses the fortran feasible sequential quadratic programming ffsqp for optimization package zhou and tits 1992 graps was developed by the authors of this article and is available as free and open source software on github at https github com lcford2 graps contained in the repository is the source code along with pre compiled executables for linux and windows compilation was performed using intel compilers for fortran this repository was made public in march 2019 the source code is less than 200 kb and the executable is slightly more than 1 mb depending on the operating system and the compiler used an example is included in the repository that is based on the reservoir system in ceará brazil along with explanations of the input files declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this material is based on work supported by the national science foundation under grant no 1442909 and grant no 1805293 any opinions findings and conclusions or recommendations expressed in this material are those of the author s and do not necessarily reflect the views of the national science foundation notations variables noted with a star are represented as ensembles in graps d i direct inflow from upstream reservoir d o direct outflow to downstream reservoir d v diversion releases from a reservoir n r net release through reservoir turbines loss fraction n r total number of reservoirs u s upstream reservoirs t number of time steps n l number of lags r f return flow n s number of uses for each reservoir n r number of restriction levels for each reservoir f the fraction of monthly releases from an upstream reservoir that contribute to the current reservoir with the contribution effective from previous releases nl months β monthly demand fraction n d number of diversions d diversion inflow into a reservoir η diversion loss fraction q natural inflow e x spill inflow from upstream reservoirs k ensemble number s p spill outflow from a reservoir s d deficit e x net spillage q net inflow r release for each use p f failure probability p r l restriction level probability s min minimum storage s max maximum storage s 0 initial storage s reservoir storage h elevation of spillway s p max maximum spillway discharge δ storage elevation curve coefficients e evaporation ψ evaporation rate lake evaporation depth p hydropower k generator efficiency ρ density of water g gravity constant h the height difference between headwater and tailwater elevations n total number of ensembles w user restriction level w user demand deficit appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104802 
25972,for sudden water pollution incidents in rivers and lakes the ability to quickly identify the pollution source is of great importance for providing early accident warning and implementing emergency control measures based on bayesian reasoning a variable fidelity surrogate differential evolution adaptive metropolis optimization dream optimization model for coupled inversion process is established in the posterior space of the pollution source in order to verify the effectiveness of the algorithm this paper takes lake a as the research area and gives a hypothetical water pollution emergency the pollution source location release time and released mass of water pollutants suddenly released into water bodies were determined according to the method proposed in this paper the results show that in the case of ensuring the accuracy of calculation the algorithm can accelerate more than 200 times and effectively improves the computational efficiency of the traditional method for obtaining the source information of sudden water pollution events keywords variable fidelity surrogate differential evolution adaptive metropolis optimization dream sudden water pollution identification of the pollution source 1 introduction 1 1 what are sudden water pollution events in rivers and lakes and why do they need to be studied deliberate or unintentional chemical spills can threaten ecological safety and human health jiabiao w et al 2018 in 2010 there were 156 environmental emergencies directly dispatched or disposed by the ministry of ecological environment in china on average once every two working days xu jing et al 2018 including many heavy water pollution emergencies such as the leakage of mining sewage in zijinshan fujian province the explosion of dalian xingang oil pipeline etc in 2011 the leakage accident was caused by the accident of phenol tank car on hangzhou xinjing expressway part of phenol flowed into xin an river with rainwater which has a significant impact on the drinking water safety of 550000 residents in hangzhou in 2015 the accident of longxi tailings pond in gansu caused more than 3000 m3 waste water to flow into the water of western han dynasty and then into jialing river which seriously polluted more than 300 km of rivers in sichuan shaanxi and gansu with an impact of more than 30 days causes great concern at home and abroad wang et al 2015 quick and accurate assessments of pollution source information are of great importance for providing accident warnings and achieving emergency control however in actual response work pollution source information is generally unknown in the early stage of an accident and even well after the accident is reported which makes it almost impossible to perform rapid source control and implement reduction measures in the early accident stage in addition many pollutants including organic matter heavy metals bacteria etc are difficult to identify with the naked eye and are found before an accident causes harm therefore spills are associated with the basic scientific tasks of emergency pollution inversion and discharge history reconstruction the main method of addressing these issues is by solving the inversion problem and determining the location of the pollution source the pollutant discharge time and the emission intensity based on the limited data obtained at fixed monitoring points in a given area such data includes the concentrations of pollutants many researchers have studied these types of problems including ghane et al 2016 xu and gomez hernandez 2016 yang et al 2016 jiabiao w et al 2018 and others 1 2 how to determine the pollution source information of sudden water pollution incident according to the mathematical characteristics of these methods they can be divided into analytical methods regularization methods deterministic methods based on optimization and random methods based on the probability density in analytical methods it is necessary to know the speed of the pollutant diffusion process and the concentration of the pollution source alapati and kabala 2015 most of the early inversion studies were based on such methods for example skaggs and kabala 1995 used the quasi reversibility qr method to identify the emission history of a single point source at a known location by solving the dimensionless convection dispersion equation we can change the well posed problem equation which is very similar to the convection dispersion equation this approach provides high computational efficiency but its accuracy is relatively poor regularization techniques can turn ill posed questions into well posed problems and can be used to directly solve inverse problems for example for a pollution source at a known location skaggs and kabala 1994 used the tikhonov regularization tr method to model the release process of a pollutant from the source in reverse with a one dimensional homogeneous steady flow model wei et al 2010 designed a method based on the optimal perturbation regularization coupling method and multipoint source fractional diffusion equations however the application of this method was based on the premise of sacrificing some solution accuracy the most commonly used methods are singular value decomposition svd and truncated singular value decomposition tsvd the gradient based optimization algorithm has been widely used in the optimization of nonlinear models if the initial value is appropriate the method can quickly obtain the optimal solution zhang et al 2013 however if the nonlinear model is non convex model the local solution may not be the global solution and the model parameters may not be accurately identified the gradient based optimization algorithm will easily fall to local minima so it is regarded as a local optimization algorithm the heuristic algorithm which does not depend on the gradient of the objective function can solve optimization problems involving nonconvex models and has been widely used in the field of pollution source identification for example parolin et al 2015 used the luus jaakola algorithm particle collision algorithm ant colony optimization algorithm and golden section method to identify the source intensity and location and the approach was applied to the macae estuary on the southeast coast of brazil however because the inversion problem is often ill posed when using a deterministic method based on optimization observation error or a small error in the model calculation may cause large deviations in the results hazart et al 2014 resulting in errors in traceability in contrast stochastic methods reflect the randomness of objective things through probability distributions and they are suitable for dealing with uncertain problems the stochastic methods that were first applied for the identification of pollution sources include multivariate nonlinear regression and the associated maximum likelihood method at present the most commonly used methods are based on statistical induction minimum relative entropy and bayesian inference a regression method can be used to solve simple pollution source identification problems but for complex nonlinear models regression methods often need to be combined with optimization algorithms the advantage of statistical induction is that it can be used for uncertainty analysis based on large amounts of data however the limited amount of pollutant concentration data obtained during the emergency response process is not sufficient for supporting inversion studies based on such methods the advantage of the minimum relative entropy method is that uncertainty analysis can be performed for a given problem to obtain a new problem that can be solved based on the prior problem distribution woodbury et al 1996 applied the quasi inverse method and the minimum relative approach the entropy method can identify the emission history of a single point source at a known location one of the most popular stochastic methods is the inverse probability density method based on the adjoint equation neupauer and wilson 1999 first introduced the concept of the inverse probability density function for pollution source control in addition they showed that the inverse position function and the transfer time probability distribution function are the adjoint functions of the forward concentration ghane et al 2016 and cheng et al 2010 used this method to identify the time and location of pollution sources in rivers and lakes and verified the excellent performance of the method furthermore the inverse probability method and the linear regression model have been combined to transform the pollution source identification problem into an optimization model and the differential evolution algorithm was used to identify the pollution source location emission time and total discharge amount for sudden water pollution events jiabiao et al 2018 these studies demonstrate the applicability of the algorithm and its good application prospects however it is relatively difficult to trace a contaminant under high dimensional unsteady flow conditions and the stability of the method requires further study the widely used bayesian mcmc method is another stochastic method based on probability theory based on bayesian inference and the mcmc sampling technique the method transforms point source identification into the posterior estimation of pollution source parameters based on the prior information of the likelihood function and the relevant problem parameters the posterior parameters are determined the resulting probability distribution rather than a single optimal solution provides more information regarding the inversion of pollution events than do other methods and this approach can be used to simultaneously quantify the uncertainty of tracking results many heuristic algorithms such as genetic algorithms gas singh and datta 2006 zhang and xin 2017 artificial neural networks srivastava and singh 2014 singh and datta 2007 and differential evolution algorithms yang et al 2016 are used to accelerate calculations 1 3 existing problems and the research content of this paper in the specific inversion process based on a bayesian stochastic method the pollution source information is regarded as a random variable given a known monitoring value this approach can fully characterize the inherent uncertainty of the model parameters and avoid interdependence to some extent the decision making risk associated with optimal parameter distortion is especially important in studies of the traceability of sudden water pollution events however due to the complexity of surface water simulation systems obtaining source information from inversion modelling requires high computational costs usually requiring tens of thousands of repeated calls to hydrodynamic water quality models considering resource limitations and the urgency of emergency response the application of surrogate models can greatly improve the speed of inversion a surrogate model also known as a proxy model can approximate the main features of the original model with very low computational costs and obtain the relationship between the input and output of the studied system the function of such a model is similar to that of a simulation mechanism model i e for the same input the output of the surrogate model is very close to the output of the full model compared with simulation models surrogate models have a small calculation load and takes less time to run which will directly improve the inversion speed and shorten the emergency response time in fact in the past few decades surrogate models have been widely used in industrial design and groundwater inversion for example zeng et al 2012 and zhang et al 2013 applied sparse grid interpolation in groundwater models to reduce the computational burden the common surrogate models can be divided into three categories projection based surrogate models simplified models and data driven surrogate models smith 2013 by projecting the control equations onto the low dimensional subspace we can construct a projection based surrogate system there are two disadvantages in the projection based surrogate system firstly the construction of orthogonal basis vectors is based on the output samples of some models which makes it difficult to apply this method to parameter inversion secondly the construction of projection based surrogate system needs to modify the model source code asher mj et al 2015 for the former problem people can choose the best sample which contains both parameters and time domain output lieberman c et al 2010 the latter problem is caused by the principle of projection based on surrogate system so it can not be solved jiangjiang zhang 2017 by neglecting the secondary process and only considering the primary process the simplified model can be constructed with lower computational cost which can greatly reduce the computational complexity of model simulation piperni p et al 2013 although these methods are very flexible and have great potential application value their implementation process is determined by specific problems so they do not have universal applicability jiangjiang zhang 2017 since the first two model types include modifications to the system model itself the implementation of such models is difficult in many cases especially when the system model is a black box a data driven surrogate system does not require modifications to the system model this approach has few parameters is easy to implement and is universally applicable therefore it is suitable for sudden water pollution events that require an immediate response based on the above analysis to quickly and accurately obtain pollution source information this study builds a variable fidelity surrogate dream optimized coupled inversion method based on bayesian inference for the posterior space of the pollution source in this method the variable fidelity model is based on a combination of high and low fidelity models using an additive bridge function and the high and low fidelity models use latin hypercube sampling and high and low precision hydrodynamic water quality numerical calculations to generate samples by using the sample point update strategy to reduce the uncertainty in the surrogate model and improve inversion a differential evolution adaptive metropolis optimization algorithm dream coupled surrogate optimization process and minimized surrogate prediction msp criterion for the adaptive insertion of new samples are applied to improve the surrogate model of the pollution source the accuracy of the solution in the posterior space is high thus this approach provides an efficient source by source calculation model for the source intensity emission time and location in order to verify the effectiveness of the method this paper takes lake a as the research background and assumes a sudden water pollution accident to simulate the process of pollutant tracing the calculation results show that the new method effectively improves the calculation efficiency meets the needs of sudden river pollution source analysis and can be used for the active identification of river pollution source information and the corresponding emergency response this paper is organized as follows section 2 presents the surrogate modelling based on variable fidelity model framework section 3 presents the procedures of methods of tracing the pollution source section 4 presents the process and results of case study section 5 provides the conclusions 2 variable fidelity model framework 2 1 surrogate system a surrogate model is also called a metamodel simplified model model simulator or response surface such a model can approximate the main features of the original model with a very low computational cost and obtain the relationship between the system input and output some commonly used surrogate models include the kriging krg radial basis function rbf polynomial response surface prs and support vector regression svr models razavi s et al 2012 the gaussian process gp is a supervised learning algorithm that approximates the output of the original model using a gaussian stochastic process zhang j et al 2016 this method builds a stochastic system that conforms to the gaussian distribution to model output based on the input of the original model of the n m group x m 1 m 2 m n m and output f x g m 1 g m 2 g m n m i e 1 g n μ k where μ is the mean function which can be used as the output of the surrogate model the covariance k is used to quantify the uncertainty of the surrogate model the mean function and covariance function of gps vary and the mean function can be a constant mean function linear mean function or polynomial mean function additionally the covariance function can be a linear covariance function polynomial covariance function or square exponential covariance function this paper uses the gpml gaussian process for machine learning toolkit developed by rasmussen and nickis 2010 and applies the constant linear form function as the mean function of the gaussian process 2 2 variable fidelity model process in the process of building a surrogate system the original model is usually required to generate training data the quality of the training data directly affects the accuracy of the surrogate system high fidelity models often more accurately simulate the migration of pollutants in rivers such as by considering the viscous flux and using high resolution grids however this approach increases the computational cost although low fidelity models have a low calculation cost they can only simulate the basic trend of the migration process and the quality of the training data cannot be guaranteed in this paper the difference between the two fidelity models is mainly reflected by the difference in meshing which is related to the grid elements mesh quality and number of meshes and the different computational solutions as related to the turbulence models used and format differences to balance the advantages of both model types this paper builds a variable fidelity model vfm han z h et al 2008 based on high and low fidelity models the specific modelling steps are as follows i define the variables to be determined and the sampling range the variables to be determined in this paper are the intensity discharge time and location of sudden pollution sources near a river ii two types of samples are generated using the design of experiment doe approach high fidelity samples and low fidelity samples then high and low fidelity hydrodynamic water quality models are used to calculate the corresponding response values the samples are obtained using latin hypercube sampling lhs the sample set includes input and output data in which the input data such as the source location release time and released mass are obtained using latin hypercube sampling lhs high and low fidelity hydrodynamic water quality models are used to calculate the corresponding output data iii after obtaining the high and low fidelity samples and responses a gaussian process model is used to establish the high and low fidelity surrogate models iv the bridge function proxy model is established according to the differences in the response values of the high and low fidelity surrogate models for the high fidelity samples v the vfm is a combination of the low fidelity model and the bridge function model vi the refinement is repeated until the average error between the vfm and the high fidelity model is below a certain threshold here 5 go to step vii otherwise execute the sample updating strategy add a new sample and calculate the high fidelity model response then go to step iii to update the vfm this paper uses the sample updating strategy proposed by han et al 2008 the stop criterion is a preset sample estimate vii according to the obtained vfm if a set of input data is given the corresponding output data can be obtained 2 3 bridge function the most common approach that can be used to handle the differences between the low fidelity model and high fidelity model is to create a bridge function to correct the response of the low fidelity model and align it with the response of the original model in the literature on multi scale fidelity modelling this process is called correction tuning scaling or calibration among the three commonly used bridge functions the multiplication bridge function is a local bridge function that can map the low fidelity model to the high fidelity model in a small space but the global scope cannot be effectively mapped additionally since the multiplication bridge function is a ratio representation of the high and low fidelity models if the value of the low fidelity model is small or even close to zero the bridge function approaches infinity which is unreasonable the addition bridge function is a global bridge function based on the difference between the high and low fidelity models and can avoid the disadvantages of the multiplication bridge function assuming that g h m and g l m represent the response values of the high fidelity model and low fidelity model of the original system respectively the vfm can be obtained by applying additive mapping γ m to the low fidelity model the mathematical expression of this process is as follows han zh et al 2013 2 γ m g h m g l m where the bridge function is approximated by a gp and the vfm g v f m m is approximated by 3 g v f m m g l m γ m where m is the input data of the model i e the pollution source information mentioned in this paper including the pollution source location coordinate pollution discharge time and pollution discharge intensity g l m is a proxy model of the low fidelity model constructed using a gp 3 method of tracing the pollution source 3 1 bayesian reasoning according to bayes theorem the uncertainty parameter m in the above contaminant transport model can be estimated from the measurement data d for simplicity we rewrote the model to the following compact form 4 d g m ε d where g m is a forward simulation operator and ε d is the monitoring error according to bayesian reasoning 5 p m d p d m p m where p m is the prior probability of identifying parameter m and p m d is the corresponding posterior probability assuming that the observed data noise obeys a gaussian distribution with a mean of 0 and a variance of σ d the likelihood function can be constructed as follows 6 p d m 1 2 π σ d n d 2 e x p 1 2 g m d t σ d 1 g m d where n d is the number of monitoring data points in inversion assessment based on a vfm g m can be replaced by the vfm g v f m m therefore eq 6 can be rewritten as 7 p d m 1 2 π σ d n d 2 e x p 1 2 g vfm m d t σ d 1 g vfm m d based on the monitored value d the posterior distribution of the model parameter m can typically be estimated however these probability distribution functions are very complex and can not be calculated directly so it is necessary to estimate the posterior distribution of interest parameters through random sampling dream zs is a random variable generation technique designed to generate samples from a given probability distribution where it is used for bayesian statistical inference of model parameters 3 2 dream zs as a type of mcmc method the early dram method delay rejecting adaptive metropolis method dram has been widely used in the inversion of groundwater model parameters due to its high sampling efficiency in sampling a probability distribution and stable performance zeng et al 2012 zhang et al 2016 however the algorithm is a singular approach that is mainly suitable for cases in which the posterior distribution of a parameter has a single peak when the parameter displays multiple peaks the dram method often cannot obtain a reasonable convergence result vrugt et al 2016 therefore vrugt et al 2003 considering the mh algorithm hastings wk 1970 and the global optimization method sce ua duan q et al 1992 proposed an efficient multichain mcmc algorithm namely the scem ua algorithm by improving the multichain mcmc algorithm differential evolution markov chain de mc braak et al 2006 vrugt and colleagues further proposed the dream zs algorithm vrugt et al 2008 the algorithm uses several parallel markov chains at the same time each parallel chain has different search starting points the search step and direction are adaptively adjusted during the search process and multiple global optimal regions are effectively searched this diversity makes the parallel chains converge in the high probability density interval 3 3 coupling the surrogate model and dream algorithm the use of the surrogate model to replace the more time consuming and accurate numerical simulation of inversion is based on the establishment of a surrogate model with reasonable accuracy the success of this surrogate algorithm relies heavily on the accuracy of the surrogate model if the accuracy is too low inversion may be poor by adding new sample points according to certain criteria constructing an accurate surrogate model in a local scope and improving the accuracy of the model in the posterior space by cyclically updating the proxy model modelling issues can be avoided to achieve this goal this paper utilizes the advantages of the traditional inversion method based on the msp criterion and constructs a suboptimization problem of the corresponding inversion problem in the dream inversion process then the traditional optimization method is applied to update each sample with a small computational cost the optimal solution is predicted then the optimal solution of these predictions is used to perform accurate numerical analysis the result is added as a new sample to the existing data set and the surrogate model is continuously updated as a result the sample point sequence converges to a local or global optimal solution the specific process is as follows step 1 obtain a time series of pollutant concentration d at the monitoring point and determine the variable m to be analysed and the corresponding parameter range in practical application d is obtained by measurement however due to the lack of measured data d is obtained by hydrodynamic water quality simulation in this paper step 2 construct the initial variable fidelity model firstly the initial sample input m is obtained by lhs and the high and low fidelity models are used to calculate the pollutant concentration value g m at the monitoring position then the initial vfm is established according to a gp step 3 based on bayesian inference and dream zs algorithm inversion as shown in eq 5 6 7 the original hydrodynamic water quality model is replaced by the vfm in the inversion process dream zs is designed to generate samples from a given probability distribution where it is used for bayesian statistical inference of model parameters if the algorithm converges it is terminated otherwise the process proceeds to step 4 step 4 construct the following suboptimization problem 8 min j ˆ m ω g v f m m d 2 2 where ω 1 d η meas and η meas is a constant step 5 the new sample input point m is obtained by solving the sub optimization problem firstly select the optimal result for each chain obtained in the current iteration m b e s t from step 3 then determine the trust domain 9 m max m b e s t δ r m min min m b e s t δ r m max and use the genetic algorithm amor v m et al 2002 to find the local optimal solution m in the trust domain 10 m argmin j ˆ m where the radius of the trust region is controlled by a gaussian decreasing function 11 r r b e s t r min exp n i 2 n max 2 r min r m ax r min are the maximum and minimum trust region radii respectively n i is the current iteration number n max is the maximum number of iterations and δ is the control factor step 6 update the vfm m is substituted as a new input into the high fidelity model to obtain the corresponding output g h m thereby producing a new sample point m g h m thus a new sample set is generated and the vfm is re established step 7 go to step 3 and continue to trace the source with the new vfm 4 case study 4 1 description of the problem lake a is kunming lake in xixian new area xian city shaanxi province china and it consists of a north lake and south lake it s still under construction the total reservoir area is 10 4 square kilometres with a total reservoir capacity of 46 million cubic metres the north lake with an area of 7 square kilometres and a storage capacity of 22 million cubic metres is the flood diversion area of the feng river and provides storage when the river floods thus this lake is of great cultural and ecological value the lake is located in an urban area and is prone to sudden water pollution events due to intense human activities when pollution occurs the source information must be identified and the pollution must be dealt with promptly therefore this case simulates the determination process for a hypothetical sudden water pollution event we assume that at 00 00 on a certain day a vehicle carrying a chemical substance with a molar mass of 24 g mol has a traffic accident in the reservoir area and that the pollutant is instantaneously discharged the source can thus be expressed as follows 12 s m δ x x s y y s h t t o n h t t o f f where δ is the dirac function h is the heaviside unit step function m m l 2t 1 is the source strength x s y s is the point source position t o n t is the start of the leakage time and t o f f t is the end of the leakage time the location of the given pollutant leakage event is the north lake of doumen reservoir with coordinates x s y s 570500 3785500 the leakage intensity is 800 g m2 s and the leakage event lasts for 0 5 h in this paper the inversion of three different fidelity models is compared under the same calculation conditions to verify the effectiveness of the method 4 2 high and low fidelity models the establishment of high and low fidelity models for sudden water pollution assessment in river areas includes grid division and computational steps the north lake of doumen reservoir is a plain reservoir the topographical changes are relatively small and the vertical scale is much smaller than the horizontal plane scale therefore it is assumed that the pressure along the water depth follows a hydrostatic pressure distribution and that the pollutant is evenly distributed vertically after the accident based on the concentration a two dimensional hydrodynamic water quality model with a vertical averaging method is used to simulate the pollutant diffusion process the governing equation is as follows 13 u t f s s where u is the conserved physical quantity f is the flux and s s is the source term if the viscous flux is not considered the low fidelity model is as follows see zhao d h et al 1994 u h h u h v h c i f x h u h u 2 g h 2 2 h u v h u c i f y h v h u v h v 2 g h 2 2 h h v c i s s 0 g h s o x s f x g h s o y s f y d k h c i k c i h c i s i s 0 x z b x s 0 y z b y where f x f y represent the fluxes in the x and y directions respectively h l is the water depth u lt 1 and v l t 1 are the flow velocity components in the x and y directions respectively g lt 2 is the acceleration of gravity z b l is the river bottom elevation n m tl 1 3 is the manning coefficient c i m l 3 is the vertical average concentration of the pollutant k c i t 1 is the comprehensive degradation coefficient of the pollutant where different pollutant components have different coefficients d k l2t 1 is the pollutant diffusion coefficient s o x and s f x are the bottom slope and friction gradient in the x direction respectively s 0 y and s f y are the bottom slope and friction gradient in the y direction respectively s i is the source and sink term and is the gradient operator this paper considers only one pollutant component considering the viscous flux the k ε model is combined with the above formula to obtain the high fidelity model see erpicum et al 2014 f x h u h u 2 g h 2 2 2 h ν ν t u x h u v h ν ν t u y v x h κ u h ν t σ t κ x h ε u h ν t σ t ε x h u c i f y h v h u v h ν ν t u y v x h v 2 g h 2 2 2 h ν ν t v y h κ v h ν t σ t κ y h ε v h ν t σ t ε y h v c i s s 0 g h s o x s f x g h s o y s f y p h p κ b h ε c ε 1 ε κ p h p ε b c ε 2 ε 2 κ h d k h c i k c i h c i s i u h h u h v h κ h ε h c i ν t c μ κ 2 ε p h h ν t 2 u x 2 2 v y 2 u y v x 2 c f g n m 2 h 1 3 p k b c f 1 2 u 3 p s b c s γ c s 2 c μ 1 2 c f 3 4 u 4 h u c f u 2 v 2 s 0 x z b x s 0 y z b y s f x n m 2 u u 2 v 2 h 4 3 s f y n m 2 v u 2 v 2 h 4 3 where κ l2t 2 is the turbulent kinetic energy ε l2t 3 is the turbulent dynamic energy dissipation rate ν l2t 1 is the dynamic viscosity other variables have the same meaning as the low fidelity model and the characteristic parameters are as follows c μ 0 09 c ε 1 1 44 c ε 2 1 92 c ε γ 1 8 3 6 σ k 1 0 σ ε 1 3 in general for the same working conditions the rationality of the selection of the modelling method and numerical method can directly influence the success or failure of the simulation see fig 1 in terms of models those that consider the viscous flux better reflect the real situation than do those that do not consider this term different numerical methods will also affect the accuracy of the simulation theoretically as long as the boundary conditions are correct and the parameters are reasonable when the mesh is infinitely refined the numerical solution process will converge however in reality the increase in the number of units will increase the calculation time and the consumption of computational resources in this paper the finer unstructured triangular meshes were used for the high fidelity model hf and the coarser unstructured triangular meshes were used for the low fidelity model lf the finite element method is applied to simulate pollutant diffusion table 1 shows the difference between the two models fig 3 shows the meshing results see fig 2 see table 2 the purpose of the forward simulation is to obtain enough sample data to build and correct the fidelity model because a sudden water pollution event usually lasts for a short period of time the hydrological conditions change little during this period so the following model parameters and initial boundary conditions are given according to historical data and the relevant design requirements among them the hydrological data is from the 69 year series data of qindu hydrological station of fenghe from 1944 to 2012 the hydrodynamic data is from our field measurement model parameter determination generally the method of selecting the parameters used in a model is an empirical formula based method or is based on measured data the former is ideal for certain water bodies the latter requires a large amount of historical data for a reservoir under construction it is difficult to obtain this amount of data because the duration of sudden water pollution events is short it is assumed that the model parameters change little in the simulation period according to historical hydrological data and the relevant design requirements the following hydraulic parameters are used in the hydrodynamic simulation a manning s roughness of 0 02 tl 1 3 a dynamic viscosity of 0 0013 m2 s a pollutant diffusion coefficient of 0 0021 m2 s in the x direction and 0 0014 m2 s in the y direction and the degradation coefficient is 0 006 d initial conditions i for the hydraulic conditions the initial flow rate is 0 and the initial water level is 3 m ii for the water quality conditions the initial pollutant concentration is 0 boundary conditions i for the hydraulic conditions no sliding boundary conditions non penetrating boundary are adopted for the solid boundaries i e v n l 0 where n is the solid boundary direction vector and l is the river bank boundary flow velocity conditions are assigned for the water inlet boundary the velocity in the x direction is 0 08 m s and that in the y direction is 0 06 m s the effluent boundary is assigned a water level of 3 m ii for the water quality conditions the inflow concentration is 0 the flux in the outflow concentration is zero and the solid boundary pollutant concentration is c n l 0 the implementation of the forward model comes from previous research results by our research group it is developed in matlab and uses a finite element method for the discretization of the equations 4 3 establishment of the variable fidelity model the variables to be identified in tracing of pollution sources are the horizontal and vertical coordinates of the pollution source pollutant leakage strength duration of pollutant leakage the initial estimation range of each parameter can be estimated based on an analysis of monitoring data and expert experience the vfm is established with the variable to be assessed as the optimized controllable input variable the outputs of the surrogate model are the values at 2 monitoring locations see fig 4 monitoring site1 is located at coordinate 571050 m 3785560 m monitoring site2 is located at coordinate 570550 m 3785850 m from 00 00 03 00 0 10800 s the time series of pollutant concentrations measured at intervals of 1200 s with the established vfm includes 5 input variables i e m x s y s m t o n t o f f and 22 output values i e time series of pollutant concentration at two monitoring points the results are shown in fig 5 to establish the vfm first 220 sample points are extracted in the target interval of the design variables among the 220 sample points 160 points are used to calibrate the low fidelity model 40 to calibrate the high fidelity model and 20 points are used to validate both models the reason why more lf is used in the process of constructing variable fidelity models is that its calculation cost is small about 1 6 of hf see table 1 which helps to shorten the calculation time the two types of sample proportions are selected based on the experience and there are no special requirements the sampling locations of variables x s y s are subject to the boundaries of the study area and the remaining variables are determined by using lhs in the prior interval of the given parameters the sampling results input data are substituted into the high and low fidelity models and the time series of pollutant concentrations at monitoring points are calculated as the system output data because the range of acceptable values differ greatly for each variable normalization is needed the values are all converted to a unified interval in this paper this interval is 0 1 and the processed data are used as the training sample the variable fidelity surrogate model is established with this method the sampling results are shown in fig 6 to study the accuracy of the high fidelity surrogate model gp hf the low fidelity surrogate model gp lf and the vfm surrogate model gp vfm 20 randomly selected samples see fig 6 were substituted into the high fidelity model and the three types of surrogate model for testing to assess the computational efficiency and accuracy of the variable fidelity surrogate system we have used four indicators the coefficient of determination r2 bias proportion bp variance proportion vp and covariance proportion cp to evaluate the prediction performance of the model r 2 i 1 n y ˆ i y 2 i 1 n y i y ˆ i 2 b p i 1 n y ˆ i n y 2 i 1 n y i y ˆ i 2 n v p s y ˆ s y 2 i 1 n y i y ˆ i 2 n c p 2 1 r s y ˆ s y i 1 n y i y ˆ i 2 n where y i represents the output value of the ith sample obtained from the transport simulation model y ˆ i is the output value of the ith sample obtained from the surrogate model y is the average of y i s y s y ˆ represents the standard deviation of y i and y ˆ i if the indexes bp and vp are small and r2 and cp are large it indicates that the approximation degree of the surrogate model is high therefore when the simulation model meets the accuracy requirements the surrogate model can be used to replace the simulation model the accuracy evaluation indicators of the surrogate model are shown in table 3 the results show that r2 is very close to 1 indicating a good fit the small bp value proves that the deviation degree between the predicted mean and the actual value is small and the small vp indicates that the deviation degree between the predicted variance and the actual variance is also small cp represents the remaining non systematic errors by comparing the prediction accuracy of the three models it can be found that the variable fidelity surrogate model gp vfm has better prediction effect therefore the agent model can be used to replace the physical model and coupled with the optimization model to determine the source of sudden water pollution 4 4 inversion of the pollution source in eq 12 if the point source location leakage time and leakage intensity are known where m x s y s m t o n t o f f the pollutant distribution can be simulated according to the hydrodynamic water quality model in eq 13 this is called the forward simulation the pollution source inversion problem involves the opposite approach that is according to the downstream pollutant monitoring data d the pollution source information m is inversely derived assuming that the source information m is unknown two sensors obtain monitoring data the concentration time series at the two monitoring points are obtained by the forward high fidelity water quality simulation as additional information to identify the pollution source information in reverse using the method described herein the parameters of the dream zs algorithm are set as follows the number of parallel chains n is 10 the number of evolutionary algebra steps for the parallel chains is 20000 the prior distribution of each parameter is evenly distributed and the above values are constant the parameters of the genetic algorithm are set as follows the maximum number of evolutionary algebra steps in the algorithm is 50 the population size is 30 the crossover probability is 0 9 and the mutation probability is 0 01 the values of these parameters are related to the problems to be solved usually they can be selected according to experience and references for a more detailed description of the algorithm refer to references amor v m et al 2002 vrugt et al 2016 the low fidelity surrogate model high fidelity surrogate model and gp vfm were combined in an optimization process to determine the pollution source through the dream zs algorithm the statistical sample characteristics were estimated by using the nuclear density and the results are shown in fig 7 since the algorithm in this paper is a random algorithm with uncertainty to avoid accidental interference 5 times are performed for each case taking the posterior mean value of the parameter as the parameter estimator the mean value and error of the five calculation results are calculated in the dream simulation there are 10 parallel chains the length of each chain is 20 000 the iterations of surrogate refinement is implemented 20 times as shown in fig 7 we compare the marginal posterior probability density functions ppdf of the five contaminant source parameters x s y s m t o n t o f f estimated by the dream simulation i e the lf based dream approach pink curves the hf based on dream approach blue curves the gp vfm based dream approach red curves the mean value and error of the five calculation results are calculated in table 4 fig 8 compared the distance between the pollution source location determined based on the three models and the real source it is found that the three approaches can accurately estimate the posterior distribution of the model parameters the hf based on dream approach and the gp vfm based on dream approach are only about 50 m away from the real source although there is still a difference from the real area the estimated source area is 7 square metres in terms of the search space the scope has been greatly reduced by combining this approach with physical methods accurate source identification can be achieved 4 5 comparison of inversion results under different working conditions 4 5 1 comparison of simulation results under different source parameters in order to investigate the reliability of the algorithm under different source parameters two groups of different pollution source parameters labeled s1 and s2 see table 5 are given the time series of pollutant concentration at the corresponding monitoring point at the same monitoring point is calculated see fig 9 we can take it as a known condition to determine the pollution parameters according to the method in this paper the results are shown in figs 10 13 in the dream simulation there are 10 parallel chains the length of each chain is 20 000 which means that the total number of three model evaluations is 200 000 the iterations of surrogate refinement is implemented 20 times as shown in figs 10 and 12 the chains for the five source parameters can accurately catch the true values black crosses in figs 11 and 13 we compare the marginal posterior probability density functions ppdf of the five contaminant source parameters x s y s m t o n t o f f estimated by the dream simulation i e the lf based dream approach pink curves the hf based dream approach blue curves the gp vfm based dream approach red curves it is found that the three approaches can accurately estimate the posterior distribution of the model parameters 4 5 2 effects of different monitoring locations given two groups of different monitoring schemes labeled ms1 ms2 see table 6 with unchanged pollution source parameters the high fidelity model was calculated to obtain the time series of pollutant concentration at the corresponding monitoring location as shown in fig 14 the monitoring value are taken as the known condition and the pollution source parameters are determined according to the method in this paper the results are shown in figs 15 and 16 from the results of figs 15 and 16 based on the high and low fidelity models and the vfm the calculated posterior density curve of the parameters well covers the true value and its peak position is close to the true value indicating that the algorithm is accurate by comparing the estimates of the five parameters in the posterior space it is not difficult to find that the inversion of the high fidelity model and the vfm is better than that of the low fidelity model in practical application whether the forward model can accurately reflect the diffusion process of pollutants and coincide with the measured value at the monitoring point will effectively affect the accuracy of estimation if we further compare the inversion results based on the high and low fidelity models we will find that the low fidelity model based on 160 training samples is not more accurate than the high fidelity model based on 40 training samples which indicates that in addition to the sample quality the sample size and structural parameters directly affect the accuracy of modelling table 7 compares the error levels of the three methods the errors for the variable m are approximately 0 75 and those for the other variables are different partly because the sample interval of this parameter is relatively small and those of the other indicators are large additionally the small sample size results in a low sample density which affects the accuracy of the model overall vfm dream yields the lowest relative error and the best performance the three surrogate models complete the inversion simulation in a relatively short amount of time up to 5 8 h because of the high cost of iterative process calculation based on physical model this paper iterates 20 000 times and it is expected to take 56 d this paper only simulates pollutant source determination based on surrogate model if the accuracy of the case is acceptable the use of the surrogate model about 5 8 h will significantly improve the calculation speed according to rough estimation the time consumption is more than 200 times faster than the former obviously the specific speed is also related to the number of iterations 5 conclusion for sudden water pollution incidents in rivers the ability to quickly identify the pollution source is of great importance for early accident warning and emergency control based on bayesian reasoning a variable fidelity surrogate dream optimization model for coupled inversion is established in the posterior space of a pollution source in this approach the vfm surrogate includes high and low fidelity models fused by an addition bridge function these high and low fidelity models are established based on a gp generated according to lhs and high and low precision hydrodynamic water quality numerical calculations to reduce the uncertainty of the surrogate model and improve the efficiency of source identification this paper integrates the variable fidelity surrogate and dream optimization process based on the msp criterion new samples are inserted into the sample set to improve the precision of the surrogate model in the posterior space of the pollution source thereby creating an efficient computational model for determining the source location release time and released mass of sudden water pollution events that affect rivers the proposed approach is tested with numerical and engineering examples the results show that the new approach can effectively improve the calculation efficiency and identify source information for sudden water pollution incidents in rivers thus it can be used for the identification of source information and establishment of emergency response measures the specific conclusions are as follows i the surrogate model is used to approximate the complex hydrodynamic water quality model and provide a globally interpretable approach that avoids nonlinearity issues however this model does not perfectly represent the underlying response function of the original model nor can it capture complex feature relationships nonetheless these issues do not prevent the application of the surrogate model in the determination of pollution sources the results show that it is feasible to replace the original model with the surrogate model in pollution source determination based on bayesian inference and the reliability of this approach is greatly affected by the sample size structural parameters and other factors ii unlike using simple model approximations in this study high and low fidelity models are used to establish a vfm with high local accuracy in the traceability process the vfm is coupled with an inversion model and driven by historical data to add sample points to the sample set and approach the local and global solutions therefore it is unnecessary for the surrogate model to have high approximation accuracy in the entire research area and the model only needs to have high accuracy in important areas especially near the optimal solution this approach not only reduces the calculation cost but also improves the accuracy of inversion iii new samples are adaptively inserted based on the msp strategy to improve the accuracy of the replacement model in the posterior space of the pollution source during this process the reliable ranges of inversion variables are controlled by a gaussian decreasing function and the search space of the suboptimization problem is gradually reduced to decrease the cost of selecting new sample points and improve convergence the selection of the control factor will affect the calculation results if the radius of the reliable range shrinks too fast precocity issues can occur further research is needed to develop better control strategies or other methods to improve convergence iv the greatest advantage of the surrogate model is the low computational cost this paper shows that the algorithm effectively improves the efficiency of inversion and is more than 200 times faster than other methods as the number of iterations increases the advantages of the surrogate model will likely become more obvious in the future we will determine if the proposed numerical method of determining pollution sources can be combined with intelligent sensors and big data technology if so physical methods such as artificial inversion or unmanned ship monitoring could effectively solve the problem of determining the source of sudden water pollution in river or reservoir areas declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by the national natural science foundation of china grant no 51979222 and 91747206 the natural science foundation of shaanxi province china grant no 2019jlm 62 and the natural science basic research plan in shaan xi province of china grant no 2019jm 284 we would like to extend special thanks to the editor and reviewers for insightful advice and comments on the manuscript appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104811 
25972,for sudden water pollution incidents in rivers and lakes the ability to quickly identify the pollution source is of great importance for providing early accident warning and implementing emergency control measures based on bayesian reasoning a variable fidelity surrogate differential evolution adaptive metropolis optimization dream optimization model for coupled inversion process is established in the posterior space of the pollution source in order to verify the effectiveness of the algorithm this paper takes lake a as the research area and gives a hypothetical water pollution emergency the pollution source location release time and released mass of water pollutants suddenly released into water bodies were determined according to the method proposed in this paper the results show that in the case of ensuring the accuracy of calculation the algorithm can accelerate more than 200 times and effectively improves the computational efficiency of the traditional method for obtaining the source information of sudden water pollution events keywords variable fidelity surrogate differential evolution adaptive metropolis optimization dream sudden water pollution identification of the pollution source 1 introduction 1 1 what are sudden water pollution events in rivers and lakes and why do they need to be studied deliberate or unintentional chemical spills can threaten ecological safety and human health jiabiao w et al 2018 in 2010 there were 156 environmental emergencies directly dispatched or disposed by the ministry of ecological environment in china on average once every two working days xu jing et al 2018 including many heavy water pollution emergencies such as the leakage of mining sewage in zijinshan fujian province the explosion of dalian xingang oil pipeline etc in 2011 the leakage accident was caused by the accident of phenol tank car on hangzhou xinjing expressway part of phenol flowed into xin an river with rainwater which has a significant impact on the drinking water safety of 550000 residents in hangzhou in 2015 the accident of longxi tailings pond in gansu caused more than 3000 m3 waste water to flow into the water of western han dynasty and then into jialing river which seriously polluted more than 300 km of rivers in sichuan shaanxi and gansu with an impact of more than 30 days causes great concern at home and abroad wang et al 2015 quick and accurate assessments of pollution source information are of great importance for providing accident warnings and achieving emergency control however in actual response work pollution source information is generally unknown in the early stage of an accident and even well after the accident is reported which makes it almost impossible to perform rapid source control and implement reduction measures in the early accident stage in addition many pollutants including organic matter heavy metals bacteria etc are difficult to identify with the naked eye and are found before an accident causes harm therefore spills are associated with the basic scientific tasks of emergency pollution inversion and discharge history reconstruction the main method of addressing these issues is by solving the inversion problem and determining the location of the pollution source the pollutant discharge time and the emission intensity based on the limited data obtained at fixed monitoring points in a given area such data includes the concentrations of pollutants many researchers have studied these types of problems including ghane et al 2016 xu and gomez hernandez 2016 yang et al 2016 jiabiao w et al 2018 and others 1 2 how to determine the pollution source information of sudden water pollution incident according to the mathematical characteristics of these methods they can be divided into analytical methods regularization methods deterministic methods based on optimization and random methods based on the probability density in analytical methods it is necessary to know the speed of the pollutant diffusion process and the concentration of the pollution source alapati and kabala 2015 most of the early inversion studies were based on such methods for example skaggs and kabala 1995 used the quasi reversibility qr method to identify the emission history of a single point source at a known location by solving the dimensionless convection dispersion equation we can change the well posed problem equation which is very similar to the convection dispersion equation this approach provides high computational efficiency but its accuracy is relatively poor regularization techniques can turn ill posed questions into well posed problems and can be used to directly solve inverse problems for example for a pollution source at a known location skaggs and kabala 1994 used the tikhonov regularization tr method to model the release process of a pollutant from the source in reverse with a one dimensional homogeneous steady flow model wei et al 2010 designed a method based on the optimal perturbation regularization coupling method and multipoint source fractional diffusion equations however the application of this method was based on the premise of sacrificing some solution accuracy the most commonly used methods are singular value decomposition svd and truncated singular value decomposition tsvd the gradient based optimization algorithm has been widely used in the optimization of nonlinear models if the initial value is appropriate the method can quickly obtain the optimal solution zhang et al 2013 however if the nonlinear model is non convex model the local solution may not be the global solution and the model parameters may not be accurately identified the gradient based optimization algorithm will easily fall to local minima so it is regarded as a local optimization algorithm the heuristic algorithm which does not depend on the gradient of the objective function can solve optimization problems involving nonconvex models and has been widely used in the field of pollution source identification for example parolin et al 2015 used the luus jaakola algorithm particle collision algorithm ant colony optimization algorithm and golden section method to identify the source intensity and location and the approach was applied to the macae estuary on the southeast coast of brazil however because the inversion problem is often ill posed when using a deterministic method based on optimization observation error or a small error in the model calculation may cause large deviations in the results hazart et al 2014 resulting in errors in traceability in contrast stochastic methods reflect the randomness of objective things through probability distributions and they are suitable for dealing with uncertain problems the stochastic methods that were first applied for the identification of pollution sources include multivariate nonlinear regression and the associated maximum likelihood method at present the most commonly used methods are based on statistical induction minimum relative entropy and bayesian inference a regression method can be used to solve simple pollution source identification problems but for complex nonlinear models regression methods often need to be combined with optimization algorithms the advantage of statistical induction is that it can be used for uncertainty analysis based on large amounts of data however the limited amount of pollutant concentration data obtained during the emergency response process is not sufficient for supporting inversion studies based on such methods the advantage of the minimum relative entropy method is that uncertainty analysis can be performed for a given problem to obtain a new problem that can be solved based on the prior problem distribution woodbury et al 1996 applied the quasi inverse method and the minimum relative approach the entropy method can identify the emission history of a single point source at a known location one of the most popular stochastic methods is the inverse probability density method based on the adjoint equation neupauer and wilson 1999 first introduced the concept of the inverse probability density function for pollution source control in addition they showed that the inverse position function and the transfer time probability distribution function are the adjoint functions of the forward concentration ghane et al 2016 and cheng et al 2010 used this method to identify the time and location of pollution sources in rivers and lakes and verified the excellent performance of the method furthermore the inverse probability method and the linear regression model have been combined to transform the pollution source identification problem into an optimization model and the differential evolution algorithm was used to identify the pollution source location emission time and total discharge amount for sudden water pollution events jiabiao et al 2018 these studies demonstrate the applicability of the algorithm and its good application prospects however it is relatively difficult to trace a contaminant under high dimensional unsteady flow conditions and the stability of the method requires further study the widely used bayesian mcmc method is another stochastic method based on probability theory based on bayesian inference and the mcmc sampling technique the method transforms point source identification into the posterior estimation of pollution source parameters based on the prior information of the likelihood function and the relevant problem parameters the posterior parameters are determined the resulting probability distribution rather than a single optimal solution provides more information regarding the inversion of pollution events than do other methods and this approach can be used to simultaneously quantify the uncertainty of tracking results many heuristic algorithms such as genetic algorithms gas singh and datta 2006 zhang and xin 2017 artificial neural networks srivastava and singh 2014 singh and datta 2007 and differential evolution algorithms yang et al 2016 are used to accelerate calculations 1 3 existing problems and the research content of this paper in the specific inversion process based on a bayesian stochastic method the pollution source information is regarded as a random variable given a known monitoring value this approach can fully characterize the inherent uncertainty of the model parameters and avoid interdependence to some extent the decision making risk associated with optimal parameter distortion is especially important in studies of the traceability of sudden water pollution events however due to the complexity of surface water simulation systems obtaining source information from inversion modelling requires high computational costs usually requiring tens of thousands of repeated calls to hydrodynamic water quality models considering resource limitations and the urgency of emergency response the application of surrogate models can greatly improve the speed of inversion a surrogate model also known as a proxy model can approximate the main features of the original model with very low computational costs and obtain the relationship between the input and output of the studied system the function of such a model is similar to that of a simulation mechanism model i e for the same input the output of the surrogate model is very close to the output of the full model compared with simulation models surrogate models have a small calculation load and takes less time to run which will directly improve the inversion speed and shorten the emergency response time in fact in the past few decades surrogate models have been widely used in industrial design and groundwater inversion for example zeng et al 2012 and zhang et al 2013 applied sparse grid interpolation in groundwater models to reduce the computational burden the common surrogate models can be divided into three categories projection based surrogate models simplified models and data driven surrogate models smith 2013 by projecting the control equations onto the low dimensional subspace we can construct a projection based surrogate system there are two disadvantages in the projection based surrogate system firstly the construction of orthogonal basis vectors is based on the output samples of some models which makes it difficult to apply this method to parameter inversion secondly the construction of projection based surrogate system needs to modify the model source code asher mj et al 2015 for the former problem people can choose the best sample which contains both parameters and time domain output lieberman c et al 2010 the latter problem is caused by the principle of projection based on surrogate system so it can not be solved jiangjiang zhang 2017 by neglecting the secondary process and only considering the primary process the simplified model can be constructed with lower computational cost which can greatly reduce the computational complexity of model simulation piperni p et al 2013 although these methods are very flexible and have great potential application value their implementation process is determined by specific problems so they do not have universal applicability jiangjiang zhang 2017 since the first two model types include modifications to the system model itself the implementation of such models is difficult in many cases especially when the system model is a black box a data driven surrogate system does not require modifications to the system model this approach has few parameters is easy to implement and is universally applicable therefore it is suitable for sudden water pollution events that require an immediate response based on the above analysis to quickly and accurately obtain pollution source information this study builds a variable fidelity surrogate dream optimized coupled inversion method based on bayesian inference for the posterior space of the pollution source in this method the variable fidelity model is based on a combination of high and low fidelity models using an additive bridge function and the high and low fidelity models use latin hypercube sampling and high and low precision hydrodynamic water quality numerical calculations to generate samples by using the sample point update strategy to reduce the uncertainty in the surrogate model and improve inversion a differential evolution adaptive metropolis optimization algorithm dream coupled surrogate optimization process and minimized surrogate prediction msp criterion for the adaptive insertion of new samples are applied to improve the surrogate model of the pollution source the accuracy of the solution in the posterior space is high thus this approach provides an efficient source by source calculation model for the source intensity emission time and location in order to verify the effectiveness of the method this paper takes lake a as the research background and assumes a sudden water pollution accident to simulate the process of pollutant tracing the calculation results show that the new method effectively improves the calculation efficiency meets the needs of sudden river pollution source analysis and can be used for the active identification of river pollution source information and the corresponding emergency response this paper is organized as follows section 2 presents the surrogate modelling based on variable fidelity model framework section 3 presents the procedures of methods of tracing the pollution source section 4 presents the process and results of case study section 5 provides the conclusions 2 variable fidelity model framework 2 1 surrogate system a surrogate model is also called a metamodel simplified model model simulator or response surface such a model can approximate the main features of the original model with a very low computational cost and obtain the relationship between the system input and output some commonly used surrogate models include the kriging krg radial basis function rbf polynomial response surface prs and support vector regression svr models razavi s et al 2012 the gaussian process gp is a supervised learning algorithm that approximates the output of the original model using a gaussian stochastic process zhang j et al 2016 this method builds a stochastic system that conforms to the gaussian distribution to model output based on the input of the original model of the n m group x m 1 m 2 m n m and output f x g m 1 g m 2 g m n m i e 1 g n μ k where μ is the mean function which can be used as the output of the surrogate model the covariance k is used to quantify the uncertainty of the surrogate model the mean function and covariance function of gps vary and the mean function can be a constant mean function linear mean function or polynomial mean function additionally the covariance function can be a linear covariance function polynomial covariance function or square exponential covariance function this paper uses the gpml gaussian process for machine learning toolkit developed by rasmussen and nickis 2010 and applies the constant linear form function as the mean function of the gaussian process 2 2 variable fidelity model process in the process of building a surrogate system the original model is usually required to generate training data the quality of the training data directly affects the accuracy of the surrogate system high fidelity models often more accurately simulate the migration of pollutants in rivers such as by considering the viscous flux and using high resolution grids however this approach increases the computational cost although low fidelity models have a low calculation cost they can only simulate the basic trend of the migration process and the quality of the training data cannot be guaranteed in this paper the difference between the two fidelity models is mainly reflected by the difference in meshing which is related to the grid elements mesh quality and number of meshes and the different computational solutions as related to the turbulence models used and format differences to balance the advantages of both model types this paper builds a variable fidelity model vfm han z h et al 2008 based on high and low fidelity models the specific modelling steps are as follows i define the variables to be determined and the sampling range the variables to be determined in this paper are the intensity discharge time and location of sudden pollution sources near a river ii two types of samples are generated using the design of experiment doe approach high fidelity samples and low fidelity samples then high and low fidelity hydrodynamic water quality models are used to calculate the corresponding response values the samples are obtained using latin hypercube sampling lhs the sample set includes input and output data in which the input data such as the source location release time and released mass are obtained using latin hypercube sampling lhs high and low fidelity hydrodynamic water quality models are used to calculate the corresponding output data iii after obtaining the high and low fidelity samples and responses a gaussian process model is used to establish the high and low fidelity surrogate models iv the bridge function proxy model is established according to the differences in the response values of the high and low fidelity surrogate models for the high fidelity samples v the vfm is a combination of the low fidelity model and the bridge function model vi the refinement is repeated until the average error between the vfm and the high fidelity model is below a certain threshold here 5 go to step vii otherwise execute the sample updating strategy add a new sample and calculate the high fidelity model response then go to step iii to update the vfm this paper uses the sample updating strategy proposed by han et al 2008 the stop criterion is a preset sample estimate vii according to the obtained vfm if a set of input data is given the corresponding output data can be obtained 2 3 bridge function the most common approach that can be used to handle the differences between the low fidelity model and high fidelity model is to create a bridge function to correct the response of the low fidelity model and align it with the response of the original model in the literature on multi scale fidelity modelling this process is called correction tuning scaling or calibration among the three commonly used bridge functions the multiplication bridge function is a local bridge function that can map the low fidelity model to the high fidelity model in a small space but the global scope cannot be effectively mapped additionally since the multiplication bridge function is a ratio representation of the high and low fidelity models if the value of the low fidelity model is small or even close to zero the bridge function approaches infinity which is unreasonable the addition bridge function is a global bridge function based on the difference between the high and low fidelity models and can avoid the disadvantages of the multiplication bridge function assuming that g h m and g l m represent the response values of the high fidelity model and low fidelity model of the original system respectively the vfm can be obtained by applying additive mapping γ m to the low fidelity model the mathematical expression of this process is as follows han zh et al 2013 2 γ m g h m g l m where the bridge function is approximated by a gp and the vfm g v f m m is approximated by 3 g v f m m g l m γ m where m is the input data of the model i e the pollution source information mentioned in this paper including the pollution source location coordinate pollution discharge time and pollution discharge intensity g l m is a proxy model of the low fidelity model constructed using a gp 3 method of tracing the pollution source 3 1 bayesian reasoning according to bayes theorem the uncertainty parameter m in the above contaminant transport model can be estimated from the measurement data d for simplicity we rewrote the model to the following compact form 4 d g m ε d where g m is a forward simulation operator and ε d is the monitoring error according to bayesian reasoning 5 p m d p d m p m where p m is the prior probability of identifying parameter m and p m d is the corresponding posterior probability assuming that the observed data noise obeys a gaussian distribution with a mean of 0 and a variance of σ d the likelihood function can be constructed as follows 6 p d m 1 2 π σ d n d 2 e x p 1 2 g m d t σ d 1 g m d where n d is the number of monitoring data points in inversion assessment based on a vfm g m can be replaced by the vfm g v f m m therefore eq 6 can be rewritten as 7 p d m 1 2 π σ d n d 2 e x p 1 2 g vfm m d t σ d 1 g vfm m d based on the monitored value d the posterior distribution of the model parameter m can typically be estimated however these probability distribution functions are very complex and can not be calculated directly so it is necessary to estimate the posterior distribution of interest parameters through random sampling dream zs is a random variable generation technique designed to generate samples from a given probability distribution where it is used for bayesian statistical inference of model parameters 3 2 dream zs as a type of mcmc method the early dram method delay rejecting adaptive metropolis method dram has been widely used in the inversion of groundwater model parameters due to its high sampling efficiency in sampling a probability distribution and stable performance zeng et al 2012 zhang et al 2016 however the algorithm is a singular approach that is mainly suitable for cases in which the posterior distribution of a parameter has a single peak when the parameter displays multiple peaks the dram method often cannot obtain a reasonable convergence result vrugt et al 2016 therefore vrugt et al 2003 considering the mh algorithm hastings wk 1970 and the global optimization method sce ua duan q et al 1992 proposed an efficient multichain mcmc algorithm namely the scem ua algorithm by improving the multichain mcmc algorithm differential evolution markov chain de mc braak et al 2006 vrugt and colleagues further proposed the dream zs algorithm vrugt et al 2008 the algorithm uses several parallel markov chains at the same time each parallel chain has different search starting points the search step and direction are adaptively adjusted during the search process and multiple global optimal regions are effectively searched this diversity makes the parallel chains converge in the high probability density interval 3 3 coupling the surrogate model and dream algorithm the use of the surrogate model to replace the more time consuming and accurate numerical simulation of inversion is based on the establishment of a surrogate model with reasonable accuracy the success of this surrogate algorithm relies heavily on the accuracy of the surrogate model if the accuracy is too low inversion may be poor by adding new sample points according to certain criteria constructing an accurate surrogate model in a local scope and improving the accuracy of the model in the posterior space by cyclically updating the proxy model modelling issues can be avoided to achieve this goal this paper utilizes the advantages of the traditional inversion method based on the msp criterion and constructs a suboptimization problem of the corresponding inversion problem in the dream inversion process then the traditional optimization method is applied to update each sample with a small computational cost the optimal solution is predicted then the optimal solution of these predictions is used to perform accurate numerical analysis the result is added as a new sample to the existing data set and the surrogate model is continuously updated as a result the sample point sequence converges to a local or global optimal solution the specific process is as follows step 1 obtain a time series of pollutant concentration d at the monitoring point and determine the variable m to be analysed and the corresponding parameter range in practical application d is obtained by measurement however due to the lack of measured data d is obtained by hydrodynamic water quality simulation in this paper step 2 construct the initial variable fidelity model firstly the initial sample input m is obtained by lhs and the high and low fidelity models are used to calculate the pollutant concentration value g m at the monitoring position then the initial vfm is established according to a gp step 3 based on bayesian inference and dream zs algorithm inversion as shown in eq 5 6 7 the original hydrodynamic water quality model is replaced by the vfm in the inversion process dream zs is designed to generate samples from a given probability distribution where it is used for bayesian statistical inference of model parameters if the algorithm converges it is terminated otherwise the process proceeds to step 4 step 4 construct the following suboptimization problem 8 min j ˆ m ω g v f m m d 2 2 where ω 1 d η meas and η meas is a constant step 5 the new sample input point m is obtained by solving the sub optimization problem firstly select the optimal result for each chain obtained in the current iteration m b e s t from step 3 then determine the trust domain 9 m max m b e s t δ r m min min m b e s t δ r m max and use the genetic algorithm amor v m et al 2002 to find the local optimal solution m in the trust domain 10 m argmin j ˆ m where the radius of the trust region is controlled by a gaussian decreasing function 11 r r b e s t r min exp n i 2 n max 2 r min r m ax r min are the maximum and minimum trust region radii respectively n i is the current iteration number n max is the maximum number of iterations and δ is the control factor step 6 update the vfm m is substituted as a new input into the high fidelity model to obtain the corresponding output g h m thereby producing a new sample point m g h m thus a new sample set is generated and the vfm is re established step 7 go to step 3 and continue to trace the source with the new vfm 4 case study 4 1 description of the problem lake a is kunming lake in xixian new area xian city shaanxi province china and it consists of a north lake and south lake it s still under construction the total reservoir area is 10 4 square kilometres with a total reservoir capacity of 46 million cubic metres the north lake with an area of 7 square kilometres and a storage capacity of 22 million cubic metres is the flood diversion area of the feng river and provides storage when the river floods thus this lake is of great cultural and ecological value the lake is located in an urban area and is prone to sudden water pollution events due to intense human activities when pollution occurs the source information must be identified and the pollution must be dealt with promptly therefore this case simulates the determination process for a hypothetical sudden water pollution event we assume that at 00 00 on a certain day a vehicle carrying a chemical substance with a molar mass of 24 g mol has a traffic accident in the reservoir area and that the pollutant is instantaneously discharged the source can thus be expressed as follows 12 s m δ x x s y y s h t t o n h t t o f f where δ is the dirac function h is the heaviside unit step function m m l 2t 1 is the source strength x s y s is the point source position t o n t is the start of the leakage time and t o f f t is the end of the leakage time the location of the given pollutant leakage event is the north lake of doumen reservoir with coordinates x s y s 570500 3785500 the leakage intensity is 800 g m2 s and the leakage event lasts for 0 5 h in this paper the inversion of three different fidelity models is compared under the same calculation conditions to verify the effectiveness of the method 4 2 high and low fidelity models the establishment of high and low fidelity models for sudden water pollution assessment in river areas includes grid division and computational steps the north lake of doumen reservoir is a plain reservoir the topographical changes are relatively small and the vertical scale is much smaller than the horizontal plane scale therefore it is assumed that the pressure along the water depth follows a hydrostatic pressure distribution and that the pollutant is evenly distributed vertically after the accident based on the concentration a two dimensional hydrodynamic water quality model with a vertical averaging method is used to simulate the pollutant diffusion process the governing equation is as follows 13 u t f s s where u is the conserved physical quantity f is the flux and s s is the source term if the viscous flux is not considered the low fidelity model is as follows see zhao d h et al 1994 u h h u h v h c i f x h u h u 2 g h 2 2 h u v h u c i f y h v h u v h v 2 g h 2 2 h h v c i s s 0 g h s o x s f x g h s o y s f y d k h c i k c i h c i s i s 0 x z b x s 0 y z b y where f x f y represent the fluxes in the x and y directions respectively h l is the water depth u lt 1 and v l t 1 are the flow velocity components in the x and y directions respectively g lt 2 is the acceleration of gravity z b l is the river bottom elevation n m tl 1 3 is the manning coefficient c i m l 3 is the vertical average concentration of the pollutant k c i t 1 is the comprehensive degradation coefficient of the pollutant where different pollutant components have different coefficients d k l2t 1 is the pollutant diffusion coefficient s o x and s f x are the bottom slope and friction gradient in the x direction respectively s 0 y and s f y are the bottom slope and friction gradient in the y direction respectively s i is the source and sink term and is the gradient operator this paper considers only one pollutant component considering the viscous flux the k ε model is combined with the above formula to obtain the high fidelity model see erpicum et al 2014 f x h u h u 2 g h 2 2 2 h ν ν t u x h u v h ν ν t u y v x h κ u h ν t σ t κ x h ε u h ν t σ t ε x h u c i f y h v h u v h ν ν t u y v x h v 2 g h 2 2 2 h ν ν t v y h κ v h ν t σ t κ y h ε v h ν t σ t ε y h v c i s s 0 g h s o x s f x g h s o y s f y p h p κ b h ε c ε 1 ε κ p h p ε b c ε 2 ε 2 κ h d k h c i k c i h c i s i u h h u h v h κ h ε h c i ν t c μ κ 2 ε p h h ν t 2 u x 2 2 v y 2 u y v x 2 c f g n m 2 h 1 3 p k b c f 1 2 u 3 p s b c s γ c s 2 c μ 1 2 c f 3 4 u 4 h u c f u 2 v 2 s 0 x z b x s 0 y z b y s f x n m 2 u u 2 v 2 h 4 3 s f y n m 2 v u 2 v 2 h 4 3 where κ l2t 2 is the turbulent kinetic energy ε l2t 3 is the turbulent dynamic energy dissipation rate ν l2t 1 is the dynamic viscosity other variables have the same meaning as the low fidelity model and the characteristic parameters are as follows c μ 0 09 c ε 1 1 44 c ε 2 1 92 c ε γ 1 8 3 6 σ k 1 0 σ ε 1 3 in general for the same working conditions the rationality of the selection of the modelling method and numerical method can directly influence the success or failure of the simulation see fig 1 in terms of models those that consider the viscous flux better reflect the real situation than do those that do not consider this term different numerical methods will also affect the accuracy of the simulation theoretically as long as the boundary conditions are correct and the parameters are reasonable when the mesh is infinitely refined the numerical solution process will converge however in reality the increase in the number of units will increase the calculation time and the consumption of computational resources in this paper the finer unstructured triangular meshes were used for the high fidelity model hf and the coarser unstructured triangular meshes were used for the low fidelity model lf the finite element method is applied to simulate pollutant diffusion table 1 shows the difference between the two models fig 3 shows the meshing results see fig 2 see table 2 the purpose of the forward simulation is to obtain enough sample data to build and correct the fidelity model because a sudden water pollution event usually lasts for a short period of time the hydrological conditions change little during this period so the following model parameters and initial boundary conditions are given according to historical data and the relevant design requirements among them the hydrological data is from the 69 year series data of qindu hydrological station of fenghe from 1944 to 2012 the hydrodynamic data is from our field measurement model parameter determination generally the method of selecting the parameters used in a model is an empirical formula based method or is based on measured data the former is ideal for certain water bodies the latter requires a large amount of historical data for a reservoir under construction it is difficult to obtain this amount of data because the duration of sudden water pollution events is short it is assumed that the model parameters change little in the simulation period according to historical hydrological data and the relevant design requirements the following hydraulic parameters are used in the hydrodynamic simulation a manning s roughness of 0 02 tl 1 3 a dynamic viscosity of 0 0013 m2 s a pollutant diffusion coefficient of 0 0021 m2 s in the x direction and 0 0014 m2 s in the y direction and the degradation coefficient is 0 006 d initial conditions i for the hydraulic conditions the initial flow rate is 0 and the initial water level is 3 m ii for the water quality conditions the initial pollutant concentration is 0 boundary conditions i for the hydraulic conditions no sliding boundary conditions non penetrating boundary are adopted for the solid boundaries i e v n l 0 where n is the solid boundary direction vector and l is the river bank boundary flow velocity conditions are assigned for the water inlet boundary the velocity in the x direction is 0 08 m s and that in the y direction is 0 06 m s the effluent boundary is assigned a water level of 3 m ii for the water quality conditions the inflow concentration is 0 the flux in the outflow concentration is zero and the solid boundary pollutant concentration is c n l 0 the implementation of the forward model comes from previous research results by our research group it is developed in matlab and uses a finite element method for the discretization of the equations 4 3 establishment of the variable fidelity model the variables to be identified in tracing of pollution sources are the horizontal and vertical coordinates of the pollution source pollutant leakage strength duration of pollutant leakage the initial estimation range of each parameter can be estimated based on an analysis of monitoring data and expert experience the vfm is established with the variable to be assessed as the optimized controllable input variable the outputs of the surrogate model are the values at 2 monitoring locations see fig 4 monitoring site1 is located at coordinate 571050 m 3785560 m monitoring site2 is located at coordinate 570550 m 3785850 m from 00 00 03 00 0 10800 s the time series of pollutant concentrations measured at intervals of 1200 s with the established vfm includes 5 input variables i e m x s y s m t o n t o f f and 22 output values i e time series of pollutant concentration at two monitoring points the results are shown in fig 5 to establish the vfm first 220 sample points are extracted in the target interval of the design variables among the 220 sample points 160 points are used to calibrate the low fidelity model 40 to calibrate the high fidelity model and 20 points are used to validate both models the reason why more lf is used in the process of constructing variable fidelity models is that its calculation cost is small about 1 6 of hf see table 1 which helps to shorten the calculation time the two types of sample proportions are selected based on the experience and there are no special requirements the sampling locations of variables x s y s are subject to the boundaries of the study area and the remaining variables are determined by using lhs in the prior interval of the given parameters the sampling results input data are substituted into the high and low fidelity models and the time series of pollutant concentrations at monitoring points are calculated as the system output data because the range of acceptable values differ greatly for each variable normalization is needed the values are all converted to a unified interval in this paper this interval is 0 1 and the processed data are used as the training sample the variable fidelity surrogate model is established with this method the sampling results are shown in fig 6 to study the accuracy of the high fidelity surrogate model gp hf the low fidelity surrogate model gp lf and the vfm surrogate model gp vfm 20 randomly selected samples see fig 6 were substituted into the high fidelity model and the three types of surrogate model for testing to assess the computational efficiency and accuracy of the variable fidelity surrogate system we have used four indicators the coefficient of determination r2 bias proportion bp variance proportion vp and covariance proportion cp to evaluate the prediction performance of the model r 2 i 1 n y ˆ i y 2 i 1 n y i y ˆ i 2 b p i 1 n y ˆ i n y 2 i 1 n y i y ˆ i 2 n v p s y ˆ s y 2 i 1 n y i y ˆ i 2 n c p 2 1 r s y ˆ s y i 1 n y i y ˆ i 2 n where y i represents the output value of the ith sample obtained from the transport simulation model y ˆ i is the output value of the ith sample obtained from the surrogate model y is the average of y i s y s y ˆ represents the standard deviation of y i and y ˆ i if the indexes bp and vp are small and r2 and cp are large it indicates that the approximation degree of the surrogate model is high therefore when the simulation model meets the accuracy requirements the surrogate model can be used to replace the simulation model the accuracy evaluation indicators of the surrogate model are shown in table 3 the results show that r2 is very close to 1 indicating a good fit the small bp value proves that the deviation degree between the predicted mean and the actual value is small and the small vp indicates that the deviation degree between the predicted variance and the actual variance is also small cp represents the remaining non systematic errors by comparing the prediction accuracy of the three models it can be found that the variable fidelity surrogate model gp vfm has better prediction effect therefore the agent model can be used to replace the physical model and coupled with the optimization model to determine the source of sudden water pollution 4 4 inversion of the pollution source in eq 12 if the point source location leakage time and leakage intensity are known where m x s y s m t o n t o f f the pollutant distribution can be simulated according to the hydrodynamic water quality model in eq 13 this is called the forward simulation the pollution source inversion problem involves the opposite approach that is according to the downstream pollutant monitoring data d the pollution source information m is inversely derived assuming that the source information m is unknown two sensors obtain monitoring data the concentration time series at the two monitoring points are obtained by the forward high fidelity water quality simulation as additional information to identify the pollution source information in reverse using the method described herein the parameters of the dream zs algorithm are set as follows the number of parallel chains n is 10 the number of evolutionary algebra steps for the parallel chains is 20000 the prior distribution of each parameter is evenly distributed and the above values are constant the parameters of the genetic algorithm are set as follows the maximum number of evolutionary algebra steps in the algorithm is 50 the population size is 30 the crossover probability is 0 9 and the mutation probability is 0 01 the values of these parameters are related to the problems to be solved usually they can be selected according to experience and references for a more detailed description of the algorithm refer to references amor v m et al 2002 vrugt et al 2016 the low fidelity surrogate model high fidelity surrogate model and gp vfm were combined in an optimization process to determine the pollution source through the dream zs algorithm the statistical sample characteristics were estimated by using the nuclear density and the results are shown in fig 7 since the algorithm in this paper is a random algorithm with uncertainty to avoid accidental interference 5 times are performed for each case taking the posterior mean value of the parameter as the parameter estimator the mean value and error of the five calculation results are calculated in the dream simulation there are 10 parallel chains the length of each chain is 20 000 the iterations of surrogate refinement is implemented 20 times as shown in fig 7 we compare the marginal posterior probability density functions ppdf of the five contaminant source parameters x s y s m t o n t o f f estimated by the dream simulation i e the lf based dream approach pink curves the hf based on dream approach blue curves the gp vfm based dream approach red curves the mean value and error of the five calculation results are calculated in table 4 fig 8 compared the distance between the pollution source location determined based on the three models and the real source it is found that the three approaches can accurately estimate the posterior distribution of the model parameters the hf based on dream approach and the gp vfm based on dream approach are only about 50 m away from the real source although there is still a difference from the real area the estimated source area is 7 square metres in terms of the search space the scope has been greatly reduced by combining this approach with physical methods accurate source identification can be achieved 4 5 comparison of inversion results under different working conditions 4 5 1 comparison of simulation results under different source parameters in order to investigate the reliability of the algorithm under different source parameters two groups of different pollution source parameters labeled s1 and s2 see table 5 are given the time series of pollutant concentration at the corresponding monitoring point at the same monitoring point is calculated see fig 9 we can take it as a known condition to determine the pollution parameters according to the method in this paper the results are shown in figs 10 13 in the dream simulation there are 10 parallel chains the length of each chain is 20 000 which means that the total number of three model evaluations is 200 000 the iterations of surrogate refinement is implemented 20 times as shown in figs 10 and 12 the chains for the five source parameters can accurately catch the true values black crosses in figs 11 and 13 we compare the marginal posterior probability density functions ppdf of the five contaminant source parameters x s y s m t o n t o f f estimated by the dream simulation i e the lf based dream approach pink curves the hf based dream approach blue curves the gp vfm based dream approach red curves it is found that the three approaches can accurately estimate the posterior distribution of the model parameters 4 5 2 effects of different monitoring locations given two groups of different monitoring schemes labeled ms1 ms2 see table 6 with unchanged pollution source parameters the high fidelity model was calculated to obtain the time series of pollutant concentration at the corresponding monitoring location as shown in fig 14 the monitoring value are taken as the known condition and the pollution source parameters are determined according to the method in this paper the results are shown in figs 15 and 16 from the results of figs 15 and 16 based on the high and low fidelity models and the vfm the calculated posterior density curve of the parameters well covers the true value and its peak position is close to the true value indicating that the algorithm is accurate by comparing the estimates of the five parameters in the posterior space it is not difficult to find that the inversion of the high fidelity model and the vfm is better than that of the low fidelity model in practical application whether the forward model can accurately reflect the diffusion process of pollutants and coincide with the measured value at the monitoring point will effectively affect the accuracy of estimation if we further compare the inversion results based on the high and low fidelity models we will find that the low fidelity model based on 160 training samples is not more accurate than the high fidelity model based on 40 training samples which indicates that in addition to the sample quality the sample size and structural parameters directly affect the accuracy of modelling table 7 compares the error levels of the three methods the errors for the variable m are approximately 0 75 and those for the other variables are different partly because the sample interval of this parameter is relatively small and those of the other indicators are large additionally the small sample size results in a low sample density which affects the accuracy of the model overall vfm dream yields the lowest relative error and the best performance the three surrogate models complete the inversion simulation in a relatively short amount of time up to 5 8 h because of the high cost of iterative process calculation based on physical model this paper iterates 20 000 times and it is expected to take 56 d this paper only simulates pollutant source determination based on surrogate model if the accuracy of the case is acceptable the use of the surrogate model about 5 8 h will significantly improve the calculation speed according to rough estimation the time consumption is more than 200 times faster than the former obviously the specific speed is also related to the number of iterations 5 conclusion for sudden water pollution incidents in rivers the ability to quickly identify the pollution source is of great importance for early accident warning and emergency control based on bayesian reasoning a variable fidelity surrogate dream optimization model for coupled inversion is established in the posterior space of a pollution source in this approach the vfm surrogate includes high and low fidelity models fused by an addition bridge function these high and low fidelity models are established based on a gp generated according to lhs and high and low precision hydrodynamic water quality numerical calculations to reduce the uncertainty of the surrogate model and improve the efficiency of source identification this paper integrates the variable fidelity surrogate and dream optimization process based on the msp criterion new samples are inserted into the sample set to improve the precision of the surrogate model in the posterior space of the pollution source thereby creating an efficient computational model for determining the source location release time and released mass of sudden water pollution events that affect rivers the proposed approach is tested with numerical and engineering examples the results show that the new approach can effectively improve the calculation efficiency and identify source information for sudden water pollution incidents in rivers thus it can be used for the identification of source information and establishment of emergency response measures the specific conclusions are as follows i the surrogate model is used to approximate the complex hydrodynamic water quality model and provide a globally interpretable approach that avoids nonlinearity issues however this model does not perfectly represent the underlying response function of the original model nor can it capture complex feature relationships nonetheless these issues do not prevent the application of the surrogate model in the determination of pollution sources the results show that it is feasible to replace the original model with the surrogate model in pollution source determination based on bayesian inference and the reliability of this approach is greatly affected by the sample size structural parameters and other factors ii unlike using simple model approximations in this study high and low fidelity models are used to establish a vfm with high local accuracy in the traceability process the vfm is coupled with an inversion model and driven by historical data to add sample points to the sample set and approach the local and global solutions therefore it is unnecessary for the surrogate model to have high approximation accuracy in the entire research area and the model only needs to have high accuracy in important areas especially near the optimal solution this approach not only reduces the calculation cost but also improves the accuracy of inversion iii new samples are adaptively inserted based on the msp strategy to improve the accuracy of the replacement model in the posterior space of the pollution source during this process the reliable ranges of inversion variables are controlled by a gaussian decreasing function and the search space of the suboptimization problem is gradually reduced to decrease the cost of selecting new sample points and improve convergence the selection of the control factor will affect the calculation results if the radius of the reliable range shrinks too fast precocity issues can occur further research is needed to develop better control strategies or other methods to improve convergence iv the greatest advantage of the surrogate model is the low computational cost this paper shows that the algorithm effectively improves the efficiency of inversion and is more than 200 times faster than other methods as the number of iterations increases the advantages of the surrogate model will likely become more obvious in the future we will determine if the proposed numerical method of determining pollution sources can be combined with intelligent sensors and big data technology if so physical methods such as artificial inversion or unmanned ship monitoring could effectively solve the problem of determining the source of sudden water pollution in river or reservoir areas declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by the national natural science foundation of china grant no 51979222 and 91747206 the natural science foundation of shaanxi province china grant no 2019jlm 62 and the natural science basic research plan in shaan xi province of china grant no 2019jm 284 we would like to extend special thanks to the editor and reviewers for insightful advice and comments on the manuscript appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104811 
25973,the canadian prairies are dominated by numerous depressions which can modify the lateral transfer of water to prairie streams few studies were conducted to simulate the pothole dynamics using their actual spatial distributions this study proposes a computationally efficient prairie region inundation mapping prima model as a hydrologic routing model for a more accurate and comprehensive storage dynamics simulation and inundation mapping in the prairies prima shows potential for simulating the potholes extents when comparing its results with remote sensing data of pothole areas with an accuracy of 85 averaged over two prairie basins in saskatchewan canada prima is three eight times as computationally efficient as the recently developed wetland dem ponding model wdpm due to its computational efficiency and ability to provide a good simulation of inundation extents prima shows strengths as a possible tool for pothole inundation mapping and storage dynamics simulations graphical abstract image 1 keywords prairie pothole inundation mapping distributed modelling hysteresis contributing area remote sensing 1 introduction the north american prairies are characterized by numerous depressions anteau et al 2016 zhang et al 2009 known as wetlands sloughs prairie potholes puddles geographically isolated wetlands giws and dugouts due to the existence of these potholes the runoff production in prairies follows a fill and spill mechanism shaw et al 2012 wherein each pothole contributes flow to downstream potholes after being filled therefore the majority of the prairies are designated as being non contributing wherein the surface water runs off into isolated or internally drained basins the derivation of the non contributing area map over the prairies was quite subjective and was derived from the visual interpretation of topographic contour maps shaw et al 2013 more details on the non contributing area map and its derivation are provided in the supplemental file section s1 potholes can be connected by surface or subsurface flow of water however this connection differs dramatically in length and time the subsurface connection between potholes is slow and can improve the quality of water in the basin whereas surface connection is fast and limited to significant precipitation events ameli and creed 2017 simulating the hydrological behavior of pothole dominated landscapes is challenging because of the difficulty in characterizing the fill and spill mechanism which leads to a hysteretic relationship between the basin storage and the contributing area shook and pomeroy 2011 thus the prairies are often referred to as the graveyard of hydrological models the prairie potholes complexities can be simulated using conceptual approaches e g mekonnen et al 2014 or satellite dem imageries based approaches e g chu et al 2013 shaw et al 2013 2012 shook et al 2013 shook and pomeroy 2011 muhammad et al 2019 mekonnen et al 2014 introduced the conceptual pdmrof probability distribution model based runoff generation approach which assumes that runoff is a function of the basin storage capacity in pdmrof the capacity of different potholes in the basin is assumed to follows a pareto distribution and the runoff from potholes is calculated by integrating the probability density function the pdmrof concept showed potential to simulate the prairie streamflow dynamics when implemented into different models such as mesh mekonnen et al 2014 and hbv ahmed et al 2020 however as a conceptual approximation the pdmrof cannot represent the spatial extents of pothole water storage and it might be difficult to map its parameters to field measurements land cover data classified from satellite imageries can be used to identify the potholes evenson et al 2016 muhammad et al 2019 then each of the identified potholes can be simulated using a separate reservoir that contributes surface flow to the downstream area after exceeding its maximum capacity this concept of representing potholes as separate reservoirs was used in the conceptual pothole cascading model pcm shook et al 2013 in pcm the properties of the potholes are obtained from dems and a small number of potholes is used to represent all potholes in the basin the methodologies that use separate reservoirs to represent the potholes do not represent the fill merge split processes of the potholes and some of them may not represent the spatial distribution of water on the landscape dems can be used to delineate depressions in the basin for example the puddle delineation pd algorithm chu et al 2010 was proposed to delineate the landscape and it differentiates the landscape into pothole and non pothole areas the topographic characteristics of the potholes cascading order surface area and storage and flow direction for the non pothole area are obtained from dems the output of the pd algorithm can be used by hydrologic models to route flows and simulate the fill spill mechanism of the prairies e g puddle to puddle p2p model chu et al 2013 and swat nasab et al 2017 the wetland dem ponding model wdpm is a dem based model that distributes water on the landscape shook et al 2013 shook and pomeroy 2011 wdpm was the first explicit method able to simulate the spatial distribution of water on the prairies and it was used to simulate the contributing area however wdpm was found to be computationally expensive requiring thousands of iterations to reach the steady state solution converge despite not using conventional flow equations to transfer water dem based hydraulic models such as mike she dhi 1998 and the recently developed hec ras model hydrologic engineering center 2016 could be used to simulate the fill and spill phenomena in the prairies however these models use the saint venant equation to simulate the movement of water requiring numerical solutions of the differential equations which is computationally expensive there is a need for a computationally efficient model that can simulate the pothole storage dynamics using their actual spatial distributions this type of model could be easily adapted to simulate the impact of the potholes on the system response in prairie basins and for inundation mapping and or flood risk assessments in the prairie pothole region the main objective of this study is to develop a novel computationally efficient prairie region inundation mapping prima model and test its applicability as a fully distributed simplified hydrological routing model to simulate the spatiotemporal surface water movement and storage dynamics over the landscape prima is based on the cellular automata ca wolfram 1984 approach as a novel method for simulating filling spilling and merging splitting processes of the potholes and calculating the amount and direction of flow over the landscape prima is based on five necessary modifications of the original ca approach implemented to improve the computational efficiency and ensure the model is mimicking the behavior of the pothole systems 2 material and methods in this section the proposed model prima the study area and the validation of the model against remote sensing data of pothole area are fully described further the study consists of models simulations to compare prima s computational efficiency and performance against wdpm as a reference in terms of simulating the complex response of the pothole dominated watersheds and the spatial extents of water over the landscape a detailed description of the methodology is given below 2 1 prairie region inundation mapping prima model 2 1 1 water redistribution and routing wrr component the novel water redistribution and routing wrr component in prima is based on the ca approach which has been used for simulating the movement of water dimitriadis et al 2016 li et al 2013 liu et al 2015 parsons and fonstad 2007 ca based models can replace hydraulics differential equations with a set of rules which are hydrological simplifications of the saint venant and or manning s equations bates et al 2010 to represent the surface water movements di gregorio and serra 1999 wolfram 1984 the simplicity of the ca models makes them more computationally efficient than other hydraulic models the wrr component in prima is based on five modifications introduced in this study see the supplemental file section s2 to the ca model liu et al 2015 this component moves water sequentially between dem cells following the topography the wrr component is a combination of the minimization algorithm di gregorio and serra 1999 and manning s equation to determine the amount and timing of flow leaving a central cell to its eight neighboring cells respectively the minimization algorithm attempts to minimize the difference in water surface elevation between contiguous cells a hypothetical example and a flowchart of the wrr component which is iterative and applied to each cell in the dem is presented in fig 1 the following rules apply 1 using the water elevation of the current central cell wel 0 and the surrounding cells wel i i 1 8 calculate the average water elevation av m by eq 1 1 a v w e l 0 i 1 n w e l i n 1 where n is the number of neighboring cells involved in the calculation of the water redistribution 2 eliminate those cells having water elevations greater than the average water elevation i e where wel i av 3 recalculate the average water level for the remaining cells as in step 1 and apply the elimination rule in step 2 4 apply step 3 until no more cells can be eliminated from the calculations 5 distribute the outflow from the current cell to the remaining neighboring cells such that all of them have the same water elevation av 6 the travel time is calculated as the quotient of the grid cell size divided by its water velocity from manning s equation the velocity v m sec of water is calculated based on the fraction of water leaving the current cell to its lowest water elevation neighboring cell in the dem and is assigned to the current cell assuming a wide cross section as 2 v d 2 3 s n where d is the maximum outflow depth from the current cell to its neighboring cells δ 3 4 in the given example in fig 1 m s is the surface water slope m m and n is manning s roughness coefficient unitless after applying the wrr component to all cells in the dem the following steps are applied 1 the minimum travel time for all grid cells is assigned as prima s global time step to maintain the simulation stability and to ensure that the water does not cross more than one cell during a single time interval 2 water reaching the outlet cells is removed from the dem and stored as outflow volume 2 1 2 infiltration and evaporation losses component simple vertical water budget calculations were implemented in prima to allow for simulating the spatiotemporal variation of the water extent and the comparison against remote sensing observations the vertical water budget infiltration and evaporation processes were represented in prima using a simple bucket type approach ahmed et al 2020 the infiltration to and evapotranspiration from the soil are functions of the soil moisture storage the evaporation from the potholes is a function of the mean monthly temperature and potential evapotranspiration a simple degree day approach was also implemented to allow for distinguishing rainfall and snowfall and calculate the snowmelt rates the rainfall snowmelt determined by the degree day approach was added to the ponded water on each grid cell then the amount of infiltration and evaporation were calculated and subtracted from the ponded water if the grid cell does not have ponded water i e dry cell the calculated evapotranspiration is subtracted from the soil moisture storage of the cell after applying the vertical water budget calculations the remaining ponded water is redistributed over the landscape using the wrr component section 2 1 1 it is important to note that prima does not allow for horizontal transfer of water in the sub surface system in summary prima loops through the dem cells from the highest to the lowest elevation to simulate the water movement from uplands to lowlands and to reduce the required number of iterations a flowchart of prima is presented in fig 2 each run of prima includes the following steps 1 the dem cells are sorted by elevation from highest to lowest 2 excess water depth provided as an arbitrary value or calculated by the losses component is added to or removed from the dem 3 the program iterates over each dem cell in order of elevation the amount of water exchanged and water velocity are calculated for each grid cell current cell and its neighboring cells using the wrr component 4 the model s global time step is calculated as the minimum travel time among all cells 5 water reaching the outlet cell s is drained removed from the dem and stored as outflow volume 6 the model checks if i the cumulative global time step is greater than the specific forcing simulation resolution e g hourly or daily ii the water depth change is smaller than a user predefined elevation tolerance the depth change is the maximum change in water elevation over all cells calculated every n iteration e g 1000 ii the outflow volume change is less than a user predefined volume tolerance the volume change is calculated as the change in the cumulative outflow volume every n iteration 7 if any of the conditions above in step 6 is met the model run terminates otherwise the model re iterates over the dem cells i e repeats step 3 to step 7 step 2 to step 7 are repeated for every addition and or removal of water depth and volume change are error measurements used to terminate the run because the model may take thousands of iterations to make negligible changes in the water surface elevation we choose to calculate depth and volume change every n iteration interval to ensure that the model reached the steady state solution and it was not trapped in a local optima solution prima is a flexible model wherein any component process can be activated or deactivated fig 2 as an example it can allow for the redistribution of water over the landscape without allowing the water to leave through outlet cells the concept behind the modifications supplemental file section s2 is to reduce the running time of prima by draining water from multiple outlet river cells supplemental file section s3 and to allow for travel time calculations so that it can be implemented in the future into a hydrological land surface model as a runoff generation algorithm in terms of input and output data prima requires the topographic data dem outlet cell s location elevation and volume tolerance and the excess water depths as either uniform or spatially variable to be distributed over the landscape as inputs the excess water depths can be provided as arbitrary depths or calculated by the losses component if the losses component is used the model requires precipitation and temperature as input forcings a preliminary run of prima can help in identifying possible outflow cells and reasonable tolerance values prima generates water depth raster value of state variables soil moisture snowmelt snow water equivalent etc outflow volume rate and run summary number of iterations and execution time as outputs prima does not do any pre processing to identify depressions or flat areas in the dem the model uses the wrr component to determine if water is trapped in pothole cells or flat area cells 2 2 study area and data in order to fully evaluate prima it was important to test it in areas where dem at high resolution and remote sensing data of the observed water areas were readily available and the fill and spill response is well understood and characterized thus smith creek research basin subbasin 5 scrb5 and saint denis national wildlife area sdnwa in saskatchewan canada fig 3 were selected for this study because of an extensive history of studies in the region fang et al 2010 mengistu and spence 2016 shaw et al 2012 shook and pomeroy 2011 van der kamp et al 2003 the basins are useful for testing the behavior of the models because they represent two extremes within the variety of topography in the prairie ecozone scrb5 with an area of approximately 11 km2 is relatively flat slopes of 2 5 and has a well developed stream with a prominent valley on the other hand sdnwa is hummocky slopes of 10 15 has no defined drainage system nor an obvious outlet fig 3 and has an area of approximately 22 km2 both basins have more than 1000 potholes with areas larger than 100 m2 however sdnwa is dominated by large potholes ponds area 10 000 m2 that are scattered over the landscape and occupy almost one third of the basin area the dominant land cover on both basins is cropland the simulations were performed using available lidar based dems for both basins the scrb5 lidar dem has a horizontal resolution of 5 m and were collected between october 14to october 16 2008 shook and pomeroy 2011 the sdnwa lidar dem was collected on the august 9 2005 with 5 m horizontal resolutions shook et al 2013 there was some water in the potholes when the lidar data were collected at each basin and hence all modelling and simulations were done relative to the initial conditions of water elevation the dems were not conditioned to account for the existing culverts in the study areas the observed water extents areas were identified from remote sensing data rapideye satellite imageries that are available for scrb5 and the area above pond 90 within sdnwa sdnwa 90 fig 3 for the 2011 spring snowmelt period the images have a horizontal resolution of 5 m and were captured on may 13 2011 and may 18 2011 for sdnwa 90 and scrb5 respectively shook et al 2013 water depth observations at different potholes are available at sdnwa 90 bam et al 2018 but they are intermittent and thus the observation that were available within the 2011 snowmelt period were used fourteen different potholes were found to have one recorded water depth during that period 13 of the measurements were available on may 12 2011 and the remaining one on may 13 2011 the locations of the measurements are plotted in fig 3 both the observed water areas and depths were used to assess prima s performance in simulating the complex potholes extents dynamics and storage 2 3 simulating the extents of surface water areas by prima the simulation period of prima was set from april 1 2011 to the date the image was captured for each of the two studied basins may 13 2011 and may 18 2011 for sdnwa 90 and scrb5 respectively to simulate the spring snowmelt event the model used the gridded canadian precipitation analysis capa product lespinas et al 2015 and the global environmental multiscale gem atmospheric model mailhot et al 2006 output as the respective precipitation and temperature forcing on a daily time scale fig 4 the forcing was spatially uniform over the basins because each basin was located inside one pixel of the gem capa data the soil moisture and the potholes almost reached their storage capacity for the studied areas prior to the 2011 flood event mengistu and spence 2016 shook et al 2013 thus the initial soil moisture storage was assumed to be close to the water holding capacity however we assumed different scenarios for the initial filling conditions of the potholes assuming that all potholes are 0 25 50 75 and 100 full to test the effect of the pothole conditions on changing the outflow of the basin and the resulting water extents the 100 full condition of the potholes was obtained by adding a significant water depth to the landscape and then redistribute that water using prima s wrr component until all depressions are filled then using qgis software the capacity of individual potholes was identified for each scenario the depth of stored water in individual potholes was obtained by multiplying its capacity by the fraction of filling i e 0 25 for 25 full scenario in this test all components of prima were used i e water redistribution and losses the excess water depths calculated by the losses component for each day were redistributed over the landscape and drained from the outlet cells the accumulated precipitation during fall and winter was used as initial accumulated snow on ground a summary of prima s parameters and their values are presented in table 1 the parameters were determined from the literature and available landcover data or were set to their default values according to ahmed et al 2020 the parameters in table 1 were not calibrated to simulate the observed water areas the exceedance probabilities and the spatial distribution of the water areas at the end of the simulation were compared to that of the observed water areas for both basins the average of absolute deviations was used as a goodness of fit measurement to assess the accuracy of prima s exceedance probabilities of water areas two performance metrics were used to further validate prima s spatial water extents against remote sensing data sensitivity s v and specificity s c s v and s c quantify the probability of correctly predicting a grid cell within the basin as inundated or non inundated respectively bharath and elshorbagy 2018 and are defined as 3 s v f c f c f o c 4 s c n f c n f c n f o c where f c is the total number of observed inundated cells that were correctly predicted as inundated by the model f oc is the total number of observed inundated cells that were falsely predicted as non inundated by the model nf c is the total number of observed non inundated cells that were correctly predicted as non inundated by the model and nf oc is the total number of observed non inundated cells that were falsely predicted as inundated by the model both sv and s c range from 0 to 1 with values closer to 1 demonstrating high probability of accurately predicting inundated and non inundated areas respectively also the error in simulating the water depth in the 14 identified potholes over sdnwa 90 was assessed 2 4 experimental setup prima vs wdpm it was important to evaluate the computational efficiency and the resulting water extents of prima against another simple hydraulic model wdpm that was proven to be successful in redistribution of water over the complex prairie landscape shook et al 2013 shook and pomeroy 2011 wdpm iteratively redistributes excess water over a dem using the method of shapiro and westervelt 1992 in which the water is redistributed from a central cell to its eight neighboring cells with each cell taking 1 8 of the water depth difference between itself and the central cell wdpm does not calculate water velocities or travel time all water is assumed to flow instantaneously based on the 1 8 water depth difference rule per iteration wdpm was used as a reference to further assess the performance and results of the proposed prima model for this section and for the sake of comparing the performance of prima to wdpm the losses component and the travel time calculations were not used only the wrr component in prima was used and the water was drained from the outlet cell until both models converged reached the steady state solution the models were tested by applying arbitrary depths of water to the dem and redistribute them without draining the excess water which is referred to as add test after redistribution of water the excess water was drained from the basin outlet and this test is referred to as drain test this was implemented because wdpm can either add water or drain excess water unlike prima that can redistribute and drain the water at the same time there was no attempt to account for groundwater contribution to the outlet the performance of prima and wdpm were assessed relative to the number of iterations required for convergence because the models codes are quite different wdpm was written in c for parallel processing whereas prima was written in fortran 95 for serial processing the term efficiency in the following discussions refers to the number of iterations required to achieve a model state 2 4 1 effect of elevation tolerance on the water distribution of prima and wdpm models the models sensitivity to changing the elevation tolerance was tested on scrb5 scrb5 was selected to test the effect of changing the models tolerances on the produced water extents for both the pothole areas and the riverbanks the models were tested for 1 the addition and 2 draining of water the addition tests were carried out at scrb5 by adding an arbitrary depth of water 100 mm to the empty dem and redistributing it until each model converged for elevation tolerances of 1000 500 100 10 and 1 mm because the models results might be different we used the final water distribution of prima with 1 mm tolerance as an initial state for the drainage test for both models this was conducted to test the agreement between both models results for the same initial condition and water distribution over the landscape both models drained the excess water from the landscape with 1 mm and 1 m3 as the respective elevation and volume tolerances the number of iterations and the final spatial distribution of the water over the landscape for the add and drain tests were compared 2 4 2 simulating the contributing area curves by prima and wdpm contributing area fraction curves were generated for both basins using both models the curves represent the envelope of the relationship between the basin s contributing area fraction and the storage of water the curves were constructed by repeatedly adding water to an initially empty dem until all depressions are completely filled for a fine elevation and volume tolerance 1 mm and 1 m3 following each addition of water the basin was drained for both test areas then an incremental water depth of 1 mm was added and the basin was drained again the contributing area fraction is calculated as the fraction of the outflow volume corresponding to the added 1 mm 3 results and discussion 3 1 suitability of prima for the prairies the exceedance probability of the observed and the simulated water areas for different pothole initial filling conditions for both basins are shown in fig 5 for scrb5 the exceedance probability of the near full scenarios 75 and 100 pothole full showed good agreement with the exceedance probability of the observed water areas for sdnwa 90 the exceedance probabilities of the near full scenarios were almost similar and showed reasonable agreement to that of the observed water areas in terms of the goodness of fit statistic table 2 the near full scenarios showed the smallest error among all scenarios with the 75 scenario being slightly better than the 100 full scenario the near full scenarios showed the best optimal combination of predicting inundated s v and non inundated areas s c over the two basins table 2 although the water extents of the near full scenarios were quite similar in each of the basins the 100 full scenario tended to slightly overestimate the inundated areas when compared to the 75 full scenario s c table 2 the 75 full scenario showed the best performance in predicting both the inundated s v and non inundated s c areas in both basins with values of 0 85 and 0 88 respectively table 2 averaged over the two basins this agrees with the literature about the conditions of the 2011 flood event as the potholes were almost full prior to the snowmelt event mengistu and spence 2016 shook et al 2013 the remaining scenarios 0 25 and 50 showed underestimation of the water areas especially for the larger potholes fig 5 and s v table 2 the actual water extents of the observed water areas and prima s simulated water areas at the end of the simulation period for the 75 full scenario best simulation are shown in fig 6 for both basins prima showed good agreement with the observed water areas extents especially for the larger potholes and the upstream portion of the main river at scrb5 for sdnwa 90 prima s water extents showed good agreement with the observed large potholes however there were some over estimation of the ponded area in the central and northeastern parts of the basin fig 6 with difference between simulated and observed inundated extents in that area of 0 15 km2 the percent observed and simulated ponded area are 8 and 15 respectively at sdnwa 90 the model predicted potholes in central and northeastern part of the basin as inundated that were not observed as inundated by the remote sensing data this overestimation caused some disagreement between the observed and simulated areas exceedance probabilities fig 5 sdnwa 90 in terms of the simulated water depth in the potholes the average percent bias for 14 potholes at sdnwa 90 was found to be 2 and the max absolute error was 23 for the 75 full scenario fig 6 right panel the average total outflow volume of the below 50 full scenarios 0 25 and 50 was 43 000 m3 and was smaller than that of the 75 and 100 scenarios almost 56 500 and 191 600 m3 respectively for scrb5 similarly for sdnwa 90 the average total outflow volume of the below 50 full scenarios 0 25 and 50 was 350 m3 and was significantly smaller than that of 75 and 100 full scenarios almost 8 000 and 88 500 m3 respectively in the below 50 full scenarios the potholes did not reach their capacity especially the larger ones and hence most of the water is being stored with little runoff reaching the outlet however for the 100 full scenarios the majority of the surface runoff can reach the outlet as all potholes are almost full and the water might be lost due to infiltration or evaporation there is a difference in the outflow volume of the 75 and 100 full scenarios of scrb5 while the difference between the same scenarios increased dramatically for sdnwa 90 the biggest pothole has a capacity of almost 1 6 105 and 8 8 105 m3 within scrb5 and sdnwa 90 respectively in scrb5 there is a 0 9 105 m3 volume difference between the initial stored water in the largest pothole for the 100 and 75 full scenario however for sdnwa 90 the value reaches 4 8 105 m3 volume difference between the two scenarios which explains the great difference between the outflow volumes as there is more available storage within the biggest pothole to reduce the outflow sdnwa 90 is dominated by large potholes with large storage capacities when completely filled caused the outflow volume to change dramatically compared to other scenarios 0 75 full these results show the great effects of the pothole sizes and initial conditions on changing the outflow dramatically and the corresponding water extents and frequency distribution the main idea here was not to model the basins outflow because their outflow observations are not available however we used the simple vertical water balance losses calculations without calibrating some of its parameters because outflows were not of interest and our main interest was to assess prima s novel wrr component in reproducing reasonable water extents we did this to run the model with reasonable fluxes rather than assuming different arbitrary depths to fit the observations although a simple processes representation without calibration was incorporated to represent the fluxes the model showed reasonable to good agreement with the observed water areas exceedance probability fig 5 and table 2 and extents and depths fig 6 for the 75 full scenario if the basin response is of interest the model should be calibrated to accurately simulate the outflows and this should further improve the water extent simulation there were differences between the best simulated scenario 75 full and observed water extents and exceedance probabilities for both test basins these differences may stem from different simplifications assumptions and or used data for instance the initial conditions snow on ground soil moisture filling of potholes were assumed to be uniform over the basins it is known that these values are spatially variable and this assumption might have affected the results for example wind can redistribute snow on ground and results in a heterogeneous snow cover further sublimation and mid winter melt events can reduce the accumulated snow on ground during winter shook et al 2015 these processes affect the amount of snow available for melt on each basin area grid cell and consequently affect the amount of flow to certain potholes however the calculations of these processes or the spatially variable initial conditions required either detailed observation which are not available or detailed physically based model implementation over the basins which is beyond the scope of this work further the dems were collected 3 and 6 years prior to the date the remote sensing data were acquired for scrb5 and sdnwa 90 respectively during that period the artificial drainage might have affected the potholes extents capacity and or connectivity also there was some water when the dems were collected and this might have affected the actual capacity of the depressions despite of the afro mentioned assumptions limitations the reasonable to good agreement between prima s results and the observations suggests that prima s novel wrr is working reasonably well integrating prima with a land surface model should help in better identification of initial conditions and in forcing the model with more accurate fluxes which should result in improved results and more realistic use of prima 3 2 prima vs wdpm 3 2 1 effect of elevation tolerance prima and wdpm required the same number of iterations 2 000 to distribute the added water when using a coarse elevation tolerance more than 100 mm as shown in fig 7 a and consequently the water extents of the coarse elevation tolerances were similar for both models however prima was three times as efficient for the very fine tolerance 1 mm for both the adding and draining tests wdpm was twice as efficient when adding water for the 10 mm tolerance fig 7 b demonstrates that the maximum water depth which occurs at the basin outlet increased for both wdpm and prima as increasingly fine tolerances are used the use of fine tolerances increased the number of iterations required in each run allowing water to be distributed more effectively over the dem the prima runs demonstrated that the maximum water depths increased compared to wdpm indicating that prima was more efficient at redistributing water toward the outlet fig 7 b when the water was drained both wdpm and prima had very similar values for the maximum depth of water on their dems despite the use of very different algorithms the quantity of water retained by the drained dem is essentially the same similar results are shown in fig 7 c which plots the fractional water covered area the fractional water areas were reduced as increasingly fine tolerances were used which was also expected as the use of more iterations would be expected to further concentrate the water in smaller areas in all cases prima produced smaller water areas than did the wdpm which when combined with the greater maximum depths for the prima runs seen in fig 7 b implies that prima concentrates the water more rapidly than does wdpm there was a negligible change in prima s fractional water covered area with tolerances of 10 mm compared to tolerance of 1 mm fig 7 c this shows the efficiency of the prima model in concentrating more water in smaller areas and moving more water downstream the river near the outlet with less number of iterations 10 mm iterations compared to 1 mm iterations fig 7 a as with the maximum water depths the drained water areas produced by prima and wdpm are essentially the same for tolerances of 1 mm further discussion and analysis on the effect of elevation tolerance are provided in the supplemental file section s4 3 2 2 contributing area curves fig 8 shows the contributing area fraction of the basin vs the volumetric fraction of storage for scrb5 and sdnwa as computed by both wdpm and prima the scrb5 curves required the addition of up to 400 mm of water the plots of both models are very similar the greatest difference being that the draining of the final addition of water required more than 4 times as many iterations 1 58 million by wdpm as by prima 0 36 million the sdnwa curves required the addition of up to 500 mm of water the contributing area fraction for both models being essentially identical prima was again more efficient than wdpm requiring 1 6 million iterations as opposed to 6 5 million iterations to drain the final addition of water the shape of the sdnwa curves is very different from the scrb5 curves fig 8 explaining the greater depth of water required to fill the basin and the very large number of iterations required to drain it as described above sdnwa has no permanent drainage system and does not have an obvious outlet the basin outlet shown in fig 3 is the lowest point on the divide and lies above much of the basin the large pond near the outlet also visible in fig 3 acted as a gatekeeper phillips et al 2011 preventing any outflow until it was filled it has been demonstrated that prima gives similar results to wdpm but with a reduced computational cost a detailed comparison of both models performance and a discussion on why prima is more computationally efficient than wdpm is presented in the supplemental file section s4 4 conclusions the prairie region inundation mapping model prima is proposed as a simplified and comprehensive fully distributed hydrological routing model to allow for a more accurate simulation of the complex pothole systems in the prairies prima can simulate the infiltration and evaporation losses movement of surface runoff with travel time calculations pothole storage dynamics the fill and spill mechanism and the spatial extent of the water over the prairie landscape a number of modifications are implemented to develop prima as an improved and computationally efficient ca based surface runoff generation algorithm in the prairies prima showed reasonable to good simulation of the pothole water extents when compared against remote sensing data of water areas with an accuracy of 85 averaged over the two basins the percent bias in simulating the water depth in the potholes was 2 averaged over all available pothole depth records at sdnwa 90 the model showed some overestimation in the inundated areas because of some assumption that were made during the simulation e g uniform initial conditions snow on ground the initial conditions of the potholes have significant effects on changing the outflow volume and the resulting water extents of the potholes when the new river cell approach developed in this study was used for draining water the number of iterations required by prima was reduced by almost 48 times compared to the traditional outlet cell approach supplemental file section s3 overall prima was three to eight times as computationally efficient as wdpm in terms of the number of iterations used to arrive at the final water distribution both wdpm and prima took many hours to run the number of iterations required by the model and their execution time are functions of the applied depth the complexity i e the number size and connectivity of the potholes and the area of the basin the grid resolution of the dem and the specified tolerance s prima runs were performed in a serial mode whereas wdpm runs were performed in a parallel mode as a test of their relative computational costs wdpm was also run in a serial mode on the same machine using a 3 4 ghz intel core i7 processor and 16 gb of ram as was prima for the case of draining 100 mm of added water in this test prima executed 97 000 iterations in 10 247 s 0 105 s iteration whereas wdpm executed 311 000 iterations in 15 337 s 0 049 s iteration prima is more efficient in that it moves more water per iteration but wdpm had approximately half of the computational cost of prima per iteration prima filters out the neighboring cells with water elevation higher than the average of the water elevation of the central cell and the neighboring cells which requires more calculations per iteration compared to wdpm although each prima iteration required more cpu time the total cpu time was reduced by about one third compared to wdpm prima showed potential for simulating the pothole flooding extents using a very small number of iterations 2 000 for a basin with a well developed drainage system and a prominent stream valley due to its efficiency prima can be used for inundation mapping purposes like wdpm but with a reduced computational cost to identify the pothole flooding and associated flood risk which is useful in urban planning and decision making more importantly prima has the potential to be implemented into hydrologic models as a prairie runoff generation algorithm for accurate simulation of the prairie spatiotemporal dynamics and connectivity which can help in better simulation of the prairie hydrology further efforts are needed to parallelize prima code test the effect of dem resolutions on the simulation of the storage dynamics and flooding extents and to test the applicability of integrating prima into a land surface model data availability the authors do not have the permission to share the used data in this paper e g lidar dems and remote sensing data as they were provided by a third party declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the authors would like to thank kevin shook for providing the lidar dem remote sensing data and wdpm results for the two study basins and for providing insights and technical support the financial support of the natural sciences and engineering research council of canada nserc strategic network through the canadian floodnet research network is acknowledged grant number netgp451456 13 the funding of the department of civil geological and environmental engineering university of saskatchewan devolved scholarship is greatly appreciated compute canada is also acknowledged for providing the computing facilities to run prima wdpm was downloaded from https www usask ca hydrology wdpm php appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104850 
25973,the canadian prairies are dominated by numerous depressions which can modify the lateral transfer of water to prairie streams few studies were conducted to simulate the pothole dynamics using their actual spatial distributions this study proposes a computationally efficient prairie region inundation mapping prima model as a hydrologic routing model for a more accurate and comprehensive storage dynamics simulation and inundation mapping in the prairies prima shows potential for simulating the potholes extents when comparing its results with remote sensing data of pothole areas with an accuracy of 85 averaged over two prairie basins in saskatchewan canada prima is three eight times as computationally efficient as the recently developed wetland dem ponding model wdpm due to its computational efficiency and ability to provide a good simulation of inundation extents prima shows strengths as a possible tool for pothole inundation mapping and storage dynamics simulations graphical abstract image 1 keywords prairie pothole inundation mapping distributed modelling hysteresis contributing area remote sensing 1 introduction the north american prairies are characterized by numerous depressions anteau et al 2016 zhang et al 2009 known as wetlands sloughs prairie potholes puddles geographically isolated wetlands giws and dugouts due to the existence of these potholes the runoff production in prairies follows a fill and spill mechanism shaw et al 2012 wherein each pothole contributes flow to downstream potholes after being filled therefore the majority of the prairies are designated as being non contributing wherein the surface water runs off into isolated or internally drained basins the derivation of the non contributing area map over the prairies was quite subjective and was derived from the visual interpretation of topographic contour maps shaw et al 2013 more details on the non contributing area map and its derivation are provided in the supplemental file section s1 potholes can be connected by surface or subsurface flow of water however this connection differs dramatically in length and time the subsurface connection between potholes is slow and can improve the quality of water in the basin whereas surface connection is fast and limited to significant precipitation events ameli and creed 2017 simulating the hydrological behavior of pothole dominated landscapes is challenging because of the difficulty in characterizing the fill and spill mechanism which leads to a hysteretic relationship between the basin storage and the contributing area shook and pomeroy 2011 thus the prairies are often referred to as the graveyard of hydrological models the prairie potholes complexities can be simulated using conceptual approaches e g mekonnen et al 2014 or satellite dem imageries based approaches e g chu et al 2013 shaw et al 2013 2012 shook et al 2013 shook and pomeroy 2011 muhammad et al 2019 mekonnen et al 2014 introduced the conceptual pdmrof probability distribution model based runoff generation approach which assumes that runoff is a function of the basin storage capacity in pdmrof the capacity of different potholes in the basin is assumed to follows a pareto distribution and the runoff from potholes is calculated by integrating the probability density function the pdmrof concept showed potential to simulate the prairie streamflow dynamics when implemented into different models such as mesh mekonnen et al 2014 and hbv ahmed et al 2020 however as a conceptual approximation the pdmrof cannot represent the spatial extents of pothole water storage and it might be difficult to map its parameters to field measurements land cover data classified from satellite imageries can be used to identify the potholes evenson et al 2016 muhammad et al 2019 then each of the identified potholes can be simulated using a separate reservoir that contributes surface flow to the downstream area after exceeding its maximum capacity this concept of representing potholes as separate reservoirs was used in the conceptual pothole cascading model pcm shook et al 2013 in pcm the properties of the potholes are obtained from dems and a small number of potholes is used to represent all potholes in the basin the methodologies that use separate reservoirs to represent the potholes do not represent the fill merge split processes of the potholes and some of them may not represent the spatial distribution of water on the landscape dems can be used to delineate depressions in the basin for example the puddle delineation pd algorithm chu et al 2010 was proposed to delineate the landscape and it differentiates the landscape into pothole and non pothole areas the topographic characteristics of the potholes cascading order surface area and storage and flow direction for the non pothole area are obtained from dems the output of the pd algorithm can be used by hydrologic models to route flows and simulate the fill spill mechanism of the prairies e g puddle to puddle p2p model chu et al 2013 and swat nasab et al 2017 the wetland dem ponding model wdpm is a dem based model that distributes water on the landscape shook et al 2013 shook and pomeroy 2011 wdpm was the first explicit method able to simulate the spatial distribution of water on the prairies and it was used to simulate the contributing area however wdpm was found to be computationally expensive requiring thousands of iterations to reach the steady state solution converge despite not using conventional flow equations to transfer water dem based hydraulic models such as mike she dhi 1998 and the recently developed hec ras model hydrologic engineering center 2016 could be used to simulate the fill and spill phenomena in the prairies however these models use the saint venant equation to simulate the movement of water requiring numerical solutions of the differential equations which is computationally expensive there is a need for a computationally efficient model that can simulate the pothole storage dynamics using their actual spatial distributions this type of model could be easily adapted to simulate the impact of the potholes on the system response in prairie basins and for inundation mapping and or flood risk assessments in the prairie pothole region the main objective of this study is to develop a novel computationally efficient prairie region inundation mapping prima model and test its applicability as a fully distributed simplified hydrological routing model to simulate the spatiotemporal surface water movement and storage dynamics over the landscape prima is based on the cellular automata ca wolfram 1984 approach as a novel method for simulating filling spilling and merging splitting processes of the potholes and calculating the amount and direction of flow over the landscape prima is based on five necessary modifications of the original ca approach implemented to improve the computational efficiency and ensure the model is mimicking the behavior of the pothole systems 2 material and methods in this section the proposed model prima the study area and the validation of the model against remote sensing data of pothole area are fully described further the study consists of models simulations to compare prima s computational efficiency and performance against wdpm as a reference in terms of simulating the complex response of the pothole dominated watersheds and the spatial extents of water over the landscape a detailed description of the methodology is given below 2 1 prairie region inundation mapping prima model 2 1 1 water redistribution and routing wrr component the novel water redistribution and routing wrr component in prima is based on the ca approach which has been used for simulating the movement of water dimitriadis et al 2016 li et al 2013 liu et al 2015 parsons and fonstad 2007 ca based models can replace hydraulics differential equations with a set of rules which are hydrological simplifications of the saint venant and or manning s equations bates et al 2010 to represent the surface water movements di gregorio and serra 1999 wolfram 1984 the simplicity of the ca models makes them more computationally efficient than other hydraulic models the wrr component in prima is based on five modifications introduced in this study see the supplemental file section s2 to the ca model liu et al 2015 this component moves water sequentially between dem cells following the topography the wrr component is a combination of the minimization algorithm di gregorio and serra 1999 and manning s equation to determine the amount and timing of flow leaving a central cell to its eight neighboring cells respectively the minimization algorithm attempts to minimize the difference in water surface elevation between contiguous cells a hypothetical example and a flowchart of the wrr component which is iterative and applied to each cell in the dem is presented in fig 1 the following rules apply 1 using the water elevation of the current central cell wel 0 and the surrounding cells wel i i 1 8 calculate the average water elevation av m by eq 1 1 a v w e l 0 i 1 n w e l i n 1 where n is the number of neighboring cells involved in the calculation of the water redistribution 2 eliminate those cells having water elevations greater than the average water elevation i e where wel i av 3 recalculate the average water level for the remaining cells as in step 1 and apply the elimination rule in step 2 4 apply step 3 until no more cells can be eliminated from the calculations 5 distribute the outflow from the current cell to the remaining neighboring cells such that all of them have the same water elevation av 6 the travel time is calculated as the quotient of the grid cell size divided by its water velocity from manning s equation the velocity v m sec of water is calculated based on the fraction of water leaving the current cell to its lowest water elevation neighboring cell in the dem and is assigned to the current cell assuming a wide cross section as 2 v d 2 3 s n where d is the maximum outflow depth from the current cell to its neighboring cells δ 3 4 in the given example in fig 1 m s is the surface water slope m m and n is manning s roughness coefficient unitless after applying the wrr component to all cells in the dem the following steps are applied 1 the minimum travel time for all grid cells is assigned as prima s global time step to maintain the simulation stability and to ensure that the water does not cross more than one cell during a single time interval 2 water reaching the outlet cells is removed from the dem and stored as outflow volume 2 1 2 infiltration and evaporation losses component simple vertical water budget calculations were implemented in prima to allow for simulating the spatiotemporal variation of the water extent and the comparison against remote sensing observations the vertical water budget infiltration and evaporation processes were represented in prima using a simple bucket type approach ahmed et al 2020 the infiltration to and evapotranspiration from the soil are functions of the soil moisture storage the evaporation from the potholes is a function of the mean monthly temperature and potential evapotranspiration a simple degree day approach was also implemented to allow for distinguishing rainfall and snowfall and calculate the snowmelt rates the rainfall snowmelt determined by the degree day approach was added to the ponded water on each grid cell then the amount of infiltration and evaporation were calculated and subtracted from the ponded water if the grid cell does not have ponded water i e dry cell the calculated evapotranspiration is subtracted from the soil moisture storage of the cell after applying the vertical water budget calculations the remaining ponded water is redistributed over the landscape using the wrr component section 2 1 1 it is important to note that prima does not allow for horizontal transfer of water in the sub surface system in summary prima loops through the dem cells from the highest to the lowest elevation to simulate the water movement from uplands to lowlands and to reduce the required number of iterations a flowchart of prima is presented in fig 2 each run of prima includes the following steps 1 the dem cells are sorted by elevation from highest to lowest 2 excess water depth provided as an arbitrary value or calculated by the losses component is added to or removed from the dem 3 the program iterates over each dem cell in order of elevation the amount of water exchanged and water velocity are calculated for each grid cell current cell and its neighboring cells using the wrr component 4 the model s global time step is calculated as the minimum travel time among all cells 5 water reaching the outlet cell s is drained removed from the dem and stored as outflow volume 6 the model checks if i the cumulative global time step is greater than the specific forcing simulation resolution e g hourly or daily ii the water depth change is smaller than a user predefined elevation tolerance the depth change is the maximum change in water elevation over all cells calculated every n iteration e g 1000 ii the outflow volume change is less than a user predefined volume tolerance the volume change is calculated as the change in the cumulative outflow volume every n iteration 7 if any of the conditions above in step 6 is met the model run terminates otherwise the model re iterates over the dem cells i e repeats step 3 to step 7 step 2 to step 7 are repeated for every addition and or removal of water depth and volume change are error measurements used to terminate the run because the model may take thousands of iterations to make negligible changes in the water surface elevation we choose to calculate depth and volume change every n iteration interval to ensure that the model reached the steady state solution and it was not trapped in a local optima solution prima is a flexible model wherein any component process can be activated or deactivated fig 2 as an example it can allow for the redistribution of water over the landscape without allowing the water to leave through outlet cells the concept behind the modifications supplemental file section s2 is to reduce the running time of prima by draining water from multiple outlet river cells supplemental file section s3 and to allow for travel time calculations so that it can be implemented in the future into a hydrological land surface model as a runoff generation algorithm in terms of input and output data prima requires the topographic data dem outlet cell s location elevation and volume tolerance and the excess water depths as either uniform or spatially variable to be distributed over the landscape as inputs the excess water depths can be provided as arbitrary depths or calculated by the losses component if the losses component is used the model requires precipitation and temperature as input forcings a preliminary run of prima can help in identifying possible outflow cells and reasonable tolerance values prima generates water depth raster value of state variables soil moisture snowmelt snow water equivalent etc outflow volume rate and run summary number of iterations and execution time as outputs prima does not do any pre processing to identify depressions or flat areas in the dem the model uses the wrr component to determine if water is trapped in pothole cells or flat area cells 2 2 study area and data in order to fully evaluate prima it was important to test it in areas where dem at high resolution and remote sensing data of the observed water areas were readily available and the fill and spill response is well understood and characterized thus smith creek research basin subbasin 5 scrb5 and saint denis national wildlife area sdnwa in saskatchewan canada fig 3 were selected for this study because of an extensive history of studies in the region fang et al 2010 mengistu and spence 2016 shaw et al 2012 shook and pomeroy 2011 van der kamp et al 2003 the basins are useful for testing the behavior of the models because they represent two extremes within the variety of topography in the prairie ecozone scrb5 with an area of approximately 11 km2 is relatively flat slopes of 2 5 and has a well developed stream with a prominent valley on the other hand sdnwa is hummocky slopes of 10 15 has no defined drainage system nor an obvious outlet fig 3 and has an area of approximately 22 km2 both basins have more than 1000 potholes with areas larger than 100 m2 however sdnwa is dominated by large potholes ponds area 10 000 m2 that are scattered over the landscape and occupy almost one third of the basin area the dominant land cover on both basins is cropland the simulations were performed using available lidar based dems for both basins the scrb5 lidar dem has a horizontal resolution of 5 m and were collected between october 14to october 16 2008 shook and pomeroy 2011 the sdnwa lidar dem was collected on the august 9 2005 with 5 m horizontal resolutions shook et al 2013 there was some water in the potholes when the lidar data were collected at each basin and hence all modelling and simulations were done relative to the initial conditions of water elevation the dems were not conditioned to account for the existing culverts in the study areas the observed water extents areas were identified from remote sensing data rapideye satellite imageries that are available for scrb5 and the area above pond 90 within sdnwa sdnwa 90 fig 3 for the 2011 spring snowmelt period the images have a horizontal resolution of 5 m and were captured on may 13 2011 and may 18 2011 for sdnwa 90 and scrb5 respectively shook et al 2013 water depth observations at different potholes are available at sdnwa 90 bam et al 2018 but they are intermittent and thus the observation that were available within the 2011 snowmelt period were used fourteen different potholes were found to have one recorded water depth during that period 13 of the measurements were available on may 12 2011 and the remaining one on may 13 2011 the locations of the measurements are plotted in fig 3 both the observed water areas and depths were used to assess prima s performance in simulating the complex potholes extents dynamics and storage 2 3 simulating the extents of surface water areas by prima the simulation period of prima was set from april 1 2011 to the date the image was captured for each of the two studied basins may 13 2011 and may 18 2011 for sdnwa 90 and scrb5 respectively to simulate the spring snowmelt event the model used the gridded canadian precipitation analysis capa product lespinas et al 2015 and the global environmental multiscale gem atmospheric model mailhot et al 2006 output as the respective precipitation and temperature forcing on a daily time scale fig 4 the forcing was spatially uniform over the basins because each basin was located inside one pixel of the gem capa data the soil moisture and the potholes almost reached their storage capacity for the studied areas prior to the 2011 flood event mengistu and spence 2016 shook et al 2013 thus the initial soil moisture storage was assumed to be close to the water holding capacity however we assumed different scenarios for the initial filling conditions of the potholes assuming that all potholes are 0 25 50 75 and 100 full to test the effect of the pothole conditions on changing the outflow of the basin and the resulting water extents the 100 full condition of the potholes was obtained by adding a significant water depth to the landscape and then redistribute that water using prima s wrr component until all depressions are filled then using qgis software the capacity of individual potholes was identified for each scenario the depth of stored water in individual potholes was obtained by multiplying its capacity by the fraction of filling i e 0 25 for 25 full scenario in this test all components of prima were used i e water redistribution and losses the excess water depths calculated by the losses component for each day were redistributed over the landscape and drained from the outlet cells the accumulated precipitation during fall and winter was used as initial accumulated snow on ground a summary of prima s parameters and their values are presented in table 1 the parameters were determined from the literature and available landcover data or were set to their default values according to ahmed et al 2020 the parameters in table 1 were not calibrated to simulate the observed water areas the exceedance probabilities and the spatial distribution of the water areas at the end of the simulation were compared to that of the observed water areas for both basins the average of absolute deviations was used as a goodness of fit measurement to assess the accuracy of prima s exceedance probabilities of water areas two performance metrics were used to further validate prima s spatial water extents against remote sensing data sensitivity s v and specificity s c s v and s c quantify the probability of correctly predicting a grid cell within the basin as inundated or non inundated respectively bharath and elshorbagy 2018 and are defined as 3 s v f c f c f o c 4 s c n f c n f c n f o c where f c is the total number of observed inundated cells that were correctly predicted as inundated by the model f oc is the total number of observed inundated cells that were falsely predicted as non inundated by the model nf c is the total number of observed non inundated cells that were correctly predicted as non inundated by the model and nf oc is the total number of observed non inundated cells that were falsely predicted as inundated by the model both sv and s c range from 0 to 1 with values closer to 1 demonstrating high probability of accurately predicting inundated and non inundated areas respectively also the error in simulating the water depth in the 14 identified potholes over sdnwa 90 was assessed 2 4 experimental setup prima vs wdpm it was important to evaluate the computational efficiency and the resulting water extents of prima against another simple hydraulic model wdpm that was proven to be successful in redistribution of water over the complex prairie landscape shook et al 2013 shook and pomeroy 2011 wdpm iteratively redistributes excess water over a dem using the method of shapiro and westervelt 1992 in which the water is redistributed from a central cell to its eight neighboring cells with each cell taking 1 8 of the water depth difference between itself and the central cell wdpm does not calculate water velocities or travel time all water is assumed to flow instantaneously based on the 1 8 water depth difference rule per iteration wdpm was used as a reference to further assess the performance and results of the proposed prima model for this section and for the sake of comparing the performance of prima to wdpm the losses component and the travel time calculations were not used only the wrr component in prima was used and the water was drained from the outlet cell until both models converged reached the steady state solution the models were tested by applying arbitrary depths of water to the dem and redistribute them without draining the excess water which is referred to as add test after redistribution of water the excess water was drained from the basin outlet and this test is referred to as drain test this was implemented because wdpm can either add water or drain excess water unlike prima that can redistribute and drain the water at the same time there was no attempt to account for groundwater contribution to the outlet the performance of prima and wdpm were assessed relative to the number of iterations required for convergence because the models codes are quite different wdpm was written in c for parallel processing whereas prima was written in fortran 95 for serial processing the term efficiency in the following discussions refers to the number of iterations required to achieve a model state 2 4 1 effect of elevation tolerance on the water distribution of prima and wdpm models the models sensitivity to changing the elevation tolerance was tested on scrb5 scrb5 was selected to test the effect of changing the models tolerances on the produced water extents for both the pothole areas and the riverbanks the models were tested for 1 the addition and 2 draining of water the addition tests were carried out at scrb5 by adding an arbitrary depth of water 100 mm to the empty dem and redistributing it until each model converged for elevation tolerances of 1000 500 100 10 and 1 mm because the models results might be different we used the final water distribution of prima with 1 mm tolerance as an initial state for the drainage test for both models this was conducted to test the agreement between both models results for the same initial condition and water distribution over the landscape both models drained the excess water from the landscape with 1 mm and 1 m3 as the respective elevation and volume tolerances the number of iterations and the final spatial distribution of the water over the landscape for the add and drain tests were compared 2 4 2 simulating the contributing area curves by prima and wdpm contributing area fraction curves were generated for both basins using both models the curves represent the envelope of the relationship between the basin s contributing area fraction and the storage of water the curves were constructed by repeatedly adding water to an initially empty dem until all depressions are completely filled for a fine elevation and volume tolerance 1 mm and 1 m3 following each addition of water the basin was drained for both test areas then an incremental water depth of 1 mm was added and the basin was drained again the contributing area fraction is calculated as the fraction of the outflow volume corresponding to the added 1 mm 3 results and discussion 3 1 suitability of prima for the prairies the exceedance probability of the observed and the simulated water areas for different pothole initial filling conditions for both basins are shown in fig 5 for scrb5 the exceedance probability of the near full scenarios 75 and 100 pothole full showed good agreement with the exceedance probability of the observed water areas for sdnwa 90 the exceedance probabilities of the near full scenarios were almost similar and showed reasonable agreement to that of the observed water areas in terms of the goodness of fit statistic table 2 the near full scenarios showed the smallest error among all scenarios with the 75 scenario being slightly better than the 100 full scenario the near full scenarios showed the best optimal combination of predicting inundated s v and non inundated areas s c over the two basins table 2 although the water extents of the near full scenarios were quite similar in each of the basins the 100 full scenario tended to slightly overestimate the inundated areas when compared to the 75 full scenario s c table 2 the 75 full scenario showed the best performance in predicting both the inundated s v and non inundated s c areas in both basins with values of 0 85 and 0 88 respectively table 2 averaged over the two basins this agrees with the literature about the conditions of the 2011 flood event as the potholes were almost full prior to the snowmelt event mengistu and spence 2016 shook et al 2013 the remaining scenarios 0 25 and 50 showed underestimation of the water areas especially for the larger potholes fig 5 and s v table 2 the actual water extents of the observed water areas and prima s simulated water areas at the end of the simulation period for the 75 full scenario best simulation are shown in fig 6 for both basins prima showed good agreement with the observed water areas extents especially for the larger potholes and the upstream portion of the main river at scrb5 for sdnwa 90 prima s water extents showed good agreement with the observed large potholes however there were some over estimation of the ponded area in the central and northeastern parts of the basin fig 6 with difference between simulated and observed inundated extents in that area of 0 15 km2 the percent observed and simulated ponded area are 8 and 15 respectively at sdnwa 90 the model predicted potholes in central and northeastern part of the basin as inundated that were not observed as inundated by the remote sensing data this overestimation caused some disagreement between the observed and simulated areas exceedance probabilities fig 5 sdnwa 90 in terms of the simulated water depth in the potholes the average percent bias for 14 potholes at sdnwa 90 was found to be 2 and the max absolute error was 23 for the 75 full scenario fig 6 right panel the average total outflow volume of the below 50 full scenarios 0 25 and 50 was 43 000 m3 and was smaller than that of the 75 and 100 scenarios almost 56 500 and 191 600 m3 respectively for scrb5 similarly for sdnwa 90 the average total outflow volume of the below 50 full scenarios 0 25 and 50 was 350 m3 and was significantly smaller than that of 75 and 100 full scenarios almost 8 000 and 88 500 m3 respectively in the below 50 full scenarios the potholes did not reach their capacity especially the larger ones and hence most of the water is being stored with little runoff reaching the outlet however for the 100 full scenarios the majority of the surface runoff can reach the outlet as all potholes are almost full and the water might be lost due to infiltration or evaporation there is a difference in the outflow volume of the 75 and 100 full scenarios of scrb5 while the difference between the same scenarios increased dramatically for sdnwa 90 the biggest pothole has a capacity of almost 1 6 105 and 8 8 105 m3 within scrb5 and sdnwa 90 respectively in scrb5 there is a 0 9 105 m3 volume difference between the initial stored water in the largest pothole for the 100 and 75 full scenario however for sdnwa 90 the value reaches 4 8 105 m3 volume difference between the two scenarios which explains the great difference between the outflow volumes as there is more available storage within the biggest pothole to reduce the outflow sdnwa 90 is dominated by large potholes with large storage capacities when completely filled caused the outflow volume to change dramatically compared to other scenarios 0 75 full these results show the great effects of the pothole sizes and initial conditions on changing the outflow dramatically and the corresponding water extents and frequency distribution the main idea here was not to model the basins outflow because their outflow observations are not available however we used the simple vertical water balance losses calculations without calibrating some of its parameters because outflows were not of interest and our main interest was to assess prima s novel wrr component in reproducing reasonable water extents we did this to run the model with reasonable fluxes rather than assuming different arbitrary depths to fit the observations although a simple processes representation without calibration was incorporated to represent the fluxes the model showed reasonable to good agreement with the observed water areas exceedance probability fig 5 and table 2 and extents and depths fig 6 for the 75 full scenario if the basin response is of interest the model should be calibrated to accurately simulate the outflows and this should further improve the water extent simulation there were differences between the best simulated scenario 75 full and observed water extents and exceedance probabilities for both test basins these differences may stem from different simplifications assumptions and or used data for instance the initial conditions snow on ground soil moisture filling of potholes were assumed to be uniform over the basins it is known that these values are spatially variable and this assumption might have affected the results for example wind can redistribute snow on ground and results in a heterogeneous snow cover further sublimation and mid winter melt events can reduce the accumulated snow on ground during winter shook et al 2015 these processes affect the amount of snow available for melt on each basin area grid cell and consequently affect the amount of flow to certain potholes however the calculations of these processes or the spatially variable initial conditions required either detailed observation which are not available or detailed physically based model implementation over the basins which is beyond the scope of this work further the dems were collected 3 and 6 years prior to the date the remote sensing data were acquired for scrb5 and sdnwa 90 respectively during that period the artificial drainage might have affected the potholes extents capacity and or connectivity also there was some water when the dems were collected and this might have affected the actual capacity of the depressions despite of the afro mentioned assumptions limitations the reasonable to good agreement between prima s results and the observations suggests that prima s novel wrr is working reasonably well integrating prima with a land surface model should help in better identification of initial conditions and in forcing the model with more accurate fluxes which should result in improved results and more realistic use of prima 3 2 prima vs wdpm 3 2 1 effect of elevation tolerance prima and wdpm required the same number of iterations 2 000 to distribute the added water when using a coarse elevation tolerance more than 100 mm as shown in fig 7 a and consequently the water extents of the coarse elevation tolerances were similar for both models however prima was three times as efficient for the very fine tolerance 1 mm for both the adding and draining tests wdpm was twice as efficient when adding water for the 10 mm tolerance fig 7 b demonstrates that the maximum water depth which occurs at the basin outlet increased for both wdpm and prima as increasingly fine tolerances are used the use of fine tolerances increased the number of iterations required in each run allowing water to be distributed more effectively over the dem the prima runs demonstrated that the maximum water depths increased compared to wdpm indicating that prima was more efficient at redistributing water toward the outlet fig 7 b when the water was drained both wdpm and prima had very similar values for the maximum depth of water on their dems despite the use of very different algorithms the quantity of water retained by the drained dem is essentially the same similar results are shown in fig 7 c which plots the fractional water covered area the fractional water areas were reduced as increasingly fine tolerances were used which was also expected as the use of more iterations would be expected to further concentrate the water in smaller areas in all cases prima produced smaller water areas than did the wdpm which when combined with the greater maximum depths for the prima runs seen in fig 7 b implies that prima concentrates the water more rapidly than does wdpm there was a negligible change in prima s fractional water covered area with tolerances of 10 mm compared to tolerance of 1 mm fig 7 c this shows the efficiency of the prima model in concentrating more water in smaller areas and moving more water downstream the river near the outlet with less number of iterations 10 mm iterations compared to 1 mm iterations fig 7 a as with the maximum water depths the drained water areas produced by prima and wdpm are essentially the same for tolerances of 1 mm further discussion and analysis on the effect of elevation tolerance are provided in the supplemental file section s4 3 2 2 contributing area curves fig 8 shows the contributing area fraction of the basin vs the volumetric fraction of storage for scrb5 and sdnwa as computed by both wdpm and prima the scrb5 curves required the addition of up to 400 mm of water the plots of both models are very similar the greatest difference being that the draining of the final addition of water required more than 4 times as many iterations 1 58 million by wdpm as by prima 0 36 million the sdnwa curves required the addition of up to 500 mm of water the contributing area fraction for both models being essentially identical prima was again more efficient than wdpm requiring 1 6 million iterations as opposed to 6 5 million iterations to drain the final addition of water the shape of the sdnwa curves is very different from the scrb5 curves fig 8 explaining the greater depth of water required to fill the basin and the very large number of iterations required to drain it as described above sdnwa has no permanent drainage system and does not have an obvious outlet the basin outlet shown in fig 3 is the lowest point on the divide and lies above much of the basin the large pond near the outlet also visible in fig 3 acted as a gatekeeper phillips et al 2011 preventing any outflow until it was filled it has been demonstrated that prima gives similar results to wdpm but with a reduced computational cost a detailed comparison of both models performance and a discussion on why prima is more computationally efficient than wdpm is presented in the supplemental file section s4 4 conclusions the prairie region inundation mapping model prima is proposed as a simplified and comprehensive fully distributed hydrological routing model to allow for a more accurate simulation of the complex pothole systems in the prairies prima can simulate the infiltration and evaporation losses movement of surface runoff with travel time calculations pothole storage dynamics the fill and spill mechanism and the spatial extent of the water over the prairie landscape a number of modifications are implemented to develop prima as an improved and computationally efficient ca based surface runoff generation algorithm in the prairies prima showed reasonable to good simulation of the pothole water extents when compared against remote sensing data of water areas with an accuracy of 85 averaged over the two basins the percent bias in simulating the water depth in the potholes was 2 averaged over all available pothole depth records at sdnwa 90 the model showed some overestimation in the inundated areas because of some assumption that were made during the simulation e g uniform initial conditions snow on ground the initial conditions of the potholes have significant effects on changing the outflow volume and the resulting water extents of the potholes when the new river cell approach developed in this study was used for draining water the number of iterations required by prima was reduced by almost 48 times compared to the traditional outlet cell approach supplemental file section s3 overall prima was three to eight times as computationally efficient as wdpm in terms of the number of iterations used to arrive at the final water distribution both wdpm and prima took many hours to run the number of iterations required by the model and their execution time are functions of the applied depth the complexity i e the number size and connectivity of the potholes and the area of the basin the grid resolution of the dem and the specified tolerance s prima runs were performed in a serial mode whereas wdpm runs were performed in a parallel mode as a test of their relative computational costs wdpm was also run in a serial mode on the same machine using a 3 4 ghz intel core i7 processor and 16 gb of ram as was prima for the case of draining 100 mm of added water in this test prima executed 97 000 iterations in 10 247 s 0 105 s iteration whereas wdpm executed 311 000 iterations in 15 337 s 0 049 s iteration prima is more efficient in that it moves more water per iteration but wdpm had approximately half of the computational cost of prima per iteration prima filters out the neighboring cells with water elevation higher than the average of the water elevation of the central cell and the neighboring cells which requires more calculations per iteration compared to wdpm although each prima iteration required more cpu time the total cpu time was reduced by about one third compared to wdpm prima showed potential for simulating the pothole flooding extents using a very small number of iterations 2 000 for a basin with a well developed drainage system and a prominent stream valley due to its efficiency prima can be used for inundation mapping purposes like wdpm but with a reduced computational cost to identify the pothole flooding and associated flood risk which is useful in urban planning and decision making more importantly prima has the potential to be implemented into hydrologic models as a prairie runoff generation algorithm for accurate simulation of the prairie spatiotemporal dynamics and connectivity which can help in better simulation of the prairie hydrology further efforts are needed to parallelize prima code test the effect of dem resolutions on the simulation of the storage dynamics and flooding extents and to test the applicability of integrating prima into a land surface model data availability the authors do not have the permission to share the used data in this paper e g lidar dems and remote sensing data as they were provided by a third party declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment the authors would like to thank kevin shook for providing the lidar dem remote sensing data and wdpm results for the two study basins and for providing insights and technical support the financial support of the natural sciences and engineering research council of canada nserc strategic network through the canadian floodnet research network is acknowledged grant number netgp451456 13 the funding of the department of civil geological and environmental engineering university of saskatchewan devolved scholarship is greatly appreciated compute canada is also acknowledged for providing the computing facilities to run prima wdpm was downloaded from https www usask ca hydrology wdpm php appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104850 
25974,with the advancement of web technologies the service oriented approach has become increasingly popular to share and couple environmental models interoperability is critical to the access and integration of web services the ogc web processing service wps defines an interoperable protocol for process execution on the web however it has limitations in supporting time step calculations that are usually required by complex models e g time marching models the open modeling interface openmi standard brings about interoperability for time step based interactions between modeling components this paper introduces an approach to sharing environmental models on the web by coupling openmi and wps key issues including maintaining state data representation model description and model execution are considered in our approach an open source software is developed to support the deployment of the environment models in an interoperable way topmodel and swmm5 use cases illustrate the applicability of the approach and implementation keywords environmental models web service ogc wps openmi 1 introduction integrated environmental modeling iem provides a holistic approach to understand the environmental systems argent 2004 jakeman and letcher 2003 laniak et al 2013 lerner et al 2011 which requires an integration of models across disciplinary and organizational boundaries a great challenge facing the iem community is how to make heterogeneous models easily connected and interoperable goodall et al 2011 laniak et al 2013 qiao et al 2019 there are mainly two kinds of modeling frameworks for iem namely component based and service oriented frameworks gao et al 2019 the former focuses on local computer environments while the latter enables the integration of distributed environmental models component based modeling has been investigated extensively in traditional iem tools granell et al 2013 many interface standards and modeling frameworks are already available such as the open modeling interface openmi gregersen et al 2007 the earth system modeling framework esmf hill et al 2004 and community surface dynamics modeling system csdms peckham et al 2013 among them the openmi standard referred to as openmi in this paper is widely studied and used to couple diverse models laniak et al 2013 it provides well defined interfaces that enable the runtime exchange of data between modeling components moore and tindall 2005 which has been published as an ogc standard vanecek and roger 2014 the component based frameworks often run models following multiple simulation phases and basing on time step computations goodall et al 2011 gregersen et al 2007 peckham et al 2013 which has its advantage in having concrete and community wide tools gao et al 2019 however the component based modeling frameworks often require the same programming language and work in local computer environments castronova et al 2013a gao et al 2019 the service oriented approach has great potential to address the issue of accessibility and interoperability for environmental models in distributed environments li et al 2017 mcdonald et al 2019 nativi et al 2013 rajib et al 2016 sun et al 2020 wen et al 2013 it allows various models accessible layered on different programming languages and platforms belete et al 2017 goodall et al 2011 granell et al 2010 and coupled over the internet interoperability is critical to the access and integration of web services in addition to standard interfaces domain specific descriptions also have been recognized as an integral part of web services jiang et al 2017 yue et al 2016 zhang et al 2019 and predefined data structures are significant for the communication between service providers and consumers the open geospatial consortium ogc web processing service wps specifies the general service interfaces to execute computing processes and retrieve metadata describing their purpose and functionality müller and pross 2015 recently there has been increasing interest in applying wps to share environmental models on the web qiao et al 2019 sun et al 2019 zhang et al 2018 time marching models perform a numerical time stepping procedure to obtain the time varying behavior of the models for example a runoff simulation model may march daily by recalculating model equations using a time step of one day coupled time marching models often exchange data at simulation runtime e g a daily runoff simulation model takes the daily precipitation and evapotranspiration as input during the simulation this characteristic requires the services to hold states and be accessed based on time steps castronova et al 2013a while the wps requires receiving all the necessary inputs before executing a specific process and does not provide appropriate interfaces for exchanging data dynamically most of the current wps solutions to share environmental models cannot support complex interactions castronova et al 2013a addressed some issues to support time dependent calculations using ogc wps in a rest representational state transfer pattern they used the http put method to send and store data for the server side models based on time steps and used the http delete method to remove the files associated with an individual simulation to maintain state intermediate results from previous time steps calculations were serialized for subsequent calculations however challenges still remain and some points can be improved such as providing a structured way for encapsulating complex data and a standardized mechanism for model initialization and finishing castronova et al 2013a this paper introduces an approach to interoperable web sharing of environmental models by coupling openmi and wps which can help address these challenges a detailed comparison of their approach and ours is provided in the discussion section openmi provides a widely accepted method to exchange data based on time steps in the iem community a service oriented approach that follows the same logic can ensure high functionality and retain the existing user habits ogc wps is a mature standard for sharing computing processes on the web in the geospatial domain it already offers the solutions to invoke and retrieve metadata of the processes in an interoperable way which is a good starting point to share environmental models key considerations during coupling the two standards are presented including maintaining state data representation model description and model execution we develop an open source software to facilitate the deployment of the environmental models following the proposed method the topography based hydrological model and swmm5 use cases demonstrate the applicability of the approach and implementation the remainder of this paper is organized as follows section 2 and section 3 introduce the design and results of the proposed method section 4 provides the implementations that support the extended wps protocol section 5 discusses the proposed method conclusions and pointers to future work are described in section 6 2 design the proposed approach integrates openmi into ogc wps to support time marching models findable accessible interoperable and reusable in distributed environments key issues including state maintenance data representation model description and model execution are considered 2 1 maintaining state in the web environment it is required to maintain a connection between a specific client and the model instance that it is invoking the client needs to identify the simulation session and drive the model calculation based on time steps in the wps conceptual model a wps server will instantiate a processing job i e process instance when receiving an execution request in the case of asynchronous execution mode the client can control and monitor the processing job with a job id müller and pross 2015 this mechanism allows a job object to save calculations from previous time steps on the server an additional control option is required to perform subsequent calculations when the server receives input data for the next step calculation in this way individual client server interactions can be stateful by extending the job control options the client can retrieve the status information of the model instance using the wps operation named getstatus wps specification also defines a basic status set to communicate the status of a server side job to the client müller and pross 2015 including succeeded failed accepted and running however it requires more states to indicate what the environmental models are currently doing on the server the openmi explicitly specifies the possible states that may occur in the lifecycle of openmi components created indicates that linkable component instance has just been created this status is followed by initializing that means the linkable component is initializing itself which ends in an initialized status then the component updates its calculations when receiving time dependent input data it comes updated state after a step computation is finished successfully and will wait for the input data for next step calculation after the time step computation is over the status changes to done finished indicates that the component has been successfully finalized the status will change into failed when an error occurs during the initialization preparation updating and finishing processes table 1 shows the reused states and their semantics in the wps context in openmi waitingfordata indicates that the component is waiting for input data from other components to update itself which is an implicit state in our design 2 2 data representation service oriented frameworks often require data communication between client applications and web resources it is crucial to encapsulate data into a standard format understood by both sides iargument and ibaseexchangeitem are core interfaces that represent data in openmi the former represents data used to initialize the component and the latter represents the data used to exchange at runtime the primary exchange item ibaseexchangeitem includes a value definition that represents what it exchanges the extended interface itimespaceexchangeitem also specifies where and when the value applies which includes three dimensions value what time when and spatial where definition wps defines three common data types for process input and output data a complexdata such as gml or geo referenced imagery b literaldata a value with an optional unit c boundingboxdata defined as a minimum bounding rectangle in geographic coordinates wps specification defines two distinct ways for data exchange between wps client and server by reference and by value müller and pross 2015 traditionally small or atomic data such as doubles or strings are transmitted by value large data are usually submitted by reference a single input output element can represent the argument and any dimension of exchange items the structured or nested inputs and outputs can represent multiple dimensions of exchange items for example in a runoff simulation model evaporation can be nesting parents that may have two nested children the value and time of the precipitation 2 3 model description openmi defines idescribable and identifiable interfaces to describe the related model entity the idescribale interface provides caption and description attributes which give a caption and additional descriptive information about the entity the iidentifiable interface provides the id attribute to identify the entity uniquely within its context openmi also defines a registration file named the omi file which contains metadata about a component such as the dependent platform and arguments needed to initialize this component but openmi does not define any description profile for runtime exchange items which may be requested by third party software when integrating openmi compliant components detailed descriptions of models on the web are essential which enable model services discoverable by both humans and computer systems and provide fundamental guidance on using these services wps specification defines a general information model that specifies the information structures to describe a processing function including an identifier id title caption abstract description keywords and metadata and input and output parameters the information model can cover the information provided by idescribable and identifiable interfaces however it has limitations on distinguishing the inputs outputs used for different execution phases such as initialization and time step computations a specific metadata with a unique title name i e iotype is used to indicate the correspondence between execution phases and inputs outputs fig 1 shows the mapping from openmi entities classes in dark grey to the wps description framework the input elements with the title common describe the iargument interface which is used for model initialization the input output elements with the title runtime describe the itimespaceexchangeitem ibaseexchangeitem interface which is used for time step based computations they may contain three nested children that are identified by element quantity and time which describe the value location and time dimension of runtime exchange items wps 2 0 defines a statusinfo document to provide status information about jobs on a wps server such as 1 status the identifier of the status of the job 2 nextpoll the next suggested status polling data and time 3 percentcomplete the percentage of the process that has been completed besides the client also needs to know the current step that the server is running or to run when performing the time step computation to submit the appropriate data or make a decision about the next request a new element named currenttime is added to the statusinfo document when the states are updating or updated 2 4 model execution wps operations related to asynchronous execution include execute getstatus getresult and dismiss the execute request allows a client to run a specific process in the wps server once the wps server receives the execute request it will return status information immediately and create an asynchronous processing job then the getstatus operation allows the client to check the execution status using the job identifier contained in the response when the processing job has been completed the getresult request allows the clients to retrieve the processing result the dismiss request enables the clients to terminate the asynchronous processing jobs ibaselinkablecomponent is the core interface to implement an openmi compliant component it specifies the arguments that are used when initializing the model component and input s and output s that define the exchange items generally openmi compliant components run in three phases initialize run and finish wps execute and model initialization operations are both used to translate the arguments to the model process and start its execution therefore the execute operation can be used to perform model initialization in addition the model also needs time step inputs to continue its running a new operation performstep for time step based execution is added by combining execute and getstatus operations getstatus operation supports dynamic communication between the client and the server both dismiss operation from wps and finish method from openmi are designed to terminate or finish the processing jobs however the dismiss operation requires the server to free the associated resources as soon as possible which means the results will be removed while the final result needs to be stored in the server and provided to users after the finish method is invoked a new finish operation to perform finishing is added instead of using dismiss operation directly a key challenge during model execution is how to optimize data and operations between traditional clients and servers on the web laniak et al 2013 two improvements can be made to reduce the time cost of communication large data can be put on the same server with environmental models and then provided to them by reference in addition exchange items usually contain three dimensions spatial location value and time the value changes with time while the spatial location usually remains the same for example the location of a specific temperature station will not change in a particular simulation such information can be submitted once in execute request 3 results wps 2 0 is extended to support environmental model sharing on the web according to the design in section 2 this section introduces an xml schema for the extensions to wps and describes how the client communicates with the wps server when executing a time marching model 3 1 xml schema for wps extensions xml is a software and hardware independent markup language that is widely used for interoperability an xml schema describes the structure of a type of xml document extensions to wps are provided in detail using the xml schema table 2 shows the details of xml encoding extensions for sharing environmental models in general extensions are made at three levels 1 extending or restricting xml elements without changing the xml schema such as using metadata element to distinguish the input output type 2 adding new optional values to the enumeration value defined in the schema such as more state choices 3 adding new operations i e performstep and finish operations the extensions can be fully compatible with the existing wps specification the full extended xml schema for wps is available on github https github com geoprocessing wps4m schemas 3 2 model execution using extended wps fig 2 shows the interfaces involved during model execution on the wps server and its related states the interfaces are designed by a trade off between minimum extensions to wps and functionalities two additional operations performstep and finish are added to perform time step based computations and finish the model on the server the execute request allows the clients to start the model execution process which includes configuration information such as start time and end time of simulation and arguments needed for model initialization when the wps server receives the request it will create a new job and a model instance and returns the jobid to the client immediately with the state created once the model instance is created the job will initialize it with the arguments contained in the execute request and the state changes to initializing and then to initialized if initialization succeeds when the state changes into initialized it indicates that the server is ready to perform time step based computations and is waiting for input data from the client the user sends time series data to the model instance on the server using the performstep operation when the server receives the appropriate data it will perform time step based computation and the state will change to updating when one step computation is done the state changes to updated and waits for the input data for next step computation this process will loop until it reaches the end time of the simulation and then comes to the done state in this execution phase the client can use getstatus request to check the current time of the computations and what the model is doing updating or updated and then decides whether to send performstep request again after the loop is done finish operation is used to finish the simulation such as writing the result files closing all open files and freeing memory the status will switch to finishing and then to finished getresult operation allows the client to retrieve the full results of the simulation dismiss operation allows the client to terminate the model simulation 4 implementation 4 1 software implementation an open source software named wps4m is developed based on the existing 52 north javaps project https github com 52north javaps javaps enables the deployment of geospatial processes following the wps 2 0 specification it is implemented using the java programming language and features a pluggable architecture for process and data encodings javaps is built upon the arctic sea project https github com 52north arctic sea which is a framework that eases the development of ogc related services e g ogc wps both arctic sea and javaps projects are extended to meet the requirements needed for publishing environmental models fig 3 shows our extensions dashed rectangles to the architecture of 52 north project the overall architecture is based on the spring framework that offers incoming request retrieval as well as dependency injection and bean handling the binding component refers to request encoding type such as kvp key value pair for http get and pox plain old xml the operator component is responsible for deriving the appropriate request object every wps request is represented as a separate object performsteprequest and finishrequest objects are added to encapsulate performstep and finish requests javaps implements one corresponding operationhandler for each wps operation performstephandler and finishhandler are added for performstep and finish operation respectively in javaps project the engine component is responsible for process job and persistence management a process is implemented as an algorithm that can be provided using external algorithm repositories such as javaps jts backend https github com 52north javaps jts backend an ienvmodel interface is added to deploy environmental models which extend the ialgorithm interface in javaps project ienvmodel interface contains three core methods execute performstep and finish which will be invoked by the job controller correspondingly according to the requests received by the wps server a new project wps4m is developed to facilitate the deployment of environmental models and allows service developers to focus on the model implementation wps4m is an external model repository which can be integrated into javaps through configuration it is available on github https github com geoprocessing wps4m we also developed a simple wps4m client https github com geoprocessing wps4m client git wps4m client to demonstrate how to access server side models that follow the proposed protocol 4 2 case study implementation this paper chooses two environmental models topography based hydrological model topmodel and storm water management model version 5 swmm5 as case studies to illustrate the applicability of the proposed approach these two models have been implemented in open source desktop programs that are released in public domain e g hydromodeler and epa swmm5 which makes the programs customizable the traditional ways of using these models require users to download these programs and install them on personal computers while following our approach the models can be deployed once on the server side but consumed everywhere on the client side and integrated flexibly with other models in the web environment the case studies demonstrate two ways to implement legacy environmental models as web services 1 rewriting a model from scratch like the topmodel case 2 reusing the codes of legacy models like the swmm5 case the former way is applicable in scenarios where source codes of existing models are not available or not compatible with the wps framework e g wps4m the latter one is applicable in scenarios where the legacy programs have exposed necessary interfaces that can be invoked by the wps framework 4 2 1 the implementation of topmodel service topmodel is a numerical time marching model used to simulate watershed runoff beven 1997 beven and kirkby 1979 which has been applied to various regions throughout the world beven 2011 in this model the watershed is subdivided into a matrix of equal sized cells and the topographic index calculated from each cell is used to represent hydrological similarity all cells with the same value of the index are assumed to respond in a similar hydrological way beven 1997 topmodel requires precipitation and evapotranspiration as input for its time step based computations and outputs watershed runoff the total runoff includes the flows from overland and subsurface as shown in the following equations hornberger et al 1998 1 q t o t a l q s u b s u r f a c e q o v e r l a n d 2 q o v e r l a n d a s a t a p q r e t u r n 3 q s u b s u r f a c e t m a x exp γ exp s m where a s a t is the saturated area a is the watershed area p is the rate of throughfall q r e t u r n is the return flow t m a x is the saturated soil transmissivity γ is the average topographic index for the watershed s is the average saturation deficit m is a scaling parameter defined by the user to simplify the deployment process wps4m provides a wrapper class named abstractmodelwrapper that implements the ienvmodel interface the wrapper already encapsulates the common functions such as parsing the inputs to parameter name and value pairs hydromodeler castronova et al 2013b provided a c open source implementation of topmodel it is rewritten and published as a service by extending the java wrapper class there are four declared methods to implement the createdescription method allows developers to define the input structures of the topmodel the execute method is used to start the model simulation and perform the initialization time step calculations happen in performstep method the finishmodel method writes the runoff data into files on the server then this model can be integrated into wps4m by adding it to the configuration file wps4m provides the implementation of topmodel as an example of the deployment of time marching models when the topmodel service is available users can access it using the standard operations fig 4 shows the communication between the client and topmodel on the wps server and some request examples the description fragments response to describeprocess operation show that the topindex input is required for initialization and the precipitation input is required for time step based computations according to this the users can build execute and performstep requests the execute request sends initialization parameters e g topoindex to and receives a job id from the server then the users can send time independent data e g precipitation to the running model with the specific id and obtain the runoff of each time step at last finish operation allows the users to finish the simulation and retrieve the simulation result fig 5 shows the daily runoff generated by topmodel which covers coweeta 18 watershed located in the northwest of carolina usa in 2006 the used daily precipitation data is from hydrodesktop ames et al 2012 the used daily evapotranspiration data was produced by hargreaves hargreaves and samani 1982 that takes daily temperature as inputs the topographic index is from the example used in our previous work yue et al 2015 the results are the same as those generated by the local topmodel component yue et al 2015 both the client and the server run on personal computers each computer is equipped with a 2 20 ghz inter r core tm i7 8750h processor and 8 0 gb of memory the total time cost is about 5 s which mainly consists of the model computation communication between the client and the server and parsing xml it is inevitable to increase the cost time when exchanging messages through the web compared with the component based approaches in this use case the time cost of the model computation only makes up about 20 with the increase of the computational complexity the proportion of the time cost by model computation will increase for example the proportion is about 90 when we set the execution time of a step as 100 ms manually therefore when the model calculation is massive the time cost of communication and parsing xml have relatively small impacts on the total time cost 4 3 the implementation of swmm5 service swmm5 is a rainfall runoff simulation model of water runoff quantity and quality which is mainly used in urban areas gironás et al 2010 it simulates the runoff for each subcatchment and routes these runoff and external inflows through drainage systems swmm5 has been widely used for planning analysis and design related to stormwater runoff and urban drainage systems burger et al 2014 for example sadler et al 2019 used it to simulate model predictive control mpc for urban drainage systems swmm5 can simulate the hydrologic and hydraulic states of the study area based on time steps e g the runoff generated by subcatchments flow rates in system links it uses a particular hydrological object named rain gage to supply precipitation data in study area rain gages utilize rainfall data from time series or external files the surface runoff from each subcatchment drains into a single discharge point that can be either a node of the drainage system or other subcatchments in swmm5 a project file contains all the information used to describe the study area and the options used for simulation it is the only file required to run swmm5 a report file and an output file can be saved after the swmm5 runs successfully the report files contain status report and summary results and the output file contains the time series results there are several open source implementations of swmm5 model the united states environmental protection agency provides the source codes for the swmm5 computational engine and the swmm5 graphical user interface gui they are referred to as epa swmm5 and is written in c programming language the computation engine exposes several core methods e g swmm start swmm step swmm end that can be accessed by other programming languages such as python and c sharp open water analytics developed an enhanced version of epa swmm5 owa swmm5 https github com openwateranalytics stormwater management model it exposed more methods that allow custom routines to be executed between each time step of the simulation the attributes of each object defined in swmm5 can be assigned at each time step for example the rain gage can receive precipitation data before each time step computation in this case study we reused owa swmm5 source codes by using the java native interface jni when deploying swmm5 model service in wps4m jni is a standard programming interface that allows java to use native codes e g c c a simple java toolkit swmm4j was developed to use native interfaces of owa swmm5 which is available at github https github com geoprocessing swmm4j git then the swmm5 model was integrated into wps4m by mapping the methods exposed by the swmm4j to those defined by the abstractmodelwrapper class the execute method in the wps4m wrapper is used to start the swmm4 model using the project file received through the wps execute request the performstep method is used to advance the swmm4 step by step using the precipitation data received through performstep request the finishmodel method writes the results into files including the report file and output file which are accessible through wps getresult request the schema of the model used to demonstrate the application of the swmm5 service is shown in fig 6 it consists of subcatchment areas s1 through s7 storm sewer conduits c1 through c11 conduit junctions j1 through j11 two rain gages and one outfall o1 i e the terminal node of the system raingage1 is assigned to the subcatchments s1 s2 and s3 raingage2 is assigned to the other subcatchments this use case is based on the example from the official swmm applications manual gironás et al 2010 it is used to simulate the water runoff through the drainage system during a 2 h design event storm compared with topmodel swmm5 is much more complex in terms of runtime exchange items there may be several element sets i e the spatial portions for the same quantity and one element set has several quantities for example in our use case the rainfall data provided to the drainage system is from two rain gages each conduit can provide both flow rate and flow depth at each computation step our approach can represent the multiple dimensions of exchange items using nested inputs outputs fig 7 shows some xml fragments from one performstep request and its response in the request the rain gages have two nested children time and rainfall in the response the conduit c1 provides three nested children time flow rate and flow depth fig 8 shows the rainfall data the runoff generated by two subcatchments s1 and s4 and the flow rates of conduits c1 and c11 the final results are the same as those generated by the owa swmm5 the rain from raingage2 for s4 started 20 min after the start of the rain event from raingage1 for s1 the conduit c1 only transports the runoff generated by s1 while c11 transports all the runoff generated by the whole study area since it is the last conduit before the water leaving the system through the outfall o1 5 discussion the work in this paper is an extension of our previous work gao et al 2019 yue et al 2015 the environmental domains advocate openmi as a promising framework for accessing and coupling environmental models while the geospatial domain tends to treat models as algorithms accessible through the wps protocol the wps has some limitations in dealing with complex semantics and time step computations of models in our earliest work yue et al 2015 we propose to use the workflow system to support integrated environmental modeling which can integrate sensor web geoprocessing services and openmi compliant model components into workflows traditional spatial analysis algorithms are accessible through wps while complex environmental models are accessible through openmi compliant components the models are still not accessible on the web since openmi is not a web protocol thus later we explore the model as a service perspective and find the websocket protocol is promising for web access to time marching models in openmi gao et al 2019 competing solutions like http and websocket are emerging gao et al 2019 there is no one size fits all approach if performance is preferred the websocket approach can be followed this paper emphasizes interoperability and provides the solution by incorporating openmi in the wps context thus bridging traditional environmental domain to the geospatial domain in an interoperable way the designed service interfaces are compatible with wps and allow the environmental models run by similar logic with openmi which are friendly to users who are familiar with these two standards the proposed approach allows the clients to drive the server side model running based on time steps which makes it possible to feed in new observations during the simulation it is flexible to use in different architectures such as rest and soap which benefits from the flexibility of wps the wps4m project provides a wrapper class to facilitate the deployment of environmental models following the proposed approach openmi compliant components can be easily re implemented as web services using our developed software the coupling of environmental models and traditional gis is widely studied clark 1998 goodchild et al 1992 ng et al 2018 santoalla et al 2018 there are two general approaches to couple environmental models and gis loose and tight approach huang and jiang 2002 loose coupling relies on the exchange of data files between gis and environmental models tight coupling usually builds the environmental modeling system on top of gis to use its various analysis functions the web is a promising environment for loosely coupling environmental models and gis the proposed approach in this paper can improve the web based coupling method by providing a consistent view of environmental models and gis functions on the web and facilitating the implementation of the service composition engine by following one compatible specification there have been efforts to maintain code and metadata repositories for environmental models for example the csdms model repository now holds over 300 open source models and tools https csdms colorado edu wiki model download portal hydroshare is a collaborative hydrologic environment that enables storage management sharing publication and annotation of diverse types of hydrologic data and models horsburgh et al 2016 https www hydroshare org both csdms and hydroshare allow scientists to submit new models to their repositories and describe the model using their metadata standards then other scientists can download the models install and run on their local computers from resource sharing aspect hydroshare also supports the sharing of the input files used by and the outputs generated by a specific model as a whole resource i e a model instance horsburgh et al 2016 which ensures the reproductivity of the study results our approach focuses on standard interfaces for sharing the model logic the reproductivity can be achieved by recording the standard request and response during model execution from model execution aspect hydroshare allows users to use mature popular apps e g cuahsi jupyterhub to run their models remotely via web interface and internet protocols while our approach adopts service oriented architecture by executing models in the form of ogc web services from model description aspect hydroshare uses a metadata framework that is built from general standards e g dublin core to support discovery sharing and interpretation of environmental models morsy et al 2017 our approach extends the wps description framework to describe models through standardized interface compared to the work of castronova et al 2013a following improvements have been maded by our approach first castronova et al 2013a used simple key value pairs to represent exchange items which lacks detail in describing the metadata for inputs and outputs our approach used nested structures for data encapsulation which allow multiple dimensions of exchange items to be represented secondly castronova et al 2013a provided a workaround method to designate whether performing the initialization routine by adding a mandatory input parameter which may lead to confusion explicit operations are defined for different simulation phases in our approach thirdly we use the server side job to maintain state in the memory instead of serializing the step calculation results into the disk additional functions serialization and deserialization are not required when deploying new environmental models using our approach lastly we extended the wps description framework to describe model inputs and outputs for different simulation phases which was lacked in the approach from castronova et al 2013a model description especially in the web environment is very important for model discovery and use our approach also allows users to retrieve the current status of a running model i e model instance for informative and control flow decision purposes it involves the getstatus request and the status description when the client side application exited abnormally and tried to recreate the connection to the server such status information becomes much more important for the clients to decide what to do next waiting for the step computation or passing the appropriate data to the server considering the features of the web environment and the existing operations in wps not all openmi interfaces are exposed directly orchestration and choreography are two general approaches to compose web services peckham et al 2013 the choreography approach requires each involved service to describe its part in the interaction which means each service needs to know its successor and ancestor while the orchestration approach specifically focuses on the coordination of composition actions and services which enables the services coupled loosely peltz 2003 in openmi ibaseinput may have a provider ibaseoutput otherwise ibaseoutput may have one or several consumers two openmi compliant components are integrated using an approach similar to choreography however the orchestration approach is always used when integrating ogc wps services rautenbach et al 2013 yue et al 2015 zhang et al 2017 therefore some properties and methods are not necessary when publishing openmi compliant components as wps services for example the validate method in openmi must be invoked after building the various provider consumer relations between different components in orchestration situations it is the orchestration engine instead of the service itself to perform such functions 6 conclusion and future work this paper proposes an approach to sharing environmental models by coupling ogc wps and openmi which supports time step calculations on the web key considerations including maintaining state data representation model description and model execution are presented in our approach we design the service interfaces by following the execution logic of openmi the proposed solutions to model description and representation of exchange data are compatible with wps specification an open source software built on the 52 north javaps project is developed to facilitate the deployment of the environmental models the topmodel and swmm5 use cases demonstrate the applicability of the approach and implementation dynamic exchange data among web service is still a challenge to integrate environmental models online laniak et al 2013 many existing works tried to address this issue by using a legacy component based framework for example goodall et al 2011 castronova et al 2013a and gao et al 2019 developed openmi compliant components to act as a client that is responsible for handling data transfers to and from the model service and then integrated the component using openmi framework jiang et al 2017 published environmental models as service using the basic model interface bmi and then integrated these services within the csdms framework in future work we will investigate how to integrate model services and geoprocessing services following a full service oriented paradigm in this way the preprocessing of input arguments model running and data operations e g spatial adaption and time interpolation can all be done through web services declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we appreciate the editors and anonymous reviewers for their very constructive comments that help improve the quality of the paper the work was supported by the national natural science foundation of china no 41901313 and 41901315 and major state research development program of china no 2017yfb0504103 
25974,with the advancement of web technologies the service oriented approach has become increasingly popular to share and couple environmental models interoperability is critical to the access and integration of web services the ogc web processing service wps defines an interoperable protocol for process execution on the web however it has limitations in supporting time step calculations that are usually required by complex models e g time marching models the open modeling interface openmi standard brings about interoperability for time step based interactions between modeling components this paper introduces an approach to sharing environmental models on the web by coupling openmi and wps key issues including maintaining state data representation model description and model execution are considered in our approach an open source software is developed to support the deployment of the environment models in an interoperable way topmodel and swmm5 use cases illustrate the applicability of the approach and implementation keywords environmental models web service ogc wps openmi 1 introduction integrated environmental modeling iem provides a holistic approach to understand the environmental systems argent 2004 jakeman and letcher 2003 laniak et al 2013 lerner et al 2011 which requires an integration of models across disciplinary and organizational boundaries a great challenge facing the iem community is how to make heterogeneous models easily connected and interoperable goodall et al 2011 laniak et al 2013 qiao et al 2019 there are mainly two kinds of modeling frameworks for iem namely component based and service oriented frameworks gao et al 2019 the former focuses on local computer environments while the latter enables the integration of distributed environmental models component based modeling has been investigated extensively in traditional iem tools granell et al 2013 many interface standards and modeling frameworks are already available such as the open modeling interface openmi gregersen et al 2007 the earth system modeling framework esmf hill et al 2004 and community surface dynamics modeling system csdms peckham et al 2013 among them the openmi standard referred to as openmi in this paper is widely studied and used to couple diverse models laniak et al 2013 it provides well defined interfaces that enable the runtime exchange of data between modeling components moore and tindall 2005 which has been published as an ogc standard vanecek and roger 2014 the component based frameworks often run models following multiple simulation phases and basing on time step computations goodall et al 2011 gregersen et al 2007 peckham et al 2013 which has its advantage in having concrete and community wide tools gao et al 2019 however the component based modeling frameworks often require the same programming language and work in local computer environments castronova et al 2013a gao et al 2019 the service oriented approach has great potential to address the issue of accessibility and interoperability for environmental models in distributed environments li et al 2017 mcdonald et al 2019 nativi et al 2013 rajib et al 2016 sun et al 2020 wen et al 2013 it allows various models accessible layered on different programming languages and platforms belete et al 2017 goodall et al 2011 granell et al 2010 and coupled over the internet interoperability is critical to the access and integration of web services in addition to standard interfaces domain specific descriptions also have been recognized as an integral part of web services jiang et al 2017 yue et al 2016 zhang et al 2019 and predefined data structures are significant for the communication between service providers and consumers the open geospatial consortium ogc web processing service wps specifies the general service interfaces to execute computing processes and retrieve metadata describing their purpose and functionality müller and pross 2015 recently there has been increasing interest in applying wps to share environmental models on the web qiao et al 2019 sun et al 2019 zhang et al 2018 time marching models perform a numerical time stepping procedure to obtain the time varying behavior of the models for example a runoff simulation model may march daily by recalculating model equations using a time step of one day coupled time marching models often exchange data at simulation runtime e g a daily runoff simulation model takes the daily precipitation and evapotranspiration as input during the simulation this characteristic requires the services to hold states and be accessed based on time steps castronova et al 2013a while the wps requires receiving all the necessary inputs before executing a specific process and does not provide appropriate interfaces for exchanging data dynamically most of the current wps solutions to share environmental models cannot support complex interactions castronova et al 2013a addressed some issues to support time dependent calculations using ogc wps in a rest representational state transfer pattern they used the http put method to send and store data for the server side models based on time steps and used the http delete method to remove the files associated with an individual simulation to maintain state intermediate results from previous time steps calculations were serialized for subsequent calculations however challenges still remain and some points can be improved such as providing a structured way for encapsulating complex data and a standardized mechanism for model initialization and finishing castronova et al 2013a this paper introduces an approach to interoperable web sharing of environmental models by coupling openmi and wps which can help address these challenges a detailed comparison of their approach and ours is provided in the discussion section openmi provides a widely accepted method to exchange data based on time steps in the iem community a service oriented approach that follows the same logic can ensure high functionality and retain the existing user habits ogc wps is a mature standard for sharing computing processes on the web in the geospatial domain it already offers the solutions to invoke and retrieve metadata of the processes in an interoperable way which is a good starting point to share environmental models key considerations during coupling the two standards are presented including maintaining state data representation model description and model execution we develop an open source software to facilitate the deployment of the environmental models following the proposed method the topography based hydrological model and swmm5 use cases demonstrate the applicability of the approach and implementation the remainder of this paper is organized as follows section 2 and section 3 introduce the design and results of the proposed method section 4 provides the implementations that support the extended wps protocol section 5 discusses the proposed method conclusions and pointers to future work are described in section 6 2 design the proposed approach integrates openmi into ogc wps to support time marching models findable accessible interoperable and reusable in distributed environments key issues including state maintenance data representation model description and model execution are considered 2 1 maintaining state in the web environment it is required to maintain a connection between a specific client and the model instance that it is invoking the client needs to identify the simulation session and drive the model calculation based on time steps in the wps conceptual model a wps server will instantiate a processing job i e process instance when receiving an execution request in the case of asynchronous execution mode the client can control and monitor the processing job with a job id müller and pross 2015 this mechanism allows a job object to save calculations from previous time steps on the server an additional control option is required to perform subsequent calculations when the server receives input data for the next step calculation in this way individual client server interactions can be stateful by extending the job control options the client can retrieve the status information of the model instance using the wps operation named getstatus wps specification also defines a basic status set to communicate the status of a server side job to the client müller and pross 2015 including succeeded failed accepted and running however it requires more states to indicate what the environmental models are currently doing on the server the openmi explicitly specifies the possible states that may occur in the lifecycle of openmi components created indicates that linkable component instance has just been created this status is followed by initializing that means the linkable component is initializing itself which ends in an initialized status then the component updates its calculations when receiving time dependent input data it comes updated state after a step computation is finished successfully and will wait for the input data for next step calculation after the time step computation is over the status changes to done finished indicates that the component has been successfully finalized the status will change into failed when an error occurs during the initialization preparation updating and finishing processes table 1 shows the reused states and their semantics in the wps context in openmi waitingfordata indicates that the component is waiting for input data from other components to update itself which is an implicit state in our design 2 2 data representation service oriented frameworks often require data communication between client applications and web resources it is crucial to encapsulate data into a standard format understood by both sides iargument and ibaseexchangeitem are core interfaces that represent data in openmi the former represents data used to initialize the component and the latter represents the data used to exchange at runtime the primary exchange item ibaseexchangeitem includes a value definition that represents what it exchanges the extended interface itimespaceexchangeitem also specifies where and when the value applies which includes three dimensions value what time when and spatial where definition wps defines three common data types for process input and output data a complexdata such as gml or geo referenced imagery b literaldata a value with an optional unit c boundingboxdata defined as a minimum bounding rectangle in geographic coordinates wps specification defines two distinct ways for data exchange between wps client and server by reference and by value müller and pross 2015 traditionally small or atomic data such as doubles or strings are transmitted by value large data are usually submitted by reference a single input output element can represent the argument and any dimension of exchange items the structured or nested inputs and outputs can represent multiple dimensions of exchange items for example in a runoff simulation model evaporation can be nesting parents that may have two nested children the value and time of the precipitation 2 3 model description openmi defines idescribable and identifiable interfaces to describe the related model entity the idescribale interface provides caption and description attributes which give a caption and additional descriptive information about the entity the iidentifiable interface provides the id attribute to identify the entity uniquely within its context openmi also defines a registration file named the omi file which contains metadata about a component such as the dependent platform and arguments needed to initialize this component but openmi does not define any description profile for runtime exchange items which may be requested by third party software when integrating openmi compliant components detailed descriptions of models on the web are essential which enable model services discoverable by both humans and computer systems and provide fundamental guidance on using these services wps specification defines a general information model that specifies the information structures to describe a processing function including an identifier id title caption abstract description keywords and metadata and input and output parameters the information model can cover the information provided by idescribable and identifiable interfaces however it has limitations on distinguishing the inputs outputs used for different execution phases such as initialization and time step computations a specific metadata with a unique title name i e iotype is used to indicate the correspondence between execution phases and inputs outputs fig 1 shows the mapping from openmi entities classes in dark grey to the wps description framework the input elements with the title common describe the iargument interface which is used for model initialization the input output elements with the title runtime describe the itimespaceexchangeitem ibaseexchangeitem interface which is used for time step based computations they may contain three nested children that are identified by element quantity and time which describe the value location and time dimension of runtime exchange items wps 2 0 defines a statusinfo document to provide status information about jobs on a wps server such as 1 status the identifier of the status of the job 2 nextpoll the next suggested status polling data and time 3 percentcomplete the percentage of the process that has been completed besides the client also needs to know the current step that the server is running or to run when performing the time step computation to submit the appropriate data or make a decision about the next request a new element named currenttime is added to the statusinfo document when the states are updating or updated 2 4 model execution wps operations related to asynchronous execution include execute getstatus getresult and dismiss the execute request allows a client to run a specific process in the wps server once the wps server receives the execute request it will return status information immediately and create an asynchronous processing job then the getstatus operation allows the client to check the execution status using the job identifier contained in the response when the processing job has been completed the getresult request allows the clients to retrieve the processing result the dismiss request enables the clients to terminate the asynchronous processing jobs ibaselinkablecomponent is the core interface to implement an openmi compliant component it specifies the arguments that are used when initializing the model component and input s and output s that define the exchange items generally openmi compliant components run in three phases initialize run and finish wps execute and model initialization operations are both used to translate the arguments to the model process and start its execution therefore the execute operation can be used to perform model initialization in addition the model also needs time step inputs to continue its running a new operation performstep for time step based execution is added by combining execute and getstatus operations getstatus operation supports dynamic communication between the client and the server both dismiss operation from wps and finish method from openmi are designed to terminate or finish the processing jobs however the dismiss operation requires the server to free the associated resources as soon as possible which means the results will be removed while the final result needs to be stored in the server and provided to users after the finish method is invoked a new finish operation to perform finishing is added instead of using dismiss operation directly a key challenge during model execution is how to optimize data and operations between traditional clients and servers on the web laniak et al 2013 two improvements can be made to reduce the time cost of communication large data can be put on the same server with environmental models and then provided to them by reference in addition exchange items usually contain three dimensions spatial location value and time the value changes with time while the spatial location usually remains the same for example the location of a specific temperature station will not change in a particular simulation such information can be submitted once in execute request 3 results wps 2 0 is extended to support environmental model sharing on the web according to the design in section 2 this section introduces an xml schema for the extensions to wps and describes how the client communicates with the wps server when executing a time marching model 3 1 xml schema for wps extensions xml is a software and hardware independent markup language that is widely used for interoperability an xml schema describes the structure of a type of xml document extensions to wps are provided in detail using the xml schema table 2 shows the details of xml encoding extensions for sharing environmental models in general extensions are made at three levels 1 extending or restricting xml elements without changing the xml schema such as using metadata element to distinguish the input output type 2 adding new optional values to the enumeration value defined in the schema such as more state choices 3 adding new operations i e performstep and finish operations the extensions can be fully compatible with the existing wps specification the full extended xml schema for wps is available on github https github com geoprocessing wps4m schemas 3 2 model execution using extended wps fig 2 shows the interfaces involved during model execution on the wps server and its related states the interfaces are designed by a trade off between minimum extensions to wps and functionalities two additional operations performstep and finish are added to perform time step based computations and finish the model on the server the execute request allows the clients to start the model execution process which includes configuration information such as start time and end time of simulation and arguments needed for model initialization when the wps server receives the request it will create a new job and a model instance and returns the jobid to the client immediately with the state created once the model instance is created the job will initialize it with the arguments contained in the execute request and the state changes to initializing and then to initialized if initialization succeeds when the state changes into initialized it indicates that the server is ready to perform time step based computations and is waiting for input data from the client the user sends time series data to the model instance on the server using the performstep operation when the server receives the appropriate data it will perform time step based computation and the state will change to updating when one step computation is done the state changes to updated and waits for the input data for next step computation this process will loop until it reaches the end time of the simulation and then comes to the done state in this execution phase the client can use getstatus request to check the current time of the computations and what the model is doing updating or updated and then decides whether to send performstep request again after the loop is done finish operation is used to finish the simulation such as writing the result files closing all open files and freeing memory the status will switch to finishing and then to finished getresult operation allows the client to retrieve the full results of the simulation dismiss operation allows the client to terminate the model simulation 4 implementation 4 1 software implementation an open source software named wps4m is developed based on the existing 52 north javaps project https github com 52north javaps javaps enables the deployment of geospatial processes following the wps 2 0 specification it is implemented using the java programming language and features a pluggable architecture for process and data encodings javaps is built upon the arctic sea project https github com 52north arctic sea which is a framework that eases the development of ogc related services e g ogc wps both arctic sea and javaps projects are extended to meet the requirements needed for publishing environmental models fig 3 shows our extensions dashed rectangles to the architecture of 52 north project the overall architecture is based on the spring framework that offers incoming request retrieval as well as dependency injection and bean handling the binding component refers to request encoding type such as kvp key value pair for http get and pox plain old xml the operator component is responsible for deriving the appropriate request object every wps request is represented as a separate object performsteprequest and finishrequest objects are added to encapsulate performstep and finish requests javaps implements one corresponding operationhandler for each wps operation performstephandler and finishhandler are added for performstep and finish operation respectively in javaps project the engine component is responsible for process job and persistence management a process is implemented as an algorithm that can be provided using external algorithm repositories such as javaps jts backend https github com 52north javaps jts backend an ienvmodel interface is added to deploy environmental models which extend the ialgorithm interface in javaps project ienvmodel interface contains three core methods execute performstep and finish which will be invoked by the job controller correspondingly according to the requests received by the wps server a new project wps4m is developed to facilitate the deployment of environmental models and allows service developers to focus on the model implementation wps4m is an external model repository which can be integrated into javaps through configuration it is available on github https github com geoprocessing wps4m we also developed a simple wps4m client https github com geoprocessing wps4m client git wps4m client to demonstrate how to access server side models that follow the proposed protocol 4 2 case study implementation this paper chooses two environmental models topography based hydrological model topmodel and storm water management model version 5 swmm5 as case studies to illustrate the applicability of the proposed approach these two models have been implemented in open source desktop programs that are released in public domain e g hydromodeler and epa swmm5 which makes the programs customizable the traditional ways of using these models require users to download these programs and install them on personal computers while following our approach the models can be deployed once on the server side but consumed everywhere on the client side and integrated flexibly with other models in the web environment the case studies demonstrate two ways to implement legacy environmental models as web services 1 rewriting a model from scratch like the topmodel case 2 reusing the codes of legacy models like the swmm5 case the former way is applicable in scenarios where source codes of existing models are not available or not compatible with the wps framework e g wps4m the latter one is applicable in scenarios where the legacy programs have exposed necessary interfaces that can be invoked by the wps framework 4 2 1 the implementation of topmodel service topmodel is a numerical time marching model used to simulate watershed runoff beven 1997 beven and kirkby 1979 which has been applied to various regions throughout the world beven 2011 in this model the watershed is subdivided into a matrix of equal sized cells and the topographic index calculated from each cell is used to represent hydrological similarity all cells with the same value of the index are assumed to respond in a similar hydrological way beven 1997 topmodel requires precipitation and evapotranspiration as input for its time step based computations and outputs watershed runoff the total runoff includes the flows from overland and subsurface as shown in the following equations hornberger et al 1998 1 q t o t a l q s u b s u r f a c e q o v e r l a n d 2 q o v e r l a n d a s a t a p q r e t u r n 3 q s u b s u r f a c e t m a x exp γ exp s m where a s a t is the saturated area a is the watershed area p is the rate of throughfall q r e t u r n is the return flow t m a x is the saturated soil transmissivity γ is the average topographic index for the watershed s is the average saturation deficit m is a scaling parameter defined by the user to simplify the deployment process wps4m provides a wrapper class named abstractmodelwrapper that implements the ienvmodel interface the wrapper already encapsulates the common functions such as parsing the inputs to parameter name and value pairs hydromodeler castronova et al 2013b provided a c open source implementation of topmodel it is rewritten and published as a service by extending the java wrapper class there are four declared methods to implement the createdescription method allows developers to define the input structures of the topmodel the execute method is used to start the model simulation and perform the initialization time step calculations happen in performstep method the finishmodel method writes the runoff data into files on the server then this model can be integrated into wps4m by adding it to the configuration file wps4m provides the implementation of topmodel as an example of the deployment of time marching models when the topmodel service is available users can access it using the standard operations fig 4 shows the communication between the client and topmodel on the wps server and some request examples the description fragments response to describeprocess operation show that the topindex input is required for initialization and the precipitation input is required for time step based computations according to this the users can build execute and performstep requests the execute request sends initialization parameters e g topoindex to and receives a job id from the server then the users can send time independent data e g precipitation to the running model with the specific id and obtain the runoff of each time step at last finish operation allows the users to finish the simulation and retrieve the simulation result fig 5 shows the daily runoff generated by topmodel which covers coweeta 18 watershed located in the northwest of carolina usa in 2006 the used daily precipitation data is from hydrodesktop ames et al 2012 the used daily evapotranspiration data was produced by hargreaves hargreaves and samani 1982 that takes daily temperature as inputs the topographic index is from the example used in our previous work yue et al 2015 the results are the same as those generated by the local topmodel component yue et al 2015 both the client and the server run on personal computers each computer is equipped with a 2 20 ghz inter r core tm i7 8750h processor and 8 0 gb of memory the total time cost is about 5 s which mainly consists of the model computation communication between the client and the server and parsing xml it is inevitable to increase the cost time when exchanging messages through the web compared with the component based approaches in this use case the time cost of the model computation only makes up about 20 with the increase of the computational complexity the proportion of the time cost by model computation will increase for example the proportion is about 90 when we set the execution time of a step as 100 ms manually therefore when the model calculation is massive the time cost of communication and parsing xml have relatively small impacts on the total time cost 4 3 the implementation of swmm5 service swmm5 is a rainfall runoff simulation model of water runoff quantity and quality which is mainly used in urban areas gironás et al 2010 it simulates the runoff for each subcatchment and routes these runoff and external inflows through drainage systems swmm5 has been widely used for planning analysis and design related to stormwater runoff and urban drainage systems burger et al 2014 for example sadler et al 2019 used it to simulate model predictive control mpc for urban drainage systems swmm5 can simulate the hydrologic and hydraulic states of the study area based on time steps e g the runoff generated by subcatchments flow rates in system links it uses a particular hydrological object named rain gage to supply precipitation data in study area rain gages utilize rainfall data from time series or external files the surface runoff from each subcatchment drains into a single discharge point that can be either a node of the drainage system or other subcatchments in swmm5 a project file contains all the information used to describe the study area and the options used for simulation it is the only file required to run swmm5 a report file and an output file can be saved after the swmm5 runs successfully the report files contain status report and summary results and the output file contains the time series results there are several open source implementations of swmm5 model the united states environmental protection agency provides the source codes for the swmm5 computational engine and the swmm5 graphical user interface gui they are referred to as epa swmm5 and is written in c programming language the computation engine exposes several core methods e g swmm start swmm step swmm end that can be accessed by other programming languages such as python and c sharp open water analytics developed an enhanced version of epa swmm5 owa swmm5 https github com openwateranalytics stormwater management model it exposed more methods that allow custom routines to be executed between each time step of the simulation the attributes of each object defined in swmm5 can be assigned at each time step for example the rain gage can receive precipitation data before each time step computation in this case study we reused owa swmm5 source codes by using the java native interface jni when deploying swmm5 model service in wps4m jni is a standard programming interface that allows java to use native codes e g c c a simple java toolkit swmm4j was developed to use native interfaces of owa swmm5 which is available at github https github com geoprocessing swmm4j git then the swmm5 model was integrated into wps4m by mapping the methods exposed by the swmm4j to those defined by the abstractmodelwrapper class the execute method in the wps4m wrapper is used to start the swmm4 model using the project file received through the wps execute request the performstep method is used to advance the swmm4 step by step using the precipitation data received through performstep request the finishmodel method writes the results into files including the report file and output file which are accessible through wps getresult request the schema of the model used to demonstrate the application of the swmm5 service is shown in fig 6 it consists of subcatchment areas s1 through s7 storm sewer conduits c1 through c11 conduit junctions j1 through j11 two rain gages and one outfall o1 i e the terminal node of the system raingage1 is assigned to the subcatchments s1 s2 and s3 raingage2 is assigned to the other subcatchments this use case is based on the example from the official swmm applications manual gironás et al 2010 it is used to simulate the water runoff through the drainage system during a 2 h design event storm compared with topmodel swmm5 is much more complex in terms of runtime exchange items there may be several element sets i e the spatial portions for the same quantity and one element set has several quantities for example in our use case the rainfall data provided to the drainage system is from two rain gages each conduit can provide both flow rate and flow depth at each computation step our approach can represent the multiple dimensions of exchange items using nested inputs outputs fig 7 shows some xml fragments from one performstep request and its response in the request the rain gages have two nested children time and rainfall in the response the conduit c1 provides three nested children time flow rate and flow depth fig 8 shows the rainfall data the runoff generated by two subcatchments s1 and s4 and the flow rates of conduits c1 and c11 the final results are the same as those generated by the owa swmm5 the rain from raingage2 for s4 started 20 min after the start of the rain event from raingage1 for s1 the conduit c1 only transports the runoff generated by s1 while c11 transports all the runoff generated by the whole study area since it is the last conduit before the water leaving the system through the outfall o1 5 discussion the work in this paper is an extension of our previous work gao et al 2019 yue et al 2015 the environmental domains advocate openmi as a promising framework for accessing and coupling environmental models while the geospatial domain tends to treat models as algorithms accessible through the wps protocol the wps has some limitations in dealing with complex semantics and time step computations of models in our earliest work yue et al 2015 we propose to use the workflow system to support integrated environmental modeling which can integrate sensor web geoprocessing services and openmi compliant model components into workflows traditional spatial analysis algorithms are accessible through wps while complex environmental models are accessible through openmi compliant components the models are still not accessible on the web since openmi is not a web protocol thus later we explore the model as a service perspective and find the websocket protocol is promising for web access to time marching models in openmi gao et al 2019 competing solutions like http and websocket are emerging gao et al 2019 there is no one size fits all approach if performance is preferred the websocket approach can be followed this paper emphasizes interoperability and provides the solution by incorporating openmi in the wps context thus bridging traditional environmental domain to the geospatial domain in an interoperable way the designed service interfaces are compatible with wps and allow the environmental models run by similar logic with openmi which are friendly to users who are familiar with these two standards the proposed approach allows the clients to drive the server side model running based on time steps which makes it possible to feed in new observations during the simulation it is flexible to use in different architectures such as rest and soap which benefits from the flexibility of wps the wps4m project provides a wrapper class to facilitate the deployment of environmental models following the proposed approach openmi compliant components can be easily re implemented as web services using our developed software the coupling of environmental models and traditional gis is widely studied clark 1998 goodchild et al 1992 ng et al 2018 santoalla et al 2018 there are two general approaches to couple environmental models and gis loose and tight approach huang and jiang 2002 loose coupling relies on the exchange of data files between gis and environmental models tight coupling usually builds the environmental modeling system on top of gis to use its various analysis functions the web is a promising environment for loosely coupling environmental models and gis the proposed approach in this paper can improve the web based coupling method by providing a consistent view of environmental models and gis functions on the web and facilitating the implementation of the service composition engine by following one compatible specification there have been efforts to maintain code and metadata repositories for environmental models for example the csdms model repository now holds over 300 open source models and tools https csdms colorado edu wiki model download portal hydroshare is a collaborative hydrologic environment that enables storage management sharing publication and annotation of diverse types of hydrologic data and models horsburgh et al 2016 https www hydroshare org both csdms and hydroshare allow scientists to submit new models to their repositories and describe the model using their metadata standards then other scientists can download the models install and run on their local computers from resource sharing aspect hydroshare also supports the sharing of the input files used by and the outputs generated by a specific model as a whole resource i e a model instance horsburgh et al 2016 which ensures the reproductivity of the study results our approach focuses on standard interfaces for sharing the model logic the reproductivity can be achieved by recording the standard request and response during model execution from model execution aspect hydroshare allows users to use mature popular apps e g cuahsi jupyterhub to run their models remotely via web interface and internet protocols while our approach adopts service oriented architecture by executing models in the form of ogc web services from model description aspect hydroshare uses a metadata framework that is built from general standards e g dublin core to support discovery sharing and interpretation of environmental models morsy et al 2017 our approach extends the wps description framework to describe models through standardized interface compared to the work of castronova et al 2013a following improvements have been maded by our approach first castronova et al 2013a used simple key value pairs to represent exchange items which lacks detail in describing the metadata for inputs and outputs our approach used nested structures for data encapsulation which allow multiple dimensions of exchange items to be represented secondly castronova et al 2013a provided a workaround method to designate whether performing the initialization routine by adding a mandatory input parameter which may lead to confusion explicit operations are defined for different simulation phases in our approach thirdly we use the server side job to maintain state in the memory instead of serializing the step calculation results into the disk additional functions serialization and deserialization are not required when deploying new environmental models using our approach lastly we extended the wps description framework to describe model inputs and outputs for different simulation phases which was lacked in the approach from castronova et al 2013a model description especially in the web environment is very important for model discovery and use our approach also allows users to retrieve the current status of a running model i e model instance for informative and control flow decision purposes it involves the getstatus request and the status description when the client side application exited abnormally and tried to recreate the connection to the server such status information becomes much more important for the clients to decide what to do next waiting for the step computation or passing the appropriate data to the server considering the features of the web environment and the existing operations in wps not all openmi interfaces are exposed directly orchestration and choreography are two general approaches to compose web services peckham et al 2013 the choreography approach requires each involved service to describe its part in the interaction which means each service needs to know its successor and ancestor while the orchestration approach specifically focuses on the coordination of composition actions and services which enables the services coupled loosely peltz 2003 in openmi ibaseinput may have a provider ibaseoutput otherwise ibaseoutput may have one or several consumers two openmi compliant components are integrated using an approach similar to choreography however the orchestration approach is always used when integrating ogc wps services rautenbach et al 2013 yue et al 2015 zhang et al 2017 therefore some properties and methods are not necessary when publishing openmi compliant components as wps services for example the validate method in openmi must be invoked after building the various provider consumer relations between different components in orchestration situations it is the orchestration engine instead of the service itself to perform such functions 6 conclusion and future work this paper proposes an approach to sharing environmental models by coupling ogc wps and openmi which supports time step calculations on the web key considerations including maintaining state data representation model description and model execution are presented in our approach we design the service interfaces by following the execution logic of openmi the proposed solutions to model description and representation of exchange data are compatible with wps specification an open source software built on the 52 north javaps project is developed to facilitate the deployment of the environmental models the topmodel and swmm5 use cases demonstrate the applicability of the approach and implementation dynamic exchange data among web service is still a challenge to integrate environmental models online laniak et al 2013 many existing works tried to address this issue by using a legacy component based framework for example goodall et al 2011 castronova et al 2013a and gao et al 2019 developed openmi compliant components to act as a client that is responsible for handling data transfers to and from the model service and then integrated the component using openmi framework jiang et al 2017 published environmental models as service using the basic model interface bmi and then integrated these services within the csdms framework in future work we will investigate how to integrate model services and geoprocessing services following a full service oriented paradigm in this way the preprocessing of input arguments model running and data operations e g spatial adaption and time interpolation can all be done through web services declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we appreciate the editors and anonymous reviewers for their very constructive comments that help improve the quality of the paper the work was supported by the national natural science foundation of china no 41901313 and 41901315 and major state research development program of china no 2017yfb0504103 
