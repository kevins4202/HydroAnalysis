index,text
4335,a significant volume of research over the past two decades has highlighted both direct and indirect links between climate change and groundwater quality however to date few studies have sought to explore the relationship s between drought conditions and groundwater quality in i private unregulated groundwater sources or ii temperate maritime climates not commonly prone to drought events the republic of ireland roi represents an appropriate case study due to its high reliance on private groundwater supplies and while the region is largely unaffected by climatological extremes modelling studies indicate that drier summers and drought conditions will increase in frequency accordingly the current study sought to quantify the effects of the summer 2018 drought experienced throughout europe on private groundwater quality in the southwest of ireland via an opportunistic field study a repeated measures sampling campaign comprised of drought june july and post drought october november analyses of 74 wells was undertaken with complementary mapping and statistical analyses both total coliforms tcs and escherichia coli e coli were present during both drought tcs 42 74 56 8 e coli 7 74 9 5 and post drought tcs 42 74 56 8 e coli 18 74 24 3 sampling periods e coli contamination during drought conditions was unexpected due to an absence of recharge or infiltration for microbial transport bivariate analyses suggest a hydrodynamic change with the significance of e coli sources and pathways shown to switch between sampling periods i e a shift from a combination of regional and local site specific contamination mechanisms to solely site specific mechanisms more specifically during drought conditions septic tank density p 0 001 and local subsoil type p 0 009 were both associated with the presence of e coli while neither variable was significant during post drought conditions the current study is the first to provide a quantitative comparison of private groundwater quality during and after a large scale drought event in a temperate maritime climate and may be used to improve our understanding of the effects of extreme events and thus necessary preventative and monitoring strategies going forward keywords groundwater drought climate change microbial contamination domestic water supplies 1 introduction the intergovernmental panel on climate change ipcc estimates that the global mean surface temperature increased by 0 85 c 0 6 1 06 c between 1880 and 2012 predicting a further 2 4 c increase over the next century ipcc 2013 climate change modelling projects a higher incidence and duration of severe weather events including significant flooding and drought conditions posing significant global environmental and human health risks and a particularly unique challenge for regions unaccustomed to severe climatic phenomena stanke et al 2013 cioffi et al 2017 forzieri et al 2017 categorised as a natural hazard by the world meteorological organization wmo droughts are technically defined as a period of lower than average precipitation failing to meet human and environmental hydrological demands wmo 2008 within the sphere of emerging climatic hazards droughts are frequently considered the least understood being classified as complex cumulative slow onset i e creeping persistent and regionally extensive pulwarty and sivakumar 2014 depending on their duration severity and impact droughts may be classified into four types namely meteorological hydrological agricultural and socio economic van loon 2015 irrespective of classification drought events serve to deplete available freshwater resources thus altering hydrodynamic regimes in both surface water and groundwater dominated catchments with environmental and socio economic impacts frequently outlasting the drought period kayam et al 2009 mishra and singh 2010 daneshmand et al 2014 notably droughts represent a relative as opposed to absolute condition as opposed to flooding and are thus contingent on normal i e baseline and antecedent hydrological conditions wilhite 2016 hydrologically groundwater drought is defined as below normal groundwater level s and or storage with depletion of soil water i e holding capacity during prolonged drought resulting in declining groundwater levels hisdal et al 2004 van loon 2015 albeit dependant on antecedent pre event conditions fluctuating recharge rates piezometric surface and groundwater temperatures in the vadose zone and producing aquifers all affect contaminant transport thus affecting both local and regional groundwater quality ghazavi et al 2012 multiple mechanisms including decreased dilution potential decreased subsurface attenuation and retention and decreased aquifer transmissivity may combine to alter the susceptibility of aquifers to both point and diffuse contamination van vliet 2007 shahid et al 2017 moreover periods of drought and particularly those experienced in non arid regions are invariably associated with increased anthropogenic water demand in concurrence with decreased resource availability potentially leading to over exploitation and exacerbation of groundwater quality deterioration stanke et al 2013 typically meteorological drought conditions propagate through the hydrological cycle with surface water resources affected relatively rapidly while groundwater resources are typically the last impacted hydrological component groundwater environments are often associated with an inherent resilience to external stimuli and the capacity to buffer effects of short term climate hazards vaux 2011 sonnenborg et al 2012 however those subsurface attributes which support this buffering capacity e g overlying subsoil type thickness and permeability and the associated temporal decoupling from surface processes may also result in groundwater reserves remaining affected for prolonged periods following pronounced drought events sonnenborg et al 2012 several studies have explored the impacts of drought on groundwater yields panda et al 2012 lee et al 2019 and chemical contamination kampbell et al 2003 appleyard and cook 2009 polemio et al 2009 however there is a paucity of research which has explored the impacts of drought conditions on the microbial quality of domestic groundwater supplies overall investigations focusing on the nexus between drought events groundwater quality and potential waterborne human infections remain extremely limited within the scientific literature for example work by levy et al 2016 which investigated the impact of climate change on waterborne diseases noted that literature relating to drought and disease was particularly sparse even when including all water exposures i e surface water groundwater etc the pronounced meteorological drought experienced across europe during summer 2018 presented a unique opportunity to investigate the effects of an extended period of low rainfall and high relative temperatures on the incidence of faecal indicator organisms fios in unregulated domestic groundwater supplies a repeated measures fieldwork sampling campaign during and post event was undertaken followed by statistical risk factor analyses to evaluate the regional and local hydrodynamics of fio presence as such this study aims to provide a critical and novel characterization of the impacts of a meteorological and hydrological drought on groundwater microbial quality in a temperate maritime region not normally characterised by drought occurrence study findings may provide some clarity around the effects of a sporadic drought event on groundwater microbiological parameters and associated contaminant sources and pathways while limited in scope due to the one off nature of the event presented results are directly relevant to a range of stakeholders and provide key feedback with applications in safeguarding against human health effects linked to climate change and exposure to potentially pathogenic microorganisms moreover this research provides valuable insight for future research highlighting potential sources and pathways for microbial contamination in groundwater under drought conditions 2 methods 2 1 study region the study region is situated in the south west of the republic of ireland roi extending 6800 km2 8 1 of total area of roi largely comprising the administrative county of cork fig 1 the roi has a temperate maritime climate cfb under the k√∂ppen climate classification system however both annual precipitation 30 year annual mean 977 6 mm and relative humidity 30 year annual mean 71 9 are somewhat higher regionally than national averages due to the coastal atlantic location in addition to a slightly lower mean annual temperature 10 7 c met eireann 2020a met eireann 2020b like much of ireland county cork is characterised by relatively high private groundwater reliance with 23 014 domestic i e one household supplied groundwater supply sources in operation equivalent to 16 of households across the county cso 2012 a significant majority of groundwater wells are located in categorically rural areas 98 4 n 22 651 where access to public infrastructure i e municipal mains is limited regional bedrock geology is dominated by devonian and lower carboniferous sandstones siltstones and mudstones of the munster basin in areas of topographic relief with a limited number of low lying areas underlain by lower carboniferous limestone formations meere et al 2013 the sandstones have limited fracture permeability and are classified as locally productive aquifers whereas solution enlargement of limestone fissures has given rise to a high permeability karstic flow regime locally kelly et al 2015 regionally well drained soils predominate resulting in a high prevalence of areas characterised by extreme 33 and high 32 groundwater vulnerability in addition to those underlain by thin 1m or absent top soil layers i e outcrop subcrop or karst 21 gsi 2019 the prevalence of high permeability soils is further reflected by estimated recharge coefficients 40 of the county is characterised by intermediate 40 of effective precipitation available for recharge to high 70 90 recharge capacity gsi 2019 depth to bedrock is reflected in the groundwater vulnerability classification with extreme vulnerability areas representative of bedrock located between 0 and 3 m from the surface similarly high vulnerability areas are typically characterised as having 3 5 m depth to bedrock overlain with a moderately permeable till overall local geological characteristics and associated parameters indicate a significant degree of groundwater susceptibility to external i e surface stimuli 2 2 the 2018 european drought an irish perspective while largely unaffected by temperature extremes recent analyses of historical data 1850 2015 indicate that the roi has exhibited an increasing tendency towards wetter winters and drier summers murphy et al 2019 measured surface temperatures have increased by 0 8 c since 1900 an average of 0 06 c per decade until 1979 increasing to 0 14 c per decade between 1980 and 2008 coll and sweeney 2013 climate projections indicate regional conditions will deteriorate with even wetter winters and drier and warmer summers expected thus increasing the likelihood and intensity i e duration of future drought events murphy et al 2019 the potential impacts of climate change in the roi were brought into sharp focus during the summer of 2018 when a meteorological drought was recorded from june 25th to july 14th absolute drought conditions defined as no precipitation recorded for 15 days met eireann 2020a were recorded at 21 out of 25 meteorological stations with partial drought conditions enduring until july 24th in parts of the country records of total summer may july rainfall 109 5 mm at cork airport fig 1 represented the driest summer on local record 56 years in duration concomitantly heat wave conditions were recorded at 15 stations throughout the country at various times between june 24th and july 4th with temperatures of 32 0 c recorded in the mid west shannon airport county clare on june 28th the highest temperature ever recorded at a mainland irish synoptic station met eireann 2018 national estimates of drought indicators 2018 including standardised precipitation index spi mckee et al 1993 and percent of normal index pni werick et al 1994 indicate pronounced drought conditions occurred during june and july peaking in june subject to some degree of regional variability falzoi et al 2019 the effects of the drought were particularly marked in south west ireland which contains the study area fig 1 characterized by extremely dry and extreme drought 40 of normal precipitation spi and pni classifications respectively reported trends are mirrored in regional thermo pluviometric data collated from all available local synoptic weather stations fig 1 key features include significantly lower rates of rainfall 10 mm in july 2018 and higher maximum air temperatures 7 c during drought months compared with 10 year mean trends fig 2 data also demonstrate atypical pre event climatic conditions falzoi et al 2019 with unusually low levels of precipitation prevailing throughout may and a summer long 1st june to 31st august rainfall deficit values derived from the national soil moisture deficit smd model schulte et al 2005 i e the volume of infiltrating precipitation mm necessary to attain soil field saturation have been employed to estimate hydrological dynamics within soil environments and thus provide a soil specific drought metric fig 3 provides smd data extracted from cork airport synoptic station with extremely high values 75 80 mm calculated during the drought period note 110 mm values reflect soils entirely devoid of available moisture indicative of a high soil water deficit and altered surface groundwater hydrological connectivity met eireann 2020b 2 3 supply source selection and study design private well owners were invited to participate in the present study by e mail via a university college cork ucc list server participants were selected based on the following criteria a willingness to commit to two sampling rounds b no treatment system installed c proximity to research laboratory to ensure prompt analysis and d to ensure a level of diversity in terms of groundwater vulnerability overall 74 suitable groundwater supply sources were identified and sampled 61 74 82 4 of which had never been sampled before thus reducing bias relating to historically defective supplies with a temporal i e repeated measures sampling campaign undertaken comprising two distinct phases representative of drought june july 2018 and post drought october november 2018 conditions resulting in 148 private groundwater samples collected and analysed post drought samples were collected approximately 3 4 months after the drought event october november 2018 a period considered adequate to allow restoration of baseline seasonal hydro geo logical conditions with respect to surface groundwater connectivity and soil moisture conditions fig 3 pluvial data fig 4 highlight the marked difference between the two sampling periods with a cumulative variance of 30 4 mm rainfall during the two 8 week sampling periods as expected mean and maximum temperatures were also markedly higher 9 c during june july following 10 year averages fig 2 monthly trends suggest rainfall had significantly recovered by the second sampling phase although october 2018 still exhibited lower rainfall than average in turn smd values indicate soil hydrology had recovered from the deficits experienced during the drought with values comparable with long term averages fig 3 groundwater sampling was carried out in accordance with standard methods for the examination of water and wastewater apha 2005 untreated samples 100 ml were taken directly from a pre sterilised 70 ethanol kitchen or outdoor tap after a flushing period relative to temperature stabilization generally between 2 and 6 min and collected in sterile disposable 120 ml sampling bottles samples were immediately transferred to a cooler box and transported to a laboratory with analysis undertaken within 4 6 h all samples were assayed for total coliforms tcs and escherichia coli using a standard us environmental protection agency usepa approved commercial culture kit colilert idexx laboratories inc westbrook me usa and in accordance with manufacturer s specifications negative controls sterile deionised water were used during all phases of laboratory analyses 2 4 contamination risk factors and dataset development variables associated with three risk factor categories source infrastructure contaminant source setback adjacency and hydrogeological setting were collated and spatially geo matched to source specific geographical location gps coordinates table 1 site specific infrastructural data were collated via a participant survey completed by all well owners during the first drought sampling phase source depth and age were coded as discretized ranked variables e g 1 5 m 2 5 20 m 3 21 50 m 4 51 100 m while well type bored or dug well and presence absence of contaminant sources within 100 m e g septic tank presence absence were coded as dichotomous categorical variables see supplementary material the geographical coordinates of each sampling point were acquired through requisition of participants unique eircode irish national postcode system converted to gps coordinates and added to a geo spatial database created in arcmap 10 3 contaminant source data were subsequently also sourced from existing national databases table 1 agricultural cattle sheep numbers and wastewater septic tank unit number data were extracted from the central statistics office cso census of agriculture 2010 and census of ireland 2016 datasets respectively cso data were compiled and spatially indexed to one of 3440 electoral divisions eds these represent the smallest administrative unit in the roi table 1 similarly local hydrogeological data obtained from the geological survey ireland gsi were joined to each sampled groundwater supply table 1 subsoil permeability was discretised ranked and coded ranging from low permeability 1 to thin or absent 4 analogous to o dwyer et al 2018 groundwater recharge estimates were collated from the national groundwater recharge map hunter williams et al 2013 which is derived from existing hydrogeological and meteorological data layers meteorological data were sourced from the irish meteorological office met eireann and the 5 day antecedent rainfall mm relative to each date of sampling was compiled using the cork airport synoptic station as the most representative of the study area 2 5 statistical analysis prior to analyses all independent variables were evaluated for normality via q q plots and shapiro wilkes tests numerous variables exhibited a non normal distribution thus non parametric analyses were employed for all subsequent analyses a mcnemars exact test was used to evaluate the statistical association between the paired dependant variables of interest i e presence absence of tcs and ec during drought and post drought conditions bivariate risk factor analyses were undertaken using the mann whitney u or chi square tests as appropriate to determine the level of association among e coli presence absence dependant variable and identified risk factors independent variables table 1 under both drought and post drought conditions following bivariate analyses variables exhibiting significance at the 90 confidence interval were selected for inclusion in two logistic regression models drought and post drought a forced entry method was employed whereby all variables were analysed simultaneously followed by backward eliminated of variables which contributed least to the model p 0 1 the hosmer lemeshow test was used to validate model goodness of fit with nagelkerke s pseudo r2 used to estimate effect size and explained system variance ibm spss 26 was employed for all statistical analyses with a confidence level of 95 Œ± 0 05 employed throughout by convention 3 results 3 1 general contamination status summary statistics for supply source contamination during drought and non drought sampling periods are presented in table 2 during drought conditions 56 8 n 42 74 and 9 5 n 7 74 of supply sources tested positive for tcs and e coli respectively comparatively upon alleviation of drought conditions tcs and e coli were detected in 56 n 42 74 and 24 3 n 18 74 of private wells an exact mcnemar s test determined that there was a statistically significant difference in e coli presence during and post drought x2 6 722 p 0 008 however the same relationship was not present for tcs with analogous detection rates encountered during both sampling periods a total of 6 and 22 supply sources exhibited dual i e repeated detection of e coli figs 5 and 6 and tcs during the two sampling phases respectively 3 2 risk factor analysis summary statistics derived from risk factor analysis specific to drought and post drought conditions are presented in tables 3 and 4 with key trends described as follows 3 2 1 drought conditions as shown table 3 no collated infrastructural variables i e well type depth age were significantly associated with e coli presence however within the contaminant source category two variables were statistically associated with e coli occurrence namely presence of an on site septic tank x2 9 761 p 0 008 and electoral division ed specific septic tank density u 3 407 p 0 001 in supplies where e coli was detected the mean density of septic tanks per ed was significantly higher n 349 s d 116 6 compared to areas where e coli was absent n 168 9 s d 118 05 local subsoil type was the only additional variable significantly associated 95 confidence level with e coli presence x2 20 345 p 0 009 e coli detection was significantly higher in sources situated in lower palaeozoic sandstone tills n 6 additionally subsoil permeability x2 6 676 p 0 083 and groundwater vulnerability x2 6 914 p 0 075 were also significant albeit at the 90 confidence level with subsoil permeability and groundwater vulnerability deemed to be colinear following bivariate analysis variables n 5 associated with e coli presence at the 90 confidence level or above p 0 1 were included in a logistic regression model prior to the application of backwards elimination whereby variables which contributed least to the model p 0 1 were removed n 4 the number of septic tanks per area was the sole variable deemed statistically satisfactory for inclusion in the final model model outputs indicate that during drought conditions the likelihood of e coli contamination increases by a factor of 1 009 95 ci 1 003 1 015 p 0 002 for every additional septic tank system in an area overall model predictive sensitivity was skewed with e coli absence correctly classified in 97 1 of samples while e coli presence was correctly classified in 14 3 of samples the hosmer and lemeshow test for goodness of fit produced an insignificant p value p 0 927 thus the null hypothesis that the observed and expected event rates e coli present absent are matched within subgroups of the sample population is accepted the nagelkerke coefficient of determination cumulative r2 was 0 284 thus the model input variable explains 28 of system variability 3 2 2 post drought conditions no infrastructural variables were significantly associated with e coli presence during the post drought period table 4 variables relating to agricultural practices including local presence of manure spreading x2 3 335 p 0 067 and number of cattle per ed x2 1 778 p 0 074 were identified as approaching statistical significance for post drought conditions in situ septic tank presence and septic number per ed were not significantly associated with e coli presence similarly no hydrogeological parameters demonstrated any statistical significance with e coli presence meteorological conditions i e 5 day antecedent rainfall u 31 70 p 0 002 was the only variable to be statistically associated with e coli presence during the post drought sampling regime analogous to the drought sampling regime subsequent to bivariate analysis variables n 3 associated with e coli presence at the 90 confidence level or above p 0 1 were included in a logistic regression model prior to the application of backwards elimination whereby variables which contributed least to the model p 0 1 were removed n 2 in this instance 5 day antecedent rainfall was the primary predictor with an increase in rainfall increasing the likelihood of e coli presence within the sampled supplies or 1 106 95 ci 1 022 1 197 p 0 012 model predictive sensitivity was similarly skewed with e coli absence correctly classified in 93 of samples while e coli presence was correctly classified in 11 1 of samples the hosmer and lemeshow test for goodness of fit produced an insignificant p value p 0 628 and the nagelkerke coefficient of determination cumulative r2 was 0 124 thus the model input variable explains 12 of system variability 4 discussion the presented study represents the first to provide field evidence of the effects of drought on the microbial quality of private groundwater supplies in a temperate maritime climate overall e coli detection rates during drought 9 5 and post drought 24 3 conditions indicate domestic groundwater supply sources are susceptible to frequent faecal contamination irrespective of precipitation patterns and temperatures reported e coli detection rates fail to meet the legislative microbiological standards 0 100 ml set out by the drinking water directive 98 83 ec european commission 1998 lower prevalence of fibs have been reported during drought periods latchmore et al 2020 for example latchmore et al 2020 have recently reported e coli detection rates of 1 6 to 5 2 during summer sampling seasons over an 8 year study period with detection rates of 1 7 to 1 9 during 2016 which was considered a drought year notably the drought associated detection rate found during the current study was significantly higher findings are thus highly relevant for groundwater consumers and public health authorities particularly considering the predominant lack of treatment among supplies analysed 59 74 79 7 and potential health risks associated with consumption of contaminated groundwater o dwyer et al 2018 a significantly higher e coli detection rate was recorded during the post drought sampling regime indicating enhanced microbial mobilization to groundwater supplies following resumption of normal hydrogeological conditions determined by groundwater physiochemistry of nearby monitoring wells interestingly even within the relatively short sampling timeframe 2 months a statistical association was found between 5 day antecedent rainfall and the presence of e coli within sampled supplies further substantiating the role of precipitation in contaminant transport within irish groundwater supplies hynds et al 2012 o dwyer et al 2014 o dwyer et al 2018 however it must be noted that during the post drought sampling event the precipitation values were marginally above the 10 year mean trend 6 mm in november fig 2 which may have increased the likelihood of microbial contamination for example carlton et al 2014 have previously shown that rainfall events occurring after an 8 week dry period enhanced microbial transfer capacity via runoff and surface water consequently leading to increased detection rates and prevalence of pathogenic and non pathogenic e coli in drinking water sources notwithstanding presented findings i e higher prevalence of e coli during wet conditions concur with previously reported seasonal trends with respect to the occurrence of e coli in groundwater supplies during normal conditions in hydrodynamically predictable regions e g leber et al 2011 shrestha et al 2014 chuah and ziegler 2018 for example bacci and chapman 2011 report a thermotolerant coliform incidence rate of 24 based on microbial analyses of private wells n 75 from a similar geographical region however this is lower than e coli estimates reported from elsewhere in the roi e g 51 4 o dwyer et al 2018 risk factor analysis and subsequent comparison between drought and post drought periods highlight the relevance of two specific hazard sources for source supply contamination during drought conditions both the presence of a septic tank p 0 008 and the number of septic tanks in the locality p 0 001 were associated with e coli presence with the latter providing predictive capacity p 0 012 groundwater source susceptibility i e environmental fate of fios modelling by hynds et al 2012 and o dwyer et al 2018 have previously found that two or more mechanisms typically co occur both spatially and temporally to result in private source contamination for example o dwyer et al 2018 report that intrinsic e g aquifer classification presence of karst bedrocks specific e g local livestock density local septic tank density and infrastructural e g individual source depth and type were concurrently predictive of source contamination e coli in the southwest of ireland over a 2 year sampling period likewise hynds et al 2012 report that intrinsic e g bedrock type and infrastructural e g source wellhead finish liner clearance attributes were highly predictive 90 of e coli presence in private wells across five irish study areas from 2008 to 2011 accordingly findings from the current study provide strong evidence of a significant hydrodynamic shift whereby specific mechanisms localised preferential flow predominate across both sampling periods drought and post drought hydrogeological factors were shown to exert negligible influence on e coli contamination with local subsoil type p 0 009 during drought conditions representing the only significant factor at a 95 confidence level representing further evidence to the abovementioned hypothesis i e cessation of intrinsic and infrastructural contamination mechanisms with localised specific mechanisms predominant in the absence of significant aquifer specific attributes i e decreased vertical catchment hydrological connectivity periods of hydrological drought inherently affect subsurface temperature and soil moisture both of which influence e coli survival and inactivation rates in potential sources e g manure sub soil and groundwater environments john and rose 2005 van elsas et al 2012 blaustein et al 2013 levy et al 2016 for example the potential interplay among soil desiccation and compaction concentration of faecal material in dry surfaces and eventual amplification of e coli contamination through post drought rainfall pulsing should be considered yusa et al 2015 levy et al 2016 conversely soil desiccation can lead to higher inactivation of e coli which generally benefit from the environments provided by semi aquatic habitats at higher latitudes ishii and sadowsky 2008 evaluating the potential influence of higher temperatures on e coli inactivation is challenging considering the nexus between sub surface temperatures subsurface microbial competition predation and nutrient availability john and rose 2005 levy et al 2016 while the effects of fluctuating temperatures on non host e coli survival is not fully understood van elsas et al 2012 previous soil microcosm and groundwater based investigations suggest variations in temperature including exposure to higher temperatures 20 c increase deactivation rate of e coli through physiological responses sjogren 1994 john and rose 2005 semenov et al 2007 blaustein et al 2013 as such the exceptionally high relative temperatures up to 32 c recorded during the summer of 2018 may also help explain the substantially lower detection rates observed this is particularly relevant considering the potential for autochthonous e coli populations developing potential phenotypic adaption to the local climate and subsurface buffering brennan et al 2010 considering the concurrent soil moisture deficit during the 2018 drought in concurrence with the reported environmental survival of e coli 6 10 weeks the presence of e coli in groundwater supplies overlain by lower palaeozoic sandstone tills which have a relatively high permeability may be indicative of legacy contamination events thus supporting the hypothesis that e coli naturalisation within groundwater may have occurred as reported elsewhere filip and demnerova 2009 thus the presence of e coli during drought conditions may be indicative of e coli naturalisation i e environmental adaptation within select irish groundwater supplies or during specific climactic periods this potentially important finding requires further investigation as the apparent survival and adaptation of e coli within groundwater would invalidate the use of e coli as a faecal indicator bacterial species hagedorn et al 2011 5 conclusion the presented opportunistic field study offers a rare if not first insight into the relationship between drought conditions and groundwater quality in private unregulated groundwater sources located in temperate maritime climates not typically associated with drought events e coli presence was noted across both sampling regimes drought and post drought underscoring the persistence of microbial contamination in groundwater within the roi and thus the potentially ever present public health threat to private supply users in light of the significantly reduced level of subsurface transport during the sampled drought event i e recharge infiltration it is tentatively hypothesised that e coli contamination identified during summer 2018 represents a significant hydrodynamic shirt whereby localised specific contamination mechanisms predominate with a partial or total cessation of intrinsic and infrastructural mechanisms moreover findings that legacy contamination events i e faecal materials deposited in sub surface prior to drought onset are likely the primary source of e coli in sampled groundwater sources thus pointing to potential naturalisation adaptation of e coli within vulnerable groundwater systems overlain by high permeability tills given the increased threat of drought conditions in temperate maritime regions under expected climate change scenarios the current study shines a light on the potential challenges facing groundwater users while reiterating the persistent issue of microbial contamination of domestic drinking water wells in the roi with available groundwater contamination data generally restricted to expected seasonal cycles i e current information on groundwater microbial contamination during drought events is extremely limited the evidence presented provides key insights into the influence of drought on microbial contamination of private groundwater supplies findings promote the need for further research in this area to increase our understanding of groundwater contamination mechanisms in response to extreme meteorological events credit authorship contribution statement jean o dwyer conceptualization methodology writing original draft writing review editing carlos chique data curation writing original draft john weatherill data curation writing review editing paul hynds conceptualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2021 126669 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
4335,a significant volume of research over the past two decades has highlighted both direct and indirect links between climate change and groundwater quality however to date few studies have sought to explore the relationship s between drought conditions and groundwater quality in i private unregulated groundwater sources or ii temperate maritime climates not commonly prone to drought events the republic of ireland roi represents an appropriate case study due to its high reliance on private groundwater supplies and while the region is largely unaffected by climatological extremes modelling studies indicate that drier summers and drought conditions will increase in frequency accordingly the current study sought to quantify the effects of the summer 2018 drought experienced throughout europe on private groundwater quality in the southwest of ireland via an opportunistic field study a repeated measures sampling campaign comprised of drought june july and post drought october november analyses of 74 wells was undertaken with complementary mapping and statistical analyses both total coliforms tcs and escherichia coli e coli were present during both drought tcs 42 74 56 8 e coli 7 74 9 5 and post drought tcs 42 74 56 8 e coli 18 74 24 3 sampling periods e coli contamination during drought conditions was unexpected due to an absence of recharge or infiltration for microbial transport bivariate analyses suggest a hydrodynamic change with the significance of e coli sources and pathways shown to switch between sampling periods i e a shift from a combination of regional and local site specific contamination mechanisms to solely site specific mechanisms more specifically during drought conditions septic tank density p 0 001 and local subsoil type p 0 009 were both associated with the presence of e coli while neither variable was significant during post drought conditions the current study is the first to provide a quantitative comparison of private groundwater quality during and after a large scale drought event in a temperate maritime climate and may be used to improve our understanding of the effects of extreme events and thus necessary preventative and monitoring strategies going forward keywords groundwater drought climate change microbial contamination domestic water supplies 1 introduction the intergovernmental panel on climate change ipcc estimates that the global mean surface temperature increased by 0 85 c 0 6 1 06 c between 1880 and 2012 predicting a further 2 4 c increase over the next century ipcc 2013 climate change modelling projects a higher incidence and duration of severe weather events including significant flooding and drought conditions posing significant global environmental and human health risks and a particularly unique challenge for regions unaccustomed to severe climatic phenomena stanke et al 2013 cioffi et al 2017 forzieri et al 2017 categorised as a natural hazard by the world meteorological organization wmo droughts are technically defined as a period of lower than average precipitation failing to meet human and environmental hydrological demands wmo 2008 within the sphere of emerging climatic hazards droughts are frequently considered the least understood being classified as complex cumulative slow onset i e creeping persistent and regionally extensive pulwarty and sivakumar 2014 depending on their duration severity and impact droughts may be classified into four types namely meteorological hydrological agricultural and socio economic van loon 2015 irrespective of classification drought events serve to deplete available freshwater resources thus altering hydrodynamic regimes in both surface water and groundwater dominated catchments with environmental and socio economic impacts frequently outlasting the drought period kayam et al 2009 mishra and singh 2010 daneshmand et al 2014 notably droughts represent a relative as opposed to absolute condition as opposed to flooding and are thus contingent on normal i e baseline and antecedent hydrological conditions wilhite 2016 hydrologically groundwater drought is defined as below normal groundwater level s and or storage with depletion of soil water i e holding capacity during prolonged drought resulting in declining groundwater levels hisdal et al 2004 van loon 2015 albeit dependant on antecedent pre event conditions fluctuating recharge rates piezometric surface and groundwater temperatures in the vadose zone and producing aquifers all affect contaminant transport thus affecting both local and regional groundwater quality ghazavi et al 2012 multiple mechanisms including decreased dilution potential decreased subsurface attenuation and retention and decreased aquifer transmissivity may combine to alter the susceptibility of aquifers to both point and diffuse contamination van vliet 2007 shahid et al 2017 moreover periods of drought and particularly those experienced in non arid regions are invariably associated with increased anthropogenic water demand in concurrence with decreased resource availability potentially leading to over exploitation and exacerbation of groundwater quality deterioration stanke et al 2013 typically meteorological drought conditions propagate through the hydrological cycle with surface water resources affected relatively rapidly while groundwater resources are typically the last impacted hydrological component groundwater environments are often associated with an inherent resilience to external stimuli and the capacity to buffer effects of short term climate hazards vaux 2011 sonnenborg et al 2012 however those subsurface attributes which support this buffering capacity e g overlying subsoil type thickness and permeability and the associated temporal decoupling from surface processes may also result in groundwater reserves remaining affected for prolonged periods following pronounced drought events sonnenborg et al 2012 several studies have explored the impacts of drought on groundwater yields panda et al 2012 lee et al 2019 and chemical contamination kampbell et al 2003 appleyard and cook 2009 polemio et al 2009 however there is a paucity of research which has explored the impacts of drought conditions on the microbial quality of domestic groundwater supplies overall investigations focusing on the nexus between drought events groundwater quality and potential waterborne human infections remain extremely limited within the scientific literature for example work by levy et al 2016 which investigated the impact of climate change on waterborne diseases noted that literature relating to drought and disease was particularly sparse even when including all water exposures i e surface water groundwater etc the pronounced meteorological drought experienced across europe during summer 2018 presented a unique opportunity to investigate the effects of an extended period of low rainfall and high relative temperatures on the incidence of faecal indicator organisms fios in unregulated domestic groundwater supplies a repeated measures fieldwork sampling campaign during and post event was undertaken followed by statistical risk factor analyses to evaluate the regional and local hydrodynamics of fio presence as such this study aims to provide a critical and novel characterization of the impacts of a meteorological and hydrological drought on groundwater microbial quality in a temperate maritime region not normally characterised by drought occurrence study findings may provide some clarity around the effects of a sporadic drought event on groundwater microbiological parameters and associated contaminant sources and pathways while limited in scope due to the one off nature of the event presented results are directly relevant to a range of stakeholders and provide key feedback with applications in safeguarding against human health effects linked to climate change and exposure to potentially pathogenic microorganisms moreover this research provides valuable insight for future research highlighting potential sources and pathways for microbial contamination in groundwater under drought conditions 2 methods 2 1 study region the study region is situated in the south west of the republic of ireland roi extending 6800 km2 8 1 of total area of roi largely comprising the administrative county of cork fig 1 the roi has a temperate maritime climate cfb under the k√∂ppen climate classification system however both annual precipitation 30 year annual mean 977 6 mm and relative humidity 30 year annual mean 71 9 are somewhat higher regionally than national averages due to the coastal atlantic location in addition to a slightly lower mean annual temperature 10 7 c met eireann 2020a met eireann 2020b like much of ireland county cork is characterised by relatively high private groundwater reliance with 23 014 domestic i e one household supplied groundwater supply sources in operation equivalent to 16 of households across the county cso 2012 a significant majority of groundwater wells are located in categorically rural areas 98 4 n 22 651 where access to public infrastructure i e municipal mains is limited regional bedrock geology is dominated by devonian and lower carboniferous sandstones siltstones and mudstones of the munster basin in areas of topographic relief with a limited number of low lying areas underlain by lower carboniferous limestone formations meere et al 2013 the sandstones have limited fracture permeability and are classified as locally productive aquifers whereas solution enlargement of limestone fissures has given rise to a high permeability karstic flow regime locally kelly et al 2015 regionally well drained soils predominate resulting in a high prevalence of areas characterised by extreme 33 and high 32 groundwater vulnerability in addition to those underlain by thin 1m or absent top soil layers i e outcrop subcrop or karst 21 gsi 2019 the prevalence of high permeability soils is further reflected by estimated recharge coefficients 40 of the county is characterised by intermediate 40 of effective precipitation available for recharge to high 70 90 recharge capacity gsi 2019 depth to bedrock is reflected in the groundwater vulnerability classification with extreme vulnerability areas representative of bedrock located between 0 and 3 m from the surface similarly high vulnerability areas are typically characterised as having 3 5 m depth to bedrock overlain with a moderately permeable till overall local geological characteristics and associated parameters indicate a significant degree of groundwater susceptibility to external i e surface stimuli 2 2 the 2018 european drought an irish perspective while largely unaffected by temperature extremes recent analyses of historical data 1850 2015 indicate that the roi has exhibited an increasing tendency towards wetter winters and drier summers murphy et al 2019 measured surface temperatures have increased by 0 8 c since 1900 an average of 0 06 c per decade until 1979 increasing to 0 14 c per decade between 1980 and 2008 coll and sweeney 2013 climate projections indicate regional conditions will deteriorate with even wetter winters and drier and warmer summers expected thus increasing the likelihood and intensity i e duration of future drought events murphy et al 2019 the potential impacts of climate change in the roi were brought into sharp focus during the summer of 2018 when a meteorological drought was recorded from june 25th to july 14th absolute drought conditions defined as no precipitation recorded for 15 days met eireann 2020a were recorded at 21 out of 25 meteorological stations with partial drought conditions enduring until july 24th in parts of the country records of total summer may july rainfall 109 5 mm at cork airport fig 1 represented the driest summer on local record 56 years in duration concomitantly heat wave conditions were recorded at 15 stations throughout the country at various times between june 24th and july 4th with temperatures of 32 0 c recorded in the mid west shannon airport county clare on june 28th the highest temperature ever recorded at a mainland irish synoptic station met eireann 2018 national estimates of drought indicators 2018 including standardised precipitation index spi mckee et al 1993 and percent of normal index pni werick et al 1994 indicate pronounced drought conditions occurred during june and july peaking in june subject to some degree of regional variability falzoi et al 2019 the effects of the drought were particularly marked in south west ireland which contains the study area fig 1 characterized by extremely dry and extreme drought 40 of normal precipitation spi and pni classifications respectively reported trends are mirrored in regional thermo pluviometric data collated from all available local synoptic weather stations fig 1 key features include significantly lower rates of rainfall 10 mm in july 2018 and higher maximum air temperatures 7 c during drought months compared with 10 year mean trends fig 2 data also demonstrate atypical pre event climatic conditions falzoi et al 2019 with unusually low levels of precipitation prevailing throughout may and a summer long 1st june to 31st august rainfall deficit values derived from the national soil moisture deficit smd model schulte et al 2005 i e the volume of infiltrating precipitation mm necessary to attain soil field saturation have been employed to estimate hydrological dynamics within soil environments and thus provide a soil specific drought metric fig 3 provides smd data extracted from cork airport synoptic station with extremely high values 75 80 mm calculated during the drought period note 110 mm values reflect soils entirely devoid of available moisture indicative of a high soil water deficit and altered surface groundwater hydrological connectivity met eireann 2020b 2 3 supply source selection and study design private well owners were invited to participate in the present study by e mail via a university college cork ucc list server participants were selected based on the following criteria a willingness to commit to two sampling rounds b no treatment system installed c proximity to research laboratory to ensure prompt analysis and d to ensure a level of diversity in terms of groundwater vulnerability overall 74 suitable groundwater supply sources were identified and sampled 61 74 82 4 of which had never been sampled before thus reducing bias relating to historically defective supplies with a temporal i e repeated measures sampling campaign undertaken comprising two distinct phases representative of drought june july 2018 and post drought october november 2018 conditions resulting in 148 private groundwater samples collected and analysed post drought samples were collected approximately 3 4 months after the drought event october november 2018 a period considered adequate to allow restoration of baseline seasonal hydro geo logical conditions with respect to surface groundwater connectivity and soil moisture conditions fig 3 pluvial data fig 4 highlight the marked difference between the two sampling periods with a cumulative variance of 30 4 mm rainfall during the two 8 week sampling periods as expected mean and maximum temperatures were also markedly higher 9 c during june july following 10 year averages fig 2 monthly trends suggest rainfall had significantly recovered by the second sampling phase although october 2018 still exhibited lower rainfall than average in turn smd values indicate soil hydrology had recovered from the deficits experienced during the drought with values comparable with long term averages fig 3 groundwater sampling was carried out in accordance with standard methods for the examination of water and wastewater apha 2005 untreated samples 100 ml were taken directly from a pre sterilised 70 ethanol kitchen or outdoor tap after a flushing period relative to temperature stabilization generally between 2 and 6 min and collected in sterile disposable 120 ml sampling bottles samples were immediately transferred to a cooler box and transported to a laboratory with analysis undertaken within 4 6 h all samples were assayed for total coliforms tcs and escherichia coli using a standard us environmental protection agency usepa approved commercial culture kit colilert idexx laboratories inc westbrook me usa and in accordance with manufacturer s specifications negative controls sterile deionised water were used during all phases of laboratory analyses 2 4 contamination risk factors and dataset development variables associated with three risk factor categories source infrastructure contaminant source setback adjacency and hydrogeological setting were collated and spatially geo matched to source specific geographical location gps coordinates table 1 site specific infrastructural data were collated via a participant survey completed by all well owners during the first drought sampling phase source depth and age were coded as discretized ranked variables e g 1 5 m 2 5 20 m 3 21 50 m 4 51 100 m while well type bored or dug well and presence absence of contaminant sources within 100 m e g septic tank presence absence were coded as dichotomous categorical variables see supplementary material the geographical coordinates of each sampling point were acquired through requisition of participants unique eircode irish national postcode system converted to gps coordinates and added to a geo spatial database created in arcmap 10 3 contaminant source data were subsequently also sourced from existing national databases table 1 agricultural cattle sheep numbers and wastewater septic tank unit number data were extracted from the central statistics office cso census of agriculture 2010 and census of ireland 2016 datasets respectively cso data were compiled and spatially indexed to one of 3440 electoral divisions eds these represent the smallest administrative unit in the roi table 1 similarly local hydrogeological data obtained from the geological survey ireland gsi were joined to each sampled groundwater supply table 1 subsoil permeability was discretised ranked and coded ranging from low permeability 1 to thin or absent 4 analogous to o dwyer et al 2018 groundwater recharge estimates were collated from the national groundwater recharge map hunter williams et al 2013 which is derived from existing hydrogeological and meteorological data layers meteorological data were sourced from the irish meteorological office met eireann and the 5 day antecedent rainfall mm relative to each date of sampling was compiled using the cork airport synoptic station as the most representative of the study area 2 5 statistical analysis prior to analyses all independent variables were evaluated for normality via q q plots and shapiro wilkes tests numerous variables exhibited a non normal distribution thus non parametric analyses were employed for all subsequent analyses a mcnemars exact test was used to evaluate the statistical association between the paired dependant variables of interest i e presence absence of tcs and ec during drought and post drought conditions bivariate risk factor analyses were undertaken using the mann whitney u or chi square tests as appropriate to determine the level of association among e coli presence absence dependant variable and identified risk factors independent variables table 1 under both drought and post drought conditions following bivariate analyses variables exhibiting significance at the 90 confidence interval were selected for inclusion in two logistic regression models drought and post drought a forced entry method was employed whereby all variables were analysed simultaneously followed by backward eliminated of variables which contributed least to the model p 0 1 the hosmer lemeshow test was used to validate model goodness of fit with nagelkerke s pseudo r2 used to estimate effect size and explained system variance ibm spss 26 was employed for all statistical analyses with a confidence level of 95 Œ± 0 05 employed throughout by convention 3 results 3 1 general contamination status summary statistics for supply source contamination during drought and non drought sampling periods are presented in table 2 during drought conditions 56 8 n 42 74 and 9 5 n 7 74 of supply sources tested positive for tcs and e coli respectively comparatively upon alleviation of drought conditions tcs and e coli were detected in 56 n 42 74 and 24 3 n 18 74 of private wells an exact mcnemar s test determined that there was a statistically significant difference in e coli presence during and post drought x2 6 722 p 0 008 however the same relationship was not present for tcs with analogous detection rates encountered during both sampling periods a total of 6 and 22 supply sources exhibited dual i e repeated detection of e coli figs 5 and 6 and tcs during the two sampling phases respectively 3 2 risk factor analysis summary statistics derived from risk factor analysis specific to drought and post drought conditions are presented in tables 3 and 4 with key trends described as follows 3 2 1 drought conditions as shown table 3 no collated infrastructural variables i e well type depth age were significantly associated with e coli presence however within the contaminant source category two variables were statistically associated with e coli occurrence namely presence of an on site septic tank x2 9 761 p 0 008 and electoral division ed specific septic tank density u 3 407 p 0 001 in supplies where e coli was detected the mean density of septic tanks per ed was significantly higher n 349 s d 116 6 compared to areas where e coli was absent n 168 9 s d 118 05 local subsoil type was the only additional variable significantly associated 95 confidence level with e coli presence x2 20 345 p 0 009 e coli detection was significantly higher in sources situated in lower palaeozoic sandstone tills n 6 additionally subsoil permeability x2 6 676 p 0 083 and groundwater vulnerability x2 6 914 p 0 075 were also significant albeit at the 90 confidence level with subsoil permeability and groundwater vulnerability deemed to be colinear following bivariate analysis variables n 5 associated with e coli presence at the 90 confidence level or above p 0 1 were included in a logistic regression model prior to the application of backwards elimination whereby variables which contributed least to the model p 0 1 were removed n 4 the number of septic tanks per area was the sole variable deemed statistically satisfactory for inclusion in the final model model outputs indicate that during drought conditions the likelihood of e coli contamination increases by a factor of 1 009 95 ci 1 003 1 015 p 0 002 for every additional septic tank system in an area overall model predictive sensitivity was skewed with e coli absence correctly classified in 97 1 of samples while e coli presence was correctly classified in 14 3 of samples the hosmer and lemeshow test for goodness of fit produced an insignificant p value p 0 927 thus the null hypothesis that the observed and expected event rates e coli present absent are matched within subgroups of the sample population is accepted the nagelkerke coefficient of determination cumulative r2 was 0 284 thus the model input variable explains 28 of system variability 3 2 2 post drought conditions no infrastructural variables were significantly associated with e coli presence during the post drought period table 4 variables relating to agricultural practices including local presence of manure spreading x2 3 335 p 0 067 and number of cattle per ed x2 1 778 p 0 074 were identified as approaching statistical significance for post drought conditions in situ septic tank presence and septic number per ed were not significantly associated with e coli presence similarly no hydrogeological parameters demonstrated any statistical significance with e coli presence meteorological conditions i e 5 day antecedent rainfall u 31 70 p 0 002 was the only variable to be statistically associated with e coli presence during the post drought sampling regime analogous to the drought sampling regime subsequent to bivariate analysis variables n 3 associated with e coli presence at the 90 confidence level or above p 0 1 were included in a logistic regression model prior to the application of backwards elimination whereby variables which contributed least to the model p 0 1 were removed n 2 in this instance 5 day antecedent rainfall was the primary predictor with an increase in rainfall increasing the likelihood of e coli presence within the sampled supplies or 1 106 95 ci 1 022 1 197 p 0 012 model predictive sensitivity was similarly skewed with e coli absence correctly classified in 93 of samples while e coli presence was correctly classified in 11 1 of samples the hosmer and lemeshow test for goodness of fit produced an insignificant p value p 0 628 and the nagelkerke coefficient of determination cumulative r2 was 0 124 thus the model input variable explains 12 of system variability 4 discussion the presented study represents the first to provide field evidence of the effects of drought on the microbial quality of private groundwater supplies in a temperate maritime climate overall e coli detection rates during drought 9 5 and post drought 24 3 conditions indicate domestic groundwater supply sources are susceptible to frequent faecal contamination irrespective of precipitation patterns and temperatures reported e coli detection rates fail to meet the legislative microbiological standards 0 100 ml set out by the drinking water directive 98 83 ec european commission 1998 lower prevalence of fibs have been reported during drought periods latchmore et al 2020 for example latchmore et al 2020 have recently reported e coli detection rates of 1 6 to 5 2 during summer sampling seasons over an 8 year study period with detection rates of 1 7 to 1 9 during 2016 which was considered a drought year notably the drought associated detection rate found during the current study was significantly higher findings are thus highly relevant for groundwater consumers and public health authorities particularly considering the predominant lack of treatment among supplies analysed 59 74 79 7 and potential health risks associated with consumption of contaminated groundwater o dwyer et al 2018 a significantly higher e coli detection rate was recorded during the post drought sampling regime indicating enhanced microbial mobilization to groundwater supplies following resumption of normal hydrogeological conditions determined by groundwater physiochemistry of nearby monitoring wells interestingly even within the relatively short sampling timeframe 2 months a statistical association was found between 5 day antecedent rainfall and the presence of e coli within sampled supplies further substantiating the role of precipitation in contaminant transport within irish groundwater supplies hynds et al 2012 o dwyer et al 2014 o dwyer et al 2018 however it must be noted that during the post drought sampling event the precipitation values were marginally above the 10 year mean trend 6 mm in november fig 2 which may have increased the likelihood of microbial contamination for example carlton et al 2014 have previously shown that rainfall events occurring after an 8 week dry period enhanced microbial transfer capacity via runoff and surface water consequently leading to increased detection rates and prevalence of pathogenic and non pathogenic e coli in drinking water sources notwithstanding presented findings i e higher prevalence of e coli during wet conditions concur with previously reported seasonal trends with respect to the occurrence of e coli in groundwater supplies during normal conditions in hydrodynamically predictable regions e g leber et al 2011 shrestha et al 2014 chuah and ziegler 2018 for example bacci and chapman 2011 report a thermotolerant coliform incidence rate of 24 based on microbial analyses of private wells n 75 from a similar geographical region however this is lower than e coli estimates reported from elsewhere in the roi e g 51 4 o dwyer et al 2018 risk factor analysis and subsequent comparison between drought and post drought periods highlight the relevance of two specific hazard sources for source supply contamination during drought conditions both the presence of a septic tank p 0 008 and the number of septic tanks in the locality p 0 001 were associated with e coli presence with the latter providing predictive capacity p 0 012 groundwater source susceptibility i e environmental fate of fios modelling by hynds et al 2012 and o dwyer et al 2018 have previously found that two or more mechanisms typically co occur both spatially and temporally to result in private source contamination for example o dwyer et al 2018 report that intrinsic e g aquifer classification presence of karst bedrocks specific e g local livestock density local septic tank density and infrastructural e g individual source depth and type were concurrently predictive of source contamination e coli in the southwest of ireland over a 2 year sampling period likewise hynds et al 2012 report that intrinsic e g bedrock type and infrastructural e g source wellhead finish liner clearance attributes were highly predictive 90 of e coli presence in private wells across five irish study areas from 2008 to 2011 accordingly findings from the current study provide strong evidence of a significant hydrodynamic shift whereby specific mechanisms localised preferential flow predominate across both sampling periods drought and post drought hydrogeological factors were shown to exert negligible influence on e coli contamination with local subsoil type p 0 009 during drought conditions representing the only significant factor at a 95 confidence level representing further evidence to the abovementioned hypothesis i e cessation of intrinsic and infrastructural contamination mechanisms with localised specific mechanisms predominant in the absence of significant aquifer specific attributes i e decreased vertical catchment hydrological connectivity periods of hydrological drought inherently affect subsurface temperature and soil moisture both of which influence e coli survival and inactivation rates in potential sources e g manure sub soil and groundwater environments john and rose 2005 van elsas et al 2012 blaustein et al 2013 levy et al 2016 for example the potential interplay among soil desiccation and compaction concentration of faecal material in dry surfaces and eventual amplification of e coli contamination through post drought rainfall pulsing should be considered yusa et al 2015 levy et al 2016 conversely soil desiccation can lead to higher inactivation of e coli which generally benefit from the environments provided by semi aquatic habitats at higher latitudes ishii and sadowsky 2008 evaluating the potential influence of higher temperatures on e coli inactivation is challenging considering the nexus between sub surface temperatures subsurface microbial competition predation and nutrient availability john and rose 2005 levy et al 2016 while the effects of fluctuating temperatures on non host e coli survival is not fully understood van elsas et al 2012 previous soil microcosm and groundwater based investigations suggest variations in temperature including exposure to higher temperatures 20 c increase deactivation rate of e coli through physiological responses sjogren 1994 john and rose 2005 semenov et al 2007 blaustein et al 2013 as such the exceptionally high relative temperatures up to 32 c recorded during the summer of 2018 may also help explain the substantially lower detection rates observed this is particularly relevant considering the potential for autochthonous e coli populations developing potential phenotypic adaption to the local climate and subsurface buffering brennan et al 2010 considering the concurrent soil moisture deficit during the 2018 drought in concurrence with the reported environmental survival of e coli 6 10 weeks the presence of e coli in groundwater supplies overlain by lower palaeozoic sandstone tills which have a relatively high permeability may be indicative of legacy contamination events thus supporting the hypothesis that e coli naturalisation within groundwater may have occurred as reported elsewhere filip and demnerova 2009 thus the presence of e coli during drought conditions may be indicative of e coli naturalisation i e environmental adaptation within select irish groundwater supplies or during specific climactic periods this potentially important finding requires further investigation as the apparent survival and adaptation of e coli within groundwater would invalidate the use of e coli as a faecal indicator bacterial species hagedorn et al 2011 5 conclusion the presented opportunistic field study offers a rare if not first insight into the relationship between drought conditions and groundwater quality in private unregulated groundwater sources located in temperate maritime climates not typically associated with drought events e coli presence was noted across both sampling regimes drought and post drought underscoring the persistence of microbial contamination in groundwater within the roi and thus the potentially ever present public health threat to private supply users in light of the significantly reduced level of subsurface transport during the sampled drought event i e recharge infiltration it is tentatively hypothesised that e coli contamination identified during summer 2018 represents a significant hydrodynamic shirt whereby localised specific contamination mechanisms predominate with a partial or total cessation of intrinsic and infrastructural mechanisms moreover findings that legacy contamination events i e faecal materials deposited in sub surface prior to drought onset are likely the primary source of e coli in sampled groundwater sources thus pointing to potential naturalisation adaptation of e coli within vulnerable groundwater systems overlain by high permeability tills given the increased threat of drought conditions in temperate maritime regions under expected climate change scenarios the current study shines a light on the potential challenges facing groundwater users while reiterating the persistent issue of microbial contamination of domestic drinking water wells in the roi with available groundwater contamination data generally restricted to expected seasonal cycles i e current information on groundwater microbial contamination during drought events is extremely limited the evidence presented provides key insights into the influence of drought on microbial contamination of private groundwater supplies findings promote the need for further research in this area to increase our understanding of groundwater contamination mechanisms in response to extreme meteorological events credit authorship contribution statement jean o dwyer conceptualization methodology writing original draft writing review editing carlos chique data curation writing original draft john weatherill data curation writing review editing paul hynds conceptualization writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2021 126669 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
4336,in this study two types of seasonal long short term memory lstm artificial neural networks named sequenced lstm slstm and wavelet lstm wlstm were presented to model runoff sediment process of three gauging stations located in missouri and upper mississippi regions in both daily and monthly scales for this purpose twenty year observed streamflow and suspended sediment load ssl data were employed in both daily and monthly scales the proposed seasonal models have full profits of classic lstm model in time series processing and handle sole lstm model s weaknesses in failing to capture seasonal information of the process which usually exist in hydro climate time series the proposed models enhanced the long short autoregressive dependency of runoff sediment data by taking into consideration of very long seasonal dependency of data the obtained outputs indicate the outperformance of proposed seasonal lstm models to the classic lstm and feed forward neural network models in test step up to about 25 and 28 in daily and monthly scales respectively keywords sediment load modelling deep learning long short term memory model wavelet transform mississippi river missouri river 1 introduction in different fields of water resource and environmental engineering like water supply flood control irrigation and water quality runoff and sedimentation the process modeling is a vital issue for precise estimation of relationship between the phenomena to well planning and management of water resources hence estimating the suspended sediment load ssl of a watershed is a crucial task for decision makers and managers which necessary controls are taken according to their precise assessment to avert the blockage of the sediments in a region of the basin and its future problems moeeni and bonakdari 2018 negm et al 2018 considering the stochastic nature of runoff sediment and since the historical data often change dramatically within a short long time simulating the runoff sediment process is a complicated task for engineers in the last decades with the development of artificial intelligence ai techniques the sedimentation modeling gets more accurate and effective with regard to the non linear nature and complexity of the employed data for examples feed forward neural network ffnn nagy et al 2002 buyukyildiz and kumcu 2017 fuzzy logic fl rajaee et al 2009 azamathulla et al 2012 support vector regression svr nourani and andalib 2015 nourani et al 2016 and genetic programming gp zounemat kermani et al 2020 are popular ai based approaches for runoff sediment modeling for hydrologists the presence of a long term continuous and homogeneous time series for runoff and sediment brings about quite a few advantages for instance long term trend analysis can bring out starting of trend time trend changes over time and abrupt trend detection in a time series most of the hydrologic models forecast the increment or reduction trends in means of the hydrological components yet further hydrologic models should represent all the deterministic patterns of the hydrological time series such as serial dependence persistence and periodicity this needs realizing current statistical and hydrological features of persistence and periodic characteristics of a long term time series in general features of hydrological time series can be categorize into t√ºrkes et al 2002 i a stochastic component comprised of random events or stable processes ii deterministic components comprised of non random variations of time series in which such deterministic components may further comprise of persistence periodicity seasonality trends jumps or step wise changes catastrophic events e g disasters or extreme events and different combinations of these it is well known that trends of hydrological processes mostly are non linear and rarely purely sinusoidal waves this characteristic is also true in runoff sediment time series conventional ai methods can only process natural data in their raw form while the state of art deep learning dl methods such as long short term memory lstm enable computational algorithms that are comprised of multiple processing layers to discover underlying patterns of data dl methods can identify complex patterns of time series and change its internal variables employing the back propagation bp techniques in modeling by conventional ai models such as ffnn the main features of dataset must be identified and set by the user according to physical understanding via mathematical measures such as correlation coefficient or mutual information while dl methods can automatically discover such a process through the hidden layers the hydrological processes are always represented by typical time sequential data the traditional time series modeling and forecasting generally rely on memoryless methods such as ffnns lecun et al 2015 they simulate the next step value of a time series considering a fixed number window of previous steps lag whereas in lstm dynamic window size lag is considered to find autoregressive dependency of output time series values at next time steps to the pervious values shen 2018 in the field of hydrologic modeling by lstm model kratzert et al 2018 simulated rainfall runoff and the results reveled that lstm has capability of recognizing the long term relation of inputs and targets hu et al 2018 deployed ffnn and lstm models for rainfall runoff modeling based on flood events the outcomes indicated that the two applied models are appropriate for runoff simulation and outperform the conceptual and physical based methods gude et al 2020 evaluated dl methods for gauge height simulation and the associated uncertainty they showed that dl methods can provide more accurate results in comparison to the physical and statistical models while presenting information in 15 min increments rather than six hour increments ying et al 2020 employed elman dl method to simulate the suspended sediment concentration and indicated that suspended sediment concentration estimated by dl method can meet the needs of sediment dynamics research but the calculation effect is not so good in high water stand low water stand fastest flood and fastest ebb periods kaveh et al 2021 evaluated the efficiency of lstm model in estimating suspended sediment concentration in a river in the united states comparing the result of lstm model with that of ffnn and adaptive neuro fuzzy inference system anfis models indicated that lstm model led to more accurate results chen et al 2020 presented self attentive lstm model to forecast the runoff they employed self attention mechanism to simulate interdependencies within short previous time steps and indicated that outperformance of the proposed model was associated with exploitation of information in short lag time vu et al 2021 applied lstm network to fill the gaps of groundwater level time series of an aquifer in france wu et al 2020 provided a methodology by coupling the convolutional neural network cnn with lstm model to combine satellite and gauge precipitation data in this way the spatial information was discovered by cnn and time dependence information was detected by lstm the results revealed the high performance of proposed method fang et al 2021 presented a local spatial sequential lstm model to estimate the flood susceptibility they demonstrated that the proposed model could discover the attribution information of flood conditioning factors and the local spatial information of flood data and the results led to reliable estimation performance such a superiority and efficiency of lstm model was reported by le et al 2019 in the fields of flood forecasting gao et al 2020 and fan et al 2020 in the field of runoff modeling zhang et al 2018 and shin et al 2020 in the field of groundwater modeling although lstm network can extract the autoregressive dependency of data in a time series yet it could not effectively capture seasonality term of time series which is an important component in hydrological processes and by increasing the learning horizon and decreasing the network convergence speed the prediction by classic lstm model is not significantly enhanced yang et al 2019 to address this in this paper two enhanced lstm models as sequenced lstm slstm and wavelet lstm wlstm are proposed and examined for runoff sediment modeling of a river in the slstm model two runoff sediment sequences are innovatively presented comprising of autoregressive sequence as and seasonal sequence ss the as includes runoff sediment observations taking place in vicinity of the prediction target indicating the temporal development of the present runoff and ssl and estimates the smooth variation of ssl the ss includes runoff sediment observations at the same duration in the previous years to efficiently recognize very long term characteristics and dependency of the time series in fact as contains the short term features of time series and includes the real time data while ss includes the long term features of time series considering that one year is the main period of the hydrological phenomena to capture the seasonality component of the process the data with delay of one year was considered as ss as and ss create a new characteristic set next new set is fed to the lstm model to perform calibration task on the other hand in the second proposed model the wavelet transform wt is linked to an lstm model wt is one of the robust mathematical tools which provides a time frequency presentation of an analyzed signal in the time domain daubechies 1990 for analyzing the variations periodicities and trends in time series wt has known as a practical tool in recent years lately new hybrid models integrated with wt have developed for prediction goals for instance wavelet artificial neural networks have successfully been utilized recently for modeling the hydrologic processes nourani et al 2014 potoƒçki et al 2017 such studies reveal that wt reasonably increase the prediction accuracy thus in the second proposed model wlstm time series are first decomposed by the wt and the dominant obtained sub series including the different scales or seasonality components of process are then fed as inputs to the lstm model to predict ssl in future it should be noted that some papers reported that employing wavelet analyze as a pre processing method enhanced the modeling performance but some other demonstrated that it worsened the modeling results sachindra et al 2019 for this reason in addition to wlstm model slstm model was also developed and applied for the modeling it is noteworthy that different methods have been applied in literature for seasonal modeling of various processes containing seasonality components in this way some of the researchers developed individual models for each season chen et al 2019 some others designed models only for one or selected seasons jothiprakash and kote 2011 aranda and garc√≠a bartual 2020 for this purpose either different inputs are considered for each model or a series of joint inputs as well as some different inputs are considered for each season mccrackin et al 2014 on the other hand some researchers detected and considered the periodicity of the hydrological processes employing methods as wavelet analyze for seasonal modeling nourani et al 2014 ehrman et al 2000 in this paper it was tried to capture the seasonal periodic component of the process by developing the wlstm and slstm models and considering that the modeling was not separated to different seasons so same inputs were imposed to all seasons but as in slstm model the data of one year ago are imposed to the model the objective of this paper was to utilize the proposed enhanced seasonal lstm models for runoff sediment modeling one step ahead employing data from three stations located in the missouri and upper mississippi regions through single station and multi station scenarios in daily and monthly scales the reason for including a multi station modeling scenario in this study was that it could handle the uncertainty and non linearity of the ssl modeling via exploring the interaction of stations information by recognizing the spatial and temporal variabilities and consequently could provide more robust results moreover the multi station scenario made it possible to estimate the ssl of the downstream station using only upstream data and therefore measuring ssl values in downstream station becomes unnecessary in future although the dl models could recently find successful applications in different fields of hydrology to the best knowledge of the authors this study presents the first seasonal lstm modeling in the context of hydrology in general and for runoff sediment prediction in particular the rest of the paper is arranged as follow in section 2 the architecture of proposed seasonal lstm models are explained as well as a brief explanation of lstm memory block in the section 3 the obtained results and relative discussion are presented along with relevant tables and figures finally in section 4 the achieved conclusions and suggestions for future studies are provided 2 materials and methods in this section first a brief explanation of lstm memory block is provided second the architecture of two proposed seasonal lstm models i e slstm and wlstm models are described third the basic information about ffnn model is provided finally the employed data and efficiency criteria are presented 2 1 architectures of proposed seasonal lstm models conventional neural networks could not recognize the long short temporal information of sequences to solve this problem particular recurrent neural network rnn structure was already developed that can learn temporal dependency using a loop framework yet by dismantling the current and depending information rnn fails to capture remote dependency initially introduced by hochreiter and schmidhuber 1997 lstm network is a special form of rnn structure it is a chain structure with repetitive memory blocks in which three gate structures are designed fig 1 shows the schematic of an lstm memory block for instance with 5 hidden units and input dimension of 2 the memory unit is safeguarded and controlled by either forgetting deleting or keeping information to the cell unit via the gate lstm network overcomes the back propagated gradient vanishing and explosion via memory unit and handle the rnn weakness the detailed algorithms of an lstm layer are denoted by eqs 1 6 1 f t œÉ w f h t 1 x t b f 2 i t œÉ w i h t 1 x t b i 3 c t t a n h w c h t 1 x t b c 4 c t f t c t 1 i t c t 5 o t œÉ w o h t 1 x t b o 6 h t o t t a n h c t here œÉ is the standard logistics sigmoid function determined by 7 œÉ x 1 1 e x tanh function hyperbolic tangent function is defined as 8 tanh x sinh x cosh x e x e x e x e x the first step in constructing an lstm network is to identify information that is not required and will be eliminated from the cell in that step which is called forget gate ft in this step the sigmoid function determines which part of the old information should be removed by a linear calculation on the current input xt and the previous result ht 1 the linear equations in various steps have various weights w and biases b in each lstm cell the closer to 0 the more it forgets from the previous cell state ct 1 next step is deciding and storing information from the new input xt in the cell state as well as to update the cell state the second gate is the input gate determining what new information is going to be kept by adding it to the cell state in this way first the sigmoid layer decides whether the new information should be updated or ignored 0 or 1 and second the tanh function gives weight to the values which passed by deciding their level of importance 1 to 1 the two values are multiplied to update the new cell state this new memory is then added to old memory ct 1 resulting in ct the third and last gate is the output gate which controls the information of the cell state ct that flows into the new hidden state ht the output values ht is based on the output cell state ot but is a filtered version the lstm layer is accessible as a standard package in most of machine learning softwares e g tensorflow matlab and in this study the lstm layer components of the matlab framework was used for the modeling as a drawback conventional lstm can capture long term information included in time series via lag time expanding that may increase convergence time without any increase in the modeling accuracy yang et al 2019 this study proposed two seasonal lstm models to improve the long term characteristics through which runoff sediment can be more accurately modeled with no increase in the modeling time and cost 2 1 1 sequenced lstm slstm model the proposed slstm not only can detect the autoregressive component of the runoff sediment process but also efficiently recognizes the seasonality dependency of time series as shown in fig 2 input data sets of slstm including as and ss are formed as 9 as t x t n 1 x t n x t 3 x t 2 x t 1 10 ss t x t n k i x t 1 k i x t k i in which t refers to time step n refers to the lag window i is the time step between ss and output and k equals to 365 and 12 for daily and monthly scales respectively as one cycle of the process seasonality and x indicates the input time series here both runoff and sediment it should be noted that as leap years are 366 days and considering that in daily scale 365 days was considered as one cycle of the process seasonality 1 or 2 days would not affect the modeling unless on that day a special and significant event happens and considering that the leap years occurs every 4 years and if few special event happen it can be treated manually as outliers proposed slstm model estimates runoff sediment relationship using as and ss features timing features of runoff sediment are explained via as which includes short term properties of the process even lstm network can extract the long term information trials should be performed considering various time lags different training horizons of temporal properties lstm network could not capture very long temporal information seasonality effectively and by increasing the learning horizon and decreasing the network convergence speed the prediction is not significantly enhanced therefore ss may help to strength ability of model to detect very long properties providing effective estimations of runoff sediment in the same convergence speed network capability to discover long term information within the data could be enhanced employing as and ss as model inputs furthermore there is no need to increase the time lag for deepening the structure as mentioned runoff sediment time series contain trend and periodic terms the trend term indicates the overall change pattern of runoff sediment time series and is self controlled while the periodic term which is affected by external factors indicates the seasonal dependency of data concerning ss and considering the trend of runoff sediment the sss with one year increments were considered in this study since one year is the most dominant cycle in the hydrological processes if the aim is to predict sediment at time t sslt the conjunction of as of discharge asq as of ssl ass ss of discharge ssq and ss of ssl sss should be imposed to the model as inputs to estimate sslt suspended sediment load at time t 2 1 2 wavelet lstm wlstm model the second proposed seasonal lstm model is a hybrid model that links the wt to lstm network wt is one of the mathematical tools that can be helpful as an effective data pre processing when dealing with seasonal and non stationary processes and time series one of the main capabilities of wt is its ability to decompose the main time series into several sub time series each of the obtained sub time series has a specific feature representing a specific frequency or seasonal period in order to reduce the complexity of the original time series wavelet based decomposition is employed to analyze the seasonal feature of the obtained sub time series separately decomposing the main time series into sub series causes the network to apply special weights to each sub series and in fact by decomposing the time series the features of time series could be captured efficiently and it can improve the training step and thus increases the accuracy of the modeling generally known as definite sets of primary functions derived from the disintegration of signals using wt wavelets are scaled and translated copies of a finite length or fast decaying oscillating waveform known as the mother wavelet the analysis of a non stationary series using wt is superior due to wavelets features of being compactly supported zero mean and having irregular shape this irregularly shaped characteristic which is helpful in recognizing signals with changing shapes or discontinuity could be influential to present localized data and functions as well as the compactly assisted belongings it may generate a plenty of data to calculate the wavelet coefficients at every possible scale and it would include a remarkable amount of work to solve this problem and making the analysis much more efficient and accurate at the same time it is suggested to choose scales and positions based on the powers of two dyadic scales and positions this transform is called discrete wt and has the form as addison et al 2001 11 œà m p t œÑ s s 0 m 2 œà t p œÑ 0 s 0 m s 0 m where m and p are integers that control respectively the wavelet dilation scale and translation time s0 is a defined fixed dilation step greater than 1 and œÑ0 is the location parameter and must be greater than zero the most general and simplest choice for the parameters s0 and œÑ0 is 2 and 1 time steps respectively this power of two logarithmic scaling of the translations and dilations is known as dyadic grid arrangement and is the simplest and most reliable case for empirical goals mallat 1989 in fact a wavelet is composed of a set of child wavelets œà m p that are obtained from the main function of œà known as the mother wavelet addison et al 2001 in other words a child wavelet can be obtained by scaling and shifting the mother wavelet in hydrological and water resources wavelet based forecasting commonly adopted the mallat discrete wt algorithm where performing wt would send some amount of future information into forecasting to address this issue and to decompose the future data that are not available a trous at algorithm was used in this study that does not need the future data quilty and adamowski 2018 in the proposed wlstm method the runoff and ssl time series are first decomposed into sub signals at different scales i e a large scale sub signal trend component and several small scale sub signals fluctuations in order to get temporal characteristics of the input signal annual or seasonality are decomposed into large scale sub signal and daily monthly and weekly fluctuations in the small periods are decomposed into small scale sub signals after decomposition of time series by wt the obtained sub series should be fed into the lstm network as mentioned previously employing wavelet sometimes may lead to higher accuracy and sometimes worsened the results nourani et al 2014 if individual model is developed for each decomposed sub series as in some applications such a method is used the error will be magnified in nonlinear modeling so in this regard one model was developed in this study for decomposed sub series instead of developing individual models for each sub series on the other hand imposing all decomposed sub series at once may lead to greater error sachindra et al 2019 to this end dominant decomposed sub series were defined via trial and error or correlation coefficient measure and employed in this paper instead of feeding all the decompositions to the model at once to prevent irrelevant information from entering the model the proposed seasonal lstm models in this study includes an input two hidden i e one lstm and one fully connected layers and an output layers each hidden layer includes five hidden units for a seasonal lstm model some feature sequences asq ssq ass sss or decomposed sub series are first fed into input layer and then to the first hidden layer to recognize and capture different and important temporal information and features fig 3 after that there is a fully connected layer to combine and integrated the obtained pre predictions information of lstm layer and to be further fitted to the simulated ssl and give the final ssl estimations fig 3 depicts the schematic structure of proposed seasonal lstm models it is noteworthy that in slstm and wlstm models the dominant inputs could be determined and employed using measures such as correlation coefficient mutual information or trial and error procedure 2 2 feed forward neural network ffnn the ffnn with bp learning algorithm is well known utilized strategy in dealing with various engineering issues the ffnn was the first kind of artificial neural networks designed the expression feed forward means that the information moves forward and there are no rotations in the network the ffnn is mostly exerted in hydrologic researches as a predicting model for more information refer to asce 2000 the results of proposed seasonal lstm models are compared with the results of classic ffnn model 2 3 data used in the study data from three stations over the missouri and mississippi rivers located in three distinct states i e nebraska illinois missouri were considered to carry out this research see fig 4 the stations belong to three discrete basins and sub basins of the missouri and upper mississippi regions the first station is located in nebraska city and refers to the missouri nishnabotna basin the missouri river is one of the longest river in north america that drains a sparsely populated semi arid watershed of more than 1 3 million square km the missouri river is muddy looking and was nicknamed big muddy since it carries a huge amount of sediment and it contributes more than half the silt that arrives in the gulf of mexico forming the mississippi delta the other two stations belong to upper mississippi salt and upper mississippi meramec basins and are respectively located in i below grafton after the confluence of mississippi and illinois rivers ii in st louis after the confluence of the mississippi and missouri rivers the mississippi river is the longest river in north america draining with its major tributaries an area of approximately 3 1 million square km it has become one of the busiest commercial waterways in the world for being artery of a highly industrialized nation it has also been subjected to a remarkable degree of human control and modification as its the wild neighbor of some of the continent s richest farmlands like a huge funnel the river has taken sediment and debris from contributory areas near the lip of the funnel and deposited much of the product in the alluvial plain of the funnel s spout that shows the interdependence of the entire mississippi region to have an overview of whole characteristics of the basins table 1 presents some general information of the stations and land use statistics of the related sub basins according to fig 4 and table 1 stations a and b are respectively in upstream of the missouri and mississippi rivers and station c is located in downstream of the mississippi river concerning the drainage area station c has the largest drainage area unlike station b that has the smallest as table 1 clearly demonstrates the major land uses of related watershed of station b are agriculture and forest lands whereas station a has fewer green areas than the other two stations this may cause related basin of station a to be subjected to severe soil erosion and consequently larger amount of ssl comparing to station b accordingly it is expected that station a has more interactions with station c due to its unsuitable land cover and land use states mean daily and monthly sum of daily data in a month streamflow and ssl data for a 20 year period from october 1997 to september 2017 derived from united states geological survey usgs website https waterdata usgs gov nwis were used to assess the proposed methodology of this study it worth noting that the first 75 of the data were assigned for calibration and the remaining 25 used as the verification dataset to facilitate training and to improve the accuracy of the models prior to modeling all input data were normalized by nourani et al 2021 12 s norm t s t s m i n s m a x s m i n 1 where s norm t is the normalized value of the observed data s t s m a x and s m i n are the maximum and minimum values of the observed data respectively for better understanding of catchments flow and ssl conditions statistical analysis of data in daily scale are summarized in table 2 as table 2 reports stations a and b have a similar amount of mean ssl values while the mean flow discharge of station b is almost three times bigger than the mean flow discharge of station a that indicates much sedimentation happening in the missouri region not surprisingly both flow discharge and ssl amounts of station c are remarkably larger than the other two stations since it is located after the confluence of the missouri and mississippi rivers furthermore higher values of coefficient of variation cv for ssl compared to the flow discharge data show the higher deviation of ssl data from the mean and describe ssl as a more sporadic phenomenon than the flow rate series itself the modeling was carried out through two scenarios including single station and multi station modeling in order to predict the runoff sediment process one step ahead i first scenario in this scenario as single station modeling it was tried to estimate the ssl of each station in the current time step t using its own historical data ii second scenario in the second scenario as multi station modeling the aim was to predict ssl of station c using discharge values of stations c ssl and discharge values of stations a and b when ssl of station c is not measured because of technical and or financial shortages due to the particular spatial locations of these three stations as can be seen in fig 3 it is expected to represent a practical technique to be used in order to eliminate the need of measuring ssl 2 4 efficiency criteria the coefficient of determination dc or nash sutcliffe efficiency and normalized root mean square error nrmse were employed to analyze and evaluate the performance of the models dc and nrmse of the models can be calculated by following equations nourani 2017 13 dc 1 t 1 n s t s com t 2 t 1 n s t s 2 14 nrmse t 1 n s t s com t 2 n s m a x s m i n where n refers to number of data s t shows the observed data s indicates the mean of observed data s com t shows the output estimated data s m a x and s m i n are the maximum and minimum values of the observed data respectively dc ranges from to 1 with a perfect score of 1 and nrmse ranges from 0 to with the perfect value of 0 it was proved by legates and mccabe 1999 that performance of hydrological and environmental models can be determined by dc and nrmse criteria 3 results and discussion 3 1 results of scenario 1 single station scenario in this study the proposed seasonal lstm models were applied to simulate the runoff sediment process of three stations by two different scenarios in daily and monthly scales in this way after normalizing the data in the first step for scenario 1 the as and ss sequences were created and set as the slstm model inputs for evaluating the capability of slstm network in capturing long temporal patterns various combinations of as and ss were examined and the best combination of as and ss was determined in this way four time series of ass asq sss and ssq were considered as the input candidates of slstm model employing the trial and error procedure the asq ass and sss were determined as the dominant inputs it seems that the impact of ssq could be represented by employing the sss by increasing the lag time modeling convergence time gets longer so detection of long temporal dependency was reinforced by employing ss sequence in the next step for wlstm modeling the ssl and discharge time series were decomposed at level 8 into 9 sub time series one approximation and 8 detailed sub series and level 4 into 5 sub time series one approximation and 4 detailed sub series at the daily and monthly time scales respectively into a trend term and periodic terms by wt employing db4 mother wavelet and the obtained sub series were considered as the candidate inputs of the lstm model in this way first of all the ssl and discharge time series were decomposed and then the approximation and detail sub series were normalized to the range of 0 1 and 1 1 respectively next the trial and error procedure was employed to determine the dominant sub series to fed to the lstm model due to the relative relationship between runoff and ssl it was assumed that both time series included the same frequencies so both time series were decomposed in the same level nourani et al 2019 it should be mentioned that there are different types of mother wavelet which are used in accordance with the type of the process according to previous studies the db4 mother wavelet is more appropriate than other functions in order to simulate the hydrological process nourani et al 2019 thus db4 mother wavelet was employed in this study the reason for choice of 8 and 4 for the daily and monthly scales is that in the dyadic wavelet transform number 2 to the power of 8 and 4 is close to 365 days and 12 months which have been selected as seasonal period in this study for instance fig 5 shows decomposed sub series of ssl time series for station a at level 8 in which s is the ssl time series a8 and di are the approximation and detail sub series respectively as mentioned the normalized sub series of ssl and discharge were considered as candidate inputs of wlstm model and finally 3 dominant time series were determined as inputs of lstm model finally to evaluate the proposed methods in capturing long term information with regard to convergence speed and exactness results of seasonal lstm models were compared with those of lstm real time model employing the only as sequences as well as the classic ffnn model given that despite lstm model the ffnn model just considers a fixed lag time of data to detect autoregressive dependency of data so s t 1 s t 2 q t and q t 1 in daily and s t 1 s t 2 q t and q t 1 in monthly scales were determined as inputs of ffnn model in scenario 1 discharge series of station a were considered with greater lags in comparison with station b that could be related to the remoteness of station a in other words it takes more time for the discharge and sediment load of station a to reach station c it should notice that ffnn model was trained with scheme of scaled conjugate gradient of bp approach which is fast and avoids a time consuming search sharghi et al 2019 furthermore tan sig transfer function was applied for both intermediate and target layers of ffnn models since their nonlinearity in combination with each other can be able to identify far more complex patterns sharghi et al 2019 the hyper parameters of lstm based models determined by trial and error except iterations are tabulated in table 3 all assessments were conducted on a desktop computer with an intel i5 8250u cpu 1 60 ghz 1 80 ghz operating system of microsoft windows 10 professional x64 and 8 gb ram and a graphics card with a normal module nvidia geforce mx 110 results of proposed dl and conventional ffnn models in both daily and monthly time scales via scenario 1 are presented in table 4 the estimated and observed ssl time series in test step for stations a b and c for scenarios 1 in daily scale are presented respectively in figs 6 8 furthermore the scatter plots of test step of wlstm and ffnn methods for stations a b and c for scenarios 1 in daily scale are depicted in fig 9 the most effective and accurate model is marked in bold according to the results presented in table 4 it is clear that performance of the seasonal lstm models are better than the lstm real time up to about 10 and 21 respectively for daily and monthly scales in test phase in terms of dc by almost equal modeling run time the improvement of seasonal lstm models is more considerable in monthly scale regarding that in monthly scale the seasonality component is more dominant and effective in this respect table 4 shows that the proposed seasonal lstm models could successfully discover long temporal pattern without raising the modeling run time and it could lead to more accurate estimations also the seasonal lstm models outperformed ffnn model the results of seasonal lstm models are more accurate than the outputs of ffnn model in test phase in terms of dc up to 23 and 28 respectively for daily and monthly scales since lstm considers dynamic lag to search and detect autoregressive dependency of data while ffnn considers a fixed static lag actually in ffnn model the fixed lag is considered but in lstm model different lags in different steps may be considered according the properties of the time series in various times in other words in the lstm based models the lag time is searched automatically by the model and the dominant lag time in each step is defined dynamically while in ffnn model the lag time should be defined and set in input data by the user whereas in some time spans of modeling long but in some others short term lag of data shall be considered dynamically among two proposed enhanced lstm models the wlstm model yielded more accurate results than the slstm model given that wlstm model is a multi scale method that can detect the underling patterns of data more efficiently regarding that ai methods and specially the dl models are sensitive to the numbers of training samples in single station modeling as table 4 shows the overall outputs indicate the supremacy and robustness of daily scale modeling comparing to the monthly modeling owing to the fact that a large amount of data are involved in daily scale training on the other hand unlike daily scale in which lstm real time model exhibited better results than ffnn model in monthly scale the outcomes of lstm real time and ffnn models were not so different since in daily scale the ssl values may be affected by longer lag times than it is considered manually in ffnn model while lstm real time model could dynamically search longer lag times furthermore by comparing the results of 3 stations it can be seen that the results of 3 stations are almost similar in daily scale while station b led to slightly poor outcomes than two other stations in monthly scale and this may be due to the reason that the station b has more fluctuations which complicates the modeling 3 2 results of scenario 2 multi station scenario the main goal of modeling through scenario 2 was to estimate ssl of the station c through a multi station approach meaning that discharge and ssl of the other two upstream stations were being used in the ssl modeling of station c without employing the ssl data of station c as the input the spatial locations of these three stations indicate a high correlation among them to this end as s a as s b as q a as q b as q c ss s a ss s b ss q a ss q b and ss q c were the input candidates of slstm model in scenario 2 next the as s a as s b as q a as q b and as q c were determined as dominant inputs through trial and error procedure next like scenario 1 wt was employed to decompose the ssl and discharge time series of stations a b and discharge time series of station c into a trend term and periodic terms using db4 mother wavelet at same levels noted in scenario 1 and the dominant sub series were considered as the inputs of wlstm model using the process same as scenario 1 also the hyper parameters of lstm based models in scenario 2 was same as scenario 1 see table 3 moreover s t 10 a s t 6 b q t 10 a q t 6 b q t c q t 1 c in daily and s t 3 a s t 1 b q t 3 a q t 1 b q t c in monthly scales were determined as inputs of ffnn model in scenario 2 in scenario 2 like scenario 1 ffnn model was trained considering tangent sigmoid as activation functions of hidden and output layers the and employing using scaled conjugate gradient scheme of bp algorithm results of proposed dl and conventional ffnn models in both daily and monthly time scales via scenario 2 are presented in table 5 the most effective and accurate model is marked in bold according to the results presented in table 5 and like scenario 1 the performance of the seasonal lstm models was better than the lstm real time up to about 24 and 10 respectively for daily and monthly scales in test phase in terms of dc by almost equal modeling run time same as scenario 1 the results of seasonal lstm models are more accurate than the outputs of ffnn model in test phase in terms of dc up to 25 and 11 respectively for daily and monthly scales and the wlstm model led to more accurate outcomes than slstm model it worth noting that unlike single station modeling in which daily modeling exhibited better results in multi station scenario the monthly outcomes showed more reliable results than the daily modeling as the stations in monthly modeling interact with each other in a long lag time so application of the proposed enhanced lstm models could improve the modeling performance in daily scale more than monthly scale furthermore in scenario 2 different combinations of inputs were investigated to define the interaction degree between stations and to detect the main source of ssl to station c so in one case the data of station a were applied to model ssl of station c and in another case the data of station b were utilized to model ssl of station c the results of both cases were similar which shows the similar interconnection of stations a and b with station c while according to initial supposition station b would have more correlation to station c based on short distance this anomalous outcome could be related to the poor condition of land cover which consequently adds to the sediment load through soil erosion process on the other hand multi station scenario indicated the high interaction of stations and showed the importance of considering the physical aspects and geomorphology features of the study area e g land cover in sediment prediction for example according to table 2 upstream basin of station a has less forest and agricultural lands comparing to station b which causes the basin of station a to be subjected to severe soil erosion and to carry more sediment to station c which was supported by modeling scenario 2 as stations a and b demonstrated almost same correlation with ssl value of station c despite the great distance of station a the estimated and observed ssl time series in test step of station c for scenario 2 in monthly scale are presented in fig 10 according to figs 6 10 almost all models presented acceptable results but the proposed models have more accurate outcomes furthermore as it can be seen from figs 6 10 the proposed models led to more reliable outcomes than conventional ffnn model given that the proposed models unlike the ffnn model consider dynamic lag time and this is significant in estimating the maximum values of time series which usually have longer dependency to the previous values where ffnn model provided underestimations that is more clear in fig 9 b besides according to fig 10 almost all models led to acceptable estimations considering the fact that in scenario 2 none of the models employed the observed ssl values of station c however this multi station scenario could be employed when the ssl data of station c are not available due to financial or technical issues to have a visualized comparison of models performances in testing step taylor diagrams for station c in daily and monthly scales for both modeling scenarios are presented in fig 11 taylor diagram is one of the most useful ways for graphical representation of modeling accuracy based on several statistical indicators taylor 2001 it presents the summary statistical indicators of the measured and predicted ssl including dc and nrmse in other words the taylor diagram offers a reliable graphical depiction of modeling performance according to the estimated and observed values it can be seen from fig 11 that in both time scales seasonal lstm models could improve the accuracy of the prediction results and showed better results 4 conclusions considering the complexity of hydrological time series and on the other hand the high capability of dl methods in discovering the underlying complex patterns of time series the application of two types of newly proposed seasonal lstm models for runoff sediment modeling was investigated in this study discharge and ssl data of three gauging stations in two different hydrological regions of the united states were used via two distinct modeling scenarios in both daily and monthly scales to evaluate the robustness of the proposed models the first scenario as the single station modeling was developed to compute the ssl in the current time step for all stations using each station s own data in the next modeling scenario as the multi station modeling observed data of two upstream stations i e a and b utilized to estimate the ssl of downstream station c without using the ssl records of station c but employing its streamflow data moreover the modeling results were compared with conventional ffnn model by employing the ss sequence in slstm and utilizing wt in wlstm models to detect the long term properties of runoff sediment the proposed models could discover long temporal information more effectively and with no increase in the modeling run time the as and the ss were considered as the input set of slstm model to capture the underlying patterns of various intervals and the slstm leads to better estimations than lstm utilizing as data on the other hand decomposed sub series obtained via wt could discover the trend and periodic components of data properly thus the results proved the superiority of proposed wlstm model to the other applied ai models even lstm real time moreover the seasonal lstm models are designed to be highly scalable and it can incorporate different properties such as runoff and sediment overall the proposed seasonal lstm methodology could enhance the lstm real time and ffnn models efficiency in test step up to 25 and 28 in daily and monthly scales respectively based on the aforementioned investigations the effectiveness of the proposed enhanced lstm models for sediment prediction could be approved other hydrological processes e g groundwater precipitation etc can be also modeled by the proposed seasonal lstm models in this study only yearly frequency was investigated for future studies it is suggested to detect other dominant frequencies to be imposed into the slstm model also it is suggested to employ the proposed methodology to multi step ahead modeling declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
4336,in this study two types of seasonal long short term memory lstm artificial neural networks named sequenced lstm slstm and wavelet lstm wlstm were presented to model runoff sediment process of three gauging stations located in missouri and upper mississippi regions in both daily and monthly scales for this purpose twenty year observed streamflow and suspended sediment load ssl data were employed in both daily and monthly scales the proposed seasonal models have full profits of classic lstm model in time series processing and handle sole lstm model s weaknesses in failing to capture seasonal information of the process which usually exist in hydro climate time series the proposed models enhanced the long short autoregressive dependency of runoff sediment data by taking into consideration of very long seasonal dependency of data the obtained outputs indicate the outperformance of proposed seasonal lstm models to the classic lstm and feed forward neural network models in test step up to about 25 and 28 in daily and monthly scales respectively keywords sediment load modelling deep learning long short term memory model wavelet transform mississippi river missouri river 1 introduction in different fields of water resource and environmental engineering like water supply flood control irrigation and water quality runoff and sedimentation the process modeling is a vital issue for precise estimation of relationship between the phenomena to well planning and management of water resources hence estimating the suspended sediment load ssl of a watershed is a crucial task for decision makers and managers which necessary controls are taken according to their precise assessment to avert the blockage of the sediments in a region of the basin and its future problems moeeni and bonakdari 2018 negm et al 2018 considering the stochastic nature of runoff sediment and since the historical data often change dramatically within a short long time simulating the runoff sediment process is a complicated task for engineers in the last decades with the development of artificial intelligence ai techniques the sedimentation modeling gets more accurate and effective with regard to the non linear nature and complexity of the employed data for examples feed forward neural network ffnn nagy et al 2002 buyukyildiz and kumcu 2017 fuzzy logic fl rajaee et al 2009 azamathulla et al 2012 support vector regression svr nourani and andalib 2015 nourani et al 2016 and genetic programming gp zounemat kermani et al 2020 are popular ai based approaches for runoff sediment modeling for hydrologists the presence of a long term continuous and homogeneous time series for runoff and sediment brings about quite a few advantages for instance long term trend analysis can bring out starting of trend time trend changes over time and abrupt trend detection in a time series most of the hydrologic models forecast the increment or reduction trends in means of the hydrological components yet further hydrologic models should represent all the deterministic patterns of the hydrological time series such as serial dependence persistence and periodicity this needs realizing current statistical and hydrological features of persistence and periodic characteristics of a long term time series in general features of hydrological time series can be categorize into t√ºrkes et al 2002 i a stochastic component comprised of random events or stable processes ii deterministic components comprised of non random variations of time series in which such deterministic components may further comprise of persistence periodicity seasonality trends jumps or step wise changes catastrophic events e g disasters or extreme events and different combinations of these it is well known that trends of hydrological processes mostly are non linear and rarely purely sinusoidal waves this characteristic is also true in runoff sediment time series conventional ai methods can only process natural data in their raw form while the state of art deep learning dl methods such as long short term memory lstm enable computational algorithms that are comprised of multiple processing layers to discover underlying patterns of data dl methods can identify complex patterns of time series and change its internal variables employing the back propagation bp techniques in modeling by conventional ai models such as ffnn the main features of dataset must be identified and set by the user according to physical understanding via mathematical measures such as correlation coefficient or mutual information while dl methods can automatically discover such a process through the hidden layers the hydrological processes are always represented by typical time sequential data the traditional time series modeling and forecasting generally rely on memoryless methods such as ffnns lecun et al 2015 they simulate the next step value of a time series considering a fixed number window of previous steps lag whereas in lstm dynamic window size lag is considered to find autoregressive dependency of output time series values at next time steps to the pervious values shen 2018 in the field of hydrologic modeling by lstm model kratzert et al 2018 simulated rainfall runoff and the results reveled that lstm has capability of recognizing the long term relation of inputs and targets hu et al 2018 deployed ffnn and lstm models for rainfall runoff modeling based on flood events the outcomes indicated that the two applied models are appropriate for runoff simulation and outperform the conceptual and physical based methods gude et al 2020 evaluated dl methods for gauge height simulation and the associated uncertainty they showed that dl methods can provide more accurate results in comparison to the physical and statistical models while presenting information in 15 min increments rather than six hour increments ying et al 2020 employed elman dl method to simulate the suspended sediment concentration and indicated that suspended sediment concentration estimated by dl method can meet the needs of sediment dynamics research but the calculation effect is not so good in high water stand low water stand fastest flood and fastest ebb periods kaveh et al 2021 evaluated the efficiency of lstm model in estimating suspended sediment concentration in a river in the united states comparing the result of lstm model with that of ffnn and adaptive neuro fuzzy inference system anfis models indicated that lstm model led to more accurate results chen et al 2020 presented self attentive lstm model to forecast the runoff they employed self attention mechanism to simulate interdependencies within short previous time steps and indicated that outperformance of the proposed model was associated with exploitation of information in short lag time vu et al 2021 applied lstm network to fill the gaps of groundwater level time series of an aquifer in france wu et al 2020 provided a methodology by coupling the convolutional neural network cnn with lstm model to combine satellite and gauge precipitation data in this way the spatial information was discovered by cnn and time dependence information was detected by lstm the results revealed the high performance of proposed method fang et al 2021 presented a local spatial sequential lstm model to estimate the flood susceptibility they demonstrated that the proposed model could discover the attribution information of flood conditioning factors and the local spatial information of flood data and the results led to reliable estimation performance such a superiority and efficiency of lstm model was reported by le et al 2019 in the fields of flood forecasting gao et al 2020 and fan et al 2020 in the field of runoff modeling zhang et al 2018 and shin et al 2020 in the field of groundwater modeling although lstm network can extract the autoregressive dependency of data in a time series yet it could not effectively capture seasonality term of time series which is an important component in hydrological processes and by increasing the learning horizon and decreasing the network convergence speed the prediction by classic lstm model is not significantly enhanced yang et al 2019 to address this in this paper two enhanced lstm models as sequenced lstm slstm and wavelet lstm wlstm are proposed and examined for runoff sediment modeling of a river in the slstm model two runoff sediment sequences are innovatively presented comprising of autoregressive sequence as and seasonal sequence ss the as includes runoff sediment observations taking place in vicinity of the prediction target indicating the temporal development of the present runoff and ssl and estimates the smooth variation of ssl the ss includes runoff sediment observations at the same duration in the previous years to efficiently recognize very long term characteristics and dependency of the time series in fact as contains the short term features of time series and includes the real time data while ss includes the long term features of time series considering that one year is the main period of the hydrological phenomena to capture the seasonality component of the process the data with delay of one year was considered as ss as and ss create a new characteristic set next new set is fed to the lstm model to perform calibration task on the other hand in the second proposed model the wavelet transform wt is linked to an lstm model wt is one of the robust mathematical tools which provides a time frequency presentation of an analyzed signal in the time domain daubechies 1990 for analyzing the variations periodicities and trends in time series wt has known as a practical tool in recent years lately new hybrid models integrated with wt have developed for prediction goals for instance wavelet artificial neural networks have successfully been utilized recently for modeling the hydrologic processes nourani et al 2014 potoƒçki et al 2017 such studies reveal that wt reasonably increase the prediction accuracy thus in the second proposed model wlstm time series are first decomposed by the wt and the dominant obtained sub series including the different scales or seasonality components of process are then fed as inputs to the lstm model to predict ssl in future it should be noted that some papers reported that employing wavelet analyze as a pre processing method enhanced the modeling performance but some other demonstrated that it worsened the modeling results sachindra et al 2019 for this reason in addition to wlstm model slstm model was also developed and applied for the modeling it is noteworthy that different methods have been applied in literature for seasonal modeling of various processes containing seasonality components in this way some of the researchers developed individual models for each season chen et al 2019 some others designed models only for one or selected seasons jothiprakash and kote 2011 aranda and garc√≠a bartual 2020 for this purpose either different inputs are considered for each model or a series of joint inputs as well as some different inputs are considered for each season mccrackin et al 2014 on the other hand some researchers detected and considered the periodicity of the hydrological processes employing methods as wavelet analyze for seasonal modeling nourani et al 2014 ehrman et al 2000 in this paper it was tried to capture the seasonal periodic component of the process by developing the wlstm and slstm models and considering that the modeling was not separated to different seasons so same inputs were imposed to all seasons but as in slstm model the data of one year ago are imposed to the model the objective of this paper was to utilize the proposed enhanced seasonal lstm models for runoff sediment modeling one step ahead employing data from three stations located in the missouri and upper mississippi regions through single station and multi station scenarios in daily and monthly scales the reason for including a multi station modeling scenario in this study was that it could handle the uncertainty and non linearity of the ssl modeling via exploring the interaction of stations information by recognizing the spatial and temporal variabilities and consequently could provide more robust results moreover the multi station scenario made it possible to estimate the ssl of the downstream station using only upstream data and therefore measuring ssl values in downstream station becomes unnecessary in future although the dl models could recently find successful applications in different fields of hydrology to the best knowledge of the authors this study presents the first seasonal lstm modeling in the context of hydrology in general and for runoff sediment prediction in particular the rest of the paper is arranged as follow in section 2 the architecture of proposed seasonal lstm models are explained as well as a brief explanation of lstm memory block in the section 3 the obtained results and relative discussion are presented along with relevant tables and figures finally in section 4 the achieved conclusions and suggestions for future studies are provided 2 materials and methods in this section first a brief explanation of lstm memory block is provided second the architecture of two proposed seasonal lstm models i e slstm and wlstm models are described third the basic information about ffnn model is provided finally the employed data and efficiency criteria are presented 2 1 architectures of proposed seasonal lstm models conventional neural networks could not recognize the long short temporal information of sequences to solve this problem particular recurrent neural network rnn structure was already developed that can learn temporal dependency using a loop framework yet by dismantling the current and depending information rnn fails to capture remote dependency initially introduced by hochreiter and schmidhuber 1997 lstm network is a special form of rnn structure it is a chain structure with repetitive memory blocks in which three gate structures are designed fig 1 shows the schematic of an lstm memory block for instance with 5 hidden units and input dimension of 2 the memory unit is safeguarded and controlled by either forgetting deleting or keeping information to the cell unit via the gate lstm network overcomes the back propagated gradient vanishing and explosion via memory unit and handle the rnn weakness the detailed algorithms of an lstm layer are denoted by eqs 1 6 1 f t œÉ w f h t 1 x t b f 2 i t œÉ w i h t 1 x t b i 3 c t t a n h w c h t 1 x t b c 4 c t f t c t 1 i t c t 5 o t œÉ w o h t 1 x t b o 6 h t o t t a n h c t here œÉ is the standard logistics sigmoid function determined by 7 œÉ x 1 1 e x tanh function hyperbolic tangent function is defined as 8 tanh x sinh x cosh x e x e x e x e x the first step in constructing an lstm network is to identify information that is not required and will be eliminated from the cell in that step which is called forget gate ft in this step the sigmoid function determines which part of the old information should be removed by a linear calculation on the current input xt and the previous result ht 1 the linear equations in various steps have various weights w and biases b in each lstm cell the closer to 0 the more it forgets from the previous cell state ct 1 next step is deciding and storing information from the new input xt in the cell state as well as to update the cell state the second gate is the input gate determining what new information is going to be kept by adding it to the cell state in this way first the sigmoid layer decides whether the new information should be updated or ignored 0 or 1 and second the tanh function gives weight to the values which passed by deciding their level of importance 1 to 1 the two values are multiplied to update the new cell state this new memory is then added to old memory ct 1 resulting in ct the third and last gate is the output gate which controls the information of the cell state ct that flows into the new hidden state ht the output values ht is based on the output cell state ot but is a filtered version the lstm layer is accessible as a standard package in most of machine learning softwares e g tensorflow matlab and in this study the lstm layer components of the matlab framework was used for the modeling as a drawback conventional lstm can capture long term information included in time series via lag time expanding that may increase convergence time without any increase in the modeling accuracy yang et al 2019 this study proposed two seasonal lstm models to improve the long term characteristics through which runoff sediment can be more accurately modeled with no increase in the modeling time and cost 2 1 1 sequenced lstm slstm model the proposed slstm not only can detect the autoregressive component of the runoff sediment process but also efficiently recognizes the seasonality dependency of time series as shown in fig 2 input data sets of slstm including as and ss are formed as 9 as t x t n 1 x t n x t 3 x t 2 x t 1 10 ss t x t n k i x t 1 k i x t k i in which t refers to time step n refers to the lag window i is the time step between ss and output and k equals to 365 and 12 for daily and monthly scales respectively as one cycle of the process seasonality and x indicates the input time series here both runoff and sediment it should be noted that as leap years are 366 days and considering that in daily scale 365 days was considered as one cycle of the process seasonality 1 or 2 days would not affect the modeling unless on that day a special and significant event happens and considering that the leap years occurs every 4 years and if few special event happen it can be treated manually as outliers proposed slstm model estimates runoff sediment relationship using as and ss features timing features of runoff sediment are explained via as which includes short term properties of the process even lstm network can extract the long term information trials should be performed considering various time lags different training horizons of temporal properties lstm network could not capture very long temporal information seasonality effectively and by increasing the learning horizon and decreasing the network convergence speed the prediction is not significantly enhanced therefore ss may help to strength ability of model to detect very long properties providing effective estimations of runoff sediment in the same convergence speed network capability to discover long term information within the data could be enhanced employing as and ss as model inputs furthermore there is no need to increase the time lag for deepening the structure as mentioned runoff sediment time series contain trend and periodic terms the trend term indicates the overall change pattern of runoff sediment time series and is self controlled while the periodic term which is affected by external factors indicates the seasonal dependency of data concerning ss and considering the trend of runoff sediment the sss with one year increments were considered in this study since one year is the most dominant cycle in the hydrological processes if the aim is to predict sediment at time t sslt the conjunction of as of discharge asq as of ssl ass ss of discharge ssq and ss of ssl sss should be imposed to the model as inputs to estimate sslt suspended sediment load at time t 2 1 2 wavelet lstm wlstm model the second proposed seasonal lstm model is a hybrid model that links the wt to lstm network wt is one of the mathematical tools that can be helpful as an effective data pre processing when dealing with seasonal and non stationary processes and time series one of the main capabilities of wt is its ability to decompose the main time series into several sub time series each of the obtained sub time series has a specific feature representing a specific frequency or seasonal period in order to reduce the complexity of the original time series wavelet based decomposition is employed to analyze the seasonal feature of the obtained sub time series separately decomposing the main time series into sub series causes the network to apply special weights to each sub series and in fact by decomposing the time series the features of time series could be captured efficiently and it can improve the training step and thus increases the accuracy of the modeling generally known as definite sets of primary functions derived from the disintegration of signals using wt wavelets are scaled and translated copies of a finite length or fast decaying oscillating waveform known as the mother wavelet the analysis of a non stationary series using wt is superior due to wavelets features of being compactly supported zero mean and having irregular shape this irregularly shaped characteristic which is helpful in recognizing signals with changing shapes or discontinuity could be influential to present localized data and functions as well as the compactly assisted belongings it may generate a plenty of data to calculate the wavelet coefficients at every possible scale and it would include a remarkable amount of work to solve this problem and making the analysis much more efficient and accurate at the same time it is suggested to choose scales and positions based on the powers of two dyadic scales and positions this transform is called discrete wt and has the form as addison et al 2001 11 œà m p t œÑ s s 0 m 2 œà t p œÑ 0 s 0 m s 0 m where m and p are integers that control respectively the wavelet dilation scale and translation time s0 is a defined fixed dilation step greater than 1 and œÑ0 is the location parameter and must be greater than zero the most general and simplest choice for the parameters s0 and œÑ0 is 2 and 1 time steps respectively this power of two logarithmic scaling of the translations and dilations is known as dyadic grid arrangement and is the simplest and most reliable case for empirical goals mallat 1989 in fact a wavelet is composed of a set of child wavelets œà m p that are obtained from the main function of œà known as the mother wavelet addison et al 2001 in other words a child wavelet can be obtained by scaling and shifting the mother wavelet in hydrological and water resources wavelet based forecasting commonly adopted the mallat discrete wt algorithm where performing wt would send some amount of future information into forecasting to address this issue and to decompose the future data that are not available a trous at algorithm was used in this study that does not need the future data quilty and adamowski 2018 in the proposed wlstm method the runoff and ssl time series are first decomposed into sub signals at different scales i e a large scale sub signal trend component and several small scale sub signals fluctuations in order to get temporal characteristics of the input signal annual or seasonality are decomposed into large scale sub signal and daily monthly and weekly fluctuations in the small periods are decomposed into small scale sub signals after decomposition of time series by wt the obtained sub series should be fed into the lstm network as mentioned previously employing wavelet sometimes may lead to higher accuracy and sometimes worsened the results nourani et al 2014 if individual model is developed for each decomposed sub series as in some applications such a method is used the error will be magnified in nonlinear modeling so in this regard one model was developed in this study for decomposed sub series instead of developing individual models for each sub series on the other hand imposing all decomposed sub series at once may lead to greater error sachindra et al 2019 to this end dominant decomposed sub series were defined via trial and error or correlation coefficient measure and employed in this paper instead of feeding all the decompositions to the model at once to prevent irrelevant information from entering the model the proposed seasonal lstm models in this study includes an input two hidden i e one lstm and one fully connected layers and an output layers each hidden layer includes five hidden units for a seasonal lstm model some feature sequences asq ssq ass sss or decomposed sub series are first fed into input layer and then to the first hidden layer to recognize and capture different and important temporal information and features fig 3 after that there is a fully connected layer to combine and integrated the obtained pre predictions information of lstm layer and to be further fitted to the simulated ssl and give the final ssl estimations fig 3 depicts the schematic structure of proposed seasonal lstm models it is noteworthy that in slstm and wlstm models the dominant inputs could be determined and employed using measures such as correlation coefficient mutual information or trial and error procedure 2 2 feed forward neural network ffnn the ffnn with bp learning algorithm is well known utilized strategy in dealing with various engineering issues the ffnn was the first kind of artificial neural networks designed the expression feed forward means that the information moves forward and there are no rotations in the network the ffnn is mostly exerted in hydrologic researches as a predicting model for more information refer to asce 2000 the results of proposed seasonal lstm models are compared with the results of classic ffnn model 2 3 data used in the study data from three stations over the missouri and mississippi rivers located in three distinct states i e nebraska illinois missouri were considered to carry out this research see fig 4 the stations belong to three discrete basins and sub basins of the missouri and upper mississippi regions the first station is located in nebraska city and refers to the missouri nishnabotna basin the missouri river is one of the longest river in north america that drains a sparsely populated semi arid watershed of more than 1 3 million square km the missouri river is muddy looking and was nicknamed big muddy since it carries a huge amount of sediment and it contributes more than half the silt that arrives in the gulf of mexico forming the mississippi delta the other two stations belong to upper mississippi salt and upper mississippi meramec basins and are respectively located in i below grafton after the confluence of mississippi and illinois rivers ii in st louis after the confluence of the mississippi and missouri rivers the mississippi river is the longest river in north america draining with its major tributaries an area of approximately 3 1 million square km it has become one of the busiest commercial waterways in the world for being artery of a highly industrialized nation it has also been subjected to a remarkable degree of human control and modification as its the wild neighbor of some of the continent s richest farmlands like a huge funnel the river has taken sediment and debris from contributory areas near the lip of the funnel and deposited much of the product in the alluvial plain of the funnel s spout that shows the interdependence of the entire mississippi region to have an overview of whole characteristics of the basins table 1 presents some general information of the stations and land use statistics of the related sub basins according to fig 4 and table 1 stations a and b are respectively in upstream of the missouri and mississippi rivers and station c is located in downstream of the mississippi river concerning the drainage area station c has the largest drainage area unlike station b that has the smallest as table 1 clearly demonstrates the major land uses of related watershed of station b are agriculture and forest lands whereas station a has fewer green areas than the other two stations this may cause related basin of station a to be subjected to severe soil erosion and consequently larger amount of ssl comparing to station b accordingly it is expected that station a has more interactions with station c due to its unsuitable land cover and land use states mean daily and monthly sum of daily data in a month streamflow and ssl data for a 20 year period from october 1997 to september 2017 derived from united states geological survey usgs website https waterdata usgs gov nwis were used to assess the proposed methodology of this study it worth noting that the first 75 of the data were assigned for calibration and the remaining 25 used as the verification dataset to facilitate training and to improve the accuracy of the models prior to modeling all input data were normalized by nourani et al 2021 12 s norm t s t s m i n s m a x s m i n 1 where s norm t is the normalized value of the observed data s t s m a x and s m i n are the maximum and minimum values of the observed data respectively for better understanding of catchments flow and ssl conditions statistical analysis of data in daily scale are summarized in table 2 as table 2 reports stations a and b have a similar amount of mean ssl values while the mean flow discharge of station b is almost three times bigger than the mean flow discharge of station a that indicates much sedimentation happening in the missouri region not surprisingly both flow discharge and ssl amounts of station c are remarkably larger than the other two stations since it is located after the confluence of the missouri and mississippi rivers furthermore higher values of coefficient of variation cv for ssl compared to the flow discharge data show the higher deviation of ssl data from the mean and describe ssl as a more sporadic phenomenon than the flow rate series itself the modeling was carried out through two scenarios including single station and multi station modeling in order to predict the runoff sediment process one step ahead i first scenario in this scenario as single station modeling it was tried to estimate the ssl of each station in the current time step t using its own historical data ii second scenario in the second scenario as multi station modeling the aim was to predict ssl of station c using discharge values of stations c ssl and discharge values of stations a and b when ssl of station c is not measured because of technical and or financial shortages due to the particular spatial locations of these three stations as can be seen in fig 3 it is expected to represent a practical technique to be used in order to eliminate the need of measuring ssl 2 4 efficiency criteria the coefficient of determination dc or nash sutcliffe efficiency and normalized root mean square error nrmse were employed to analyze and evaluate the performance of the models dc and nrmse of the models can be calculated by following equations nourani 2017 13 dc 1 t 1 n s t s com t 2 t 1 n s t s 2 14 nrmse t 1 n s t s com t 2 n s m a x s m i n where n refers to number of data s t shows the observed data s indicates the mean of observed data s com t shows the output estimated data s m a x and s m i n are the maximum and minimum values of the observed data respectively dc ranges from to 1 with a perfect score of 1 and nrmse ranges from 0 to with the perfect value of 0 it was proved by legates and mccabe 1999 that performance of hydrological and environmental models can be determined by dc and nrmse criteria 3 results and discussion 3 1 results of scenario 1 single station scenario in this study the proposed seasonal lstm models were applied to simulate the runoff sediment process of three stations by two different scenarios in daily and monthly scales in this way after normalizing the data in the first step for scenario 1 the as and ss sequences were created and set as the slstm model inputs for evaluating the capability of slstm network in capturing long temporal patterns various combinations of as and ss were examined and the best combination of as and ss was determined in this way four time series of ass asq sss and ssq were considered as the input candidates of slstm model employing the trial and error procedure the asq ass and sss were determined as the dominant inputs it seems that the impact of ssq could be represented by employing the sss by increasing the lag time modeling convergence time gets longer so detection of long temporal dependency was reinforced by employing ss sequence in the next step for wlstm modeling the ssl and discharge time series were decomposed at level 8 into 9 sub time series one approximation and 8 detailed sub series and level 4 into 5 sub time series one approximation and 4 detailed sub series at the daily and monthly time scales respectively into a trend term and periodic terms by wt employing db4 mother wavelet and the obtained sub series were considered as the candidate inputs of the lstm model in this way first of all the ssl and discharge time series were decomposed and then the approximation and detail sub series were normalized to the range of 0 1 and 1 1 respectively next the trial and error procedure was employed to determine the dominant sub series to fed to the lstm model due to the relative relationship between runoff and ssl it was assumed that both time series included the same frequencies so both time series were decomposed in the same level nourani et al 2019 it should be mentioned that there are different types of mother wavelet which are used in accordance with the type of the process according to previous studies the db4 mother wavelet is more appropriate than other functions in order to simulate the hydrological process nourani et al 2019 thus db4 mother wavelet was employed in this study the reason for choice of 8 and 4 for the daily and monthly scales is that in the dyadic wavelet transform number 2 to the power of 8 and 4 is close to 365 days and 12 months which have been selected as seasonal period in this study for instance fig 5 shows decomposed sub series of ssl time series for station a at level 8 in which s is the ssl time series a8 and di are the approximation and detail sub series respectively as mentioned the normalized sub series of ssl and discharge were considered as candidate inputs of wlstm model and finally 3 dominant time series were determined as inputs of lstm model finally to evaluate the proposed methods in capturing long term information with regard to convergence speed and exactness results of seasonal lstm models were compared with those of lstm real time model employing the only as sequences as well as the classic ffnn model given that despite lstm model the ffnn model just considers a fixed lag time of data to detect autoregressive dependency of data so s t 1 s t 2 q t and q t 1 in daily and s t 1 s t 2 q t and q t 1 in monthly scales were determined as inputs of ffnn model in scenario 1 discharge series of station a were considered with greater lags in comparison with station b that could be related to the remoteness of station a in other words it takes more time for the discharge and sediment load of station a to reach station c it should notice that ffnn model was trained with scheme of scaled conjugate gradient of bp approach which is fast and avoids a time consuming search sharghi et al 2019 furthermore tan sig transfer function was applied for both intermediate and target layers of ffnn models since their nonlinearity in combination with each other can be able to identify far more complex patterns sharghi et al 2019 the hyper parameters of lstm based models determined by trial and error except iterations are tabulated in table 3 all assessments were conducted on a desktop computer with an intel i5 8250u cpu 1 60 ghz 1 80 ghz operating system of microsoft windows 10 professional x64 and 8 gb ram and a graphics card with a normal module nvidia geforce mx 110 results of proposed dl and conventional ffnn models in both daily and monthly time scales via scenario 1 are presented in table 4 the estimated and observed ssl time series in test step for stations a b and c for scenarios 1 in daily scale are presented respectively in figs 6 8 furthermore the scatter plots of test step of wlstm and ffnn methods for stations a b and c for scenarios 1 in daily scale are depicted in fig 9 the most effective and accurate model is marked in bold according to the results presented in table 4 it is clear that performance of the seasonal lstm models are better than the lstm real time up to about 10 and 21 respectively for daily and monthly scales in test phase in terms of dc by almost equal modeling run time the improvement of seasonal lstm models is more considerable in monthly scale regarding that in monthly scale the seasonality component is more dominant and effective in this respect table 4 shows that the proposed seasonal lstm models could successfully discover long temporal pattern without raising the modeling run time and it could lead to more accurate estimations also the seasonal lstm models outperformed ffnn model the results of seasonal lstm models are more accurate than the outputs of ffnn model in test phase in terms of dc up to 23 and 28 respectively for daily and monthly scales since lstm considers dynamic lag to search and detect autoregressive dependency of data while ffnn considers a fixed static lag actually in ffnn model the fixed lag is considered but in lstm model different lags in different steps may be considered according the properties of the time series in various times in other words in the lstm based models the lag time is searched automatically by the model and the dominant lag time in each step is defined dynamically while in ffnn model the lag time should be defined and set in input data by the user whereas in some time spans of modeling long but in some others short term lag of data shall be considered dynamically among two proposed enhanced lstm models the wlstm model yielded more accurate results than the slstm model given that wlstm model is a multi scale method that can detect the underling patterns of data more efficiently regarding that ai methods and specially the dl models are sensitive to the numbers of training samples in single station modeling as table 4 shows the overall outputs indicate the supremacy and robustness of daily scale modeling comparing to the monthly modeling owing to the fact that a large amount of data are involved in daily scale training on the other hand unlike daily scale in which lstm real time model exhibited better results than ffnn model in monthly scale the outcomes of lstm real time and ffnn models were not so different since in daily scale the ssl values may be affected by longer lag times than it is considered manually in ffnn model while lstm real time model could dynamically search longer lag times furthermore by comparing the results of 3 stations it can be seen that the results of 3 stations are almost similar in daily scale while station b led to slightly poor outcomes than two other stations in monthly scale and this may be due to the reason that the station b has more fluctuations which complicates the modeling 3 2 results of scenario 2 multi station scenario the main goal of modeling through scenario 2 was to estimate ssl of the station c through a multi station approach meaning that discharge and ssl of the other two upstream stations were being used in the ssl modeling of station c without employing the ssl data of station c as the input the spatial locations of these three stations indicate a high correlation among them to this end as s a as s b as q a as q b as q c ss s a ss s b ss q a ss q b and ss q c were the input candidates of slstm model in scenario 2 next the as s a as s b as q a as q b and as q c were determined as dominant inputs through trial and error procedure next like scenario 1 wt was employed to decompose the ssl and discharge time series of stations a b and discharge time series of station c into a trend term and periodic terms using db4 mother wavelet at same levels noted in scenario 1 and the dominant sub series were considered as the inputs of wlstm model using the process same as scenario 1 also the hyper parameters of lstm based models in scenario 2 was same as scenario 1 see table 3 moreover s t 10 a s t 6 b q t 10 a q t 6 b q t c q t 1 c in daily and s t 3 a s t 1 b q t 3 a q t 1 b q t c in monthly scales were determined as inputs of ffnn model in scenario 2 in scenario 2 like scenario 1 ffnn model was trained considering tangent sigmoid as activation functions of hidden and output layers the and employing using scaled conjugate gradient scheme of bp algorithm results of proposed dl and conventional ffnn models in both daily and monthly time scales via scenario 2 are presented in table 5 the most effective and accurate model is marked in bold according to the results presented in table 5 and like scenario 1 the performance of the seasonal lstm models was better than the lstm real time up to about 24 and 10 respectively for daily and monthly scales in test phase in terms of dc by almost equal modeling run time same as scenario 1 the results of seasonal lstm models are more accurate than the outputs of ffnn model in test phase in terms of dc up to 25 and 11 respectively for daily and monthly scales and the wlstm model led to more accurate outcomes than slstm model it worth noting that unlike single station modeling in which daily modeling exhibited better results in multi station scenario the monthly outcomes showed more reliable results than the daily modeling as the stations in monthly modeling interact with each other in a long lag time so application of the proposed enhanced lstm models could improve the modeling performance in daily scale more than monthly scale furthermore in scenario 2 different combinations of inputs were investigated to define the interaction degree between stations and to detect the main source of ssl to station c so in one case the data of station a were applied to model ssl of station c and in another case the data of station b were utilized to model ssl of station c the results of both cases were similar which shows the similar interconnection of stations a and b with station c while according to initial supposition station b would have more correlation to station c based on short distance this anomalous outcome could be related to the poor condition of land cover which consequently adds to the sediment load through soil erosion process on the other hand multi station scenario indicated the high interaction of stations and showed the importance of considering the physical aspects and geomorphology features of the study area e g land cover in sediment prediction for example according to table 2 upstream basin of station a has less forest and agricultural lands comparing to station b which causes the basin of station a to be subjected to severe soil erosion and to carry more sediment to station c which was supported by modeling scenario 2 as stations a and b demonstrated almost same correlation with ssl value of station c despite the great distance of station a the estimated and observed ssl time series in test step of station c for scenario 2 in monthly scale are presented in fig 10 according to figs 6 10 almost all models presented acceptable results but the proposed models have more accurate outcomes furthermore as it can be seen from figs 6 10 the proposed models led to more reliable outcomes than conventional ffnn model given that the proposed models unlike the ffnn model consider dynamic lag time and this is significant in estimating the maximum values of time series which usually have longer dependency to the previous values where ffnn model provided underestimations that is more clear in fig 9 b besides according to fig 10 almost all models led to acceptable estimations considering the fact that in scenario 2 none of the models employed the observed ssl values of station c however this multi station scenario could be employed when the ssl data of station c are not available due to financial or technical issues to have a visualized comparison of models performances in testing step taylor diagrams for station c in daily and monthly scales for both modeling scenarios are presented in fig 11 taylor diagram is one of the most useful ways for graphical representation of modeling accuracy based on several statistical indicators taylor 2001 it presents the summary statistical indicators of the measured and predicted ssl including dc and nrmse in other words the taylor diagram offers a reliable graphical depiction of modeling performance according to the estimated and observed values it can be seen from fig 11 that in both time scales seasonal lstm models could improve the accuracy of the prediction results and showed better results 4 conclusions considering the complexity of hydrological time series and on the other hand the high capability of dl methods in discovering the underlying complex patterns of time series the application of two types of newly proposed seasonal lstm models for runoff sediment modeling was investigated in this study discharge and ssl data of three gauging stations in two different hydrological regions of the united states were used via two distinct modeling scenarios in both daily and monthly scales to evaluate the robustness of the proposed models the first scenario as the single station modeling was developed to compute the ssl in the current time step for all stations using each station s own data in the next modeling scenario as the multi station modeling observed data of two upstream stations i e a and b utilized to estimate the ssl of downstream station c without using the ssl records of station c but employing its streamflow data moreover the modeling results were compared with conventional ffnn model by employing the ss sequence in slstm and utilizing wt in wlstm models to detect the long term properties of runoff sediment the proposed models could discover long temporal information more effectively and with no increase in the modeling run time the as and the ss were considered as the input set of slstm model to capture the underlying patterns of various intervals and the slstm leads to better estimations than lstm utilizing as data on the other hand decomposed sub series obtained via wt could discover the trend and periodic components of data properly thus the results proved the superiority of proposed wlstm model to the other applied ai models even lstm real time moreover the seasonal lstm models are designed to be highly scalable and it can incorporate different properties such as runoff and sediment overall the proposed seasonal lstm methodology could enhance the lstm real time and ffnn models efficiency in test step up to 25 and 28 in daily and monthly scales respectively based on the aforementioned investigations the effectiveness of the proposed enhanced lstm models for sediment prediction could be approved other hydrological processes e g groundwater precipitation etc can be also modeled by the proposed seasonal lstm models in this study only yearly frequency was investigated for future studies it is suggested to detect other dominant frequencies to be imposed into the slstm model also it is suggested to employ the proposed methodology to multi step ahead modeling declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
4337,understanding drought dynamics spatially and temporally is essential for environmental and socio economic systems especially under climate change with expected increasing drought this study aims to update the knowledge about drought spatiotemporal changes over mainland china in the 21st century by utilizing the latest coupled model intercomparison project phase 6 cmip6 datasets precipitation from the nine cmip6 models is first bias corrected using a quantile mapping approach and then used to calculate the 3 month standardized precipitation index spi for the historical period and future three scenarios ssp1 2 6 ssp2 4 5 and ssp5 8 5 we conduct a clustering algorithm to identify three dimensional i e latitude longitude and time drought events to access the drought dynamics in both space and time results show that the drought events occur more frequently during june to september than the other months for both historical and future periods the drought frequency is significantly increasing during sep dec in the 21st century increase by about 8 5 p value 0 05 compared with the historical period drought centers shift toward south under the future projections indicating a relatively high drought frequency in south china additionally drought events are projected to have shorter duration smaller affected area but higher intensity under all three scenarios the study contributes to our comprehensive understanding of how future drought characteristics will change due to climate change from the perspective of spatiotemporal dynamics these results are helpful to drought risk management and provide information for developing drought adaptation strategies keywords meteorological drought events cmip6 spi mainland china climate change 1 introduction drought occurs when there is a continuous negative abnormality in precipitation water resources soil moisture and other hydroclimatic related variables compared to normal level over an extended period demuth and stahl 2001 it is one of the prevailing natural hazards that can exert extensive damage on human environmental and economic systems aryal and zhu 2019 mishra and singh 2011 sam et al 2019 sheffield and wood 2008a zhai et al 2020 drought has been observed to increase in frequency and tend to be more severe in severity over africa southern europe east and south asia eastern australia and many parts of the northern mid high latitudes from the historical observations since the middle 20th century chen and sun 2015 dai 2011 2013 ma et al 2019 mishra and singh 2010 spinoni et al 2014 wu et al 2020 giving the ongoing climate change and intensified human activities it is crucial to understand how droughts change in both space and time for both historical observations and future projections cook et al 2020 pokhrel et al 2021 sun et al 2019 ukkola et al 2020 zscheischler et al 2018 drought is expected to worsen under future projections cook et al 2020 2014 dai et al 2018 naumann et al 2018 trenberth et al 2014 wang 2005 zhang and zhou 2015 the fifth assessment report ar5 of intergovernmental panel on climate change ipcc claimed that drought is likely to become more intense and extensive in the subtropical dry zone by the end of 21st century ahmadalipour et al 2017 cook et al 2014 dai et al 2018 trenberth et al 2014 wang et al 2018 global warming will enlarge the likelihood of severe pervasive and irreversible impacts of drought on human society and ecosystems cai et al 2014 dai et al 2018 ipcc 2014 recent released coupled model intercomparison project phase 6 cmip6 provides advanced ensemble simulations based on the latest state of art climate models eyring et al 2016 the cmip6 is believed to have higher skill in reproducing the observed large scale mean surface temperature and precipitation patterns than cmip3 and cmip5 bock et al 2020 xin et al 2020 zhu et al 2020 new scenarios have also been designed in cmip6 based on the combination of the shared socioeconomic pathways ssps o neill et al 2017 and forcing levels of the representative concentration pathways rcp moss et al 2010 they consider the substantial improvements of emissions land use scenarios model parameterization and physical process several previous studies have updated the future changes in drought with the outputs from cmip6 models cook et al 2020 indicated robust drying and increases in extreme drought occurrence across many regions by the end of the 21st century based on a multi model ensemble from cmip6 zhai et al 2020 found significant increases in drought conditions primarily in the north west sub region over south asia with longer duration and higher intensity during 2020 2099 using a five model ensemble mean from cmip6 under three scenarios ssp1 2 6 ssp2 4 5 and ssp5 8 5 ukkola et al 2020 revealed robust changes in the frequency and duration of seasonal meteorological drought over 45 percent of the global land area used cmip6 projections as changes in drought exhibit high variability in both space and time it is of great significance to characterize future drought changes in various climate zones and regions cook et al 2014 dai et al 2018 gu et al 2020 naumann et al 2018 drought naturally occurs over spatially adjacent areas and temporally continuous periods it is characterized by duration representing the persistent period area indicating the spatial coverage and severity reflecting overall magnitude of a drought event haslinger and bl√∂schl 2017 herrera estrada et al 2017 li et al 2020 liu et al 2021 lloyd hughes 2012 sheffield et al 2009 zhu et al 2019 however many previous studies primarily analyzed one dimensional time series of drought variables precipitation drought indices streamflow etc the one dimensional methods include areal average based station based or grid based ways chen and sun 2017 chen et al 2013a 2013b leng et al 2015 liang et al 2018 wang and chen 2014 in recent years a growing body of literature has started paying attention to three dimensional drought structures andreadis et al 2005 huang et al 2018 liu et al 2021 lloyd hughes 2012 sheffield et al 2009 zhai et al 2017 a clustering algorithm based on three dimensional structures i e latitude longitude and time was developed to identify individual drought events and their spatial extent for comprehensive assessments of drought spatiotemporal dynamics andreadis et al 2005 lloyd hughes 2012 sheffield et al 2009 the clustering algorithm has been successfully used for drought event identification in historical period over china li et al 2020 shao et al 2018 wang et al 2011 xu et al 2015 zhai et al 2017 the continental united states andreadis et al 2005 wang et al 2009 korea kim et al 2011 europe haslinger and bl√∂schl 2017 africa zhan et al 2016 and globally sheffield et al 2009 additionally future changes in the spatiotemporal pattern of drought events for china identified by the clustering algorithm have been analyzed based on cmip5 models huang et al 2018 su et al 2018 yang et al 2020 although some efforts have been made using cmip6 simulations to analyze projected changes in drought cook et al 2020 ukkola et al 2020 zhai et al 2020 the ability of the new released cmip6 simulations in drought identification and evaluation considering the three dimensional structures has not been well documented china as one of the major hot spots for high intensity and frequently occurred droughts has suffered from severe drought events leading to large socioeconomic losses as the impacts of large scale climate variability including the east asian monsoon chen and sun 2015 li et al 2020 qin et al 2015 wang et al 2012 yu et al 2014 zou et al 2005 for example the extreme drought event that swept across southwest china during 2009 2010 led to extensive damage with economic losses of nearly 30 billion u s dollars short of drinking water of over 16 million people and 11 million livestock destruction of crops in 4 million hectares of farmland lu et al 2011 yang et al 2012 evidence from the historical observations showed a tendency towards more severe droughts since the late 1990s for china temporally yu et al 2014 there was an increasing trend of drought events over southwest and northern china spatially ma and ren 2007 ma et al 2018 cmip5 future projections show increases in drought risk with increasing drought intensity areal coverage and economic losses over china chen and sun 2017 gu et al 2020 huang et al 2018 leng et al 2015 liang et al 2018 su et al 2018 sun et al 2019 wang and chen 2014 considering the improvements in precipitation simulation based on cmip6 models compared with cmip5 models bock et al 2020 xin et al 2020 zhu et al 2020 it is also of great interest to access the future changes in drought over china the meteorological drought generally related to the deficiencies in precipitation has attracted the most attention as it is usually the prerequisite to other types of droughts e g hydrological droughts and agricultural droughts hu et al 2021 she and xia 2018 wilhite and glantz 1985 wu and chen 2019 this study characterizes the meteorological drought in mainland china by the widely extensively used drought index the standardized precipitation index spi mckee et al 1993 for its simplicity in computation and ability in drought detection at multiple time scales cheval 2015 hayes et al 2011 li et al 2020 nasrollahi et al 2015 song et al 2020 the projections from nine cmip6 models are employed to represent the future changes in precipitation and drought the outputs from cmip6 models are corrected using the quantile mapping method bo√© et al 2007 gudmundsson et al 2012 to reduce the bias relative to observation the spatiotemporal drought dynamics and comprehensive assessments of the spatial extent of drought are investigated by the clustering algorithm andreadis et al 2005 lloyd hughes 2012 sheffield et al 2009 furthermore the drought center trajectory in future periods is also analyzed to reveal the potential spatial change of drought in mainland china the objectives of this study are to 1 evaluate the ability of cmip6 models after bias correction in reproducing precipitation pattern and drought characteristics during the historical period 2 assess the drought characteristics in the future projections using the clustering algorithm 3 investigate the change in the trajectory of drought center in the 21st century over mainland china 2 data and methodology 2 1 datasets the monthly gridded precipitation for mainland china from 1980 to 2014 with a spatial resolution of 0 5 0 5 is obtained from the national meteorological information center http data cma cn which is generated from 2400 gauged observed precipitation data with strict quality controls wu and gao 2013 zhou et al 2016 these data are served as the baseline for the bias correction of raw cmip6 models monthly outputs of precipitation data from nine global climate models gcms from cmip6 including the historical 1850 2014 simulations and projections 2015 2100 under three scenarios are collected from https esgf node llnl gov search cmip6 to assess future changes of drought in the study area table 1 eyring et al 2016 the three scenarios from scenariomip o neill et al 2016 are ssp1 2 6 2 6 w m 2 low forcing sustainability pathway ssp2 4 5 4 5 w m 2 medium forcing middle of the road pathway and ssp5 8 5 8 5 w m 2 high end forcing pathway respectively these future precipitation data are first resampled to the spatial resolution of 0 5 0 5 by the bilinear interpolation method and then are bias corrected based on observed data in each grid cell over mainland china we divide the entire period into the historical period 1980 2014 and four future periods 2021 2040 2041 2060 2061 2080 2081 2100 we divide mainland china into six subregions http www resdc cn fig 1 to better investigate the spatial pattern of future changes in drought the subregions include northeast china nec north china nc northwest china nwc southwest china swc central south china csc east china ec 2 2 bias correction method the quantile mapping qm method bo√© et al 2007 cannon et al 2015 chen et al 2013a 2013b fang et al 2015 teng et al 2015 is employed to correct the bias between observed and gcm simulated monthly precipitation by matching their empirical cumulative distribution functions ecdfs the qm is favorable due to its simplicity nonparametric configuration and effective correction of bias in mean standard deviation and quantiles which outperforms many other empirical statistical downscaling theme√ül et al 2012 2011 the bias corrected precipitation p m t at time t by qm is obtained from the following transfer function 1 p m t f o 1 f m p m t where fo and fm are the ecdfs of respectively observed data po denoted by the subscript o and modeled data pm denoted by the subscript m f 1 indicates the inverse ecdfs i e quantile functions temporal cross validation is applied dividing the data period into calibration and independent validation period with the same length the gcm simulated precipitation is first split into two periods 1980 1996 and 1997 2015 then we construct the continuous precipitation series from two validation periods each generated using another period for calibration the qm method is conducted using the r package qmap gudmundsson et al 2012 we use the bias correction method for both the historical and future precipitation simulations for each grid cell the effectiveness of the bias correction method for gcm simulated precipitation is evaluated based on the taylor diagram which provides a concise statistical summary of how well patterns match each other taylor 2001 the taylor diagram presents three statistics the pearson correlation coefficient cc the centered root mean square error rms and the standard deviation sd the cc related to the azimuthal angle gauges similarity in pattern between the simulated and observed models the rms in the simulated model is proportional to the distance from the point on the x axis identified as observed the sd of the simulated model is proportional to the radial distance from the origin it is used to quantify the degree of correspondence between the modeled and observed behavior in terms of these three statistics with higher cc lower rms and closer sd relative to the observation indicating the better performance of models 2 3 drought event identification 2 3 1 standardized precipitation index the spi mckee et al 1993 uses the two parameter gamma distribution to fit the cumulative monthly precipitation time series and transform the cumulative probability density functions to the drought index value by the inverse standard norm distribution the general process of spi calculation is summarized as eq 2 2 spi œÜ 1 h h x q 1 q g x g x k Œ∏ 1 Œ∏ k Œ≥ k 0 x t k 1 e t Œ∏ d t x 0 where œÜ 1 is the inverse function of the standard normal distribution h x is the cumulative probability of precipitation q is the frequency of zero precipitation x is the precipitation without zero values g is the gamma cumulative distribution function k and Œ∏ are the shape and scale parameters of gamma distribution estimated by the maximum likelihood method for drought comparison between different periods we apply the derived parameters of gamma distribution based on historical precipitation to fit future projections ahmadalipour et al 2017 leng et al 2015 2 3 2 clustering algorithm the drought event is identified when the spi value is below a specific threshold value usually 1 li et al 2020 xu et al 2015 zhai et al 2017 spi can be computed for different time scales to capture both short term 3 months and long term drought 3months in this study the 3 month time scale is used to characterize drought events with the capability of representing the seasonal droughts xu et al 2015 the three dimensional i e latitude longitude and time clustering algorithm proposed by andreadis et al 2005 is applied to identify individual drought events with coherent space and time structure the algorithm merges grid cells under drought which are adjacent in space and continuous in time generally by two separate steps namely spatial clustering and temporal connection andreadis et al 2005 li et al 2020 lloyd hughes 2012 sheffield et al 2009 xu et al 2015 i for each month a spatial smoothing preprocessing step is first done with a 3 3 median spatial filter for ensuring minimum distortion of the original data all grid cells with an spi value below a predetermined value 1 in this study are identified as being under drought and the drought patches are extracted by clustering the spatially adjacent grid cells under drought then any drought patches with areas less than a minimum cluster area threshold are filtered out which no longer participate in the following computation consequently the individual drought patches are identified over time ii on two continuous months the drought patches merge to form a larger drought event or break up into multiple smaller drought events in both cases the smaller droughts are considered to be part of the larger drought hence any pairs of drought patches between the two successive months with an overlap area larger than a predetermined threshold are considered the same drought event this step is repeated for all months and finally all individual drought events are identified as a series of drought patches in continuous time in this study the minimum cluster area threshold is set to 150 000 km2 approximately sixty 0 5 0 5 grid cells which is suggested to be the most suitable threshold in china with an area of 9 6 million km2 by wang et al 2011 a similar threshold is selected in several previous researches on drought event identification in china li et al 2020 shao et al 2018 wang et al 2011 xu et al 2015 2 3 3 drought event characteristics to further characterize the identified drought events several statistic indices are calculated as follows xu et al 2015 i duration d is the persistent time of a drought event month ii severity s indicates a water storage deficit over the whole drought duration and spatial extent and is calculated as 3 s n i 1 n ion j 1 n lat k 1 n t s i j k s i j k s p s i j k s 0 a r e a i j k t i m e i j k where sn is the severity of the nth drought event km2month s is the severity of single grid cells km2month s 0 is the threshold of spi for identification of drought s 0 1 in this study area is the areas of grid cells km2 time is the heights of grid cells one month iii affected area a is the projected area over the longitude latitude surface in the 3 dimensional space time domain iv intensity i is the ratio of the drought severity to the drought duration and the affected area i s d a v centroid c represents the center location llong llat ltime of drought event which is computed by the weighted severity in the 3 dimensional space time domain as eq 4 4 l long i 1 n l on j 1 n lat k 1 n t s i j k l o n g i s n l lat i 1 n l on j 1 n lat k 1 n t s i j k l a t j s n l time i 1 n l on j 1 n lat k 1 n t s i j k t i m e k s n where llong llat ltime are the longitude latitude and month of a drought centroid long lat time are the longitude latitude and month of the center of grid cells 3 results and discussion 3 1 evaluation of cmip6 models 3 1 1 simulation of cmip6 models of historical precipitation the taylor diagram of raw and corrected precipitation of nine gcms fig 2 shows that both raw and corrected gcms have relatively high cc 0 9 0 95 and low rms 15 20 however the sds of most eight of nine corrected models are smaller than the raw models indicating a better performance of corrected models than the raw models therefore the corrected gcms generally agree better with observation with slightly lower rms and more consistent sd than the observed data from comparing the annual cycle of precipitation in fig 2 b the raw gcm simulated precipitation values overestimate the observation and show large variation the average value of monthly precipitation of the corrected models is much more consistent with the raw modes the standard deviation of the corrected models is smaller with the observation than the raw models which indicates the efficiency of the qm method for bias correction of gcms overall the evaluation of bias correction in both cases indicates a good capability of the corrected gcms in simulating precipitation over mainland china 3 1 2 future precipitation changes in mainland china with cmip6 the annual mean precipitation anomalies relative to the historical average 1980 2014 over six sub regions under different scenarios is shown in fig 3 cmip6 models exhibit a significant p value 0 05 in the mann kendall trend test mann 1945 increasing precipitation trend throughout the 21st century in six sub regions over mainland china under all the scenarios the ssp5 8 5 shows the largest and continued increasing annual precipitation trend over different sub regions among the three scenarios the ssp1 2 6 and ssp2 4 5 exhibit increasing trend but with smaller rate in annual precipitation relative to the ssp5 8 5 we also examine the temporal change of future annual precipitation across mainland china using sen s slope method hipel and mcleod 1994 sen 1968 the spatial pattern of these changes is shown in fig s1 the annual precipitation will increase throughout mainland china in the 21st century all three scenarios reflect a similar spatial pattern of future changes in annual precipitation over mainland china in comparison ssp5 8 5 of highest greenhouse gas emissions reveals a robust increase compared to the other two scenarios under ssp5 8 5 the percentage change in annual mean precipitation reaches 50 for 2015 2100 relative to 1980 2014 over west china for ssp2 4 5 and ssp5 8 5 there are statistically significant increases throughout mainland china at the 95 confidence level p values 0 05 with a stronger increasing trend in southwest china and southeast china this projected precipitation trend is in line with the study by chen and frauenfeld 2014 using cmip5 simulations 3 2 simulation of historical droughts by the cmip6 to examine the capacity of cmip6 models in reproducing the drought characteristics over mainland china the spi is first calculated using the observed and bias corrected gcm precipitation and the drought events are then identified based on the clustering algorithm the overall statistics of drought events in the historical period are summarized in table 2 here we only consider the drought events longer than three months shao et al 2018 wang et al 2011 xu et al 2015 the total numbers of drought events identified by nine cmip6 models are slightly higher compared to the observation 84 with cesm2 having the most 99 and bcc csm2 mr having the least 84 but equal to the observation the multi model ensemble mean number of drought events is 92 which is 8 more than the observation regarding drought intensity ec earth3 veg presents the largest value 0 303 while canesm5 presents the smallest value 0 259 the ensemble mean of drought intensity is 0 278 which shows good consistency with the observation 0 288 the ensemble mean of drought severity and area based on cmip6 simulations are 2 504 106km2month and 1 820 106km2 respectively which are both slightly larger than the observation with the corresponding values of 1 959 106km2month and 1 506 106km2 respectively among the nine models cesm2 waccm identifies the largest severity 2 751 106km2month and area 2 018 106km2 while ipsl cm6a lr gives the smallest severity 2 211 106km2month and area 1 638 106km2 as for drought duration all models except ec earth3 veg show a slightly larger value than the observation 4 55 months and the ensemble mean 4 90 months drought duration is close to the observation although there is a slight difference between several statistics among the nine cmip6 models and the observations the satisfied performance of considered cmip6 models in capturing drought events during the historical period over mainland china supports the future drought assessment in the study area 3 3 future changes of drought events 3 3 1 changes in drought statistics during 2021 2100 table 3 summarizes the statistics of drought events duration 3 months during future periods under different scenarios a slightly decreasing trend of drought events can be seen in future periods while the intensity of drought events turns to be higher in the future under different scenarios we found a robust change in the number 52 35 and intensity 0 313 0 391 of droughts under ssp5 8 5 continued decreases in severity can be seen for ssp1 2 6 and ssp2 4 5 while an increasing trend in severity is shown before 2080 for ssp5 8 5 future drought events show a slightly decreasing trend in both area and duration under different scenarios 3 3 2 temporal change pattern of drought here we identify the occurrence month of drought centroid for each drought event and then calculate the frequency of drought event for each month in different periods under three scenarios the future changes of drought event frequency in various months are given in fig 4 we found a higher frequency of drought event from june to september the multi model ensemble mean frequency is 43 8 during the historical period 40 3 in 2021 2040 43 2 in 2041 2060 41 0 in 2061 2080 and 45 2 in 2081 2100 respectively in both historical period and future periods under different scenarios the frequent drought events in summer june july and august and autumn september october and november will largely affect the crop production as this period is essential for the normal growth and development of crops liu et al 2018 porter and semenov 2005 potop et al 2012 long term water shortages may greatly reduce crop yields and even cause severe damage to crops compared with historical simulations the frequency of drought events decreases from april to july while there appears a positive trend from september to december for example the mean drought frequency for april july is 39 3 in the historical period 33 3 in 2021 2040 32 6 in 2041 2060 35 9 in 2061 2080 and 33 3 in 2081 2100 respectively under the ssp5 8 5 whereas the corresponding values for september december turn to be 31 6 38 1 34 9 37 9 and 35 5 respectively additionally a non parametric kruskal wallis hollander et al 2013 test is applied to determine the significance of the difference in the frequency of drought events between future and historical periods in general the cmip6 models project a slight without statistical significance in kruskal wallis test p value 0 05 decrease in the frequency of drought events during april july in the 21st century 2015 2100 by about 4 5 an average difference between future and historical period under three scenarios and a significant p value 0 05 increase in drought frequency during september december by about 8 5 relative to the historical period 1980 2014 it is worth noting that there are obvious differences in future changes of drought event frequency under different scenarios such as the continuously decreasing trend of frequency on may and june in future periods for ssp5 8 5 which is inconsistent with the other two scenarios 3 3 3 spatial change pattern of drought we count the drought events with centroids located in six subregions in different periods under three scenarios respectively to investigate the future changes in the spatial pattern of drought in the study area as shown in fig 5 there appears an increase in future drought frequency in csc and swc and a decrease in nc and nec relative to the historical period the cmip6 models seem to project more robust changes under ssp5 8 5 table s1 for example the future drought frequency in nc may significantly p value 0 01 drop by 5 7 under ssp5 8 5 while the decrease turns to be 2 3 p value 0 05 under ssp2 4 5 and 1 8 without statistical significance under ssp1 2 6 the ssp5 8 5 also indicate the significant p value 0 05 increase in future drought frequency by 3 6 in csc and 5 2 in swc respectively we further explore the spatial shift of drought centroid in future periods under different scenarios the spatial distribution of drought centroids for ipsl cm6a lr in both historical and future periods under three scenarios is shown in fig 6 the grey points and gradient colored points denote historical and future drought events respectively at the end of 21st century the projected positions of drought centroids are primarily located in south china for ssp2 4 5 and ssp5 8 5 indicating a southward shift of drought events relative to the historical period more significant changes in the far term future after 2080 are observed for ssp5 8 5 similar results for other cmip6 models can be found in figs s2 s9 additionally the shift direction of future drought center is obtained based on the average locations of drought centroids in the historical period and future period under different scenarios in fig 7 the colors denote different cmip6 models and the arrows represent the direction of shift path of future drought centroids under the three scenarios the majority of shift paths are pointing southwest suggesting a southwestward shift of future drought centroids for most cmip6 models although there exist some differences in the shift path in a few models and scenarios miroc6 and cams csm1 0 exhibit a northward shift of drought center under ssp1 2 6 while the shift turns to the southwest for ssp2 4 5 and ssp5 8 5 by miroc6 most models indicate a southwestward shift of drought center with consistence under three scenarios while ec earth3 veg projects a southeastward shift previous studies also indicated similar changes in drought frequency and severity using cmip3 cmip5 and other datasets for historical and future periods for example chen et al 2013 projected a slight increase in the frequency of some regions of southern china using cmip3 models and a regcm3 simulation chen et al 2013 indicated increasing drought risk in southwest china based on the projected decrease in pdsi derived from cmip5 simulations these studies projected increasing drought frequency in southern china based on mere spatial changes of drought index or grid based drought event identification with a one dimensional method huang et al 2018 applied the intensity area duration analysis for future changes in drought events at a space time dimension they found the drought center would probably shift towards southeast china projected by the regional climate model cosmo clm the present study using the cmip6 multi model projections supports the previous findings that there are increasing drought events in southern china chen et al 2013a 2013b huang et al 2018 wang and chen 2014 wen et al 2020 yao et al 2016 3 3 4 changes of probability density function of drought characteristics future changes in mean and standard deviation of drought statistic indices intensity severity area and duration in different periods under three scenarios over mainland china are presented in fig 8 we use the non parametric kruskal wallis test to determine the significance of the difference in the mean and standard deviation of drought statistic indices during different periods in terms of the mean value there appears a significant increase in intensity p value 0 05 for three scenarios while decreasing trend with statistical significance p value 0 05 in area and duration of droughts can be found under three scenarios except for the case that the mean area shows the insignificant difference p value 0 14 for ssp5 8 5 the severity of droughts shows a slight decrease p value 0 05 in future periods under ssp1 2 6 while the trend is not evident for the other two scenarios p values are 0 15 and 0 83 for ssp2 4 5 and ssp5 8 5 respectively results indicate the trend of future drought events with higher intensity shorter duration and smaller affected area as for the standard deviation the intensity variability exhibits a clear increase p value 0 05 in future periods under three scenarios implying an increasing probability of drought event with extreme intensity in addition to the higher average intensity during the 21st century there is insignificant difference in the standard deviation of severity during different periods for ssp2 4 5 p value 0 14 and ssp5 8 5 p value 0 54 while the standard deviation of drought affected area and duration show a robust decrease p value 0 05 in future periods under three scenarios compared to the historical period suggesting a more uniform distribution of area and duration of future droughts furthermore the kernel density estimates sheather and jones 1991 of four drought statistic indices intensity severity area and duration in different periods under three scenarios are illustrated in fig 9 for a clear understanding of the distribution of drought event characteristics in terms of drought intensity the mode of density curves peak point of the curve in future periods shows a lower and rightward trend than the historical period indicating increases of future drought events with higher intensity besides it should also be noted that the density curve is flattened to both the left and right showing increasing variance as consistent with fig 9 b and due to the shift of the density curve toward the right larger density in future periods can be seen at the tail of density curves i e intensity 0 5 relative to the historical period which demonstrates that future drought events with more extreme intensity are increasing the changes are more robust for ssp5 8 5 with a wider range of density curves it can be concluded that future droughts will become more extreme under global warming there is no apparent change in the severity of future drought events future changes in density of area and duration of future droughts exhibit a higher peak of density at a lower value indicating a more frequent tendency of future drought events with shorter duration and smaller area we further calculate the quantiles multi model ensemble mean of area and duration corresponding to the probability of 90 in different periods under three scenarios table 4 a continued decreasing trend of 90th quantiles of area and duration can be found during the 21st century under different scenarios for instance the robust decrease of area 90th quantile reaches 1 092 106km2 after 2080 for ssp5 8 5 2 784 106km2 compared to the historical period 3 876 106km2 in terms of duration the decrease is 1 8 months after 2080 for ssp5 8 5 6 2 months relative to the historical period 8 0 months thus there is a tendency for drought events with shorter duration and a smaller area in future periods to further investigate future changes of extreme drought events with higher intensity the ecdf multi model ensemble mean of drought intensity in different periods under three scenarios is shown in fig 10 the zooming plots with exceeding probabilities p x x of extreme intensity 20 are also presented evident increases in probabilities of drought intensity 0 5 can be found in future periods under three scenarios especially for the ssp5 8 5 for example compared to the probability of 2 6 in the historical period future values are 7 7 in 2021 2040 11 3 in 2041 2060 11 2 in 2061 2080 and 18 1 in 2080 2100 for the ssp5 8 5 respectively thus extreme drought events with high intensity will be more frequent during the 21st century together these results provide important insights that the duration and extent of drought events will decrease in contrast the drought intensity is projected to increase over mainland china in the 21st century under different scenarios relative to the historical period according to the cmip6 simulations the finding of an increasing trend in drought intensity broadly supports evidence from previous observations chen and sun 2017 huang et al 2018 liang et al 2018 su et al 2018 wang and chen 2014 with the frequent occurrence of high intensity drought events the security and stability of water supply and agricultural production will face huge threats in china the cmip6 models project a shorter duration and affected area of drought events relative to those during the historical period in this study which is contrary to previous studies several lines of evidence suggest the increase in drought duration during the 21st century chen and sun 2017 huang et al 2018 leng et al 2015 this rather contradictory result may be partly due to the fact that the drought duration was identified in these studies by a grid based one dimensional approach without considering the spatial temporal structure of drought events concerning the drought affected area huang et al 2018 indicated that the areas will slightly decrease for short term drought events 1 3 months but increase strongly for long term drought events 12 months and the tendency was not consistent for all scenarios it is encouraging to compare the projected changes in drought areas with that found by su et al 2018 they suggested that the average drought coverage will decrease for the 1 5 c and 2 0 c levels relative to the reference period over china overall the cmip6 models used in this study project that future drought events tend to have higher intensity shorter duration and smaller affected area in the 21st century 4 discussion the current study set out to assess the meteorological drought change in the 21st century based on cmip6 multi model ensemble projections over mainland china one major finding is an increase in the frequency of drought events in southern china in the future several reports have shown that drought stress could inhibit vegetation growth peng et al 2011 zhang et al 2010 zhao and running 2010 the southward shift of drought center means the projected drought risk for northern china is likely to decrease which benefits the ecosystem restoration programs in northern china such as the three north shelter forest program on the other hand the potential impacts of projected frequent extreme drought with high intensity in the 21st century on vegetation growth cannot be ignored several studies indicated the improvement of cmip6 models over cmip5 models in drought assessment which is partly attributed to better performance in several drought related processes ecosystem processes hydrologic process like terrestrial water storage runoff and surface energy partitioning cook et al 2020 zhai et al 2020 regional drought changes in the current study are broadly consistent between cmip5 and cmip6 cook et al 2020 zhai et al 2020 however our finding of future changes in drought duration and area based on cmip6 projections is contrary to previous studies which have suggested an increase in drought events with longer duration and larger extent using cmip5 projections chen and sun 2017 huang et al 2018 leng et al 2015 with the improvement of cmip6 more research is needed to further determine how cmip6 offers confidence in drought risk assessment previous studies have indicated that precipitation in china and even globally is increasing in the future chen and frauenfeld 2014 jiang et al 2020 westra et al 2014 as precipitation is a determining driver for meteorological drought it is expected that future drought conditions will be mitigated with increasing precipitation however it turns out that droughts are projected to intensify in the future according to the results of the present study and previous relevant studies trenberth et al 2014 ukkola et al 2020 wu et al 2020 the question of whether an increase in precipitation mitigates drought conditions has been discussed by gu et al 2020 the study evaluated the drought frequency drought duration and severity across drought events in the historical and future periods near and far from 31 climate model simulations using spi the results indicated deteriorated drought conditions in central and southern china and mitigated drought conditions in northeastern and western china which is in line with our study gu et al 2020 also suggested that a wetting atmosphere does not imply mitigation in meteorological drought conditions a possible reason could be that changes in mean precipitation control changes in drought duration while the variability drives intensity changes in addition to the mean precipitation ukkola et al 2020 the increase in precipitation variability leads to an increase in extreme precipitation in the future resulting in severe floods and droughts for example sillmann et al 2013 projected future intensification of dry conditions in australia central america south africa and the mediterranean due to the increases in consecutive dry days decreases in heavy precipitation days and maximum consecutive 5 day precipitation finally we should underline that there also exists some uncertainties in the detected changes of drought derived from the used future precipitation datasets and the selected drought calculation indices burke and brown 2008 emphasized the importance of drought indices selection and argued that future drought projections will depend on the specifically used drought definition the spi used in this study is designed to identify meteorological droughts according to the variability of precipitation if some other variables such as temperature evaporation soil moisture are considered in the drought index calculation the tendency of drought may also be changed under climate change dai et al 2004 hu and willson 2000 touma et al 2015 trenberth et al 2014 for example touma et al 2015 showed that the great influence of temperature changes in the standardized precipitation evapotranspiration index spei and the supply demand drought index sddi indices led to stronger future changes in its characteristics than the changes in spi and the standardized runoff index sri various studies have confirmed that the mitigating effect of increasing precipitation on drought will be offset by rising temperatures or consequently increasing evaporation sheffield and wood 2008b wang and chen 2014 the study should be repeated using different drought indices such as the spei vicente serrano et al 2010 the palmer drought severity index pdsi palmer 1965 which consider precipitation temperature and other various hydroclimatic variables 5 conclusion this study is designed to update the knowledge about the future changes of drought events identified with a clustering algorithm in space and time in the 21st century over mainland china using the latest start of the art cmip6 simulations the quantile mapping approach reduces the biases of precipitation of cmip6 simulations a clustering algorithm is used to identify the meteorological drought characterized by the spi at 3 month time scale in both historical and future periods for three scenarios ssp1 2 6 ssp2 4 5 ssp5 8 5 the major conclusion can be summarized as follows 1 the corrected precipitation of the cmip6 models by the qm method shows better performance than the raw precipitation in capturing the drought situation over the study area a higher frequency of drought events from june to september is identified both in historical and future periods under different scenarios there appears a significant increase in future drought frequency during sep dec 2 the research has also shown that cmip6 simulations project a southward shift of the drought center over mainland china relative to the historical period indicating a relatively high drought frequency in south china in the 21st century 3 future drought events are projected to have shorter duration smaller area but higher intensity under the three scenarios over mainland china in the 21st century besides an increasing probability of drought events with extreme intensity is projected according to the investigation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was financially supported by the national natural science foundation of china grant no 41890823 the national key research and development program of china grant no 2016yfc0402709 and the fundamental research funds for the central universities grant no 2042020kf0005 thanks to the national meteorological information center http data cma cn for offering the precipitation dataset we also thank the climate modeling groups for producing and making available their model output the earth system grid federation to archive the data and provide access all data from cmip6 simulations used in this study are freely available from the earth system grid federation https esgf node llnl gov search cmip6 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2021 126643 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
4337,understanding drought dynamics spatially and temporally is essential for environmental and socio economic systems especially under climate change with expected increasing drought this study aims to update the knowledge about drought spatiotemporal changes over mainland china in the 21st century by utilizing the latest coupled model intercomparison project phase 6 cmip6 datasets precipitation from the nine cmip6 models is first bias corrected using a quantile mapping approach and then used to calculate the 3 month standardized precipitation index spi for the historical period and future three scenarios ssp1 2 6 ssp2 4 5 and ssp5 8 5 we conduct a clustering algorithm to identify three dimensional i e latitude longitude and time drought events to access the drought dynamics in both space and time results show that the drought events occur more frequently during june to september than the other months for both historical and future periods the drought frequency is significantly increasing during sep dec in the 21st century increase by about 8 5 p value 0 05 compared with the historical period drought centers shift toward south under the future projections indicating a relatively high drought frequency in south china additionally drought events are projected to have shorter duration smaller affected area but higher intensity under all three scenarios the study contributes to our comprehensive understanding of how future drought characteristics will change due to climate change from the perspective of spatiotemporal dynamics these results are helpful to drought risk management and provide information for developing drought adaptation strategies keywords meteorological drought events cmip6 spi mainland china climate change 1 introduction drought occurs when there is a continuous negative abnormality in precipitation water resources soil moisture and other hydroclimatic related variables compared to normal level over an extended period demuth and stahl 2001 it is one of the prevailing natural hazards that can exert extensive damage on human environmental and economic systems aryal and zhu 2019 mishra and singh 2011 sam et al 2019 sheffield and wood 2008a zhai et al 2020 drought has been observed to increase in frequency and tend to be more severe in severity over africa southern europe east and south asia eastern australia and many parts of the northern mid high latitudes from the historical observations since the middle 20th century chen and sun 2015 dai 2011 2013 ma et al 2019 mishra and singh 2010 spinoni et al 2014 wu et al 2020 giving the ongoing climate change and intensified human activities it is crucial to understand how droughts change in both space and time for both historical observations and future projections cook et al 2020 pokhrel et al 2021 sun et al 2019 ukkola et al 2020 zscheischler et al 2018 drought is expected to worsen under future projections cook et al 2020 2014 dai et al 2018 naumann et al 2018 trenberth et al 2014 wang 2005 zhang and zhou 2015 the fifth assessment report ar5 of intergovernmental panel on climate change ipcc claimed that drought is likely to become more intense and extensive in the subtropical dry zone by the end of 21st century ahmadalipour et al 2017 cook et al 2014 dai et al 2018 trenberth et al 2014 wang et al 2018 global warming will enlarge the likelihood of severe pervasive and irreversible impacts of drought on human society and ecosystems cai et al 2014 dai et al 2018 ipcc 2014 recent released coupled model intercomparison project phase 6 cmip6 provides advanced ensemble simulations based on the latest state of art climate models eyring et al 2016 the cmip6 is believed to have higher skill in reproducing the observed large scale mean surface temperature and precipitation patterns than cmip3 and cmip5 bock et al 2020 xin et al 2020 zhu et al 2020 new scenarios have also been designed in cmip6 based on the combination of the shared socioeconomic pathways ssps o neill et al 2017 and forcing levels of the representative concentration pathways rcp moss et al 2010 they consider the substantial improvements of emissions land use scenarios model parameterization and physical process several previous studies have updated the future changes in drought with the outputs from cmip6 models cook et al 2020 indicated robust drying and increases in extreme drought occurrence across many regions by the end of the 21st century based on a multi model ensemble from cmip6 zhai et al 2020 found significant increases in drought conditions primarily in the north west sub region over south asia with longer duration and higher intensity during 2020 2099 using a five model ensemble mean from cmip6 under three scenarios ssp1 2 6 ssp2 4 5 and ssp5 8 5 ukkola et al 2020 revealed robust changes in the frequency and duration of seasonal meteorological drought over 45 percent of the global land area used cmip6 projections as changes in drought exhibit high variability in both space and time it is of great significance to characterize future drought changes in various climate zones and regions cook et al 2014 dai et al 2018 gu et al 2020 naumann et al 2018 drought naturally occurs over spatially adjacent areas and temporally continuous periods it is characterized by duration representing the persistent period area indicating the spatial coverage and severity reflecting overall magnitude of a drought event haslinger and bl√∂schl 2017 herrera estrada et al 2017 li et al 2020 liu et al 2021 lloyd hughes 2012 sheffield et al 2009 zhu et al 2019 however many previous studies primarily analyzed one dimensional time series of drought variables precipitation drought indices streamflow etc the one dimensional methods include areal average based station based or grid based ways chen and sun 2017 chen et al 2013a 2013b leng et al 2015 liang et al 2018 wang and chen 2014 in recent years a growing body of literature has started paying attention to three dimensional drought structures andreadis et al 2005 huang et al 2018 liu et al 2021 lloyd hughes 2012 sheffield et al 2009 zhai et al 2017 a clustering algorithm based on three dimensional structures i e latitude longitude and time was developed to identify individual drought events and their spatial extent for comprehensive assessments of drought spatiotemporal dynamics andreadis et al 2005 lloyd hughes 2012 sheffield et al 2009 the clustering algorithm has been successfully used for drought event identification in historical period over china li et al 2020 shao et al 2018 wang et al 2011 xu et al 2015 zhai et al 2017 the continental united states andreadis et al 2005 wang et al 2009 korea kim et al 2011 europe haslinger and bl√∂schl 2017 africa zhan et al 2016 and globally sheffield et al 2009 additionally future changes in the spatiotemporal pattern of drought events for china identified by the clustering algorithm have been analyzed based on cmip5 models huang et al 2018 su et al 2018 yang et al 2020 although some efforts have been made using cmip6 simulations to analyze projected changes in drought cook et al 2020 ukkola et al 2020 zhai et al 2020 the ability of the new released cmip6 simulations in drought identification and evaluation considering the three dimensional structures has not been well documented china as one of the major hot spots for high intensity and frequently occurred droughts has suffered from severe drought events leading to large socioeconomic losses as the impacts of large scale climate variability including the east asian monsoon chen and sun 2015 li et al 2020 qin et al 2015 wang et al 2012 yu et al 2014 zou et al 2005 for example the extreme drought event that swept across southwest china during 2009 2010 led to extensive damage with economic losses of nearly 30 billion u s dollars short of drinking water of over 16 million people and 11 million livestock destruction of crops in 4 million hectares of farmland lu et al 2011 yang et al 2012 evidence from the historical observations showed a tendency towards more severe droughts since the late 1990s for china temporally yu et al 2014 there was an increasing trend of drought events over southwest and northern china spatially ma and ren 2007 ma et al 2018 cmip5 future projections show increases in drought risk with increasing drought intensity areal coverage and economic losses over china chen and sun 2017 gu et al 2020 huang et al 2018 leng et al 2015 liang et al 2018 su et al 2018 sun et al 2019 wang and chen 2014 considering the improvements in precipitation simulation based on cmip6 models compared with cmip5 models bock et al 2020 xin et al 2020 zhu et al 2020 it is also of great interest to access the future changes in drought over china the meteorological drought generally related to the deficiencies in precipitation has attracted the most attention as it is usually the prerequisite to other types of droughts e g hydrological droughts and agricultural droughts hu et al 2021 she and xia 2018 wilhite and glantz 1985 wu and chen 2019 this study characterizes the meteorological drought in mainland china by the widely extensively used drought index the standardized precipitation index spi mckee et al 1993 for its simplicity in computation and ability in drought detection at multiple time scales cheval 2015 hayes et al 2011 li et al 2020 nasrollahi et al 2015 song et al 2020 the projections from nine cmip6 models are employed to represent the future changes in precipitation and drought the outputs from cmip6 models are corrected using the quantile mapping method bo√© et al 2007 gudmundsson et al 2012 to reduce the bias relative to observation the spatiotemporal drought dynamics and comprehensive assessments of the spatial extent of drought are investigated by the clustering algorithm andreadis et al 2005 lloyd hughes 2012 sheffield et al 2009 furthermore the drought center trajectory in future periods is also analyzed to reveal the potential spatial change of drought in mainland china the objectives of this study are to 1 evaluate the ability of cmip6 models after bias correction in reproducing precipitation pattern and drought characteristics during the historical period 2 assess the drought characteristics in the future projections using the clustering algorithm 3 investigate the change in the trajectory of drought center in the 21st century over mainland china 2 data and methodology 2 1 datasets the monthly gridded precipitation for mainland china from 1980 to 2014 with a spatial resolution of 0 5 0 5 is obtained from the national meteorological information center http data cma cn which is generated from 2400 gauged observed precipitation data with strict quality controls wu and gao 2013 zhou et al 2016 these data are served as the baseline for the bias correction of raw cmip6 models monthly outputs of precipitation data from nine global climate models gcms from cmip6 including the historical 1850 2014 simulations and projections 2015 2100 under three scenarios are collected from https esgf node llnl gov search cmip6 to assess future changes of drought in the study area table 1 eyring et al 2016 the three scenarios from scenariomip o neill et al 2016 are ssp1 2 6 2 6 w m 2 low forcing sustainability pathway ssp2 4 5 4 5 w m 2 medium forcing middle of the road pathway and ssp5 8 5 8 5 w m 2 high end forcing pathway respectively these future precipitation data are first resampled to the spatial resolution of 0 5 0 5 by the bilinear interpolation method and then are bias corrected based on observed data in each grid cell over mainland china we divide the entire period into the historical period 1980 2014 and four future periods 2021 2040 2041 2060 2061 2080 2081 2100 we divide mainland china into six subregions http www resdc cn fig 1 to better investigate the spatial pattern of future changes in drought the subregions include northeast china nec north china nc northwest china nwc southwest china swc central south china csc east china ec 2 2 bias correction method the quantile mapping qm method bo√© et al 2007 cannon et al 2015 chen et al 2013a 2013b fang et al 2015 teng et al 2015 is employed to correct the bias between observed and gcm simulated monthly precipitation by matching their empirical cumulative distribution functions ecdfs the qm is favorable due to its simplicity nonparametric configuration and effective correction of bias in mean standard deviation and quantiles which outperforms many other empirical statistical downscaling theme√ül et al 2012 2011 the bias corrected precipitation p m t at time t by qm is obtained from the following transfer function 1 p m t f o 1 f m p m t where fo and fm are the ecdfs of respectively observed data po denoted by the subscript o and modeled data pm denoted by the subscript m f 1 indicates the inverse ecdfs i e quantile functions temporal cross validation is applied dividing the data period into calibration and independent validation period with the same length the gcm simulated precipitation is first split into two periods 1980 1996 and 1997 2015 then we construct the continuous precipitation series from two validation periods each generated using another period for calibration the qm method is conducted using the r package qmap gudmundsson et al 2012 we use the bias correction method for both the historical and future precipitation simulations for each grid cell the effectiveness of the bias correction method for gcm simulated precipitation is evaluated based on the taylor diagram which provides a concise statistical summary of how well patterns match each other taylor 2001 the taylor diagram presents three statistics the pearson correlation coefficient cc the centered root mean square error rms and the standard deviation sd the cc related to the azimuthal angle gauges similarity in pattern between the simulated and observed models the rms in the simulated model is proportional to the distance from the point on the x axis identified as observed the sd of the simulated model is proportional to the radial distance from the origin it is used to quantify the degree of correspondence between the modeled and observed behavior in terms of these three statistics with higher cc lower rms and closer sd relative to the observation indicating the better performance of models 2 3 drought event identification 2 3 1 standardized precipitation index the spi mckee et al 1993 uses the two parameter gamma distribution to fit the cumulative monthly precipitation time series and transform the cumulative probability density functions to the drought index value by the inverse standard norm distribution the general process of spi calculation is summarized as eq 2 2 spi œÜ 1 h h x q 1 q g x g x k Œ∏ 1 Œ∏ k Œ≥ k 0 x t k 1 e t Œ∏ d t x 0 where œÜ 1 is the inverse function of the standard normal distribution h x is the cumulative probability of precipitation q is the frequency of zero precipitation x is the precipitation without zero values g is the gamma cumulative distribution function k and Œ∏ are the shape and scale parameters of gamma distribution estimated by the maximum likelihood method for drought comparison between different periods we apply the derived parameters of gamma distribution based on historical precipitation to fit future projections ahmadalipour et al 2017 leng et al 2015 2 3 2 clustering algorithm the drought event is identified when the spi value is below a specific threshold value usually 1 li et al 2020 xu et al 2015 zhai et al 2017 spi can be computed for different time scales to capture both short term 3 months and long term drought 3months in this study the 3 month time scale is used to characterize drought events with the capability of representing the seasonal droughts xu et al 2015 the three dimensional i e latitude longitude and time clustering algorithm proposed by andreadis et al 2005 is applied to identify individual drought events with coherent space and time structure the algorithm merges grid cells under drought which are adjacent in space and continuous in time generally by two separate steps namely spatial clustering and temporal connection andreadis et al 2005 li et al 2020 lloyd hughes 2012 sheffield et al 2009 xu et al 2015 i for each month a spatial smoothing preprocessing step is first done with a 3 3 median spatial filter for ensuring minimum distortion of the original data all grid cells with an spi value below a predetermined value 1 in this study are identified as being under drought and the drought patches are extracted by clustering the spatially adjacent grid cells under drought then any drought patches with areas less than a minimum cluster area threshold are filtered out which no longer participate in the following computation consequently the individual drought patches are identified over time ii on two continuous months the drought patches merge to form a larger drought event or break up into multiple smaller drought events in both cases the smaller droughts are considered to be part of the larger drought hence any pairs of drought patches between the two successive months with an overlap area larger than a predetermined threshold are considered the same drought event this step is repeated for all months and finally all individual drought events are identified as a series of drought patches in continuous time in this study the minimum cluster area threshold is set to 150 000 km2 approximately sixty 0 5 0 5 grid cells which is suggested to be the most suitable threshold in china with an area of 9 6 million km2 by wang et al 2011 a similar threshold is selected in several previous researches on drought event identification in china li et al 2020 shao et al 2018 wang et al 2011 xu et al 2015 2 3 3 drought event characteristics to further characterize the identified drought events several statistic indices are calculated as follows xu et al 2015 i duration d is the persistent time of a drought event month ii severity s indicates a water storage deficit over the whole drought duration and spatial extent and is calculated as 3 s n i 1 n ion j 1 n lat k 1 n t s i j k s i j k s p s i j k s 0 a r e a i j k t i m e i j k where sn is the severity of the nth drought event km2month s is the severity of single grid cells km2month s 0 is the threshold of spi for identification of drought s 0 1 in this study area is the areas of grid cells km2 time is the heights of grid cells one month iii affected area a is the projected area over the longitude latitude surface in the 3 dimensional space time domain iv intensity i is the ratio of the drought severity to the drought duration and the affected area i s d a v centroid c represents the center location llong llat ltime of drought event which is computed by the weighted severity in the 3 dimensional space time domain as eq 4 4 l long i 1 n l on j 1 n lat k 1 n t s i j k l o n g i s n l lat i 1 n l on j 1 n lat k 1 n t s i j k l a t j s n l time i 1 n l on j 1 n lat k 1 n t s i j k t i m e k s n where llong llat ltime are the longitude latitude and month of a drought centroid long lat time are the longitude latitude and month of the center of grid cells 3 results and discussion 3 1 evaluation of cmip6 models 3 1 1 simulation of cmip6 models of historical precipitation the taylor diagram of raw and corrected precipitation of nine gcms fig 2 shows that both raw and corrected gcms have relatively high cc 0 9 0 95 and low rms 15 20 however the sds of most eight of nine corrected models are smaller than the raw models indicating a better performance of corrected models than the raw models therefore the corrected gcms generally agree better with observation with slightly lower rms and more consistent sd than the observed data from comparing the annual cycle of precipitation in fig 2 b the raw gcm simulated precipitation values overestimate the observation and show large variation the average value of monthly precipitation of the corrected models is much more consistent with the raw modes the standard deviation of the corrected models is smaller with the observation than the raw models which indicates the efficiency of the qm method for bias correction of gcms overall the evaluation of bias correction in both cases indicates a good capability of the corrected gcms in simulating precipitation over mainland china 3 1 2 future precipitation changes in mainland china with cmip6 the annual mean precipitation anomalies relative to the historical average 1980 2014 over six sub regions under different scenarios is shown in fig 3 cmip6 models exhibit a significant p value 0 05 in the mann kendall trend test mann 1945 increasing precipitation trend throughout the 21st century in six sub regions over mainland china under all the scenarios the ssp5 8 5 shows the largest and continued increasing annual precipitation trend over different sub regions among the three scenarios the ssp1 2 6 and ssp2 4 5 exhibit increasing trend but with smaller rate in annual precipitation relative to the ssp5 8 5 we also examine the temporal change of future annual precipitation across mainland china using sen s slope method hipel and mcleod 1994 sen 1968 the spatial pattern of these changes is shown in fig s1 the annual precipitation will increase throughout mainland china in the 21st century all three scenarios reflect a similar spatial pattern of future changes in annual precipitation over mainland china in comparison ssp5 8 5 of highest greenhouse gas emissions reveals a robust increase compared to the other two scenarios under ssp5 8 5 the percentage change in annual mean precipitation reaches 50 for 2015 2100 relative to 1980 2014 over west china for ssp2 4 5 and ssp5 8 5 there are statistically significant increases throughout mainland china at the 95 confidence level p values 0 05 with a stronger increasing trend in southwest china and southeast china this projected precipitation trend is in line with the study by chen and frauenfeld 2014 using cmip5 simulations 3 2 simulation of historical droughts by the cmip6 to examine the capacity of cmip6 models in reproducing the drought characteristics over mainland china the spi is first calculated using the observed and bias corrected gcm precipitation and the drought events are then identified based on the clustering algorithm the overall statistics of drought events in the historical period are summarized in table 2 here we only consider the drought events longer than three months shao et al 2018 wang et al 2011 xu et al 2015 the total numbers of drought events identified by nine cmip6 models are slightly higher compared to the observation 84 with cesm2 having the most 99 and bcc csm2 mr having the least 84 but equal to the observation the multi model ensemble mean number of drought events is 92 which is 8 more than the observation regarding drought intensity ec earth3 veg presents the largest value 0 303 while canesm5 presents the smallest value 0 259 the ensemble mean of drought intensity is 0 278 which shows good consistency with the observation 0 288 the ensemble mean of drought severity and area based on cmip6 simulations are 2 504 106km2month and 1 820 106km2 respectively which are both slightly larger than the observation with the corresponding values of 1 959 106km2month and 1 506 106km2 respectively among the nine models cesm2 waccm identifies the largest severity 2 751 106km2month and area 2 018 106km2 while ipsl cm6a lr gives the smallest severity 2 211 106km2month and area 1 638 106km2 as for drought duration all models except ec earth3 veg show a slightly larger value than the observation 4 55 months and the ensemble mean 4 90 months drought duration is close to the observation although there is a slight difference between several statistics among the nine cmip6 models and the observations the satisfied performance of considered cmip6 models in capturing drought events during the historical period over mainland china supports the future drought assessment in the study area 3 3 future changes of drought events 3 3 1 changes in drought statistics during 2021 2100 table 3 summarizes the statistics of drought events duration 3 months during future periods under different scenarios a slightly decreasing trend of drought events can be seen in future periods while the intensity of drought events turns to be higher in the future under different scenarios we found a robust change in the number 52 35 and intensity 0 313 0 391 of droughts under ssp5 8 5 continued decreases in severity can be seen for ssp1 2 6 and ssp2 4 5 while an increasing trend in severity is shown before 2080 for ssp5 8 5 future drought events show a slightly decreasing trend in both area and duration under different scenarios 3 3 2 temporal change pattern of drought here we identify the occurrence month of drought centroid for each drought event and then calculate the frequency of drought event for each month in different periods under three scenarios the future changes of drought event frequency in various months are given in fig 4 we found a higher frequency of drought event from june to september the multi model ensemble mean frequency is 43 8 during the historical period 40 3 in 2021 2040 43 2 in 2041 2060 41 0 in 2061 2080 and 45 2 in 2081 2100 respectively in both historical period and future periods under different scenarios the frequent drought events in summer june july and august and autumn september october and november will largely affect the crop production as this period is essential for the normal growth and development of crops liu et al 2018 porter and semenov 2005 potop et al 2012 long term water shortages may greatly reduce crop yields and even cause severe damage to crops compared with historical simulations the frequency of drought events decreases from april to july while there appears a positive trend from september to december for example the mean drought frequency for april july is 39 3 in the historical period 33 3 in 2021 2040 32 6 in 2041 2060 35 9 in 2061 2080 and 33 3 in 2081 2100 respectively under the ssp5 8 5 whereas the corresponding values for september december turn to be 31 6 38 1 34 9 37 9 and 35 5 respectively additionally a non parametric kruskal wallis hollander et al 2013 test is applied to determine the significance of the difference in the frequency of drought events between future and historical periods in general the cmip6 models project a slight without statistical significance in kruskal wallis test p value 0 05 decrease in the frequency of drought events during april july in the 21st century 2015 2100 by about 4 5 an average difference between future and historical period under three scenarios and a significant p value 0 05 increase in drought frequency during september december by about 8 5 relative to the historical period 1980 2014 it is worth noting that there are obvious differences in future changes of drought event frequency under different scenarios such as the continuously decreasing trend of frequency on may and june in future periods for ssp5 8 5 which is inconsistent with the other two scenarios 3 3 3 spatial change pattern of drought we count the drought events with centroids located in six subregions in different periods under three scenarios respectively to investigate the future changes in the spatial pattern of drought in the study area as shown in fig 5 there appears an increase in future drought frequency in csc and swc and a decrease in nc and nec relative to the historical period the cmip6 models seem to project more robust changes under ssp5 8 5 table s1 for example the future drought frequency in nc may significantly p value 0 01 drop by 5 7 under ssp5 8 5 while the decrease turns to be 2 3 p value 0 05 under ssp2 4 5 and 1 8 without statistical significance under ssp1 2 6 the ssp5 8 5 also indicate the significant p value 0 05 increase in future drought frequency by 3 6 in csc and 5 2 in swc respectively we further explore the spatial shift of drought centroid in future periods under different scenarios the spatial distribution of drought centroids for ipsl cm6a lr in both historical and future periods under three scenarios is shown in fig 6 the grey points and gradient colored points denote historical and future drought events respectively at the end of 21st century the projected positions of drought centroids are primarily located in south china for ssp2 4 5 and ssp5 8 5 indicating a southward shift of drought events relative to the historical period more significant changes in the far term future after 2080 are observed for ssp5 8 5 similar results for other cmip6 models can be found in figs s2 s9 additionally the shift direction of future drought center is obtained based on the average locations of drought centroids in the historical period and future period under different scenarios in fig 7 the colors denote different cmip6 models and the arrows represent the direction of shift path of future drought centroids under the three scenarios the majority of shift paths are pointing southwest suggesting a southwestward shift of future drought centroids for most cmip6 models although there exist some differences in the shift path in a few models and scenarios miroc6 and cams csm1 0 exhibit a northward shift of drought center under ssp1 2 6 while the shift turns to the southwest for ssp2 4 5 and ssp5 8 5 by miroc6 most models indicate a southwestward shift of drought center with consistence under three scenarios while ec earth3 veg projects a southeastward shift previous studies also indicated similar changes in drought frequency and severity using cmip3 cmip5 and other datasets for historical and future periods for example chen et al 2013 projected a slight increase in the frequency of some regions of southern china using cmip3 models and a regcm3 simulation chen et al 2013 indicated increasing drought risk in southwest china based on the projected decrease in pdsi derived from cmip5 simulations these studies projected increasing drought frequency in southern china based on mere spatial changes of drought index or grid based drought event identification with a one dimensional method huang et al 2018 applied the intensity area duration analysis for future changes in drought events at a space time dimension they found the drought center would probably shift towards southeast china projected by the regional climate model cosmo clm the present study using the cmip6 multi model projections supports the previous findings that there are increasing drought events in southern china chen et al 2013a 2013b huang et al 2018 wang and chen 2014 wen et al 2020 yao et al 2016 3 3 4 changes of probability density function of drought characteristics future changes in mean and standard deviation of drought statistic indices intensity severity area and duration in different periods under three scenarios over mainland china are presented in fig 8 we use the non parametric kruskal wallis test to determine the significance of the difference in the mean and standard deviation of drought statistic indices during different periods in terms of the mean value there appears a significant increase in intensity p value 0 05 for three scenarios while decreasing trend with statistical significance p value 0 05 in area and duration of droughts can be found under three scenarios except for the case that the mean area shows the insignificant difference p value 0 14 for ssp5 8 5 the severity of droughts shows a slight decrease p value 0 05 in future periods under ssp1 2 6 while the trend is not evident for the other two scenarios p values are 0 15 and 0 83 for ssp2 4 5 and ssp5 8 5 respectively results indicate the trend of future drought events with higher intensity shorter duration and smaller affected area as for the standard deviation the intensity variability exhibits a clear increase p value 0 05 in future periods under three scenarios implying an increasing probability of drought event with extreme intensity in addition to the higher average intensity during the 21st century there is insignificant difference in the standard deviation of severity during different periods for ssp2 4 5 p value 0 14 and ssp5 8 5 p value 0 54 while the standard deviation of drought affected area and duration show a robust decrease p value 0 05 in future periods under three scenarios compared to the historical period suggesting a more uniform distribution of area and duration of future droughts furthermore the kernel density estimates sheather and jones 1991 of four drought statistic indices intensity severity area and duration in different periods under three scenarios are illustrated in fig 9 for a clear understanding of the distribution of drought event characteristics in terms of drought intensity the mode of density curves peak point of the curve in future periods shows a lower and rightward trend than the historical period indicating increases of future drought events with higher intensity besides it should also be noted that the density curve is flattened to both the left and right showing increasing variance as consistent with fig 9 b and due to the shift of the density curve toward the right larger density in future periods can be seen at the tail of density curves i e intensity 0 5 relative to the historical period which demonstrates that future drought events with more extreme intensity are increasing the changes are more robust for ssp5 8 5 with a wider range of density curves it can be concluded that future droughts will become more extreme under global warming there is no apparent change in the severity of future drought events future changes in density of area and duration of future droughts exhibit a higher peak of density at a lower value indicating a more frequent tendency of future drought events with shorter duration and smaller area we further calculate the quantiles multi model ensemble mean of area and duration corresponding to the probability of 90 in different periods under three scenarios table 4 a continued decreasing trend of 90th quantiles of area and duration can be found during the 21st century under different scenarios for instance the robust decrease of area 90th quantile reaches 1 092 106km2 after 2080 for ssp5 8 5 2 784 106km2 compared to the historical period 3 876 106km2 in terms of duration the decrease is 1 8 months after 2080 for ssp5 8 5 6 2 months relative to the historical period 8 0 months thus there is a tendency for drought events with shorter duration and a smaller area in future periods to further investigate future changes of extreme drought events with higher intensity the ecdf multi model ensemble mean of drought intensity in different periods under three scenarios is shown in fig 10 the zooming plots with exceeding probabilities p x x of extreme intensity 20 are also presented evident increases in probabilities of drought intensity 0 5 can be found in future periods under three scenarios especially for the ssp5 8 5 for example compared to the probability of 2 6 in the historical period future values are 7 7 in 2021 2040 11 3 in 2041 2060 11 2 in 2061 2080 and 18 1 in 2080 2100 for the ssp5 8 5 respectively thus extreme drought events with high intensity will be more frequent during the 21st century together these results provide important insights that the duration and extent of drought events will decrease in contrast the drought intensity is projected to increase over mainland china in the 21st century under different scenarios relative to the historical period according to the cmip6 simulations the finding of an increasing trend in drought intensity broadly supports evidence from previous observations chen and sun 2017 huang et al 2018 liang et al 2018 su et al 2018 wang and chen 2014 with the frequent occurrence of high intensity drought events the security and stability of water supply and agricultural production will face huge threats in china the cmip6 models project a shorter duration and affected area of drought events relative to those during the historical period in this study which is contrary to previous studies several lines of evidence suggest the increase in drought duration during the 21st century chen and sun 2017 huang et al 2018 leng et al 2015 this rather contradictory result may be partly due to the fact that the drought duration was identified in these studies by a grid based one dimensional approach without considering the spatial temporal structure of drought events concerning the drought affected area huang et al 2018 indicated that the areas will slightly decrease for short term drought events 1 3 months but increase strongly for long term drought events 12 months and the tendency was not consistent for all scenarios it is encouraging to compare the projected changes in drought areas with that found by su et al 2018 they suggested that the average drought coverage will decrease for the 1 5 c and 2 0 c levels relative to the reference period over china overall the cmip6 models used in this study project that future drought events tend to have higher intensity shorter duration and smaller affected area in the 21st century 4 discussion the current study set out to assess the meteorological drought change in the 21st century based on cmip6 multi model ensemble projections over mainland china one major finding is an increase in the frequency of drought events in southern china in the future several reports have shown that drought stress could inhibit vegetation growth peng et al 2011 zhang et al 2010 zhao and running 2010 the southward shift of drought center means the projected drought risk for northern china is likely to decrease which benefits the ecosystem restoration programs in northern china such as the three north shelter forest program on the other hand the potential impacts of projected frequent extreme drought with high intensity in the 21st century on vegetation growth cannot be ignored several studies indicated the improvement of cmip6 models over cmip5 models in drought assessment which is partly attributed to better performance in several drought related processes ecosystem processes hydrologic process like terrestrial water storage runoff and surface energy partitioning cook et al 2020 zhai et al 2020 regional drought changes in the current study are broadly consistent between cmip5 and cmip6 cook et al 2020 zhai et al 2020 however our finding of future changes in drought duration and area based on cmip6 projections is contrary to previous studies which have suggested an increase in drought events with longer duration and larger extent using cmip5 projections chen and sun 2017 huang et al 2018 leng et al 2015 with the improvement of cmip6 more research is needed to further determine how cmip6 offers confidence in drought risk assessment previous studies have indicated that precipitation in china and even globally is increasing in the future chen and frauenfeld 2014 jiang et al 2020 westra et al 2014 as precipitation is a determining driver for meteorological drought it is expected that future drought conditions will be mitigated with increasing precipitation however it turns out that droughts are projected to intensify in the future according to the results of the present study and previous relevant studies trenberth et al 2014 ukkola et al 2020 wu et al 2020 the question of whether an increase in precipitation mitigates drought conditions has been discussed by gu et al 2020 the study evaluated the drought frequency drought duration and severity across drought events in the historical and future periods near and far from 31 climate model simulations using spi the results indicated deteriorated drought conditions in central and southern china and mitigated drought conditions in northeastern and western china which is in line with our study gu et al 2020 also suggested that a wetting atmosphere does not imply mitigation in meteorological drought conditions a possible reason could be that changes in mean precipitation control changes in drought duration while the variability drives intensity changes in addition to the mean precipitation ukkola et al 2020 the increase in precipitation variability leads to an increase in extreme precipitation in the future resulting in severe floods and droughts for example sillmann et al 2013 projected future intensification of dry conditions in australia central america south africa and the mediterranean due to the increases in consecutive dry days decreases in heavy precipitation days and maximum consecutive 5 day precipitation finally we should underline that there also exists some uncertainties in the detected changes of drought derived from the used future precipitation datasets and the selected drought calculation indices burke and brown 2008 emphasized the importance of drought indices selection and argued that future drought projections will depend on the specifically used drought definition the spi used in this study is designed to identify meteorological droughts according to the variability of precipitation if some other variables such as temperature evaporation soil moisture are considered in the drought index calculation the tendency of drought may also be changed under climate change dai et al 2004 hu and willson 2000 touma et al 2015 trenberth et al 2014 for example touma et al 2015 showed that the great influence of temperature changes in the standardized precipitation evapotranspiration index spei and the supply demand drought index sddi indices led to stronger future changes in its characteristics than the changes in spi and the standardized runoff index sri various studies have confirmed that the mitigating effect of increasing precipitation on drought will be offset by rising temperatures or consequently increasing evaporation sheffield and wood 2008b wang and chen 2014 the study should be repeated using different drought indices such as the spei vicente serrano et al 2010 the palmer drought severity index pdsi palmer 1965 which consider precipitation temperature and other various hydroclimatic variables 5 conclusion this study is designed to update the knowledge about the future changes of drought events identified with a clustering algorithm in space and time in the 21st century over mainland china using the latest start of the art cmip6 simulations the quantile mapping approach reduces the biases of precipitation of cmip6 simulations a clustering algorithm is used to identify the meteorological drought characterized by the spi at 3 month time scale in both historical and future periods for three scenarios ssp1 2 6 ssp2 4 5 ssp5 8 5 the major conclusion can be summarized as follows 1 the corrected precipitation of the cmip6 models by the qm method shows better performance than the raw precipitation in capturing the drought situation over the study area a higher frequency of drought events from june to september is identified both in historical and future periods under different scenarios there appears a significant increase in future drought frequency during sep dec 2 the research has also shown that cmip6 simulations project a southward shift of the drought center over mainland china relative to the historical period indicating a relatively high drought frequency in south china in the 21st century 3 future drought events are projected to have shorter duration smaller area but higher intensity under the three scenarios over mainland china in the 21st century besides an increasing probability of drought events with extreme intensity is projected according to the investigation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was financially supported by the national natural science foundation of china grant no 41890823 the national key research and development program of china grant no 2016yfc0402709 and the fundamental research funds for the central universities grant no 2042020kf0005 thanks to the national meteorological information center http data cma cn for offering the precipitation dataset we also thank the climate modeling groups for producing and making available their model output the earth system grid federation to archive the data and provide access all data from cmip6 simulations used in this study are freely available from the earth system grid federation https esgf node llnl gov search cmip6 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2021 126643 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
4338,when simulation optimization s o is used for groundwater contamination source identification gcsi to reduce the calculation load and time generated by calling the simulation model a surrogate model is often used instead of the simulation model however when the conversion relationship between the simulation model inputs and outputs is complex and the model is highly nonlinear the commonly used approach of building a surrogate model may lose its advantage this study use and check a deep learning method with the long short term memory lstm network which has great potential for characterizing the input output conversion relationship of complex nonlinear numerical simulations to a surrogate model of the simulation model the accuracy of the surrogate model developed by the lstm method was compared with that of commonly used surrogate models developed by the kriging method the radial basis function network method and the kernel extreme learning machine method the surrogate model with the highest accuracy was linked to the optimization model and the optimization model was solved to identify the contamination source information results show that compared with the other three methods the surrogate model constructed by the lstm method had the best accuracy and generalization performance lstm is therefore an effective method of building a surrogate model linking the lstm surrogate model to an optimization model and then solving the optimization model can save approximately 99 of the computing load and time otherwise required keywords groundwater contamination lstm s o surrogate model 1 introduction it is essential to obtain and interpret information about groundwater contamination sources gcss when assessing the risk of groundwater contamination managing contamination risk and determining groundwater contamination liability xing et al 2019 however gcss are buried underground and it is challenging to obtain information about them including the number location initial activity time and release history of gcss etc hence gcsi is particularly important li et al 2019 gcsi was seriously analyzed in the 1980 s gorelick et al 1983 since then many methods have been applied in gcsi including direct approaches skaggs and kabala 1994 liu and ball 1999 probabilistic and geostatistical simulation approaches bagtzoglou et al 1992 neupauer and wilson 2001 xu and g√≥mez hern√°ndez 2018 optimization approaches gorelick et al 1983 mahar and datta 2001 analytical solutions and regression approaches sidauruk et al 2010 alapati and kabala 2000 and image identification method mo et al 2019 among these the s o method has been widely used in gcsi mirghani et al 2009 ayvaz 2010 datta et al 2011 when the s o method is applied to gcsi the contaminant transport simulation model is often linked to the optimization model as an equality constraint condition the contaminant transport simulation model could ensure that contaminant transport meets the requirements of groundwater transport laws hou et al 2017 xing et al 2019 however solving the optimization model often requires performing hundreds or thousands of iterative calculations each of these calculations must call the simulation model which generates a heavy calculation load and consumes considerable time hou et al 2016 previous studies have shown that constructing a surrogate model for the simulation model and then linking the surrogate model to the optimization model for iterative calculation can effectively avoid these disadvantages zhao et al 2020 hou and lu 2018 therefore the application of surrogate models in gcsi has developed rapidly the current methods for building surrogate models are divided into three categories including data driven methods projection based methods and multifidelity methods asher et al 2015 among them the data driven methods include support vector machine methods zhang et al 2009 ouyang et al 2017a kernel limit learning machine methods jiang et al 2015 hou et al 2019 kriging methods zhao et al 2016 radial basis function neural networks bagtzoglou and hossain 2009 miao et al 2019 hou et al 2017 and artificial neural networks ann khu and werner 2003 etc the projection based methods include proper orthogonal decomposition mcphee and yeh 2008 siade et al 2012 dynamic mode decomposition ghommem et al 2013 and fourier mode reduction willcox and megretski 2005 etc the multifidelity methods include heterogeneous multiscale method weinan and engquist 2012 multigrid method ashby and falgout 1996 and residual free bubbles sangalli 2003 etc more recently it was found that the surrogate models created by various methods have their own limitations and therefore an ensemble surrogate model based on multiple single surrogate models came into being viana et al 2009 acar 2010 ouyang et al 2017b the surrogate models developed by the above methods have high accuracy and achieve good identification results when applied to gcsi jiang et al 2015 zhao et al 2016 hou et al 2019 however if the nonlinearity of the contaminant transport numerical simulation model increases for example if the dimensions or types of variables including the contamination source location release intensity and simulation model parameters etc to be identified increase the conversion relationship between the numerical simulation model inputs and outputs will become complex and the advantage of the methods described above for building a surrogate model is likely to be lost the generalization ability and accuracy of the surrogate models built by these methods will be reduced which can pose challenges to acquiring gcsi an ensemble surrogate model may relieve some difficulties but if the accuracy of a single surrogate model is poor the generalization ability and accuracy of the ensemble surrogate model will be obviously affected when constructing a surrogate model therefore it is particularly important to find methods that are robust and have excellent generalization capabilities as a deep learning method long short term memory lstm has a great advantage in imitating the input output conversion process of a complex nonlinear numerical simulation model the lstm is a special type of recurrent neural network rnn designed to solve the long term dependence of the general rnn compared with the general rnn there is a gating mechanism between the single recurrent structures of lstm which is used to control the forgetting or continuing transmission of input information because of the superior performance of the lstm method it has been applied in many fields such as machine translation ren et al 2020 part of speech tagging popov 2016 speech recognition ying et al 2020 in recent years lstm has also been applied to the study of groundwater system zhang et al 2018 used lstm to establish a machine learning model to predict the water table depth an et al 2020 combined time frequency analysis methods and lstm to simulate karst spring water discharge due to the superior performance of the lstm this study applies it to gcsi to solve the gcsi problem this study proposes the use of the lstm method in building a surrogate model for the contaminant transport simulation model the accuracy of the surrogate model built by the lstm method is compared with the accuracy of the surrogate models developed by the kriging method the radial basis function rbf method and the kernel extreme learning machine kelm method which have been commonly used in the past won and ray 2004 razavi et al 2012 bhosekar and ierapetritou 2018 the surrogate model with the highest accuracy was selected from the four surrogate models then the best surrogate model was linked to the gcsi optimization model and a genetic algorithm was used to solve the optimization model to identify the gcss information 2 methodology 2 1 long short term memory lstm network an lstm network is a kind of deep learning network based on a rnn hochreiter and schmidhuber 1997 graves and schmidhuber 2005 therefore to explain the principle of lstm the rnn should be introduced first fig 1 shows the network structure of the rnn through the loop connection on the hidden layer the network status at the previous moment can be transferred to the current moment and the status at the current moment can also be transferred to the next moment graves et al 2013 the unrolled recurrent neural network is shown in fig 1 at time t the hidden layer h receives data from two sources the value of the hidden layer h t 1 at the previous moment in the network and the current input data s t and calculates the output of the current moment using the value of the hidden layer the input s t 1 at time t 1 can later influence the output at time t through the loop structure the calculation of h t requires h t 1 the calculation of h t 1 requires h t 2 and so on hence the state at any given moment in the rnn depends on all the states in the past the forward propagation calculation of rnn is expanded according to a time series and then the network parameters are updated using backpropagation through time sun et al 2017 however the gradient may disappear as the depth of the network increases and the calculations may explode rapidly during training as such rnns are often difficult to train to overcome this disadvantage hochreiter and schmidhuber 1997 proposed the lstm method the repeating module in a standard rnn is shown in fig 1 and the repeating module in an lstm is shown in fig 2 b fig 2a and 2b clearly show that the difference between an lstm and an rnn is that the former adds a processor to the algorithm to determine whether the information is useful this processor structure is called a cell a cell contains three gate control mechanisms which are called the forget gate the input gate and the output gate an lstm stores and updates information through these gates the forget gate is a key component of the lstm it controls the retention of important information and forgets unimportant information it can also avoid the problem of gradient disappearance and explosion caused by gradient backpropagation the input gate is used to control how much of the current input data s t flows into the memory unit that is how much can be saved to c t the output gate controls the effect of the memory unit c t on the current value h t after the information enters the lstm network the gate mechanism judges whether the current information is useful according to determination rules useful information is left behind and useless information is forgotten through the forget gate the gate control mechanism of lstm is realized by a sigmoid function and a dot multiplication operation fig 2b shows the transfer mechanism of these three gates the outputs of the three gates can be calculated as follows forget gate 1 f t œÉ w f s t u f h t 1 b f input gate 2 i t œÉ w i s t u i h t 1 b i memory update 3 c t t a n h w c s t u c h t 1 b c 4 c t f t c t 1 i t c t output gate 5 o t œÉ w o s t u o h t 1 b o 6 h t o t t a n h c t 7 y t softmax w v h t b v where œÉ œÑ 1 1 e œÑ is called a sigmoid function which is a nonlinear activation function commonly used in machine learning it can map a real value to the interval 0 1 to describe how much information passes through when the output value of the gate is 0 no information is passed and when the value is 1 all information can be passed w f w i and w o are weight matrices that connect the input to the forget gate the input gate and the output gate respectively w v is weight matrice of the visual output layer u f u i and u o are weight matrices that connect the hidden layers to the forget gate the input gate and the output gate b f b i b o b v are the bias vectors of the input gate the forget gate the output gate and visual output layer respectively represents the multiplication of corresponding elements y t is the output of visual output layer 2 2 radial basis function rbf an rbf network is a kind of locally approximated feedforward neural network benghanem and mellit 2010 daliakopoulos et al 2005 yoon et al 2011 it consists of three layers of neurons the first being the input layer the second being the hidden layer also called the radial base layer referred to as the rbf layer and the third being the output layer fig 3 nonlinear mapping is performed from the input layer to the radial base layer and linear mapping is further performed from the radial base layer to the output layer compared with the error backpropagation bp neural network the rbf network converges faster and can find a global minimum daƒü and dereli 2008 kokshenev and padua braga 2010 the basic principle of the rbf network is as follows if there are n dimensional input vectors s and m dimensional output vectors œÜ the output of the i th neuron in the hidden layer of the rbf network is 8 œà i s exp s œâ i 2 q i 2 i 1 2 m where s is an n dimensional input vector œâ i is the center of the rbf of the i th hidden layer neuron i 1 2 m q i is the width of the corresponding neuron also known as the expansion constant m is the number of hidden layer radial base layer neurons the output of the k th neuron of the output layer is calculated using the following formula 9 œÜ k s w ik œà i s œÉ k k 1 2 m where œÜ k is the response value of the j th neuron of the output layer w ik is the weight of the connection of the i th hidden layer neuron to the k th output layer neuron œÉ k is the bias term of the k th output neuron giesl 2008 boyd 2010 and m is the total number of output data points the rbf neural network has a simple structure and a fast convergence speed and therefore has been widely used in many fields krzy≈ºak 2001 zhang 2007 karayiannis and yaohua xiong 2006 hu et al 2018 2 3 kriging the kriging method was proposed by the south african geologist krige and it is also known as the spatial local interpolation method matheron 1963 cressie 1990 kleijnen and van beers 2005 bargaoui and chebbi 2009 the kriging method is generally used in the field of geology dhar and datta 2010 applied the kriging method to optimize the design of the groundwater monitoring network nikroo et al 2010 interpolated the groundwater depth based on the kriging method in addition since 1989 the kriging method has been widely used to develop surrogate models of simulation models since 1989 sacks et al 1989 ryu et al 2002 hemker et al 2008 coetzee et al 2012 the basic equations of the kriging method for predicting the response relationship between inputs and outputs are as follows 10 œÜ s j 1 k f i s Œ≤ i Œæ s f s t Œ≤ Œæ s the basic equations of the kriging method can be divided into two parts with œÜ s being the output of the kriging model f s t Œ≤ is the deterministic regression function and Œæ s is the gaussian random function f s f 1 s f 2 s f k s is the basis function and Œ≤ Œ≤ 1 Œ≤ 2 Œ≤ k is the vector of basis function coefficients which can be obtained using the training data more details of the principles of the kriging method can be found in guo et al 2018 in this study when kriging method was used to build the surrogate model the basis function was second order polynomial regression function and the correlation function was gaussian correlation function 2 4 kernel extreme learning machines kelm an extreme learning machine elm randomly generates the connection weights between the input layer and the hidden layer as well as the hidden layer neuron threshold this can cause the output matrix to oscillate and reduce model stability shi et al 2014 wong et al 2015 to avoid this problem huang et al 2004 huang et al 2015 proposed the kernel extreme learning machine kelm the basic principle of kelm is as follows chen et al 2014 jiang et al 2015 for a given set m of training sample data sets s j Œª j when the hidden layer node of the elm is l and the activation function is g the elm output can be expressed as follows 11 œÜ j f l s j i 1 l Œ≤ i g œâ i s j b i j 1 2 n where g is the activation function b i is the bias of the i th hidden neuron œâ i is a weight vector that connects the input neuron and the i th hidden neuron Œ≤ i is a weight vector that connects the output neuron and the i th hidden neuron g œâ i s j b i is the output function of the hidden layer and œÜ j is the output calculated by the elm kelm is an improved method based on the elm and combined with kernel function more details of the principles of the kelm method can be found in huo et al 2018 and song et al 2018 3 case study 3 1 site overview based on examples of gcsi analysis ayvaz 2010 xing et al 2019 performed in previous studies a gcsi analysis was conducted in this study the case study area was a two dimensional inhomogeneous medium with irregular boundaries the groundwater flow was steady flow as shown in fig 4 the ab and cd boundaries of the area were the specified head boundaries the ac and bd boundaries of the area were no flow boundaries fig 4 the contaminant at the hypothetical sites was a conservative contaminant that would not undergo biological transformation or chemical changes the total simulated time of contamination transport was 10 years with a total of twenty simulation periods every six months was one simulation period based on 30 days in each month table 1 shows the basic parameters of the aquifer and the distribution of hydraulic conductivity of the aquifer is shown in fig 5 the vertical direction of the area received uniform recharge at a rate of 0 000864 m d s1 released contaminant to the aquifer from the second to the fifth simulation period and then stopped s2 released contaminant to the aquifer from the third to the fifth simulation period and then stopped the actual gcss information are given in table 2 there were six observation wells in the area the contaminant concentration of each observation well was observed from the 9th simulation period to the last simulation period fig 4 shows the locations of observation wells and the contamination source during gcsi analysis the location the initial release period the release intensity of the gcss during each release period and the periods during which contaminant stopped releasing were unknown the purpose of gcsi was to identify the unknown variables the possible location of the gcss were known as shown in fig 4 and contaminant was possibly released during the first six simulation periods 3 2 numerical simulation model according to the specific conditions of the study area we developed a numerical simulation model of groundwater flow and contaminant transport the governing equation of groundwater flow in the model can be expressed as pinder and bredehoeft 1968 singh and datta 2006 12 x i k ij h z h x j w q Œ¥ x x w Œ¥ y y w 0 i j x y where k ij is the hydraulic conductivity tensor in this study isotropic hydraulic conductivity is used as shown in fig 5 h is the hydraulic head z is the elevation of the aquifer bottom w is the vertical recharge rate and q is the pumping rate x w and y w are the abscissa and ordinate of the pumping well respectively the governing equation of groundwater contaminant transport is pinder and bredehoeft 1968 singh and datta 2006 13 Œ∏ c t x i Œ∏ d ij c x j x i Œ∏ c u i c s w i j x y t 0 where c is the dissolved concentration c s is the dissolved concentration in a source or sink flux Œ∏ is the effective porosity u x and u y are the components of pore water velocity in x and y directions respectively d ij is the hydrodynamic dispersion tensor u x u y and d ij are described as 14 u x k x Œ∏ h x x u y k y Œ∏ h x y 15 d xx Œ± l u x 2 Œ± t u y 2 u d yy Œ± l u y 2 Œ± t u x 2 u d xy d yx Œ± l Œ± t u x u y u where u is the magnitude of pore water velocity Œ± l and Œ± t denote longitudinal and transverse dispersivities respectively after the simulation model was constructed the modflow and mt3dms toolboxes in the gms software were used to perform numerical calculations langevin and guo 2010 morway et al 2013 ehtiat et al 2018 the real gcss information in the study area were input into the simulation model and the simulated contaminant concentrations were obtained fig 6 shows the distribution of contamination plumes in the study area and fig 7 shows the contaminant concentration in each observation well because of systematic and accidental error the contaminant concentrations will inevitably contain noise in an actual gcsi therefore the simulated concentrations shown in fig 7 were perturbed by a normally distributed error to simulate the case where the concentrations contain error the following formula was used to calculate the perturbed concentrations 16 c c Œµ Œ± c where c is the perturbed concentrations Œµ is normally distributed white noise with variance 0 1 and average 0 in the error matrix and Œ± is the noise intensity if Œ± 0 10 the noise level is low if 0 10 Œ± 0 15 the noise level is moderate if Œ± 0 15 the noise level is high the noise intensity in this study was equal to 0 05 after the observation data were obtained they were entered into gcsi processing 3 3 surrogate models when the optimization model is used to analyze gcsi the simulation model should be linked to the optimization model as an equality constraint when solving the optimization model hundreds of thousands of iterative calculations are required and the heavy calculation load and calculation time consumed by calling the simulation model are immeasurable developing a more accurate surrogate model for the simulation model can avoid this problem constructing a surrogate model for this study involved the following steps a apply the latin hypercube sampling method to sample the variables to be identified within their feasible regions the variables to be identified in this study were the two gcss location and the release intensity during the possible gcss release period there were four location variables x coordinates of s1 y coordinates of s1 x coordinates of s2 and y coordinates of s2 and the contaminant release intensity 12 variables of two gcss during six possible release periods to identify the approximate start and stop release time of contaminant released by gcss more possible release periods can be set five hundred groups of variables to be identified were produced and then input into the simulation model in sequence to be identified then the simulation model was run to obtain the concentrations at the observation wells corresponding to the 500 groups of inputs b according to the principle of the four methods lstm kriging rbf and kelm matlab was used to write training code 400 groups of input the variables to be identified and output the concentrations at the observation wells data were used to train the surrogate model through four methods c the remaining 100 groups of inputs and outputs were input into four surrogate models and 100 sets of output data for each of the four surrogate models were obtained the coefficient of determination r 2 the relative error re and the root mean square error rmse were used to test the accuracy of the four surrogate models the accuracy of the four surrogate models was compared and analyzed the surrogate model with the highest accuracy was selected and linked to the optimization model 3 4 nonlinear optimization model the optimization model is an important tool for gcsi this study used a nonlinear programming optimization model li et al 2019 the nonlinear optimization model includes three parts the objective function the decision variables and the constraints guo et al 2019 after the surrogate model of the simulation model was developed an optimization model was constructed to identify the accurate location and release history of groundwater contamination sources the optimization model was expressed as follows 17 min g x 1 y 1 x 2 y 2 q 1 m q 2 m t 1 12 k 1 6 c k t t c k t 0 2 500 x 1 900 1000 y 1 1380 350 x 2 750 700 y 2 950 0 q 1 m 100 m 1 2 6 0 q 2 m 100 m 1 2 6 c k t t f x 1 y 1 x 2 y 2 q 1 m q 2 m where min g x 1 y 1 x 2 y 2 q 1 m q 2 m represents the objective function q 1 m is the release intensity for s1 during each release period q 2 m is the release intensity for s2 during each release period q 2 m is the simulated concentration of the contaminant at the observation well and ct k 0 is the measured value of the contaminant concentration at the observation well c k t t f x 1 y 1 x 2 y 2 q 1 m q 2 m was the surrogate model with the highest accuracy 4 results and discussion 4 1 comparisons on the training cost and accuracy the training cost and accuracy of the four surrogate models were compared and analyzed the training cost includes two parts which are the time to obtain training and test samples and the time to train surrogate models the training cost was shown in table 3 it can be seen from table 3 that the application of the lstm method to build surrogate model took the longest time and the application of the kriging method to build surrogate model took the shortest time which took 43 8 h and 22 3 h respectively the training cost of lstm surrogate model was twice that of kriging surrogate model however the training cost of the surrogate models were negligible compared with the time saved in solving the optimization model therefore compared with the training cost the accuracy of the surrogate models were the most important combining table 3 with fig 8 revealed that the r 2 of the surrogate model constructed by the lstm method was between 0 9707 and 0 9864 the rmse was between 68 2401 and 98 1363 and the re was between 6 12 and 9 65 clearly the r 2 of observation wells 1 3 4 and 6 was in all cases greater than 0 98 and the re was not greater than 7 5 the accuracy of the surrogate model was very high the surrogate model of observation well 2 and 5 had slightly lower accuracy r 2 is 0 9798 and 0 9707 respectively and their re are both higher than 9 the r 2 of the surrogate model developed by the kriging method was between 0 8308 and 0 959 the rmse was between 108 9547 and 283 9715 and the re was between 19 50 and 31 10 the accuracy of the kriging surrogate model was uneven and compared to the surrogate model constructed using the lstm the kriging surrogate model was less accurate the r 2 of the surrogate model constructed using rbf was between 0 9161 and 0 982 the rmse was between 70 9619 and 199 9221 and the re was between 6 60 and 23 56 compared with the surrogate model constructed using lstm the surrogate model constructed using the rbf method showed worse generalization performance and the accuracy of the surrogate model was unstable the r 2 of the surrogate model of observation well 2 was only 0 9161 and the rmse and re were 199 9221 and 23 56 respectively the accuracy of this surrogate model was poor and its application to gcsi would be inappropriate the r 2 of the surrogate model constructed using kelm was between 0 8571 and 0 9639 the rmse was between 102 2076 and 260 9804 and the re was between 8 68 and 25 75 clearly compared with the surrogate model of each observation developed by the lstm method the surrogate model developed by the kelm method has no advantages from the perspective of the overall analysis the average r 2 of the surrogate model of the six observation wells constructed using the lstm was 0 9835 shown in table 3 which was the highest of the four surrogate models the average rmse and average re were the lowest among the surrogate models at 88 1485 and 7 63 respectively although the accuracy of the lstm surrogate model of observation well 2 and 5 were slightly lower than that of the other observation wells the accuracy of the lstm surrogate model of observation well 2 and 5 were much higher than that of the kriging kelm rbf surrogate models comparing the average level of the concentration fitting accuracy of all observation wells and the concentration fitting accuracy of a single observation well it was found that the accuracy of the surrogate model established by the lstm method was higher than that of the other three surrogate models the surrogate model constructed by the lstm method had relatively high accuracy and better generalization ability therefore compared with the other three surrogate models the lstm surrogate model for gcsi was the correct choice 4 2 identified contamination sources the lstm surrogate model was connected to the optimization model and a genetic algorithm ga was used to solve the optimization model guo et al 2019 xing et al 2019 the parameters values assigned for the ga were shown in table 4 when solving the optimization model the surrogate model was used instead of the simulation model for iterative calculations the location and initial release period of the gcss and the release intensity of the gcss at each release period were identified fig 9 is the convergence profile and the optimal individual value of the optimization model the numerical calculation used a pc equipped with an intel i5 2 8 ghz processor and 16 gb ram the time required to call the simulation model once when solving the optimization model was approximately 2 3 min but the time required to call the surrogate model was only 0 0465 s based on the parameters of ga in table 4 calling the simulation model for iterative calculations can spend 18 400 h calling the surrogate model for iterative calculations takes about 6 18 h plus the 43 2 h spent building the surrogate model for a total of 49 38 h the calculation load and time savings by calling the surrogate model were above 99 it should be noted here that when solving the optimization model the time it takes to call the four surrogate models is basically the same table 5 provides information on the two gcss the s1 released very few contaminant in the first simulation period which was negligible compared to the contaminant released in other periods the s1 with the contaminant being released at the start of the second simulation period and continuing until the fifth simulation period the contaminant released intensity of s2 during the first and second release periods are very small and the third period can be regarded as its initial release period the contaminant was continuously released for three periods fig 10 shows a comparison between the results of the gcsi and the real gcss values on the whole the location identification results of s1 and s2 were close to the real value and had high accuracy the identification results of the release intensity especially the release intensity of s1 during the sp1 and sp2 had a large deviation from the real value while the release intensity of s2 was closer to the real value and the identified accuracy was very high 4 3 discussion it can be clearly seen from the identification results that the identified source fluxes of s1 during the sp2 period were significantly underestimated than the real source flux while the identified source fluxes during the sp3 period were overestimated than the real source flux in contrast the identification results of s2 were obviously closer to the real information the possible reasons for this result include the following aspects 1 the source fluxes of s1 was much stronger than that of s2 so s1 had a greater contribution to the output concentration of simulation model which in turn might lead to the concentration having a greater impact on the identification results of s1 and a smaller impact on s2 2 the layout of observation wells were not reasonable the concentration obtained at the observation wells were more sensitive to s2 but less sensitive to s1 in addition too few observation wells too few observation data and insufficient constraints might also lead to the above results if increase the number of observation wells and the number of observation times it is likely to get better recognition results xing et al 2019 applied least squares support vector machines lssvm kriging and rbf methods to establish surrogate models for simulation model the results show that the accuracy of the three surrogate models had no obvious ranking of superiority and inferiority in the study of xing et al 2019 when there was only one type of input variable release intensity and the dimension of input variable was 8 the r2 of the surrogate model was only 0 9766 and the re was only 7 56 in the study of hou et al 2018 when the type of variable was 7 and the dimension of input variable was 8 source information and simulation model parameters the accuracy of the surrogate model based on kelm was higher than that of the surrogate model established by xing et al 2019 using lssvm method this was also proved in the study of li et al 2020 the variable type of input surrogate model was one and the input was 16 dimensions it can be seen that in general the accuracy of kelm surrogate model is higher than lssvm surrogate model therefore this study chose kelm method to compare the accuracy of the surrogate model which is more convincing the results showed that compared with kriging rbf and kelm surrogate models lstm surrogate model had higher accuracy lstm method can be used to establish ensemble surrogate model and also can provide more choices for other researchers however lstm has its own limitations compared with the other methods mentioned above its training speed is slower when the sequence length in this study it can be understood as the variable to be identified exceeds a certain limit related to the problem studied or the number of network layers increases to a certain number the gradient may disappear or explode during the training process which will lead to the decline of the accuracy of surrogate models this paper conducted a case study and the results show that lstm does have potential in establishing the surrogate model however this does not fully show that lstm has obvious advantages when applied to all case studies the commonly used methods to build surrogate models include the data driven methods projection based methods and multifidelity methods lstm method belongs to the data driven methods for decision making problems where very short runtimes are required data driven approaches are the obvious choice and are suitable for most researchers the multifidelity and projection based methods is more appropriate to developers of groundwater model codes rather than everyday users because they are intrusive asher et al 2015 5 conclusions the surrogate model is often linked to an optimization model instead of a contaminant transport simulation model when applying optimization methods to gcsi however when the conversion relationship between the inputs and outputs of the contaminant transport simulation model is complex and highly nonlinear better accuracy of the surrogate models is needed to effectively identify the gcs location and release history this study applied the lstm method a deep learning method to construct a surrogate model for the contaminant transport simulation model the accuracy of the lstm surrogate model was compared with that of surrogate models developed using the kriging rbf network and kelm methods that are often used for gcsi results showed that the accuracy of the lstm surrogate model was the best linking the lstm surrogate model to the optimization model can save about 99 of the computational load and time required to solve the optimization model the lstm method is thus an effective method to build surrogate models credit authorship contribution statement jiuhui li conceptualization methodology software writing original draft wenxi lu conceptualization writing review editing supervision project administration jiannan luo validation data curation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to acknowledge the support provided by the national natural science foundation of china 41972252 the jilin province science and technology development project grant no 20170101066jc and the graduate innovation fund of jilin university no 101832020cx246 special thanks is given to the journal editors for their efforts to evaluate the work and the valuable comments of the anonymous reviewers are also greatly appreciated 
4338,when simulation optimization s o is used for groundwater contamination source identification gcsi to reduce the calculation load and time generated by calling the simulation model a surrogate model is often used instead of the simulation model however when the conversion relationship between the simulation model inputs and outputs is complex and the model is highly nonlinear the commonly used approach of building a surrogate model may lose its advantage this study use and check a deep learning method with the long short term memory lstm network which has great potential for characterizing the input output conversion relationship of complex nonlinear numerical simulations to a surrogate model of the simulation model the accuracy of the surrogate model developed by the lstm method was compared with that of commonly used surrogate models developed by the kriging method the radial basis function network method and the kernel extreme learning machine method the surrogate model with the highest accuracy was linked to the optimization model and the optimization model was solved to identify the contamination source information results show that compared with the other three methods the surrogate model constructed by the lstm method had the best accuracy and generalization performance lstm is therefore an effective method of building a surrogate model linking the lstm surrogate model to an optimization model and then solving the optimization model can save approximately 99 of the computing load and time otherwise required keywords groundwater contamination lstm s o surrogate model 1 introduction it is essential to obtain and interpret information about groundwater contamination sources gcss when assessing the risk of groundwater contamination managing contamination risk and determining groundwater contamination liability xing et al 2019 however gcss are buried underground and it is challenging to obtain information about them including the number location initial activity time and release history of gcss etc hence gcsi is particularly important li et al 2019 gcsi was seriously analyzed in the 1980 s gorelick et al 1983 since then many methods have been applied in gcsi including direct approaches skaggs and kabala 1994 liu and ball 1999 probabilistic and geostatistical simulation approaches bagtzoglou et al 1992 neupauer and wilson 2001 xu and g√≥mez hern√°ndez 2018 optimization approaches gorelick et al 1983 mahar and datta 2001 analytical solutions and regression approaches sidauruk et al 2010 alapati and kabala 2000 and image identification method mo et al 2019 among these the s o method has been widely used in gcsi mirghani et al 2009 ayvaz 2010 datta et al 2011 when the s o method is applied to gcsi the contaminant transport simulation model is often linked to the optimization model as an equality constraint condition the contaminant transport simulation model could ensure that contaminant transport meets the requirements of groundwater transport laws hou et al 2017 xing et al 2019 however solving the optimization model often requires performing hundreds or thousands of iterative calculations each of these calculations must call the simulation model which generates a heavy calculation load and consumes considerable time hou et al 2016 previous studies have shown that constructing a surrogate model for the simulation model and then linking the surrogate model to the optimization model for iterative calculation can effectively avoid these disadvantages zhao et al 2020 hou and lu 2018 therefore the application of surrogate models in gcsi has developed rapidly the current methods for building surrogate models are divided into three categories including data driven methods projection based methods and multifidelity methods asher et al 2015 among them the data driven methods include support vector machine methods zhang et al 2009 ouyang et al 2017a kernel limit learning machine methods jiang et al 2015 hou et al 2019 kriging methods zhao et al 2016 radial basis function neural networks bagtzoglou and hossain 2009 miao et al 2019 hou et al 2017 and artificial neural networks ann khu and werner 2003 etc the projection based methods include proper orthogonal decomposition mcphee and yeh 2008 siade et al 2012 dynamic mode decomposition ghommem et al 2013 and fourier mode reduction willcox and megretski 2005 etc the multifidelity methods include heterogeneous multiscale method weinan and engquist 2012 multigrid method ashby and falgout 1996 and residual free bubbles sangalli 2003 etc more recently it was found that the surrogate models created by various methods have their own limitations and therefore an ensemble surrogate model based on multiple single surrogate models came into being viana et al 2009 acar 2010 ouyang et al 2017b the surrogate models developed by the above methods have high accuracy and achieve good identification results when applied to gcsi jiang et al 2015 zhao et al 2016 hou et al 2019 however if the nonlinearity of the contaminant transport numerical simulation model increases for example if the dimensions or types of variables including the contamination source location release intensity and simulation model parameters etc to be identified increase the conversion relationship between the numerical simulation model inputs and outputs will become complex and the advantage of the methods described above for building a surrogate model is likely to be lost the generalization ability and accuracy of the surrogate models built by these methods will be reduced which can pose challenges to acquiring gcsi an ensemble surrogate model may relieve some difficulties but if the accuracy of a single surrogate model is poor the generalization ability and accuracy of the ensemble surrogate model will be obviously affected when constructing a surrogate model therefore it is particularly important to find methods that are robust and have excellent generalization capabilities as a deep learning method long short term memory lstm has a great advantage in imitating the input output conversion process of a complex nonlinear numerical simulation model the lstm is a special type of recurrent neural network rnn designed to solve the long term dependence of the general rnn compared with the general rnn there is a gating mechanism between the single recurrent structures of lstm which is used to control the forgetting or continuing transmission of input information because of the superior performance of the lstm method it has been applied in many fields such as machine translation ren et al 2020 part of speech tagging popov 2016 speech recognition ying et al 2020 in recent years lstm has also been applied to the study of groundwater system zhang et al 2018 used lstm to establish a machine learning model to predict the water table depth an et al 2020 combined time frequency analysis methods and lstm to simulate karst spring water discharge due to the superior performance of the lstm this study applies it to gcsi to solve the gcsi problem this study proposes the use of the lstm method in building a surrogate model for the contaminant transport simulation model the accuracy of the surrogate model built by the lstm method is compared with the accuracy of the surrogate models developed by the kriging method the radial basis function rbf method and the kernel extreme learning machine kelm method which have been commonly used in the past won and ray 2004 razavi et al 2012 bhosekar and ierapetritou 2018 the surrogate model with the highest accuracy was selected from the four surrogate models then the best surrogate model was linked to the gcsi optimization model and a genetic algorithm was used to solve the optimization model to identify the gcss information 2 methodology 2 1 long short term memory lstm network an lstm network is a kind of deep learning network based on a rnn hochreiter and schmidhuber 1997 graves and schmidhuber 2005 therefore to explain the principle of lstm the rnn should be introduced first fig 1 shows the network structure of the rnn through the loop connection on the hidden layer the network status at the previous moment can be transferred to the current moment and the status at the current moment can also be transferred to the next moment graves et al 2013 the unrolled recurrent neural network is shown in fig 1 at time t the hidden layer h receives data from two sources the value of the hidden layer h t 1 at the previous moment in the network and the current input data s t and calculates the output of the current moment using the value of the hidden layer the input s t 1 at time t 1 can later influence the output at time t through the loop structure the calculation of h t requires h t 1 the calculation of h t 1 requires h t 2 and so on hence the state at any given moment in the rnn depends on all the states in the past the forward propagation calculation of rnn is expanded according to a time series and then the network parameters are updated using backpropagation through time sun et al 2017 however the gradient may disappear as the depth of the network increases and the calculations may explode rapidly during training as such rnns are often difficult to train to overcome this disadvantage hochreiter and schmidhuber 1997 proposed the lstm method the repeating module in a standard rnn is shown in fig 1 and the repeating module in an lstm is shown in fig 2 b fig 2a and 2b clearly show that the difference between an lstm and an rnn is that the former adds a processor to the algorithm to determine whether the information is useful this processor structure is called a cell a cell contains three gate control mechanisms which are called the forget gate the input gate and the output gate an lstm stores and updates information through these gates the forget gate is a key component of the lstm it controls the retention of important information and forgets unimportant information it can also avoid the problem of gradient disappearance and explosion caused by gradient backpropagation the input gate is used to control how much of the current input data s t flows into the memory unit that is how much can be saved to c t the output gate controls the effect of the memory unit c t on the current value h t after the information enters the lstm network the gate mechanism judges whether the current information is useful according to determination rules useful information is left behind and useless information is forgotten through the forget gate the gate control mechanism of lstm is realized by a sigmoid function and a dot multiplication operation fig 2b shows the transfer mechanism of these three gates the outputs of the three gates can be calculated as follows forget gate 1 f t œÉ w f s t u f h t 1 b f input gate 2 i t œÉ w i s t u i h t 1 b i memory update 3 c t t a n h w c s t u c h t 1 b c 4 c t f t c t 1 i t c t output gate 5 o t œÉ w o s t u o h t 1 b o 6 h t o t t a n h c t 7 y t softmax w v h t b v where œÉ œÑ 1 1 e œÑ is called a sigmoid function which is a nonlinear activation function commonly used in machine learning it can map a real value to the interval 0 1 to describe how much information passes through when the output value of the gate is 0 no information is passed and when the value is 1 all information can be passed w f w i and w o are weight matrices that connect the input to the forget gate the input gate and the output gate respectively w v is weight matrice of the visual output layer u f u i and u o are weight matrices that connect the hidden layers to the forget gate the input gate and the output gate b f b i b o b v are the bias vectors of the input gate the forget gate the output gate and visual output layer respectively represents the multiplication of corresponding elements y t is the output of visual output layer 2 2 radial basis function rbf an rbf network is a kind of locally approximated feedforward neural network benghanem and mellit 2010 daliakopoulos et al 2005 yoon et al 2011 it consists of three layers of neurons the first being the input layer the second being the hidden layer also called the radial base layer referred to as the rbf layer and the third being the output layer fig 3 nonlinear mapping is performed from the input layer to the radial base layer and linear mapping is further performed from the radial base layer to the output layer compared with the error backpropagation bp neural network the rbf network converges faster and can find a global minimum daƒü and dereli 2008 kokshenev and padua braga 2010 the basic principle of the rbf network is as follows if there are n dimensional input vectors s and m dimensional output vectors œÜ the output of the i th neuron in the hidden layer of the rbf network is 8 œà i s exp s œâ i 2 q i 2 i 1 2 m where s is an n dimensional input vector œâ i is the center of the rbf of the i th hidden layer neuron i 1 2 m q i is the width of the corresponding neuron also known as the expansion constant m is the number of hidden layer radial base layer neurons the output of the k th neuron of the output layer is calculated using the following formula 9 œÜ k s w ik œà i s œÉ k k 1 2 m where œÜ k is the response value of the j th neuron of the output layer w ik is the weight of the connection of the i th hidden layer neuron to the k th output layer neuron œÉ k is the bias term of the k th output neuron giesl 2008 boyd 2010 and m is the total number of output data points the rbf neural network has a simple structure and a fast convergence speed and therefore has been widely used in many fields krzy≈ºak 2001 zhang 2007 karayiannis and yaohua xiong 2006 hu et al 2018 2 3 kriging the kriging method was proposed by the south african geologist krige and it is also known as the spatial local interpolation method matheron 1963 cressie 1990 kleijnen and van beers 2005 bargaoui and chebbi 2009 the kriging method is generally used in the field of geology dhar and datta 2010 applied the kriging method to optimize the design of the groundwater monitoring network nikroo et al 2010 interpolated the groundwater depth based on the kriging method in addition since 1989 the kriging method has been widely used to develop surrogate models of simulation models since 1989 sacks et al 1989 ryu et al 2002 hemker et al 2008 coetzee et al 2012 the basic equations of the kriging method for predicting the response relationship between inputs and outputs are as follows 10 œÜ s j 1 k f i s Œ≤ i Œæ s f s t Œ≤ Œæ s the basic equations of the kriging method can be divided into two parts with œÜ s being the output of the kriging model f s t Œ≤ is the deterministic regression function and Œæ s is the gaussian random function f s f 1 s f 2 s f k s is the basis function and Œ≤ Œ≤ 1 Œ≤ 2 Œ≤ k is the vector of basis function coefficients which can be obtained using the training data more details of the principles of the kriging method can be found in guo et al 2018 in this study when kriging method was used to build the surrogate model the basis function was second order polynomial regression function and the correlation function was gaussian correlation function 2 4 kernel extreme learning machines kelm an extreme learning machine elm randomly generates the connection weights between the input layer and the hidden layer as well as the hidden layer neuron threshold this can cause the output matrix to oscillate and reduce model stability shi et al 2014 wong et al 2015 to avoid this problem huang et al 2004 huang et al 2015 proposed the kernel extreme learning machine kelm the basic principle of kelm is as follows chen et al 2014 jiang et al 2015 for a given set m of training sample data sets s j Œª j when the hidden layer node of the elm is l and the activation function is g the elm output can be expressed as follows 11 œÜ j f l s j i 1 l Œ≤ i g œâ i s j b i j 1 2 n where g is the activation function b i is the bias of the i th hidden neuron œâ i is a weight vector that connects the input neuron and the i th hidden neuron Œ≤ i is a weight vector that connects the output neuron and the i th hidden neuron g œâ i s j b i is the output function of the hidden layer and œÜ j is the output calculated by the elm kelm is an improved method based on the elm and combined with kernel function more details of the principles of the kelm method can be found in huo et al 2018 and song et al 2018 3 case study 3 1 site overview based on examples of gcsi analysis ayvaz 2010 xing et al 2019 performed in previous studies a gcsi analysis was conducted in this study the case study area was a two dimensional inhomogeneous medium with irregular boundaries the groundwater flow was steady flow as shown in fig 4 the ab and cd boundaries of the area were the specified head boundaries the ac and bd boundaries of the area were no flow boundaries fig 4 the contaminant at the hypothetical sites was a conservative contaminant that would not undergo biological transformation or chemical changes the total simulated time of contamination transport was 10 years with a total of twenty simulation periods every six months was one simulation period based on 30 days in each month table 1 shows the basic parameters of the aquifer and the distribution of hydraulic conductivity of the aquifer is shown in fig 5 the vertical direction of the area received uniform recharge at a rate of 0 000864 m d s1 released contaminant to the aquifer from the second to the fifth simulation period and then stopped s2 released contaminant to the aquifer from the third to the fifth simulation period and then stopped the actual gcss information are given in table 2 there were six observation wells in the area the contaminant concentration of each observation well was observed from the 9th simulation period to the last simulation period fig 4 shows the locations of observation wells and the contamination source during gcsi analysis the location the initial release period the release intensity of the gcss during each release period and the periods during which contaminant stopped releasing were unknown the purpose of gcsi was to identify the unknown variables the possible location of the gcss were known as shown in fig 4 and contaminant was possibly released during the first six simulation periods 3 2 numerical simulation model according to the specific conditions of the study area we developed a numerical simulation model of groundwater flow and contaminant transport the governing equation of groundwater flow in the model can be expressed as pinder and bredehoeft 1968 singh and datta 2006 12 x i k ij h z h x j w q Œ¥ x x w Œ¥ y y w 0 i j x y where k ij is the hydraulic conductivity tensor in this study isotropic hydraulic conductivity is used as shown in fig 5 h is the hydraulic head z is the elevation of the aquifer bottom w is the vertical recharge rate and q is the pumping rate x w and y w are the abscissa and ordinate of the pumping well respectively the governing equation of groundwater contaminant transport is pinder and bredehoeft 1968 singh and datta 2006 13 Œ∏ c t x i Œ∏ d ij c x j x i Œ∏ c u i c s w i j x y t 0 where c is the dissolved concentration c s is the dissolved concentration in a source or sink flux Œ∏ is the effective porosity u x and u y are the components of pore water velocity in x and y directions respectively d ij is the hydrodynamic dispersion tensor u x u y and d ij are described as 14 u x k x Œ∏ h x x u y k y Œ∏ h x y 15 d xx Œ± l u x 2 Œ± t u y 2 u d yy Œ± l u y 2 Œ± t u x 2 u d xy d yx Œ± l Œ± t u x u y u where u is the magnitude of pore water velocity Œ± l and Œ± t denote longitudinal and transverse dispersivities respectively after the simulation model was constructed the modflow and mt3dms toolboxes in the gms software were used to perform numerical calculations langevin and guo 2010 morway et al 2013 ehtiat et al 2018 the real gcss information in the study area were input into the simulation model and the simulated contaminant concentrations were obtained fig 6 shows the distribution of contamination plumes in the study area and fig 7 shows the contaminant concentration in each observation well because of systematic and accidental error the contaminant concentrations will inevitably contain noise in an actual gcsi therefore the simulated concentrations shown in fig 7 were perturbed by a normally distributed error to simulate the case where the concentrations contain error the following formula was used to calculate the perturbed concentrations 16 c c Œµ Œ± c where c is the perturbed concentrations Œµ is normally distributed white noise with variance 0 1 and average 0 in the error matrix and Œ± is the noise intensity if Œ± 0 10 the noise level is low if 0 10 Œ± 0 15 the noise level is moderate if Œ± 0 15 the noise level is high the noise intensity in this study was equal to 0 05 after the observation data were obtained they were entered into gcsi processing 3 3 surrogate models when the optimization model is used to analyze gcsi the simulation model should be linked to the optimization model as an equality constraint when solving the optimization model hundreds of thousands of iterative calculations are required and the heavy calculation load and calculation time consumed by calling the simulation model are immeasurable developing a more accurate surrogate model for the simulation model can avoid this problem constructing a surrogate model for this study involved the following steps a apply the latin hypercube sampling method to sample the variables to be identified within their feasible regions the variables to be identified in this study were the two gcss location and the release intensity during the possible gcss release period there were four location variables x coordinates of s1 y coordinates of s1 x coordinates of s2 and y coordinates of s2 and the contaminant release intensity 12 variables of two gcss during six possible release periods to identify the approximate start and stop release time of contaminant released by gcss more possible release periods can be set five hundred groups of variables to be identified were produced and then input into the simulation model in sequence to be identified then the simulation model was run to obtain the concentrations at the observation wells corresponding to the 500 groups of inputs b according to the principle of the four methods lstm kriging rbf and kelm matlab was used to write training code 400 groups of input the variables to be identified and output the concentrations at the observation wells data were used to train the surrogate model through four methods c the remaining 100 groups of inputs and outputs were input into four surrogate models and 100 sets of output data for each of the four surrogate models were obtained the coefficient of determination r 2 the relative error re and the root mean square error rmse were used to test the accuracy of the four surrogate models the accuracy of the four surrogate models was compared and analyzed the surrogate model with the highest accuracy was selected and linked to the optimization model 3 4 nonlinear optimization model the optimization model is an important tool for gcsi this study used a nonlinear programming optimization model li et al 2019 the nonlinear optimization model includes three parts the objective function the decision variables and the constraints guo et al 2019 after the surrogate model of the simulation model was developed an optimization model was constructed to identify the accurate location and release history of groundwater contamination sources the optimization model was expressed as follows 17 min g x 1 y 1 x 2 y 2 q 1 m q 2 m t 1 12 k 1 6 c k t t c k t 0 2 500 x 1 900 1000 y 1 1380 350 x 2 750 700 y 2 950 0 q 1 m 100 m 1 2 6 0 q 2 m 100 m 1 2 6 c k t t f x 1 y 1 x 2 y 2 q 1 m q 2 m where min g x 1 y 1 x 2 y 2 q 1 m q 2 m represents the objective function q 1 m is the release intensity for s1 during each release period q 2 m is the release intensity for s2 during each release period q 2 m is the simulated concentration of the contaminant at the observation well and ct k 0 is the measured value of the contaminant concentration at the observation well c k t t f x 1 y 1 x 2 y 2 q 1 m q 2 m was the surrogate model with the highest accuracy 4 results and discussion 4 1 comparisons on the training cost and accuracy the training cost and accuracy of the four surrogate models were compared and analyzed the training cost includes two parts which are the time to obtain training and test samples and the time to train surrogate models the training cost was shown in table 3 it can be seen from table 3 that the application of the lstm method to build surrogate model took the longest time and the application of the kriging method to build surrogate model took the shortest time which took 43 8 h and 22 3 h respectively the training cost of lstm surrogate model was twice that of kriging surrogate model however the training cost of the surrogate models were negligible compared with the time saved in solving the optimization model therefore compared with the training cost the accuracy of the surrogate models were the most important combining table 3 with fig 8 revealed that the r 2 of the surrogate model constructed by the lstm method was between 0 9707 and 0 9864 the rmse was between 68 2401 and 98 1363 and the re was between 6 12 and 9 65 clearly the r 2 of observation wells 1 3 4 and 6 was in all cases greater than 0 98 and the re was not greater than 7 5 the accuracy of the surrogate model was very high the surrogate model of observation well 2 and 5 had slightly lower accuracy r 2 is 0 9798 and 0 9707 respectively and their re are both higher than 9 the r 2 of the surrogate model developed by the kriging method was between 0 8308 and 0 959 the rmse was between 108 9547 and 283 9715 and the re was between 19 50 and 31 10 the accuracy of the kriging surrogate model was uneven and compared to the surrogate model constructed using the lstm the kriging surrogate model was less accurate the r 2 of the surrogate model constructed using rbf was between 0 9161 and 0 982 the rmse was between 70 9619 and 199 9221 and the re was between 6 60 and 23 56 compared with the surrogate model constructed using lstm the surrogate model constructed using the rbf method showed worse generalization performance and the accuracy of the surrogate model was unstable the r 2 of the surrogate model of observation well 2 was only 0 9161 and the rmse and re were 199 9221 and 23 56 respectively the accuracy of this surrogate model was poor and its application to gcsi would be inappropriate the r 2 of the surrogate model constructed using kelm was between 0 8571 and 0 9639 the rmse was between 102 2076 and 260 9804 and the re was between 8 68 and 25 75 clearly compared with the surrogate model of each observation developed by the lstm method the surrogate model developed by the kelm method has no advantages from the perspective of the overall analysis the average r 2 of the surrogate model of the six observation wells constructed using the lstm was 0 9835 shown in table 3 which was the highest of the four surrogate models the average rmse and average re were the lowest among the surrogate models at 88 1485 and 7 63 respectively although the accuracy of the lstm surrogate model of observation well 2 and 5 were slightly lower than that of the other observation wells the accuracy of the lstm surrogate model of observation well 2 and 5 were much higher than that of the kriging kelm rbf surrogate models comparing the average level of the concentration fitting accuracy of all observation wells and the concentration fitting accuracy of a single observation well it was found that the accuracy of the surrogate model established by the lstm method was higher than that of the other three surrogate models the surrogate model constructed by the lstm method had relatively high accuracy and better generalization ability therefore compared with the other three surrogate models the lstm surrogate model for gcsi was the correct choice 4 2 identified contamination sources the lstm surrogate model was connected to the optimization model and a genetic algorithm ga was used to solve the optimization model guo et al 2019 xing et al 2019 the parameters values assigned for the ga were shown in table 4 when solving the optimization model the surrogate model was used instead of the simulation model for iterative calculations the location and initial release period of the gcss and the release intensity of the gcss at each release period were identified fig 9 is the convergence profile and the optimal individual value of the optimization model the numerical calculation used a pc equipped with an intel i5 2 8 ghz processor and 16 gb ram the time required to call the simulation model once when solving the optimization model was approximately 2 3 min but the time required to call the surrogate model was only 0 0465 s based on the parameters of ga in table 4 calling the simulation model for iterative calculations can spend 18 400 h calling the surrogate model for iterative calculations takes about 6 18 h plus the 43 2 h spent building the surrogate model for a total of 49 38 h the calculation load and time savings by calling the surrogate model were above 99 it should be noted here that when solving the optimization model the time it takes to call the four surrogate models is basically the same table 5 provides information on the two gcss the s1 released very few contaminant in the first simulation period which was negligible compared to the contaminant released in other periods the s1 with the contaminant being released at the start of the second simulation period and continuing until the fifth simulation period the contaminant released intensity of s2 during the first and second release periods are very small and the third period can be regarded as its initial release period the contaminant was continuously released for three periods fig 10 shows a comparison between the results of the gcsi and the real gcss values on the whole the location identification results of s1 and s2 were close to the real value and had high accuracy the identification results of the release intensity especially the release intensity of s1 during the sp1 and sp2 had a large deviation from the real value while the release intensity of s2 was closer to the real value and the identified accuracy was very high 4 3 discussion it can be clearly seen from the identification results that the identified source fluxes of s1 during the sp2 period were significantly underestimated than the real source flux while the identified source fluxes during the sp3 period were overestimated than the real source flux in contrast the identification results of s2 were obviously closer to the real information the possible reasons for this result include the following aspects 1 the source fluxes of s1 was much stronger than that of s2 so s1 had a greater contribution to the output concentration of simulation model which in turn might lead to the concentration having a greater impact on the identification results of s1 and a smaller impact on s2 2 the layout of observation wells were not reasonable the concentration obtained at the observation wells were more sensitive to s2 but less sensitive to s1 in addition too few observation wells too few observation data and insufficient constraints might also lead to the above results if increase the number of observation wells and the number of observation times it is likely to get better recognition results xing et al 2019 applied least squares support vector machines lssvm kriging and rbf methods to establish surrogate models for simulation model the results show that the accuracy of the three surrogate models had no obvious ranking of superiority and inferiority in the study of xing et al 2019 when there was only one type of input variable release intensity and the dimension of input variable was 8 the r2 of the surrogate model was only 0 9766 and the re was only 7 56 in the study of hou et al 2018 when the type of variable was 7 and the dimension of input variable was 8 source information and simulation model parameters the accuracy of the surrogate model based on kelm was higher than that of the surrogate model established by xing et al 2019 using lssvm method this was also proved in the study of li et al 2020 the variable type of input surrogate model was one and the input was 16 dimensions it can be seen that in general the accuracy of kelm surrogate model is higher than lssvm surrogate model therefore this study chose kelm method to compare the accuracy of the surrogate model which is more convincing the results showed that compared with kriging rbf and kelm surrogate models lstm surrogate model had higher accuracy lstm method can be used to establish ensemble surrogate model and also can provide more choices for other researchers however lstm has its own limitations compared with the other methods mentioned above its training speed is slower when the sequence length in this study it can be understood as the variable to be identified exceeds a certain limit related to the problem studied or the number of network layers increases to a certain number the gradient may disappear or explode during the training process which will lead to the decline of the accuracy of surrogate models this paper conducted a case study and the results show that lstm does have potential in establishing the surrogate model however this does not fully show that lstm has obvious advantages when applied to all case studies the commonly used methods to build surrogate models include the data driven methods projection based methods and multifidelity methods lstm method belongs to the data driven methods for decision making problems where very short runtimes are required data driven approaches are the obvious choice and are suitable for most researchers the multifidelity and projection based methods is more appropriate to developers of groundwater model codes rather than everyday users because they are intrusive asher et al 2015 5 conclusions the surrogate model is often linked to an optimization model instead of a contaminant transport simulation model when applying optimization methods to gcsi however when the conversion relationship between the inputs and outputs of the contaminant transport simulation model is complex and highly nonlinear better accuracy of the surrogate models is needed to effectively identify the gcs location and release history this study applied the lstm method a deep learning method to construct a surrogate model for the contaminant transport simulation model the accuracy of the lstm surrogate model was compared with that of surrogate models developed using the kriging rbf network and kelm methods that are often used for gcsi results showed that the accuracy of the lstm surrogate model was the best linking the lstm surrogate model to the optimization model can save about 99 of the computational load and time required to solve the optimization model the lstm method is thus an effective method to build surrogate models credit authorship contribution statement jiuhui li conceptualization methodology software writing original draft wenxi lu conceptualization writing review editing supervision project administration jiannan luo validation data curation declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors would like to acknowledge the support provided by the national natural science foundation of china 41972252 the jilin province science and technology development project grant no 20170101066jc and the graduate innovation fund of jilin university no 101832020cx246 special thanks is given to the journal editors for their efforts to evaluate the work and the valuable comments of the anonymous reviewers are also greatly appreciated 
4339,machine learning algorithms have shown promise for streamflow forecasts reservoir operations and scheduling but have exhibited lower accuracy in predicting extended time horizons for reservoir inflows newer deep learning algorithms exhibited improved inflow forecasting accuracy but existing research has been mostly limited to real time operation and short term planning we propose a new multi step forecasting approach to improve long term forecasting accuracy for both water supply and inflow volumes this approach uses historical snow water equivalent swe and reservoir inflow time series data to train an encoder decoder algorithm to predict the reservoir inflow of future time steps during the april july runoff period the optimal model and hyperparameters are selected through five fold time series cross validation for variations between long short term memory lstm and convolutional neural network cnn encoder decoder algorithms we evaluated each algorithm using 30 years of reservoir inflow and swe data at the upper stillwater reservoir located in utah the optimal model was an lstm encoder decoder algorithm with 16 nodes per layer using this algorithm we investigate the trade off between model complexity and accuracy for long term water supply relative to a process based ensemble streamflow prediction esp model as a baseline and simpler statistical methods traditionally used in forecasting sarima var tbats long term water supply forecasts of the optimal deep learning algorithm proved superior to statistical methods and rivaled those of the esp 50 exceedance probability forecast i e the most probable forecast evaluated over five consecutive hold out periods keywords reservoir forecasting deep learning snow inflow 1 introduction classical statistical models such as seasonal auto regressive integrated moving average sarima papamichail and georgiou 2001 vector auto regression var iddrisu et al 2016 and trigonometric box cox transformation with arma residuals trend and seasonal components tbats elizaga et al 2014 have long been employed for reservoir inflow and outflow prediction these models are well suited to short term forecasts but have limited capacity for long term forecasts due to the convergence of the auto regressive part of the model to the mean of the time series shumway and stoffer 2000 while long term forecasts pose greater challenges due to long term dependencies forecast skill is also confounded by hydro meteorological predictability in snow dominated catchments anghileri et al 2016 thus non linear patterns governed by weather often exacerbate statistical model applications where linear decomposition is assumed valid better suited to the task physical models simulate hydrologic forecasts in which probabilistic ensembles require atmospheric input for future scenarios while quantifying uncertainty through statistical post processing krzysztofowicz 1999 krzysztofowicz 2002 raftery et al 2005 gneiting et al 2007 bennett et al 2016 widely used throughout the hydrologic community as a benchmark physical model siddique and mejia 2017 lee et al 2017 harrigan et al 2018 ensemble streamflow prediction esp uses physically based conceptual hydrologic models with states set to current basin conditions to create a probabilistic outlook consisting of a distribution of possible future events franz et al 2003 as part of the national oceanic and atmospheric administration noaa weather service the colorado basin river forecasting center cbrfc utilizes a suite of such physical models that are made available to water managers for enhanced decision making the esp model is capable of projecting accurate long term water supply forecasts representing the total expected inflow within a basin during a four month runoff period direct inputs for this model consist of observed precipitation temperature freezing levels and streamflow however snow water equivalent swe is not included rather the snow model within each basin is first initialized with current conditions to then simulate the building and melting of snowpack based on climate forecasted precipitation and temperature time series inputs day 1985 bradley et al 2004 jeong and kim 2005 mcenery et al 2005 spatial variability of snow properties are mainly accounted for by the melting rather than the accumulation processes and are commonly described using a snow depletion curve shamir and georgakakos 2007 esp has proven to be an accurate tool for long term water supply forecasting but it is limited in mapping non linear relationships at both micro e g landscape runoff processes and macro levels e g anthropogenic climate change due to forecast system errors stemming from model calibration data and initial states franz et al 2003 the need for more complex models within hydrologic forecasting is two fold standard statistical methods are limited in capturing non linear patterns and physical process based methods often lack sufficient information to accurately characterize basins based on hydrologic parameters nearing et al 2020 while both statistical and physical methods rely on information from the past to make predictions of the future the underlying complexity of hydrologic relationships between the input and output requires a model sophisticated enough to recognize non linear patterns without compromise to accuracy machine learning algorithms allow us to capture complex non linear patterns from data that would otherwise require extrapolation thus providing an alternative approach to forecasting among a variety of related hydrologic fields rainfall runoff prediction for ungauged basins kratzert et al 2019a hydropower production forecasting stokelj et al 2002 spatial swe estimation for mountainous areas zheng et al 2018 and quantifying climate and catchment control on hydrological drought konapala and mishra 2020 machine learning has seen broad application to reservoir inflow forecasting in both direct and multi step scenarios and generally resulted in more reliable forecasts of inflow extremes coulibaly et al 2000 trained a feed forward neural network using an early stopping training approach for real time reservoir inflow forecasting with lead times of one to seven days an improvement to daily reservoir inflow forecasts was later made using a robust weighted average ensemble that takes advantage of three different models nearest neighbors a physical model and an artificial neural network coulibaly et al 2005 an additive ensemble for monthly reservoir inflow forecasting was developed by bai et al 2015 incorporating an auto regressive model least squares support vector machine and adaptive neuro fuzzy inference system to subforecast trend period and stochastic terms bourdin et al 2014 wang et al 2012 and ahmed et al 2015 all used ensemble learning methods coupled with meteorological predictions to forecast reservoir inflows with respective forecast horizons of three eight and fourteen days long range streamflow forecasts extending twelve months were assessed by bennett et al 2016 in which calibrated climate forecasts are combined with a physical runoff model and a three staged error model to simulate reservoir inflows similarly liu et al 2017 developed a long term streamflow forecasting scheme extending nine months utilizing random forest and support vector regression for precipitation post processing of numerical weather predictions to feed into a hydrological watershed model deep learning has improved reservoir inflow forecasting but has been mostly limited to real time operation and short term planning deep belief networks for multi scale feature learning bai et al 2016 improved on prior efforts bai et al 2015 in direct step forecasting using an additive ensemble approach budu 2014 and chiamsathit et al 2016 both applied multi layered perceptrons for direct step forecasting scenarios that achieved reasonable accuracy among daily and monthly time step respectively time lagged recurrent neural networks tlrns have been studied muluye and coulibaly 2007 kote and jothiprakash 2008 taghi sattari et al 2012 where a preceding record of reservoir inflow is used to investigate the performance of a back propagation through time bptt algorithm better prediction of inflow into a reservoir using tlrn was achieved by kote and jothiprakash 2008 by modifying the artificial neural network to include seasonal monsoon effects accurate mapping of high and low flows was achieved following a monthly time step more recently multi step flood forecast models chang and tsai 2016 zhou et al 2019 kao et al 2020 have been developed for predicting reservoir inflows using adaptive neuro fuzzy inference systems and long short term memory lstm based encoder decoder frameworks however the time step for these forecasts are hourly with forecast horizons only extending four six and eight hours while deep learning has benefited short term hydrologic forecasting attempts at long term forecasts for reservoir inflows have generally exhibited lower accuracy due to greater uncertainty and long term dependencies similar to bennett et al 2016 and liu et al 2017 earlier attempts at long term forecasting muluye and coulibaly 2007 kote and jothiprakash 2008 failed to accurately predict the peak inflow at extended time horizons we evaluate a new approach to long term forecasting of reservoir inflow based on swe data at a macroscale using deep learning techniques this approach aims to use historical swe and historical reservoir inflow time series data to train an encoder decoder algorithm to predict the reservoir inflow of future time steps for the proceeding four months compared to a multitude of benchmark physical models deep learning models have been shown to perform better when trained from a large sample of hydrologic variability between catchments kratzert et al 2019b thus the insight behind this approach is to train a deep learning model to capture non linear behavior characterized by inter annual swe variability at a macroscale non linear hydrologic variables such as swe increasingly compromise model reliability with expected increases in frequency for consecutive snow drought years and increases in variability for snowpack amount and timing marshall et al 2019 especially within snow dominated catchments the magnitude of inter annual swe variability affects reservoir operations by reducing the reliability of reservoir inflows rhoades et al 2018 and hydroelectric power generation fleming and weber 2012 in comparison to esp the proposed deep learning model s multi step forecast represents the expected hydrograph to enter the downstream reservoir based on initial swe conditions while the esp forecast represents a single value for the total inflow furthermore the sequential nature of the encoder decoder model is driven by a preceding record of inter annual reservoir inflow and swe variability whereas the esp model is first initialized with swe but then driven by anticipated rainfall and temperature fluctuations from regional climate models we evaluate four different variants of the encoder decoder algorithm regular lstm lstm residual lstm lstm regular cnn lstm and residual cnn lstm in contrast to recurrent neural networks rnn such as lstms convolutional neural networks cnn operate independently of previous time steps to capture fixed size contexts allowing for parallel computation within a given sequence the stacking of convolutional layers allows for precise control of the dependencies to be modeled by effectively increasing the context size gehring et al 2017 implementing residual connections in cnns has been shown to further improve model performance by increasing the depth of the model architecture he et al 2016 deep learning frameworks utilizing residual connections in cnns have seen extensive application in other fields liu and song 2018 ning et al 2019 cengi l and √ßinar 2018 but have not yet been evaluated in the context of reservoir inflow forecasting a key distinction between the proposed models in this paper and the others listed above is that deep residual lstms and cnns are used in an attempt to improve the learning process for long range forecast accuracy while at the expense of greater model complexity thus we investigate the trade off between model complexity and accuracy using a physical esp model as a baseline and simpler statistical methods traditionally used in forecasting sarima var tbats the specific research questions of this study are 1 what is a suitable trade off between model complexity and accuracy for long term water supply forecasting 2 what are the characteristics of the hold out years in which the selected deep learning model excels in accuracy 3 how does accuracy for long term water supply forecasting compare between a purely data driven model and a physical process based model we close this section with a description of the study area data model and methods in materials and methods next we outline three standard statistical forecasting models used for comparison along with a description for the esp model as a baseline the results and discussion sections are broken down into model selection model evaluation and model complexity versus accuracy based on the process of five fold cross validation lastly we provide a summary of conclusions and recommendations for future research 2 materials and methods 2 1 study region this study focuses on the upper stillwater reservoir located at the top of the central utah water conservancy district s cuwcd collection system in the uinta mountains upper stillwater is located within the duchesne basin positioned in northeastern utah the reservoir drainage area ending at the rock creek above south fork gaging station 09277800 is 99 square miles has a mean annual precipitation of 34 3 inches and a mean annual flow of 149 cubic feet per second the inflow to the reservoir is unregulated and driven by snowmelt during a four month runoff period between april and august upper stillwater was constructed between december 1983 and november 1987 is operated and maintained by the cuwcd has a capacity of 32 009 acre ft and a max surface area of 314 acres there is currently an available diversion capacity of 285 cubic feet per second cfs by way of the stillwater tunnel a feature of the strawberry aqueduct collection system in addition to a river outlet and spillway with a total capacity of 75 000 cfs releasing directly into rock creek cuwcd is one of utah s four large specialty water districts that provides potable and secondary water to various water associations conservancy districts irrigation companies and local residents the water district spans eight counties with over 3 5 billion in infrastructure there are currently eight lakes reservoirs maintained and operated by cuwcd that house non potable water in excess of 1 6 million acre ft the storage levels for these reservoirs act as a barometer for the state s water resources and provide insight for how to appropriately prepare for future water usage fig 1 shows upper stillwater located in the middle of the figure surrounded by a network of snow telemetry monitoring sites 2 2 data the data used for model training is taken from cuwcd operations reports and the national resource conservation service nrcs the daily inflow into the reservoir is computed from reservoir elevation change compared to losses e g seepage and evaporation and releases to rock creek to determine the acre foot change the two data types used for this study are reservoir inflow and swe the depth of water in that would result if the snowpack were melted specific to rock creek at upper stillwater reservoir the historical daily data span from january 1990 to the present with new values updated daily the time series for reservoir inflow fig 2 is quality controlled by cuwcd and continuous with no gaps requiring interpolation the reservoir inflow time series illustrates a seasonal runoff period fed by snowmelt that begins in april and ends in august a governing assumption of this study is that water managers only have until the end of march to make a final decision regarding the level of storage space to leave vacant in the reservoir for the runoff season therefore the critical period to forecast reservoir inflow is a four month window between the first of april and end of july such an extended forecast horizon requires a model capable of learning long term dependencies swe data were collected from the nrcs monitoring network of snow telemetry sites for the same period as upper stillwater s reservoir inflow time series table 1 for each of the available monitoring sites a maximum of three daily data points required interpolation over the entire period the dependence of reservoir inflow fig 2 on swe fig 3 is the primary relationship that the model will attempt to learn the daily data were then prepared for training the algorithm the data were first re sampled into weekly averages and scaled between 0 and 1 based on the chosen activation function for the model see section 2 3 a sliding window length of 20 weeks is used as input to predict the next 18 weeks i e april july the length of the window is selected based on the time series data for swe depending on the precipitation distribution during a given winter season the process of swe accumulation ranges between 15 to 25 weeks on average a 20 week input window length is assumed to be the most common year to year scenario for the model during training 2 3 deep learning model the objective of the model is to forecast multiple time steps forward based on multiple inputs from the past the inputs are the multiple time series of reservoir inflow and swe and the output is a future reservoir inflow sequence prediction starting at the final point in the input data therefore a multivariate sequence to sequence prediction model is required this type of model is broken down into two separate models one for reading the input sequence and encoding it into a fixed length vector encoder and a second model for decoding the fixed length vector and outputting the predicted sequence decoder sutskever et al 2014 following the decoder a time distributed fully connected layer is used as the final component to condense the output and yield a forecasted sequence of values fig 4 illustrates four variants for this algorithm all of which receive a sliding window input of multiple variables and transmits a sliding window output of a single target vector this architecture has proven to be effective for numerous sequence to sequence problems including multi step flood forecasting kao et al 2020 network traffic forecasting zhang and you 2020 weather forecasting yuan et al 2019 and predicting solar performance ratio yen et al 2019 each encoder decoder model is written in python using the keras deep learning library gulli and pal 2017 another important feature of the proposed model is how each node sequentially transmits information to the next within each layer along with how the weights within the network are optimized the exponential linear unit elu clevert et al 2016 and adam optimizer kingma and ba 2017 are selected for the activation function and optimizer respectively encoder decoder variants although rnns facilitate sequential learning they fail to connect information from the past input to the present output data when the gap between the two grows too large which creates the issue of long term dependencies this problem is addressed with the use of lstm networks hochreiter and schmidhuber 1997 a successor to the rnn an essential feature of the lstm cell is its state that runs directly through the network enabling addition or removal of information from the cell state via regulated gates lstm networks are thus an essential component used for constructing encoder decoder variants reliant on capturing long term dependencies as illustrated in fig 4 four variants of the encoder decoder algorithm are used with varying degrees of complexity in the first variant regular lstm lstm a simple lstm encoder is connected to an lstm decoder by a repeat vector which acts as an adapter to fit the encoder and decoder parts of the network together the extracted features from the encoder are fed into the decoder to yield a forecasted sequence of values a repeat vector is configured to repeat the fixed length vector one time for each time step in the output sequence a dense layer is used as the output for the network the same weights are used to output each time step in the output sequence by wrapping the dense layer in a time distributed wrapper this allows the same output layer to be reused for each element in the output sequence the second variant residual lstm lstm follows the exact same flow process but it instead consists of residual connections between layers as shortcuts residual connections are used to center layer gradients and propagated errors increasing the depth of the network while making it easier to optimize wang et al 2018 thus the model is optimized for a residual mapping of feature extraction encoder to be used as input to forecast a sequence of values decoder from the sliding window of time series input in contrast to the lstm encoder a regular and residual cnn is used instead for the third and fourth variant architectures a one dimensional cnn is a model with one or more hidden layers that operate over a 1d sequence e g sentence or time series through convolutions in a multi layer cnn the stacking of cnn layers creates a hierarchical structure that provides a shorter path to capture long term dependencies therefore the model creates hierarchical representations over the input sequence allowing nearby input elements to interact at lower layers while distant elements interact at higher layers gehring et al 2017 causal padding van den oord et al 2016 is used for each cnn layer to ensure the model does not violate the temporal order i e model does not have look ahead bias the cnn encoder is designed to gradually reduce the dimensionality of the input feature matrix while increasing the number of feature abstractions this is done using filters see section 2 3 and pooling layers whose purpose is to condense a cnn layer s output to the most prominent elements max pooling is used at the end of each residual section fig 4 in which one residual block is connected in sequence for a given number of filters and kernel size max pooling and flatten are operations used at the end of the encoder to downsize the extracted features into a fixed length vector proportional to the number of nodes in the lstm decoder the final output represents the extracted elements as features from the input sequences that will be fed as a flattened sequence for the decoder model training and selection while the historical data record spans 30 years for both swe and reservoir inflow there is still a need to increase the number of weekly averaged samples to improve model performance with unseen data i e generalization the index sequential method ism is a popular pre processing method used within the colorado river basin when planning for the future by generating synthetic sequences from past hydrology kendall and dracup 1991 lukas and payton 2020 ouarda et al 1997 uncertainty of future hydrology stems from inherent randomness now coupled with anthropogenic climate change thus requiring predictive models to become more resilient towards changing hydrology one approach to this challenge is to use an ism where multiple sequences of past data allow a model to consider uncertainties of future hydrology against a broader range of possible sequences in the future salehabadi et al 2020 the ism generates n synthetic sequences by shifting over each water year by one index and repeating n times for a historical record of length n this method has been widely used with physical models such as the colorado river simulation system crss for predictive management of potential future hydrology usbr 2012 lukas and payton 2020 but it is limited in providing sufficient variety of statistically plausible sequences that capture hydrologic extremes i e longer or more intense droughts salehabadi et al 2020 a modified version of the ism is applied here for model training where synthetic sequences of the historical record are generated by randomly indexing each water year as a block fig 5 to effectively shuffle the original 30 year sequence table 2 salehabadi et al 2020 refer to this bootstrap method as water year block disaggregation in which randomized samples of each water year are generated in collection with multiple sites rather than repeatedly shifting each water year over by one index this non parametric bootstrap method allows for greater variability to be created within each sequence to effectively improve model performance vogel and shallcross 1996 srinivas and srinivasan 2005 srinivas and srinivasan 2006 the process of randomized sampling is assumed valid specifically for this study due to the seasonal characteristics of both reservoir inflow and swe within a given water year period thus allowing for seamless indexing this method is then taken a step further by also scaling each sampled water year at random between 0 5 and 1 5 for varying degrees of magnitude centered around the original the key benefit to this additional modification is expanding the model s exposure to hydrologic extremes for a greater capacity to generalize overall the described process of generating synthetic sequences is used to increase the size of the training dataset five fold prior to testing each hold out year through cross validation the optimal hyperparameters for each model architecture are determined through five fold time series cross validation five consecutive hold out periods are used between 2011 and 2015 to first determine the optimal hyperparameters while the next five year period i e 2016 2020 is used to determine the optimal model architecture based on the selected hyperparameters for each of the four encoder decoder variants configurations of lstm nodes and cnn kernels are alternated three separate times this method results in 12 separate models to train and compare against during cross validation based on varying model complexity defined as trainable parameters the number of available lstm nodes for the regular and residual lstm encoder decoders are 8 16 and 32 while the available sizes for the cnn kernels within the regular and residual cnn layers are 2 4 and 8 in contrast to the kernels the number of filters in each cnn layer are kept constant during cross validation in order to simplify the process by reducing the number of parameter configurations to search over likewise the number of nodes within each residual model s lstm decoder are kept constant at 16 nodes per layer a time distributed fully connected layer is placed at the end of each model to condense the output from the decoder by setting the number of nodes to half of the lstm decoder layers are defined as arrays of nodes that sequentially transmit information from one to the next within each layer nodes are connected by multiple weights for a given number of inputs and outputs a single node receives input data processes the input as a weighted sum and then propagates new information to its successor based on a given activation function the relative density for a given model is defined as the average number of trainable parameters per layer filters and kernels are interrelated hyperparameters specific to cnns a kernel represents a matrix of weights that slide over the input sequence calculating the dot product between the sequence values and matrix weights therefore the size of the kernel represents the length of the window it spans for deep feature extraction a complete tour of a kernel over the input sequence represents a filter thus kernel s operating over multiple channels of input establish a filter feature map the input window represents the multivariate time series of reservoir inflow and swe accumulation during the winter season until the first of april epochs represent the number of full passes of the dataset that the model uses during training the batch size is the fraction of data that the model is exposed to during each epoch a maximum of 50 epochs are to be used for each model during training an early stopping algorithm is also used during training to prevent over fitting with excessive epochs a training session will terminate early if there are 10 consecutive epochs with no improvement in minimizing the mean squared error across the first five test years i e 2011 2015 the model is trained one configuration at a time and then tested against a hold out set to develop the performance metrics normalized mean absolute error nmae normalized root mean squared error nrmse normalized median absolute error nmedae nash sutcliffe model efficiency coefficient nse and explained variance expvar 1 nmae i 1 n q obs i q pred i i 1 n q obs i 2 nrmse 1 n i 1 n q obs i q pred i 2 1 n i 1 n q obs i 3 nmedae med q obs q pred 1 n i 1 n q obs i 4 nse 1 i 1 n q obs i q pred i 2 i 1 n q obs i 1 n i 1 n q obs i 2 5 expvar 1 i 1 n q obs i q pred i 1 n i 1 n q obs i 2 i 1 n q obs i 1 n i 1 n q obs i 2 the performance metrics are calculated from a hold out dataset that is not used during model training the hold out set spans 38 weeks and consists of two consecutive parts input 20 weeks and output 18 weeks the input spans from november through march the output from april through july for example the hold out dataset used to forecast the 2015 runoff period begins in november 2014 and ends at the start of april 2015 table 3 a rank is then assigned to each model based on an equally weighted sum for each performance metric averaged over the five year test period ranging from 1 best to 3 worst a ranking system is displayed rather than the calculated performance metrics in order to avoid confusion for actual model performance since the 2011 2015 hold out data is used for hyperparameter tuning not model evaluation uncertainty uncertainty is quantified within two different areas for this study the model s underlying uncertainty towards producing forecasts and the intrinsic uncertainty of reservoir inflow at both weekly average and april july inflow periods the model s uncertainty is quantified using a 95 confidence interval and is calculated from an ensemble of model runs due to the stochastic nature of the deep learning model a slightly different forecast will be returned each time the model is trained therefore each model is trained multiple times to establish a normal distribution of model predictions for each time step in the forecast a normal distribution is assumed valid at each time step as the number of ensemble members is increased by application of the central limit theorem by design forecast points are labeled as outliers if they lie beyond the whiskers of their respective boxplot following the tukey method tukey 1970 the other area of uncertainty within this study is towards the variability of reservoir inflow as the main target for prediction a shaded exceedance probability plot is used to visualize the realm of likelihood for each time step within the multi step forecast fig 6 the shaded plot is created by calculating the daily exceedance probability across the historical 30 years of data and then resampling the values into weekly averages within the april july runoff period therefore the shaded plot is composed of 30 individual traces for weekly average reservoir inflow ordered from greatest exceedance probability lowest inflow to lowest exceedance probability highest inflow lastly the exceedance probability for the total april july inflow volume is also determined for visual comparison the 2016 2020 hold out years are annotated within the subplot fig 6 to provide context for each year s relative uniqueness among the entire historical record total inflow the total inflow volume is determined as the area under the curve for the forecasted hydrograph however each time step within the four month forecast represents a weekly average of reservoir inflow therefore each hydrograph is first re sampled backwards to a daily time step for greater granularity before calculating the total inflow volume for simplicity an assumption is made that weekly averaged inflow is representative as a daily constant during each week thus when re sampling to a daily time step the weekly averaged inflow is scaled to represent each day in a given week the sum of all daily inflows for the re sampled hydrograph are then converted from cubic feet per second to acre feet per day to yield the total inflow volume over the four month period the relative error for total inflow volume is calculated between a given model and the benchmark esp model across two different performance metrics mean absolute relative error mare and root mean squared relative error rmsre 6 mare 1 n i 1 n v obs i v pred i v obs i v benchmark i 7 rmsre 1 n i 1 n v obs i v pred i v obs i v benchmark i 2 by design the relative error within each metric provides a baseline representative of the esp model s performance the values of each calculated metric will either reside above or below a value of 1 0 to indicate worse or improved model performance relative to the esp model therefore the comparison of relative error between the selected deep learning model and various statistical models serves as the main tool for assessing the trade off in accuracy for model complexity 2 4 statistical methods for comparison the deep learning forecasts are compared against three widely used statistical models seasonal auto regressive integrated moving average sarima vector auto regression var and trigonometric seasonal box cox transformation with arma residuals trend and seasonal components tbats instead of a weekly average time step frequency each model is trained on monthly averaged data due to their limitations to forecast into such extended horizons the forecasts for the total expected inflow volume are also re sampled backwards to a daily time step following the same general assumption mentioned in section 2 3 2 4 1 sarima model a discrete time series z 1 z 2 z 3 z n 1 z n of measurements at equal time intervals is simulated by a stochastic sarima model box et al 2015 given by 8 œÜ b œÜ b s 1 b d 1 b s dz t Œ∏ b Œ∏ b s e t here t represents the discrete time and s denotes the length of each season the b term corresponds to the backward shift operator which is defined by bz t z t 1 and b s z t z t s the independently and normally distributed white noise residual is represented by e t nid 0 œÉ e 2 which has a zero mean and variance defined by œÉ e 2 from the left hand side of eq 8 the first two terms œÜ and œÜ represent series expansions given by 9 œÜ b 1 œÜ 1 b œÜ 2 b 2 œÜ p b p 10 œÜ b s 1 œÜ 1 b s œÜ 2 b 2 s œÜ p b ps eq 9 represents the nonseasonal auto regressive operator of order p and œÜ i i 1 2 p depicts the nonseasonal auto regressive parameters 1 b d is the nonseasonal difference operator of order d which produces nonseasonal stationarity of the dth differenced data usually d 0 1 or 2 whereas eq 10 depicts the seasonal auto regressive operator of order p and œÜ i herein i 1 2 p are the seasonal auto regressive parameters 1 b s d is the seasonal differencing operator of the order d to produce seasonal stationarity of the dth differenced data usually in the order of d 0 1 or 2 from the right hand side of eq 8 the first two terms Œ∏ and Œ∏ represent series expansions given by 11 Œ∏ b 1 Œ∏ 1 b Œ∏ 2 b 2 Œ∏ q b q 12 Œ∏ b s 1 Œ∏ 1 b s Œ∏ 2 b 2 s Œ∏ q b qs eq 11 is the nonseasonal moving average operator of the order q eq 12 is the seasonal moving average operator of order q and Œ∏ i i 1 2 q are the seasonal moving average parameters the natural log of the reservoir inflow time series is taken to stabilize the variance of the time series and to transform any skew in the distribution into a normal distribution papamichail and georgiou 2001 using an annual seasonal term s of 12 months the sarima model parameters p d q p d q are configured using the autoarima function provided in the pmdarima statistical library in python 3 0 2 4 2 var model var is another frequently used model for multivariate time series the basic var model of order p as suggested by sims 1980 is given by 13 y t a 1 y t 1 a 2 y t 2 a p y t p cd t u t where y t y 1 t y 2 t y kt represents a vector of k observable endogenous variables and d t consists of all deterministic variables which carry a constant a linear trend seasonal dummy variables and user specified variables u t is a k dimensional unobservable zero mean white noise process which has a positive definite co variance matrix e u t u t u a i and c are parameter matrices of suitable dimension upon which various restrictions can be imposed for a k dimensional auto regression with an effective sample size n the optimal lag order p is selected that minimizes the akaike information criteria aic given by the following equation 14 aic p ln p 2 n k 2 p p is the quasi maximum likelihood estimate of the innovation covariance matrix p ventzislav and lutz 2005 sin and white 1996 the parameters in eq 13 are estimated by the method of generalized least squares this is done by first estimating the individual equations of the system by ordinary least squares the residuals can then be utilized to estimate the white noise co variance matrix u as u t 1 t 1 t u t u t which is used to compute the generalized least square estimator iddrisu et al 2016 the var model is developed through the statsmodels python module and utilizes the same swe data as the deep learning model 2 4 3 tbats model tbats is a combination of three methodologies i exponential smoothing method ii box cox transformation and iii arma model for residuals the box cox transformation helps to deal with non linear data and arma model for residuals can de correlate the time series data the trigonometric expression of seasonality terms serves to reduce the parameters of model when seasonal frequencies e g annual streamflow or annual peak flow are high and improves the model flexibility i e lower bias with higher variance enabling it to handle complex seasonality the tbats model is comprised of the following terms y t Œª l t 1 œï b t 1 i 1 t s t m i i d t l t l t 1 œï b t 1 Œ± d t b t œï b t 1 Œ≤ d t d t i 1 p œÜ i d t i i 1 q Œ∏ i e t i e t where y t Œª time series at moment t box cox transformed s t i ith seasonal component l t i local level b t trend with damping d t arma p q process for residuals e t gaussian white noise seasonal part s t i j 1 k i s j t i s j t i s j t 1 i cos œâ i s j t 1 i sin œâ i Œ≥ 1 i d t s j t i s j t 1 i sin œâ i s j t 1 i cos œâ i Œ≥ 2 i d t œâ i 2 œÄ j m i model parameters t amount of seasonalities m i length of ith seasonal period k i amount of harmonics for ith seasonal period Œª box cox transformation Œ± Œ≤ smoothing œï trend damping œÜ i Œ∏ i arma p q coefficients Œ≥ 1 i Œ≥ 2 i seasonal smoothing two for each period based on a fourier series each seasonality is modeled by a trigonometric representation as an innovations state space model tbats admits a larger parameter space with the possibility of better forecasts hyndman 2008 additionally the model handles non linear features typically seen in time series while taking into account any auto correlation within the residuals de livera et al 2011 the tbats model is implemented using the tbats package in python 3 0 and incorporates quarterly bi annual and annual seasonal periods 3 ensemble streamflow prediction ensemble streamflow prediction esp initially introduced by the national weather service nws day 1985 is a conditional monte carlo simulation approach commonly used for statistical post processing of forecasts and estimating the inherent uncertainties najafi et al 2012 esp combines physical modeling of the river basin with a probabilistic representation of the future conditions using historical weather data the esp technique originally assumed that if historical weather patterns are taken together they represent possible future conditions day 1985 each of the historical years of weather starting at the forecast date is assigned a probability 1 m of re occurring where m is the total number of weather patterns considered reflecting the traditional weights employed to represent such an empirical distribution faber and stedinger 2001 snowmelt and runoff consequent to the current weather conditions of temperature and precipitation are calculated with a physical process based model a hydrograph of the resultant streamflows is produced for a given weather pattern this process is repeated for the weather patterns of each historical year producing traces of streamflow together these traces provide the probabilistic structure to forecast future streamflows a suitable probability density function esp p d f is fitted to the generated streamflow ensemble describing the likelihood of an event occurring during a certain time period t that is forecasted in this study the most probable streamflow q t with 50 exceedance probability is used for comparison the 50 exceedance probability is representative for medium flows pr m and is defined by pr m t pr q t q l t pr q t q u t where q l and q u are respectively the lower and upper limits of the medium flow category jeong and kim 2005 4 results 4 1 model selection results of the five fold cross validation are displayed below in which optimal hyperparameters are first selected followed by individual model selection table 4 the optimal number of nodes per layer for the regular lstm lstm was centered at 16 while both options for increasing and decreasing this number resulted in poorer performance in comparison to the residual lstm lstm there is an observed trend of increasing model performance when increasing the number of nodes from 8 16 and 32 conversely this opposite trend is observed for the residual cnn lstm variant except now the number of lstm nodes is fixed at 16 and the kernel length is increased from 2 4 and 8 the cnn lstm follows in the same trend of its residual counterpart where model performance appears to decrease as the size of the kernel is increased in summary the four optimal model configurations are 1 regular lstm lstm with 16 nodes per layer 2 residual lstm lstm with 32 nodes per layer 3 regular cnn lstm with a kernel size of 2 and 16 nodes per lstm layer and 4 residual cnn lstm with a kernel size of 2 and 16 nodes per lstm layer while optimal model selection is also based on averaged performance metrics the hold out data is now between 2016 and 2020 table 5 such that the final model selection is unbiased to the hyperparameter tuning the best performing model is the regular lstm lstm with the lowest reported errors nmae nrmse and nmedae and highest values for nse and expvar the regular lstm lstm is the second most complex model among the four with a parameter density of 1 204 trainable parameters per layer based on a total of 4 817 parameters and four layers fig 4 the second best performing model is the residual lstm lstm which is the most complex model within the study requiring 40 993 trainable parameters the two worst performing models are the regular and residual cnn lstms with model complexities defined by 3 813 and 6 625 trainable parameters respectively overall there is a general trend of increasing model accuracy with increasing network density 4 2 model evaluation this section is organized by each hold out test year from 2016 through 2020 fig 7 illustrates the multi step forecasts from the selected lstm lstm model for the hold out inflow periods which are composed with the actual values solid black line forecasted values dashed black line boxplots and shaded exceedance probabilities for reservoir inflow the box plots are based on a sample population of 30 individual model trainings per test year table 6 summarizes the performance metrics for each hold out year fig 8 compares the total forecasted inflow volume between the baseline esp model at the edges and center of the model s 95 confidence interval the forecasted hydrograph for 2016 accurately predicts both the rising and falling limbs but fails to identify the peak inflow even when using the upper bound of the boxplot whiskers the greatest errors are associated with the peak inflow that occurs during june and extends into the 10 20 exceedance probability range whereas both of the observed rising and falling limbs are centered within a 50 exceedance probability the forecasted total inflow volume is under predicted due to the under predicted peak inflow the 2017 forecast is less accurate in determining the rising and falling limbs for the inflow hydrograph however the accuracy for the peak inflow is improved even during an exceptional water year for swe accumulation and reservoir inflow the observed exceedance probabilities of both rising and falling limbs range between 10 30 while the peak inflow again extends into the range of 10 20 the center of the model s confidence interval very accurately predicts the total expected inflow volume for an associated exceedance probability of about 18 fig 6 while both 2016 and 2017 hold out periods exhibit very different inflows the performance metrics are shown to be relatively similar with values for nse and expvar between 0 75 and 0 77 respectively table 6 the forecasted hydrograph for 2018 is the smallest among the five hold out periods with a total inflow exceedance probability of nearly 80 fig 6 the rising limb appears to be under predicted while the falling limb is overestimated along with the peak inflow being inaccurately predicted for both magnitude and timing however the difference between the observed and the forecasted values is relatively small compared to larger years i e 2017 and 2019 as the performance metrics for 2018 report both nse and expvar values of about 0 83 table 6 in comparison to a period of large inflow the forecast for 2019 also demonstrates skill in predicting the rising limb but then greatly under predicts both the peak inflow and the falling limb the 2019 inflow period in particular was exceptional with both the peak inflow and total inflow exceedance probabilities being less than 10 fig 6 7 thus the performance metrics are reported the worst for this year among all of the hold out years likewise the predicted total inflow volume for this period was the least accurate but was still improved when using the upper bound of the model s confidence interval fig 8 lastly the 2020 forecasted hydrograph is the most accurate among the five year test period both the rising and falling limbs are accurately predicted with the exception of a slight under prediction for the peak inflow the performance metrics for the 2020 runoff period were exceptional with the highest values for nse and expvar 0 975 and 0 976 respectively along with the lowest reported errors for nmae 0 109 nrmse 0 153 and nmedae 0 064 table 6 overall the five year average of performance metrics demonstrate a high performance for the selected lstm lstm model however there are consecutive reoccurring periods where the model exhibited significant error and uncertainty the greatest errors stem from under predicted peak inflows which coincide with greater model uncertainty i e wider distributions illustrated from the respective boxplots likewise the widest time step distributions among each forecast are located at the descending inflection point fig 7 4 3 complexity vs accuracy the trade off between model complexity and accuracy is plotted by comparing the relative error for the total inflow volume as a function of model complexity defined by trainable parameters fig 9 each model s error is relative to the benchmark esp 50 exceedance probability i e the most probable forecast for the 2016 2020 hold out periods all three statistical models are shown to under perform at this forecasting task with relative errors residing above the benchmark line sarima is the least complicated model used within this study and in turn is reported to have the worst performance tbats is slightly more sophisticated than sarima and is shown to have greater accuracy for the total inflow volume var is the most successful among the three statistical models with improved accuracy but at a cost for greater complexity however the model s accuracy is still outmatched by the benchmark esp the selected lstm lstm is the most complex among the statistical models and in turn there is a significant improvement in accuracy both the mare and rmsre indicate that the proposed deep learning approach is roughly 50 more accurate than the esp model 5 discussion previous studies including direct step coulibaly et al 2005 taghi sattari et al 2012 bai et al 2016 and multi step coulibaly et al 2000 muluye and coulibaly 2007 kao et al 2020 deep learning algorithms have improved forecasting accuracy but have yet to accurately forecast reservoir inflows at extended long term horizons extended four month forecasts by muluye and coulibaly 2007 demonstrated reasonable predictions of low and medium reservoir inflows but then either under or over predicted the peaks the optimal encoder decoder algorithm is proven superior to statistical methods and competes with the baseline esp model for long term water supply forecasting fig 9 however periods where the model exhibits the lowest accuracy were due to under predicted peak inflows fig 7 the selected model s best performance occurs during the most probable inflows at both weekly average and april july inflow periods the 2020 forecast yielded the greatest performance metrics while the observed inflow has an associated exceedance probability of nearly 50 along both timescales fig 6 7 likewise the 2018 test period is the second most accurate table 6 with the greatest observed exceedance probabilities due to exceptionally low flows conversely the model s worst performance occurs during the 2019 forecast in which the observed total and peak inflow exceedance probabilities are reported to be less than 10 the 2016 test period also under predicts the observed peak inflow resulting in the second worst forecast for total inflow fig 8 similar to muluye and coulibaly 2007 the selected lstm lstm model excels in accuracy during periods with high probability for total inflow i e medium to low flows but then under predicts the peaks as the total inflow exceedance probability decreases compared to existing statistical models the proposed deep learning approach effectively predicts the ensuing reservoir inflow as a four month hydrograph while the deep learning approach is shown to be more accurate on average over the five hold out periods there is a significant increase in model complexity application of this model is validated by the drop in relative error below the benchmark esp line fig 9 indicating roughly a 50 improvement in accuracy this improvement is indicative of the significant influence that inter annual swe variability has towards forecasting reservoir inflow climate driven forecasts made by the esp model are significantly more accurate than the statistical methods but are still outmatched by the deep learning approach among each of the five hold out years the selected model is shown to yield a more accurate forecast for total inflow fig 8 similar to kratzert et al 2019b accuracy for long term water supply forecasting is improved when using a purely data driven deep learning model trained from a large sample of hydrologic variability between catchments the trade off in complexity for accuracy is demonstrated to effectively predict long term water supply by observing inter annual swe variability at a macroscale furthermore the observed inter annual variability exhibited between the 2016 2020 hold out years fig 6 indicate a wide spectrum of hydrologic inflows thus further validating the metrics reported for the accuracy complexity trade off as being robust in contrast to the statistical comparison for an accuracy complexity trade off the results from the cross validation study show that model accuracy improves with increasing model density table 5 model complexity for the deep learning methods are a function of trainable parameters which are dependent on both the quantity and type of layers therefore a model s density provides the most useful comparison for inspecting an accuracy complexity trade off while an lstm layer has significantly more trainable parameters the depth of the cnn lstm networks are much greater fig 4 thus increasing the risk for vanishing gradients as errors are back propagated during model training the concept of residual connections is applied to address this issue by centering layer gradients and propagated errors but at a cost of greater model complexity through added layers by coincidence both regular and residual cnn lstm models arrived at the same hyperparameter value for kernel size table 4 while the number of lstm nodes and cnn filters per layer remained fixed therefore the poor results from the cnn lstms are likely due to poor architecture design and would likely be improved with a widened parameter search during cross validation overall the simple lstm lstm architecture is proven to be the most accurate but the model performance could potentially be improved further by investigating increased architectural depth with stacked layers 6 conclusion this research improves on prior multi step forecasting efforts to strengthen water manager abilities for long term planning of reservoir operations involving water supply and inflow volume given the comparatively high performance of the proposed approach in the study region the lstm encoder decoder architecture warrants further study for long term multi step reservoir inflow forecasting considerations for future research include i further experiments with model architecture and hyperparameters ii investigating additional independent variables and iii modeling additional reservoirs influenced by snowmelt runoff potential experiments with model architecture may include utilizing an attention mechanism to observe the intermediate states of the encoder rather than only the final states the inclusion of other independent climate variables such as atmospheric temperature and solar radiation may further improve accuracy finally modeling additional reservoirs will provide valuable insight into potential model transferability credit authorship contribution statement zachary c herbert conceptualization methodology software validation formal analysis investigation resources data curation writing original draft writing review editing visualization project administration zeeshan asghar writing review editing carlos a oroza supervision writing review editing project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by the central utah water conservancy district and university of utah a special thanks to blake buehler heath clark jared hansen troy ovard the anonymous reviewers and the associate editor for their instrumental comments and critique geographic information system gis mapping was provided by lindsy bentley all reservoir inflow data is obtained from cuwcd operations reports and all snow water equivalent swe data is made publicly available by the nrcs online repository at https www nrcs usda gov the associated data with this paper can be accessed through mendeley data with the following doi 10 17632 nxmx5w8y63 1 
4339,machine learning algorithms have shown promise for streamflow forecasts reservoir operations and scheduling but have exhibited lower accuracy in predicting extended time horizons for reservoir inflows newer deep learning algorithms exhibited improved inflow forecasting accuracy but existing research has been mostly limited to real time operation and short term planning we propose a new multi step forecasting approach to improve long term forecasting accuracy for both water supply and inflow volumes this approach uses historical snow water equivalent swe and reservoir inflow time series data to train an encoder decoder algorithm to predict the reservoir inflow of future time steps during the april july runoff period the optimal model and hyperparameters are selected through five fold time series cross validation for variations between long short term memory lstm and convolutional neural network cnn encoder decoder algorithms we evaluated each algorithm using 30 years of reservoir inflow and swe data at the upper stillwater reservoir located in utah the optimal model was an lstm encoder decoder algorithm with 16 nodes per layer using this algorithm we investigate the trade off between model complexity and accuracy for long term water supply relative to a process based ensemble streamflow prediction esp model as a baseline and simpler statistical methods traditionally used in forecasting sarima var tbats long term water supply forecasts of the optimal deep learning algorithm proved superior to statistical methods and rivaled those of the esp 50 exceedance probability forecast i e the most probable forecast evaluated over five consecutive hold out periods keywords reservoir forecasting deep learning snow inflow 1 introduction classical statistical models such as seasonal auto regressive integrated moving average sarima papamichail and georgiou 2001 vector auto regression var iddrisu et al 2016 and trigonometric box cox transformation with arma residuals trend and seasonal components tbats elizaga et al 2014 have long been employed for reservoir inflow and outflow prediction these models are well suited to short term forecasts but have limited capacity for long term forecasts due to the convergence of the auto regressive part of the model to the mean of the time series shumway and stoffer 2000 while long term forecasts pose greater challenges due to long term dependencies forecast skill is also confounded by hydro meteorological predictability in snow dominated catchments anghileri et al 2016 thus non linear patterns governed by weather often exacerbate statistical model applications where linear decomposition is assumed valid better suited to the task physical models simulate hydrologic forecasts in which probabilistic ensembles require atmospheric input for future scenarios while quantifying uncertainty through statistical post processing krzysztofowicz 1999 krzysztofowicz 2002 raftery et al 2005 gneiting et al 2007 bennett et al 2016 widely used throughout the hydrologic community as a benchmark physical model siddique and mejia 2017 lee et al 2017 harrigan et al 2018 ensemble streamflow prediction esp uses physically based conceptual hydrologic models with states set to current basin conditions to create a probabilistic outlook consisting of a distribution of possible future events franz et al 2003 as part of the national oceanic and atmospheric administration noaa weather service the colorado basin river forecasting center cbrfc utilizes a suite of such physical models that are made available to water managers for enhanced decision making the esp model is capable of projecting accurate long term water supply forecasts representing the total expected inflow within a basin during a four month runoff period direct inputs for this model consist of observed precipitation temperature freezing levels and streamflow however snow water equivalent swe is not included rather the snow model within each basin is first initialized with current conditions to then simulate the building and melting of snowpack based on climate forecasted precipitation and temperature time series inputs day 1985 bradley et al 2004 jeong and kim 2005 mcenery et al 2005 spatial variability of snow properties are mainly accounted for by the melting rather than the accumulation processes and are commonly described using a snow depletion curve shamir and georgakakos 2007 esp has proven to be an accurate tool for long term water supply forecasting but it is limited in mapping non linear relationships at both micro e g landscape runoff processes and macro levels e g anthropogenic climate change due to forecast system errors stemming from model calibration data and initial states franz et al 2003 the need for more complex models within hydrologic forecasting is two fold standard statistical methods are limited in capturing non linear patterns and physical process based methods often lack sufficient information to accurately characterize basins based on hydrologic parameters nearing et al 2020 while both statistical and physical methods rely on information from the past to make predictions of the future the underlying complexity of hydrologic relationships between the input and output requires a model sophisticated enough to recognize non linear patterns without compromise to accuracy machine learning algorithms allow us to capture complex non linear patterns from data that would otherwise require extrapolation thus providing an alternative approach to forecasting among a variety of related hydrologic fields rainfall runoff prediction for ungauged basins kratzert et al 2019a hydropower production forecasting stokelj et al 2002 spatial swe estimation for mountainous areas zheng et al 2018 and quantifying climate and catchment control on hydrological drought konapala and mishra 2020 machine learning has seen broad application to reservoir inflow forecasting in both direct and multi step scenarios and generally resulted in more reliable forecasts of inflow extremes coulibaly et al 2000 trained a feed forward neural network using an early stopping training approach for real time reservoir inflow forecasting with lead times of one to seven days an improvement to daily reservoir inflow forecasts was later made using a robust weighted average ensemble that takes advantage of three different models nearest neighbors a physical model and an artificial neural network coulibaly et al 2005 an additive ensemble for monthly reservoir inflow forecasting was developed by bai et al 2015 incorporating an auto regressive model least squares support vector machine and adaptive neuro fuzzy inference system to subforecast trend period and stochastic terms bourdin et al 2014 wang et al 2012 and ahmed et al 2015 all used ensemble learning methods coupled with meteorological predictions to forecast reservoir inflows with respective forecast horizons of three eight and fourteen days long range streamflow forecasts extending twelve months were assessed by bennett et al 2016 in which calibrated climate forecasts are combined with a physical runoff model and a three staged error model to simulate reservoir inflows similarly liu et al 2017 developed a long term streamflow forecasting scheme extending nine months utilizing random forest and support vector regression for precipitation post processing of numerical weather predictions to feed into a hydrological watershed model deep learning has improved reservoir inflow forecasting but has been mostly limited to real time operation and short term planning deep belief networks for multi scale feature learning bai et al 2016 improved on prior efforts bai et al 2015 in direct step forecasting using an additive ensemble approach budu 2014 and chiamsathit et al 2016 both applied multi layered perceptrons for direct step forecasting scenarios that achieved reasonable accuracy among daily and monthly time step respectively time lagged recurrent neural networks tlrns have been studied muluye and coulibaly 2007 kote and jothiprakash 2008 taghi sattari et al 2012 where a preceding record of reservoir inflow is used to investigate the performance of a back propagation through time bptt algorithm better prediction of inflow into a reservoir using tlrn was achieved by kote and jothiprakash 2008 by modifying the artificial neural network to include seasonal monsoon effects accurate mapping of high and low flows was achieved following a monthly time step more recently multi step flood forecast models chang and tsai 2016 zhou et al 2019 kao et al 2020 have been developed for predicting reservoir inflows using adaptive neuro fuzzy inference systems and long short term memory lstm based encoder decoder frameworks however the time step for these forecasts are hourly with forecast horizons only extending four six and eight hours while deep learning has benefited short term hydrologic forecasting attempts at long term forecasts for reservoir inflows have generally exhibited lower accuracy due to greater uncertainty and long term dependencies similar to bennett et al 2016 and liu et al 2017 earlier attempts at long term forecasting muluye and coulibaly 2007 kote and jothiprakash 2008 failed to accurately predict the peak inflow at extended time horizons we evaluate a new approach to long term forecasting of reservoir inflow based on swe data at a macroscale using deep learning techniques this approach aims to use historical swe and historical reservoir inflow time series data to train an encoder decoder algorithm to predict the reservoir inflow of future time steps for the proceeding four months compared to a multitude of benchmark physical models deep learning models have been shown to perform better when trained from a large sample of hydrologic variability between catchments kratzert et al 2019b thus the insight behind this approach is to train a deep learning model to capture non linear behavior characterized by inter annual swe variability at a macroscale non linear hydrologic variables such as swe increasingly compromise model reliability with expected increases in frequency for consecutive snow drought years and increases in variability for snowpack amount and timing marshall et al 2019 especially within snow dominated catchments the magnitude of inter annual swe variability affects reservoir operations by reducing the reliability of reservoir inflows rhoades et al 2018 and hydroelectric power generation fleming and weber 2012 in comparison to esp the proposed deep learning model s multi step forecast represents the expected hydrograph to enter the downstream reservoir based on initial swe conditions while the esp forecast represents a single value for the total inflow furthermore the sequential nature of the encoder decoder model is driven by a preceding record of inter annual reservoir inflow and swe variability whereas the esp model is first initialized with swe but then driven by anticipated rainfall and temperature fluctuations from regional climate models we evaluate four different variants of the encoder decoder algorithm regular lstm lstm residual lstm lstm regular cnn lstm and residual cnn lstm in contrast to recurrent neural networks rnn such as lstms convolutional neural networks cnn operate independently of previous time steps to capture fixed size contexts allowing for parallel computation within a given sequence the stacking of convolutional layers allows for precise control of the dependencies to be modeled by effectively increasing the context size gehring et al 2017 implementing residual connections in cnns has been shown to further improve model performance by increasing the depth of the model architecture he et al 2016 deep learning frameworks utilizing residual connections in cnns have seen extensive application in other fields liu and song 2018 ning et al 2019 cengi l and √ßinar 2018 but have not yet been evaluated in the context of reservoir inflow forecasting a key distinction between the proposed models in this paper and the others listed above is that deep residual lstms and cnns are used in an attempt to improve the learning process for long range forecast accuracy while at the expense of greater model complexity thus we investigate the trade off between model complexity and accuracy using a physical esp model as a baseline and simpler statistical methods traditionally used in forecasting sarima var tbats the specific research questions of this study are 1 what is a suitable trade off between model complexity and accuracy for long term water supply forecasting 2 what are the characteristics of the hold out years in which the selected deep learning model excels in accuracy 3 how does accuracy for long term water supply forecasting compare between a purely data driven model and a physical process based model we close this section with a description of the study area data model and methods in materials and methods next we outline three standard statistical forecasting models used for comparison along with a description for the esp model as a baseline the results and discussion sections are broken down into model selection model evaluation and model complexity versus accuracy based on the process of five fold cross validation lastly we provide a summary of conclusions and recommendations for future research 2 materials and methods 2 1 study region this study focuses on the upper stillwater reservoir located at the top of the central utah water conservancy district s cuwcd collection system in the uinta mountains upper stillwater is located within the duchesne basin positioned in northeastern utah the reservoir drainage area ending at the rock creek above south fork gaging station 09277800 is 99 square miles has a mean annual precipitation of 34 3 inches and a mean annual flow of 149 cubic feet per second the inflow to the reservoir is unregulated and driven by snowmelt during a four month runoff period between april and august upper stillwater was constructed between december 1983 and november 1987 is operated and maintained by the cuwcd has a capacity of 32 009 acre ft and a max surface area of 314 acres there is currently an available diversion capacity of 285 cubic feet per second cfs by way of the stillwater tunnel a feature of the strawberry aqueduct collection system in addition to a river outlet and spillway with a total capacity of 75 000 cfs releasing directly into rock creek cuwcd is one of utah s four large specialty water districts that provides potable and secondary water to various water associations conservancy districts irrigation companies and local residents the water district spans eight counties with over 3 5 billion in infrastructure there are currently eight lakes reservoirs maintained and operated by cuwcd that house non potable water in excess of 1 6 million acre ft the storage levels for these reservoirs act as a barometer for the state s water resources and provide insight for how to appropriately prepare for future water usage fig 1 shows upper stillwater located in the middle of the figure surrounded by a network of snow telemetry monitoring sites 2 2 data the data used for model training is taken from cuwcd operations reports and the national resource conservation service nrcs the daily inflow into the reservoir is computed from reservoir elevation change compared to losses e g seepage and evaporation and releases to rock creek to determine the acre foot change the two data types used for this study are reservoir inflow and swe the depth of water in that would result if the snowpack were melted specific to rock creek at upper stillwater reservoir the historical daily data span from january 1990 to the present with new values updated daily the time series for reservoir inflow fig 2 is quality controlled by cuwcd and continuous with no gaps requiring interpolation the reservoir inflow time series illustrates a seasonal runoff period fed by snowmelt that begins in april and ends in august a governing assumption of this study is that water managers only have until the end of march to make a final decision regarding the level of storage space to leave vacant in the reservoir for the runoff season therefore the critical period to forecast reservoir inflow is a four month window between the first of april and end of july such an extended forecast horizon requires a model capable of learning long term dependencies swe data were collected from the nrcs monitoring network of snow telemetry sites for the same period as upper stillwater s reservoir inflow time series table 1 for each of the available monitoring sites a maximum of three daily data points required interpolation over the entire period the dependence of reservoir inflow fig 2 on swe fig 3 is the primary relationship that the model will attempt to learn the daily data were then prepared for training the algorithm the data were first re sampled into weekly averages and scaled between 0 and 1 based on the chosen activation function for the model see section 2 3 a sliding window length of 20 weeks is used as input to predict the next 18 weeks i e april july the length of the window is selected based on the time series data for swe depending on the precipitation distribution during a given winter season the process of swe accumulation ranges between 15 to 25 weeks on average a 20 week input window length is assumed to be the most common year to year scenario for the model during training 2 3 deep learning model the objective of the model is to forecast multiple time steps forward based on multiple inputs from the past the inputs are the multiple time series of reservoir inflow and swe and the output is a future reservoir inflow sequence prediction starting at the final point in the input data therefore a multivariate sequence to sequence prediction model is required this type of model is broken down into two separate models one for reading the input sequence and encoding it into a fixed length vector encoder and a second model for decoding the fixed length vector and outputting the predicted sequence decoder sutskever et al 2014 following the decoder a time distributed fully connected layer is used as the final component to condense the output and yield a forecasted sequence of values fig 4 illustrates four variants for this algorithm all of which receive a sliding window input of multiple variables and transmits a sliding window output of a single target vector this architecture has proven to be effective for numerous sequence to sequence problems including multi step flood forecasting kao et al 2020 network traffic forecasting zhang and you 2020 weather forecasting yuan et al 2019 and predicting solar performance ratio yen et al 2019 each encoder decoder model is written in python using the keras deep learning library gulli and pal 2017 another important feature of the proposed model is how each node sequentially transmits information to the next within each layer along with how the weights within the network are optimized the exponential linear unit elu clevert et al 2016 and adam optimizer kingma and ba 2017 are selected for the activation function and optimizer respectively encoder decoder variants although rnns facilitate sequential learning they fail to connect information from the past input to the present output data when the gap between the two grows too large which creates the issue of long term dependencies this problem is addressed with the use of lstm networks hochreiter and schmidhuber 1997 a successor to the rnn an essential feature of the lstm cell is its state that runs directly through the network enabling addition or removal of information from the cell state via regulated gates lstm networks are thus an essential component used for constructing encoder decoder variants reliant on capturing long term dependencies as illustrated in fig 4 four variants of the encoder decoder algorithm are used with varying degrees of complexity in the first variant regular lstm lstm a simple lstm encoder is connected to an lstm decoder by a repeat vector which acts as an adapter to fit the encoder and decoder parts of the network together the extracted features from the encoder are fed into the decoder to yield a forecasted sequence of values a repeat vector is configured to repeat the fixed length vector one time for each time step in the output sequence a dense layer is used as the output for the network the same weights are used to output each time step in the output sequence by wrapping the dense layer in a time distributed wrapper this allows the same output layer to be reused for each element in the output sequence the second variant residual lstm lstm follows the exact same flow process but it instead consists of residual connections between layers as shortcuts residual connections are used to center layer gradients and propagated errors increasing the depth of the network while making it easier to optimize wang et al 2018 thus the model is optimized for a residual mapping of feature extraction encoder to be used as input to forecast a sequence of values decoder from the sliding window of time series input in contrast to the lstm encoder a regular and residual cnn is used instead for the third and fourth variant architectures a one dimensional cnn is a model with one or more hidden layers that operate over a 1d sequence e g sentence or time series through convolutions in a multi layer cnn the stacking of cnn layers creates a hierarchical structure that provides a shorter path to capture long term dependencies therefore the model creates hierarchical representations over the input sequence allowing nearby input elements to interact at lower layers while distant elements interact at higher layers gehring et al 2017 causal padding van den oord et al 2016 is used for each cnn layer to ensure the model does not violate the temporal order i e model does not have look ahead bias the cnn encoder is designed to gradually reduce the dimensionality of the input feature matrix while increasing the number of feature abstractions this is done using filters see section 2 3 and pooling layers whose purpose is to condense a cnn layer s output to the most prominent elements max pooling is used at the end of each residual section fig 4 in which one residual block is connected in sequence for a given number of filters and kernel size max pooling and flatten are operations used at the end of the encoder to downsize the extracted features into a fixed length vector proportional to the number of nodes in the lstm decoder the final output represents the extracted elements as features from the input sequences that will be fed as a flattened sequence for the decoder model training and selection while the historical data record spans 30 years for both swe and reservoir inflow there is still a need to increase the number of weekly averaged samples to improve model performance with unseen data i e generalization the index sequential method ism is a popular pre processing method used within the colorado river basin when planning for the future by generating synthetic sequences from past hydrology kendall and dracup 1991 lukas and payton 2020 ouarda et al 1997 uncertainty of future hydrology stems from inherent randomness now coupled with anthropogenic climate change thus requiring predictive models to become more resilient towards changing hydrology one approach to this challenge is to use an ism where multiple sequences of past data allow a model to consider uncertainties of future hydrology against a broader range of possible sequences in the future salehabadi et al 2020 the ism generates n synthetic sequences by shifting over each water year by one index and repeating n times for a historical record of length n this method has been widely used with physical models such as the colorado river simulation system crss for predictive management of potential future hydrology usbr 2012 lukas and payton 2020 but it is limited in providing sufficient variety of statistically plausible sequences that capture hydrologic extremes i e longer or more intense droughts salehabadi et al 2020 a modified version of the ism is applied here for model training where synthetic sequences of the historical record are generated by randomly indexing each water year as a block fig 5 to effectively shuffle the original 30 year sequence table 2 salehabadi et al 2020 refer to this bootstrap method as water year block disaggregation in which randomized samples of each water year are generated in collection with multiple sites rather than repeatedly shifting each water year over by one index this non parametric bootstrap method allows for greater variability to be created within each sequence to effectively improve model performance vogel and shallcross 1996 srinivas and srinivasan 2005 srinivas and srinivasan 2006 the process of randomized sampling is assumed valid specifically for this study due to the seasonal characteristics of both reservoir inflow and swe within a given water year period thus allowing for seamless indexing this method is then taken a step further by also scaling each sampled water year at random between 0 5 and 1 5 for varying degrees of magnitude centered around the original the key benefit to this additional modification is expanding the model s exposure to hydrologic extremes for a greater capacity to generalize overall the described process of generating synthetic sequences is used to increase the size of the training dataset five fold prior to testing each hold out year through cross validation the optimal hyperparameters for each model architecture are determined through five fold time series cross validation five consecutive hold out periods are used between 2011 and 2015 to first determine the optimal hyperparameters while the next five year period i e 2016 2020 is used to determine the optimal model architecture based on the selected hyperparameters for each of the four encoder decoder variants configurations of lstm nodes and cnn kernels are alternated three separate times this method results in 12 separate models to train and compare against during cross validation based on varying model complexity defined as trainable parameters the number of available lstm nodes for the regular and residual lstm encoder decoders are 8 16 and 32 while the available sizes for the cnn kernels within the regular and residual cnn layers are 2 4 and 8 in contrast to the kernels the number of filters in each cnn layer are kept constant during cross validation in order to simplify the process by reducing the number of parameter configurations to search over likewise the number of nodes within each residual model s lstm decoder are kept constant at 16 nodes per layer a time distributed fully connected layer is placed at the end of each model to condense the output from the decoder by setting the number of nodes to half of the lstm decoder layers are defined as arrays of nodes that sequentially transmit information from one to the next within each layer nodes are connected by multiple weights for a given number of inputs and outputs a single node receives input data processes the input as a weighted sum and then propagates new information to its successor based on a given activation function the relative density for a given model is defined as the average number of trainable parameters per layer filters and kernels are interrelated hyperparameters specific to cnns a kernel represents a matrix of weights that slide over the input sequence calculating the dot product between the sequence values and matrix weights therefore the size of the kernel represents the length of the window it spans for deep feature extraction a complete tour of a kernel over the input sequence represents a filter thus kernel s operating over multiple channels of input establish a filter feature map the input window represents the multivariate time series of reservoir inflow and swe accumulation during the winter season until the first of april epochs represent the number of full passes of the dataset that the model uses during training the batch size is the fraction of data that the model is exposed to during each epoch a maximum of 50 epochs are to be used for each model during training an early stopping algorithm is also used during training to prevent over fitting with excessive epochs a training session will terminate early if there are 10 consecutive epochs with no improvement in minimizing the mean squared error across the first five test years i e 2011 2015 the model is trained one configuration at a time and then tested against a hold out set to develop the performance metrics normalized mean absolute error nmae normalized root mean squared error nrmse normalized median absolute error nmedae nash sutcliffe model efficiency coefficient nse and explained variance expvar 1 nmae i 1 n q obs i q pred i i 1 n q obs i 2 nrmse 1 n i 1 n q obs i q pred i 2 1 n i 1 n q obs i 3 nmedae med q obs q pred 1 n i 1 n q obs i 4 nse 1 i 1 n q obs i q pred i 2 i 1 n q obs i 1 n i 1 n q obs i 2 5 expvar 1 i 1 n q obs i q pred i 1 n i 1 n q obs i 2 i 1 n q obs i 1 n i 1 n q obs i 2 the performance metrics are calculated from a hold out dataset that is not used during model training the hold out set spans 38 weeks and consists of two consecutive parts input 20 weeks and output 18 weeks the input spans from november through march the output from april through july for example the hold out dataset used to forecast the 2015 runoff period begins in november 2014 and ends at the start of april 2015 table 3 a rank is then assigned to each model based on an equally weighted sum for each performance metric averaged over the five year test period ranging from 1 best to 3 worst a ranking system is displayed rather than the calculated performance metrics in order to avoid confusion for actual model performance since the 2011 2015 hold out data is used for hyperparameter tuning not model evaluation uncertainty uncertainty is quantified within two different areas for this study the model s underlying uncertainty towards producing forecasts and the intrinsic uncertainty of reservoir inflow at both weekly average and april july inflow periods the model s uncertainty is quantified using a 95 confidence interval and is calculated from an ensemble of model runs due to the stochastic nature of the deep learning model a slightly different forecast will be returned each time the model is trained therefore each model is trained multiple times to establish a normal distribution of model predictions for each time step in the forecast a normal distribution is assumed valid at each time step as the number of ensemble members is increased by application of the central limit theorem by design forecast points are labeled as outliers if they lie beyond the whiskers of their respective boxplot following the tukey method tukey 1970 the other area of uncertainty within this study is towards the variability of reservoir inflow as the main target for prediction a shaded exceedance probability plot is used to visualize the realm of likelihood for each time step within the multi step forecast fig 6 the shaded plot is created by calculating the daily exceedance probability across the historical 30 years of data and then resampling the values into weekly averages within the april july runoff period therefore the shaded plot is composed of 30 individual traces for weekly average reservoir inflow ordered from greatest exceedance probability lowest inflow to lowest exceedance probability highest inflow lastly the exceedance probability for the total april july inflow volume is also determined for visual comparison the 2016 2020 hold out years are annotated within the subplot fig 6 to provide context for each year s relative uniqueness among the entire historical record total inflow the total inflow volume is determined as the area under the curve for the forecasted hydrograph however each time step within the four month forecast represents a weekly average of reservoir inflow therefore each hydrograph is first re sampled backwards to a daily time step for greater granularity before calculating the total inflow volume for simplicity an assumption is made that weekly averaged inflow is representative as a daily constant during each week thus when re sampling to a daily time step the weekly averaged inflow is scaled to represent each day in a given week the sum of all daily inflows for the re sampled hydrograph are then converted from cubic feet per second to acre feet per day to yield the total inflow volume over the four month period the relative error for total inflow volume is calculated between a given model and the benchmark esp model across two different performance metrics mean absolute relative error mare and root mean squared relative error rmsre 6 mare 1 n i 1 n v obs i v pred i v obs i v benchmark i 7 rmsre 1 n i 1 n v obs i v pred i v obs i v benchmark i 2 by design the relative error within each metric provides a baseline representative of the esp model s performance the values of each calculated metric will either reside above or below a value of 1 0 to indicate worse or improved model performance relative to the esp model therefore the comparison of relative error between the selected deep learning model and various statistical models serves as the main tool for assessing the trade off in accuracy for model complexity 2 4 statistical methods for comparison the deep learning forecasts are compared against three widely used statistical models seasonal auto regressive integrated moving average sarima vector auto regression var and trigonometric seasonal box cox transformation with arma residuals trend and seasonal components tbats instead of a weekly average time step frequency each model is trained on monthly averaged data due to their limitations to forecast into such extended horizons the forecasts for the total expected inflow volume are also re sampled backwards to a daily time step following the same general assumption mentioned in section 2 3 2 4 1 sarima model a discrete time series z 1 z 2 z 3 z n 1 z n of measurements at equal time intervals is simulated by a stochastic sarima model box et al 2015 given by 8 œÜ b œÜ b s 1 b d 1 b s dz t Œ∏ b Œ∏ b s e t here t represents the discrete time and s denotes the length of each season the b term corresponds to the backward shift operator which is defined by bz t z t 1 and b s z t z t s the independently and normally distributed white noise residual is represented by e t nid 0 œÉ e 2 which has a zero mean and variance defined by œÉ e 2 from the left hand side of eq 8 the first two terms œÜ and œÜ represent series expansions given by 9 œÜ b 1 œÜ 1 b œÜ 2 b 2 œÜ p b p 10 œÜ b s 1 œÜ 1 b s œÜ 2 b 2 s œÜ p b ps eq 9 represents the nonseasonal auto regressive operator of order p and œÜ i i 1 2 p depicts the nonseasonal auto regressive parameters 1 b d is the nonseasonal difference operator of order d which produces nonseasonal stationarity of the dth differenced data usually d 0 1 or 2 whereas eq 10 depicts the seasonal auto regressive operator of order p and œÜ i herein i 1 2 p are the seasonal auto regressive parameters 1 b s d is the seasonal differencing operator of the order d to produce seasonal stationarity of the dth differenced data usually in the order of d 0 1 or 2 from the right hand side of eq 8 the first two terms Œ∏ and Œ∏ represent series expansions given by 11 Œ∏ b 1 Œ∏ 1 b Œ∏ 2 b 2 Œ∏ q b q 12 Œ∏ b s 1 Œ∏ 1 b s Œ∏ 2 b 2 s Œ∏ q b qs eq 11 is the nonseasonal moving average operator of the order q eq 12 is the seasonal moving average operator of order q and Œ∏ i i 1 2 q are the seasonal moving average parameters the natural log of the reservoir inflow time series is taken to stabilize the variance of the time series and to transform any skew in the distribution into a normal distribution papamichail and georgiou 2001 using an annual seasonal term s of 12 months the sarima model parameters p d q p d q are configured using the autoarima function provided in the pmdarima statistical library in python 3 0 2 4 2 var model var is another frequently used model for multivariate time series the basic var model of order p as suggested by sims 1980 is given by 13 y t a 1 y t 1 a 2 y t 2 a p y t p cd t u t where y t y 1 t y 2 t y kt represents a vector of k observable endogenous variables and d t consists of all deterministic variables which carry a constant a linear trend seasonal dummy variables and user specified variables u t is a k dimensional unobservable zero mean white noise process which has a positive definite co variance matrix e u t u t u a i and c are parameter matrices of suitable dimension upon which various restrictions can be imposed for a k dimensional auto regression with an effective sample size n the optimal lag order p is selected that minimizes the akaike information criteria aic given by the following equation 14 aic p ln p 2 n k 2 p p is the quasi maximum likelihood estimate of the innovation covariance matrix p ventzislav and lutz 2005 sin and white 1996 the parameters in eq 13 are estimated by the method of generalized least squares this is done by first estimating the individual equations of the system by ordinary least squares the residuals can then be utilized to estimate the white noise co variance matrix u as u t 1 t 1 t u t u t which is used to compute the generalized least square estimator iddrisu et al 2016 the var model is developed through the statsmodels python module and utilizes the same swe data as the deep learning model 2 4 3 tbats model tbats is a combination of three methodologies i exponential smoothing method ii box cox transformation and iii arma model for residuals the box cox transformation helps to deal with non linear data and arma model for residuals can de correlate the time series data the trigonometric expression of seasonality terms serves to reduce the parameters of model when seasonal frequencies e g annual streamflow or annual peak flow are high and improves the model flexibility i e lower bias with higher variance enabling it to handle complex seasonality the tbats model is comprised of the following terms y t Œª l t 1 œï b t 1 i 1 t s t m i i d t l t l t 1 œï b t 1 Œ± d t b t œï b t 1 Œ≤ d t d t i 1 p œÜ i d t i i 1 q Œ∏ i e t i e t where y t Œª time series at moment t box cox transformed s t i ith seasonal component l t i local level b t trend with damping d t arma p q process for residuals e t gaussian white noise seasonal part s t i j 1 k i s j t i s j t i s j t 1 i cos œâ i s j t 1 i sin œâ i Œ≥ 1 i d t s j t i s j t 1 i sin œâ i s j t 1 i cos œâ i Œ≥ 2 i d t œâ i 2 œÄ j m i model parameters t amount of seasonalities m i length of ith seasonal period k i amount of harmonics for ith seasonal period Œª box cox transformation Œ± Œ≤ smoothing œï trend damping œÜ i Œ∏ i arma p q coefficients Œ≥ 1 i Œ≥ 2 i seasonal smoothing two for each period based on a fourier series each seasonality is modeled by a trigonometric representation as an innovations state space model tbats admits a larger parameter space with the possibility of better forecasts hyndman 2008 additionally the model handles non linear features typically seen in time series while taking into account any auto correlation within the residuals de livera et al 2011 the tbats model is implemented using the tbats package in python 3 0 and incorporates quarterly bi annual and annual seasonal periods 3 ensemble streamflow prediction ensemble streamflow prediction esp initially introduced by the national weather service nws day 1985 is a conditional monte carlo simulation approach commonly used for statistical post processing of forecasts and estimating the inherent uncertainties najafi et al 2012 esp combines physical modeling of the river basin with a probabilistic representation of the future conditions using historical weather data the esp technique originally assumed that if historical weather patterns are taken together they represent possible future conditions day 1985 each of the historical years of weather starting at the forecast date is assigned a probability 1 m of re occurring where m is the total number of weather patterns considered reflecting the traditional weights employed to represent such an empirical distribution faber and stedinger 2001 snowmelt and runoff consequent to the current weather conditions of temperature and precipitation are calculated with a physical process based model a hydrograph of the resultant streamflows is produced for a given weather pattern this process is repeated for the weather patterns of each historical year producing traces of streamflow together these traces provide the probabilistic structure to forecast future streamflows a suitable probability density function esp p d f is fitted to the generated streamflow ensemble describing the likelihood of an event occurring during a certain time period t that is forecasted in this study the most probable streamflow q t with 50 exceedance probability is used for comparison the 50 exceedance probability is representative for medium flows pr m and is defined by pr m t pr q t q l t pr q t q u t where q l and q u are respectively the lower and upper limits of the medium flow category jeong and kim 2005 4 results 4 1 model selection results of the five fold cross validation are displayed below in which optimal hyperparameters are first selected followed by individual model selection table 4 the optimal number of nodes per layer for the regular lstm lstm was centered at 16 while both options for increasing and decreasing this number resulted in poorer performance in comparison to the residual lstm lstm there is an observed trend of increasing model performance when increasing the number of nodes from 8 16 and 32 conversely this opposite trend is observed for the residual cnn lstm variant except now the number of lstm nodes is fixed at 16 and the kernel length is increased from 2 4 and 8 the cnn lstm follows in the same trend of its residual counterpart where model performance appears to decrease as the size of the kernel is increased in summary the four optimal model configurations are 1 regular lstm lstm with 16 nodes per layer 2 residual lstm lstm with 32 nodes per layer 3 regular cnn lstm with a kernel size of 2 and 16 nodes per lstm layer and 4 residual cnn lstm with a kernel size of 2 and 16 nodes per lstm layer while optimal model selection is also based on averaged performance metrics the hold out data is now between 2016 and 2020 table 5 such that the final model selection is unbiased to the hyperparameter tuning the best performing model is the regular lstm lstm with the lowest reported errors nmae nrmse and nmedae and highest values for nse and expvar the regular lstm lstm is the second most complex model among the four with a parameter density of 1 204 trainable parameters per layer based on a total of 4 817 parameters and four layers fig 4 the second best performing model is the residual lstm lstm which is the most complex model within the study requiring 40 993 trainable parameters the two worst performing models are the regular and residual cnn lstms with model complexities defined by 3 813 and 6 625 trainable parameters respectively overall there is a general trend of increasing model accuracy with increasing network density 4 2 model evaluation this section is organized by each hold out test year from 2016 through 2020 fig 7 illustrates the multi step forecasts from the selected lstm lstm model for the hold out inflow periods which are composed with the actual values solid black line forecasted values dashed black line boxplots and shaded exceedance probabilities for reservoir inflow the box plots are based on a sample population of 30 individual model trainings per test year table 6 summarizes the performance metrics for each hold out year fig 8 compares the total forecasted inflow volume between the baseline esp model at the edges and center of the model s 95 confidence interval the forecasted hydrograph for 2016 accurately predicts both the rising and falling limbs but fails to identify the peak inflow even when using the upper bound of the boxplot whiskers the greatest errors are associated with the peak inflow that occurs during june and extends into the 10 20 exceedance probability range whereas both of the observed rising and falling limbs are centered within a 50 exceedance probability the forecasted total inflow volume is under predicted due to the under predicted peak inflow the 2017 forecast is less accurate in determining the rising and falling limbs for the inflow hydrograph however the accuracy for the peak inflow is improved even during an exceptional water year for swe accumulation and reservoir inflow the observed exceedance probabilities of both rising and falling limbs range between 10 30 while the peak inflow again extends into the range of 10 20 the center of the model s confidence interval very accurately predicts the total expected inflow volume for an associated exceedance probability of about 18 fig 6 while both 2016 and 2017 hold out periods exhibit very different inflows the performance metrics are shown to be relatively similar with values for nse and expvar between 0 75 and 0 77 respectively table 6 the forecasted hydrograph for 2018 is the smallest among the five hold out periods with a total inflow exceedance probability of nearly 80 fig 6 the rising limb appears to be under predicted while the falling limb is overestimated along with the peak inflow being inaccurately predicted for both magnitude and timing however the difference between the observed and the forecasted values is relatively small compared to larger years i e 2017 and 2019 as the performance metrics for 2018 report both nse and expvar values of about 0 83 table 6 in comparison to a period of large inflow the forecast for 2019 also demonstrates skill in predicting the rising limb but then greatly under predicts both the peak inflow and the falling limb the 2019 inflow period in particular was exceptional with both the peak inflow and total inflow exceedance probabilities being less than 10 fig 6 7 thus the performance metrics are reported the worst for this year among all of the hold out years likewise the predicted total inflow volume for this period was the least accurate but was still improved when using the upper bound of the model s confidence interval fig 8 lastly the 2020 forecasted hydrograph is the most accurate among the five year test period both the rising and falling limbs are accurately predicted with the exception of a slight under prediction for the peak inflow the performance metrics for the 2020 runoff period were exceptional with the highest values for nse and expvar 0 975 and 0 976 respectively along with the lowest reported errors for nmae 0 109 nrmse 0 153 and nmedae 0 064 table 6 overall the five year average of performance metrics demonstrate a high performance for the selected lstm lstm model however there are consecutive reoccurring periods where the model exhibited significant error and uncertainty the greatest errors stem from under predicted peak inflows which coincide with greater model uncertainty i e wider distributions illustrated from the respective boxplots likewise the widest time step distributions among each forecast are located at the descending inflection point fig 7 4 3 complexity vs accuracy the trade off between model complexity and accuracy is plotted by comparing the relative error for the total inflow volume as a function of model complexity defined by trainable parameters fig 9 each model s error is relative to the benchmark esp 50 exceedance probability i e the most probable forecast for the 2016 2020 hold out periods all three statistical models are shown to under perform at this forecasting task with relative errors residing above the benchmark line sarima is the least complicated model used within this study and in turn is reported to have the worst performance tbats is slightly more sophisticated than sarima and is shown to have greater accuracy for the total inflow volume var is the most successful among the three statistical models with improved accuracy but at a cost for greater complexity however the model s accuracy is still outmatched by the benchmark esp the selected lstm lstm is the most complex among the statistical models and in turn there is a significant improvement in accuracy both the mare and rmsre indicate that the proposed deep learning approach is roughly 50 more accurate than the esp model 5 discussion previous studies including direct step coulibaly et al 2005 taghi sattari et al 2012 bai et al 2016 and multi step coulibaly et al 2000 muluye and coulibaly 2007 kao et al 2020 deep learning algorithms have improved forecasting accuracy but have yet to accurately forecast reservoir inflows at extended long term horizons extended four month forecasts by muluye and coulibaly 2007 demonstrated reasonable predictions of low and medium reservoir inflows but then either under or over predicted the peaks the optimal encoder decoder algorithm is proven superior to statistical methods and competes with the baseline esp model for long term water supply forecasting fig 9 however periods where the model exhibits the lowest accuracy were due to under predicted peak inflows fig 7 the selected model s best performance occurs during the most probable inflows at both weekly average and april july inflow periods the 2020 forecast yielded the greatest performance metrics while the observed inflow has an associated exceedance probability of nearly 50 along both timescales fig 6 7 likewise the 2018 test period is the second most accurate table 6 with the greatest observed exceedance probabilities due to exceptionally low flows conversely the model s worst performance occurs during the 2019 forecast in which the observed total and peak inflow exceedance probabilities are reported to be less than 10 the 2016 test period also under predicts the observed peak inflow resulting in the second worst forecast for total inflow fig 8 similar to muluye and coulibaly 2007 the selected lstm lstm model excels in accuracy during periods with high probability for total inflow i e medium to low flows but then under predicts the peaks as the total inflow exceedance probability decreases compared to existing statistical models the proposed deep learning approach effectively predicts the ensuing reservoir inflow as a four month hydrograph while the deep learning approach is shown to be more accurate on average over the five hold out periods there is a significant increase in model complexity application of this model is validated by the drop in relative error below the benchmark esp line fig 9 indicating roughly a 50 improvement in accuracy this improvement is indicative of the significant influence that inter annual swe variability has towards forecasting reservoir inflow climate driven forecasts made by the esp model are significantly more accurate than the statistical methods but are still outmatched by the deep learning approach among each of the five hold out years the selected model is shown to yield a more accurate forecast for total inflow fig 8 similar to kratzert et al 2019b accuracy for long term water supply forecasting is improved when using a purely data driven deep learning model trained from a large sample of hydrologic variability between catchments the trade off in complexity for accuracy is demonstrated to effectively predict long term water supply by observing inter annual swe variability at a macroscale furthermore the observed inter annual variability exhibited between the 2016 2020 hold out years fig 6 indicate a wide spectrum of hydrologic inflows thus further validating the metrics reported for the accuracy complexity trade off as being robust in contrast to the statistical comparison for an accuracy complexity trade off the results from the cross validation study show that model accuracy improves with increasing model density table 5 model complexity for the deep learning methods are a function of trainable parameters which are dependent on both the quantity and type of layers therefore a model s density provides the most useful comparison for inspecting an accuracy complexity trade off while an lstm layer has significantly more trainable parameters the depth of the cnn lstm networks are much greater fig 4 thus increasing the risk for vanishing gradients as errors are back propagated during model training the concept of residual connections is applied to address this issue by centering layer gradients and propagated errors but at a cost of greater model complexity through added layers by coincidence both regular and residual cnn lstm models arrived at the same hyperparameter value for kernel size table 4 while the number of lstm nodes and cnn filters per layer remained fixed therefore the poor results from the cnn lstms are likely due to poor architecture design and would likely be improved with a widened parameter search during cross validation overall the simple lstm lstm architecture is proven to be the most accurate but the model performance could potentially be improved further by investigating increased architectural depth with stacked layers 6 conclusion this research improves on prior multi step forecasting efforts to strengthen water manager abilities for long term planning of reservoir operations involving water supply and inflow volume given the comparatively high performance of the proposed approach in the study region the lstm encoder decoder architecture warrants further study for long term multi step reservoir inflow forecasting considerations for future research include i further experiments with model architecture and hyperparameters ii investigating additional independent variables and iii modeling additional reservoirs influenced by snowmelt runoff potential experiments with model architecture may include utilizing an attention mechanism to observe the intermediate states of the encoder rather than only the final states the inclusion of other independent climate variables such as atmospheric temperature and solar radiation may further improve accuracy finally modeling additional reservoirs will provide valuable insight into potential model transferability credit authorship contribution statement zachary c herbert conceptualization methodology software validation formal analysis investigation resources data curation writing original draft writing review editing visualization project administration zeeshan asghar writing review editing carlos a oroza supervision writing review editing project administration declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by the central utah water conservancy district and university of utah a special thanks to blake buehler heath clark jared hansen troy ovard the anonymous reviewers and the associate editor for their instrumental comments and critique geographic information system gis mapping was provided by lindsy bentley all reservoir inflow data is obtained from cuwcd operations reports and all snow water equivalent swe data is made publicly available by the nrcs online repository at https www nrcs usda gov the associated data with this paper can be accessed through mendeley data with the following doi 10 17632 nxmx5w8y63 1 
