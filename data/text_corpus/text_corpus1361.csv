index,text
6805,projections of future changes in hydroclimatic variables are available through various general circulation models gcms that are part of the coupled model intercomparison project phase 5 cmip5 assessment of the performance of these models in simulating past climate both individually and as an ensemble has received much attention and is commonly the first step in assessing their suitability of application in this study we have evaluated the ability of various ensemble models to simulate past temperature and precipitation in the gulf basin region of north america chosen as an illustrative case study we have developed ensembles from 34 cmip5 gcms using six diverse approaches including random forest support vector regression neural networks linear regression and weighted k nearest neighbors and compared the performance of the ensembles with each other and the individual gcms using a robust set of metrics and nonparametric tests for temperature random forest outperforms all other ensembles and the best performing gcm by a statistically significant margin and is able to simulate temporal and spatial patterns in temperature well none of the ensembles are able to adequately simulate observed precipitation patterns in the study area likely due to spatial differences in precipitation drivers in the region as well as the coarseness of the dataset itself however the random forest support vector regression neural network and linear regression ensembles achieved statistically significant improvements to precipitation simulation as compared to the individual gcms and the simple arithmetic mean ensemble which has been used in several studies keywords cmip5 climate model ensemble random forest support vector regression k nearest neighbours gulf basin region of united states 1 introduction projections of variability in climate variables such as temperature and precipitation have recently been published through the coupled model intercomparison project phase 5 cmip5 taylor et al 2012 outputs from over 50 models representing the efforts of 20 institutions around the world are available as part of this dataset the cmip5 includes near term 10 30 year scale and long term century scale experiments which are integrated using atmospheric ocean general circulation models aogcms additionally the cmip5 builds on previous efforts through the inclusion of earth system components es that go beyond aogcms by incorporating biogeochemical feedbacks within the climate system stouffer et al 2011 and by using models of a higher spatial resolution climate projections from this multimodel dataset have been widely used in modeling the impacts of future change on biological ecological hydrological and socio economic systems at various spatial scales e g bopp et al 2013 su et al 2013 touma et al 2015 selvanathan et al 2016 etc as such there is more confidence about the reliability of these models at the continental scales than at the regional scales uncertainties about their suitability at these smaller spatial scales still exist and raise concerns about their use for long term planning one of the critical questions being asked in water resource planning is what models perform well in simulating precipitation and temperature for a given region current practice is to use the ability of a model to simulate historical observations for a particular region as a measure of its reliability it is understood and accepted that this may not be reflective of the model s capacity to produce reliable future forecasts nonetheless it is widely acknowledged e g palmer et al 2005 jacob et al 2007 semenov and stratonovitch 2010 that the ability of gcms to simulate past patterns of climate variability increases our confidence in them several recent studies e g johnson et al 2011 jiang and yang 2012 sperber et al 2013 ramesh and goswami 2014 venkataraman et al 2016a raju et al 2017 have evaluated the performance of individual models from the cmip5 and or its predecessor the cmip3 using a variety of metrics the general consensus among these studies is that a gcms are better at simulating temperature than precipitation b no single model is able to represent trends on a spatially consistent basis and c the multimodel ensemble mean weighted or unweighted is superior to any individual model while the use of the multimodel ensemble approach has long been advocated in the climate modeling community numerous techniques have evolved to prescribe the ideal combination of models or subsets thereof to produce deterministic or probabilistic forecasts these techniques can be as simple as linear combinations of models where all models are assigned equal weight or involve linear regression to determine the weights e g krishnamurti et al 1999 2000 kharin and zweirs 2002 the inadequacies of such approaches have been pointed out by raftery et al 2005 duan et al 2007 and sansom et al 2013 other methods such as bayesian model averaging bma produce probabilistic forecasts and have been shown to improve model reliability and have been applied in various hydroclimatological studies sloughter et al 2007 duan et al 2007 duan and phillips 2010 najafi et al 2011 dechant and moradkhani 2014 najafi and moradkhani 2015 giorgi and mearns 2002 have developed a reliability ensemble averaging rea technique to jointly assess the ability of individual models within an ensemble to capture observations and the level of agreement within this ensemble for projected future change the motivation behind this approach was to minimize the contribution of poorly performing models within an ensemble building on this framework smith et al 2009 showed that despite disagreements among gcms useful information on future climate changes can be derived on a probabilistic scale machine learning algorithms such as artificial neural networks ann have also been employed in meteorological time series forecasting shrivastava et al 2012 provide a comprehensive review of the use of ann and conclude that these tools are best suited for smaller scale applications non parametric methods such as k nearest neighbours k nn have also been widely used in simulating hydroclimatic variables by re arranging historical data to arrive at new weather series e g young 1994 lall and sharma 1996 yates et al 2003 sharif and burn 2007 demonstrate that the k nn approach can be used not only to simulate observed records but to also generate future weather predictions the application of data mining techniques such as decision tree analysis in climate model assessment is still at its infancy in particular methods such as random forests henceforth rf breiman 2001 that have gained popularity in ecological classification e g guisan and thuiller 2005 prasad et al 2006 cutler et al 2007 and image processing e g ham et al 2005 plaza et al 2009 are still a relatively new entry to the field of climate studies specifically in the context of ensemble development eccel et al 2007 compared various linear and non linear approaches to predict minimum temperatures based on the output of numerical weather prediction models and concluded that the rf models yielded the best result albeit with marginal improvement over other multivariate approaches gaal et al 2012 modeled the impact of climate change on wine producing regions in hungary using rf as the technique allowed them to analyse the importance of various variables on identifying regions suitable for grapevine cultivation recently pang et al 2017 evaluated the applicability of rf to statistical downscaling of temperature in the pearl river basin of china they found that the rf model outperformed the ann and support vector machine svm models in simulating daily mean temperatures in the region rf was also used for downscaling land surface temperature by huetengs and vohland 2016 in general these studies emphasise the superiority of rf models over other approaches due to its inherent ability to assess the importance of each input variable in the eventual prediction as such the approaches outlined above can be classified as linear e g generalized linear model and nonlinear e g k nearest neighbors neural networks random forest etc knutti et al 2010 report that there is no universally accepted method for building ensemble models and highlight the need for using a robust set of metrics for evaluating models as well as effectively communicating the uncertainties therein to stakeholders the objective of the present study is therefore to compare a diverse set of approaches for building model ensembles in the context of simulating historical climate observations using an appropriate set of metrics specifically we compare the performance of each of these approaches with the best performing model of the cmip5 suite simulation of near surface air temperature henceforth referred to as simply temperature and precipitation rate in the gulf basin region of the united states for the time period 1950 to 1999 have been presented as an illustrative case study the same modeling approach can be used in other geographical regions as well there are some notable novel aspects and improvements of this study compared with previous work first to the best of our knowledge there has been no reported use of rf in climate model ensemble development and our study represents the first of this kind secondly we have adopted a highly rigorous statistical approach to comparing on a pair wise basis the performance of the aforementioned techniques against each other as well as against the best performing individual model in the cmip5 suite venkataraman et al 2016a presented the first look at the performance of cmip5 gcms in simulating hydroclimatic variables in this region using a simple arithmetic mean ensemble wherein each gcm was assigned the same weight they suggested that while this approach was sufficient for the purposes of studying future drought trends in the region more rigorous gcm combination approaches are needed to arrive at a consensus among model projections we present the development and analysis of such approaches in this study one of the principle findings is a statistically significant advantage in performance for the random forest linear regression support vector regression and neural network ensembles as compared to the individual gcms and the arithmetic mean ensemble in the case of temperature simulation random forest outperforms all other models considered by a statistically significant margin and given its robustness against overfitting we recommend it as the best ensemble model within the scope of the present study the structure of the paper is as follows section 2 presents a description of the study region and data under consideration performance metrics and cross validation procedures and methods for optimizing ensemble models built using six machine learning algorithms these ensemble models are evaluated in section 3 in terms of overall performance metrics nonparametric statistical tests and plots with respect to time longitude and latitude a discussion of our findings including the comparison of various approaches is presented in section 4 2 methodology 2 1 study region and data overview as shown in fig 1 the study region is largely comprised of the state of texas in general precipitation decreases from the sub humid eastern boundary of the study region which receives an annual average precipitation of 1500 mm to the semi arid western boundary which receives less than 500 mm annually the region is marked by large inter annual variability in precipitation with frequent droughts and floods which has led many climatologists to label the region s climate problematic norwine and john 2007 nielsen gammon 2011 peterson et al 2012 several earlier studies such as jiang and yang 2012 and langford et al 2014 that have used either the cmip3 or cmip5 models have already noted the inadequacy of these models in simulating the spatio temporal variation of precipitation in this region furthermore they attest to the challenging nature of capturing the mechanisms responsible for precipitation trends in the region on the other hand the ability of climate models to simulate historical temperature characteristics in the region has been well documented hernandez and uddameri 2014 venkataraman et al 2016b the data set under consideration contains monthly measurements of average surface air temperature c and precipitation rate mm day from 1950 to 1999 at 97 grid points in the gulf basin region with longitudes ranging from 103 5 to 93 5 and latitudes ranging from 25 5 to 34 5 as displayed in fig 1 therefore there are n 50 12 97 58 200 distinct ordered triples of time longitude and latitude coordinates denoted by t i ϕ i λ i i 1 n for each of the learning algorithms described in sections 2 3 through 2 8 two ensemble models are developed in the first model the dependent variable y i is the average surface air temperature at t i ϕ i λ i which is modeled as a function of x i x i 1 x ip the corresponding column vector of one degree regridded temperature predictions provided by p 34 cmip5 gcms reclamation 2013 the names and sources of these models are summarized in tables 1 and 2 the second ensemble model is the same except the observed precipitation rate is modeled in terms of the one degree regridded gcm projections for precipitation for the same locations 1 degree observed data for temperature and precipitation was compiled from the same source for training and validation purposes 2 2 performance metrics and cross validation procedures an essential part of each ensemble model is the ability to obtain predictions y f x for temperature precipitation based on the gcm projections given a training sample y i x i i 1 n the residuals are e i y i y i which yield the rmse normalized rmse the coefficient of determination r 2 and the mean absolute error 2 1 rmse 1 n i 1 n e i 2 2 2 nrmse rmse s y 2 3 r 2 1 i 1 n e i 2 i 1 n y i y 2 1 n n 1 nrmse 2 2 4 mae 1 n i 1 n e i eqs 2 2 and 2 3 show that nrmse and r 2 are monotonic functions of rmse so the first three performance metrics are essentially equivalent 1 the coefficient of determination r 2 is also known as the nash sutcliffe efficiency coefficient nash and sutcliffe 1970 1 the mae is an important alternative to these metrics willmott and matsuura 2005 and for completeness both r 2 and mae are displayed in table 4 for the temperature and precipitation ensembles note that negative values of r 2 can occur if a model s performance is sufficiently poor most learning algorithms have hyperparameters that can be modified to alter model performance such as the number of neighbors in k nn or the number of neurons in a neural network it may be tempting to increase the model s complexity until it achieves nearly perfect performance on the training data unfortunately this leads to the problem of overfitting where a model focuses on idiosyncratic features of the training data resulting in poor accuracy on future test data overfitting is avoided through cross validation where the original data set is divided into separate training and test sets for the purposes of fitting and evaluating the model respectively a more efficient approach is k fold cross validation stone 1977 kohavi 1995 where the data are randomly divided into k subsets or folds sequentially each fold is held out as a validation set while a model is fit to the remaining k 1 folds which serve as a training set the performance of the fitted model is assessed on the validation set and once this process is complete for all folds their performance metrics are aggregated the average coefficient of determination and mae on the k training sets is denoted by r train 2 and mae train and similarly the average performance on the k validation sets is denoted by r cv 2 and mae cv if a large number of models with many different hyperparameter configurations are evaluated with k fold cross validation overfitting may still occur rao and fung 2008 recommend sequestering a portion of the data as a test set which is only used after model optimization is complete to estimate the model s ability to generalize to future test cases for the gcm ensemble models the time series nature of the data must also be accounted for the goal is to use data from a given time period to build models which will generalize to future climate observations with these points in minds the following cross validation structure is adopted for the gcm ensemble models 1 training data all data from 1950 to 1989 2 sequestered test data all data from 1990 to 1999 3 model optimization stage all model optimization including routine fitting and hyperparameter tuning is performed using k fold cross validation on the training data with k equal to two or ten depending on the computational efficiency of the given learning algorithm 4 final assessment on training data once model optimization is complete a final model is fit to the training data for each algorithm and r cv 2 and mae cv are calculated using 10 fold cross validation so that comparisons between models can be made 5 assessment on test data the performance on the sequestered test set r test 2 and mae test are then used as estimates of generalization performance 2 3 arithmetic mean the simplest ensemble model uses the arithmetic mean of the gcm projections to predict the observed temperature precipitation y f x 1 p j 1 p x j this model has no parameters to optimize so it is incapable of learning from training data but it has been used for preliminary evaluation of drought characteristics in the same study region by venkataraman et al 2016a and it is included here as a comparison baseline 2 4 linear regression the linear regression model postulates the existence of a parameter vector β β 0 β p such that 2 5 y i β 0 j 1 p β j x ij i i 1 n where the i s are independent identically distributed disturbance terms with mean zero and constant variance σ 2 0 these model assumptions can be tested via diagnostic plots and hypothesis tests involving the residuals kutner et al 2005 and if they are satisfied the ordinary least squares estimator β is unbiased under the additional assumption of normality of the disturbance terms t tests and f tests can be employed to test linear hypotheses involving the parameter vector freedman 2009 once a reliable model is obtained predictions for y are produced as expected y f x β 0 j 1 p β j x j diagnostic plots and hypothesis tests can aid in determining whether higher order terms x j d or interaction terms x j 1 x j 2 should be included in the model regression models with many predictor variables also pose the problem of variable selection for which several solutions have been proposed including best subsets and stepwise regression increasing model complexity with these approaches runs the risk of overfitting as mentioned in section 2 2 which is exactly what occurred for the linear gcm ensembles adding additional regression terms and applying variable selection techniques decreased model performance as measured with r cv 2 and mae cv for both the temperature and precipitation ensembles therefore the final linear ensembles included all 34 gcm s with no higher order terms or interaction terms as displayed in eq 2 5 2 5 weighted k nearest neighbors as the name suggests this algorithm produces predictions for a new test case x r p by first identifying its nearest neighbors in the training data which requires a distance metric hechenbichler and schliep 2004 the minkowski distance of order q is d x 1 x 2 x 1 x 2 q j 1 p x 1 j x 2 j q 1 q which is simply euclidean distance when q 2 denote the k 1 nearest neighbors of x by x 1 x k 1 where the k 1 th neighbor is used to standardize distances as follows d i d x x i d x x k 1 this results in standardized distances satisfying 0 d i 1 which are transformed with a kernel function k to produce weights w i k d i the kernel function k is nonnegative attains its maximum at 0 and k u is nonstrictly decreasing as u increases hechenbichler and schliep 2016 have implemented weighted k nearest neighbors in their r package kknn which includes ten kernel functions for example the rectangular kernel k u 1 2 u 1 results in unweighted k nn and the biweight and triweight kernels are given below 2 6 k u 15 16 1 u 2 2 u 1 2 7 k u 35 32 1 u 2 3 u 1 once the weights w i are determined the estimate for y is the corresponding weighted average y f x i 1 k w i y i i 1 k w i to optimize the weighted k nn gcm ensembles r cv 2 values were computed for all ten kernel functions and for values of k 1 15 for the temperature ensemble k 10 and the triweight kernel 2 7 were optimal while for the precipitation ensemble the optimal choices were k 6 and the biweight kernel 2 6 2 6 artificial neural network the architecture used for the neural network ensemble ripley 1996 consists of three layers of neurons as displayed in fig 2 data for a new vector of gcm projections x x 1 x p enters the network at the input layer which is connected via weighted edges to a hidden layer h 1 h m and ultimately to the output layer where the prediction y is determined let w kj 1 be the weight of the edge connecting x j to h k and let w k 2 be the weight of the edge connecting h k to the output layer the output from the k th hidden neuron is then h k ϕ w k 0 1 j 1 p w kj 1 x j where ϕ is an activation function the activation function used by the r package nnet venables and ripley 2016 is the logistic function ϕ x 1 1 exp x the estimate for y is y w 0 2 k 1 m w k 2 h k there are a total of m p 1 m 1 unknown weight parameters which are estimated from training data using an application of gradient descent known as the error back propagation algorithm werbos 1974 the number of neurons in the hidden layer must be precisely tuned to avoid overfitting for the gcm temperature neural network fig 3 shows that r cv 2 is optimized with one hidden neuron and the same is true for the gcm precipitation neural network neural networks with one neuron in the hidden layer have performance comparable to linear models as confirmed in table 4 suggesting the need for further work based on a deep learning approach with multiple hidden layers schmidhuber 2015 2 7 random forest decision trees are built by successively partitioning the training data y i x i i 1 n based on the values of the independent variables when the independent variables x j are quantitative the splits can be binary as displayed in fig 4 or multiway splits can be used by binning the values of x j any given split partitions the data into several nodes and the impurity of each node is defined to be the sum of square deviations from the mean for the values of y in that node decision tree algorithms such as aid and cart morgan and sonquist 1963 breiman et al 1984 generate splits which minimize the sum of the impurity measures for the resulting nodes until a stopping criterion is met when presented with a new test case x the decision tree classifies it into one of the leaf nodes and the estimate y f x is determined by either averaging the values of y in that node or by using a linear model fit to that node random forests breiman 2001 liaw and wiener 2015 are constructed by first generating a sequence of independent and identically distributed random vectors θ 1 θ k which are used to build k decision trees f k f θ k the random forest then produces estimates for y by averaging the estimates of these decision trees y 1 k k 1 k f x θ k the randomness from the vectors θ 1 θ k influences growth of the trees in two ways the training data for each tree is a bootstrap sample random sample with replacement from the original training data and when each decision tree split is determined only a random set of n try p 3 features x j are considered as the number of trees tends to infinity the mean square error of a random forest converges to e y x y e θ f x θ 2 where e y x and e θ are expectations with respect to the probability distributions of y x and θ respectively breiman 2001 therefore random forests are robust against overfitting as the number of trees increases but one should ensure that a sufficient number of trees are being used to achieve high performance for example fig 5 shows the effect of increasing the number of trees on r cv 2 for the gcm temperature random forest and k 1000 trees is well beyond the point of diminishing returns the same is also true for the gcm precipitation random forest so k 1000 trees were used in the final random forest models for both temperature and precipitation 2 8 support vector regression in support vector regression svr vapnik 1998 1999 the vector of predictor variables x is mapped into a feature space via a potentially nonlinear mapping g r p r m and then linear regression in the feature space produces estimates for y y f x b k 1 m w k g k x the norm of the weight vector w is regarded as a measure of model complexity that should be minimized subject to certain prediction error constraints more specifically svr uses the ε insensitive loss function l ε y f x 0 if y f x ε y f x ε otherwise meaning that prediction errors less than or equal to ε are not penalized prediction errors in excess of ε for training data y i x i i 1 n are represented by slack variables ξ i ξ i and the penalty for these errors is controlled by a cost parameter c 0 more formally given fixed values ε c 0 the regression parameters b w 1 w m are estimated by solving the following minimization problem minimize 1 2 w 2 c i 1 n ξ i ξ i subject to y i f x i ε ξ i y i f x i ε ξ i ξ i ξ i 0 this optimization problem is solved using the method of lagrange multipliers and the solution to the dual problem is f x b i 1 n α i α i k x i x where the dual variables satisfy 0 α i α i c and k is a kernel function satisfying mercer s conditions 2 note that this is a different type of kernel function than the ones used for weighted k nearest neighbors 2 examples of kernel functions include the linear polynomial and radial basis kernels k x 1 x 2 x 1 x 2 k x 1 x 2 γ x 1 x 2 c d k x 1 x 2 exp γ x 1 x 2 2 which are implemented in the r package e1071 meyer et al 2017 optimizing an svr ensemble requires choosing a kernel function optimizing its hyperparameters e g γ c and d and optimizing the hyperparameters c and ε the e1071 package internally scales the data to have mean zero and standard deviation one so the recommended value for c cherkassky and ma 2004 is c max y 3 s y y 3 s y 3 the recommended value for ε is ε 3 σ ln n n where σ is the input noise level approximated by the standard deviation of the residuals in the linear model 2 5 to complete the svr optimization hyperparameters for the polynomial and radial basis kernels were tuned via a grid search with 2 fold cross validation on the training data optimal results for the temperature and precipitation ensembles are given in table 3 3 results six methods for constructing gcm temperature and precipitation ensembles including the optimization procedures used to arrive at a final model in each case were presented in 2 3 through 2 8 this section provides a comprehensive assessment of these models by comparing them to each other and to the individual gcms using overall performance metrics nonparametric statistical tests and temporal and spatial plots 3 1 overall assessment the performance metrics guiding model optimization are r cv 2 and mae cv computed using cross validation on the training data from 1950 1989 but the best estimates of generalization performance are r test 2 and mae test obtained from the sequestered test data from 1990 1999 table 4 because it would be unwieldy to present performance data for all 34 gcms the last three rows of this table include the worst performing gcm the gcm nearest median performance and the best performing gcm determined for temperature and precipitation separately table 4 can largely be summarized with four key remarks which are relevant to both the temperature and precipitation models 1 the best gcm ensembles were obtained using linear regression neural networks random forest and support vector regression with random forest achieving the best performance the mae test for random forest is 30 lower than the best gcm for temperature and 14 6 lower than the best gcm for precipitation compared to the arithmetic mean ensemble which has previously been used in this region by venkataraman et al 2016a random forest achieves a 21 8 lower mae test for temperature and an 8 2 lower mae test for precipitation 2 inspecting the values of r test 2 and mae test the ensembles mentioned in remark 1 have approximately the same performance while the other models presented in table 4 are noticeably worse this suggests tentatively dividing the models into two tiers tier 1 linear regression neural network random forest svr tier 2 arithmetic mean k nn and the individual gcms differences in model performance are analyzed in section 3 2 with the aid of nonparametric hypothesis tests one of the conclusions is that every tier 1 model outperforms every tier 2 model by a statistically significant margin for both temperature and precipitation 3 despite having the best cross validated performance on the training data weighted k nn had poor performance on the test data the k nn algorithm makes predictions for new test cases by computing weighted averages of nearby cases in the training data and it is likely that the test data from 1990 1999 were too different from the training data from 1950 1989 for the k nn algorithm to extrapolate properly this highlights the importance of using a sequestered test set in addition to cross validation and taking the time series structure into account if a sequestered test set had not been used or if it had been randomly selected from the data as a whole the performance of k nn would have been dramatically overstated 4 precipitation is much more difficult to predict than temperature overall remark 1 strongly supports using a gcm ensemble when making future projections especially in cases where determining the best individual gcm is difficult 3 2 nonparametric tests of model differences section 3 1 highlighted some differences in performance between the nine models displayed in table 4 which raises an important question is it likely that these differences could simply be the result of random chance or do we have statistically significant evidence that certain models are simulating temperature and precipitation better than others it is important to note that this is a separate question from the magnitude of performance differences given the relevance of factors such as the sample size and underlying variability of the data it is possible for a small difference to be statistically significant and it is possible for a large difference to be the result of random chance differences in performance for the temperature and precipitation models displayed in table 4 are systematically investigated using nonparametric statistical tests below consider a bootstrap sample from the test data i e a random sample with replacement from the test data of the same size denoted by y i x i i 1 n 11 640 and let f 1 f m be a list of models ensembles and or individual gcms using a bootstrap sample guarantees that for any model f m its absolute residuals e mi y i f m x i are statistically independent on the other hand for any observation y i x i the residuals e 1 i e mi for the m different models may be statistically dependent so quade s test quade 1979 trawinski et al 2012 is applied to test the null hypothesis h 0 for every observation y i x i each ranking of the absolute residuals e 1 i e mi is equally likely under this null hypothesis no model has a higher likelihood than any other of producing smaller absolute residuals so the performance of all m models are the same assuming this hypothesis is rejected post hoc pairwise tests comparing models f m 1 and f m 2 are performed with paired wilcoxon signed ranks tests wilcoxon 1945 and nemenyi s procedure nemenyi 1963 trawinski et al 2012 because m m 1 2 pairwise comparisons are made the adjusted p value apv for each test is apv m 1 m 2 min 1 p m 1 m 2 m m 1 2 the results of all hypothesis tests for the temperature and precipitation models are summarized below as there are six ensemble approaches and three gcm models best performing median performing and worst performing there are 36 pairs of comparisons possible 1 for the nine temperature models in table 4 quade s test rejected the null hypothesis p 10 15 and all 36 pairwise hypotheses were rejected with apvs less than 10 15 i e there is a highly statistically significant difference in model performance between each pair of temperature models presented in table 4 2 for the nine precipitation models quade s test also rejected the null hypothesis p 10 15 but the pairwise tests are more complicated rather than present results for 36 pairwise tests attention is restricted to the tier 1 and tier 2 models discussed in section 3 1 tier 1 linear regression neural network random forest svr tier 2 arithmetic mean k nn and the individual gcms 3 applied to the precipitation models in tier 1 none of the pairwise tests rejected the null hypothesis all apvs were greater than 0 58 4 for every tier 1 precipitation model f m 1 and every tier 2 precipitation model f m 2 the corresponding pairwise hypothesis was rejected apv m 1 m 2 10 15 by remarks 1 and 4 above every tier 1 model outperforms every tier 2 model by a statistically significant margin for both temperature and precipitation in particular for both temperature and precipitation the random forest ensemble significantly outperforms the arithmetic mean ensemble which has been applied previously in this study region venkataraman et al 2016a in the case of temperature simulation random forest has a statistically significant advantage over all other models now we consider how the tier 1 models compare to each other by remark 3 there is not a statistically significant difference in performance between any pair of tier 1 precipitation models among the tier 1 temperature models random forest has a statistically significant advantage but the difference in performance is small as measured by r test 2 and mae test moving forward we regard all the models in tier 1 as roughly equivalent with random forest serving as the best representative from this tier by a small margin random forest has the additional advantage of being robust to overfitting as discussed in section 2 7 3 3 model visualization to complete the model assessment we turn our attention to plots with respect to time longitude and latitude presented in figs 6 through 9 all of these plots were constructed solely using the test data from 1990 1999 recall that each observation y i x i is associated with an ordered triple of time longitude and latitude coordinates t i ϕ i λ i each time coordinate t in the test data occurs once for each of the 97 combinations of longitude and latitude in the study region and we define y t 1 97 t i t y i to be the average value of y for these observations the gcms and ensemble models specify predictions y i f x i for each observation and we similarly define y t 1 97 t i t y i plots of y t vs t and y t vs t for the temperature gcms and random forest are displayed on the left side of fig 6 moving from the weakest gcm to the best gcm and then to the random forest ensemble the correspondence between temperature predictions and observed temperature improves a clearer picture emerges by plotting deviations of each model from the observed temperature given by the negative residuals e t y t y t which are displayed on the right side of fig 6 this residual plot shows that fgoals g2 systematically underestimates temperature in the gulf basin for this time period while the high accuracy of the random forest is reflected by its smaller residuals the spatial distribution of r test 2 for the temperature gcms arithmetic mean ensemble and random forest are displayed in fig 7 once again random forest achieves substantially better performance than the individual gcms with values of r test 2 exceeding 0 9 at most pairs of longitude latitude coordinates this spatial plot also displays the advantage of using random forest over the arithmetic mean ensemble specifically in the southwest part of the study region plots of the precipitation gcms and random forest with respect to time are displayed in fig 8 the higher difficulty of predicting precipitation is understandable in light of its high interannual variability it s worth noting that the random forest appears to hedge against this variability by producing low standard deviation estimates s rf 0 871 compared to the standard deviation for the observed precipitation rate s y 1 91 in fact the standard deviation of mpi esm lr s mpilr 1 42 more closely matches the observed data despite having a higher mae than the random forest in summary caution is warranted when using a gcm precipitation ensemble for applications that are sensitive to variance such as estimating the likelihood of anomalous events an ensemble that has been specifically trained to detect anomalous events would be a better choice the spatial distribution of r test 2 for the arithmetic mean and random forest precipitation models is displayed in fig 9 3 as reported in table 4 the individual precipitation gcms have negative values of r test 2 so their spatial distributions are not included in fig 9 3 this plot confirms the advantage of random forest over the arithmetic mean precipitation ensemble and suggests that proximity to the gulf of mexico exacerbates the difficulty of predicting precipitation in the gulf basin region 4 discussion and conclusions gcms serve as a means of studying climate change and evaluating their performance in simulating hydroclimatic variables represents an essential first step in their application in natural resource planning and management in this study we have developed and compared the ability of various ensembles to simulate past climate in the gulf basin region of north america chosen as an illustrative example these ensembles were developed from temperature and precipitation outputs of 34 cmip5 gcms the major findings of this study are summarized and discussed here the best learning algorithm for modeling temperature in the gulf basin is random forest which outperformed all other ensembles and the individual gcms by statistically significant margins the advantage of rf over linear models neural networks and svr and its coefficient of determination r test 2 0 9385 are consistent with the findings of pang et al 2017 for statistical downscaling of temperature additional advantages of rf include its ease of deployment in terms of computational resources and hyperparameter optimization its robustness against overfitting and its consistency in the temporal and spatial domains figs 6 and 7 it must be noted that other approaches such as linear regression neural network and svr have been grouped together with the rf as tier 1 models and in the interest of brevity the temporal and spatial visualizations from these approaches have not been shown in particular the neural network ensemble developed here is characterized by one neuron in the hidden layer which resulted in its performance being no better than that of a linear regression model it is possible that ensembles built with deep learning approaches with multiple hidden layers perform better and the investigation of such methods is warranted as stated earlier several studies have pointed out the relatively poor ability of the cmip5 gcms in simulating precipitation particularly at regional scales not surprisingly we find that none of the gcm models on their own or as part of one of the ensembles developed herein are able to capture the spatio temporal characteristics of precipitation in the study region interplay of a number of factors is likely responsible for this anomaly firstly the marked interannual variability in precipitation in the region as noted by various authors such as gutzler and nims 2005 norwine and john 2007 wong et al 2015 trammell et al 2016 etc appears to be a challenge for the gcms and ensembles to capture only the mpi lr model comes close to reproducing the standard deviation of the observed data however if other metrics such as r 2 are considered the rf marginally outperforms all other approaches albeit with a low r 2 itself secondly it is evident from fig 9 that rf performs poorly in the eastern area of the study region which is known to receive moisture surplus due to its proximity to the gulf of mexico in fact jiang and yang 2012 as well as venkataraman et al 2016a report that while the rest of the state of texas is projected to get drier toward the end of the 21st century due to climate change effects this particular region remains largely unaffected due to the gulf of mexico moisture effects the nature of these mechanisms appear to be poorly modeled by the cmip5 these observations highlight the critical need for applying an appropriate set of metrics for evaluating model performance that consider their spatial nature as well it is also imperative to understand that shortcomings in algorithms used in ensemble development don t necessarily reflect on the approach itself but may rather expose errors in the gcm outputs lastly we must also consider the nature of the data used in the study in explaining our findings we have used 1 1 regridded temperature and precipitation outputs of 34 gcms in our study the data has been neither bias corrected nor statistically downscaled considering that the purpose of our study is not climate impact assessment but rather climate model consensus and ensemble development overall our findings are consistent with other studies such as eccel et al 2007 and pang et al 2017 who conclude that rf models marginally outperform alternative approaches for statistical downscaling additionally we conclude that in regional studies involving complex and diverse mechanisms that drive precipitation a robust testing and validation approach should be employed in developing ensembles conflict of interest none declared acknowledgments we acknowledge the world climate research programme s working group on coupled modeling which is responsible for cmip and we thank the climate modeling groups listed in tables 1 and 2 for producing and making available their model output for cmip the u s department of energy s program for climate model diagnosis and intercomparison provides coordinating support and led development of software infrastructure in partnership with the global organization for earth system science portals we also acknowledge the college of science and technology at tarleton state university for financial support of ms juliann booth and ms nina culver during their tenure as ms students in the department of mathematics furthermore we acknowledge ms culver s contributions during the preliminary phase of the study 
6805,projections of future changes in hydroclimatic variables are available through various general circulation models gcms that are part of the coupled model intercomparison project phase 5 cmip5 assessment of the performance of these models in simulating past climate both individually and as an ensemble has received much attention and is commonly the first step in assessing their suitability of application in this study we have evaluated the ability of various ensemble models to simulate past temperature and precipitation in the gulf basin region of north america chosen as an illustrative case study we have developed ensembles from 34 cmip5 gcms using six diverse approaches including random forest support vector regression neural networks linear regression and weighted k nearest neighbors and compared the performance of the ensembles with each other and the individual gcms using a robust set of metrics and nonparametric tests for temperature random forest outperforms all other ensembles and the best performing gcm by a statistically significant margin and is able to simulate temporal and spatial patterns in temperature well none of the ensembles are able to adequately simulate observed precipitation patterns in the study area likely due to spatial differences in precipitation drivers in the region as well as the coarseness of the dataset itself however the random forest support vector regression neural network and linear regression ensembles achieved statistically significant improvements to precipitation simulation as compared to the individual gcms and the simple arithmetic mean ensemble which has been used in several studies keywords cmip5 climate model ensemble random forest support vector regression k nearest neighbours gulf basin region of united states 1 introduction projections of variability in climate variables such as temperature and precipitation have recently been published through the coupled model intercomparison project phase 5 cmip5 taylor et al 2012 outputs from over 50 models representing the efforts of 20 institutions around the world are available as part of this dataset the cmip5 includes near term 10 30 year scale and long term century scale experiments which are integrated using atmospheric ocean general circulation models aogcms additionally the cmip5 builds on previous efforts through the inclusion of earth system components es that go beyond aogcms by incorporating biogeochemical feedbacks within the climate system stouffer et al 2011 and by using models of a higher spatial resolution climate projections from this multimodel dataset have been widely used in modeling the impacts of future change on biological ecological hydrological and socio economic systems at various spatial scales e g bopp et al 2013 su et al 2013 touma et al 2015 selvanathan et al 2016 etc as such there is more confidence about the reliability of these models at the continental scales than at the regional scales uncertainties about their suitability at these smaller spatial scales still exist and raise concerns about their use for long term planning one of the critical questions being asked in water resource planning is what models perform well in simulating precipitation and temperature for a given region current practice is to use the ability of a model to simulate historical observations for a particular region as a measure of its reliability it is understood and accepted that this may not be reflective of the model s capacity to produce reliable future forecasts nonetheless it is widely acknowledged e g palmer et al 2005 jacob et al 2007 semenov and stratonovitch 2010 that the ability of gcms to simulate past patterns of climate variability increases our confidence in them several recent studies e g johnson et al 2011 jiang and yang 2012 sperber et al 2013 ramesh and goswami 2014 venkataraman et al 2016a raju et al 2017 have evaluated the performance of individual models from the cmip5 and or its predecessor the cmip3 using a variety of metrics the general consensus among these studies is that a gcms are better at simulating temperature than precipitation b no single model is able to represent trends on a spatially consistent basis and c the multimodel ensemble mean weighted or unweighted is superior to any individual model while the use of the multimodel ensemble approach has long been advocated in the climate modeling community numerous techniques have evolved to prescribe the ideal combination of models or subsets thereof to produce deterministic or probabilistic forecasts these techniques can be as simple as linear combinations of models where all models are assigned equal weight or involve linear regression to determine the weights e g krishnamurti et al 1999 2000 kharin and zweirs 2002 the inadequacies of such approaches have been pointed out by raftery et al 2005 duan et al 2007 and sansom et al 2013 other methods such as bayesian model averaging bma produce probabilistic forecasts and have been shown to improve model reliability and have been applied in various hydroclimatological studies sloughter et al 2007 duan et al 2007 duan and phillips 2010 najafi et al 2011 dechant and moradkhani 2014 najafi and moradkhani 2015 giorgi and mearns 2002 have developed a reliability ensemble averaging rea technique to jointly assess the ability of individual models within an ensemble to capture observations and the level of agreement within this ensemble for projected future change the motivation behind this approach was to minimize the contribution of poorly performing models within an ensemble building on this framework smith et al 2009 showed that despite disagreements among gcms useful information on future climate changes can be derived on a probabilistic scale machine learning algorithms such as artificial neural networks ann have also been employed in meteorological time series forecasting shrivastava et al 2012 provide a comprehensive review of the use of ann and conclude that these tools are best suited for smaller scale applications non parametric methods such as k nearest neighbours k nn have also been widely used in simulating hydroclimatic variables by re arranging historical data to arrive at new weather series e g young 1994 lall and sharma 1996 yates et al 2003 sharif and burn 2007 demonstrate that the k nn approach can be used not only to simulate observed records but to also generate future weather predictions the application of data mining techniques such as decision tree analysis in climate model assessment is still at its infancy in particular methods such as random forests henceforth rf breiman 2001 that have gained popularity in ecological classification e g guisan and thuiller 2005 prasad et al 2006 cutler et al 2007 and image processing e g ham et al 2005 plaza et al 2009 are still a relatively new entry to the field of climate studies specifically in the context of ensemble development eccel et al 2007 compared various linear and non linear approaches to predict minimum temperatures based on the output of numerical weather prediction models and concluded that the rf models yielded the best result albeit with marginal improvement over other multivariate approaches gaal et al 2012 modeled the impact of climate change on wine producing regions in hungary using rf as the technique allowed them to analyse the importance of various variables on identifying regions suitable for grapevine cultivation recently pang et al 2017 evaluated the applicability of rf to statistical downscaling of temperature in the pearl river basin of china they found that the rf model outperformed the ann and support vector machine svm models in simulating daily mean temperatures in the region rf was also used for downscaling land surface temperature by huetengs and vohland 2016 in general these studies emphasise the superiority of rf models over other approaches due to its inherent ability to assess the importance of each input variable in the eventual prediction as such the approaches outlined above can be classified as linear e g generalized linear model and nonlinear e g k nearest neighbors neural networks random forest etc knutti et al 2010 report that there is no universally accepted method for building ensemble models and highlight the need for using a robust set of metrics for evaluating models as well as effectively communicating the uncertainties therein to stakeholders the objective of the present study is therefore to compare a diverse set of approaches for building model ensembles in the context of simulating historical climate observations using an appropriate set of metrics specifically we compare the performance of each of these approaches with the best performing model of the cmip5 suite simulation of near surface air temperature henceforth referred to as simply temperature and precipitation rate in the gulf basin region of the united states for the time period 1950 to 1999 have been presented as an illustrative case study the same modeling approach can be used in other geographical regions as well there are some notable novel aspects and improvements of this study compared with previous work first to the best of our knowledge there has been no reported use of rf in climate model ensemble development and our study represents the first of this kind secondly we have adopted a highly rigorous statistical approach to comparing on a pair wise basis the performance of the aforementioned techniques against each other as well as against the best performing individual model in the cmip5 suite venkataraman et al 2016a presented the first look at the performance of cmip5 gcms in simulating hydroclimatic variables in this region using a simple arithmetic mean ensemble wherein each gcm was assigned the same weight they suggested that while this approach was sufficient for the purposes of studying future drought trends in the region more rigorous gcm combination approaches are needed to arrive at a consensus among model projections we present the development and analysis of such approaches in this study one of the principle findings is a statistically significant advantage in performance for the random forest linear regression support vector regression and neural network ensembles as compared to the individual gcms and the arithmetic mean ensemble in the case of temperature simulation random forest outperforms all other models considered by a statistically significant margin and given its robustness against overfitting we recommend it as the best ensemble model within the scope of the present study the structure of the paper is as follows section 2 presents a description of the study region and data under consideration performance metrics and cross validation procedures and methods for optimizing ensemble models built using six machine learning algorithms these ensemble models are evaluated in section 3 in terms of overall performance metrics nonparametric statistical tests and plots with respect to time longitude and latitude a discussion of our findings including the comparison of various approaches is presented in section 4 2 methodology 2 1 study region and data overview as shown in fig 1 the study region is largely comprised of the state of texas in general precipitation decreases from the sub humid eastern boundary of the study region which receives an annual average precipitation of 1500 mm to the semi arid western boundary which receives less than 500 mm annually the region is marked by large inter annual variability in precipitation with frequent droughts and floods which has led many climatologists to label the region s climate problematic norwine and john 2007 nielsen gammon 2011 peterson et al 2012 several earlier studies such as jiang and yang 2012 and langford et al 2014 that have used either the cmip3 or cmip5 models have already noted the inadequacy of these models in simulating the spatio temporal variation of precipitation in this region furthermore they attest to the challenging nature of capturing the mechanisms responsible for precipitation trends in the region on the other hand the ability of climate models to simulate historical temperature characteristics in the region has been well documented hernandez and uddameri 2014 venkataraman et al 2016b the data set under consideration contains monthly measurements of average surface air temperature c and precipitation rate mm day from 1950 to 1999 at 97 grid points in the gulf basin region with longitudes ranging from 103 5 to 93 5 and latitudes ranging from 25 5 to 34 5 as displayed in fig 1 therefore there are n 50 12 97 58 200 distinct ordered triples of time longitude and latitude coordinates denoted by t i ϕ i λ i i 1 n for each of the learning algorithms described in sections 2 3 through 2 8 two ensemble models are developed in the first model the dependent variable y i is the average surface air temperature at t i ϕ i λ i which is modeled as a function of x i x i 1 x ip the corresponding column vector of one degree regridded temperature predictions provided by p 34 cmip5 gcms reclamation 2013 the names and sources of these models are summarized in tables 1 and 2 the second ensemble model is the same except the observed precipitation rate is modeled in terms of the one degree regridded gcm projections for precipitation for the same locations 1 degree observed data for temperature and precipitation was compiled from the same source for training and validation purposes 2 2 performance metrics and cross validation procedures an essential part of each ensemble model is the ability to obtain predictions y f x for temperature precipitation based on the gcm projections given a training sample y i x i i 1 n the residuals are e i y i y i which yield the rmse normalized rmse the coefficient of determination r 2 and the mean absolute error 2 1 rmse 1 n i 1 n e i 2 2 2 nrmse rmse s y 2 3 r 2 1 i 1 n e i 2 i 1 n y i y 2 1 n n 1 nrmse 2 2 4 mae 1 n i 1 n e i eqs 2 2 and 2 3 show that nrmse and r 2 are monotonic functions of rmse so the first three performance metrics are essentially equivalent 1 the coefficient of determination r 2 is also known as the nash sutcliffe efficiency coefficient nash and sutcliffe 1970 1 the mae is an important alternative to these metrics willmott and matsuura 2005 and for completeness both r 2 and mae are displayed in table 4 for the temperature and precipitation ensembles note that negative values of r 2 can occur if a model s performance is sufficiently poor most learning algorithms have hyperparameters that can be modified to alter model performance such as the number of neighbors in k nn or the number of neurons in a neural network it may be tempting to increase the model s complexity until it achieves nearly perfect performance on the training data unfortunately this leads to the problem of overfitting where a model focuses on idiosyncratic features of the training data resulting in poor accuracy on future test data overfitting is avoided through cross validation where the original data set is divided into separate training and test sets for the purposes of fitting and evaluating the model respectively a more efficient approach is k fold cross validation stone 1977 kohavi 1995 where the data are randomly divided into k subsets or folds sequentially each fold is held out as a validation set while a model is fit to the remaining k 1 folds which serve as a training set the performance of the fitted model is assessed on the validation set and once this process is complete for all folds their performance metrics are aggregated the average coefficient of determination and mae on the k training sets is denoted by r train 2 and mae train and similarly the average performance on the k validation sets is denoted by r cv 2 and mae cv if a large number of models with many different hyperparameter configurations are evaluated with k fold cross validation overfitting may still occur rao and fung 2008 recommend sequestering a portion of the data as a test set which is only used after model optimization is complete to estimate the model s ability to generalize to future test cases for the gcm ensemble models the time series nature of the data must also be accounted for the goal is to use data from a given time period to build models which will generalize to future climate observations with these points in minds the following cross validation structure is adopted for the gcm ensemble models 1 training data all data from 1950 to 1989 2 sequestered test data all data from 1990 to 1999 3 model optimization stage all model optimization including routine fitting and hyperparameter tuning is performed using k fold cross validation on the training data with k equal to two or ten depending on the computational efficiency of the given learning algorithm 4 final assessment on training data once model optimization is complete a final model is fit to the training data for each algorithm and r cv 2 and mae cv are calculated using 10 fold cross validation so that comparisons between models can be made 5 assessment on test data the performance on the sequestered test set r test 2 and mae test are then used as estimates of generalization performance 2 3 arithmetic mean the simplest ensemble model uses the arithmetic mean of the gcm projections to predict the observed temperature precipitation y f x 1 p j 1 p x j this model has no parameters to optimize so it is incapable of learning from training data but it has been used for preliminary evaluation of drought characteristics in the same study region by venkataraman et al 2016a and it is included here as a comparison baseline 2 4 linear regression the linear regression model postulates the existence of a parameter vector β β 0 β p such that 2 5 y i β 0 j 1 p β j x ij i i 1 n where the i s are independent identically distributed disturbance terms with mean zero and constant variance σ 2 0 these model assumptions can be tested via diagnostic plots and hypothesis tests involving the residuals kutner et al 2005 and if they are satisfied the ordinary least squares estimator β is unbiased under the additional assumption of normality of the disturbance terms t tests and f tests can be employed to test linear hypotheses involving the parameter vector freedman 2009 once a reliable model is obtained predictions for y are produced as expected y f x β 0 j 1 p β j x j diagnostic plots and hypothesis tests can aid in determining whether higher order terms x j d or interaction terms x j 1 x j 2 should be included in the model regression models with many predictor variables also pose the problem of variable selection for which several solutions have been proposed including best subsets and stepwise regression increasing model complexity with these approaches runs the risk of overfitting as mentioned in section 2 2 which is exactly what occurred for the linear gcm ensembles adding additional regression terms and applying variable selection techniques decreased model performance as measured with r cv 2 and mae cv for both the temperature and precipitation ensembles therefore the final linear ensembles included all 34 gcm s with no higher order terms or interaction terms as displayed in eq 2 5 2 5 weighted k nearest neighbors as the name suggests this algorithm produces predictions for a new test case x r p by first identifying its nearest neighbors in the training data which requires a distance metric hechenbichler and schliep 2004 the minkowski distance of order q is d x 1 x 2 x 1 x 2 q j 1 p x 1 j x 2 j q 1 q which is simply euclidean distance when q 2 denote the k 1 nearest neighbors of x by x 1 x k 1 where the k 1 th neighbor is used to standardize distances as follows d i d x x i d x x k 1 this results in standardized distances satisfying 0 d i 1 which are transformed with a kernel function k to produce weights w i k d i the kernel function k is nonnegative attains its maximum at 0 and k u is nonstrictly decreasing as u increases hechenbichler and schliep 2016 have implemented weighted k nearest neighbors in their r package kknn which includes ten kernel functions for example the rectangular kernel k u 1 2 u 1 results in unweighted k nn and the biweight and triweight kernels are given below 2 6 k u 15 16 1 u 2 2 u 1 2 7 k u 35 32 1 u 2 3 u 1 once the weights w i are determined the estimate for y is the corresponding weighted average y f x i 1 k w i y i i 1 k w i to optimize the weighted k nn gcm ensembles r cv 2 values were computed for all ten kernel functions and for values of k 1 15 for the temperature ensemble k 10 and the triweight kernel 2 7 were optimal while for the precipitation ensemble the optimal choices were k 6 and the biweight kernel 2 6 2 6 artificial neural network the architecture used for the neural network ensemble ripley 1996 consists of three layers of neurons as displayed in fig 2 data for a new vector of gcm projections x x 1 x p enters the network at the input layer which is connected via weighted edges to a hidden layer h 1 h m and ultimately to the output layer where the prediction y is determined let w kj 1 be the weight of the edge connecting x j to h k and let w k 2 be the weight of the edge connecting h k to the output layer the output from the k th hidden neuron is then h k ϕ w k 0 1 j 1 p w kj 1 x j where ϕ is an activation function the activation function used by the r package nnet venables and ripley 2016 is the logistic function ϕ x 1 1 exp x the estimate for y is y w 0 2 k 1 m w k 2 h k there are a total of m p 1 m 1 unknown weight parameters which are estimated from training data using an application of gradient descent known as the error back propagation algorithm werbos 1974 the number of neurons in the hidden layer must be precisely tuned to avoid overfitting for the gcm temperature neural network fig 3 shows that r cv 2 is optimized with one hidden neuron and the same is true for the gcm precipitation neural network neural networks with one neuron in the hidden layer have performance comparable to linear models as confirmed in table 4 suggesting the need for further work based on a deep learning approach with multiple hidden layers schmidhuber 2015 2 7 random forest decision trees are built by successively partitioning the training data y i x i i 1 n based on the values of the independent variables when the independent variables x j are quantitative the splits can be binary as displayed in fig 4 or multiway splits can be used by binning the values of x j any given split partitions the data into several nodes and the impurity of each node is defined to be the sum of square deviations from the mean for the values of y in that node decision tree algorithms such as aid and cart morgan and sonquist 1963 breiman et al 1984 generate splits which minimize the sum of the impurity measures for the resulting nodes until a stopping criterion is met when presented with a new test case x the decision tree classifies it into one of the leaf nodes and the estimate y f x is determined by either averaging the values of y in that node or by using a linear model fit to that node random forests breiman 2001 liaw and wiener 2015 are constructed by first generating a sequence of independent and identically distributed random vectors θ 1 θ k which are used to build k decision trees f k f θ k the random forest then produces estimates for y by averaging the estimates of these decision trees y 1 k k 1 k f x θ k the randomness from the vectors θ 1 θ k influences growth of the trees in two ways the training data for each tree is a bootstrap sample random sample with replacement from the original training data and when each decision tree split is determined only a random set of n try p 3 features x j are considered as the number of trees tends to infinity the mean square error of a random forest converges to e y x y e θ f x θ 2 where e y x and e θ are expectations with respect to the probability distributions of y x and θ respectively breiman 2001 therefore random forests are robust against overfitting as the number of trees increases but one should ensure that a sufficient number of trees are being used to achieve high performance for example fig 5 shows the effect of increasing the number of trees on r cv 2 for the gcm temperature random forest and k 1000 trees is well beyond the point of diminishing returns the same is also true for the gcm precipitation random forest so k 1000 trees were used in the final random forest models for both temperature and precipitation 2 8 support vector regression in support vector regression svr vapnik 1998 1999 the vector of predictor variables x is mapped into a feature space via a potentially nonlinear mapping g r p r m and then linear regression in the feature space produces estimates for y y f x b k 1 m w k g k x the norm of the weight vector w is regarded as a measure of model complexity that should be minimized subject to certain prediction error constraints more specifically svr uses the ε insensitive loss function l ε y f x 0 if y f x ε y f x ε otherwise meaning that prediction errors less than or equal to ε are not penalized prediction errors in excess of ε for training data y i x i i 1 n are represented by slack variables ξ i ξ i and the penalty for these errors is controlled by a cost parameter c 0 more formally given fixed values ε c 0 the regression parameters b w 1 w m are estimated by solving the following minimization problem minimize 1 2 w 2 c i 1 n ξ i ξ i subject to y i f x i ε ξ i y i f x i ε ξ i ξ i ξ i 0 this optimization problem is solved using the method of lagrange multipliers and the solution to the dual problem is f x b i 1 n α i α i k x i x where the dual variables satisfy 0 α i α i c and k is a kernel function satisfying mercer s conditions 2 note that this is a different type of kernel function than the ones used for weighted k nearest neighbors 2 examples of kernel functions include the linear polynomial and radial basis kernels k x 1 x 2 x 1 x 2 k x 1 x 2 γ x 1 x 2 c d k x 1 x 2 exp γ x 1 x 2 2 which are implemented in the r package e1071 meyer et al 2017 optimizing an svr ensemble requires choosing a kernel function optimizing its hyperparameters e g γ c and d and optimizing the hyperparameters c and ε the e1071 package internally scales the data to have mean zero and standard deviation one so the recommended value for c cherkassky and ma 2004 is c max y 3 s y y 3 s y 3 the recommended value for ε is ε 3 σ ln n n where σ is the input noise level approximated by the standard deviation of the residuals in the linear model 2 5 to complete the svr optimization hyperparameters for the polynomial and radial basis kernels were tuned via a grid search with 2 fold cross validation on the training data optimal results for the temperature and precipitation ensembles are given in table 3 3 results six methods for constructing gcm temperature and precipitation ensembles including the optimization procedures used to arrive at a final model in each case were presented in 2 3 through 2 8 this section provides a comprehensive assessment of these models by comparing them to each other and to the individual gcms using overall performance metrics nonparametric statistical tests and temporal and spatial plots 3 1 overall assessment the performance metrics guiding model optimization are r cv 2 and mae cv computed using cross validation on the training data from 1950 1989 but the best estimates of generalization performance are r test 2 and mae test obtained from the sequestered test data from 1990 1999 table 4 because it would be unwieldy to present performance data for all 34 gcms the last three rows of this table include the worst performing gcm the gcm nearest median performance and the best performing gcm determined for temperature and precipitation separately table 4 can largely be summarized with four key remarks which are relevant to both the temperature and precipitation models 1 the best gcm ensembles were obtained using linear regression neural networks random forest and support vector regression with random forest achieving the best performance the mae test for random forest is 30 lower than the best gcm for temperature and 14 6 lower than the best gcm for precipitation compared to the arithmetic mean ensemble which has previously been used in this region by venkataraman et al 2016a random forest achieves a 21 8 lower mae test for temperature and an 8 2 lower mae test for precipitation 2 inspecting the values of r test 2 and mae test the ensembles mentioned in remark 1 have approximately the same performance while the other models presented in table 4 are noticeably worse this suggests tentatively dividing the models into two tiers tier 1 linear regression neural network random forest svr tier 2 arithmetic mean k nn and the individual gcms differences in model performance are analyzed in section 3 2 with the aid of nonparametric hypothesis tests one of the conclusions is that every tier 1 model outperforms every tier 2 model by a statistically significant margin for both temperature and precipitation 3 despite having the best cross validated performance on the training data weighted k nn had poor performance on the test data the k nn algorithm makes predictions for new test cases by computing weighted averages of nearby cases in the training data and it is likely that the test data from 1990 1999 were too different from the training data from 1950 1989 for the k nn algorithm to extrapolate properly this highlights the importance of using a sequestered test set in addition to cross validation and taking the time series structure into account if a sequestered test set had not been used or if it had been randomly selected from the data as a whole the performance of k nn would have been dramatically overstated 4 precipitation is much more difficult to predict than temperature overall remark 1 strongly supports using a gcm ensemble when making future projections especially in cases where determining the best individual gcm is difficult 3 2 nonparametric tests of model differences section 3 1 highlighted some differences in performance between the nine models displayed in table 4 which raises an important question is it likely that these differences could simply be the result of random chance or do we have statistically significant evidence that certain models are simulating temperature and precipitation better than others it is important to note that this is a separate question from the magnitude of performance differences given the relevance of factors such as the sample size and underlying variability of the data it is possible for a small difference to be statistically significant and it is possible for a large difference to be the result of random chance differences in performance for the temperature and precipitation models displayed in table 4 are systematically investigated using nonparametric statistical tests below consider a bootstrap sample from the test data i e a random sample with replacement from the test data of the same size denoted by y i x i i 1 n 11 640 and let f 1 f m be a list of models ensembles and or individual gcms using a bootstrap sample guarantees that for any model f m its absolute residuals e mi y i f m x i are statistically independent on the other hand for any observation y i x i the residuals e 1 i e mi for the m different models may be statistically dependent so quade s test quade 1979 trawinski et al 2012 is applied to test the null hypothesis h 0 for every observation y i x i each ranking of the absolute residuals e 1 i e mi is equally likely under this null hypothesis no model has a higher likelihood than any other of producing smaller absolute residuals so the performance of all m models are the same assuming this hypothesis is rejected post hoc pairwise tests comparing models f m 1 and f m 2 are performed with paired wilcoxon signed ranks tests wilcoxon 1945 and nemenyi s procedure nemenyi 1963 trawinski et al 2012 because m m 1 2 pairwise comparisons are made the adjusted p value apv for each test is apv m 1 m 2 min 1 p m 1 m 2 m m 1 2 the results of all hypothesis tests for the temperature and precipitation models are summarized below as there are six ensemble approaches and three gcm models best performing median performing and worst performing there are 36 pairs of comparisons possible 1 for the nine temperature models in table 4 quade s test rejected the null hypothesis p 10 15 and all 36 pairwise hypotheses were rejected with apvs less than 10 15 i e there is a highly statistically significant difference in model performance between each pair of temperature models presented in table 4 2 for the nine precipitation models quade s test also rejected the null hypothesis p 10 15 but the pairwise tests are more complicated rather than present results for 36 pairwise tests attention is restricted to the tier 1 and tier 2 models discussed in section 3 1 tier 1 linear regression neural network random forest svr tier 2 arithmetic mean k nn and the individual gcms 3 applied to the precipitation models in tier 1 none of the pairwise tests rejected the null hypothesis all apvs were greater than 0 58 4 for every tier 1 precipitation model f m 1 and every tier 2 precipitation model f m 2 the corresponding pairwise hypothesis was rejected apv m 1 m 2 10 15 by remarks 1 and 4 above every tier 1 model outperforms every tier 2 model by a statistically significant margin for both temperature and precipitation in particular for both temperature and precipitation the random forest ensemble significantly outperforms the arithmetic mean ensemble which has been applied previously in this study region venkataraman et al 2016a in the case of temperature simulation random forest has a statistically significant advantage over all other models now we consider how the tier 1 models compare to each other by remark 3 there is not a statistically significant difference in performance between any pair of tier 1 precipitation models among the tier 1 temperature models random forest has a statistically significant advantage but the difference in performance is small as measured by r test 2 and mae test moving forward we regard all the models in tier 1 as roughly equivalent with random forest serving as the best representative from this tier by a small margin random forest has the additional advantage of being robust to overfitting as discussed in section 2 7 3 3 model visualization to complete the model assessment we turn our attention to plots with respect to time longitude and latitude presented in figs 6 through 9 all of these plots were constructed solely using the test data from 1990 1999 recall that each observation y i x i is associated with an ordered triple of time longitude and latitude coordinates t i ϕ i λ i each time coordinate t in the test data occurs once for each of the 97 combinations of longitude and latitude in the study region and we define y t 1 97 t i t y i to be the average value of y for these observations the gcms and ensemble models specify predictions y i f x i for each observation and we similarly define y t 1 97 t i t y i plots of y t vs t and y t vs t for the temperature gcms and random forest are displayed on the left side of fig 6 moving from the weakest gcm to the best gcm and then to the random forest ensemble the correspondence between temperature predictions and observed temperature improves a clearer picture emerges by plotting deviations of each model from the observed temperature given by the negative residuals e t y t y t which are displayed on the right side of fig 6 this residual plot shows that fgoals g2 systematically underestimates temperature in the gulf basin for this time period while the high accuracy of the random forest is reflected by its smaller residuals the spatial distribution of r test 2 for the temperature gcms arithmetic mean ensemble and random forest are displayed in fig 7 once again random forest achieves substantially better performance than the individual gcms with values of r test 2 exceeding 0 9 at most pairs of longitude latitude coordinates this spatial plot also displays the advantage of using random forest over the arithmetic mean ensemble specifically in the southwest part of the study region plots of the precipitation gcms and random forest with respect to time are displayed in fig 8 the higher difficulty of predicting precipitation is understandable in light of its high interannual variability it s worth noting that the random forest appears to hedge against this variability by producing low standard deviation estimates s rf 0 871 compared to the standard deviation for the observed precipitation rate s y 1 91 in fact the standard deviation of mpi esm lr s mpilr 1 42 more closely matches the observed data despite having a higher mae than the random forest in summary caution is warranted when using a gcm precipitation ensemble for applications that are sensitive to variance such as estimating the likelihood of anomalous events an ensemble that has been specifically trained to detect anomalous events would be a better choice the spatial distribution of r test 2 for the arithmetic mean and random forest precipitation models is displayed in fig 9 3 as reported in table 4 the individual precipitation gcms have negative values of r test 2 so their spatial distributions are not included in fig 9 3 this plot confirms the advantage of random forest over the arithmetic mean precipitation ensemble and suggests that proximity to the gulf of mexico exacerbates the difficulty of predicting precipitation in the gulf basin region 4 discussion and conclusions gcms serve as a means of studying climate change and evaluating their performance in simulating hydroclimatic variables represents an essential first step in their application in natural resource planning and management in this study we have developed and compared the ability of various ensembles to simulate past climate in the gulf basin region of north america chosen as an illustrative example these ensembles were developed from temperature and precipitation outputs of 34 cmip5 gcms the major findings of this study are summarized and discussed here the best learning algorithm for modeling temperature in the gulf basin is random forest which outperformed all other ensembles and the individual gcms by statistically significant margins the advantage of rf over linear models neural networks and svr and its coefficient of determination r test 2 0 9385 are consistent with the findings of pang et al 2017 for statistical downscaling of temperature additional advantages of rf include its ease of deployment in terms of computational resources and hyperparameter optimization its robustness against overfitting and its consistency in the temporal and spatial domains figs 6 and 7 it must be noted that other approaches such as linear regression neural network and svr have been grouped together with the rf as tier 1 models and in the interest of brevity the temporal and spatial visualizations from these approaches have not been shown in particular the neural network ensemble developed here is characterized by one neuron in the hidden layer which resulted in its performance being no better than that of a linear regression model it is possible that ensembles built with deep learning approaches with multiple hidden layers perform better and the investigation of such methods is warranted as stated earlier several studies have pointed out the relatively poor ability of the cmip5 gcms in simulating precipitation particularly at regional scales not surprisingly we find that none of the gcm models on their own or as part of one of the ensembles developed herein are able to capture the spatio temporal characteristics of precipitation in the study region interplay of a number of factors is likely responsible for this anomaly firstly the marked interannual variability in precipitation in the region as noted by various authors such as gutzler and nims 2005 norwine and john 2007 wong et al 2015 trammell et al 2016 etc appears to be a challenge for the gcms and ensembles to capture only the mpi lr model comes close to reproducing the standard deviation of the observed data however if other metrics such as r 2 are considered the rf marginally outperforms all other approaches albeit with a low r 2 itself secondly it is evident from fig 9 that rf performs poorly in the eastern area of the study region which is known to receive moisture surplus due to its proximity to the gulf of mexico in fact jiang and yang 2012 as well as venkataraman et al 2016a report that while the rest of the state of texas is projected to get drier toward the end of the 21st century due to climate change effects this particular region remains largely unaffected due to the gulf of mexico moisture effects the nature of these mechanisms appear to be poorly modeled by the cmip5 these observations highlight the critical need for applying an appropriate set of metrics for evaluating model performance that consider their spatial nature as well it is also imperative to understand that shortcomings in algorithms used in ensemble development don t necessarily reflect on the approach itself but may rather expose errors in the gcm outputs lastly we must also consider the nature of the data used in the study in explaining our findings we have used 1 1 regridded temperature and precipitation outputs of 34 gcms in our study the data has been neither bias corrected nor statistically downscaled considering that the purpose of our study is not climate impact assessment but rather climate model consensus and ensemble development overall our findings are consistent with other studies such as eccel et al 2007 and pang et al 2017 who conclude that rf models marginally outperform alternative approaches for statistical downscaling additionally we conclude that in regional studies involving complex and diverse mechanisms that drive precipitation a robust testing and validation approach should be employed in developing ensembles conflict of interest none declared acknowledgments we acknowledge the world climate research programme s working group on coupled modeling which is responsible for cmip and we thank the climate modeling groups listed in tables 1 and 2 for producing and making available their model output for cmip the u s department of energy s program for climate model diagnosis and intercomparison provides coordinating support and led development of software infrastructure in partnership with the global organization for earth system science portals we also acknowledge the college of science and technology at tarleton state university for financial support of ms juliann booth and ms nina culver during their tenure as ms students in the department of mathematics furthermore we acknowledge ms culver s contributions during the preliminary phase of the study 
6806,assessing the sensitivity of groundwater systems to hydroclimate variability is critical to sustainable management of the water resources of guam us territory we assess spatial and temporal variability of isotopic and geochemical compositions of vadose and phreatic groundwater sampled from cave drip sites and production wells respectively to better understand the vulnerability of the freshwater lens on guam to variability in hydroclimate we independently evaluate the existing conceptual model of the northern guam lens aquifer that is largely based on physical as opposed to geochemical observations sampling was conducted from 2008 to 2015 over which rainfall gradually increased major ion geochemistry and sr isotope values of groundwater show varying influence from soil limestone bedrock and seawater geochemical modeling that can explain spatial variability in groundwater na and mg2 concentrations and sr ca and 87sr 86sr values indicates that groundwater compositions are dominantly controlled by mixing of freshwater with seawater and water rock interaction differences between amount weighted annual average precipitation δ18o values and groundwater δ18o values indicate a recharge bias toward the wet season consistent with other tropical carbonate island aquifer settings intra and inter annual variations in na concentrations and δ18o values in groundwater reflect sensitivity of recharge to seasonal variations in rainfall amount and changes in annual rainfall amounts our results indicate the influence of multiple modes of recharge on groundwater compositions and spatial variability in the sensitivity of groundwater to seawater mixing this sensitivity of the freshwater lens points to the vulnerability of groundwater resources to changes in recharge associated with climate land use change and increases in population keywords karst hydrogeology carbonate island aquifer northern guam lens aquifer aqueous geochemistry oxygen isotopes δ18o strontium sr isotopes 1 introduction sustainable management of fresh groundwater resources of carbonate island aquifers is critical to the health and well being of current and future island communities and economies the high matrix permeability of the bedrock in carbonate systems limits surface water resources and typically makes groundwater the dominant source of potable water complex groundwater flow paths dynamic interactions between the freshwater lens and underlying seawater and changing land use and population growth can pose challenges to management of both the quantity and quality of groundwater e g chandrajith et al 2016 contractor and jenson 2000 gingerich 2003 2013 vacher and mylroie 2002 rotzoll et al 2013 specifically guam s population is projected to increase from 180 000 in 2010 to 200 000 in 2020 us census 2011 and expansion of the us military seis 2018 could result in an additional increase in demand on the limited freshwater resources of the island gingerich 2013 links between the surface and subsurface can be strong in carbonate systems making the freshwater lens sensitive to changes in climate that affect the hydrologic cycle e g wong et al 2012 jocson et al 2002 jones and banner 2003 that is the future climate may have changes in evapotranspiration the amount timing or intensity of precipitation and or the extent of runoff and recharge vörösmarty et al 2000 allen and ingram 2002 yet the effects on future recharge and thus water availability of the northern guam lens aquifer ngla are not well known gingerich 2013 recent model projections indicate that guam may experience a decrease in tropical cyclone activity kossin et al 2016 park et al 2017 widlansky et al in revision and annual precipitation over the tropical western north pacific is projected to decrease as storm frequency and magnitude both decrease australian bureau of meteorology and csiro 2011 consequently recharge to the ngla would be expected to decline additionally antecedent moisture conditions can significantly influence how water recharges the aquifer especially in carbonate systems dominated by dual or triple porosities e g mahler et al 2006 wong et al 2012 that is carbonate systems can contain a combination of matrix porosity fracture networks and solution enhanced fracture networks shafts and conduits this range in porosity can simultaneously enable recharge through i slow percolation through the bedrock matrix ii a relatively faster descent of a wetting front down a network of dissolution widened fractures and or iii rapid recharge via conduits and shafts that drain dolines the geology and hydrogeology of the ngla have been studied for decades e g jenson et al 2006 jocson et al 2002 mink and vacher 1997 mylroie et al 2001 mylroie and jenson 2000 tracey et al 1964 ward et al 1965 yielding a conceptual model of the groundwater system that describes a transition between the freshwater lens and seawater gingerich 2013 further critical to the conceptual model is that the ngla is a triple porosity aquifer with subsurface flow through matrix pore space between carbonate sediment grains dissolution enhanced fracture networks and conduits rotzoll et al 2013 this porosity enables rapid recharge to the aquifer and makes recharge sensitive to antecedent moisture conditions contractor and jenson 2000 jocson et al 2002 schwarz et al 2009 here we present results from an isotopic and geochemical study to provide insight into the sensitivity of the freshwater lens to climate variability and provide independent constraints on the existing conceptual model of the groundwater system which up to now has relied on physical as opposed to geochemical observations and constraints we evaluate the major cation and isotopic δ18o δd 87sr 86sr compositions of vadose cave dripwater and phreatic groundwater collected during 2008 2015 to delineate the sources and processes dictating groundwater compositions and characterize the response of groundwater to changes in climate cave dripwater provides a unique window into guam s thick vadose zone 60 180 m which plays a critical role in transmitting recharge to the freshwater lens there was a continual monotonic increase of rainfall from 2008 to 2015 supplemental fig s1 and the occurrence of el niño and la niña events enabling insight into the response of the groundwater system to inter annual variations in rainfall amount our results indicate the sensitivity of recharge to both intra and inter annual variations in hydrologic conditions as isotopic and geochemical compositions of the freshwater lens vary in response to seasonal and inter annual changes in precipitation amount 2 hydrogeologic setting the ngla fig 1 is an eogenetic island karst aquifer characterized by the development of porosity in young cenozoic carbonates via meteoric diagenesis vacher and mylroie 2002 the aquifer has additionally been subject to post depositional alteration and dissolution as a result of groundwater circulation the fine grained texture and poor sorting of the volcaniclastic basement rock that underlies the rocks of the ngla results in much lower hydraulic conductivity than the overlying limestone bedrock fig 2 ward et al 1965 jocson et al 2002 miocene to pleistocene marine limestone formations the barrigada and mariana limestones occupy most of the surface of northern guam tracey et al 1964 schlanger 1964 the barrigada limestone 140 m thick forms the majority of the ngla and consists mostly of fine grained foraminiferal grainstone the mariana limestone is comprised of reef and lagoonal sediments and occupies the coastal periphery and most 65 of the surface outcrop of northern guam it is generally coarser more strongly cemented and harder than the barrigada limestone at the southwestern end of the limestone plateau adjacent to the volcanic highland of southern guam the topmost unit is a wedge of argillaceous limestone containing fine grained weathered volcanic sediment and is mapped as the hagåtña argillaceous member of the mariana limestone tracey et al 1964 groundwater flow and storage within the ngla is controlled by a triple porosity system including diffuse flow through matrix porosity fig 2a and b fracture flow through dissolution enhanced joints and fissures fig 2c and d and conduit flow in dissolution enlarged fractures and vertical shafts draining dolines and along contacts between the limestone bedrock and volcanic basement jocson et al 2002 the high inter granular porosity of uncemented parts of the vadose zone promote high storage and infiltration via percolations along diffuse flow paths through the matrix fig 2a and b in cemented parts of vadose zone flow is focused along enhanced fracture networks that enables faster and more concentrated percolation fig 2c and d vacher and mylroie 2002 showed that horizontal hydraulic conductivity in eogenetic island karst aquifers is a strong control on groundwater flow due to properties of the depositional environment and dissolution enhancement of matrix porosity at the water table the regional hydraulic properties of the ngla have been estimated from studies using field observations and numerical modeling to investigate the response of water levels to recharge jocson et al 2002 and tidal signal attenuation rotzoll et al 2013 the barrigada formation has estimated average matrix porosities of 0 13 and 0 21 above and below the water table respectively and groundwater flow largely occurs through secondary porosity k 12 000 m day whereas the hydraulic conductivity of the mariana limestone is approximately 730 m day with an average porosity of 0 13 rotzoll et al 2013 there are three groundwater zones within the ngla the basal para basal and supra basal zones gingerich 2013 fig 1 the freshwater lens in the basal groundwater zone is thin occurs entirely within the limestone units and is underlain by seawater the freshwater lens in the para basal zone is in contact with the underlying volcanic unit and as a result is less vulnerable to mixing with seawater groundwater in the supra basal zone is underlain by volcanic basement and stands above mean sea level and is therefore completely isolated from seawater intra and inter annual variations in the thickness of the freshwater lens e g storage are influenced by the amount of recharge and withdrawal jocson et al 2002 gingerich 2013 guam has a tropical wet dry climate with stable temperatures year round and a mean annual rainfall of 2 4 m about 70 of which falls in the wet season from june to december supplemental fig s2 recharge to the aquifer is estimated to be 50 of mean annual rainfall but may vary locally from 40 to 60 johnson 2012 highest recharge rates occur in areas that receive runoff from storm drain systems whereas the lowest recharge rates occur in urban areas where storm water runoff is routed to the ocean johnson 2012 additions to groundwater recharge occur in urban areas of northern guam through irrigation septic leachate and water main pipe leakage johnson 2012 antecedent moisture conditions also play a role in transmission of recharge to the freshwater lens recharge may take several months to percolate to the lens under dry conditions in contrast to rapid hours to days transmission of recharge under wet conditions or during high intensity rain events contractor and jenson 2000 jocson et al 2002 slower recharge rates under drier conditions likely reflects vertical propagation along a fine fracture network as opposed to direct recharge along discrete solution widened conduits that likely facilitates high hydraulic conductivity estimated from regional scale groundwater flow path studies furthermore inter annual variations in precipitation amount are sensitive to the el niño southern oscillation enso with wetter years corresponding to el niño events guard et al 2009 and drier years corresponding to la niña events or the year following an el niño event lander 1994a b 3 methods rainwater and groundwater from the vadose and phreatic zones were monitored over eight years partin et al 2011 noronha et al 2016 bautista et al 2018 rainfall was sampled every two weeks and cave dripwater was sampled every 4 6 weeks during 2008 2015 which spanned la niña and el niño conditions supplemental table s1 groundwater from the ngla was sampled quarterly from 10 wells from mid 2013 through 2015 the wells selected for this study are representative of the groundwater compositions for the three groundwater zones fig 1 and span the six hydrologically connected groundwater basins that make up the ngla delineated by the guam environmental protection agency cave dripwater was collected at five sites st1 st2 smp ftm and trn within jinapsan cave that we hypothesize to represent water from dominantly diffuse st2 and smp and dominantly fracture flow paths st1 ftm and trn based on their physical isotopic and geochemical properties this study draws on existing cave drip rate and cave dripwater δ18o and cation ca concentrations and mg ca and sr ca values data presented in partin et al 2011 and noronha et al 2016 which both focused on understanding how cave mineral deposits speleothems can be used to reconstruct past climate this manuscript integrates existing cave dripwater data with previously unreported cave dripwater geochemical data na and mg2 concentrations and geochemical and isotopic data on phreatic groundwater compositions to investigate groundwater recharge processes it should be noted that i the vadose zone above the cave is a few meters thick whereas the thickness of the vadose zone over the wells is well over 100 m and ii the cave is formed in mariana limestone which is well cemented compared to the well lithified to extremely friable barrigada limestone tracey et al 1964 in which the 10 sampled wells were completed fractures are visible in the cave ceiling and the more rapidly dripping sites in the cave are associated with them bautista et al 2018 water samples were collected in pre cleaned hdpe nalgene bottles and wells were sampled using plastic submersible bailers the water samples were decanted into pre cleaned glass vials with no head space for stable isotope δ18o and δd analysis and acid cleaned hdpe plastic vials for analysis of cation concentrations and sr isotope 87sr 86sr compositions well water samples were filtered using a pre cleaned 0 45 micron syringe filtration in the field samples analyzed for cation concentrations and 87sr 86sr values were acidified with concentrated ultrapure hno3 cation concentrations were determined using quadrupole inductively coupled plasma icp mass spectrometry and the icp optical emission spectrometry in the department of geoscience at the university of texas at austin ut the quadrupole icp mass spectrometry analytical uncertainty for sr2 na ca2 mg2 is 0 04 10 2 0 02 0 03 and 0 02 ppm respectively based on two times the standard error of replicate analyses of the internal standard the icp optical emission spectrometry analytical uncertainty for sr2 na ca2 mg2 is 0 04 10 2 1 19 1 91 and 0 36 ppm respectively based on two times the standard error of replicate analyses of the internal standard the median percent difference between replicate analyses of samples collected at the same place and time n 99 for the sr2 na ca2 mg2 was 2 9 4 3 4 9 and 5 6 respectively supplemental table s2 detection limits for sr2 na ca2 mg2 0 01 1 47 1 95 0 29 ppb respectively are well below the elemental concentrations measured in unknown samples supplemental table s2 waters were analyzed for stable isotope δd and δ18o compositions using a thermo scientific delta v isotope ratio mass spectrometer irms equipped with a gasbench sample introduction system and picarro l2130 i at the ut uncertainty based on two times the standard error of replicate analyses of internal standards is 8 for δd and 0 2 for δ18o for the irms and 1 for δd and 0 1 for δ18o for the picarro water isotopic measurements are reported in vsmow the mean difference between replicate analyses of δd n 31 and δ18o n 62 was 5 8 and 0 3 respectively stable isotope data were also retrieved from the global network of isotopes in precipitation gnip iaea wmo 2018 87sr 86sr values of groundwater and leachates from surficial soil and saprolite samples were determined following the methods of musgrove and banner 2004 using a thermo triton thermal ionizing mass spectrometer tims at ut the mean 87sr 86sr value for the standard nbs 987 was 0 71025 0 000016 n 30 blank values were negligible 7 pg of sr2 with respect to sample size 200 ng 87sr 86sr values used for the limestone bedrock are estimated based on secular seawater 87sr 86sr curve hodell et al 1991 mcarthur et al 2006 eidvin et al 2014 87sr 86sr 0 7088 0 7091 assuming that the limestone sr isotope compositions have experienced negligible alteration by diagenesis 87sr 86sr values for the volcanic bedrock values were obtained from hickey varags and reagan 1987 87sr 86sr 0 7035 0 7038 daily precipitation data were retrieved from the guam naval air station weather forecast office nas wfo the noaa weather service meteorological observatory wsmo and andersen air force base aafb aafb and nas wfo are available from the national climate data center stations as pgua and pgum respectively the evolution of dripwater sr ca and 87sr 86sr values due to water rock interaction is modeled following banner and hanson 1990 here we consider water rock interaction to include recrystallization of calcite iterative mass balance calculations simulate increments of water passing through a given volume of bedrock measured in moles assuming each increment of water comes to elemental and isotopic equilibrium with the bedrock these equilibrium values are calculated using the kd value of sr incorporation into calcite of 0 057 sinclair et al 2012 the initial dripwater sr ca value 0 2 were assigned based on the lowest dripwater sr ca measured at each cave fig 5 that is the lowest dripwater sr ca values represent the least evolved waters via water rock interaction initial dripwater 87sr 86sr values were assumed to have the median 87sr 86sr value 0 709107 from measurements of soil leachates e g banner et al 1996 musgrove and banner 2004 wong et al 2011 wortham et al 2017 two sets of water rock interaction trends are calculated for each cave system using the highest 0 7091 fig 5 water rock interaction i and lowest 0 7088 fig 5 water rock interaction ii bedrock 87sr 86sr values estimated from the seawater sr isotope curve hodell et al 1991 mcarthur et al 2006 eidvin et al 2014 and stoichiometric ca 400 000 ppm and a sr concentration of 160 ppm schlanger 1964 these two curves constrain the range of possible dripwater values that could evolve under water rock interaction the varying isotopic composition throughout the barrigada limestone 140 m is constrained to maximum i e younger and minimum i e older 87sr 86sr values 0 7091 and 0 7088 respectively used in the model in addition to the water rock interaction model fluid mixing is calculated between groundwater and seawater fig 5 dashed curve the groundwater endmember is represented by the average sr ca and 87sr 86sr values from wells with extensive water rock interaction y23 y15 y07 and y02 and the seawater endmember has sr ca and 87sr 86sr values of 8 9 turekian 1968 and 0 70918 respectively 4 results and discussion 4 1 delineating controls on phreatic groundwater compositions groundwater geochemical compositions are spatially variable especially with respect to mg2 and na concentrations fig 3 three end member compositions were identified i low na and mg2 concentrations ag2a and y15 herein freshwater end member ii high na and mg2 concentrations and iii low na and high mg2 concentrations fig 3 one group of groundwater compositions is consistent with variable mixing up to 1 between the freshwater end member and seawater wells a10 f02 d14 and m18 referred to herein as group 1 another group of groundwater compositions deviates from the mixing line with elevated mg2 concentrations wells y23 y02 y07 and m04 referred to herein as group 2 fig 3 elevated mg2 concentrations in group 2 might result from the interaction of infiltrating meteoric water with the carbonate bedrock that is water can interact with the bedrock via dissolution or recrystallization of the bedrock group 1 wells tend to be located to the north and or west of group 2 wells and are within the basal zone fig 1 geochemistry of group 1 groundwater indicates mixing between freshwater and seawater is a dominant control on groundwater compositions fig 3 and is consistent with the description of the basal zone as where the freshwater lens is thin and underlain by seawater fig 1 group 2 wells occur within the para basal and supra basal zones where the extent to which the freshwater lens is underlain by seawater and hence susceptible to mixing is limited groundwater na concentrations exhibit limited temporal variability with variations barely exceeding analytical uncertainty fig 4 at several wells however subtle variations are evident for example wells a10 and y15 have relatively low concentrations during the wet season 159 and 8 41 ppm respectively relative to the dry season 186 and 10 3 ppm respectively this indicates some dilution of na concentrations due to recharging rainwater reaching the freshwater lens additionally several of the wells with low na concentrations 20 mg l wells ag2a y23 y02 and m04 exhibit statistically significant decreasing trends in concentrations over the study period supplemental table s6 which is coincident with the continual increase of rainfall over the study period 2013 2015 fig 4 and supplemental fig s1 decreasing na concentrations at these sites indicate that the freshwater lens in places is sensitive to inter annual variations in recharge the occurrence of limited variations in na concentrations on intra and inter annual time scales indicates the sensitivity of parts of the aquifer system to changes in hydrologic conditions the limited magnitude of variation in geochemical compositions however reflects the ability of water in storage to buffer water compositions to large changes in hydrologic conditions e g transitions between wet to dry seasons it is pertinent however that there were no storms during the study period of sufficient intensity to induce ponding in dissolution dolines or otherwise activate regional scale conduit flow other than typhoon dolphin may 2015 that only grazed the northern tip of the island geochemical modeling of the evolution of the isotopic and geochemical compositions of groundwater can explain most groundwater sr ca and 87sr 86sr values fig 5 infiltrating water initially acquires a 87sr 86sr value from the soil that tends to be higher than that of the underlying marine carbonate bedrock 0 7088 0 7091 fig 5 inset the 87sr 86sr value of infiltrating water gradually evolves via water rock interaction to lower values that are more similar to that of the bedrock in this process water dissolves marine carbonates with higher sr concentrations relative to the sr concentrations in the calcite that is re precipitated thereby increasing the sr ca value in the infiltrating water as the 87sr 86sr value is shifted toward that of the bedrock e g water rock interaction curves in fig 5 banner et al 1996 musgrove and banner 2004 wong et al 2011 wortham et al 2017 our model is constrained by measured values for soils and assumes values for the carbonate bedrock based on the age range of the barrigada limestone and the corresponding range of values from the reconstructed temporal variability in marine sr isotope values hodell et al 1991 mcarthur et al 2006 eidvin et al 2014 the large spread in possible bedrock values results in a large range of sr ca and 87sr 86sr values to be accounted for by water rock interaction and indeed all but one of the groundwater compositions can be explained by this modeling effort i e groundwater compositions in fig 5 fall between the two water rock interaction curves we do note that the sr ca and 87sr 86sr values from wells y15 y02 y07 y23 create a trend that parallels one of the water rock interaction curves water rock interaction ii in fig 5 potentially reflecting the evolution of groundwater compositions via water rock interaction with bedrock with similar 87sr 86sr values that are close to 0 7088 importantly these wells have na and mg2 concentrations that indicate groundwater is not mixing with seawater at these locations in contrast to group 1 wells fig 3 interestingly group 1 sr ca and 87sr 86sr values do align with a mixing curve between the well interpreted as freshwater y15 and seawater fig 5 this supports the hypothesis that group 1 groundwater compositions are dominantly influenced by seawater mixing with freshwater that has undergone water rock interaction sr ca and 87sr 86sr values of m04 do not align with the rest of group 2 compositions and instead fall along the seawater mixing curve in fig 5 this however is inconsistent with na and mg2 concentrations that do not reflect seawater mixing fig 3 m04 is spatially distinct from the other group 2 wells so the sr ca and 87sr 86sr values that are distinct from other group 2 wells might reflect water rock interaction with bedrock of a different 87sr 86sr value 4 2 a view of recharge from the vadose zone cave dripwater mg2 and na concentrations are distinct relative to those of groundwater na concentrations of dripwater are elevated relative to those of group 2 but na and mg2 concentrations do not align with the freshwater seawater mixing curve as do those of group 1 well waters fig 3 the inability of the freshwater seawater mixing curve to account for the dripwater indicates that cave dripwater compositions are likely not strongly influenced by sea spray despite the proximity of the cave to the ocean both the barrigada and mariana limestone formations are high energy reef type settings in which deposition of evaporates was limited tracey et al 1964 making the bedrock an unlikely source of na variations however in cave dripwater mg2 and na concentrations can be accounted for by evapo concentration of freshwater freshwater evaporation line in fig 3 cave dripwater δd and δ18o values fall on the global meteoric water line gmwl suggesting negligible influence of evaporation on the water isotopic compositions supplemental fig s3 combined these results indicate that water reaching the cave has undergone little evaporation although the water infiltrating the cave is likely carrying salts precipitated from freshwater that previously evaporated that is rainfall that does not recharge the cave likely evaporates in the soil epikarst or shallow vadose zone leaving behind salts that are later flushed into the cave during recharge intervals seasonal variability in na concentrations is evident at two of the sites dominantly supplied by fracture flow st1 and ftm fig 4 lower concentrations follow the wet season at these two sites indicating the dilution of na concentrations of water stored in the vadose zone by infiltrating meteoric water which is consistent with seasonal increases in drip rate the presence of seasonal variability at some sites and absence at the other cave drip sites highlights the presence of distinct recharge pathways diffuse pathways that drain the more homogenous and geochemically invariant water in storage in the vadose zone fig 2a and b e g moerman et al 2014 and flow along dissolution enhanced fracture networks fig 2c and d that can by pass much of the water stored in the vadose zone dripwater na concentrations exhibit a statistically significant decreasing trend over the entire eight years of monitoring at each drip site supplemental table s6 and fig 4 the trends of decreasing na concentrations over the study interval is consistent with i increasingly higher drip rates during subsequent dry seasons over the time interval of study at all the drip sites reflecting increased storage in the vadose zone fig 4 and ii increasing precipitation amounts over the time interval of study driving dilution of na concentrations of water stored in the vadose zone by infiltrating meteoric water figs 4 and 6 δ18oprecip values exhibit prominent seasonal variability with lower values during wetter months supplemental fig s2 temporal variability in δ18oprecip is greater relative to that observed in cave dripwater and groundwater fig 6 likely reflecting homogenization of δ18oprecip variability due to mixing in the vadose and phreatic zones mean dripwater and groundwater δ18o values 6 4 and 6 2 respectively are lower than the weighted mean value for δ18oprecip 5 4 indicating preferential recharge of rainwater with low δ18o values the mean precipitation weighted δ18oprecip value for the months of august to october is 6 3 assuming that the groundwater isotopic composition is the amount weighted average of rainwater that actually recharges the system this indicates that recharge may be predominately during the wet season and is a result consistent with previous work on guam jocson et al 2002 jones and banner 2003 partin et al 2011 and other tropical carbonate islands jones and banner 2003 further preferential recharge during the wet season is consistent with the conceptual model in which water recharging the cave likely during the wet season has not experienced significant evaporation but is carrying with it dissolved salts from the evaporation of previous rainfall likely during the dry season that did not recharge the cave cave dripwater δ18o values exhibit an observable decreasing trend during the last one to two years at each site with a statistically significant p value 0 001 supplemental table s6 decrease in three of the four sites these trends occur despite the absence of a significant trend in precipitation δ18o values or annual precipitation weighted mean values of δ18oprecip from recharge months i e august september and october fig 6 the nature of rainfall however is notably different before and after the 2013 wet season with higher cumulative monthly rainfall totals during dry and wet seasons following the 2013 wet season fig 6 more intense precipitation may promote greater runoff and enhance the recharge bias toward isotopically lighter precipitation jones and banner 2003 furthermore a marked decrease in the extent to which drip rates decline over the dry season indicate that such changes in precipitation amount translate to increased recharge and water in vadose zone storage cave dripwater δ18o values are slightly more variable at fracture supplied sites relative to diffuse supplied sites fig 7 and δ18o values at diffuse supplied sites are slightly lower relative to fracture supplied sites these results are consistent with the flow paths along dissolution enhanced fracture networks facilitating more direct infiltration of water relative to diffuse flow paths leading to greater reflection of the isotopic variability occurring in δ18oprecip values and less of a recharge bias that is the difference in mean values between sites supplied by fracture vs diffuse flow paths fig 7b and c reflects differences in the sensitivity of these flow paths to changes in hydrologic conditions at the surface 5 conclusions analysis of the spatial and temporal variability of geochemical and isotopic compositions of cave dripwater and groundwater over an interval that spanned a transition from dry to wet conditions independently reinforces the existing conceptual model of recharge and groundwater flow in the ngla this model up to now has been based mainly on observations of physical hydrogeology unconditioned by geochemical observations specifically we find geochemical evidence that the sensitivity of the freshwater lens to intra and inter annual changes in recharge is spatially variable as evidenced by the geochemical variability observed in wells distributed across the study area figs 1 3 and 5 groundwater compositions are influenced by water rock interaction and mixing between recharging meteoric water and seawater up to 1 as evidenced by the ability of geochemical and isotope modeling to explain observed groundwater compositions figs 3 and 5 dissolution enhanced fracture networks constitute preferential recharge pathways as evidenced by distinct geochemical and isotopic variability of fracture supplied and diffuse supplied sites figs 4 6 and 7 recharge is sensitive to inter annual changes in precipitation amount and intensity as evidenced by intra and inter annual variations in the geochemical and isotopic compositions of groundwater figs 4 and 6 recharge predominantly occurs during the wet season based on the coincidence of average groundwater δ18o values with the amount weighted average δ18oprecip values occurring during the wet season figs s2 and 6 the freshwater lens is isotopically and geochemically buffered from the hydroclimatic variability that occurred during the study this is supported by the limited variability in isotopic and geochemical compositions of both cave dripwater and groundwater relative to the magnitude of intra and inter annual variability in precipitation amount and precipitation isotopic compositions figs 4 and 6 acknowledgements we acknowledge support from the strategic environmental research and development program serdp project number 13 rc01 004 rc 2340 and the pacific islands climate science center g12ac0003 this research was also supported by nsf ear 1452024 to j gulley d breecker and j banner nsf ags 1003700 to j partin j jenson and j banner and nsf ags 1404003 to j partin appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2018 10 049 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
6806,assessing the sensitivity of groundwater systems to hydroclimate variability is critical to sustainable management of the water resources of guam us territory we assess spatial and temporal variability of isotopic and geochemical compositions of vadose and phreatic groundwater sampled from cave drip sites and production wells respectively to better understand the vulnerability of the freshwater lens on guam to variability in hydroclimate we independently evaluate the existing conceptual model of the northern guam lens aquifer that is largely based on physical as opposed to geochemical observations sampling was conducted from 2008 to 2015 over which rainfall gradually increased major ion geochemistry and sr isotope values of groundwater show varying influence from soil limestone bedrock and seawater geochemical modeling that can explain spatial variability in groundwater na and mg2 concentrations and sr ca and 87sr 86sr values indicates that groundwater compositions are dominantly controlled by mixing of freshwater with seawater and water rock interaction differences between amount weighted annual average precipitation δ18o values and groundwater δ18o values indicate a recharge bias toward the wet season consistent with other tropical carbonate island aquifer settings intra and inter annual variations in na concentrations and δ18o values in groundwater reflect sensitivity of recharge to seasonal variations in rainfall amount and changes in annual rainfall amounts our results indicate the influence of multiple modes of recharge on groundwater compositions and spatial variability in the sensitivity of groundwater to seawater mixing this sensitivity of the freshwater lens points to the vulnerability of groundwater resources to changes in recharge associated with climate land use change and increases in population keywords karst hydrogeology carbonate island aquifer northern guam lens aquifer aqueous geochemistry oxygen isotopes δ18o strontium sr isotopes 1 introduction sustainable management of fresh groundwater resources of carbonate island aquifers is critical to the health and well being of current and future island communities and economies the high matrix permeability of the bedrock in carbonate systems limits surface water resources and typically makes groundwater the dominant source of potable water complex groundwater flow paths dynamic interactions between the freshwater lens and underlying seawater and changing land use and population growth can pose challenges to management of both the quantity and quality of groundwater e g chandrajith et al 2016 contractor and jenson 2000 gingerich 2003 2013 vacher and mylroie 2002 rotzoll et al 2013 specifically guam s population is projected to increase from 180 000 in 2010 to 200 000 in 2020 us census 2011 and expansion of the us military seis 2018 could result in an additional increase in demand on the limited freshwater resources of the island gingerich 2013 links between the surface and subsurface can be strong in carbonate systems making the freshwater lens sensitive to changes in climate that affect the hydrologic cycle e g wong et al 2012 jocson et al 2002 jones and banner 2003 that is the future climate may have changes in evapotranspiration the amount timing or intensity of precipitation and or the extent of runoff and recharge vörösmarty et al 2000 allen and ingram 2002 yet the effects on future recharge and thus water availability of the northern guam lens aquifer ngla are not well known gingerich 2013 recent model projections indicate that guam may experience a decrease in tropical cyclone activity kossin et al 2016 park et al 2017 widlansky et al in revision and annual precipitation over the tropical western north pacific is projected to decrease as storm frequency and magnitude both decrease australian bureau of meteorology and csiro 2011 consequently recharge to the ngla would be expected to decline additionally antecedent moisture conditions can significantly influence how water recharges the aquifer especially in carbonate systems dominated by dual or triple porosities e g mahler et al 2006 wong et al 2012 that is carbonate systems can contain a combination of matrix porosity fracture networks and solution enhanced fracture networks shafts and conduits this range in porosity can simultaneously enable recharge through i slow percolation through the bedrock matrix ii a relatively faster descent of a wetting front down a network of dissolution widened fractures and or iii rapid recharge via conduits and shafts that drain dolines the geology and hydrogeology of the ngla have been studied for decades e g jenson et al 2006 jocson et al 2002 mink and vacher 1997 mylroie et al 2001 mylroie and jenson 2000 tracey et al 1964 ward et al 1965 yielding a conceptual model of the groundwater system that describes a transition between the freshwater lens and seawater gingerich 2013 further critical to the conceptual model is that the ngla is a triple porosity aquifer with subsurface flow through matrix pore space between carbonate sediment grains dissolution enhanced fracture networks and conduits rotzoll et al 2013 this porosity enables rapid recharge to the aquifer and makes recharge sensitive to antecedent moisture conditions contractor and jenson 2000 jocson et al 2002 schwarz et al 2009 here we present results from an isotopic and geochemical study to provide insight into the sensitivity of the freshwater lens to climate variability and provide independent constraints on the existing conceptual model of the groundwater system which up to now has relied on physical as opposed to geochemical observations and constraints we evaluate the major cation and isotopic δ18o δd 87sr 86sr compositions of vadose cave dripwater and phreatic groundwater collected during 2008 2015 to delineate the sources and processes dictating groundwater compositions and characterize the response of groundwater to changes in climate cave dripwater provides a unique window into guam s thick vadose zone 60 180 m which plays a critical role in transmitting recharge to the freshwater lens there was a continual monotonic increase of rainfall from 2008 to 2015 supplemental fig s1 and the occurrence of el niño and la niña events enabling insight into the response of the groundwater system to inter annual variations in rainfall amount our results indicate the sensitivity of recharge to both intra and inter annual variations in hydrologic conditions as isotopic and geochemical compositions of the freshwater lens vary in response to seasonal and inter annual changes in precipitation amount 2 hydrogeologic setting the ngla fig 1 is an eogenetic island karst aquifer characterized by the development of porosity in young cenozoic carbonates via meteoric diagenesis vacher and mylroie 2002 the aquifer has additionally been subject to post depositional alteration and dissolution as a result of groundwater circulation the fine grained texture and poor sorting of the volcaniclastic basement rock that underlies the rocks of the ngla results in much lower hydraulic conductivity than the overlying limestone bedrock fig 2 ward et al 1965 jocson et al 2002 miocene to pleistocene marine limestone formations the barrigada and mariana limestones occupy most of the surface of northern guam tracey et al 1964 schlanger 1964 the barrigada limestone 140 m thick forms the majority of the ngla and consists mostly of fine grained foraminiferal grainstone the mariana limestone is comprised of reef and lagoonal sediments and occupies the coastal periphery and most 65 of the surface outcrop of northern guam it is generally coarser more strongly cemented and harder than the barrigada limestone at the southwestern end of the limestone plateau adjacent to the volcanic highland of southern guam the topmost unit is a wedge of argillaceous limestone containing fine grained weathered volcanic sediment and is mapped as the hagåtña argillaceous member of the mariana limestone tracey et al 1964 groundwater flow and storage within the ngla is controlled by a triple porosity system including diffuse flow through matrix porosity fig 2a and b fracture flow through dissolution enhanced joints and fissures fig 2c and d and conduit flow in dissolution enlarged fractures and vertical shafts draining dolines and along contacts between the limestone bedrock and volcanic basement jocson et al 2002 the high inter granular porosity of uncemented parts of the vadose zone promote high storage and infiltration via percolations along diffuse flow paths through the matrix fig 2a and b in cemented parts of vadose zone flow is focused along enhanced fracture networks that enables faster and more concentrated percolation fig 2c and d vacher and mylroie 2002 showed that horizontal hydraulic conductivity in eogenetic island karst aquifers is a strong control on groundwater flow due to properties of the depositional environment and dissolution enhancement of matrix porosity at the water table the regional hydraulic properties of the ngla have been estimated from studies using field observations and numerical modeling to investigate the response of water levels to recharge jocson et al 2002 and tidal signal attenuation rotzoll et al 2013 the barrigada formation has estimated average matrix porosities of 0 13 and 0 21 above and below the water table respectively and groundwater flow largely occurs through secondary porosity k 12 000 m day whereas the hydraulic conductivity of the mariana limestone is approximately 730 m day with an average porosity of 0 13 rotzoll et al 2013 there are three groundwater zones within the ngla the basal para basal and supra basal zones gingerich 2013 fig 1 the freshwater lens in the basal groundwater zone is thin occurs entirely within the limestone units and is underlain by seawater the freshwater lens in the para basal zone is in contact with the underlying volcanic unit and as a result is less vulnerable to mixing with seawater groundwater in the supra basal zone is underlain by volcanic basement and stands above mean sea level and is therefore completely isolated from seawater intra and inter annual variations in the thickness of the freshwater lens e g storage are influenced by the amount of recharge and withdrawal jocson et al 2002 gingerich 2013 guam has a tropical wet dry climate with stable temperatures year round and a mean annual rainfall of 2 4 m about 70 of which falls in the wet season from june to december supplemental fig s2 recharge to the aquifer is estimated to be 50 of mean annual rainfall but may vary locally from 40 to 60 johnson 2012 highest recharge rates occur in areas that receive runoff from storm drain systems whereas the lowest recharge rates occur in urban areas where storm water runoff is routed to the ocean johnson 2012 additions to groundwater recharge occur in urban areas of northern guam through irrigation septic leachate and water main pipe leakage johnson 2012 antecedent moisture conditions also play a role in transmission of recharge to the freshwater lens recharge may take several months to percolate to the lens under dry conditions in contrast to rapid hours to days transmission of recharge under wet conditions or during high intensity rain events contractor and jenson 2000 jocson et al 2002 slower recharge rates under drier conditions likely reflects vertical propagation along a fine fracture network as opposed to direct recharge along discrete solution widened conduits that likely facilitates high hydraulic conductivity estimated from regional scale groundwater flow path studies furthermore inter annual variations in precipitation amount are sensitive to the el niño southern oscillation enso with wetter years corresponding to el niño events guard et al 2009 and drier years corresponding to la niña events or the year following an el niño event lander 1994a b 3 methods rainwater and groundwater from the vadose and phreatic zones were monitored over eight years partin et al 2011 noronha et al 2016 bautista et al 2018 rainfall was sampled every two weeks and cave dripwater was sampled every 4 6 weeks during 2008 2015 which spanned la niña and el niño conditions supplemental table s1 groundwater from the ngla was sampled quarterly from 10 wells from mid 2013 through 2015 the wells selected for this study are representative of the groundwater compositions for the three groundwater zones fig 1 and span the six hydrologically connected groundwater basins that make up the ngla delineated by the guam environmental protection agency cave dripwater was collected at five sites st1 st2 smp ftm and trn within jinapsan cave that we hypothesize to represent water from dominantly diffuse st2 and smp and dominantly fracture flow paths st1 ftm and trn based on their physical isotopic and geochemical properties this study draws on existing cave drip rate and cave dripwater δ18o and cation ca concentrations and mg ca and sr ca values data presented in partin et al 2011 and noronha et al 2016 which both focused on understanding how cave mineral deposits speleothems can be used to reconstruct past climate this manuscript integrates existing cave dripwater data with previously unreported cave dripwater geochemical data na and mg2 concentrations and geochemical and isotopic data on phreatic groundwater compositions to investigate groundwater recharge processes it should be noted that i the vadose zone above the cave is a few meters thick whereas the thickness of the vadose zone over the wells is well over 100 m and ii the cave is formed in mariana limestone which is well cemented compared to the well lithified to extremely friable barrigada limestone tracey et al 1964 in which the 10 sampled wells were completed fractures are visible in the cave ceiling and the more rapidly dripping sites in the cave are associated with them bautista et al 2018 water samples were collected in pre cleaned hdpe nalgene bottles and wells were sampled using plastic submersible bailers the water samples were decanted into pre cleaned glass vials with no head space for stable isotope δ18o and δd analysis and acid cleaned hdpe plastic vials for analysis of cation concentrations and sr isotope 87sr 86sr compositions well water samples were filtered using a pre cleaned 0 45 micron syringe filtration in the field samples analyzed for cation concentrations and 87sr 86sr values were acidified with concentrated ultrapure hno3 cation concentrations were determined using quadrupole inductively coupled plasma icp mass spectrometry and the icp optical emission spectrometry in the department of geoscience at the university of texas at austin ut the quadrupole icp mass spectrometry analytical uncertainty for sr2 na ca2 mg2 is 0 04 10 2 0 02 0 03 and 0 02 ppm respectively based on two times the standard error of replicate analyses of the internal standard the icp optical emission spectrometry analytical uncertainty for sr2 na ca2 mg2 is 0 04 10 2 1 19 1 91 and 0 36 ppm respectively based on two times the standard error of replicate analyses of the internal standard the median percent difference between replicate analyses of samples collected at the same place and time n 99 for the sr2 na ca2 mg2 was 2 9 4 3 4 9 and 5 6 respectively supplemental table s2 detection limits for sr2 na ca2 mg2 0 01 1 47 1 95 0 29 ppb respectively are well below the elemental concentrations measured in unknown samples supplemental table s2 waters were analyzed for stable isotope δd and δ18o compositions using a thermo scientific delta v isotope ratio mass spectrometer irms equipped with a gasbench sample introduction system and picarro l2130 i at the ut uncertainty based on two times the standard error of replicate analyses of internal standards is 8 for δd and 0 2 for δ18o for the irms and 1 for δd and 0 1 for δ18o for the picarro water isotopic measurements are reported in vsmow the mean difference between replicate analyses of δd n 31 and δ18o n 62 was 5 8 and 0 3 respectively stable isotope data were also retrieved from the global network of isotopes in precipitation gnip iaea wmo 2018 87sr 86sr values of groundwater and leachates from surficial soil and saprolite samples were determined following the methods of musgrove and banner 2004 using a thermo triton thermal ionizing mass spectrometer tims at ut the mean 87sr 86sr value for the standard nbs 987 was 0 71025 0 000016 n 30 blank values were negligible 7 pg of sr2 with respect to sample size 200 ng 87sr 86sr values used for the limestone bedrock are estimated based on secular seawater 87sr 86sr curve hodell et al 1991 mcarthur et al 2006 eidvin et al 2014 87sr 86sr 0 7088 0 7091 assuming that the limestone sr isotope compositions have experienced negligible alteration by diagenesis 87sr 86sr values for the volcanic bedrock values were obtained from hickey varags and reagan 1987 87sr 86sr 0 7035 0 7038 daily precipitation data were retrieved from the guam naval air station weather forecast office nas wfo the noaa weather service meteorological observatory wsmo and andersen air force base aafb aafb and nas wfo are available from the national climate data center stations as pgua and pgum respectively the evolution of dripwater sr ca and 87sr 86sr values due to water rock interaction is modeled following banner and hanson 1990 here we consider water rock interaction to include recrystallization of calcite iterative mass balance calculations simulate increments of water passing through a given volume of bedrock measured in moles assuming each increment of water comes to elemental and isotopic equilibrium with the bedrock these equilibrium values are calculated using the kd value of sr incorporation into calcite of 0 057 sinclair et al 2012 the initial dripwater sr ca value 0 2 were assigned based on the lowest dripwater sr ca measured at each cave fig 5 that is the lowest dripwater sr ca values represent the least evolved waters via water rock interaction initial dripwater 87sr 86sr values were assumed to have the median 87sr 86sr value 0 709107 from measurements of soil leachates e g banner et al 1996 musgrove and banner 2004 wong et al 2011 wortham et al 2017 two sets of water rock interaction trends are calculated for each cave system using the highest 0 7091 fig 5 water rock interaction i and lowest 0 7088 fig 5 water rock interaction ii bedrock 87sr 86sr values estimated from the seawater sr isotope curve hodell et al 1991 mcarthur et al 2006 eidvin et al 2014 and stoichiometric ca 400 000 ppm and a sr concentration of 160 ppm schlanger 1964 these two curves constrain the range of possible dripwater values that could evolve under water rock interaction the varying isotopic composition throughout the barrigada limestone 140 m is constrained to maximum i e younger and minimum i e older 87sr 86sr values 0 7091 and 0 7088 respectively used in the model in addition to the water rock interaction model fluid mixing is calculated between groundwater and seawater fig 5 dashed curve the groundwater endmember is represented by the average sr ca and 87sr 86sr values from wells with extensive water rock interaction y23 y15 y07 and y02 and the seawater endmember has sr ca and 87sr 86sr values of 8 9 turekian 1968 and 0 70918 respectively 4 results and discussion 4 1 delineating controls on phreatic groundwater compositions groundwater geochemical compositions are spatially variable especially with respect to mg2 and na concentrations fig 3 three end member compositions were identified i low na and mg2 concentrations ag2a and y15 herein freshwater end member ii high na and mg2 concentrations and iii low na and high mg2 concentrations fig 3 one group of groundwater compositions is consistent with variable mixing up to 1 between the freshwater end member and seawater wells a10 f02 d14 and m18 referred to herein as group 1 another group of groundwater compositions deviates from the mixing line with elevated mg2 concentrations wells y23 y02 y07 and m04 referred to herein as group 2 fig 3 elevated mg2 concentrations in group 2 might result from the interaction of infiltrating meteoric water with the carbonate bedrock that is water can interact with the bedrock via dissolution or recrystallization of the bedrock group 1 wells tend to be located to the north and or west of group 2 wells and are within the basal zone fig 1 geochemistry of group 1 groundwater indicates mixing between freshwater and seawater is a dominant control on groundwater compositions fig 3 and is consistent with the description of the basal zone as where the freshwater lens is thin and underlain by seawater fig 1 group 2 wells occur within the para basal and supra basal zones where the extent to which the freshwater lens is underlain by seawater and hence susceptible to mixing is limited groundwater na concentrations exhibit limited temporal variability with variations barely exceeding analytical uncertainty fig 4 at several wells however subtle variations are evident for example wells a10 and y15 have relatively low concentrations during the wet season 159 and 8 41 ppm respectively relative to the dry season 186 and 10 3 ppm respectively this indicates some dilution of na concentrations due to recharging rainwater reaching the freshwater lens additionally several of the wells with low na concentrations 20 mg l wells ag2a y23 y02 and m04 exhibit statistically significant decreasing trends in concentrations over the study period supplemental table s6 which is coincident with the continual increase of rainfall over the study period 2013 2015 fig 4 and supplemental fig s1 decreasing na concentrations at these sites indicate that the freshwater lens in places is sensitive to inter annual variations in recharge the occurrence of limited variations in na concentrations on intra and inter annual time scales indicates the sensitivity of parts of the aquifer system to changes in hydrologic conditions the limited magnitude of variation in geochemical compositions however reflects the ability of water in storage to buffer water compositions to large changes in hydrologic conditions e g transitions between wet to dry seasons it is pertinent however that there were no storms during the study period of sufficient intensity to induce ponding in dissolution dolines or otherwise activate regional scale conduit flow other than typhoon dolphin may 2015 that only grazed the northern tip of the island geochemical modeling of the evolution of the isotopic and geochemical compositions of groundwater can explain most groundwater sr ca and 87sr 86sr values fig 5 infiltrating water initially acquires a 87sr 86sr value from the soil that tends to be higher than that of the underlying marine carbonate bedrock 0 7088 0 7091 fig 5 inset the 87sr 86sr value of infiltrating water gradually evolves via water rock interaction to lower values that are more similar to that of the bedrock in this process water dissolves marine carbonates with higher sr concentrations relative to the sr concentrations in the calcite that is re precipitated thereby increasing the sr ca value in the infiltrating water as the 87sr 86sr value is shifted toward that of the bedrock e g water rock interaction curves in fig 5 banner et al 1996 musgrove and banner 2004 wong et al 2011 wortham et al 2017 our model is constrained by measured values for soils and assumes values for the carbonate bedrock based on the age range of the barrigada limestone and the corresponding range of values from the reconstructed temporal variability in marine sr isotope values hodell et al 1991 mcarthur et al 2006 eidvin et al 2014 the large spread in possible bedrock values results in a large range of sr ca and 87sr 86sr values to be accounted for by water rock interaction and indeed all but one of the groundwater compositions can be explained by this modeling effort i e groundwater compositions in fig 5 fall between the two water rock interaction curves we do note that the sr ca and 87sr 86sr values from wells y15 y02 y07 y23 create a trend that parallels one of the water rock interaction curves water rock interaction ii in fig 5 potentially reflecting the evolution of groundwater compositions via water rock interaction with bedrock with similar 87sr 86sr values that are close to 0 7088 importantly these wells have na and mg2 concentrations that indicate groundwater is not mixing with seawater at these locations in contrast to group 1 wells fig 3 interestingly group 1 sr ca and 87sr 86sr values do align with a mixing curve between the well interpreted as freshwater y15 and seawater fig 5 this supports the hypothesis that group 1 groundwater compositions are dominantly influenced by seawater mixing with freshwater that has undergone water rock interaction sr ca and 87sr 86sr values of m04 do not align with the rest of group 2 compositions and instead fall along the seawater mixing curve in fig 5 this however is inconsistent with na and mg2 concentrations that do not reflect seawater mixing fig 3 m04 is spatially distinct from the other group 2 wells so the sr ca and 87sr 86sr values that are distinct from other group 2 wells might reflect water rock interaction with bedrock of a different 87sr 86sr value 4 2 a view of recharge from the vadose zone cave dripwater mg2 and na concentrations are distinct relative to those of groundwater na concentrations of dripwater are elevated relative to those of group 2 but na and mg2 concentrations do not align with the freshwater seawater mixing curve as do those of group 1 well waters fig 3 the inability of the freshwater seawater mixing curve to account for the dripwater indicates that cave dripwater compositions are likely not strongly influenced by sea spray despite the proximity of the cave to the ocean both the barrigada and mariana limestone formations are high energy reef type settings in which deposition of evaporates was limited tracey et al 1964 making the bedrock an unlikely source of na variations however in cave dripwater mg2 and na concentrations can be accounted for by evapo concentration of freshwater freshwater evaporation line in fig 3 cave dripwater δd and δ18o values fall on the global meteoric water line gmwl suggesting negligible influence of evaporation on the water isotopic compositions supplemental fig s3 combined these results indicate that water reaching the cave has undergone little evaporation although the water infiltrating the cave is likely carrying salts precipitated from freshwater that previously evaporated that is rainfall that does not recharge the cave likely evaporates in the soil epikarst or shallow vadose zone leaving behind salts that are later flushed into the cave during recharge intervals seasonal variability in na concentrations is evident at two of the sites dominantly supplied by fracture flow st1 and ftm fig 4 lower concentrations follow the wet season at these two sites indicating the dilution of na concentrations of water stored in the vadose zone by infiltrating meteoric water which is consistent with seasonal increases in drip rate the presence of seasonal variability at some sites and absence at the other cave drip sites highlights the presence of distinct recharge pathways diffuse pathways that drain the more homogenous and geochemically invariant water in storage in the vadose zone fig 2a and b e g moerman et al 2014 and flow along dissolution enhanced fracture networks fig 2c and d that can by pass much of the water stored in the vadose zone dripwater na concentrations exhibit a statistically significant decreasing trend over the entire eight years of monitoring at each drip site supplemental table s6 and fig 4 the trends of decreasing na concentrations over the study interval is consistent with i increasingly higher drip rates during subsequent dry seasons over the time interval of study at all the drip sites reflecting increased storage in the vadose zone fig 4 and ii increasing precipitation amounts over the time interval of study driving dilution of na concentrations of water stored in the vadose zone by infiltrating meteoric water figs 4 and 6 δ18oprecip values exhibit prominent seasonal variability with lower values during wetter months supplemental fig s2 temporal variability in δ18oprecip is greater relative to that observed in cave dripwater and groundwater fig 6 likely reflecting homogenization of δ18oprecip variability due to mixing in the vadose and phreatic zones mean dripwater and groundwater δ18o values 6 4 and 6 2 respectively are lower than the weighted mean value for δ18oprecip 5 4 indicating preferential recharge of rainwater with low δ18o values the mean precipitation weighted δ18oprecip value for the months of august to october is 6 3 assuming that the groundwater isotopic composition is the amount weighted average of rainwater that actually recharges the system this indicates that recharge may be predominately during the wet season and is a result consistent with previous work on guam jocson et al 2002 jones and banner 2003 partin et al 2011 and other tropical carbonate islands jones and banner 2003 further preferential recharge during the wet season is consistent with the conceptual model in which water recharging the cave likely during the wet season has not experienced significant evaporation but is carrying with it dissolved salts from the evaporation of previous rainfall likely during the dry season that did not recharge the cave cave dripwater δ18o values exhibit an observable decreasing trend during the last one to two years at each site with a statistically significant p value 0 001 supplemental table s6 decrease in three of the four sites these trends occur despite the absence of a significant trend in precipitation δ18o values or annual precipitation weighted mean values of δ18oprecip from recharge months i e august september and october fig 6 the nature of rainfall however is notably different before and after the 2013 wet season with higher cumulative monthly rainfall totals during dry and wet seasons following the 2013 wet season fig 6 more intense precipitation may promote greater runoff and enhance the recharge bias toward isotopically lighter precipitation jones and banner 2003 furthermore a marked decrease in the extent to which drip rates decline over the dry season indicate that such changes in precipitation amount translate to increased recharge and water in vadose zone storage cave dripwater δ18o values are slightly more variable at fracture supplied sites relative to diffuse supplied sites fig 7 and δ18o values at diffuse supplied sites are slightly lower relative to fracture supplied sites these results are consistent with the flow paths along dissolution enhanced fracture networks facilitating more direct infiltration of water relative to diffuse flow paths leading to greater reflection of the isotopic variability occurring in δ18oprecip values and less of a recharge bias that is the difference in mean values between sites supplied by fracture vs diffuse flow paths fig 7b and c reflects differences in the sensitivity of these flow paths to changes in hydrologic conditions at the surface 5 conclusions analysis of the spatial and temporal variability of geochemical and isotopic compositions of cave dripwater and groundwater over an interval that spanned a transition from dry to wet conditions independently reinforces the existing conceptual model of recharge and groundwater flow in the ngla this model up to now has been based mainly on observations of physical hydrogeology unconditioned by geochemical observations specifically we find geochemical evidence that the sensitivity of the freshwater lens to intra and inter annual changes in recharge is spatially variable as evidenced by the geochemical variability observed in wells distributed across the study area figs 1 3 and 5 groundwater compositions are influenced by water rock interaction and mixing between recharging meteoric water and seawater up to 1 as evidenced by the ability of geochemical and isotope modeling to explain observed groundwater compositions figs 3 and 5 dissolution enhanced fracture networks constitute preferential recharge pathways as evidenced by distinct geochemical and isotopic variability of fracture supplied and diffuse supplied sites figs 4 6 and 7 recharge is sensitive to inter annual changes in precipitation amount and intensity as evidenced by intra and inter annual variations in the geochemical and isotopic compositions of groundwater figs 4 and 6 recharge predominantly occurs during the wet season based on the coincidence of average groundwater δ18o values with the amount weighted average δ18oprecip values occurring during the wet season figs s2 and 6 the freshwater lens is isotopically and geochemically buffered from the hydroclimatic variability that occurred during the study this is supported by the limited variability in isotopic and geochemical compositions of both cave dripwater and groundwater relative to the magnitude of intra and inter annual variability in precipitation amount and precipitation isotopic compositions figs 4 and 6 acknowledgements we acknowledge support from the strategic environmental research and development program serdp project number 13 rc01 004 rc 2340 and the pacific islands climate science center g12ac0003 this research was also supported by nsf ear 1452024 to j gulley d breecker and j banner nsf ags 1003700 to j partin j jenson and j banner and nsf ags 1404003 to j partin appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2018 10 049 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
6807,monthly streamflow time series are highly non linear how to improve forecast accuracy is a great challenge in hydrological studies a lot of research has been conducted to address the streamflow forecasting problem however few methods are developed to make a systematic research the objective of this study is to understand the underlying trend of streamflow so that a regression model can be developed to forecast the flow volume in this paper a hybrid streamflow forecast framework is proposed that integrates factor analysis time series decomposition data regression and error suppression correlation coefficients between the current streamflow and the streamflow with lags are analyzed using autocorrelation function acf partial autocorrelation function pacf and grey correlation analysis gca support vector regression svr and generalized regression neural network grnn models are integrated with seasonal and trend decomposition to make monthly streamflow forecast auto regression and multi model combination error correction methods are used to ensure the accuracy in our experiments the proposed method is compared with a stochastic autoregressive integrated moving average arima streamflow forecast model fourteen models are developed and the monthly streamflow data of shigu and xiangjiaba china from 1961 to 2009 are used to evaluate our proposed method our results demonstrate that the integrated model of grey correlation analysis seasonal trend decomposition procedure based on loess stl support vector regression gca stl svr exhibits an improved performance for monthly streamflow forecast the average error of the proposed model is reduced to less than one tenth in contrast to the state of the art method and the standard deviation is also reduced by more than 30 which implies a greater consistency keywords streamflow forecast regression factor analysis time series decomposition 1 introduction accurate streamflow forecasting is of significant importance for planning and management of water resources as well as early warning and mitigation of natural disasters such as droughts and floods yu et al 2018 nevertheless affected by complex factors including precipitation evaporation runoff yield and confluence topography and human activities it is still challenging to achieve accurate streamflow forecasting senthil kumar et al 2013 until now a large variety of streamflow forecast models have been proposed mainly classified as physical and data driven models physical models are good at providing insight into catchment processes while they have been criticized for being difficult to implement in contrast data driven models have minimum information requirements and rapid development times data driven stochastic models have been used for streamflow forecasting autoregressive integrated moving average models arima and its variants are widely used papacharalampous et al 2018 while stochastic models are often limited by assumptions of normality linearity and variable independence chen et al 2018 chen and singh 2018 the second data driven type machine learning shows a strong deep learning ability and extremely suitable for simulating the complex process over the past 50 years research on machine learning has evolved from the efforts of a handful of computer engineers mitchell 2006 yuan and abouelenien 2015 yuan et al 2018 artificial neural network ann is a widely used method for long term simulation and forecast aksoy and dahamsheh 2009 moeeni and bonakdari 2016 wu and chau 2010 wu et al 2009 but ann still has some intrinsic disadvantages such as slow convergence speed less generalizing performance arriving at a local minimum and over fitting problems support vector machine svm is based on the vc dimension theory and structural risk minimization of statistical learning cortes and vapnik 1995 it transforms the problem into a quadratic optimization problem theoretically which can get the globally optimal solution and solve the practical problems such as small sample nonlinear high dimension and local minimum smola and lkopf 2004 vapnik 2010 maity et al 2010 pointed out that svr machine learning approaches were more popular due to their inherent advantages over traditional modeling techniques kalteh 2015 employed genetic algorithm support vector regression ga svr models for forecasting monthly flow on two rivers and obtained good performance papacharalampous et al 2017 conducted large scale computational experiments to compare stochastic and ml methods regarding their multi step ahead forecasting properties and suggested that the ml methods exhibit a good performance an important step of ann and svr models is to determine the significant input variables bowden et al 2005a b some of which are correlated noisy and some input variables are less informative bowden et al 2005a chen et al 2013 grey correlation analysis gca evaluates the complex phenomena affected by many factors and a good metric to quantify the degree of association between the forecasting factors and the streamflow however in any streamflow forecast model there are three types of uncertainty caused by a number of factors input uncertainty model structure uncertainty and parameter uncertainty liu and gupta 2007 in order to reduce uncertainty and improve accuracy a proven method time series decomposition has been employed in lots of researches time series decomposition has the ability to analyze streamflow temporal and spatial variation and extracting useful information as much as possible kisi 2011 wavelet analysis mallat 1989 is developed on the basis of fourier analysis it is a local transformation of space and frequency by stretching and translating the signal can be multi scale analyzed therefore it is suitable for the analysis of non stationary hydrological time series guo et al 2011 kisi 2011 liu et al 2014 seasonal and trend decomposition using loess stl uses a locally weighted regression that enables processing of any type of seasonal variation data cleveland and cleveland 1990 rojo et al 2017 predicted airborne pollen series based on the seasonal and residual stochastic components of data series decomposed by using stl lafare et al 2016 used stl to understand groundwater behavior in the permo triassic sandstone aquifer different from time series decomposition real time error correction is a post process method to reduce uncertainty and improving accuracy the kalman filter updating method can reflect various hydrological and hydraulic flow fields by updating model input or parameters but it tends to require a long computational time wu et al 2012 wu et al 2012 tested the kalman filter with simple neural networks and autoregressive models and pointed out that the results are similar in addition the multi model combination approach advocates the synchronous use of the simulated discharges of a number of models to produce an overall integrated result which can be used as an alternative to that produced by a single model chen et al 2015 shamseldin et al 1997 first introduced the multi model combination concept into the hydrologic field since then there have been several more studies which have dealt with a multi modal combination of hydrological models coulibaly et al 2005 wu et al 2015 xiong et al 2001 wu et al 2015 proposed three coupling forecast methods which included real time correction combination forecast method combination forecast real time correction method for the purpose of improving the precision of flood forecasting many studies have been done to improve streamflow forecast accuracy as described above there exist comparative studies for example comparison of driving models kalteh 2015 moeeni and bonakdari 2016 predicting factor screening methods chen et al 2013 multiple sequence decomposition guo et al 2011 kisi and cimen 2011 liu et al 2014 and multiple error correction methods chen et al 2015 wu et al 2015 a model requires a systematic integration of many components including factor analysis time series decomposition data regression and error suppression which enables accurate modeling of a hydro system in this paper ann and support vector regression svr are employed as regression models svr is more accurate but less efficient than the least squares support vector machine lssvr suykens et al 2002 because svr solves a convex quadratic programming cqp problem to determine the regression and lssvr just solves a set of reformulated linear equations generalized regression neural network is relatively simple in structure and training of network there is no need to estimate the number of hidden layers and the number of hidden cells in advance and there is an advantage of global convergence the autocorrelation function acf partial autocorrelation function pacf and grey correlation analysis gca are candidate predictors screening methods we develop hybrid models of svr and grnn for monthly streamflow forecast that couples with seasonal and trend decomposition the error correction approaches are used to enhance the performance of the proposed hybrid models the following article structure is organized as follows section 2 is the theory and methods section 3 is a description of the study area and data section 4 presents a case study section 5 concludes this paper with a summary 2 methods the proposed streamflow forecasting framework consists of forecast factors selection time series decomposition model learning and real time error correction correlation coefficients between the current streamflow and the streamflow with n month lag are analyzed using acf pacf and gca the antecedent streamflow with a greater correlation is included in the input sets support vector regression svr and generalized regression neural network models are integrated with seasonal and trend decomposition stl to forecast the monthly streamflow 2 1 support vector machine support vector machine svm which is known as classification and then extended for regression was proposed by vapnik 1995 svm is built based on the principle of the structural risk minimization rather than the empirical risk minimization support vector regression svr is used to solve the problem of regression with svm the following is a brief description of svr suppose n samples data for training are x i d i i n xi is the input vector and di means desired output svr results in 1 y w φ x b where φ x is a non linear mapping w is a hyperplane and b is offset a penalty function is used in svr 2 d i y i ε n o t a l l o c a t i n g a p e n a l t y d i y i ε a l l o c a t i n g a p e n a l t y when the estimated value is within the ε insensitive tube the loss value will be zero parameters of the regression function can be acquired by minimizing the following objective function 3 min 1 2 w 2 c i 1 n l ε y i d i 4 l ε y i d i max 0 y i d i ε where c represents the regularized constant that weighing the model complexity and the empirical error a relative importance of the empirical risk will increase when the value of c increases the slack variables ξ and ξ are introduced in eq 3 for the existence of fitting errors the optimization problem of svr will be as 5 min 1 2 w 2 c i 1 n ξ i ξ i subject to w φ x i b d i ε ξ i d i w t φ x i b ε ξ i ξ 0 ξ 0 then use lagrange multipliers to solve the above optimization problem in its dual form 6 max i 1 n α i α i y i ε i 1 n α i α i 1 2 i 1 j 1 n α i α i α j α j k x i x j subject to 0 α i c 0 α i c i 1 n α i α i 0 7 k x i x j φ x i t φ x j k x i x j is a nonlinear kernel function which can map the lower dimension input into a higher dimension linear space radial basis kernel function is used in this study 2 2 generalized regression neural network generalized regression neural network zaknich 2013 is a special radial basis function neural network compared with the widely used bp neural network grnn has the following advantages 1 structure of grnn neural network is relatively simple in addition to the input and output layers there are only two hidden layers the pattern layer and the summation layer and the number of hidden neurons in the pattern layer is the same as the number of training samples 2 training of network is simple the network training is completed once the training sample through the hidden layer 3 there is no need to estimate the number of hidden layers and the number of hidden cells in advance 4 global convergence of grnn the theoretical basis of the grnn neural network is nonlinear regression analysis let the joint probability density function of the random variable x and y be f x y when the observed value of x is x the conditional expectation of y for x is 8 y e y x yf x y d y f x y d y set the sample data set is x i y i i 1 2 n the dimension of x i is m the nonparametric estimates of probability density function f x y are as follows 9 f x y 1 n 2 π m 1 2 σ m 1 i 1 n exp x x i t x x i 2 σ 2 exp y y i 2 2 σ 2 combining eqs 9 and 8 to get 10 y e y x i 1 n exp x x i t x x i 2 σ 2 y exp y y i 2 2 σ 2 d y i 1 n exp x x i t x x i 2 σ 2 exp y y i 2 2 σ 2 d y hence the estimation becomes 11 y i 1 n y i exp x x i t x x i 2 σ 2 i 1 n exp x x i t x x i 2 σ 2 when the observed value of x is x the conditional expectation estimate of y for x is the weighted average of all sample observations yi the weight of yi is exp x x i t x x i 2 σ 2 the smoothing factor σ needs to be optimized for which we adopt cross validation 2 3 grey correlation analysis grey system theory was proposed in the 1980s based on the mathematical theory of systems engineering ju long 1982 since then the theory has become quite popular with its ability to deal with the systems that have partially unknown parameters as a superiority to conventional statistical models grey models require only a limited amount of data to estimate the behavior of unknown systems grey correlation analysis is an important part of grey system theory grey correlation analysis method can evaluate the complex phenomena affected by many factors from the overall concept in this paper we introduce the grey correlation analysis to quantify the degree of association between the primary forecasting factor and the predicted runoff the calculation process of grey correlation analysis is as follows let x 0 x 0 1 x 0 n be the system characteristic behavior sequence x i x i 1 x i n and i 1 2 m is the relevant factor sequence first of all to perform dimensionless processing on time sequence and the initial values of each sequence are as follows 12 x i x i x i 1 x i 1 x i 2 x i n where i 1 2 m let δ i k x 0 k x i k the difference sequence of the initial value is 13 δ i δ i 1 δ i 2 δ i n let m max i max k δ i k m min i min k δ i k the correlation coefficient between the target variable and related factors at each time is calculated as follows 14 γ 0 i k m ξ m δ i k ξ m where ξ 0 1 k 1 2 n and i 1 2 m average the correlation coefficients at each moment to systematically compare the degree of association between the target sequence and the correlation factor sequence the correlation formula is as follows 15 γ 0 i 1 n k 1 n γ 0 i k 2 4 stl decomposition seasonal and trend decomposition using loess stl uses the robust local weighted regression as a smoothing method to decompose the time series into seasonal trend and residual items 16 y v t r e n d v s e a s o n a l v r e s i d u a l v where y v is the time series observed value of the v period the trend component tren d v is considered low frequency seasonal components seasona l v is considered to be high frequency changes caused by seasonal interference the remaining amount residua l v is a random component loess is a local polynomial regression a common method for smoothing two dimensional scatterplots the regression is done using the weighted least squares method that is the closer the data is to the estimated point the greater the weight finally the local regression model is used to estimate the value of the response variable in this way the whole fitting curve is obtained by the point by point operation loess is a nonparametric learning method defined as follows 17 j θ i 1 m w i y i θ t f i 2 where f is the point to be estimated f i is the sample point w i is the weight w i e f i f 2 2 k 2 the core of the stl algorithm is the iterative process of loess the iterative decomposition process is as follows 1 set the initial value k 0 t v k 0 2 remove trend items y v t v k 3 the loess smoothing is performed on the subsequence to obtain the time series c v k 1 4 perform three times moving average on c v k 1 for lengths of n p n p 3 perform a loess process to get the time series l v k 1 remove periodic differences 5 remove trend items s v k 1 c v k 1 l v k 1 6 remove season items y v s v k 1 7 the loess smoothing is performed on the subsequence y v s v k 1 to obtain the time series t v k 1 8 check whether t v k 1 convergence if convergence s v s v k 1 t v k 1 r v y v s v t v if not repeat the process 2 8 2 5 real time error correction 2 5 1 ar error correction method the discrepancy between the model predicted discharge and the actually observed past discharge is defined as an error which can be used as information for correction if this error signal has a correlation it can probably be used for improved prediction with a time series model of the error signal an improved discharge forecast can be made by adding the error term to the previous model results in this study the error term was estimated using an autoregressive ar model which can be expressed as 18 e t k 1 p θ i e t k ξ where e is the streamflow forecasting error time series p represents the order of the autoregressive model θ i are the parameters of the autoregressive model and ξ is a pure white noise sequence having variance σ 2 order selection criteria were used to determine the appropriate order 2 5 2 multi model composition method estimates of n streamflow forecast models for the t th period of time is q it i 1 2 n a combined estimate q ct is defined 19 q ct i 1 n w i q it ξ t where wi is the weight assigned to the i th model i 1 n w i 1 and ξ t is the combination error term in order to obtain the weights wi an objective function is described as follows 20 mine q ct q obs t 2 m i n e i 1 n w i q it q obs t 2 and the lagrange multiplier is used to solve the above problem 3 study area and data our study takes the jinsha river as the study area the jinsha river is located in the upper reaches of the yangtze river china with the basin area of 473 200 km2 accounting for 26 of the yangtze river basin area it has a total length of 3479 km a natural drop of 5100 m it is rich in hydropower resources and plays a vital role in economic development and ecological environmental conservation of china twenty five hydropower dams in the jinsha watershed are and will be constructed which take the responsibility of flood control agricultural hydroelectric power generation and municipal and industrial water supply with the completion of these hydropower dams the jinsha river hydropower resources are effectively developed and utilized discharge forecasting is significant to the optimal operation of these dams this study focuses on the shigu and xiangjiaba gauging stations which are hydrological control station of the upper reach and the lower reach of jinsha river respectively a schematic of the jinsha river and the gauging stations is given in fig 1 we obtained the available quality controlled and partially infilled daily streamflow m3 s data at the xiangjiaba 1961 2008 and shigu 1970 2009 hydrologic stations provided by the yangtze river waterway bureau china monthly streamflow data needed in this research were aggregated from daily data 4 case study 4 1 frameworks of proposed models the steps of our proposed streamflow forecast framework are listed as follows step 1 acf pacf and gca are used to select forecast factors the correlation coefficients between the current streamflow and the streamflow with n month lag is calculated using acf pacf and gca each antecedent monthly streamflow has three correlation coefficient values the antecedent streamflow with the higher acf correlation value is included to get an acf input set similarly we get pacf input set and gca input set with the higher pacf and gca value support vector machine svr and generalized regression neural network are used as regression models the statistical indicators evaluating the accuracy of prediction are mean average percentage error mape a proportion that errors less than 5 10 20 30 and deterministic coefficient dc continuous monthly streamflow data from 1970 to 2009 are used in shigu case study then the better input set can be determined by their forecast performances then the better input set can be determined by their forecast performances step 2 on the basis of step 1 we developed hybrid svr and grnn monthly streamflow forecast models coupling with seasonal and trend decomposition methods stl svr and grnn are used as the model the better method proved in step 1 is used to make forecast factors selection the original sequences are decomposed into multiple sub series by time series pre processing techniques stl decomposition four models svr stl svr and grnn stl grnn are built to forecast monthly streamflow of xiangjiaba the performances of the models are compared using the root mean square error rmse and deterministic coefficient dc step 3 two real time error correction methods the ar model and multi model combination method are used to enhance the performance a 3 order ar model is used for error correction the parameters of the ar model are optimized according to the least square method as a comparison the multi model composition method is also used for the error correction we train svr and grnn models the framework is illustrated in fig 2 4 2 performance metrics the statistical indicators evaluating the accuracy of prediction are mean average percentage error mare the proportion that errors less than 5 10 20 30 and deterministic coefficient dc and the root mean square error rmse mape is calculated according to eq 21 an accurate model has the mape metric value close to 0 rmse is a frequently used measure of the differences between values predicted and the values actually observed rmse represents the sample standard deviation of the differences between predicted values and observed values rmse is calculated according to eq 22 dc is the proportion of the variance in the dependent variable that is predictable from the independent variables it provides a measure of the quality of outcomes replicated by the model based on the proportion of total variation of outcomes explained by the model dc is calculated according to eq 23 21 mape 1 n i 1 n y i obs y i est y i obs 100 22 rmse 1 n i 1 n y i obs y i est 2 23 dc 1 i 1 n y i y i 2 i 1 n y i y 2 4 3 forecast factor selection forecasting factor and regression models are essential for developing a streamflow forecast framework in this part the svr model and grnn model two machine learning method are built for data simulation the autocorrelation function the partial autocorrelation function and grey correlation analysis are introduced to quantify the correlation degree between the streamflow and potential forecasting factors it is an effective forecast method by using the variable s own historical records to make an estimation if q t is the streamflow to be forecasted select antecedent streamflow q t 1 q t 2 q t i q t 12 as alternative factors where q t i means streamflow i month ahead of forecast month the grey correlation degree between the alternative factors q t i and q t is calculated results are as follows γ t 1 0 88 γ t 2 0 70 γ t 3 0 64 γ t 4 0 66 γ t 5 0 74 γ t 6 0 69 γ t 7 0 55 γ t 8 0 68 γ t 9 0 74 γ t 10 0 81 γ t 11 0 87 γ t 12 0 83 therefore q t 1 q t 5 q t 9 q t 10 q t 11 q t 12 are adopted as forecast factors and svr and grnn models are used to describe the following function q t f q t 1 q t 5 q t 9 q t 10 q t 11 q t 12 the high correlation between the flow with 11 and 12 month lag and the current flow implies the annual streamflow variation whereas the flow with 1 month lag usually has a relatively similar value to the current flow flows with a shorter lag such as 5 6 and 7 months imply seasonal fluctuations as well as long term changes from atmospheric circulation pearson correlation coefficient pcc autocorrelation function and partial autocorrelation function are used to analyze the streamflow time series the autocorrelation and partial autocorrelation patterns of the shigu streamflow are presented in fig 3 fig 3 a shows acf with respect to different lag numbers it is clear that the streamflow fluctuates and our results demonstrate significant autocorrelations at the time lags 1 5 6 7 11 and 12 months fig 3 b shows the pacf with respect to the lag number the time series exhibits significant partial autocorrelations at times lags 1 2 3 and 10 months the results from pcc analysis suggest that streamflow has a high correlation with streamflow at time lags 1 5 6 7 11 and 12 months which is highly similar to the results of acf four models gca svr gca grnn pcc acf svr and pacf svr are devised to determine the forecast factors the inputs of gca svr and gca grnn are streamflow with a lag of 1 5 9 10 11 12 months the inputs of the pcc acf svr model are streamflow with a lag of 1 5 6 7 11 12 months the inputs of pacf svr model are streamflow with a lag of 1 2 3 and 10 months continuous monthly streamflow data from 1970 to 2009 are used in shigu case study in supervised learning data sets are often divided into three sets namely the training set the testing set and the validation set when the sample size is small it is common to divide data into training and testing set and the cross validation method is used in our study data from1970 to 1999 are used for training the model which accounts for 75 of the total dataset a 4 fold cross validation method is applied the remaining 25 of data i e data from 2000 to 2009 are used for model validation the mean average percentage error mape of forecasting the proportion of errors that are less than 5 10 20 and 30 and the deterministic coefficient dc are shown in table 1 an accurate model has the mape metric value close to 0 dc value close to 1 and the dynamic range of error is small the mapes of gca svr and gca grnn are 13 and 17 respectively which indicates that gca svr performs better for proportion that error less than 5 10 20 and 30 gca svr is 24 52 76 and 91 gca grnn is 14 38 71 and 84 a large proportion of smaller error range implies that the errors are small the dc of gca svr and gca grnn are 0 86 and 0 82 respectively it demonstrates that learning model svr is superior to grnn in forecasting the streamflow of jinsha river when comparing gca svr with pcc acf svr and pacf svr svr model coupled with different input factors it is found that the forecast performance of gca svr is better than that of pcc acf svr and pacf svr 4 4 hybrid models based on stl decomposition as time series pre processing techniques are effective to improve the performance stl is used to decompose the original streamflow data into multiple sub series due to its advantages of being allowed to change over time and having the better robustness to the anomaly fig 4 shows that the observed streamflow time series contains a stationary seasonal component with a period of 12 months trend component increased significantly in the 1990s and decreased in 2000s under the combined effects of climate change and human activities four models gca svr gca stl svr and gca grnn gca stl grnn are proposed in this paper to forecast monthly streamflow of xiangjiaba for xiangjiaba hydrologic station monthly streamflow data from 1961 to 2008 the first 36 years are used to model calibration the rest 12 years are used to model evaluation hybrid models and corresponding input and output are shown in table 2 qt 1 qt 11 qt 12 represent original streamflow at 1 11 and 12 months ahead forecasting month q stl t 1 q stl t 11 and q stl t 12 represent sub sequence of streamflow decomposed by stl model inputs are selected using gca forecast results of grnn stl grnn svr and stl svr in test period are shown in figs 5 and 6 it can be seen that all models fit the observed streamflow well fig 7 provides the forecast ability comparison between a single model and the hybrid model it can be observed that hybrid models coupled with stl decomposition are better than models without decomposition however it is also obvious that models have better forecasts at conventional runoff samples for flood caused by heavy rain the above models are not so satisfying as more uncertainty exists in these extreme value table 3 gives error statistic results in both training period and test period an analysis shows that svr has a better streamflow forecast results than grnn stl decomposition methods improve the accuracy of prediction compared with the results of svr and grnn model stl svr is the best model for monthly streamflow forecast on jinsha river 4 5 real time error correction analysis we use the 3rd order ar model and its parameters are determined by minimizing the least square error function the order of the ar model is determined using the bayesian information criterion bic for stl svr forecast errors bics are 15 99 14 08 14 01 15 04 and 16 06 for orders of 1 2 3 4 and 5 respectively hence the 3rd order is used in our experiments errors of ar corrected svr stl svr grnn and stl grnn model are depicted in fig 8 as a comparison the multi model composition method is also used for the error correction weight parameters of svr and grnn model are 0 78 and 0 24 respectively results of the ar error correction method and multi model composition method in the test period are given in table 4 dc and rmse value indicate that the ar corrected models are better than the original svr and stl svr model for the grnn model and stl grnn model consistency measured with dc is a little decreased but whole errors measured with rmse are improved obviously ar corrected gca stl svr exhibits the best performance multi model composition method has no significant contribution in our study 4 6 comparison between ar corrected gca stl svr model and stochastic arima forecast model ar corrected gca stl svr model is compared with a stochastic autoregressive integrated moving average arima streamflow forecast model table 5 gives the average error standard deviation sd percentiles of 25 q1 50 q2 and 75 q3 the average error of ar corrected gca stl svr model is 47 with an sd of 1196 whereas the average error of autoregressive integrated moving average arima is 510 with an sd of 1753 it is clear that the error of our proposed method is much smaller and the spread of the error range is also small for the arima model the negative errors account for a large amount which means that the arima forecast tends to underestimate the true runoff it demonstrates that the ar corrected gca stl svr model exhibits a better performance for monthly streamflow forecast of jinsha river 5 conclusions the monthly runoff forecast in the jinsha river basin has received much attention in recent years due to the impact of the subtropical monsoon inner annual alterations of runoff are extremely complex and hard to be accurately forecasted most of the existing researches used machine learning model combined with time series decomposition to improve forecasting accuracy in this paper a more systematic forecasting method was researched we analyzed acf pacf and gca correlation coefficients to determine the forecast factors stl decomposition used to divide the original sequence into trends periodic items to understand the underlying trend of streamflow error real time correction post processing techniques were introduced to form a complete forecasting framework fourteen models were finally developed in this paper and ar corrected gca stl svr was regarded as the best model for streamflow forecast of jinsha river first four models gca svr acf svr pacf svr and gca grnn were proposed experiments with continuous monthly streamflow data in shigu shows that gca svr is superior to acf svr pacf svr and gca grnn the result proves the advantage of gca as widely used forecast factors selection approaches acf and pacf measure the linear correlation of variables with the drawback of ignoring the non linear relation stl was used to decompose the original streamflow data into multiple sub series due to its advantages of being allowed to change over time and having the better robustness to the anomaly the observed streamflow time series contains a stationary seasonal component with a period of 12 months an increased trend in the 1990s and a decreased trend in 2000s under the combined effect of climate change and human activities then another four models gca svr gca stl svr and gca grnn gca stl grnn were proposed to forecast monthly streamflow in the lower reach of the jinsha river forecast results show that hybrid models coupled with stl decomposition provide better forecasts than models without decomposition gca stl svr is more effective for the purpose of improving the precision of monthly streamflow forecast then the ar model and multi model composition were applied to correct the forecast error of gca svr gca stl svr gca grnn and gca stl grnn models dc and rmse value indicate that the ar corrected models are better than the original models without correction for the grnn model and stl grnn model consistency measured with dc is a little decreased but whole errors measured with rmse are improved obviously multi model composition method has no significant contribution in our study ar corrected gca stl svr model was developed as the streamflow forecast framework of jinsha river a stochastic autoregressive integrated moving average arima streamflow forecast model was implemented to evaluate the performance of the proposed ar corrected gca stl svr model it is clear that the error of our proposed method is much smaller and the spread of the error range is also small arima forecast tends to underestimate the true runoff it demonstrates that the ar corrected gca stl svr model exhibits a better performance for monthly streamflow forecast of jinsha river acknowledgments this work is supported by the national natural science foundation of china no 51809242 and special thanks are given to the china scholarship council s support and the anonymous reviewers and editors for their constructive comments 
6807,monthly streamflow time series are highly non linear how to improve forecast accuracy is a great challenge in hydrological studies a lot of research has been conducted to address the streamflow forecasting problem however few methods are developed to make a systematic research the objective of this study is to understand the underlying trend of streamflow so that a regression model can be developed to forecast the flow volume in this paper a hybrid streamflow forecast framework is proposed that integrates factor analysis time series decomposition data regression and error suppression correlation coefficients between the current streamflow and the streamflow with lags are analyzed using autocorrelation function acf partial autocorrelation function pacf and grey correlation analysis gca support vector regression svr and generalized regression neural network grnn models are integrated with seasonal and trend decomposition to make monthly streamflow forecast auto regression and multi model combination error correction methods are used to ensure the accuracy in our experiments the proposed method is compared with a stochastic autoregressive integrated moving average arima streamflow forecast model fourteen models are developed and the monthly streamflow data of shigu and xiangjiaba china from 1961 to 2009 are used to evaluate our proposed method our results demonstrate that the integrated model of grey correlation analysis seasonal trend decomposition procedure based on loess stl support vector regression gca stl svr exhibits an improved performance for monthly streamflow forecast the average error of the proposed model is reduced to less than one tenth in contrast to the state of the art method and the standard deviation is also reduced by more than 30 which implies a greater consistency keywords streamflow forecast regression factor analysis time series decomposition 1 introduction accurate streamflow forecasting is of significant importance for planning and management of water resources as well as early warning and mitigation of natural disasters such as droughts and floods yu et al 2018 nevertheless affected by complex factors including precipitation evaporation runoff yield and confluence topography and human activities it is still challenging to achieve accurate streamflow forecasting senthil kumar et al 2013 until now a large variety of streamflow forecast models have been proposed mainly classified as physical and data driven models physical models are good at providing insight into catchment processes while they have been criticized for being difficult to implement in contrast data driven models have minimum information requirements and rapid development times data driven stochastic models have been used for streamflow forecasting autoregressive integrated moving average models arima and its variants are widely used papacharalampous et al 2018 while stochastic models are often limited by assumptions of normality linearity and variable independence chen et al 2018 chen and singh 2018 the second data driven type machine learning shows a strong deep learning ability and extremely suitable for simulating the complex process over the past 50 years research on machine learning has evolved from the efforts of a handful of computer engineers mitchell 2006 yuan and abouelenien 2015 yuan et al 2018 artificial neural network ann is a widely used method for long term simulation and forecast aksoy and dahamsheh 2009 moeeni and bonakdari 2016 wu and chau 2010 wu et al 2009 but ann still has some intrinsic disadvantages such as slow convergence speed less generalizing performance arriving at a local minimum and over fitting problems support vector machine svm is based on the vc dimension theory and structural risk minimization of statistical learning cortes and vapnik 1995 it transforms the problem into a quadratic optimization problem theoretically which can get the globally optimal solution and solve the practical problems such as small sample nonlinear high dimension and local minimum smola and lkopf 2004 vapnik 2010 maity et al 2010 pointed out that svr machine learning approaches were more popular due to their inherent advantages over traditional modeling techniques kalteh 2015 employed genetic algorithm support vector regression ga svr models for forecasting monthly flow on two rivers and obtained good performance papacharalampous et al 2017 conducted large scale computational experiments to compare stochastic and ml methods regarding their multi step ahead forecasting properties and suggested that the ml methods exhibit a good performance an important step of ann and svr models is to determine the significant input variables bowden et al 2005a b some of which are correlated noisy and some input variables are less informative bowden et al 2005a chen et al 2013 grey correlation analysis gca evaluates the complex phenomena affected by many factors and a good metric to quantify the degree of association between the forecasting factors and the streamflow however in any streamflow forecast model there are three types of uncertainty caused by a number of factors input uncertainty model structure uncertainty and parameter uncertainty liu and gupta 2007 in order to reduce uncertainty and improve accuracy a proven method time series decomposition has been employed in lots of researches time series decomposition has the ability to analyze streamflow temporal and spatial variation and extracting useful information as much as possible kisi 2011 wavelet analysis mallat 1989 is developed on the basis of fourier analysis it is a local transformation of space and frequency by stretching and translating the signal can be multi scale analyzed therefore it is suitable for the analysis of non stationary hydrological time series guo et al 2011 kisi 2011 liu et al 2014 seasonal and trend decomposition using loess stl uses a locally weighted regression that enables processing of any type of seasonal variation data cleveland and cleveland 1990 rojo et al 2017 predicted airborne pollen series based on the seasonal and residual stochastic components of data series decomposed by using stl lafare et al 2016 used stl to understand groundwater behavior in the permo triassic sandstone aquifer different from time series decomposition real time error correction is a post process method to reduce uncertainty and improving accuracy the kalman filter updating method can reflect various hydrological and hydraulic flow fields by updating model input or parameters but it tends to require a long computational time wu et al 2012 wu et al 2012 tested the kalman filter with simple neural networks and autoregressive models and pointed out that the results are similar in addition the multi model combination approach advocates the synchronous use of the simulated discharges of a number of models to produce an overall integrated result which can be used as an alternative to that produced by a single model chen et al 2015 shamseldin et al 1997 first introduced the multi model combination concept into the hydrologic field since then there have been several more studies which have dealt with a multi modal combination of hydrological models coulibaly et al 2005 wu et al 2015 xiong et al 2001 wu et al 2015 proposed three coupling forecast methods which included real time correction combination forecast method combination forecast real time correction method for the purpose of improving the precision of flood forecasting many studies have been done to improve streamflow forecast accuracy as described above there exist comparative studies for example comparison of driving models kalteh 2015 moeeni and bonakdari 2016 predicting factor screening methods chen et al 2013 multiple sequence decomposition guo et al 2011 kisi and cimen 2011 liu et al 2014 and multiple error correction methods chen et al 2015 wu et al 2015 a model requires a systematic integration of many components including factor analysis time series decomposition data regression and error suppression which enables accurate modeling of a hydro system in this paper ann and support vector regression svr are employed as regression models svr is more accurate but less efficient than the least squares support vector machine lssvr suykens et al 2002 because svr solves a convex quadratic programming cqp problem to determine the regression and lssvr just solves a set of reformulated linear equations generalized regression neural network is relatively simple in structure and training of network there is no need to estimate the number of hidden layers and the number of hidden cells in advance and there is an advantage of global convergence the autocorrelation function acf partial autocorrelation function pacf and grey correlation analysis gca are candidate predictors screening methods we develop hybrid models of svr and grnn for monthly streamflow forecast that couples with seasonal and trend decomposition the error correction approaches are used to enhance the performance of the proposed hybrid models the following article structure is organized as follows section 2 is the theory and methods section 3 is a description of the study area and data section 4 presents a case study section 5 concludes this paper with a summary 2 methods the proposed streamflow forecasting framework consists of forecast factors selection time series decomposition model learning and real time error correction correlation coefficients between the current streamflow and the streamflow with n month lag are analyzed using acf pacf and gca the antecedent streamflow with a greater correlation is included in the input sets support vector regression svr and generalized regression neural network models are integrated with seasonal and trend decomposition stl to forecast the monthly streamflow 2 1 support vector machine support vector machine svm which is known as classification and then extended for regression was proposed by vapnik 1995 svm is built based on the principle of the structural risk minimization rather than the empirical risk minimization support vector regression svr is used to solve the problem of regression with svm the following is a brief description of svr suppose n samples data for training are x i d i i n xi is the input vector and di means desired output svr results in 1 y w φ x b where φ x is a non linear mapping w is a hyperplane and b is offset a penalty function is used in svr 2 d i y i ε n o t a l l o c a t i n g a p e n a l t y d i y i ε a l l o c a t i n g a p e n a l t y when the estimated value is within the ε insensitive tube the loss value will be zero parameters of the regression function can be acquired by minimizing the following objective function 3 min 1 2 w 2 c i 1 n l ε y i d i 4 l ε y i d i max 0 y i d i ε where c represents the regularized constant that weighing the model complexity and the empirical error a relative importance of the empirical risk will increase when the value of c increases the slack variables ξ and ξ are introduced in eq 3 for the existence of fitting errors the optimization problem of svr will be as 5 min 1 2 w 2 c i 1 n ξ i ξ i subject to w φ x i b d i ε ξ i d i w t φ x i b ε ξ i ξ 0 ξ 0 then use lagrange multipliers to solve the above optimization problem in its dual form 6 max i 1 n α i α i y i ε i 1 n α i α i 1 2 i 1 j 1 n α i α i α j α j k x i x j subject to 0 α i c 0 α i c i 1 n α i α i 0 7 k x i x j φ x i t φ x j k x i x j is a nonlinear kernel function which can map the lower dimension input into a higher dimension linear space radial basis kernel function is used in this study 2 2 generalized regression neural network generalized regression neural network zaknich 2013 is a special radial basis function neural network compared with the widely used bp neural network grnn has the following advantages 1 structure of grnn neural network is relatively simple in addition to the input and output layers there are only two hidden layers the pattern layer and the summation layer and the number of hidden neurons in the pattern layer is the same as the number of training samples 2 training of network is simple the network training is completed once the training sample through the hidden layer 3 there is no need to estimate the number of hidden layers and the number of hidden cells in advance 4 global convergence of grnn the theoretical basis of the grnn neural network is nonlinear regression analysis let the joint probability density function of the random variable x and y be f x y when the observed value of x is x the conditional expectation of y for x is 8 y e y x yf x y d y f x y d y set the sample data set is x i y i i 1 2 n the dimension of x i is m the nonparametric estimates of probability density function f x y are as follows 9 f x y 1 n 2 π m 1 2 σ m 1 i 1 n exp x x i t x x i 2 σ 2 exp y y i 2 2 σ 2 combining eqs 9 and 8 to get 10 y e y x i 1 n exp x x i t x x i 2 σ 2 y exp y y i 2 2 σ 2 d y i 1 n exp x x i t x x i 2 σ 2 exp y y i 2 2 σ 2 d y hence the estimation becomes 11 y i 1 n y i exp x x i t x x i 2 σ 2 i 1 n exp x x i t x x i 2 σ 2 when the observed value of x is x the conditional expectation estimate of y for x is the weighted average of all sample observations yi the weight of yi is exp x x i t x x i 2 σ 2 the smoothing factor σ needs to be optimized for which we adopt cross validation 2 3 grey correlation analysis grey system theory was proposed in the 1980s based on the mathematical theory of systems engineering ju long 1982 since then the theory has become quite popular with its ability to deal with the systems that have partially unknown parameters as a superiority to conventional statistical models grey models require only a limited amount of data to estimate the behavior of unknown systems grey correlation analysis is an important part of grey system theory grey correlation analysis method can evaluate the complex phenomena affected by many factors from the overall concept in this paper we introduce the grey correlation analysis to quantify the degree of association between the primary forecasting factor and the predicted runoff the calculation process of grey correlation analysis is as follows let x 0 x 0 1 x 0 n be the system characteristic behavior sequence x i x i 1 x i n and i 1 2 m is the relevant factor sequence first of all to perform dimensionless processing on time sequence and the initial values of each sequence are as follows 12 x i x i x i 1 x i 1 x i 2 x i n where i 1 2 m let δ i k x 0 k x i k the difference sequence of the initial value is 13 δ i δ i 1 δ i 2 δ i n let m max i max k δ i k m min i min k δ i k the correlation coefficient between the target variable and related factors at each time is calculated as follows 14 γ 0 i k m ξ m δ i k ξ m where ξ 0 1 k 1 2 n and i 1 2 m average the correlation coefficients at each moment to systematically compare the degree of association between the target sequence and the correlation factor sequence the correlation formula is as follows 15 γ 0 i 1 n k 1 n γ 0 i k 2 4 stl decomposition seasonal and trend decomposition using loess stl uses the robust local weighted regression as a smoothing method to decompose the time series into seasonal trend and residual items 16 y v t r e n d v s e a s o n a l v r e s i d u a l v where y v is the time series observed value of the v period the trend component tren d v is considered low frequency seasonal components seasona l v is considered to be high frequency changes caused by seasonal interference the remaining amount residua l v is a random component loess is a local polynomial regression a common method for smoothing two dimensional scatterplots the regression is done using the weighted least squares method that is the closer the data is to the estimated point the greater the weight finally the local regression model is used to estimate the value of the response variable in this way the whole fitting curve is obtained by the point by point operation loess is a nonparametric learning method defined as follows 17 j θ i 1 m w i y i θ t f i 2 where f is the point to be estimated f i is the sample point w i is the weight w i e f i f 2 2 k 2 the core of the stl algorithm is the iterative process of loess the iterative decomposition process is as follows 1 set the initial value k 0 t v k 0 2 remove trend items y v t v k 3 the loess smoothing is performed on the subsequence to obtain the time series c v k 1 4 perform three times moving average on c v k 1 for lengths of n p n p 3 perform a loess process to get the time series l v k 1 remove periodic differences 5 remove trend items s v k 1 c v k 1 l v k 1 6 remove season items y v s v k 1 7 the loess smoothing is performed on the subsequence y v s v k 1 to obtain the time series t v k 1 8 check whether t v k 1 convergence if convergence s v s v k 1 t v k 1 r v y v s v t v if not repeat the process 2 8 2 5 real time error correction 2 5 1 ar error correction method the discrepancy between the model predicted discharge and the actually observed past discharge is defined as an error which can be used as information for correction if this error signal has a correlation it can probably be used for improved prediction with a time series model of the error signal an improved discharge forecast can be made by adding the error term to the previous model results in this study the error term was estimated using an autoregressive ar model which can be expressed as 18 e t k 1 p θ i e t k ξ where e is the streamflow forecasting error time series p represents the order of the autoregressive model θ i are the parameters of the autoregressive model and ξ is a pure white noise sequence having variance σ 2 order selection criteria were used to determine the appropriate order 2 5 2 multi model composition method estimates of n streamflow forecast models for the t th period of time is q it i 1 2 n a combined estimate q ct is defined 19 q ct i 1 n w i q it ξ t where wi is the weight assigned to the i th model i 1 n w i 1 and ξ t is the combination error term in order to obtain the weights wi an objective function is described as follows 20 mine q ct q obs t 2 m i n e i 1 n w i q it q obs t 2 and the lagrange multiplier is used to solve the above problem 3 study area and data our study takes the jinsha river as the study area the jinsha river is located in the upper reaches of the yangtze river china with the basin area of 473 200 km2 accounting for 26 of the yangtze river basin area it has a total length of 3479 km a natural drop of 5100 m it is rich in hydropower resources and plays a vital role in economic development and ecological environmental conservation of china twenty five hydropower dams in the jinsha watershed are and will be constructed which take the responsibility of flood control agricultural hydroelectric power generation and municipal and industrial water supply with the completion of these hydropower dams the jinsha river hydropower resources are effectively developed and utilized discharge forecasting is significant to the optimal operation of these dams this study focuses on the shigu and xiangjiaba gauging stations which are hydrological control station of the upper reach and the lower reach of jinsha river respectively a schematic of the jinsha river and the gauging stations is given in fig 1 we obtained the available quality controlled and partially infilled daily streamflow m3 s data at the xiangjiaba 1961 2008 and shigu 1970 2009 hydrologic stations provided by the yangtze river waterway bureau china monthly streamflow data needed in this research were aggregated from daily data 4 case study 4 1 frameworks of proposed models the steps of our proposed streamflow forecast framework are listed as follows step 1 acf pacf and gca are used to select forecast factors the correlation coefficients between the current streamflow and the streamflow with n month lag is calculated using acf pacf and gca each antecedent monthly streamflow has three correlation coefficient values the antecedent streamflow with the higher acf correlation value is included to get an acf input set similarly we get pacf input set and gca input set with the higher pacf and gca value support vector machine svr and generalized regression neural network are used as regression models the statistical indicators evaluating the accuracy of prediction are mean average percentage error mape a proportion that errors less than 5 10 20 30 and deterministic coefficient dc continuous monthly streamflow data from 1970 to 2009 are used in shigu case study then the better input set can be determined by their forecast performances then the better input set can be determined by their forecast performances step 2 on the basis of step 1 we developed hybrid svr and grnn monthly streamflow forecast models coupling with seasonal and trend decomposition methods stl svr and grnn are used as the model the better method proved in step 1 is used to make forecast factors selection the original sequences are decomposed into multiple sub series by time series pre processing techniques stl decomposition four models svr stl svr and grnn stl grnn are built to forecast monthly streamflow of xiangjiaba the performances of the models are compared using the root mean square error rmse and deterministic coefficient dc step 3 two real time error correction methods the ar model and multi model combination method are used to enhance the performance a 3 order ar model is used for error correction the parameters of the ar model are optimized according to the least square method as a comparison the multi model composition method is also used for the error correction we train svr and grnn models the framework is illustrated in fig 2 4 2 performance metrics the statistical indicators evaluating the accuracy of prediction are mean average percentage error mare the proportion that errors less than 5 10 20 30 and deterministic coefficient dc and the root mean square error rmse mape is calculated according to eq 21 an accurate model has the mape metric value close to 0 rmse is a frequently used measure of the differences between values predicted and the values actually observed rmse represents the sample standard deviation of the differences between predicted values and observed values rmse is calculated according to eq 22 dc is the proportion of the variance in the dependent variable that is predictable from the independent variables it provides a measure of the quality of outcomes replicated by the model based on the proportion of total variation of outcomes explained by the model dc is calculated according to eq 23 21 mape 1 n i 1 n y i obs y i est y i obs 100 22 rmse 1 n i 1 n y i obs y i est 2 23 dc 1 i 1 n y i y i 2 i 1 n y i y 2 4 3 forecast factor selection forecasting factor and regression models are essential for developing a streamflow forecast framework in this part the svr model and grnn model two machine learning method are built for data simulation the autocorrelation function the partial autocorrelation function and grey correlation analysis are introduced to quantify the correlation degree between the streamflow and potential forecasting factors it is an effective forecast method by using the variable s own historical records to make an estimation if q t is the streamflow to be forecasted select antecedent streamflow q t 1 q t 2 q t i q t 12 as alternative factors where q t i means streamflow i month ahead of forecast month the grey correlation degree between the alternative factors q t i and q t is calculated results are as follows γ t 1 0 88 γ t 2 0 70 γ t 3 0 64 γ t 4 0 66 γ t 5 0 74 γ t 6 0 69 γ t 7 0 55 γ t 8 0 68 γ t 9 0 74 γ t 10 0 81 γ t 11 0 87 γ t 12 0 83 therefore q t 1 q t 5 q t 9 q t 10 q t 11 q t 12 are adopted as forecast factors and svr and grnn models are used to describe the following function q t f q t 1 q t 5 q t 9 q t 10 q t 11 q t 12 the high correlation between the flow with 11 and 12 month lag and the current flow implies the annual streamflow variation whereas the flow with 1 month lag usually has a relatively similar value to the current flow flows with a shorter lag such as 5 6 and 7 months imply seasonal fluctuations as well as long term changes from atmospheric circulation pearson correlation coefficient pcc autocorrelation function and partial autocorrelation function are used to analyze the streamflow time series the autocorrelation and partial autocorrelation patterns of the shigu streamflow are presented in fig 3 fig 3 a shows acf with respect to different lag numbers it is clear that the streamflow fluctuates and our results demonstrate significant autocorrelations at the time lags 1 5 6 7 11 and 12 months fig 3 b shows the pacf with respect to the lag number the time series exhibits significant partial autocorrelations at times lags 1 2 3 and 10 months the results from pcc analysis suggest that streamflow has a high correlation with streamflow at time lags 1 5 6 7 11 and 12 months which is highly similar to the results of acf four models gca svr gca grnn pcc acf svr and pacf svr are devised to determine the forecast factors the inputs of gca svr and gca grnn are streamflow with a lag of 1 5 9 10 11 12 months the inputs of the pcc acf svr model are streamflow with a lag of 1 5 6 7 11 12 months the inputs of pacf svr model are streamflow with a lag of 1 2 3 and 10 months continuous monthly streamflow data from 1970 to 2009 are used in shigu case study in supervised learning data sets are often divided into three sets namely the training set the testing set and the validation set when the sample size is small it is common to divide data into training and testing set and the cross validation method is used in our study data from1970 to 1999 are used for training the model which accounts for 75 of the total dataset a 4 fold cross validation method is applied the remaining 25 of data i e data from 2000 to 2009 are used for model validation the mean average percentage error mape of forecasting the proportion of errors that are less than 5 10 20 and 30 and the deterministic coefficient dc are shown in table 1 an accurate model has the mape metric value close to 0 dc value close to 1 and the dynamic range of error is small the mapes of gca svr and gca grnn are 13 and 17 respectively which indicates that gca svr performs better for proportion that error less than 5 10 20 and 30 gca svr is 24 52 76 and 91 gca grnn is 14 38 71 and 84 a large proportion of smaller error range implies that the errors are small the dc of gca svr and gca grnn are 0 86 and 0 82 respectively it demonstrates that learning model svr is superior to grnn in forecasting the streamflow of jinsha river when comparing gca svr with pcc acf svr and pacf svr svr model coupled with different input factors it is found that the forecast performance of gca svr is better than that of pcc acf svr and pacf svr 4 4 hybrid models based on stl decomposition as time series pre processing techniques are effective to improve the performance stl is used to decompose the original streamflow data into multiple sub series due to its advantages of being allowed to change over time and having the better robustness to the anomaly fig 4 shows that the observed streamflow time series contains a stationary seasonal component with a period of 12 months trend component increased significantly in the 1990s and decreased in 2000s under the combined effects of climate change and human activities four models gca svr gca stl svr and gca grnn gca stl grnn are proposed in this paper to forecast monthly streamflow of xiangjiaba for xiangjiaba hydrologic station monthly streamflow data from 1961 to 2008 the first 36 years are used to model calibration the rest 12 years are used to model evaluation hybrid models and corresponding input and output are shown in table 2 qt 1 qt 11 qt 12 represent original streamflow at 1 11 and 12 months ahead forecasting month q stl t 1 q stl t 11 and q stl t 12 represent sub sequence of streamflow decomposed by stl model inputs are selected using gca forecast results of grnn stl grnn svr and stl svr in test period are shown in figs 5 and 6 it can be seen that all models fit the observed streamflow well fig 7 provides the forecast ability comparison between a single model and the hybrid model it can be observed that hybrid models coupled with stl decomposition are better than models without decomposition however it is also obvious that models have better forecasts at conventional runoff samples for flood caused by heavy rain the above models are not so satisfying as more uncertainty exists in these extreme value table 3 gives error statistic results in both training period and test period an analysis shows that svr has a better streamflow forecast results than grnn stl decomposition methods improve the accuracy of prediction compared with the results of svr and grnn model stl svr is the best model for monthly streamflow forecast on jinsha river 4 5 real time error correction analysis we use the 3rd order ar model and its parameters are determined by minimizing the least square error function the order of the ar model is determined using the bayesian information criterion bic for stl svr forecast errors bics are 15 99 14 08 14 01 15 04 and 16 06 for orders of 1 2 3 4 and 5 respectively hence the 3rd order is used in our experiments errors of ar corrected svr stl svr grnn and stl grnn model are depicted in fig 8 as a comparison the multi model composition method is also used for the error correction weight parameters of svr and grnn model are 0 78 and 0 24 respectively results of the ar error correction method and multi model composition method in the test period are given in table 4 dc and rmse value indicate that the ar corrected models are better than the original svr and stl svr model for the grnn model and stl grnn model consistency measured with dc is a little decreased but whole errors measured with rmse are improved obviously ar corrected gca stl svr exhibits the best performance multi model composition method has no significant contribution in our study 4 6 comparison between ar corrected gca stl svr model and stochastic arima forecast model ar corrected gca stl svr model is compared with a stochastic autoregressive integrated moving average arima streamflow forecast model table 5 gives the average error standard deviation sd percentiles of 25 q1 50 q2 and 75 q3 the average error of ar corrected gca stl svr model is 47 with an sd of 1196 whereas the average error of autoregressive integrated moving average arima is 510 with an sd of 1753 it is clear that the error of our proposed method is much smaller and the spread of the error range is also small for the arima model the negative errors account for a large amount which means that the arima forecast tends to underestimate the true runoff it demonstrates that the ar corrected gca stl svr model exhibits a better performance for monthly streamflow forecast of jinsha river 5 conclusions the monthly runoff forecast in the jinsha river basin has received much attention in recent years due to the impact of the subtropical monsoon inner annual alterations of runoff are extremely complex and hard to be accurately forecasted most of the existing researches used machine learning model combined with time series decomposition to improve forecasting accuracy in this paper a more systematic forecasting method was researched we analyzed acf pacf and gca correlation coefficients to determine the forecast factors stl decomposition used to divide the original sequence into trends periodic items to understand the underlying trend of streamflow error real time correction post processing techniques were introduced to form a complete forecasting framework fourteen models were finally developed in this paper and ar corrected gca stl svr was regarded as the best model for streamflow forecast of jinsha river first four models gca svr acf svr pacf svr and gca grnn were proposed experiments with continuous monthly streamflow data in shigu shows that gca svr is superior to acf svr pacf svr and gca grnn the result proves the advantage of gca as widely used forecast factors selection approaches acf and pacf measure the linear correlation of variables with the drawback of ignoring the non linear relation stl was used to decompose the original streamflow data into multiple sub series due to its advantages of being allowed to change over time and having the better robustness to the anomaly the observed streamflow time series contains a stationary seasonal component with a period of 12 months an increased trend in the 1990s and a decreased trend in 2000s under the combined effect of climate change and human activities then another four models gca svr gca stl svr and gca grnn gca stl grnn were proposed to forecast monthly streamflow in the lower reach of the jinsha river forecast results show that hybrid models coupled with stl decomposition provide better forecasts than models without decomposition gca stl svr is more effective for the purpose of improving the precision of monthly streamflow forecast then the ar model and multi model composition were applied to correct the forecast error of gca svr gca stl svr gca grnn and gca stl grnn models dc and rmse value indicate that the ar corrected models are better than the original models without correction for the grnn model and stl grnn model consistency measured with dc is a little decreased but whole errors measured with rmse are improved obviously multi model composition method has no significant contribution in our study ar corrected gca stl svr model was developed as the streamflow forecast framework of jinsha river a stochastic autoregressive integrated moving average arima streamflow forecast model was implemented to evaluate the performance of the proposed ar corrected gca stl svr model it is clear that the error of our proposed method is much smaller and the spread of the error range is also small arima forecast tends to underestimate the true runoff it demonstrates that the ar corrected gca stl svr model exhibits a better performance for monthly streamflow forecast of jinsha river acknowledgments this work is supported by the national natural science foundation of china no 51809242 and special thanks are given to the china scholarship council s support and the anonymous reviewers and editors for their constructive comments 
6808,this study employs a systematic literature review to investigate how insurance data can be applied in the analysis of surface water flood events the study firstly identifies the variables expressing insurance data and those explaining them together with their interrelationships damage variables may be expressed as either monetary based or number of claims based explaining variables may be subdivided into four categories meteorological geographic demographic and property building based most of the common and under researched combinations of these variables and their expression are discussed secondly a comparative analysis is presented of current models highlighting their differences and similarities the study demonstrates that the scope and approach of the models varies in relation to scale the coverage and period of incorporated insurance claims and the methods used for model development and validation thirdly the study proposes a generic and adaptable framework constructed from an aggregation of information contained in relevant literature to define a workflow for model development and future deployment the study concludes with a discussion of the challenges facing model development and opportunities for deployment keywords surface water floods pluvial insurance data insurance claims flood damage flood risk 1 introduction flooding is a common environmental hazard that endangers the physical economic and social environment barredo 2009 falconer et al 2009 kron 2005 pluvial flooding is triggered by accumulated rainfall that results in overland water flow and ponding that cannot be drained away either by natural or artificial systems bernet et al 2017 falconer et al 2009 hurford et al 2012 surface water flooding swf represents a combination of pluvial flooding stormwater flooding sewer flooding flooding from small open channel and culverted urban watercourses and overland flows from groundwater springs bernet et al 2017 falconer et al 2009 hurford et al 2012 kaźmierczak and cavan 2011 the term swf can be regarded as the optimal general definition of rainfall related pluvial flooding events bernet et al 2017 economic loss resulting from swfs including both tangible and intangible consequences has increased dramatically in recent decades and is expected to do so in the future as reported for several countries in europe and as well as usa and canada barredo et al 2012 bernet et al 2017 bouwer 2013 cheng et al 2012 kousky and michel kerjan 2017 kron 2005 moncoulon et al 2016 wobus et al 2014 zhou et al 2013 on the one hand patterns and intensities of rainfall events are expected to alter in response to climate change leading to more frequent and severe flooding events cheng et al 2012 falconer et al 2009 on the other a large body of research currently points towards increasing concentration densities of valuable assets due to urbanization and an expanding population as the principle cause of the increasing cost of natural disasters barredo et al 2012 barredo 2009 bernet et al 2017 bouwer 2011 bouwer 2013 kreibich and thieken 2008 spekkers et al 2015 consequently risk mapping and risk assessment are applied as methodologies for the identification of risk influencing factors and the evaluation of risk mitigating measures the term risk in this context is commonly expressed as the multiplication of the factors hazard vulnerability and exposure crichton 1999 field et al 2012 ipcc 2012 koks et al 2015 kron 2005 hazard refers to threatening natural events such as intense rainfall expressed in terms of probability of occurrence vulnerability refers to the capacity or inability of a society to deal with the hazard exposure refers to that of the human population involved combined with the value of the assets subject to the hazard crichton 1999 koks et al 2015 kron 2005 an understanding of each component of this risk triangle is required as a basis for analysing how risk due to flooding can be reduced most effectively research over the past decades has mostly focused on improving our understanding of the hazard component grahn and nyberg 2017 kaźmierczak and cavan 2011 koks et al 2015 mechler and bouwer 2015 mechler et al 2014 while vulnerability and exposure have started to gain attention only during the past decade in the field of flood risk assessment cutter et al 2013 koks et al 2015 lujala et al 2014 rød et al 2015 hazard is a very uncertain phenomenon which cannot be predicted the ranges of levels of vulnerability and exposure are very wide and constantly changing for this reason it is important to develop policies that are able to address a range of different outcomes falconer et al 2009 kron 2005 to achieve this it is important to understand the fundamentals of flood damage data and its possible causes or influences insurance databases represent a potential source of flood damage data consequently analytical research has been carried out in recent years to apply insurance data as a proxy for the analysis of the impact of flooding events bernet et al 2017 cortes et al 2018 grahn and nyberg 2017 sorensen and mobini 2017 spekkers et al 2015 spekkers et al 2013 torgersen et al 2015 torgersen et al 2017 zhou et al 2013 the outcomes of these studies have included an understanding and ranking of the variables that can explain damage data the development of models that can predict the likelihood of an swf event and the implementation of said models flood risk assessment frameworks these studies share a common objective the development of models that explain insurance data in terms of other rainfall related geographic and socio economic factors however the models differ in their identification and expression of the variables used their interrelationships the methods used to develop and validate the models and their further implementation and deployment the studies have concluded that such models can provide an insight into the relationship between insurance data and key explaining variables however much of the statistical variance is left unexplained emphasising the need to increase the availability completeness and reliability of relevant data on one hand and to consider alternative ways of expressing selected variables as well as the inclusion of other explaining variables and their interrelationships and the methods used to develop the models on the other hand in the light of this an aggregation and synthesis of the relevant literature is required in order to compare the similarities and dissimilarities between these studies and thereafter deliver recommendations for future application based on current best practice the aim of this study carried out in the form of a systematic literature review is to look into how insurance data can be used to analyse swf events it has the following objectives to analyse the historical development of the use of insurance data for analysing swf events section 3 1 to identify the variables that express insurance data and those explaining them section 3 2 to identify current interrelationships between insurance data and other explaining variables section 3 3 to provide a current overview of existing models and analyse their differences and similarities section 3 4 to propose a generic framework based on an aggregation of current models and methods as a basis for a discussion of the challenges related to model development as well as opportunities for their deployment section 3 5 section 1 describes the motivation aims and scope of the study the methodology is then presented in detail in section 2 the results and discussions addressing the aforementioned objectives are presented in section 3 and conclusions are set out in section 4 2 methodology the literature review presented in this study is based on an established research methodology booth et al 2011 that ensures a comprehensive search process and systematic review of the relevant literature the methodology originates from the field of health and social sciences but its principles are applicable to other fields of study the approach provides a tool capable of providing a transparent and reproducible research synthesis thus offering greater clarity internal validity and audibility booth et al 2011 the first step in the review process is to define the scope of research that directs focus on the research question booth et al 2011 in the present study the research question opts to identify how can the insurance data be used to derive models explaining swf events in this study the definition of an swf as set out in references bernet et al 2017 falconer et al 2009 hurford et al 2012 kaźmierczak and cavan 2011 is used because it covers the different types of floods described in the studied literature the cimo framework petticrew and roberts 2008 is used to define the key concepts of the research process table 1 the research question is identified as follows how o do we use insurance data i to analyse m surface water flooding events c the keywords presented in table 2 were identified based on the titles abstracts and keywords provided in the literature bernet et al 2017 cortes et al 2018 grahn and nyberg 2017 sorensen and mobini 2017 spekkers et al 2015 spekkers et al 2013 torgersen et al 2015 torgersen et al 2017 zhou et al 2013 following a preliminary screening first step using the electronic database scopus and google scholar the search scheme and exclusion criteria are shown in fig 1 and table 3 three electronic databases of peer reviewed literature were used for the final screening second step scopus web of science and engineering village are all relevant sources of information in this research area aghaei chadegani et al 2013 falagas et al 2008 jacso 2005 the keywords operators and nesting combinations are presented in table 2 the keywords were applied at title abstract keywords topic level the last search was performed on 25 april 2018 all years of publication were included in the search process literature screening based on a full content cross referencing methodology and author searching was used to check for additional sources in cases of similar studies being included in different literature sources priority was assigned to the most recent publication the final number of selected publications was 34 subsequently a data extraction process booth et al 2011 was developed to retrieve and code relevant variables and elements in order to facilitate comparison and identify patterns themes or trends table 4 shows the subgroups of data extraction that help to structure the literature review results described in the following sections 3 results and discussion 3 1 historical development graphical representation fig 2 presents an historical development of the selected literature that has addressed the relationship between insurance claims and swfs the literature is subdivided into publications that have modelled this relationship solid line and others that have simply provided relevant research results and discussions dotted line despite the fact that the potential of applying this relationship has been discussed over many decades results show that it has only been in recent years that an increasing number of publications have appeared that actively develop a model nevertheless these studies cite a limited number of cities and countries including canada denmark france germany the netherlands norway portugal spain sweden switzerland the united kingdom and the usa in some cases more than one study per country is identified some investigating different regions and others cited by either similar or different authors the increasing number of publications produced during the past decade suggests an increased interest in the use of insurance claim data to assess flood risk nevertheless only very few countries have applied such methodologies thus highlighting the potential for wider application of the approach consequently an aggregation of relevant literature may provide the basis for further development and application of these models and for this reason a review of current models and their characteristics including the variables used and their combinations should be made available 3 2 identification and categorization of the variables used to develop the relationship between damage and explaining variables a variety of different definitions in relation to the variables used to define insurance data and variables used to explain them are identified in the literature the term damage is a dependent or response variable that expresses the nature of insurance data the term explaining is a damage influencing or independent variable used to account for or explain the damage variable in the following the terms damage variable and explaining variable are used fig 3 presents a schematic overview in the form of a bubble map of the variables identified and used in these studies the literature is indexed with numbers according to table 6 the damage variables are grouped in two main categories based on their expression as follows 1 monetary based which express the damage variable in terms of a currency value they include values expressed both as a whole or as a fraction of or relative to other parameters or variables 2 number of claims based which express the damage variable in terms of the number of policies claims that are dependent on other parameters or variables the explaining variables may be subdivided into four categories based on their characteristics and their role in the risk triangle hazard exposure vulnerability the literature shows sometimes contrasting views when assigning different variables to one of the roles within the risk triangle an example is the discussion of the variable density of built environment which has been considered both as part of vulnerability and exposure koks et al 2015 the following categories are defined 1 meteorology based variables m that describe physical atmospheric or natural extreme weather events such as intense rainfall they may or do cause influence or trigger the occurrence of flood events this category belongs to the hazard risk component 2 geographic based variables g that describe the spatial characteristics and parameters of the area under investigation they may be expressed as single parameters or be combined in the form of a map describing terrain characteristics these in turn may influence the degree of the hazard if an swf event occurs and the coping mechanisms of the system this category belongs to both the exposure and vulnerability risk components 3 demographic based variables d that state the inventory of elements and assets in a given area in which the swf event may occur such variables may be used to aggregate damage variables this category belongs to both the exposure and vulnerability risk components 4 building property based variables b that describe susceptible at risk elements and the system s ability to cope with the hazard they are directly related to the relevant damage variable this category belongs to both the exposure and vulnerability risk components 3 2 1 damage variables there are two ways of expressing the damage variables 1 total number of claims and 2 claim size in both cases these values can be aggregated with other parameters or explaining variables and be expressed as relative values the review identifies a variety of ways of expression and conclusions in terms of their applications findings zhou et al 2013 show that rainfall data cannot be used to explain variation in individual cost per claim however such data may be a suitable indicator of overall costs per day in contrast in spekkers et al 2014 the cost per claim term was inadequate to express the damage variable while claim frequency appeared to provide more satisfactory results the latter sounds plausible since cost per claim is related to real estate value the cost of cleaning and the economic value of the insurance holders belongings consequently high income neighbourhoods may appear to be more easily flooded regardless of the real probability of swf events in such areas sorensen and mobini 2017 nevertheless the total number of claims term may be biased if it is not aggregated or expressed in a relative manner a neighbourhood containing a high building density or a high percentage of insured buildings will likely result in a larger total number of claims than an area that is less populated or less densely developed this observation underlies the importance of using aggregated or relative values bernet et al 2017 spekkers et al 2014 however relative values can also be misleading for example a neighbourhood containing only one storey buildings may seem to be more easily flooded than a similar neighbourhood with the same number of multi storey buildings for this reason the use of suitable parameters or variables that aggregate the damage variable may be more useful than using the cost of claims or total number of claims terms nevertheless a combination of both claim size and total number of claims aggregated by the use of different parameters or in terms of total values is proposed in order to fully exploit the relationship 3 2 2 explaining variables this review presents several variables that are used to explain the damage variables focus is directed mainly on the meteorological category since variables describing rainfall are considered to be the main causes of swf events however a variable that has not been identified is wind driven rain which may damage certain parts of a building that are not accessible to vertical rainfall similarly snow and hailstorms may also contribute to damages paid in response to insurance claims hanak and korytarova 2014 moreover and similar to the damage variable results may be dependent on how the rainfall data are aggregated for example in grahn and nyberg 2017 the intensity variable which takes both the duration of the rainfall and the aggregated volume of rain into account exhibited a statistically significant effect on flood related damages while the aggregated volume of rain alone did not this illustrates the importance of applying meteorological information that reflects the rainfall phenomenon in a temporal perspective despite the fact that rainfall may be the main cause of swf events previous research has concluded that use of this variable alone is not sufficient to explain observed variance cortes et al 2018 spekkers et al 2015 zhou et al 2013 thus underlying the importance of considering the impacts of other categories different studies include different variables within the four identified categories of explaining variables however almost none of these studies include variables from each of these categories in the same analysis the absence of key variables may explain the large unexplained variance the selection of variables also depends on the scale of the investigation macro meso or micro referring to city country neighbourhood and building scale respectively different variables are associated with different scales meaning that different variables and aggregations can be used to explain a given damage variable based on the scale of the latter at microscales detailed information regarding a given property may be very relevant moncoulon et al 2014 on the other hand the relationships between socio economic variables and the damage occurred may be weaker at district level compared to that of individual households especially where such districts are heterogeneous an example of this is in situations where there is a large variance in household incomes spekkers et al 2014 similarly the type of insurance database plays an important role in the selection of variables different socio economic variables may play different roles when insurance claims are subdivided between property and movable assets 3 2 3 other variables this review indicates that many variables have been used and screened as important when explaining the damage variable in addition to the common variables used for similar purposes in different studies special variables are also noted the latter include a binary variable depending on whether the event occurred during the day shift or night shift grahn and nyberg 2017 urban exposure grahn and nyberg 2017 or the permeability of surfaces torgersen et al 2017 property value spekkers et al 2014 or socio economic variables such as household income age and education of breadwinner or fraction of homeowner spekkers et al 2014 others include urban drainage system properties drainage capacity age of infrastructure percentage of surface water level of urbanization socio economic indices household income and property value and district related parameters percentages of low rise and high rise buildings percentage impervious surface spekkers et al 2013 as well as the weather conditions prevailing during preceding days torgersen et al 2015 other variables have been identified as influential from other studies although they are not used in any of these publications they include green spaces koks et al 2015 self protective behaviour grothmann and reusswig 2006 precautions external response and early warning merz et al 2010 as well as building condition yazdani et al 2010 a systematic map of all the variables that may affect flood occurrence may be useful for the future application of similar research moreover damage variables are also influenced by a complexity of factors associated with the social vulnerability of residents and communities to surface water flooding such as age of residents willingness to pay for insurance presence during occurrence of the event and so on vulnerability may be a complex phenomenon to quantify since it is represented as a composite of other economic social cultural and psychological factors that are themselves difficult to describe quantitatively holand et al 2011 shirley et al 2012 3 3 interrelation between variables subsequent to the identification of the various influential variables this section addresses the interrelationships between these variables a quilt plot showing the frequency of all potential combinations between the variables used is presented in table 5 results from the quilt plot include the following within the categories expressing the damage variable the occurrence of the number of claims variable is greater than the monetary terms variable within the categories expressing the explaining variables the meteorological variable is the most commonly used this makes sense since it is directly related to the cause of the floods thereafter geographic demographic and building property based variables occur in that order the most frequent combination of two categories one from damage and one from the explaining variable groups is number of claims combined with meteorological the most frequent combination of two variables one from damage and one from the explaining variable groups is number of claims per period combined with the rainfall by intensity among the categories the two most common variables are meteorological and monetary based however among variables the most common combinations involve one from meteorological and one from number of claims based groups the monetary based variables are quite widespread in terms of frequency the reason for this may be the different ways in which the databases are structured while the number of claims variables are mostly focused on the number of claims per period this may be explained by the fact that it is possible to retrieve the total number of claims from the databases during a specific timeframe the most used damage variable is number of claims over a specified timeframe the most used explaining variable is rainfall by intensity many variables exhibit low frequency of occurrence those exhibiting a single frequency and expressing the damage variable include building claim over building value or insurance coverage claims per capita or gdp total daily claims per number of claims number of claims per gauge and number of claims per number of policy holders those exhibiting a single frequency and expressing the damage variable include other meteorological parameters density and gdp the most under researched areas include the building property related and demographic categories both of these categories include variables that are vulnerability based all green coloured cells in the quilt plot suggest new combinations between variables that have already been used the red coloured cells may provide a useful insight into what should be accounted for at the initial stages for example the most exploited relationship is that between number of claims and rainfall intensity this may imply that these variables exhibit the strongest correlation and as such may provide a useful insight into which relationship should first be accounted for while a few of the damage variables specifically belonging to one of the four categories may have been considered as insignificant among the different studies in general terms the four categories have all been shown to be important as a result it may be expedient to combine variables derived from each of categories as follows 1 i combined i 1 m e t e o r o l o g i c a l i 2 g e o g r a p h i c i 3 d e m o g r a p h i c i 4 b u i l d i n g where i represent an index value the inclusion of four categories does not necessarily imply that the variance will be better explained here than in situations that include only two or three categories however part of the variance will always remain unexplained if no account is taken of variables from any of the identified categories the results are highly dependent on the selection of both the variables within the given category and their combinations similarly the choice of model used to develop this relationship significantly influences the results 3 4 modelling the relationship between the variables table 6 presents the characteristics of the case studies taken from the fourteen identified publications that have modelled the relationship between the damage and explaining variables the applied case studies share the same scope although they vary in terms of both extent and approach this covers the scale involved as well as the coverage and period of incorporated insurance claims similarly the methods used to develop and validate the models are different the methods applied in model development see table 6 include visual analysis techniques a linear logistic and poisson regression model decision trees principal component analysis and partial least squares discriminant analysis the percentage of the explained variance also varies lastly the ways in which results are visualised and deployed vary from the provision of a variable correlation function to the development of probabilistic hazard maps the results show that regression is the most commonly applied method this approach is widely used in the field of flood risk assessment barredo et al 2012 botzen and bouwer 2016 changnon et al 2001 donat et al 2011 haug et al 2011 kim et al 2012 lohmann and yue 2011 peng et al 2014 wobus et al 2014 regressions are simple to apply and to visualise the results however the variation in validation techniques used indicate that the explained variance may be relatively low one reason for this may be the choice of the modelling method however low variance may also be caused by 1 the poor availability of or variation in the aggregated data spekkers et al 2014 2 the assumptions regarding the variables included in the study either by their absence or aggregation expression 3 the percentage of insured buildings as a ratio of all the buildings or 4 alterations to insurance policies over the years the choice of modelling method is an important factor influencing outcomes different conclusions regarding model application and efficiency are drawn in different studies according to spekkers et al 2014 decision tree models perform better than global regression models in terms of the explained variance in damage data similar conclusions are drawn by merz et al 2010 in applications related to fluvial flooding however a satisfactory percentage of the variance may be explained using regression techniques moncoulon et al 2014 torgersen et al 2017 nevertheless consideration should be given to the possibility of a non linear relationship between the damage and explaining variables sorensen and mobini 2017 spekkers et al 2014 zhou et al 2013 regression based models may not be able to capture this variance furthermore satisfactory results have been derived by applying principal component analysis even when account is taken only of variables within the meteorological category torgersen et al 2015 similarly the partial least squares regression technique was also found to be suitable due to the high collinearity in the dataset torgersen et al 2017 although this in turn may lead to poor results when using ordinary least squares regression tobias 1995 many of the conclusions derived from the literature are contradictory and no specific modelling method has been proved to produce more satisfactory results than the others however the study does reveal that for a model to produce satisfactory results it is crucial to employ a combination of the variables and the methods used even if the choice and aggregation of variables corresponds to the specific characteristics of the case study in hand explained variance and consequently outcomes may be improved by accounting simultaneously for the combination of variables derived from the main four categories sensitivity analysis and bootstrapping are additional techniques that can be used to verify and validate the models 3 5 proposed generic framework for developing models for the analysis and assessment of swfs 3 5 1 on the relevance and development of the framework the results from this review indicate that the process of developing models that use insurance data to explain sfw event occurrence depends on the characteristics of the case study data availability and assumptions regarding how to interrelate data due to the specificity of their applications no overall conclusions can be drawn regarding the variables or methods that can be used or the steps for developing the models and their further application consequently a generic and adaptable framework has been developed using the aggregated results from relevant literature to define a workflow that may be implemented to develop a model of the relationship between the damage and explaining variables and its further application and deployment this framework is presented in fig 4 together with notes accompanying several of the steps table 7 it incorporates the assumptions and decisions that may be adapted to any specific case study in hand the framework should be regarded as a guide to the development and further deployment of models used in the analysis and assessment of swf events 3 5 2 on the challenges facing model development the outcome of applying an aggregated framework which includes both the model and its further deployment is dependent on the availability of the data willingness to share and their quality or reliability these parameters are the result mainly of the degree of systematic recording classification of the data combined with the uncertainties involved any scarcity inhomogeneity or lack of availability of data hinders the spatial and temporal correlation between the damage and explaining variables and in turn constrains the quality of the model outcome which may be represented by the explained variance a crucial issue that limits the potential for such model development is the availability of damage data which is derived from either insurance companies or individual data collectors it is observed a reluctance within the insurance industry to share detailed information such as the exact location of the source of flood damage compensation grahn and nyberg 2017 there may be many reasons for this reluctance such as competition for market share reputational issues loyalty towards customers or anxiety about the impact disclosure may have on residential property markets a recent study which carried out interviews and analysed the results from eight largest insurance companies in norway in regard to their willingness and demands to share damage data concludes that the largest insurance companies representing 90 of the market are willing to share their data with municipalities and governmental agencies hauge et al 2018 however in order to share their data several demands were identified an arrangement that ensures restricted manageable admission of their data especially to other inter national companies the availability of a data administrator and or intelligent infrastructure that guarantees security and confidence in data protection and compliance and adaptation to new implemented regulations regarding protections of privacy hauge et al 2018 the availability of such information would facilitate a better understanding of the vulnerability component of the risk triangle currently there exists several databases in norway and worldwide that have collected damage or other relevant data regarding swf events and a review can be accessed in labonnote 2017 labonnote et al 2018 however data are spread around a heterogeneous community of stakeholders concerned with different motivations needs and levels of data processing it is concluded that digitization and its opportunities can improve the workflow of data collection and analysis and increase the quality of data the recent availability of internet of things big data analytics and artificial intelligence can enable fast systematic and sustainable digital data analytics which can subsequently trigger a global data driven evaluation system regarding the swf event occurrence and their impact on society a commonly acknowledged issue that arises during the comparison of different studies is the lack of a consistent classification system for damage claims several schemes have been developed involving the classification of flood events by type fluvial or pluvial degree event extremity damage to assets movable or non movable or origin consequences non rainfall and rainfall related damages based on spatial resolution temporal resolution costs or degree of wetness however all these approaches have their shortcomings bernet et al 2017 which in turn may decrease the explained variance derived from the model even when data are accessible they may be characterised by levels of uncertainty associated with both damage and explanatory variables the temporal and spatial distribution of rainfall may not be correctly accounted for due to non uniform distribution or a non representative number of measurement gauges stations the spatial resolution of radar images may be too coarse to capture the spatial variability of rainfall at the subpixel scale causing an underestimation of rainfall peaks of convective cells spekkers et al 2014 data variation in a spatial context is another source of uncertainty that may be attributed to a lack of specified addresses the availability of which may enable the parametrization of geographical information at the level of other damage demographic and meteorological variables spekkers et al 2013 2014 zhou et al 2013 it should be noted that an absence of recorded damage in a given area does not necessarily mean that the area has not been affected by a flood event bernet et al 2017 it may simply indicate that no buildings were in the vicinity of the flooded area or that the buildings were properly protected against the flood event or the occurred damages were not properly registered lastly the scale of a given area may increase the variability of the outcome because different scales of district neighbourhoods cities countries may be associated with different parameters linked to climatic conditions insurance policies or the percentage of insured buildings another source of uncertainty resides in variables associated with the buildings themselves and the socio economic status of their residents which is related in turn to self protective behaviour for example building refurbishment may not have been recorded moreover tenants or owners may share different responsibilities and consequently different levels of vulnerability it is widely acknowledged that risk assessment should provide an indication of the degree of reliability of risk quantification merz and thieken 2009 although such reliability may be constrained if data are scarce missing or associated to uncertainties consequently a strategy involving the implementation of a systematic and homogenous recording process that includes information from different explaining variable categories at both local and national scales is suggested even if a comprehensive harmonization at international scale has been deemed as unlikely to be effective surminski et al 2015 policies that regulate and digitise the claims process can better facilitate both grounds for claims and more accurate inputs as a means of improving current models subsequently the application of risk assessment can be more useful to higher implementation schemes such as policies or programmes 3 5 3 on the opportunities for model application presentation the model relating the damage and explaining variables can be further applied within a risk assessment framework or sensitivity analysis these applications can be useful for stakeholders such as insurance companies government agencies and meteorological institutes fig 6 provides a schematic presentation of the interrelations between model application implementation and involved stakeholders risk assessment enables the graphical representation of risk distributed in a spatial and temporal context one outcome is the production of probabilistic maps of metropolitan areas showing the likelihood of occurrence and degree of damage based on meteorological events similar to that illustrated in moncoulon et al 2014 a graphical display such that the one used in brevik et al 2014 might then be employed see fig 5 such a framework can be used to evaluate potential increases in damage resulting from flooding that may be caused by climate change this may be achieved by incorporating a global climate model cheng et al 2012 the likelihood of both swf and fluvial flood events may be included as part of the overall hazard scenario furthermore insurance data can be collected from different sources for different purposes related to residential business or agricultural properties as well as state owned public buildings and transport infrastructure lastly multi dimensional models for flood events and specified terrains can be incorporated into risk assessment frameworks application of global sensitivity analysis saltelli et al 2008 which is identified as a research gap enables the understanding and quantification of a given system as such it is able to provide estimates of the influence of the inputs on the outputs the relationship between the damage and explaining variables is replete with uncertainties as a consequence the application of global sensitivity analysis enables a ranking of the importance of given parameters and or their uncertainties such rankings can support decision making processes by means of facilitating comparisons of relative performance and by optimizing design selection and the implementation of a policy or mitigation action 3 5 4 on the opportunities for model deployment the following is a summary of implementations of the models discussed in the foregoing policy writing and the execution of mitigation measures the importance of socio economic variables as identified from sensitivity analysis may provide an insight into their relative contributions in the vulnerability and exposure components of the risk triangle the temporal distribution of damage claims may provide insights into what values of rainfall intensity and seasons of the year most closely correlate with damage claims this facilitates a clearer identification of parameters that can reduce risk effectively the latter can be applied during policy writing and the prioritization of mitigation measures the implementation based on risk assessment of more proactive cost effective and politically achievable investments in infrastructure adaptation at local regional and national scales kousky michel kerjan 2017 an understanding of trends in damage claims and causality based on sensitivity analysis for example several authors have concluded that the causes of increases within the vulnerability component of the risk tringle are associated with socio economic factors such as population growth and increased wealth among policy holders and thus the damaged products may be more valuable rather than the hazard component associated with climate change barredo 2009 bernet et al 2017 bouwer 2011 bouwer 2013 spekkers et al 2015 improvements in insurance policy writing such as the inclusion of specific clauses related to rainfall intensity criteria spekkers et al 2013 writing and updating of design codes current design criteria related to urban drainage system capacity or the return period of design storms spekkers et al 2015 can be implemented or updated the development or validation of damage models improving customer service once a clear association between hazard intensity and its consequences is established direct weather alerts or warnings can be communicated to residents this will boost the emergency preparedness of residents which may in turn may limit damage and levels of vulnerability better management of call centres during flood events many companies have indicated that there is a sudden increase in communication demand from clients during extreme events spekkers et al 2013 4 conclusion this study has carried out a systematic literature review to investigate how insurance data can be applied in the analysis of swf event occurrence the review concludes that models that identify the relationships between insurance data and explaining variables may provide an insight into the occurrence of surface water flood events the study has identified four main categories of explaining variables meteorological geographical demographic and property building related potential ways of expressing both damage and explaining variables as well as their combinations have been discussed and recommendations for future applications proposed a generic framework providing guidelines for the development of models of similar scope and their further deployment has been aggregated on the basis of previous applications the review shows that the outcome of such models is sensitive to factors such as the selected variables and their expression aggregation the combination of variables the methodologies used to establish the model in question data availability and quality the study emphasises the importance of the systematic recording and public disclosure of insurance data as a means of improving the implementation of and outcomes from these models such models can enable sensitivity analysis and risk assessment frameworks that can be further incorporated into decision making processes policy writing and implementations the review demonstrates an increase in interest worldwide in the development of such models at local and national scales however their application is mostly geographically focused which emphasises the potential for wider application author contribution statement conceptualization kg nl es bt data curation kg nl es bt formal analysis kg funding acquisition nl es bt investigation kg methodology kg nl project administration nl es software kg nl validation kg nl es bt visualization kg roles writing original draft kg writing review editing kg nl es bt acknowledgments this study was funded by the norwegian centre for research based innovation klima 2050 www klima2050 no with the intention of promoting the development of a global data driven evaluation system that will provide policymakers with know how linked to societal risk associated with climate change and to strengthen the innovation capacity of national agencies and private companies to address the consequences of climate change research council of norway country norway grant number 237859 o30 
6808,this study employs a systematic literature review to investigate how insurance data can be applied in the analysis of surface water flood events the study firstly identifies the variables expressing insurance data and those explaining them together with their interrelationships damage variables may be expressed as either monetary based or number of claims based explaining variables may be subdivided into four categories meteorological geographic demographic and property building based most of the common and under researched combinations of these variables and their expression are discussed secondly a comparative analysis is presented of current models highlighting their differences and similarities the study demonstrates that the scope and approach of the models varies in relation to scale the coverage and period of incorporated insurance claims and the methods used for model development and validation thirdly the study proposes a generic and adaptable framework constructed from an aggregation of information contained in relevant literature to define a workflow for model development and future deployment the study concludes with a discussion of the challenges facing model development and opportunities for deployment keywords surface water floods pluvial insurance data insurance claims flood damage flood risk 1 introduction flooding is a common environmental hazard that endangers the physical economic and social environment barredo 2009 falconer et al 2009 kron 2005 pluvial flooding is triggered by accumulated rainfall that results in overland water flow and ponding that cannot be drained away either by natural or artificial systems bernet et al 2017 falconer et al 2009 hurford et al 2012 surface water flooding swf represents a combination of pluvial flooding stormwater flooding sewer flooding flooding from small open channel and culverted urban watercourses and overland flows from groundwater springs bernet et al 2017 falconer et al 2009 hurford et al 2012 kaźmierczak and cavan 2011 the term swf can be regarded as the optimal general definition of rainfall related pluvial flooding events bernet et al 2017 economic loss resulting from swfs including both tangible and intangible consequences has increased dramatically in recent decades and is expected to do so in the future as reported for several countries in europe and as well as usa and canada barredo et al 2012 bernet et al 2017 bouwer 2013 cheng et al 2012 kousky and michel kerjan 2017 kron 2005 moncoulon et al 2016 wobus et al 2014 zhou et al 2013 on the one hand patterns and intensities of rainfall events are expected to alter in response to climate change leading to more frequent and severe flooding events cheng et al 2012 falconer et al 2009 on the other a large body of research currently points towards increasing concentration densities of valuable assets due to urbanization and an expanding population as the principle cause of the increasing cost of natural disasters barredo et al 2012 barredo 2009 bernet et al 2017 bouwer 2011 bouwer 2013 kreibich and thieken 2008 spekkers et al 2015 consequently risk mapping and risk assessment are applied as methodologies for the identification of risk influencing factors and the evaluation of risk mitigating measures the term risk in this context is commonly expressed as the multiplication of the factors hazard vulnerability and exposure crichton 1999 field et al 2012 ipcc 2012 koks et al 2015 kron 2005 hazard refers to threatening natural events such as intense rainfall expressed in terms of probability of occurrence vulnerability refers to the capacity or inability of a society to deal with the hazard exposure refers to that of the human population involved combined with the value of the assets subject to the hazard crichton 1999 koks et al 2015 kron 2005 an understanding of each component of this risk triangle is required as a basis for analysing how risk due to flooding can be reduced most effectively research over the past decades has mostly focused on improving our understanding of the hazard component grahn and nyberg 2017 kaźmierczak and cavan 2011 koks et al 2015 mechler and bouwer 2015 mechler et al 2014 while vulnerability and exposure have started to gain attention only during the past decade in the field of flood risk assessment cutter et al 2013 koks et al 2015 lujala et al 2014 rød et al 2015 hazard is a very uncertain phenomenon which cannot be predicted the ranges of levels of vulnerability and exposure are very wide and constantly changing for this reason it is important to develop policies that are able to address a range of different outcomes falconer et al 2009 kron 2005 to achieve this it is important to understand the fundamentals of flood damage data and its possible causes or influences insurance databases represent a potential source of flood damage data consequently analytical research has been carried out in recent years to apply insurance data as a proxy for the analysis of the impact of flooding events bernet et al 2017 cortes et al 2018 grahn and nyberg 2017 sorensen and mobini 2017 spekkers et al 2015 spekkers et al 2013 torgersen et al 2015 torgersen et al 2017 zhou et al 2013 the outcomes of these studies have included an understanding and ranking of the variables that can explain damage data the development of models that can predict the likelihood of an swf event and the implementation of said models flood risk assessment frameworks these studies share a common objective the development of models that explain insurance data in terms of other rainfall related geographic and socio economic factors however the models differ in their identification and expression of the variables used their interrelationships the methods used to develop and validate the models and their further implementation and deployment the studies have concluded that such models can provide an insight into the relationship between insurance data and key explaining variables however much of the statistical variance is left unexplained emphasising the need to increase the availability completeness and reliability of relevant data on one hand and to consider alternative ways of expressing selected variables as well as the inclusion of other explaining variables and their interrelationships and the methods used to develop the models on the other hand in the light of this an aggregation and synthesis of the relevant literature is required in order to compare the similarities and dissimilarities between these studies and thereafter deliver recommendations for future application based on current best practice the aim of this study carried out in the form of a systematic literature review is to look into how insurance data can be used to analyse swf events it has the following objectives to analyse the historical development of the use of insurance data for analysing swf events section 3 1 to identify the variables that express insurance data and those explaining them section 3 2 to identify current interrelationships between insurance data and other explaining variables section 3 3 to provide a current overview of existing models and analyse their differences and similarities section 3 4 to propose a generic framework based on an aggregation of current models and methods as a basis for a discussion of the challenges related to model development as well as opportunities for their deployment section 3 5 section 1 describes the motivation aims and scope of the study the methodology is then presented in detail in section 2 the results and discussions addressing the aforementioned objectives are presented in section 3 and conclusions are set out in section 4 2 methodology the literature review presented in this study is based on an established research methodology booth et al 2011 that ensures a comprehensive search process and systematic review of the relevant literature the methodology originates from the field of health and social sciences but its principles are applicable to other fields of study the approach provides a tool capable of providing a transparent and reproducible research synthesis thus offering greater clarity internal validity and audibility booth et al 2011 the first step in the review process is to define the scope of research that directs focus on the research question booth et al 2011 in the present study the research question opts to identify how can the insurance data be used to derive models explaining swf events in this study the definition of an swf as set out in references bernet et al 2017 falconer et al 2009 hurford et al 2012 kaźmierczak and cavan 2011 is used because it covers the different types of floods described in the studied literature the cimo framework petticrew and roberts 2008 is used to define the key concepts of the research process table 1 the research question is identified as follows how o do we use insurance data i to analyse m surface water flooding events c the keywords presented in table 2 were identified based on the titles abstracts and keywords provided in the literature bernet et al 2017 cortes et al 2018 grahn and nyberg 2017 sorensen and mobini 2017 spekkers et al 2015 spekkers et al 2013 torgersen et al 2015 torgersen et al 2017 zhou et al 2013 following a preliminary screening first step using the electronic database scopus and google scholar the search scheme and exclusion criteria are shown in fig 1 and table 3 three electronic databases of peer reviewed literature were used for the final screening second step scopus web of science and engineering village are all relevant sources of information in this research area aghaei chadegani et al 2013 falagas et al 2008 jacso 2005 the keywords operators and nesting combinations are presented in table 2 the keywords were applied at title abstract keywords topic level the last search was performed on 25 april 2018 all years of publication were included in the search process literature screening based on a full content cross referencing methodology and author searching was used to check for additional sources in cases of similar studies being included in different literature sources priority was assigned to the most recent publication the final number of selected publications was 34 subsequently a data extraction process booth et al 2011 was developed to retrieve and code relevant variables and elements in order to facilitate comparison and identify patterns themes or trends table 4 shows the subgroups of data extraction that help to structure the literature review results described in the following sections 3 results and discussion 3 1 historical development graphical representation fig 2 presents an historical development of the selected literature that has addressed the relationship between insurance claims and swfs the literature is subdivided into publications that have modelled this relationship solid line and others that have simply provided relevant research results and discussions dotted line despite the fact that the potential of applying this relationship has been discussed over many decades results show that it has only been in recent years that an increasing number of publications have appeared that actively develop a model nevertheless these studies cite a limited number of cities and countries including canada denmark france germany the netherlands norway portugal spain sweden switzerland the united kingdom and the usa in some cases more than one study per country is identified some investigating different regions and others cited by either similar or different authors the increasing number of publications produced during the past decade suggests an increased interest in the use of insurance claim data to assess flood risk nevertheless only very few countries have applied such methodologies thus highlighting the potential for wider application of the approach consequently an aggregation of relevant literature may provide the basis for further development and application of these models and for this reason a review of current models and their characteristics including the variables used and their combinations should be made available 3 2 identification and categorization of the variables used to develop the relationship between damage and explaining variables a variety of different definitions in relation to the variables used to define insurance data and variables used to explain them are identified in the literature the term damage is a dependent or response variable that expresses the nature of insurance data the term explaining is a damage influencing or independent variable used to account for or explain the damage variable in the following the terms damage variable and explaining variable are used fig 3 presents a schematic overview in the form of a bubble map of the variables identified and used in these studies the literature is indexed with numbers according to table 6 the damage variables are grouped in two main categories based on their expression as follows 1 monetary based which express the damage variable in terms of a currency value they include values expressed both as a whole or as a fraction of or relative to other parameters or variables 2 number of claims based which express the damage variable in terms of the number of policies claims that are dependent on other parameters or variables the explaining variables may be subdivided into four categories based on their characteristics and their role in the risk triangle hazard exposure vulnerability the literature shows sometimes contrasting views when assigning different variables to one of the roles within the risk triangle an example is the discussion of the variable density of built environment which has been considered both as part of vulnerability and exposure koks et al 2015 the following categories are defined 1 meteorology based variables m that describe physical atmospheric or natural extreme weather events such as intense rainfall they may or do cause influence or trigger the occurrence of flood events this category belongs to the hazard risk component 2 geographic based variables g that describe the spatial characteristics and parameters of the area under investigation they may be expressed as single parameters or be combined in the form of a map describing terrain characteristics these in turn may influence the degree of the hazard if an swf event occurs and the coping mechanisms of the system this category belongs to both the exposure and vulnerability risk components 3 demographic based variables d that state the inventory of elements and assets in a given area in which the swf event may occur such variables may be used to aggregate damage variables this category belongs to both the exposure and vulnerability risk components 4 building property based variables b that describe susceptible at risk elements and the system s ability to cope with the hazard they are directly related to the relevant damage variable this category belongs to both the exposure and vulnerability risk components 3 2 1 damage variables there are two ways of expressing the damage variables 1 total number of claims and 2 claim size in both cases these values can be aggregated with other parameters or explaining variables and be expressed as relative values the review identifies a variety of ways of expression and conclusions in terms of their applications findings zhou et al 2013 show that rainfall data cannot be used to explain variation in individual cost per claim however such data may be a suitable indicator of overall costs per day in contrast in spekkers et al 2014 the cost per claim term was inadequate to express the damage variable while claim frequency appeared to provide more satisfactory results the latter sounds plausible since cost per claim is related to real estate value the cost of cleaning and the economic value of the insurance holders belongings consequently high income neighbourhoods may appear to be more easily flooded regardless of the real probability of swf events in such areas sorensen and mobini 2017 nevertheless the total number of claims term may be biased if it is not aggregated or expressed in a relative manner a neighbourhood containing a high building density or a high percentage of insured buildings will likely result in a larger total number of claims than an area that is less populated or less densely developed this observation underlies the importance of using aggregated or relative values bernet et al 2017 spekkers et al 2014 however relative values can also be misleading for example a neighbourhood containing only one storey buildings may seem to be more easily flooded than a similar neighbourhood with the same number of multi storey buildings for this reason the use of suitable parameters or variables that aggregate the damage variable may be more useful than using the cost of claims or total number of claims terms nevertheless a combination of both claim size and total number of claims aggregated by the use of different parameters or in terms of total values is proposed in order to fully exploit the relationship 3 2 2 explaining variables this review presents several variables that are used to explain the damage variables focus is directed mainly on the meteorological category since variables describing rainfall are considered to be the main causes of swf events however a variable that has not been identified is wind driven rain which may damage certain parts of a building that are not accessible to vertical rainfall similarly snow and hailstorms may also contribute to damages paid in response to insurance claims hanak and korytarova 2014 moreover and similar to the damage variable results may be dependent on how the rainfall data are aggregated for example in grahn and nyberg 2017 the intensity variable which takes both the duration of the rainfall and the aggregated volume of rain into account exhibited a statistically significant effect on flood related damages while the aggregated volume of rain alone did not this illustrates the importance of applying meteorological information that reflects the rainfall phenomenon in a temporal perspective despite the fact that rainfall may be the main cause of swf events previous research has concluded that use of this variable alone is not sufficient to explain observed variance cortes et al 2018 spekkers et al 2015 zhou et al 2013 thus underlying the importance of considering the impacts of other categories different studies include different variables within the four identified categories of explaining variables however almost none of these studies include variables from each of these categories in the same analysis the absence of key variables may explain the large unexplained variance the selection of variables also depends on the scale of the investigation macro meso or micro referring to city country neighbourhood and building scale respectively different variables are associated with different scales meaning that different variables and aggregations can be used to explain a given damage variable based on the scale of the latter at microscales detailed information regarding a given property may be very relevant moncoulon et al 2014 on the other hand the relationships between socio economic variables and the damage occurred may be weaker at district level compared to that of individual households especially where such districts are heterogeneous an example of this is in situations where there is a large variance in household incomes spekkers et al 2014 similarly the type of insurance database plays an important role in the selection of variables different socio economic variables may play different roles when insurance claims are subdivided between property and movable assets 3 2 3 other variables this review indicates that many variables have been used and screened as important when explaining the damage variable in addition to the common variables used for similar purposes in different studies special variables are also noted the latter include a binary variable depending on whether the event occurred during the day shift or night shift grahn and nyberg 2017 urban exposure grahn and nyberg 2017 or the permeability of surfaces torgersen et al 2017 property value spekkers et al 2014 or socio economic variables such as household income age and education of breadwinner or fraction of homeowner spekkers et al 2014 others include urban drainage system properties drainage capacity age of infrastructure percentage of surface water level of urbanization socio economic indices household income and property value and district related parameters percentages of low rise and high rise buildings percentage impervious surface spekkers et al 2013 as well as the weather conditions prevailing during preceding days torgersen et al 2015 other variables have been identified as influential from other studies although they are not used in any of these publications they include green spaces koks et al 2015 self protective behaviour grothmann and reusswig 2006 precautions external response and early warning merz et al 2010 as well as building condition yazdani et al 2010 a systematic map of all the variables that may affect flood occurrence may be useful for the future application of similar research moreover damage variables are also influenced by a complexity of factors associated with the social vulnerability of residents and communities to surface water flooding such as age of residents willingness to pay for insurance presence during occurrence of the event and so on vulnerability may be a complex phenomenon to quantify since it is represented as a composite of other economic social cultural and psychological factors that are themselves difficult to describe quantitatively holand et al 2011 shirley et al 2012 3 3 interrelation between variables subsequent to the identification of the various influential variables this section addresses the interrelationships between these variables a quilt plot showing the frequency of all potential combinations between the variables used is presented in table 5 results from the quilt plot include the following within the categories expressing the damage variable the occurrence of the number of claims variable is greater than the monetary terms variable within the categories expressing the explaining variables the meteorological variable is the most commonly used this makes sense since it is directly related to the cause of the floods thereafter geographic demographic and building property based variables occur in that order the most frequent combination of two categories one from damage and one from the explaining variable groups is number of claims combined with meteorological the most frequent combination of two variables one from damage and one from the explaining variable groups is number of claims per period combined with the rainfall by intensity among the categories the two most common variables are meteorological and monetary based however among variables the most common combinations involve one from meteorological and one from number of claims based groups the monetary based variables are quite widespread in terms of frequency the reason for this may be the different ways in which the databases are structured while the number of claims variables are mostly focused on the number of claims per period this may be explained by the fact that it is possible to retrieve the total number of claims from the databases during a specific timeframe the most used damage variable is number of claims over a specified timeframe the most used explaining variable is rainfall by intensity many variables exhibit low frequency of occurrence those exhibiting a single frequency and expressing the damage variable include building claim over building value or insurance coverage claims per capita or gdp total daily claims per number of claims number of claims per gauge and number of claims per number of policy holders those exhibiting a single frequency and expressing the damage variable include other meteorological parameters density and gdp the most under researched areas include the building property related and demographic categories both of these categories include variables that are vulnerability based all green coloured cells in the quilt plot suggest new combinations between variables that have already been used the red coloured cells may provide a useful insight into what should be accounted for at the initial stages for example the most exploited relationship is that between number of claims and rainfall intensity this may imply that these variables exhibit the strongest correlation and as such may provide a useful insight into which relationship should first be accounted for while a few of the damage variables specifically belonging to one of the four categories may have been considered as insignificant among the different studies in general terms the four categories have all been shown to be important as a result it may be expedient to combine variables derived from each of categories as follows 1 i combined i 1 m e t e o r o l o g i c a l i 2 g e o g r a p h i c i 3 d e m o g r a p h i c i 4 b u i l d i n g where i represent an index value the inclusion of four categories does not necessarily imply that the variance will be better explained here than in situations that include only two or three categories however part of the variance will always remain unexplained if no account is taken of variables from any of the identified categories the results are highly dependent on the selection of both the variables within the given category and their combinations similarly the choice of model used to develop this relationship significantly influences the results 3 4 modelling the relationship between the variables table 6 presents the characteristics of the case studies taken from the fourteen identified publications that have modelled the relationship between the damage and explaining variables the applied case studies share the same scope although they vary in terms of both extent and approach this covers the scale involved as well as the coverage and period of incorporated insurance claims similarly the methods used to develop and validate the models are different the methods applied in model development see table 6 include visual analysis techniques a linear logistic and poisson regression model decision trees principal component analysis and partial least squares discriminant analysis the percentage of the explained variance also varies lastly the ways in which results are visualised and deployed vary from the provision of a variable correlation function to the development of probabilistic hazard maps the results show that regression is the most commonly applied method this approach is widely used in the field of flood risk assessment barredo et al 2012 botzen and bouwer 2016 changnon et al 2001 donat et al 2011 haug et al 2011 kim et al 2012 lohmann and yue 2011 peng et al 2014 wobus et al 2014 regressions are simple to apply and to visualise the results however the variation in validation techniques used indicate that the explained variance may be relatively low one reason for this may be the choice of the modelling method however low variance may also be caused by 1 the poor availability of or variation in the aggregated data spekkers et al 2014 2 the assumptions regarding the variables included in the study either by their absence or aggregation expression 3 the percentage of insured buildings as a ratio of all the buildings or 4 alterations to insurance policies over the years the choice of modelling method is an important factor influencing outcomes different conclusions regarding model application and efficiency are drawn in different studies according to spekkers et al 2014 decision tree models perform better than global regression models in terms of the explained variance in damage data similar conclusions are drawn by merz et al 2010 in applications related to fluvial flooding however a satisfactory percentage of the variance may be explained using regression techniques moncoulon et al 2014 torgersen et al 2017 nevertheless consideration should be given to the possibility of a non linear relationship between the damage and explaining variables sorensen and mobini 2017 spekkers et al 2014 zhou et al 2013 regression based models may not be able to capture this variance furthermore satisfactory results have been derived by applying principal component analysis even when account is taken only of variables within the meteorological category torgersen et al 2015 similarly the partial least squares regression technique was also found to be suitable due to the high collinearity in the dataset torgersen et al 2017 although this in turn may lead to poor results when using ordinary least squares regression tobias 1995 many of the conclusions derived from the literature are contradictory and no specific modelling method has been proved to produce more satisfactory results than the others however the study does reveal that for a model to produce satisfactory results it is crucial to employ a combination of the variables and the methods used even if the choice and aggregation of variables corresponds to the specific characteristics of the case study in hand explained variance and consequently outcomes may be improved by accounting simultaneously for the combination of variables derived from the main four categories sensitivity analysis and bootstrapping are additional techniques that can be used to verify and validate the models 3 5 proposed generic framework for developing models for the analysis and assessment of swfs 3 5 1 on the relevance and development of the framework the results from this review indicate that the process of developing models that use insurance data to explain sfw event occurrence depends on the characteristics of the case study data availability and assumptions regarding how to interrelate data due to the specificity of their applications no overall conclusions can be drawn regarding the variables or methods that can be used or the steps for developing the models and their further application consequently a generic and adaptable framework has been developed using the aggregated results from relevant literature to define a workflow that may be implemented to develop a model of the relationship between the damage and explaining variables and its further application and deployment this framework is presented in fig 4 together with notes accompanying several of the steps table 7 it incorporates the assumptions and decisions that may be adapted to any specific case study in hand the framework should be regarded as a guide to the development and further deployment of models used in the analysis and assessment of swf events 3 5 2 on the challenges facing model development the outcome of applying an aggregated framework which includes both the model and its further deployment is dependent on the availability of the data willingness to share and their quality or reliability these parameters are the result mainly of the degree of systematic recording classification of the data combined with the uncertainties involved any scarcity inhomogeneity or lack of availability of data hinders the spatial and temporal correlation between the damage and explaining variables and in turn constrains the quality of the model outcome which may be represented by the explained variance a crucial issue that limits the potential for such model development is the availability of damage data which is derived from either insurance companies or individual data collectors it is observed a reluctance within the insurance industry to share detailed information such as the exact location of the source of flood damage compensation grahn and nyberg 2017 there may be many reasons for this reluctance such as competition for market share reputational issues loyalty towards customers or anxiety about the impact disclosure may have on residential property markets a recent study which carried out interviews and analysed the results from eight largest insurance companies in norway in regard to their willingness and demands to share damage data concludes that the largest insurance companies representing 90 of the market are willing to share their data with municipalities and governmental agencies hauge et al 2018 however in order to share their data several demands were identified an arrangement that ensures restricted manageable admission of their data especially to other inter national companies the availability of a data administrator and or intelligent infrastructure that guarantees security and confidence in data protection and compliance and adaptation to new implemented regulations regarding protections of privacy hauge et al 2018 the availability of such information would facilitate a better understanding of the vulnerability component of the risk triangle currently there exists several databases in norway and worldwide that have collected damage or other relevant data regarding swf events and a review can be accessed in labonnote 2017 labonnote et al 2018 however data are spread around a heterogeneous community of stakeholders concerned with different motivations needs and levels of data processing it is concluded that digitization and its opportunities can improve the workflow of data collection and analysis and increase the quality of data the recent availability of internet of things big data analytics and artificial intelligence can enable fast systematic and sustainable digital data analytics which can subsequently trigger a global data driven evaluation system regarding the swf event occurrence and their impact on society a commonly acknowledged issue that arises during the comparison of different studies is the lack of a consistent classification system for damage claims several schemes have been developed involving the classification of flood events by type fluvial or pluvial degree event extremity damage to assets movable or non movable or origin consequences non rainfall and rainfall related damages based on spatial resolution temporal resolution costs or degree of wetness however all these approaches have their shortcomings bernet et al 2017 which in turn may decrease the explained variance derived from the model even when data are accessible they may be characterised by levels of uncertainty associated with both damage and explanatory variables the temporal and spatial distribution of rainfall may not be correctly accounted for due to non uniform distribution or a non representative number of measurement gauges stations the spatial resolution of radar images may be too coarse to capture the spatial variability of rainfall at the subpixel scale causing an underestimation of rainfall peaks of convective cells spekkers et al 2014 data variation in a spatial context is another source of uncertainty that may be attributed to a lack of specified addresses the availability of which may enable the parametrization of geographical information at the level of other damage demographic and meteorological variables spekkers et al 2013 2014 zhou et al 2013 it should be noted that an absence of recorded damage in a given area does not necessarily mean that the area has not been affected by a flood event bernet et al 2017 it may simply indicate that no buildings were in the vicinity of the flooded area or that the buildings were properly protected against the flood event or the occurred damages were not properly registered lastly the scale of a given area may increase the variability of the outcome because different scales of district neighbourhoods cities countries may be associated with different parameters linked to climatic conditions insurance policies or the percentage of insured buildings another source of uncertainty resides in variables associated with the buildings themselves and the socio economic status of their residents which is related in turn to self protective behaviour for example building refurbishment may not have been recorded moreover tenants or owners may share different responsibilities and consequently different levels of vulnerability it is widely acknowledged that risk assessment should provide an indication of the degree of reliability of risk quantification merz and thieken 2009 although such reliability may be constrained if data are scarce missing or associated to uncertainties consequently a strategy involving the implementation of a systematic and homogenous recording process that includes information from different explaining variable categories at both local and national scales is suggested even if a comprehensive harmonization at international scale has been deemed as unlikely to be effective surminski et al 2015 policies that regulate and digitise the claims process can better facilitate both grounds for claims and more accurate inputs as a means of improving current models subsequently the application of risk assessment can be more useful to higher implementation schemes such as policies or programmes 3 5 3 on the opportunities for model application presentation the model relating the damage and explaining variables can be further applied within a risk assessment framework or sensitivity analysis these applications can be useful for stakeholders such as insurance companies government agencies and meteorological institutes fig 6 provides a schematic presentation of the interrelations between model application implementation and involved stakeholders risk assessment enables the graphical representation of risk distributed in a spatial and temporal context one outcome is the production of probabilistic maps of metropolitan areas showing the likelihood of occurrence and degree of damage based on meteorological events similar to that illustrated in moncoulon et al 2014 a graphical display such that the one used in brevik et al 2014 might then be employed see fig 5 such a framework can be used to evaluate potential increases in damage resulting from flooding that may be caused by climate change this may be achieved by incorporating a global climate model cheng et al 2012 the likelihood of both swf and fluvial flood events may be included as part of the overall hazard scenario furthermore insurance data can be collected from different sources for different purposes related to residential business or agricultural properties as well as state owned public buildings and transport infrastructure lastly multi dimensional models for flood events and specified terrains can be incorporated into risk assessment frameworks application of global sensitivity analysis saltelli et al 2008 which is identified as a research gap enables the understanding and quantification of a given system as such it is able to provide estimates of the influence of the inputs on the outputs the relationship between the damage and explaining variables is replete with uncertainties as a consequence the application of global sensitivity analysis enables a ranking of the importance of given parameters and or their uncertainties such rankings can support decision making processes by means of facilitating comparisons of relative performance and by optimizing design selection and the implementation of a policy or mitigation action 3 5 4 on the opportunities for model deployment the following is a summary of implementations of the models discussed in the foregoing policy writing and the execution of mitigation measures the importance of socio economic variables as identified from sensitivity analysis may provide an insight into their relative contributions in the vulnerability and exposure components of the risk triangle the temporal distribution of damage claims may provide insights into what values of rainfall intensity and seasons of the year most closely correlate with damage claims this facilitates a clearer identification of parameters that can reduce risk effectively the latter can be applied during policy writing and the prioritization of mitigation measures the implementation based on risk assessment of more proactive cost effective and politically achievable investments in infrastructure adaptation at local regional and national scales kousky michel kerjan 2017 an understanding of trends in damage claims and causality based on sensitivity analysis for example several authors have concluded that the causes of increases within the vulnerability component of the risk tringle are associated with socio economic factors such as population growth and increased wealth among policy holders and thus the damaged products may be more valuable rather than the hazard component associated with climate change barredo 2009 bernet et al 2017 bouwer 2011 bouwer 2013 spekkers et al 2015 improvements in insurance policy writing such as the inclusion of specific clauses related to rainfall intensity criteria spekkers et al 2013 writing and updating of design codes current design criteria related to urban drainage system capacity or the return period of design storms spekkers et al 2015 can be implemented or updated the development or validation of damage models improving customer service once a clear association between hazard intensity and its consequences is established direct weather alerts or warnings can be communicated to residents this will boost the emergency preparedness of residents which may in turn may limit damage and levels of vulnerability better management of call centres during flood events many companies have indicated that there is a sudden increase in communication demand from clients during extreme events spekkers et al 2013 4 conclusion this study has carried out a systematic literature review to investigate how insurance data can be applied in the analysis of swf event occurrence the review concludes that models that identify the relationships between insurance data and explaining variables may provide an insight into the occurrence of surface water flood events the study has identified four main categories of explaining variables meteorological geographical demographic and property building related potential ways of expressing both damage and explaining variables as well as their combinations have been discussed and recommendations for future applications proposed a generic framework providing guidelines for the development of models of similar scope and their further deployment has been aggregated on the basis of previous applications the review shows that the outcome of such models is sensitive to factors such as the selected variables and their expression aggregation the combination of variables the methodologies used to establish the model in question data availability and quality the study emphasises the importance of the systematic recording and public disclosure of insurance data as a means of improving the implementation of and outcomes from these models such models can enable sensitivity analysis and risk assessment frameworks that can be further incorporated into decision making processes policy writing and implementations the review demonstrates an increase in interest worldwide in the development of such models at local and national scales however their application is mostly geographically focused which emphasises the potential for wider application author contribution statement conceptualization kg nl es bt data curation kg nl es bt formal analysis kg funding acquisition nl es bt investigation kg methodology kg nl project administration nl es software kg nl validation kg nl es bt visualization kg roles writing original draft kg writing review editing kg nl es bt acknowledgments this study was funded by the norwegian centre for research based innovation klima 2050 www klima2050 no with the intention of promoting the development of a global data driven evaluation system that will provide policymakers with know how linked to societal risk associated with climate change and to strengthen the innovation capacity of national agencies and private companies to address the consequences of climate change research council of norway country norway grant number 237859 o30 
6809,we investigate the ability to improve flood inundation forecasts at short to medium range 1 7 days timescales through weather ensembles and statistical water surface elevation wse postprocessing to generate the flood inundation forecasts a one dimensional hydraulic model namely the hydrologic engineering center s river analysis system hec ras is coupled to a regional hydrological ensemble prediction system rheps the rheps is comprised of i hydrometeorological observations ii weather ensembles from the national centers for environmental prediction global ensemble forecast system reforecast version 2 gefsrv2 and iii hydrology laboratory research distributed hydrologic model hl rdhm as the hydrological model the coupled rheps hydraulic system is evaluated along the tidal delaware river near the city of philadelphia pennsylvania u s for the evaluation emphasis is placed on the tidal riverine transitional zone of the delaware river and the downstream propagation of hydrometeorological uncertainty through the flood inundation forecasts the coupled system is used to generate hourly flood inundation forecasts at lead times from 1 to 7 days over the period 2008 2013 additionally wses from the coupled system are statistically postprocessed using quantile regression qr results show that the raw flood inundation ensemble forecasts exhibit higher skill than the deterministic ones we also find that statistical postprocessing improves the skill of the raw flood inundation ensemble forecasts with greater improvements at the longer lead times 3 days overall we find that both weather ensembles and statistical wse postprocessing can be used to enhance the skill of flood inundation forecasts at short to medium range timescales in turn this may serve to enhance the spatial representation of flood forecasts several days in advance which could contribute in the future to making flood forecast communication and warnings more effective keywords hydrologic hydraulic modeling flood mapping statistical postprocessing numerical weather prediction weather ensembles 1 introduction floods are among the costliest natural disasters in the world causing direct economic losses of over 1 trillion between 1980 and 2013 ipcc 2012 in the united states u s floods are among the deadliest weather related disasters killing over 4500 people in the 1959 2005 period ashley and ashley 2008 operational flood forecasting systems can help authorities and the public to be better prepared against floods cloke and pappenberger 2009 flood forecasting is usually done by forcing a hydrological model with numerical weather prediction nwp model outputs addor et al 2011 fan et al 2014 pappenberger et al 2011 saleh et al 2016 siddique and mejia 2017 thus errors in the nwp outputs are propagated into the flood forecasts requiring the explicit consideration of uncertainty to consider uncertainty the preferred approach is to use weather ensembles representing multiple plausible realizations of the nwp model outputs addor et al 2011 cloke and pappenberger 2009 demeritt et al 2010 fan et al 2014 khan et al 2015 olsson and lindström 2008 thielen et al 2009a ultimately the weather ensembles are used to force the hydrological model and produce probabilistic flow forecasts gouweleeuw et al 2005 pappenberger et al 2011 saleh et al 2016 thielen et al 2009b when the goal is to generate flood inundation an additional step is required in the hydrometeorological forecasting chain to translate the flood forecasts into flood inundation forecasts hereafter flood inundation is used to indicate the combined spatial and temporal representation of water surface elevations wses flood inundation predictions can be generated in different ways buahin et al 2017 christian et al 2013 di baldassarre et al 2010 leedal et al 2010 sarhadi et al 2012 a common approach is to couple a hydrological model with a hydraulic or hydrodynamic model grimaldi et al 2013 lian et al 2007 mejia and reed 2011 nguyen et al 2016 yamazaki et al 2011 yu et al 2006 it is important here to recognize that the coupled hydrologic hydraulic models can be employed in two fundamentally different ways simulation or forecasting mode flood inundation simulations use simulated flows as boundary conditions to the hydraulic model generated by forcing the hydrological model with observed meteorological data or a design storm event flood inundation simulation is employed in a number of applications e g flood policy flood insurance studies infrastructure design etc using both deterministic castro bolinaga and diplas 2014 costabile et al 2015 grimaldi et al 2013 ozdemir et al 2013 sampson et al 2015 and probabilistic approaches bates et al 2004 merz et al 2007 neal et al 2013 sarhadi et al 2012 however flood inundation simulation has the major drawback that it does not provide explicit information about near future flood hazards and risks in contradistinction flood inundation forecasts use hydrometeorological forecasts to force the hydraulic model providing near future flood related information flood inundation forecasts can also be generated using deterministic di baldassarre et al 2010 mashriqui et al 2014 nguyen et al 2016 or probabilistic approaches di baldassarre et al 2010 garcía pintado et al 2015 pappenberger et al 2005b schumann et al 2013 however only few studies have investigated the benefits of using probabilistic approaches to produce flood inundation forecasts buahin et al 2017 georgas et al 2016 pappenberger et al 2005b schumann et al 2013 van cooten et al 2011 that is the vast majority of deterministic and probabilistic flood inundation studies have been done in simulation mode the findings from those studies are not directly transferable to a forecasting context because among other reasons weather forecast uncertainties can have a strong and non linear influence on hydrological predictions for flood inundation forecast mapping three main approaches have been implemented i equilibrium nobre et al 2011 ii steady buahin et al 2017 noaa and dewberry 2011 and iii hydrodynamic pappenberger et al 2005b van cooten et al 2011 the former also called the bathtub method represents flood extent by horizontally spreading the forecasted wse at a point in a river reach to neighboring digital elevation model dem cells with lower elevation nobre et al 2011 teng et al 2017 this approach is intrinsically different from the other two as it does not require the use of a hydraulic model in the steady approach hydraulic models and dem data are used off line to produce steady state hydraulic profiles and flood inundation maps for a specified range of flow forecasts at a particular river reach thus each flow forecast is paired with a spatially referenced flood inundation map to build a digital library of spatially localized reach scale maps buahin et al 2017 noaa and dewberry 2011 a limitation of the steady approach is that it cannot handle highly dynamic river conditions e g backwater and tidal reaches the hydrodynamic approach is the most general of the three mapping approaches it consists of using hydrometeorological forecasts to force in real time a hydraulic model to seamlessly produce flood inundation forecast maps pappenberger et al 2005b van cooten et al 2011 previous studies implementing the hydrodynamic approach in a forecasting context however have been focused on a single storm event pappenberger et al 2005b short range forecasts georgas et al 2016 and or have not considered weather ensembles van cooten et al 2011 our primary goal with this study is to investigate the effect of hydrometeorological uncertainty on the skill of ensemble flood inundation forecasts at short to medium range lead times 1 to 7 days to this end we generate flood inundation forecasts by coupling a regional hydrological ensemble prediction system rheps siddique and mejia 2017 with a one dimensional hydraulic model namely the hydrologic engineering center s river analysis system hec ras the rheps uses the hydrology laboratory research distributed hydrologic model hl rdhm koren et al 2004 as the hydrological model and ensemble weather forecasts precipitation and near surface temperature from the national centers for environmental prediction 11 member global ensemble forecast system reforecast version 2 gefsrv2 hamill et al 2013 additionally since probabilistic flood forecasts generated from weather ensembles tend to exhibit systematic biases addor et al 2011 siddique and mejia 2017 we employ quantile regression qr to statistically postprocess i e remove systematic biases the ensemble flood inundation forecasts to rigorously verify the forecasts we generate and verify hourly forecasts at lead times of 1 7 days over multiple years 2008 2013 the fact that the verification is done over multiple years is computationally intensive but important it makes the estimation of verification metrics more robust than using as most often done a single or few flood events to our knowledge the generation and verification of medium range flood inundation forecasts through the joint implementation of a coupled hydrometeorological hydraulic system weather ensembles and statistical postprocessing over multiple reforecast years has not been previously investigated the research questions motivating this study are as follows what is the effect of hydrometeorological uncertainty on the skill of medium range flood inundation forecasts are ensemble probabilistic flood inundation forecasts more skillful than deterministic ones can weather ensembles and statistical postprocessing of wses improve flood inundation forecasts the remainder of this paper is structured as follows sections 2 and 3 describe the study area and datasets used respectively section 4 describes the methodology followed to generate postprocess and verify the ensemble flood inundation forecasts section 5 presents and discusses the main results lastly section 6 outlines key conclusions 2 study area the tidal delaware river near philadelphia pennsylvania is selected as the study area fig 1 a and b the length of the selected study reach of the delaware river is 88 km the study reach starts upstream at trenton new jersey and ends downstream at marcus hook pennsylvania fig 1a the 10 main tributaries that join the delaware river along the study reach are all included in the analysis fig 1b at trenton river dynamics are dominated by inflows from the delaware river basin while tidal currents dominate at marcus hook tides are mainly semi diurnal with a mean range of 1 5 m at the mouth and propagate as progressive waves with speeds of 1 m s galperin and mellor 1990 the drainage areas at trenton and marcus hook are 17 552 and 26 760 km2 respectively table 1 to generate and verify the flood inundation forecasts we emphasize the tidal riverine transitional zone of the delaware river which extends from trenton at the upstream boundary to newbold at the downstream end fig 1c this transitional zone is of particular interest because it is often a blind spot of operational forecasting for example in the case of the delaware river the transitional zone is currently not part of the noaa s regional operational hydrological or coastal forecasting systems the noaa s operational hydrological forecast domain ends at trenton while the coastal forecast domain of the noaa s delaware bay operational forecasting system extends upstream to newbold 3 datasets 3 1 terrain to represent the terrain we combine a dem and bathymetry data the dem is provided by the pennsylvania department of conservation and natural resources pdcnr who derived the dem from airborne 2007 light detection and ranging lidar data the pdcnr preprocessed the raw lidar data to get a bare earth raster dem with 1 m horizontal accuracy and 18 5 cm root mean square error rmse vertical accuracy the bathymetry data is provided by the philadelphia water department pwd the data combines a bathymetric dataset produced by the national oceanic and atmospheric administration noaa and a bathymetric survey performed by the pwd at several of the tributaries in the study reach to combine the dem and bathymetry data we follow the three step approach of gesch and wilson 2001 first a buffer zone of 500 m from the shoreline is defined in order to interpolate the dem data for the floodplains and the bathymetric data for the rivers second all the dem data inside the buffer zone is converted into a 1 m point cloud the points that lie inside the open water areas are excluded since lidar data are inaccurate in those areas third with all data in point format we use arcgis to interpolate the bathymetry and lidar data together the final combined dem is a seamless terrain model of the river and floodplains with 1 1 m2 spatial resolution 3 2 flow and tidal gages we use observed flows and wse data for the period 2008 2013 to calibrate and verify the coupled hydrometeorological hydraulic system the observed flow and wse data used are summarized in table 1 the flow data are provided by the usgs at hourly resolution a total of 9 usgs flow gages are used the only flow gage located on the delaware river is the one at trenton gage number 01463500 the other flow gages are for the major tributaries to the study reach table 1 the wse data are provided by noaa there are a total of 5 sub hourly noaa tidal stations in the study reach table 1 the station at marcus hook pennsylvania constitutes the downstream boundary of the study reach fig 1b during the study period 2008 2013 all the gages have some missing values table 1 ranging from only 4 at mantua new jersey gage number 01464500 to 2881 at pennypack pennsylvania gage number 01467048 to estimate the missing values linear interpolation is performed for cases where the consecutive number of missing data is less than 48 h and or flow is in low conditions i e baseflow in the case of pennypack given that more than 2 weeks of data are missing the missing data are reconstructed using simulated flows from hl rdhm 3 3 precipitation and near surface temperature 3 3 1 observed we use observed precipitation and near surface temperature data over the period 2008 2013 as weather forcing to hl rdhm to simulate flows and initialize the rheps at the upstream boundary of the study reach and all the tributaries for the observed weather forcing we use multi sensor precipitation estimates mpes and gridded near surface observed temperature the mpes are provided by the noaa s middle atlantic river forecast center marfc siddique and mejia 2017 the resolution of the mpes is hourly over the regular 4 x 4 km2 hydrologic rainfall analysis projection hrap grid rafieeinasab et al 2015 zhang et al 2011 the observed gridded temperature data at 4 4 km2 resolution are also provided by the marfc the gridded temperature data are used by hl rdhm to estimate evapotranspiration and snow melt and accumulation snow melt is an important source of runoff in the upper parts of the delaware river basin 3 3 2 forecasted we force hl rdhm with precipitation and near surface temperature outputs from the gefsrv2 over the period 2008 2013 to produce deterministic based on the control member of the gefsrv2 and probabilistic based on the 11 member gefsrv2 ensemble flow forecasts at lead times of 1 7 days the forecasts are produced at the upstream boundary of the study reach and all the 10 tributaries the gefsrv2 outputs are retrospective forecasts 1 16 days generated using the gefs model version 9 0 1 the spatial resolution of the gefs is 55 km for days 1 8 the gefsrv2 consists of 11 members including the control member the control member is generated by using unperturbed initial conditions and the other members are generated by stochastically perturbing the initial conditions of the model further details about the gefsrv2 dataset are provided elsewhere hamill et al 2013 the gefsrv2 data are interpolated onto the 4 4 km2 hrap grid using bilinear interpolation 4 methodology this section is divided into four subsections that describe the main components of the proposed hydrometeorological hydraulic forecasting system including the hydrological model hydraulic model statistical postprocessor and verification strategy fig 2 shows the inputs outputs from each of and the interdependence among the different system components 4 1 hydrological model hl rdhm koren et al 2004 is used to both simulate hourly flows and generate deterministic and probabilistic hourly flow forecasts at the 11 forecast points upstream boundary at trenton plus 10 tributaries in the study reach to simulate hourly flows we force hl rdhm with the gridded precipitation and near surface temperature observation data while the hourly flow forecasts are generated by forcing hl rdhm with the gefsrv2 precipitation and near surface temperature outputs hl rdhm is a distributed hydrological model that operates on a regular square grid we use a 4 x 4 km2 grid cell resolution each grid cell consists of two main components water balance and hillslope and channel routing koren et al 2004 for the water balance component we use the sacramento soil accounting and heat transfer sac ht and the snow 17 submodels the sac ht is used for rainfall runoff generation and snow 17 for snow accumulation and melt the hillslope and channel routing are done using a nonlinear kinematic wave approach further information about hl rdhm in the context of recent applications can be found elsewhere lee et al 2015 nguyen et al 2016 rafieeinasab et al 2015 spies et al 2015 thorstensen et al 2016 wood et al 2016 we run the hl rdhm model using calibrated parameter multiplier values the calibrated parameters are those associated with baseflow percolation evaporation overland flow and channel routing during the calibration process the parameter fields are adjusted rather than the actual parameter values for this each a priori parameter field is multiplied by a factor the calibrated multiplier values are obtained from a previous study siddique and mejia 2017 for the delaware and schuylkill river basins these same multiplier values are applied to the other 9 tributaries in the study reach this is deemed reasonable given the hydrologic similarity due to proximity of all tributaries further these smaller tributaries have little impact on flows along the study reach i e the delaware and schuylkill rivers account for 70 of the total inland flows into the study reach 4 2 hydraulic model hec ras brunner 2016 is used in fully dynamic mode as the hydraulic model to both simulate hourly wses and produce deterministic and probabilistic hourly flood inundation forecasts along the study reach we select hec ras because it is used in operations buahin et al 2017 mashriqui et al 2014 and has shown good capabilities for flood inundation mapping merwade et al 2008 pappenberger et al 2005a the hec ras module ras mapper is used in this study to create gridded flood inundation maps the ras mapper is used to spatially interpolate wse predictions at individual cross sections along the study reach onto a terrain grid of 1 1 m2 resolution to develop the hec ras model the terrain model is discretized using 373 cross sections out of which 222 are used to model the main delaware river with an average spacing of 400 m and the remaining ones are used to model the tributaries to perform the hydraulic simulations and calibrate the model hourly observed wses and flows are used as boundary conditions to the model observed wses are used at the downstream tidal boundary at marcus hook fig 1b while observed flows are used at the upstream boundary at trenton and all the tributaries except for two ungaged tributaries rancocas and woodbury in fig 1b for these two tributaries hourly hl rdhm simulations are used since observations are not available when running the hec ras model in forecasting mode hourly flow forecasts from hl rdhm are used at the upstream boundary and all the tributaries however at the downstream boundary i e marcus hook we use observed wses when running hec ras in forecasting mode the main reason for this is that in contrast to the availability of long term weather reforecasts through the gefsrv2 dataset there are no coastal wse reforecasts available that cover multiple years because of this shortcoming we use observed wses at marcus hook when generating the flood inundation forecasts this means that our flood inundation forecasts do not consider the uncertainty associated with downstream wses instead the emphasis here is solely on the effect of hydrometeorological uncertainty on the flood inundation forecasts to calibrate the hec ras model the manning roughness parameter is adjusted at each cross section we estimate a priori roughness parameters for the main channel and floodplains using both land use land cover data from the 2011 national land cover dataset homer et al 2015 and tabulated manning roughness values for different land use land cover classes to calibrate the a priori roughness parameters the automatic technique in the hec ras software is used together with observations at four tidal stations including newbold burlington tacony palmyra bridge and philadelphia fig 1b we use the year 2011 for calibration since wses in this year are unusually high along the delaware river for the calibration the a priori roughness parameters are adjusted by flow level thus accounting for the vertical variation of the manning roughness parameter at each cross section the calibration is done by dividing the delaware river into 5 calibration reaches each of them bounded at both ends by a flow wse gage at each calibration reach the roughness parameter is calibrated using a multiplier approach the approach consists of adjusting the a priori parameter values in each calibration reach by multiplying them by a factor the average wse error is used as the objective function when calibrating hec ras 4 3 statistical postprocessor we use qr koenker 2005 koenker and bassett 1978 to statistically postprocess the wse forecasts at each cross section in the hydraulic model qr is a statistical method for estimating the quantiles of a conditional distribution we select qr because it has been shown to perform well in hydrological applications and outperform other postprocessors in terms of skill reliability and bias dogulu et al 2015 lópez lópez et al 2014 mendoza et al 2016 sharma et al 2017 weerts et al 2011 specifically we use qr to estimate the distribution of the wse errors conditional on the forecast wse ensemble mean at a given lead time and cross section along the study reach one of the advantages of qr is that it is less sensitive to outliers than classical regression methods given that it uses conditional quantiles rather than conditional means to develop the conditional distributions lópez lópez et al 2014 we apply the qr postprocessor to the untransformed forecasts since it has been shown that in hydrological forecasting applications the performance of qr is similar when using untransformed e g raw wse forecasts or transformed e g wse forecasts are normalized data lópez lópez et al 2014 sharma et al 2017 the qr model is given by 1 ε l τ a l τ b l τ f l where ε l τ is the error estimate at a lead time l for a quantile interval τ f l is the wse forecast ensemble mean at lead time l and a l τ and b l τ are the intercept and slope parameters from the linear regression in eq 1 the parameters a l τ and b l τ are determined by minimizing the sum of the residuals based on the training data as follows 2 min i 1 n w l τ ε l i ε l τ i where ε l i is computed as the difference between the ith paired baseline wse h i and the ensemble mean of the forecast wse f l i and w l τ is the quantile regression function at a lead time l for the τth quantile defined as 3 w l τ ξ l i τ 1 ξ l i if ξ l i 0 τ ξ l i if ξ l i 0 in the quantile regression function eq 3 ξ i is the residual term defined as the difference between ε l i and ε l τ i for the quantile τ and lead time l by allowing τ to vary τ 0 1 the entire conditional distribution of the wse errors can be described here we use 11 quantiles τ to cover the domain 0 1 the minimization in eq 2 is solved using linear programming koenker 2005 lastly to obtain the postprocessed forecast f l τ the following equation is applied 4 f l τ f l ε l τ in eq 4 the estimated error quantiles and the ensemble mean are added to form a calibrated discrete quantile relationship for a particular forecast lead time l and thus generate an ensemble wse forecast at a given cross section in the study reach qr is implemented using a leave one out approach 5 years for training and 1 year for postprocessing so as to not discard any data 4 4 verification strategy to verify the flow wse simulations and forecasts the following verification metrics are employed rmse percent bias pb mean absolute error mae nash sutcliffe efficiency nse willmott skill ws and mean continuous ranked probability skill score crpss see the appendix for their mathematical definition the verification analysis is performed over the period 2008 2013 to emphasize high flow events the analysis is conditioned to flows wses with exceedance probability of less than 0 05 further to verify the flow wse simulations we consider the entire study reach of the delaware river from trenton to marcus hooks fig 1b this allows us to properly assess the spatial extent of the upstream influence of tidal effects on the delaware river in contrast to verify the flood inundation forecasts we focus the analysis on the section of the study reach from trenton to newbold fig 1c where the effect of hydrometeorological uncertainty on the wse forecasts is expected to be significant three different sets of hourly flood inundation forecasts are generated and verified over the period 2008 2013 deterministic based on the gefsrv2 control member and raw and postprocessed ensembles based on the 11 members from gefsrv2 the flood inundation forecasts are verified at each cross section in the study reach against the baseline flood inundation simulations the baseline flood inundation simulations consist of wse simulations at each cross section in the study reach determined by forcing the hydraulic model with observed flows and wses thus the baseline flood inundation simulations are treated as truth when verifying the forecasts since they constitute the most accurate representation of wses along the study reach this step is necessary since there are no independent observations of flood inundation available this means that we mainly consider hydrometeorological uncertainty not hydraulic uncertainty when verifying the flood inundation forecasts 5 results and discussion this section is divided into five subsections the first subsection assesses the performance of the hydrological and hydraulic models the next three subsections show the results for the verification of the deterministic raw ensemble and postprocessed ensemble flood inundation forecasts respectively lastly we illustrate the mapping of the flood inundation forecasts using a selected flood event 5 1 performance of the hydrological and hydraulic models to assess the performance of the hydrological model hl rdhm we use the model to simulate flows at trenton and schuylkill fig 1b since these two locations jointly contribute the majority of freshwater inflows into the selected study reach the simulations are performed for the period 2008 2013 with the year 2007 used as warm up overall the performance of the hydrological model is satisfactory for example the nse is equal to 0 86 and 0 7 at trenton and schuylkill respectively table 2 likewise the pb shows that the performance of the hydrological model is reasonable with values less than â 4 table 2 the performance of the hydraulic model is also satisfactory for instance the mae rmse and pb are less than 0 14 m 0 18 m and 0 35 at all the four locations used for verification table 3 similarly in terms of skill the performance is also satisfactory with ws values between 0 79 and 0 93 in all cases table 3 the ws can vary from 0 to 1 with 1 indicating perfect agreement between observations and simulations note that we use ws to assess the performance of the hydraulic model simulations while nse is used for the hydrological model simulations the reason for this is that ws is a commonly used performance metric for evaluating wses in systems with tidal flows while nse is suitable and normally used for riverine flows the two locations with the lowest ws values newbold with ws 0 79 and burlington with ws 0 84 table 3 are likely affected by ineffective flow areas and wind effects which are not considered in the hec ras model in the vicinity of both locations there are obstructions to the river flows such as docks and bridges that may act as ineffective flow areas further in these gage locations for some of the highest flood events in the simulation period including events associated with tropical storms we observe a 2 hour phase error predicting early high tides which seems due to not accounting for wind effects for smaller flood events the phase error seems to be constant and approximately equal to 1 h thus ineffective flow areas and wind effects are likely affecting some the performance of the hydraulic model at these two locations nonetheless the overall performance of the hydraulic model is reasonable 5 2 verification of the deterministic flood inundation forecasts to generate the deterministic flood inundation forecasts at lead times of 1 7 days the hydraulic model is forced with flow forecasts over the period 2008 2013 at all boundaries except at marcus hook where observed wses for the same period are used in turn to produce the flow forecasts hl rdhm is forced with the gefsrv2 control member as opposed to the 11 ensemble members to produce a single deterministic flow forecast at each boundary condition of the hydraulic model over the same period 2008 2013 to verify the deterministic flood inundation forecasts the rmse pb and ws are used as the verification metrics in fig 3 the values of the verification metrics are shown at lead times of 1 3 and 7 days for the entire study reach from trenton to marcus hook the verification metrics indicate that the forecast error rmse in fig 3a and bias pb in fig 3b decrease while the skill ws in fig 3c increases significantly from upstream trenton to downstream marcus hook indeed the error and bias vanish and ws 1 at marcus hook river station 88 in fig 3 this is expected since as one moves in the downstream direction the storm surge and tidal effects become more dominant than the hydrometeorological forecast uncertainty recall that the downstream boundary at marcus hook is being forced with observed wses this means as previously indicated that only hydrometeorological uncertainty is accounted for and the uncertainty associated with the downstream tidal and storm surge predictions is not considered it is apparent in fig 3 that although the hydrometeorological uncertainty associated with flows at trenton propagates all the way downstream to marcus hook the uncertainty tends to become fairly insignificant after newbold river station 12 5 km in fig 3 for example the rmse fig 3a is less than 0 16 m and the pb fig 3b is within 3 at newbold the ws shows more variability at newbold fig 3c but the values are still within the range seen during model calibration table 3 another noticeable feature of fig 3 is the inflection point that occurs at approximately the 3 3 km river station this is more evident in fig 3a and b than fig 3c this inflection point separates two different regions in fig 3 the region to the left of the inflection point tends to be dominated by hydrometeorological forecast uncertainty while the region to the right is strongly influenced by tidal effects interestingly we find that this inflection point at 3 3 km appears in other verification metrics when plotted not shown here against the river station the inflection point and its location along the study reach seem to be a general feature of our flood inundation forecasts for the tidal riverine transitional zone of the delaware river it reflects the location where tide and other open ocean effects become less important and the hydrometeorological uncertainty tends to dominate the forecasts it demonstrates that the reach of the delaware river between trenton and newbold acts as a transitional zone between the riverine and tidal dominated regimes for the remainder of our results we emphasize this transitional zone where hydrometeorological uncertainty is significant the values of the verification metrics rmse pb and ws along the transitional zone are shown in fig 4 the verification maps in fig 4 are produced by interpolating the value of the metrics at each hydraulic model cross section i e the maps are not depicting flood inundation but rather the spatial variation of the metric values at the day 1 lead time fig 4 the verification metrics tend to change relatively little across the study reach for example the rmse decreases from 0 36 to 0 13 m from upstream to downstream fig 4a at the day 1 lead time while pb varies over the narrow range from 5 to 2 fig 4d this suggests that at the day 1 lead time the accuracy of the flood inundation forecasts is relatively high in contrast at the day 7 lead time the errors increase and tend to propagate further downstream the rmse varies from 1 05 m at the upstream end to 0 13 m at the downstream end fig 4c while the pb ranges from 26 to 2 fig 4f thus as the lead time increases the accuracy of the flood inundation forecasts declines the decline in accuracy with increasing lead time is attributed to the uncertainty in the weather forecasts precipitation forecasts from the gefsrv2 have previously been found to exhibit a strong negative conditional bias over the study area i e they tend to strongly underforecast large precipitation events sharma et al 2016 siddique et al 2015 yang et al 2017 note that the value of pb is always negative in fig 4d f thereby suggesting underforecasting in all cases the underforecasting bias in the gefsrv2 precipitation tends to grow as the lead time increases siddique et al 2015 here we show that such conditional bias propagates into the flood inundation forecasts to substantially influence their quality nonetheless in spite of the underforecasting bias in the flood inundation forecasts they remain skillful across lead times the skill measured by the ws stays close to 0 87 at the day 1 lead time fig 4g and varies between 0 87 and 0 4 at the day 7 lead time fig 4i it thus appears that there are benefits to generating flood inundation forecasts at the medium range lead times 3 days flood inundation forecasts have normally been investigated at short range lead times 3 days our results provide fresh insight into the medium range timescales however the results presented here are for deterministic hydrometeorological forecasts but it is well recognized that ensemble probabilistic hydrometeorological forecasts can be more skillful than deterministic ones cloke and pappenberger 2009 schaake et al 2007 more relevantly siddique and mejia 2017 showed that this is the case for flow forecasts in the delaware river basin generated by using the gefsrv2 as forcing hence we examine next the ability of ensemble hydrometeorological forecasts to improve the skill of flood inundation forecasts at short to medium range lead times 5 3 verification of the raw flood inundation ensemble forecasts to generate the raw flood inundation ensemble forecasts we force the hydraulic model with hourly flow ensemble 11 members forecasts at the upstream boundary of the delaware river and all the 10 tributaries the flow ensemble forecasts are obtained by forcing the hl rdhm with the 11 member gefsrv2 precipitation and near surface temperature outputs as was the case in the deterministic forecasting scenario we use wse observations at the downstream boundary at marcus hook to verify the raw flood inundation ensemble forecasts we use two deterministic metrics rmse and pb and one probabilistic metric crpss the crpss is a measure of forecast skill where a value of 1 means perfect forecast skill and a value of 0 means no skill i e same skill as the reference system the deterministic metrics are calculated using the mean of the 11 member wse ensemble forecasts at each cross section in the study reach while the crpss is determined using all the ensembles to emphasize high flow events the verification is conditioned as done in the deterministic forecasting scenario to wses values with exceedance probability of less than 0 05 in the observed wse probability distribution in addition we determine two different versions of the crpss one where the reference system is sampled climatology based on the baseline wse simulations and the other where the deterministic flood inundation forecasts are used as the reference system the latter is denoted as crpss2 and measures the skill gains from using ensembles relative to the single member deterministic case we find that the overall trends of the verification metrics for the raw flood inundation ensemble forecasts are qualitatively similar to those of the deterministic forecasts that is the rmse fig 5 a c and pb fig 5d f are reduced in the downstream direction at every lead time considered while the skill fig 5g i tends to increase furthermore the value of the verification metrics deteriorates as the lead time increases for instance the pb varies between 2 and 5 at the day 1 lead time fig 5d and over the much wider range of 2 and 28 at the day 7 lead time fig 5f as in the deterministic forecasts it is evident in fig 5 that the hydrometeorological uncertainty propagates further downstream in the case of the longer lead times contrasting the raw flood inundation ensemble forecasts against the deterministic ones the raw ensemble forecasts are slightly more accurate than the deterministic ones the rmse range is 0 11 0 99 m fig 5a c and 0 13 1 05 m fig 4a c for the raw ensemble and deterministic forecasts respectively however the raw ensembles also reflect the strong underforecasting bias of the gefsrv2 outputs as demonstrated by the negative values of the pb fig 5d f in terms of the forecast skill the values of the crpss2 show that the raw ensemble forecasts are able to improve the skill over the deterministic ones fig 5j l with relative skill gains that vary from 0 03 to 0 25 the relative gains increase with the lead time e g at the day 1 and 3 the crpss2 varies approximately over the range 0 03 0 05 and 0 12 0 15 respectively the latter indicates that as the forecast error increases the raw ensembles are able to provide greater improvements in skill this is the case because the quality of the reference system i e the deterministic forecasts used to compute the crpss2 declines faster than the raw ensembles as the lead time increases interestingly this skill trend reverses when one looks at the skill across the study reach at a particular lead time e g at day 7 the skill increases from 0 17 at the upstream boundary to 0 25 at the downstream end in fig 5l thus the skill benefits from using the raw ensembles may be maximized by reducing the uncertainty associated with the coupled hydrometeorological hydraulic system predictions this implies that improving both the process representation and parameter estimation calibration of the underlying hydrometeorological hydraulic system may be important for attaining the benefits associated with using the raw ensembles thus far our results fig 5 for the tidal riverine transitional zone of the delaware river indicate that weather ensembles can be used to improve flood inundation forecasting the results support the ability to potentially provide better flood related visual information to end users through the use of ensemble forecasts however further improvements to the raw flood inundation ensemble forecasts may be possible by correcting their systematic biases this is commonly done in hydrometeorological forecasting using statistical postprocessing techniques siddique and mejia 2017 hence we explore next the potential for statistical postprocessing to improve the flood inundation ensemble forecasts 5 4 verification of the statistically postprocessed flood inundation ensemble forecasts we employ qr to statistically postprocess the raw flood inundation ensemble forecasts over the period 2008 2013 using a leave one out approach consisting of 5 training years and 1 postprocessing year the qr postprocessor is applied to the raw wse ensemble forecasts at each lead time 1 7 days and cross section in the study reach to verify the postprocessed flood inundation forecasts we use the rmse pb and crpss the rmse and pb are computed using the ensemble mean while the crpss uses the ensembles as in the previous results we focus the analysis on high flow events by selecting flows with exceedance probability of less than 0 05 fig 6 summarizes the verification results for the postprocessed flood inundation ensemble forecasts overall the postprocessed flood inundation ensemble forecasts tend to perform better than raw ones across lead times fig 6 the improvement diminishes as one moves in the downstream direction i e as the effects of hydrometeorological uncertainty diminish the postprocessor is less able to produce gains following from this the gains appear somewhat greater for the longer lead times given that they tend to exhibit greater errors notice that the inflection point at the 3 3 km river station separating the hydrometeorological and tidal dominated zones is also visible in fig 6 as was the case in fig 3 for the deterministic forecasts specifically below the inflection point river stations 3 3 km the postprocessed flood inundation ensemble forecasts exhibit a more significant improvement compared to the raw ones in terms of the rmse fig 6a pb fig 6b and crpss fig 6c for example the rmse of the postprocessed flood inundation ensemble forecasts improves at the most upstream location trenton by 0 05 0 07 and 0 10 m at lead times of 1 3 and 7 days fig 6a respectively at the 4 km river station the improvements in rmse are relatively small 0 01 0 02 and 0 04 m at lead times of 1 3 and 7 days fig 6a respectively in terms of the pb the improvements i e the percent reduction in the pb values at trenton are 8 17 and 11 47 at the day 3 and 7 lead time fig 6b respectively while there is no improvement at the day 1 lead time the skill improves by 8 4 8 47 and 22 96 at lead times of 1 3 and 7 days fig 6c respectively as was the case with the raw ensemble forecasts the improvements in skill from postprocessing are greater at the longer lead times thereby suggesting that postprocessing of wses may be particularly beneficial at the medium range timescales in summary we find that postprocessing is able to improve upon the raw flood inundation ensemble forecasts we illustrate next the effects of ensembles and postprocessing on flood inundation mapping using a selected storm event 5 5 comparison of flood inundation forecast maps for a selected flood event to illustrate the mapping of the flood inundation forecasts we select the winter storm event that occurred on march 12 15 2010 this winter event had a significant impact on the middle atlantic region mar and particularly on the delaware river basin causing heavy rainfall in the states of new jersey and pennsylvania for example the accumulated rainfall over a 7 day period ending on march 16 2010 was 6 to 8 in in several counties in new jersey usgs 2010 furthermore 2 weeks prior to the selected event the mar was impacted by a snow storm that resulted in the accumulation of relatively large amounts of snow in the upper parts of the delaware river basin it is estimated that the accumulated snow contributed 6 to 10 in of snow water equivalent at the time of the march 12 15 storm these antecedent conditions contributed significantly during the selected event to increase runoff generation in the delaware river basin and ultimately flows along the study reach the measured peak flow at trenton was 2146 42 m3 s with an annual recurrence interval of 2 years usgs 2010 at the tributaries the annual recurrence interval was higher between 3 and 7 years usgs 2010 for the march 12 15 2010 storm we compare the quality of the deterministic fig 7 a c raw ensemble fig 7d f and postprocessed ensemble fig 7g i flood inundation forecast maps at lead times of 1 3 and 7 days along the tidal riverine transitional zone of the delaware river for the mapping of the latter two we use the mean of the ensemble flood inundation forecasts as in the previous verification results the forecast maps are compared against the baseline flood inundation simulation maps note that to generate the flood inundation maps we interpolate the maximum water depth at each cross section onto the 1 1 m2 terrain grid using the ras mapper interpolation technique thus at each lead time we actually compare the maximum water depth forecast map for the selected event against the baseline simulation one to assess the quality of the inundation maps on a cell by cell basis we classify each cell into one of the following 3 categories agree underforecast or overforecast we classify cells as agree when both the forecast and baseline map show inundation cells are classified as underforecasting when the baseline map shows inundation but not the forecast map and vice versa for overforecasting by contrasting the deterministic fig 7a c and raw ensemble fig 7d f flood inundation forecast maps it is apparent that the quality of the maps declines as the lead time increases for instance the percent of overforecast grid cells increases from 6 24 day 1 lead time fig 7a to 45 78 day 7 lead time fig 7c for the deterministic maps notice in fig 7 that the tendency is clearly in this case for the forecast maps to overestimate flood inundation even though the overall trend of the forecasts for the 6 year verification period as was shown before is to underestimate the baseline simulations at the day 1 lead time the maps in fig 7 are similar the deterministic fig 7a raw ensemble fig 7d and postprocessed ensemble fig 7g forecast inundation maps exhibit comparable quality with little overforecasting the overforecasting is 6 24 1 34 and 1 82 respectively as the lead time increases however differences emerge among the forecast maps contrasting the day 7 forecasts it is apparent that the postprocessed maps fig 7i have less overforecast grid cells than the deterministic fig 7c and raw ensemble fig 7f maps indeed the overforecasting is 45 78 16 29 and 4 29 for the deterministic raw ensemble and postprocessed ensemble flood inundation forecast maps respectively in summary the results in fig 7 show that the use of weather ensembles and statistical wse postprocessing can result in improved flood inundation forecast maps the results suggest that it may be possible to enhance the spatial representation and potentially the effectiveness of flood forecast communication to end users through the use of ensembles and postprocessing 6 conclusions based on the verification results for the flood inundation forecasts generated with the coupled hydrometeorological hydraulic system the following key conclusions are emphasized the verification metrics for the deterministic and raw flood inundation ensemble forecasts show qualitatively similar behavior specifically the pb shows that the deterministic and raw flood inundation ensemble forecasts tend to largely underestimate the prediction of wses across short and medium range timescales the underestimation increases with increasing lead time and it is higher for the upstream cross sections of the delaware river where wses are dominated by inland flows and hydrometeorological forecast uncertainty hydrometeorological forecast uncertainty alone can introduce significant errors to the flood inundation forecasts during high flow conditions along the tidal riverine transitional zone of the delaware river particularly at the later lead times for example flood inundation forecasts in the transitional zone show rmses that can be as high as 1 m at the day 7 lead time the raw flood inundation ensemble forecasts show skill improvements relative to the deterministic flood inundation forecasts the improvements are relatively small at the initial lead times days 1 2 but increase at the later ones thus the use of weather ensembles is in this case able to improve the quality of flood inundation forecasts statistical postprocessing of wses improves the skill of the flood inundation ensemble forecasts over the raw ensemble ones the improvement is evident across all lead times 1 7 days but it is higher at the medium range timescales 3 days through a flood mapping demonstration for a selected flood event it is shown that both weather ensembles and statistical postprocessing can improve the spatial representation of floods the demonstration shows that the percentage of overforecasted inundated areas may be reduced by jointly using weather ensembles and statistical postprocessing in this study the proposed hydrometeorological hydraulic forecasting system is implemented along the delaware river however under the condition that terrain bathymetry hydrometeorological and tidal data of similar quality are available in other river systems which is likely the case for major tidal rivers in the u s the proposed system could be applied to those rivers the latter is facilitated by the proposed system by relying on global weather forecasts gefsrv2 and modeling tools that are available to forecasters and the public there are shortcomings to the present study that could be improved in the future the verification of flood inundation forecasts could consider coastal forecasts and the hydraulic model the effects of wind forcing and independent observations of flood inundation could be used when verifying the forecasts to overcome these shortcomings however substantial and coordinated efforts may be necessary that are beyond the scope of the present study for instance the availability of medium range coastal reforecasts will depend on the capabilities and interest of the ocean coastal modeling communities in generating those forecasts similarly the availability of multiyear high quality independent flood inundation observations may require coordinated efforts among different agencies responsible for generating flood related data and information acknowledgements the first author acknowledges the funding support provided by the ecuadorian government through the secretariat for higher education science technology and innovation ecuador the authors are thankful to susan patterson of the philadelphia water department pwd and josef kardos formerly of pwd for making the bathymetry data available and offering useful suggestions about the research appendix mean absolute error the mean absolute error mae quantifies the absolute average error between the simulated values and their corresponding observations the mae is expressed as follows a 1 mae 1 n i 1 n s i o i where s i and o i denote the simulated and observed wse respectively at time i root mean square error the root mean square error rmse quantifies the absolute average error between the predicted values and their corresponding reference values the rmse is given by a 2 rmse i 1 n p i r i 2 n where p i and r i denote the predicted and reference wse respectively at time i when verifying the model simulations these variables take the values of the simulated and observed wse flow respectively when verifying the model forecasts p i and r i are set equal to the forecasted and observed wse flow respectively percent bias the percent bias pb measures the average tendency of the predicted values to be larger or smaller than the reference values the pb is given by a 3 pb i 1 n p i r i i 1 n r i 100 where p i and r i denote the predicted simulated or forecasted and reference observed wse flow respectively at time i nash sutcliffe efficiency the nash sutcliffe efficiency nse nash and sutcliffe 1970 is a commonly used metric to assess the accuracy of hydrologic models to predict flows the nse can vary between and 1 with values closer to 1 indicating a good agreement between the simulated and observed flows while a negative value indicates that the mean of the observations is a better predictor than the simulated flows the nse is defined as a 4 nse 1 i 1 n s i o i 2 i 1 n s i o i 2 where s i o i and o i are the simulated observed and mean observed flow respectively at time i willmott skill the willmott skill ws willmott 1981 is a widely used metric to evaluate the performance of hydrodynamic models for tidal rivers and estuaries the ws can vary between 0 and 1 where a ws value of 1 represents a perfect agreement between predictions and reference values and a value of 0 represents no skill the ws is given by a 5 ws 1 i 1 n p i r i 2 i 1 n p i p i r i r i 2 where p i r i p i and r i denote the predicted simulated or forecasted reference observed mean predicted mean simulated or forecasted and mean reference mean observed wse respectively at time i mean continuous ranked probability skill score the continuous ranked probability score crps is a probabilistic metric used to assess the skill of ensemble forecasts relative to a reference system let the probability distribution function of the wse ensemble forecasts be p h and the actual wse value be h a then the crps is defined as a 6 crps p f h p a h 2 d h where p f h and p a h are cumulative distribution functions given by a 7 p f h h p h d h and a 8 p a h h h h r respectively h is the heaviside function which is 1 if the argument is positive and zero otherwise to measure the skill of the main forecast system relative to a reference one we use the mean continuous ranked probability skill score crpss defined as a 9 crpss 1 crps main crps reference where crps main and crps reference are the average crps values for the main and reference forecast system respectively the crpss ranges from to 1 with negative scores indicating that the system to be evaluated has worse crps than the reference forecast system while positive scores indicate a higher skill for the main forecast system relative to the reference one with 1 indicating perfect skill 
6809,we investigate the ability to improve flood inundation forecasts at short to medium range 1 7 days timescales through weather ensembles and statistical water surface elevation wse postprocessing to generate the flood inundation forecasts a one dimensional hydraulic model namely the hydrologic engineering center s river analysis system hec ras is coupled to a regional hydrological ensemble prediction system rheps the rheps is comprised of i hydrometeorological observations ii weather ensembles from the national centers for environmental prediction global ensemble forecast system reforecast version 2 gefsrv2 and iii hydrology laboratory research distributed hydrologic model hl rdhm as the hydrological model the coupled rheps hydraulic system is evaluated along the tidal delaware river near the city of philadelphia pennsylvania u s for the evaluation emphasis is placed on the tidal riverine transitional zone of the delaware river and the downstream propagation of hydrometeorological uncertainty through the flood inundation forecasts the coupled system is used to generate hourly flood inundation forecasts at lead times from 1 to 7 days over the period 2008 2013 additionally wses from the coupled system are statistically postprocessed using quantile regression qr results show that the raw flood inundation ensemble forecasts exhibit higher skill than the deterministic ones we also find that statistical postprocessing improves the skill of the raw flood inundation ensemble forecasts with greater improvements at the longer lead times 3 days overall we find that both weather ensembles and statistical wse postprocessing can be used to enhance the skill of flood inundation forecasts at short to medium range timescales in turn this may serve to enhance the spatial representation of flood forecasts several days in advance which could contribute in the future to making flood forecast communication and warnings more effective keywords hydrologic hydraulic modeling flood mapping statistical postprocessing numerical weather prediction weather ensembles 1 introduction floods are among the costliest natural disasters in the world causing direct economic losses of over 1 trillion between 1980 and 2013 ipcc 2012 in the united states u s floods are among the deadliest weather related disasters killing over 4500 people in the 1959 2005 period ashley and ashley 2008 operational flood forecasting systems can help authorities and the public to be better prepared against floods cloke and pappenberger 2009 flood forecasting is usually done by forcing a hydrological model with numerical weather prediction nwp model outputs addor et al 2011 fan et al 2014 pappenberger et al 2011 saleh et al 2016 siddique and mejia 2017 thus errors in the nwp outputs are propagated into the flood forecasts requiring the explicit consideration of uncertainty to consider uncertainty the preferred approach is to use weather ensembles representing multiple plausible realizations of the nwp model outputs addor et al 2011 cloke and pappenberger 2009 demeritt et al 2010 fan et al 2014 khan et al 2015 olsson and lindström 2008 thielen et al 2009a ultimately the weather ensembles are used to force the hydrological model and produce probabilistic flow forecasts gouweleeuw et al 2005 pappenberger et al 2011 saleh et al 2016 thielen et al 2009b when the goal is to generate flood inundation an additional step is required in the hydrometeorological forecasting chain to translate the flood forecasts into flood inundation forecasts hereafter flood inundation is used to indicate the combined spatial and temporal representation of water surface elevations wses flood inundation predictions can be generated in different ways buahin et al 2017 christian et al 2013 di baldassarre et al 2010 leedal et al 2010 sarhadi et al 2012 a common approach is to couple a hydrological model with a hydraulic or hydrodynamic model grimaldi et al 2013 lian et al 2007 mejia and reed 2011 nguyen et al 2016 yamazaki et al 2011 yu et al 2006 it is important here to recognize that the coupled hydrologic hydraulic models can be employed in two fundamentally different ways simulation or forecasting mode flood inundation simulations use simulated flows as boundary conditions to the hydraulic model generated by forcing the hydrological model with observed meteorological data or a design storm event flood inundation simulation is employed in a number of applications e g flood policy flood insurance studies infrastructure design etc using both deterministic castro bolinaga and diplas 2014 costabile et al 2015 grimaldi et al 2013 ozdemir et al 2013 sampson et al 2015 and probabilistic approaches bates et al 2004 merz et al 2007 neal et al 2013 sarhadi et al 2012 however flood inundation simulation has the major drawback that it does not provide explicit information about near future flood hazards and risks in contradistinction flood inundation forecasts use hydrometeorological forecasts to force the hydraulic model providing near future flood related information flood inundation forecasts can also be generated using deterministic di baldassarre et al 2010 mashriqui et al 2014 nguyen et al 2016 or probabilistic approaches di baldassarre et al 2010 garcía pintado et al 2015 pappenberger et al 2005b schumann et al 2013 however only few studies have investigated the benefits of using probabilistic approaches to produce flood inundation forecasts buahin et al 2017 georgas et al 2016 pappenberger et al 2005b schumann et al 2013 van cooten et al 2011 that is the vast majority of deterministic and probabilistic flood inundation studies have been done in simulation mode the findings from those studies are not directly transferable to a forecasting context because among other reasons weather forecast uncertainties can have a strong and non linear influence on hydrological predictions for flood inundation forecast mapping three main approaches have been implemented i equilibrium nobre et al 2011 ii steady buahin et al 2017 noaa and dewberry 2011 and iii hydrodynamic pappenberger et al 2005b van cooten et al 2011 the former also called the bathtub method represents flood extent by horizontally spreading the forecasted wse at a point in a river reach to neighboring digital elevation model dem cells with lower elevation nobre et al 2011 teng et al 2017 this approach is intrinsically different from the other two as it does not require the use of a hydraulic model in the steady approach hydraulic models and dem data are used off line to produce steady state hydraulic profiles and flood inundation maps for a specified range of flow forecasts at a particular river reach thus each flow forecast is paired with a spatially referenced flood inundation map to build a digital library of spatially localized reach scale maps buahin et al 2017 noaa and dewberry 2011 a limitation of the steady approach is that it cannot handle highly dynamic river conditions e g backwater and tidal reaches the hydrodynamic approach is the most general of the three mapping approaches it consists of using hydrometeorological forecasts to force in real time a hydraulic model to seamlessly produce flood inundation forecast maps pappenberger et al 2005b van cooten et al 2011 previous studies implementing the hydrodynamic approach in a forecasting context however have been focused on a single storm event pappenberger et al 2005b short range forecasts georgas et al 2016 and or have not considered weather ensembles van cooten et al 2011 our primary goal with this study is to investigate the effect of hydrometeorological uncertainty on the skill of ensemble flood inundation forecasts at short to medium range lead times 1 to 7 days to this end we generate flood inundation forecasts by coupling a regional hydrological ensemble prediction system rheps siddique and mejia 2017 with a one dimensional hydraulic model namely the hydrologic engineering center s river analysis system hec ras the rheps uses the hydrology laboratory research distributed hydrologic model hl rdhm koren et al 2004 as the hydrological model and ensemble weather forecasts precipitation and near surface temperature from the national centers for environmental prediction 11 member global ensemble forecast system reforecast version 2 gefsrv2 hamill et al 2013 additionally since probabilistic flood forecasts generated from weather ensembles tend to exhibit systematic biases addor et al 2011 siddique and mejia 2017 we employ quantile regression qr to statistically postprocess i e remove systematic biases the ensemble flood inundation forecasts to rigorously verify the forecasts we generate and verify hourly forecasts at lead times of 1 7 days over multiple years 2008 2013 the fact that the verification is done over multiple years is computationally intensive but important it makes the estimation of verification metrics more robust than using as most often done a single or few flood events to our knowledge the generation and verification of medium range flood inundation forecasts through the joint implementation of a coupled hydrometeorological hydraulic system weather ensembles and statistical postprocessing over multiple reforecast years has not been previously investigated the research questions motivating this study are as follows what is the effect of hydrometeorological uncertainty on the skill of medium range flood inundation forecasts are ensemble probabilistic flood inundation forecasts more skillful than deterministic ones can weather ensembles and statistical postprocessing of wses improve flood inundation forecasts the remainder of this paper is structured as follows sections 2 and 3 describe the study area and datasets used respectively section 4 describes the methodology followed to generate postprocess and verify the ensemble flood inundation forecasts section 5 presents and discusses the main results lastly section 6 outlines key conclusions 2 study area the tidal delaware river near philadelphia pennsylvania is selected as the study area fig 1 a and b the length of the selected study reach of the delaware river is 88 km the study reach starts upstream at trenton new jersey and ends downstream at marcus hook pennsylvania fig 1a the 10 main tributaries that join the delaware river along the study reach are all included in the analysis fig 1b at trenton river dynamics are dominated by inflows from the delaware river basin while tidal currents dominate at marcus hook tides are mainly semi diurnal with a mean range of 1 5 m at the mouth and propagate as progressive waves with speeds of 1 m s galperin and mellor 1990 the drainage areas at trenton and marcus hook are 17 552 and 26 760 km2 respectively table 1 to generate and verify the flood inundation forecasts we emphasize the tidal riverine transitional zone of the delaware river which extends from trenton at the upstream boundary to newbold at the downstream end fig 1c this transitional zone is of particular interest because it is often a blind spot of operational forecasting for example in the case of the delaware river the transitional zone is currently not part of the noaa s regional operational hydrological or coastal forecasting systems the noaa s operational hydrological forecast domain ends at trenton while the coastal forecast domain of the noaa s delaware bay operational forecasting system extends upstream to newbold 3 datasets 3 1 terrain to represent the terrain we combine a dem and bathymetry data the dem is provided by the pennsylvania department of conservation and natural resources pdcnr who derived the dem from airborne 2007 light detection and ranging lidar data the pdcnr preprocessed the raw lidar data to get a bare earth raster dem with 1 m horizontal accuracy and 18 5 cm root mean square error rmse vertical accuracy the bathymetry data is provided by the philadelphia water department pwd the data combines a bathymetric dataset produced by the national oceanic and atmospheric administration noaa and a bathymetric survey performed by the pwd at several of the tributaries in the study reach to combine the dem and bathymetry data we follow the three step approach of gesch and wilson 2001 first a buffer zone of 500 m from the shoreline is defined in order to interpolate the dem data for the floodplains and the bathymetric data for the rivers second all the dem data inside the buffer zone is converted into a 1 m point cloud the points that lie inside the open water areas are excluded since lidar data are inaccurate in those areas third with all data in point format we use arcgis to interpolate the bathymetry and lidar data together the final combined dem is a seamless terrain model of the river and floodplains with 1 1 m2 spatial resolution 3 2 flow and tidal gages we use observed flows and wse data for the period 2008 2013 to calibrate and verify the coupled hydrometeorological hydraulic system the observed flow and wse data used are summarized in table 1 the flow data are provided by the usgs at hourly resolution a total of 9 usgs flow gages are used the only flow gage located on the delaware river is the one at trenton gage number 01463500 the other flow gages are for the major tributaries to the study reach table 1 the wse data are provided by noaa there are a total of 5 sub hourly noaa tidal stations in the study reach table 1 the station at marcus hook pennsylvania constitutes the downstream boundary of the study reach fig 1b during the study period 2008 2013 all the gages have some missing values table 1 ranging from only 4 at mantua new jersey gage number 01464500 to 2881 at pennypack pennsylvania gage number 01467048 to estimate the missing values linear interpolation is performed for cases where the consecutive number of missing data is less than 48 h and or flow is in low conditions i e baseflow in the case of pennypack given that more than 2 weeks of data are missing the missing data are reconstructed using simulated flows from hl rdhm 3 3 precipitation and near surface temperature 3 3 1 observed we use observed precipitation and near surface temperature data over the period 2008 2013 as weather forcing to hl rdhm to simulate flows and initialize the rheps at the upstream boundary of the study reach and all the tributaries for the observed weather forcing we use multi sensor precipitation estimates mpes and gridded near surface observed temperature the mpes are provided by the noaa s middle atlantic river forecast center marfc siddique and mejia 2017 the resolution of the mpes is hourly over the regular 4 x 4 km2 hydrologic rainfall analysis projection hrap grid rafieeinasab et al 2015 zhang et al 2011 the observed gridded temperature data at 4 4 km2 resolution are also provided by the marfc the gridded temperature data are used by hl rdhm to estimate evapotranspiration and snow melt and accumulation snow melt is an important source of runoff in the upper parts of the delaware river basin 3 3 2 forecasted we force hl rdhm with precipitation and near surface temperature outputs from the gefsrv2 over the period 2008 2013 to produce deterministic based on the control member of the gefsrv2 and probabilistic based on the 11 member gefsrv2 ensemble flow forecasts at lead times of 1 7 days the forecasts are produced at the upstream boundary of the study reach and all the 10 tributaries the gefsrv2 outputs are retrospective forecasts 1 16 days generated using the gefs model version 9 0 1 the spatial resolution of the gefs is 55 km for days 1 8 the gefsrv2 consists of 11 members including the control member the control member is generated by using unperturbed initial conditions and the other members are generated by stochastically perturbing the initial conditions of the model further details about the gefsrv2 dataset are provided elsewhere hamill et al 2013 the gefsrv2 data are interpolated onto the 4 4 km2 hrap grid using bilinear interpolation 4 methodology this section is divided into four subsections that describe the main components of the proposed hydrometeorological hydraulic forecasting system including the hydrological model hydraulic model statistical postprocessor and verification strategy fig 2 shows the inputs outputs from each of and the interdependence among the different system components 4 1 hydrological model hl rdhm koren et al 2004 is used to both simulate hourly flows and generate deterministic and probabilistic hourly flow forecasts at the 11 forecast points upstream boundary at trenton plus 10 tributaries in the study reach to simulate hourly flows we force hl rdhm with the gridded precipitation and near surface temperature observation data while the hourly flow forecasts are generated by forcing hl rdhm with the gefsrv2 precipitation and near surface temperature outputs hl rdhm is a distributed hydrological model that operates on a regular square grid we use a 4 x 4 km2 grid cell resolution each grid cell consists of two main components water balance and hillslope and channel routing koren et al 2004 for the water balance component we use the sacramento soil accounting and heat transfer sac ht and the snow 17 submodels the sac ht is used for rainfall runoff generation and snow 17 for snow accumulation and melt the hillslope and channel routing are done using a nonlinear kinematic wave approach further information about hl rdhm in the context of recent applications can be found elsewhere lee et al 2015 nguyen et al 2016 rafieeinasab et al 2015 spies et al 2015 thorstensen et al 2016 wood et al 2016 we run the hl rdhm model using calibrated parameter multiplier values the calibrated parameters are those associated with baseflow percolation evaporation overland flow and channel routing during the calibration process the parameter fields are adjusted rather than the actual parameter values for this each a priori parameter field is multiplied by a factor the calibrated multiplier values are obtained from a previous study siddique and mejia 2017 for the delaware and schuylkill river basins these same multiplier values are applied to the other 9 tributaries in the study reach this is deemed reasonable given the hydrologic similarity due to proximity of all tributaries further these smaller tributaries have little impact on flows along the study reach i e the delaware and schuylkill rivers account for 70 of the total inland flows into the study reach 4 2 hydraulic model hec ras brunner 2016 is used in fully dynamic mode as the hydraulic model to both simulate hourly wses and produce deterministic and probabilistic hourly flood inundation forecasts along the study reach we select hec ras because it is used in operations buahin et al 2017 mashriqui et al 2014 and has shown good capabilities for flood inundation mapping merwade et al 2008 pappenberger et al 2005a the hec ras module ras mapper is used in this study to create gridded flood inundation maps the ras mapper is used to spatially interpolate wse predictions at individual cross sections along the study reach onto a terrain grid of 1 1 m2 resolution to develop the hec ras model the terrain model is discretized using 373 cross sections out of which 222 are used to model the main delaware river with an average spacing of 400 m and the remaining ones are used to model the tributaries to perform the hydraulic simulations and calibrate the model hourly observed wses and flows are used as boundary conditions to the model observed wses are used at the downstream tidal boundary at marcus hook fig 1b while observed flows are used at the upstream boundary at trenton and all the tributaries except for two ungaged tributaries rancocas and woodbury in fig 1b for these two tributaries hourly hl rdhm simulations are used since observations are not available when running the hec ras model in forecasting mode hourly flow forecasts from hl rdhm are used at the upstream boundary and all the tributaries however at the downstream boundary i e marcus hook we use observed wses when running hec ras in forecasting mode the main reason for this is that in contrast to the availability of long term weather reforecasts through the gefsrv2 dataset there are no coastal wse reforecasts available that cover multiple years because of this shortcoming we use observed wses at marcus hook when generating the flood inundation forecasts this means that our flood inundation forecasts do not consider the uncertainty associated with downstream wses instead the emphasis here is solely on the effect of hydrometeorological uncertainty on the flood inundation forecasts to calibrate the hec ras model the manning roughness parameter is adjusted at each cross section we estimate a priori roughness parameters for the main channel and floodplains using both land use land cover data from the 2011 national land cover dataset homer et al 2015 and tabulated manning roughness values for different land use land cover classes to calibrate the a priori roughness parameters the automatic technique in the hec ras software is used together with observations at four tidal stations including newbold burlington tacony palmyra bridge and philadelphia fig 1b we use the year 2011 for calibration since wses in this year are unusually high along the delaware river for the calibration the a priori roughness parameters are adjusted by flow level thus accounting for the vertical variation of the manning roughness parameter at each cross section the calibration is done by dividing the delaware river into 5 calibration reaches each of them bounded at both ends by a flow wse gage at each calibration reach the roughness parameter is calibrated using a multiplier approach the approach consists of adjusting the a priori parameter values in each calibration reach by multiplying them by a factor the average wse error is used as the objective function when calibrating hec ras 4 3 statistical postprocessor we use qr koenker 2005 koenker and bassett 1978 to statistically postprocess the wse forecasts at each cross section in the hydraulic model qr is a statistical method for estimating the quantiles of a conditional distribution we select qr because it has been shown to perform well in hydrological applications and outperform other postprocessors in terms of skill reliability and bias dogulu et al 2015 lópez lópez et al 2014 mendoza et al 2016 sharma et al 2017 weerts et al 2011 specifically we use qr to estimate the distribution of the wse errors conditional on the forecast wse ensemble mean at a given lead time and cross section along the study reach one of the advantages of qr is that it is less sensitive to outliers than classical regression methods given that it uses conditional quantiles rather than conditional means to develop the conditional distributions lópez lópez et al 2014 we apply the qr postprocessor to the untransformed forecasts since it has been shown that in hydrological forecasting applications the performance of qr is similar when using untransformed e g raw wse forecasts or transformed e g wse forecasts are normalized data lópez lópez et al 2014 sharma et al 2017 the qr model is given by 1 ε l τ a l τ b l τ f l where ε l τ is the error estimate at a lead time l for a quantile interval τ f l is the wse forecast ensemble mean at lead time l and a l τ and b l τ are the intercept and slope parameters from the linear regression in eq 1 the parameters a l τ and b l τ are determined by minimizing the sum of the residuals based on the training data as follows 2 min i 1 n w l τ ε l i ε l τ i where ε l i is computed as the difference between the ith paired baseline wse h i and the ensemble mean of the forecast wse f l i and w l τ is the quantile regression function at a lead time l for the τth quantile defined as 3 w l τ ξ l i τ 1 ξ l i if ξ l i 0 τ ξ l i if ξ l i 0 in the quantile regression function eq 3 ξ i is the residual term defined as the difference between ε l i and ε l τ i for the quantile τ and lead time l by allowing τ to vary τ 0 1 the entire conditional distribution of the wse errors can be described here we use 11 quantiles τ to cover the domain 0 1 the minimization in eq 2 is solved using linear programming koenker 2005 lastly to obtain the postprocessed forecast f l τ the following equation is applied 4 f l τ f l ε l τ in eq 4 the estimated error quantiles and the ensemble mean are added to form a calibrated discrete quantile relationship for a particular forecast lead time l and thus generate an ensemble wse forecast at a given cross section in the study reach qr is implemented using a leave one out approach 5 years for training and 1 year for postprocessing so as to not discard any data 4 4 verification strategy to verify the flow wse simulations and forecasts the following verification metrics are employed rmse percent bias pb mean absolute error mae nash sutcliffe efficiency nse willmott skill ws and mean continuous ranked probability skill score crpss see the appendix for their mathematical definition the verification analysis is performed over the period 2008 2013 to emphasize high flow events the analysis is conditioned to flows wses with exceedance probability of less than 0 05 further to verify the flow wse simulations we consider the entire study reach of the delaware river from trenton to marcus hooks fig 1b this allows us to properly assess the spatial extent of the upstream influence of tidal effects on the delaware river in contrast to verify the flood inundation forecasts we focus the analysis on the section of the study reach from trenton to newbold fig 1c where the effect of hydrometeorological uncertainty on the wse forecasts is expected to be significant three different sets of hourly flood inundation forecasts are generated and verified over the period 2008 2013 deterministic based on the gefsrv2 control member and raw and postprocessed ensembles based on the 11 members from gefsrv2 the flood inundation forecasts are verified at each cross section in the study reach against the baseline flood inundation simulations the baseline flood inundation simulations consist of wse simulations at each cross section in the study reach determined by forcing the hydraulic model with observed flows and wses thus the baseline flood inundation simulations are treated as truth when verifying the forecasts since they constitute the most accurate representation of wses along the study reach this step is necessary since there are no independent observations of flood inundation available this means that we mainly consider hydrometeorological uncertainty not hydraulic uncertainty when verifying the flood inundation forecasts 5 results and discussion this section is divided into five subsections the first subsection assesses the performance of the hydrological and hydraulic models the next three subsections show the results for the verification of the deterministic raw ensemble and postprocessed ensemble flood inundation forecasts respectively lastly we illustrate the mapping of the flood inundation forecasts using a selected flood event 5 1 performance of the hydrological and hydraulic models to assess the performance of the hydrological model hl rdhm we use the model to simulate flows at trenton and schuylkill fig 1b since these two locations jointly contribute the majority of freshwater inflows into the selected study reach the simulations are performed for the period 2008 2013 with the year 2007 used as warm up overall the performance of the hydrological model is satisfactory for example the nse is equal to 0 86 and 0 7 at trenton and schuylkill respectively table 2 likewise the pb shows that the performance of the hydrological model is reasonable with values less than â 4 table 2 the performance of the hydraulic model is also satisfactory for instance the mae rmse and pb are less than 0 14 m 0 18 m and 0 35 at all the four locations used for verification table 3 similarly in terms of skill the performance is also satisfactory with ws values between 0 79 and 0 93 in all cases table 3 the ws can vary from 0 to 1 with 1 indicating perfect agreement between observations and simulations note that we use ws to assess the performance of the hydraulic model simulations while nse is used for the hydrological model simulations the reason for this is that ws is a commonly used performance metric for evaluating wses in systems with tidal flows while nse is suitable and normally used for riverine flows the two locations with the lowest ws values newbold with ws 0 79 and burlington with ws 0 84 table 3 are likely affected by ineffective flow areas and wind effects which are not considered in the hec ras model in the vicinity of both locations there are obstructions to the river flows such as docks and bridges that may act as ineffective flow areas further in these gage locations for some of the highest flood events in the simulation period including events associated with tropical storms we observe a 2 hour phase error predicting early high tides which seems due to not accounting for wind effects for smaller flood events the phase error seems to be constant and approximately equal to 1 h thus ineffective flow areas and wind effects are likely affecting some the performance of the hydraulic model at these two locations nonetheless the overall performance of the hydraulic model is reasonable 5 2 verification of the deterministic flood inundation forecasts to generate the deterministic flood inundation forecasts at lead times of 1 7 days the hydraulic model is forced with flow forecasts over the period 2008 2013 at all boundaries except at marcus hook where observed wses for the same period are used in turn to produce the flow forecasts hl rdhm is forced with the gefsrv2 control member as opposed to the 11 ensemble members to produce a single deterministic flow forecast at each boundary condition of the hydraulic model over the same period 2008 2013 to verify the deterministic flood inundation forecasts the rmse pb and ws are used as the verification metrics in fig 3 the values of the verification metrics are shown at lead times of 1 3 and 7 days for the entire study reach from trenton to marcus hook the verification metrics indicate that the forecast error rmse in fig 3a and bias pb in fig 3b decrease while the skill ws in fig 3c increases significantly from upstream trenton to downstream marcus hook indeed the error and bias vanish and ws 1 at marcus hook river station 88 in fig 3 this is expected since as one moves in the downstream direction the storm surge and tidal effects become more dominant than the hydrometeorological forecast uncertainty recall that the downstream boundary at marcus hook is being forced with observed wses this means as previously indicated that only hydrometeorological uncertainty is accounted for and the uncertainty associated with the downstream tidal and storm surge predictions is not considered it is apparent in fig 3 that although the hydrometeorological uncertainty associated with flows at trenton propagates all the way downstream to marcus hook the uncertainty tends to become fairly insignificant after newbold river station 12 5 km in fig 3 for example the rmse fig 3a is less than 0 16 m and the pb fig 3b is within 3 at newbold the ws shows more variability at newbold fig 3c but the values are still within the range seen during model calibration table 3 another noticeable feature of fig 3 is the inflection point that occurs at approximately the 3 3 km river station this is more evident in fig 3a and b than fig 3c this inflection point separates two different regions in fig 3 the region to the left of the inflection point tends to be dominated by hydrometeorological forecast uncertainty while the region to the right is strongly influenced by tidal effects interestingly we find that this inflection point at 3 3 km appears in other verification metrics when plotted not shown here against the river station the inflection point and its location along the study reach seem to be a general feature of our flood inundation forecasts for the tidal riverine transitional zone of the delaware river it reflects the location where tide and other open ocean effects become less important and the hydrometeorological uncertainty tends to dominate the forecasts it demonstrates that the reach of the delaware river between trenton and newbold acts as a transitional zone between the riverine and tidal dominated regimes for the remainder of our results we emphasize this transitional zone where hydrometeorological uncertainty is significant the values of the verification metrics rmse pb and ws along the transitional zone are shown in fig 4 the verification maps in fig 4 are produced by interpolating the value of the metrics at each hydraulic model cross section i e the maps are not depicting flood inundation but rather the spatial variation of the metric values at the day 1 lead time fig 4 the verification metrics tend to change relatively little across the study reach for example the rmse decreases from 0 36 to 0 13 m from upstream to downstream fig 4a at the day 1 lead time while pb varies over the narrow range from 5 to 2 fig 4d this suggests that at the day 1 lead time the accuracy of the flood inundation forecasts is relatively high in contrast at the day 7 lead time the errors increase and tend to propagate further downstream the rmse varies from 1 05 m at the upstream end to 0 13 m at the downstream end fig 4c while the pb ranges from 26 to 2 fig 4f thus as the lead time increases the accuracy of the flood inundation forecasts declines the decline in accuracy with increasing lead time is attributed to the uncertainty in the weather forecasts precipitation forecasts from the gefsrv2 have previously been found to exhibit a strong negative conditional bias over the study area i e they tend to strongly underforecast large precipitation events sharma et al 2016 siddique et al 2015 yang et al 2017 note that the value of pb is always negative in fig 4d f thereby suggesting underforecasting in all cases the underforecasting bias in the gefsrv2 precipitation tends to grow as the lead time increases siddique et al 2015 here we show that such conditional bias propagates into the flood inundation forecasts to substantially influence their quality nonetheless in spite of the underforecasting bias in the flood inundation forecasts they remain skillful across lead times the skill measured by the ws stays close to 0 87 at the day 1 lead time fig 4g and varies between 0 87 and 0 4 at the day 7 lead time fig 4i it thus appears that there are benefits to generating flood inundation forecasts at the medium range lead times 3 days flood inundation forecasts have normally been investigated at short range lead times 3 days our results provide fresh insight into the medium range timescales however the results presented here are for deterministic hydrometeorological forecasts but it is well recognized that ensemble probabilistic hydrometeorological forecasts can be more skillful than deterministic ones cloke and pappenberger 2009 schaake et al 2007 more relevantly siddique and mejia 2017 showed that this is the case for flow forecasts in the delaware river basin generated by using the gefsrv2 as forcing hence we examine next the ability of ensemble hydrometeorological forecasts to improve the skill of flood inundation forecasts at short to medium range lead times 5 3 verification of the raw flood inundation ensemble forecasts to generate the raw flood inundation ensemble forecasts we force the hydraulic model with hourly flow ensemble 11 members forecasts at the upstream boundary of the delaware river and all the 10 tributaries the flow ensemble forecasts are obtained by forcing the hl rdhm with the 11 member gefsrv2 precipitation and near surface temperature outputs as was the case in the deterministic forecasting scenario we use wse observations at the downstream boundary at marcus hook to verify the raw flood inundation ensemble forecasts we use two deterministic metrics rmse and pb and one probabilistic metric crpss the crpss is a measure of forecast skill where a value of 1 means perfect forecast skill and a value of 0 means no skill i e same skill as the reference system the deterministic metrics are calculated using the mean of the 11 member wse ensemble forecasts at each cross section in the study reach while the crpss is determined using all the ensembles to emphasize high flow events the verification is conditioned as done in the deterministic forecasting scenario to wses values with exceedance probability of less than 0 05 in the observed wse probability distribution in addition we determine two different versions of the crpss one where the reference system is sampled climatology based on the baseline wse simulations and the other where the deterministic flood inundation forecasts are used as the reference system the latter is denoted as crpss2 and measures the skill gains from using ensembles relative to the single member deterministic case we find that the overall trends of the verification metrics for the raw flood inundation ensemble forecasts are qualitatively similar to those of the deterministic forecasts that is the rmse fig 5 a c and pb fig 5d f are reduced in the downstream direction at every lead time considered while the skill fig 5g i tends to increase furthermore the value of the verification metrics deteriorates as the lead time increases for instance the pb varies between 2 and 5 at the day 1 lead time fig 5d and over the much wider range of 2 and 28 at the day 7 lead time fig 5f as in the deterministic forecasts it is evident in fig 5 that the hydrometeorological uncertainty propagates further downstream in the case of the longer lead times contrasting the raw flood inundation ensemble forecasts against the deterministic ones the raw ensemble forecasts are slightly more accurate than the deterministic ones the rmse range is 0 11 0 99 m fig 5a c and 0 13 1 05 m fig 4a c for the raw ensemble and deterministic forecasts respectively however the raw ensembles also reflect the strong underforecasting bias of the gefsrv2 outputs as demonstrated by the negative values of the pb fig 5d f in terms of the forecast skill the values of the crpss2 show that the raw ensemble forecasts are able to improve the skill over the deterministic ones fig 5j l with relative skill gains that vary from 0 03 to 0 25 the relative gains increase with the lead time e g at the day 1 and 3 the crpss2 varies approximately over the range 0 03 0 05 and 0 12 0 15 respectively the latter indicates that as the forecast error increases the raw ensembles are able to provide greater improvements in skill this is the case because the quality of the reference system i e the deterministic forecasts used to compute the crpss2 declines faster than the raw ensembles as the lead time increases interestingly this skill trend reverses when one looks at the skill across the study reach at a particular lead time e g at day 7 the skill increases from 0 17 at the upstream boundary to 0 25 at the downstream end in fig 5l thus the skill benefits from using the raw ensembles may be maximized by reducing the uncertainty associated with the coupled hydrometeorological hydraulic system predictions this implies that improving both the process representation and parameter estimation calibration of the underlying hydrometeorological hydraulic system may be important for attaining the benefits associated with using the raw ensembles thus far our results fig 5 for the tidal riverine transitional zone of the delaware river indicate that weather ensembles can be used to improve flood inundation forecasting the results support the ability to potentially provide better flood related visual information to end users through the use of ensemble forecasts however further improvements to the raw flood inundation ensemble forecasts may be possible by correcting their systematic biases this is commonly done in hydrometeorological forecasting using statistical postprocessing techniques siddique and mejia 2017 hence we explore next the potential for statistical postprocessing to improve the flood inundation ensemble forecasts 5 4 verification of the statistically postprocessed flood inundation ensemble forecasts we employ qr to statistically postprocess the raw flood inundation ensemble forecasts over the period 2008 2013 using a leave one out approach consisting of 5 training years and 1 postprocessing year the qr postprocessor is applied to the raw wse ensemble forecasts at each lead time 1 7 days and cross section in the study reach to verify the postprocessed flood inundation forecasts we use the rmse pb and crpss the rmse and pb are computed using the ensemble mean while the crpss uses the ensembles as in the previous results we focus the analysis on high flow events by selecting flows with exceedance probability of less than 0 05 fig 6 summarizes the verification results for the postprocessed flood inundation ensemble forecasts overall the postprocessed flood inundation ensemble forecasts tend to perform better than raw ones across lead times fig 6 the improvement diminishes as one moves in the downstream direction i e as the effects of hydrometeorological uncertainty diminish the postprocessor is less able to produce gains following from this the gains appear somewhat greater for the longer lead times given that they tend to exhibit greater errors notice that the inflection point at the 3 3 km river station separating the hydrometeorological and tidal dominated zones is also visible in fig 6 as was the case in fig 3 for the deterministic forecasts specifically below the inflection point river stations 3 3 km the postprocessed flood inundation ensemble forecasts exhibit a more significant improvement compared to the raw ones in terms of the rmse fig 6a pb fig 6b and crpss fig 6c for example the rmse of the postprocessed flood inundation ensemble forecasts improves at the most upstream location trenton by 0 05 0 07 and 0 10 m at lead times of 1 3 and 7 days fig 6a respectively at the 4 km river station the improvements in rmse are relatively small 0 01 0 02 and 0 04 m at lead times of 1 3 and 7 days fig 6a respectively in terms of the pb the improvements i e the percent reduction in the pb values at trenton are 8 17 and 11 47 at the day 3 and 7 lead time fig 6b respectively while there is no improvement at the day 1 lead time the skill improves by 8 4 8 47 and 22 96 at lead times of 1 3 and 7 days fig 6c respectively as was the case with the raw ensemble forecasts the improvements in skill from postprocessing are greater at the longer lead times thereby suggesting that postprocessing of wses may be particularly beneficial at the medium range timescales in summary we find that postprocessing is able to improve upon the raw flood inundation ensemble forecasts we illustrate next the effects of ensembles and postprocessing on flood inundation mapping using a selected storm event 5 5 comparison of flood inundation forecast maps for a selected flood event to illustrate the mapping of the flood inundation forecasts we select the winter storm event that occurred on march 12 15 2010 this winter event had a significant impact on the middle atlantic region mar and particularly on the delaware river basin causing heavy rainfall in the states of new jersey and pennsylvania for example the accumulated rainfall over a 7 day period ending on march 16 2010 was 6 to 8 in in several counties in new jersey usgs 2010 furthermore 2 weeks prior to the selected event the mar was impacted by a snow storm that resulted in the accumulation of relatively large amounts of snow in the upper parts of the delaware river basin it is estimated that the accumulated snow contributed 6 to 10 in of snow water equivalent at the time of the march 12 15 storm these antecedent conditions contributed significantly during the selected event to increase runoff generation in the delaware river basin and ultimately flows along the study reach the measured peak flow at trenton was 2146 42 m3 s with an annual recurrence interval of 2 years usgs 2010 at the tributaries the annual recurrence interval was higher between 3 and 7 years usgs 2010 for the march 12 15 2010 storm we compare the quality of the deterministic fig 7 a c raw ensemble fig 7d f and postprocessed ensemble fig 7g i flood inundation forecast maps at lead times of 1 3 and 7 days along the tidal riverine transitional zone of the delaware river for the mapping of the latter two we use the mean of the ensemble flood inundation forecasts as in the previous verification results the forecast maps are compared against the baseline flood inundation simulation maps note that to generate the flood inundation maps we interpolate the maximum water depth at each cross section onto the 1 1 m2 terrain grid using the ras mapper interpolation technique thus at each lead time we actually compare the maximum water depth forecast map for the selected event against the baseline simulation one to assess the quality of the inundation maps on a cell by cell basis we classify each cell into one of the following 3 categories agree underforecast or overforecast we classify cells as agree when both the forecast and baseline map show inundation cells are classified as underforecasting when the baseline map shows inundation but not the forecast map and vice versa for overforecasting by contrasting the deterministic fig 7a c and raw ensemble fig 7d f flood inundation forecast maps it is apparent that the quality of the maps declines as the lead time increases for instance the percent of overforecast grid cells increases from 6 24 day 1 lead time fig 7a to 45 78 day 7 lead time fig 7c for the deterministic maps notice in fig 7 that the tendency is clearly in this case for the forecast maps to overestimate flood inundation even though the overall trend of the forecasts for the 6 year verification period as was shown before is to underestimate the baseline simulations at the day 1 lead time the maps in fig 7 are similar the deterministic fig 7a raw ensemble fig 7d and postprocessed ensemble fig 7g forecast inundation maps exhibit comparable quality with little overforecasting the overforecasting is 6 24 1 34 and 1 82 respectively as the lead time increases however differences emerge among the forecast maps contrasting the day 7 forecasts it is apparent that the postprocessed maps fig 7i have less overforecast grid cells than the deterministic fig 7c and raw ensemble fig 7f maps indeed the overforecasting is 45 78 16 29 and 4 29 for the deterministic raw ensemble and postprocessed ensemble flood inundation forecast maps respectively in summary the results in fig 7 show that the use of weather ensembles and statistical wse postprocessing can result in improved flood inundation forecast maps the results suggest that it may be possible to enhance the spatial representation and potentially the effectiveness of flood forecast communication to end users through the use of ensembles and postprocessing 6 conclusions based on the verification results for the flood inundation forecasts generated with the coupled hydrometeorological hydraulic system the following key conclusions are emphasized the verification metrics for the deterministic and raw flood inundation ensemble forecasts show qualitatively similar behavior specifically the pb shows that the deterministic and raw flood inundation ensemble forecasts tend to largely underestimate the prediction of wses across short and medium range timescales the underestimation increases with increasing lead time and it is higher for the upstream cross sections of the delaware river where wses are dominated by inland flows and hydrometeorological forecast uncertainty hydrometeorological forecast uncertainty alone can introduce significant errors to the flood inundation forecasts during high flow conditions along the tidal riverine transitional zone of the delaware river particularly at the later lead times for example flood inundation forecasts in the transitional zone show rmses that can be as high as 1 m at the day 7 lead time the raw flood inundation ensemble forecasts show skill improvements relative to the deterministic flood inundation forecasts the improvements are relatively small at the initial lead times days 1 2 but increase at the later ones thus the use of weather ensembles is in this case able to improve the quality of flood inundation forecasts statistical postprocessing of wses improves the skill of the flood inundation ensemble forecasts over the raw ensemble ones the improvement is evident across all lead times 1 7 days but it is higher at the medium range timescales 3 days through a flood mapping demonstration for a selected flood event it is shown that both weather ensembles and statistical postprocessing can improve the spatial representation of floods the demonstration shows that the percentage of overforecasted inundated areas may be reduced by jointly using weather ensembles and statistical postprocessing in this study the proposed hydrometeorological hydraulic forecasting system is implemented along the delaware river however under the condition that terrain bathymetry hydrometeorological and tidal data of similar quality are available in other river systems which is likely the case for major tidal rivers in the u s the proposed system could be applied to those rivers the latter is facilitated by the proposed system by relying on global weather forecasts gefsrv2 and modeling tools that are available to forecasters and the public there are shortcomings to the present study that could be improved in the future the verification of flood inundation forecasts could consider coastal forecasts and the hydraulic model the effects of wind forcing and independent observations of flood inundation could be used when verifying the forecasts to overcome these shortcomings however substantial and coordinated efforts may be necessary that are beyond the scope of the present study for instance the availability of medium range coastal reforecasts will depend on the capabilities and interest of the ocean coastal modeling communities in generating those forecasts similarly the availability of multiyear high quality independent flood inundation observations may require coordinated efforts among different agencies responsible for generating flood related data and information acknowledgements the first author acknowledges the funding support provided by the ecuadorian government through the secretariat for higher education science technology and innovation ecuador the authors are thankful to susan patterson of the philadelphia water department pwd and josef kardos formerly of pwd for making the bathymetry data available and offering useful suggestions about the research appendix mean absolute error the mean absolute error mae quantifies the absolute average error between the simulated values and their corresponding observations the mae is expressed as follows a 1 mae 1 n i 1 n s i o i where s i and o i denote the simulated and observed wse respectively at time i root mean square error the root mean square error rmse quantifies the absolute average error between the predicted values and their corresponding reference values the rmse is given by a 2 rmse i 1 n p i r i 2 n where p i and r i denote the predicted and reference wse respectively at time i when verifying the model simulations these variables take the values of the simulated and observed wse flow respectively when verifying the model forecasts p i and r i are set equal to the forecasted and observed wse flow respectively percent bias the percent bias pb measures the average tendency of the predicted values to be larger or smaller than the reference values the pb is given by a 3 pb i 1 n p i r i i 1 n r i 100 where p i and r i denote the predicted simulated or forecasted and reference observed wse flow respectively at time i nash sutcliffe efficiency the nash sutcliffe efficiency nse nash and sutcliffe 1970 is a commonly used metric to assess the accuracy of hydrologic models to predict flows the nse can vary between and 1 with values closer to 1 indicating a good agreement between the simulated and observed flows while a negative value indicates that the mean of the observations is a better predictor than the simulated flows the nse is defined as a 4 nse 1 i 1 n s i o i 2 i 1 n s i o i 2 where s i o i and o i are the simulated observed and mean observed flow respectively at time i willmott skill the willmott skill ws willmott 1981 is a widely used metric to evaluate the performance of hydrodynamic models for tidal rivers and estuaries the ws can vary between 0 and 1 where a ws value of 1 represents a perfect agreement between predictions and reference values and a value of 0 represents no skill the ws is given by a 5 ws 1 i 1 n p i r i 2 i 1 n p i p i r i r i 2 where p i r i p i and r i denote the predicted simulated or forecasted reference observed mean predicted mean simulated or forecasted and mean reference mean observed wse respectively at time i mean continuous ranked probability skill score the continuous ranked probability score crps is a probabilistic metric used to assess the skill of ensemble forecasts relative to a reference system let the probability distribution function of the wse ensemble forecasts be p h and the actual wse value be h a then the crps is defined as a 6 crps p f h p a h 2 d h where p f h and p a h are cumulative distribution functions given by a 7 p f h h p h d h and a 8 p a h h h h r respectively h is the heaviside function which is 1 if the argument is positive and zero otherwise to measure the skill of the main forecast system relative to a reference one we use the mean continuous ranked probability skill score crpss defined as a 9 crpss 1 crps main crps reference where crps main and crps reference are the average crps values for the main and reference forecast system respectively the crpss ranges from to 1 with negative scores indicating that the system to be evaluated has worse crps than the reference forecast system while positive scores indicate a higher skill for the main forecast system relative to the reference one with 1 indicating perfect skill 
