index,text
26370,the ability to quickly and accurately forecast flooding is increasingly important as extreme weather events become more common this work focuses on designing a cloud based real time modeling system for supporting decision makers in assessing flood risk the system built using amazon web services aws automates access and pre processing of forecast data execution of a computationally expensive and high resolution 2d hydrodynamic model two dimensional unsteady flow tuflow and map based visualization of model outputs a graphical processing unit gpu version of tuflow was used resulting in an 80x execution time speed up compared to the central processing unit cpu version the system is designed to run automatically to produce near real time results and consume minimal computational resources until triggered by an extreme weather event the system is demonstrated for a case study in the coastal plain of virginia to forecast flooding vulnerability of transportation infrastructure during extreme weather events keywords flood warning applications cloud computing 2d hydrologic model gpus amazon web services reproducibility 1 introduction floods were the number one natural disaster in the us in terms of lives lost and property damage incurred during the 20th century perry 2000 with statistics showing that total flood insurance claims averaged more than 1 9 billion per year from 2006 to 2015 nfip statistics 2016 rainfall events are predicted to become more frequent and intense due to climate change which is expected to cause increased flooding melillo et al 2014 as society faces flooding events with increasing frequency and intensity flood modeling will become an even more important tool for decision makers such models can be used to warn municipalities and communities of forecasted flooding impacts and to test alternative flood mitigation strategies for addressing flood problems the national research council nrc has recommended increased use of two dimensional 2d hydrodynamic models for flood risk management purposes nrc 2009 there are several advantages to using 2d models rather than one dimensional 1d models such as better resolution of velocity localized depth and surface water elevation and determination of floodplain extent directly 2d hydrodynamic models are especially important for cases with complex flows such as in low relief terrains with flat or mild slopes for these low relief terrains 1d models are often not sufficient due to the limitations of assumed uniform water velocity and constant water surface elevation modeled on each cross section garcia et al 2015 executing 2d hydrodynamic models at the regional scale 10 103 100 103 km2 requires parallel computation in order to run in a timeframe reasonable for flood warning applications graphical processing units gpus have recently been shown to effectively execute parallelized 2d hydrodynamic models with observed speed ups of 20x to 100x huxley and syme 2016 garcia et al 2015 vacondio et al 2014 kalyanapu et al 2011 vacondio et al 2014 expect that gpus will continue to be attractive for 2d numerical models compared to clusters of central processing units cpus for several reasons i fast developing gpu hardware ii quickly decreasing costs and iii lower maintenance compared to large cpu clusters with the speed ups provided by gpus regional flood warning systems can now be implemented with 2d hydrodynamic models and the spatial resolution needed to provide targeted and detailed information to decision makers there are currently several related efforts aimed at improving flood warnings the national weather service nws and the united states geological survey usgs have a joint project to generate flood inundation maps at locations where a nws forecast point and a usgs stream gauge exist fowler 2016 at these locations a flood inundation map is created for multiple possible water surface elevations and by using a rating curve and forecasted discharge the data is converted into the corresponding water surface elevation then the corresponding flood inundation map is selected from a precomputed library of flood inundation maps the flood locations and simulated hydrographs flash is a system designed by researchers and developers to improve the ability of the nws to forecast flooding at the weather forecast offices wfos gourley et al 2017 the flash system uses the multi radar multi sensor mrms rainfall data with a spatial resolution of 1 km and temporal resolution of up to 2 min along with a highly efficient distributed hydrologic modeling framework to generate flood forecasts for over 10 8 million grid points across the conterminous united states conus the national flood interoperability experiment nfie is a multiagency effort in collaboration with the academic community to improve river and flood forecasts maidment 2017 a key component of nfie is a model called routing application for parallel computing of discharge rapid http rapid hub org which was developed to operate on the 2 67 million nhdplus catchments and use parallel computing to solve the 1d muskingum flow equations on this large river network maidment 2017 david et al 2013 2011 nfie showed it was possible to increase the spatial density of flooding forecast locations by more than 700x compared to the present nws river forecast system maidment 2017 the rapid model is expected to be replaced by the simulation program for river networks sprint liu and hodges 2014 which has the capability of solving the full nonlinear saint venant equations for 1d unsteady flow and depth for channel networks and promises speed up of the computation time however in some instances a 2d flood model will be necessary to accurately model water transport over large flat areas delft fews is a hydrological forecasting and warning framework that provides a platform through which operational forecasting systems can be constructed allowing for flexibility in the integration of models and data werner et al 2013 delft fews does not contain any inherent hydrologic modeling capabilities within its code base instead it relies on the integration of external hydrologic model components the objective of this research is to design and prototype a cloud based system to support decision makers as they assess flood risk to transportation infrastructure during extreme weather events the system automates access and pre processing of forecast data execution of a high resolution 2d hydrodynamic model and map based visualization of model outputs this work advances on prior approaches described earlier by presenting a cloud based framework for modeling regions with complex flows using a 2d hydrodynamic model rather than relying on precomputed flood maps flood depths and extents this approach allows for water flows to be modeled in real time based on current and forecasted conditions this approach could be adopted in existing decision support systems to leverage cloud and gpu resources within this general framework the study advances on previous work funded by the virginia department of transportation vdot for the hampton roads district of virginia which produced the regional river severe storm model r2s2 hassan water resources plc 2012 the purpose of r2s2 is to help residency administrators to efficiently allocate scarce resources to road closures and to assist first responders with entering and exiting flood prone areas this research advances r2s2 by automating what was previously a manual process of converting forecast rainfall data into model inputs running the model and visualizing the results furthermore this research addresses computational challenges with using r2s2 for real time flood warning and emergency management applications this research also moves r2s2 to the cloud and is one of the first cloud based flood warning applications with i an automated workflow for obtaining the real time forecast rainfall data ii execution of a model to identify flooded bridge and culvert locations in a time duration sufficient for warning and emergency management purposes and iii generation of an online map with locations of the flooded roadways and bridges and the ability to send automated warning messages via email this system can provide vdot with information needed when determining road closures disseminating warning messages for area residents and making other emergency management decisions that affect human safety and property damage although the current case study application of the system is focused on vdot as the primary user the approach could be used as a more general flooding decision support system for other stakeholders cloud computing is gaining attention in environmental applications to satisfy the peak performance needs of applications that use large amounts of processing power granell et al 2016 sun 2013 used google drive a cloud computing service to host an environmental decision support systems edss module that is migrated from the traditional client server based architecture google drive has the capability of providing a number of basic visual analytics features and the collaboration between the decision makers can be increased while decreasing the cost of small scale edss ercan et al 2014 used the windows azure cloud environment to run a created calibration tool built with the modified calibration method a parallel version of the dynamically dimensioned search dds for calibrating the soil and water assessment tool swat using this tool results showed a significant speed up of the model calibration for six different model scenarios wan et al 2014 introduced a public cloud based flood cyber infrastructure cyberflood cyberflood collects organizes manages and visualizes several global flood databases for decision makers and public users in real time this database is expanded by applying a methodology for the data collection in which the public reports new flood events using smartphones or web browsers hu et al 2015 implemented a web based application in the hadoop based cloud computing environment to make enhanced coupled human and natural models publicly available this allows users to access and execute the model without an increase in response time kurtz et al 2017 presented a stochastic cloud based fully operational architecture for a real time prediction and management system related to groundwater management this proposed system allows for data assimilation and is coupled with a physically based hydrologic model hydrogeosphere in a cloud environment to use the generated prediction for groundwater management the work presented here advances on prior studies by demonstrating the ability of using resources in a public cloud including instances with powerful gpus like those provided by aws to build an end to end automated cloud based system for regional scale flood forecasting this system is able to run a computationally expensive 2d hydrodynamic model and is activated automatically during extreme weather events by software that is continuously monitoring forecasted rainfall conditions it is designed to run in a time frame relevant to real time emergency management applications and automatically deliver model outputs to decision makers through online maps and email notifications the remainder of the paper is organized as follows first a study area section is presented to introduce the region where the model is applied second the data and methods section is presented to outline the available data sources the pre processing steps used to translate this data for use in the model steps taken to speed up the model and the post processing steps used to automate the model output dissemination next the results and discussion section presents a prototype of the software and the results of applying the system for an extreme weather event finally the conclusions section provides a summary of the key research outcomes and steps that could be taken to further advance this work 2 study area the study area is in a portion of the chowan river basin that is within vdot s hampton roads district in virginia usa and is approximately 5780 km2 2230 mi2 fig 1 the study area includes the meherrin nottoway and the blackwater rivers the longest flowpath along nhd flowline features is approximately 175 km 109 mi with a slope that varies from nearly 0 to 21 the study area includes 493 georeferenced vdot bridges and culverts due to a high portion of the study area consisting of low relief terrain in the coastal plain especially in the eastern part of the study area fig 2 the system utilizes a 2d hydrodynamic model called two dimensional unsteady flow tuflow https www tuflow com syme 2001 the upstream portion of the project domain consists of relatively higher relief terrain and therefore the hydrologic engineering center hydrologic modeling system hec hms a lumped hydrology model that is less computationally intensive was deemed appropriate to model flows in these areas by using hec hms to generate inflow boundary conditions from the high relief portion to the low relief portion of the study area the overall system runtime is kept smaller including these high relief upstream watersheds the project domain is approximately 11 000 km2 4240 mi2 3 data and methods 3 1 r2s2 system the r2s2 system was first developed by hassan water resources plc to integrate multiple datasets with sophisticated hydrodynamic models to provide flood risk prediction during severe storm events to the hampton roads district of vdot hassan water resources plc 2012 the r2s2 system consists of software that processes the many input files required for the tuflow model runs hec hms to establish boundary conditions for tuflow and processes output files from tuflow to determine inundated bridges and culverts fig 3 constant input data for the r2s2 include a dem with 10 m resolution soil data ssurgo 2012 and land use data with 30 m resolution the observational data must be accessed and processed in real time from federal data providers r2s2 uses real time products for rainfall including national oceanic and atmospheric administration noaa gauges see fig 8 and next generation weather radar nexrad radar data where available for specific storms the rainfall data is used as inputs for both r2s2 hydrologic models first the hec hms model uses the rainfall data to generate the hydrograph for each of the 11 subwatersheds that border the study area then the 11 outlet hydrographs generated are applied to the tuflow model as boundary conditions along with the rainfall data to generate water levels throughout the study area the model is calibrated and evaluated using historic stream gauge data eventually real time stream data will be used to set initial conditions 3 2 rainfall forecast data preparation a preliminary step for building a flood warning system is to identify and automate the pre processing of the forecast rainfall product in this study the procedure to collect and process the forecast rainfall data for model input was automated to reduce human translation errors and decrease the time between when new rainfall forecasts are available and new water level forecasts can be generated both the tuflow and hec hms models in r2s2 require input rainfall data but in different formats tuflow has three approaches for applying the rainfall directly to the computational cells i polygons covering multiple cells assigned as rainfall time series ii gridded rainfall created as ascii files for each time step or as one netcdf file and iii a rainfall control file that allows a user to specify point time series over the model domain and specify how the rainfall is interpolated to the model cells hec hms has two approaches for applying the rainfall data i a rainfall time series for each basin stored in a data storage system dss file that is prepared by hec dssvue a program for viewing editing and manipulating dss files ceiwr hec 2009 and ii gridded rainfall that is prepared by hec gridutil a utility program for managing gridded data with hec dss steissberg and mcpherson 2011 the identification of appropriate forecast datasets focused on noaa rainfall products that are in a grid format and can be quickly accessed for real time flood warning applications several potential forecast datasets were identified for the study region including products from the rapid refresh rap the high resolution rapid refresh hrrr the north american mesoscale forecast system nam and the national digital forecast database ndfd the rap hrrr and nam products are all provided by the national center for environmental prediction ncep and the ndfd is provided by the national weather service nws these forecast products were compared in terms of their spatial resolution temporal resolution and frequency of model initiation i e model cycle results of this comparison and the code written to automate the retrieval and reformatting of the rainfall data to meet the requirements of the tuflow and hec hms models are presented in section 4 1 this code is tested to retrieve the required rainfall data with the desired formats for the two models and with the correct spatial and temporal resolution 3 3 speeding up r2s2 execution tuflow is the computational bottleneck within the overall r2s2 workflow using a cpu for computation takes more than three days to run a 15 day simulation period the duration over which hurricane sandy caused high flows in the study region the use of multiple cpus and gpus has been investigated as a means of speeding up 2d hydrodynamic models kalyanapu et al 2011 brodtkorb et al 2012 rostrup and sterck 2010 castro et al 2011 lacasta et al 2013 sanders et al 2010 garcia et al 2015 as stated in the introduction gpu use offers the performance of smaller clusters at a much lower cost jacobsen et al 2010 therefore gpus were investigated for speeding up the tuflow model rather than using cpu clusters tuflow comes with a gpu module capable of operating on multiple gpus in parallel we explored the use of both local and amazon web services aws resources for gpu computations the tuflow gpu module uses an explicit scheme only while the tuflow cpu solver tuflow classic uses an implicit scheme explicit schemes could be less numerically stable compared to implicit schemes if using the same time step and grid cell size also explicit schemes require a small time step and high resolution grid cell size to compete with well developed implicit schemes boris 1989 anderson and wendt 1995 tóth et al 1998 pau and sanders 2006 zhao 2008 the differences between these two schemes could be large and need to be checked for consistency two local gpu resources with different capabilities were explored table 1 m1 is a machine with a modest gpu and other resources typical of most desktop computers m2 is a high end workstation with 64 gb of ram and two nvidia geforce titan graphics cards there are several types of aws elastic compute cloud ec2 instances designed for gpu based computations there are two sizes of g2 instances which have lower end gpus and three sizes of p2 instances which have higher end gpus table 2 the properties and hourly fees for these instances vary as shown in table 2 several tests were performed to measure the tuflow model execution times using the aws ec2 g2 8xlarge and p2 8xlarge instances the tuflow model with a 50 m grid cell size was used for these tests the g2 8xlarge instance which has 4 gpus was used to execute the model with 1 2 3 and 4 gpus likewise the p2 8xlarge instance which has 8 gpus was used to execute the model with 1 through 8 gpus each of these model runs was performed twice to ensure that model runtimes were consistent following this comparisons of results generated from cpu and gpu solvers were performed lastly using the gpu solver preliminary calibration steps were performed by varying grid cell size and input parameters 3 4 post processing and automating model output dissemination the last main step for the flood warning system is to post process and automatically disseminate the system output one of the most important outputs from the tuflow model for this study is the maximum water level at each computational cell within the study area throughout the simulation duration using these maximum water levels and the vdot bridge locations and deck elevations a post processing workflow was created and tested to automate email notifications providing bridges expected to be overtopped based on model projections in addition to sending email alerts map based visualizations can be created to display flooded bridge locations with flooded depths over the model domain we explored and tested three options for creating such maps table 3 a first and more basic option involved a user manually uploading a keyhole markup language zipped file kmz containing post processed output from the model to the google maps website for visualization providing a quick and simple method to visualize the flooded bridge locations a second more complex option was to use geosheets https www geosheets com an add on to the google sheets app that reads and displays post processed tabular data stored in a google drive account as a google sheet unlike the manual upload of a kmz file to google maps this second method can be automated to dynamically update the flooded bridge locations map this option allows for some customization of map display without needing to configure and deploy a web server the third and most complex option was developing a custom web interface using the google maps api to visualize the output kmz files along with workflow run information see section 4 4 this alternative required the deployment of a web server and was therefore more complex however it provided the highest potential for customization and supported the dissemination of other workflow run information in addition to the flooded bridge locations the post processing workflow and three visualization options were tested to ensure the correct dissemination of the model output and the proper visualization for the decision maker 3 5 design of an automated flood warning system through aws after automating the retrieval of the forecast rainfall data speeding up the 2d model and providing methods for post processing and automating the model output dissemination the final step was to create a seamless workflow using cloud services to link these individual components together without requiring intermediate user action the goal of this automated workflow was to identify the flooded bridges and or culverts in a time duration sufficient for warning and emergency management purposes based on the highest resolution reliable rainfall forecast data and by using publicly available cloud computing resources given that a single cloud instance capable of all of these tasks would be too expensive to continuously run the design of this workflow had to meet several requirements i a smaller low cost instance to monitor the rainfall data for upcoming extreme events and visualize model outputs ii a larger instance with nvidia gpu capabilities to accommodate and execute the hydrologic models and other processing scripts and iii a storage resource to archive model inputs such as the processed rainfall data and model outputs for later analysis the smaller instance would trigger the larger instance when a flood event is forecasted this smaller instance would also assume the role of maintaining the website to display and disseminate the model output so that it can be continuously available to automate these steps of the workflow the gpu instance would need to execute a batch file that i runs the pre processing scripts to prepare the rainfall data ii runs the hydrologic models iii runs the post processing script for preparing the model output for dissemination iv sends outputs to other cloud resources for archiving and visualization and v removes the model output files from the gpu instance each individual step in the workflow i e pre processing the rainfall data speed up the model and post processing and disseminating the model output was tested separately as mentioned in the previous subsections then the entire workflow was tested together locally and remotely using aws resources to test locally the batch file that initiates the workflow was run along with the workflow scripts and hydrologic models that were placed on a local machine with nvidia gpu capabilities then the batch file was tested to run the whole workflow seamlessly then a low cost smaller instance was created to monitor the forecast rainfall data the smaller machine was then set up to monitor forecasted rainfall and to trigger the local machine to run the batch file and receive post processed output for visualization purposes after performing several tests of the designed system locally the batch file and the hydrologic models were placed in a larger aws instance with nvidia gpu capabilities then the smaller machine was linked to this larger machine with an adjustment to the rainfall threshold to start the larger machine the aws based system performance was then monitored and analyzed to ensure it was working as expected 4 results and discussion 4 1 rainfall forecast data preparation comparison of the spatial resolution temporal resolution and model cycle of each dataset table 4 shows that hrrr was the best forecast rainfall product for our purposes hrrr is a weather prediction system composed of a numerical forecast model and an analysis assimilation system to initialize the model hrrr is a higher resolution model nested inside the hourly updated rap although rap can provide upper level analyses and short range forecasts hrrr is best used to examine surface and near surface parameters such as surface precipitation the hrrr model is run every hour of the day and forecasts out to 18 h on a 1 h time step for each cycle it provides a surface total precipitation product in units of mm of precipitation depth at a horizontal resolution of 3 km noaa 2012 surface total precipitation can be accessed as gridded data with dimensions of longitude latitude and time longitude and latitude are provided in the world geodetic system wgs 1984 coordinate system and time is in units of decimal days since 1 1 1 00 00 0 0 noaa 2017a hrrr data are distributed as part of the noaa operational model archive and distribution system nomads project a network of data servers that use the open source project for a network data access protocol opendap noaa 2017a although the hrrr data was selected as the primary input to the model the system could alternatively use the coarser quantitative precipitation forecast qpf from the ndfd dataset which forecasts rainfall for the upcoming 72 h to monitor for large rainfall events beyond the 18 h horizon captured by hrrr and thus allowing for a longer lead time for preparing for severe storms the use of higher resolution rainfall forecast data with a longer lead time will reduce the uncertainty of the model making it a more useful decision support tool and the system is developed in a flexible way that easily enables the application of this better forecast data that may be available in the future fig 4 shows the workflow for downloading and reformatting the forecast rainfall data pydap a pure python library client for opendap servers is used to retrieve the desired forecast data for the study area the automated workflow consists of three main parts i access the latest available forecast data from the hrrr database ii retrieve the forecast surface total precipitation with a horizontal resolution of 3 km in wgs 1984 coordinate system and iii reformat the forecast data for model input in the nad83 utm 18n projected coordinate system these rainfall data are reformatted as gridded rainfall data for tuflow using the geospatial data abstract gdal ogr python library and as subwatershed time series for hec hms using hec dssvue python and java libraries to include these direct rainfall data in tuflow a tuflow event file tef was created to define the storm event properties using the new tef file the user can run the model for a given storm event using either historic or forecast data 4 2 speeding up r2s2 execution the model speed up was evaluated using rainfall from hurricane sandy as input the rainfall lasted for four days and the total modeled time span was 15 days october 28 november 11 2012 table 5 summarizes the results of the three tuflow model scenarios using the m1 and m2 machines see table 1 using the cpu the model took 120 h to execute and using the modest gpu in the m1 machine the model took 11 5 h to execute 10x speed up compared to the cpu using the two more powerful gpus in the m2 machine the model took only 2 4 h to execute 50x speed up compared to the cpu and 5x speed up compared to the m1 machine using the single gpu the input time step did not have a significant effect on the execution time when using gpus which is due to the explicit scheme within the tuflow gpu module that takes the input time step value as an initial value and then optimizes the time step to meet the convergence condition i e courant number 1 bmt wbm 2016 a test was also conducted to determine how increasing the number of gpus influenced model execution time fig 5 as expected running the model by using different numbers of gpus produced the same output results i e no differences in the maximum water levels fig 5 a provides the results of this test using the gpu model and the aws g2 8xlarge instance with different numbers of gpus using the g2 8xlarge instance with one gpu the model takes about 4 6 h to run using the g2 8xlarge instance and increasing the number of gpus the minimum execution time is 3 h when all four gpus are used which costs about 9 per run because only four gpus were available on this instance we were not able to test whether additional gpus would continue to reduce the running time fig 5 b provides the results of this test using the gpu model and the aws p2 8xlarge instance with different numbers of gpus using the p2 8xlarge instance with one gpu the gpu model takes 2 75 h to run which is less than using the g2 8xlarge instance with 4 gpus this shows the benefit of the more modern gpus in the p2 versus g2 ec2 instances using the p2 8xlarge instance and increasing the number of gpus the minimum execution time was found to be 1 5 h which is achieved when five gpus are used this run with the minimum execution time of 1 5 h costs about 13 per run which is about 1 5x more expensive than the g2 8xlarge instance run however it was 2x faster than the g2 8xlarge instance comparing this 1 5 h execution time to the cpu execution time of 120 h shows an 80x speed up for the model using six or more gpus on this instance increases the execution time compared to using five due to known tradeoffs caused by data transfers between parallel gpu units huxley and syme 2016 because the cpu and gpu tuflow solvers use different numerical schemes it is important to understand differences in their outputs fig 6 fig 6 provides the differences in maximum water level max wl generated from executing the model using the cpu and the gpu solvers the maximum difference in max wl across the study area was around 2 5 m 8 ft with 87 of the computational cells having differences in the max wl less than 0 5 m 1 6 ft fig 7 shows the max wls at each bridge location generated by executing the model using the cpu solver versus the gpu solver the mean absolute error mae of 0 48 m 1 6 ft and the root mean square error rmse of 0 78 m 2 6 ft demonstrate a fairly significant difference in this study we did a preliminary sensitivity analysis by changing the model grid cell size and manning coefficient values but future research should investigate this difference more fully the model results using both the cpu and gpu solvers were compared against stream stage observations for the hurricane sandy event fig 8 and table 6 show the usgs stations with data availability for the event the usgs provided unpublished stage data that is considered provisional and therefore may contain erroneous or missing values due to instrument malfunction this data was processed and cleaned to address this issue before being compared to the model output data fig 8 also shows the noaa stations with the available recorded rainfall data for the hurricane sandy storm event hyetographs for this storm event at these stations are shown in fig 9 the finite volume schemes used by the 2d models are heavily dependent on the grid cell shape and size leveque 2002 caviedes voullième et al 2012 the tuflow model gpu solver uses only a cartesian grid with the capability of changing the grid cell size the tuflow model was executed using the gpu solver with grid cell sizes of 50 m 40 m 30 m and 20 m the output data from each of these runs were compared to the observed data at the six usgs stations and model results from cpu solver execution with a cell size of 50 m the modeled peaks using the gpu solver with 50 m grid cell size were significantly higher than the observed data and the model peaks using the cpu solver at four usgs stations 02045500 02047000 02047500 and 02052000 however at one of the usgs stations 02050000 the modeled peak using the gpu solver with 50 m grid cell size was significantly lower than the observed data and the modeled peak using the cpu solver finally at another usgs station 02049500 the modeled peak using the gpu solver with 50 m grid cell size was almost the same as the model peak using the cpu solver however both peaks were significantly lower than the observed data the differences between the modeled and observed peak stages could be due to the lack of adequate bathymetry data in the major rivers and tributaries in all of the minor tributaries and some stretches of the main rivers bathymetry had to be assumed because of a lack of this data this also could be due to the coarse dem resolution 10 m as the tuflow model extracts and utilizes the ground elevation at the model grid cell center for the gpu solver and at the model grid cell center and mid sides for the cpu solver bmt wbm 2016 calibration was also a challenge due to the scarcity of operating river gauges and limited available data for event based calibration over such a large study area in some instances 2d models are not used due to the low resolution of the spatial data available and the difficulties faced when calibrating the model parameters caviedes voullième et al 2012 this large study area includes only six usgs gauges that recorded stream stage during hurricane sandy three of these stations are located on the same main stream at the eastern part of the study area one is in the middle of the study area and the other two are located in the western part of the study area when the cell size of the model using the gpu solver decreases a significant reduction in the peak stages was observed at four of the six usgs stations 02045500 02047000 02047500 and 02052000 at station 02050000 the modeled peak stage using the gpu solver increased with decreasing cell size while at station 02049500 the peak stage remained nearly constant with each cell size decreasing model grid cell size improved the matching of observed peaks at four of the six observation sites and therefore we decided to use a smaller cell size in the model application the drawback of a smaller cell size is an increase in model execution time fig 10 shows the model execution time using the gpu solver with different grid cell sizes 50 m 40 m 30 m and 20 m for the m2 machine see table 1 fig 10 also shows the mae resulting from comparisons of model output generated using the gpu solver at different cell sizes and the model output generated using the cpu solver with the 50 m cell size based on these results we chose the 30 m cell size since there is only a small difference between this scenario and the scenario resulting from the gpu solver with a 20 m grid cell size model and because there is a significant increase in the model runtime 2 8x from 10 2 h to 28 h in addition to decreasing the cell size to 30 m we also adjusted the manning coefficient n to test its sensitivity and ability to improve matching of observed peak stages obtained from the six usgs stations the model initially had manning coefficient values based on the study area land use to assess the sensitivity of the model to changes in the manning coefficient this value was changed to be 0 6n 0 8n 1 0n 1 4n and 1 8n as the manning coefficient value decreased the modeled peak stages became closer to the observed peaks at stations 02045500 02047000 20047500 and 02052000 after reducing the grid cell size from 50 m to 30 m and changing the manning s coefficient from 1 4n to 0 6n the model came the closest to matching observed peak river stage this represents a preliminary calibration of the model that should be more fully explored through additional research the analysis of model response to changing the grid cell size and manning s coefficient was done by applying rainfall time series for hurricane sandy from five rain gauges to polygons that each covered multiple model grid cells tuflfow also has the capability of using direct rainfall data that applies input rainfall values to every cell in the 2d hydrodynamic model when the rainfall is directly applied to the cells the model routes flow based on the cell topography on a cell by cell basis huxley and syme 2016 huxley and syme 2016 investigated using this new method by applying the direct gridded rainfall data and found that gpu direct rainfall hydraulic modeling can be used as an alternative to runoff routing hydrology modeling to check the model behavior using the direct gridded rainfall data method with the chosen grid cell size and manning s coefficient values rainfall data from hurricane sandy was obtained from the tropical rainfall measuring mission trmm this data has resolution of 0 25 0 25 resulting in 16 cells covering the entire study area we hoped to use rainfall data from nexrad provided by noaa but there was no data available for the dates of hurricane sandy for our study area fig 11 and table 7 show the results of using the gridded rainfall data provided by trmm when executing the model using the gpu solver with a grid cell size of 30 m and manning s coefficient value of 0 6n using the gridded rainfall data with this coarse resolution produces results very similar to those found when using the rainfall gauge data and the polygon method the model results almost match the observation peaks at the 02045500 02047000 02047500 and 02052000 usgs stations the other two usgs stations 02049500 and 02050000 where the modeled peaks are further from the observed peaks are located on the same stream at the eastern part of the study area along with station 02047500 this area has the mildest slopes in the study area almost flat see fig 2 the station furthest upstream is 02047500 at this station the model predicts a slightly higher peak than the observed data and the modeled peak using the cpu model the second station 02049500 has a much lower peak than the observed data however the modeled peak using the cpu solver is even lower than the modeled peak using the gpu solver the peak at station 02050000 is much higher than the observed peak and the modeled peak using the cpu solver the variation between the observed and modeled peaks at these three stations could be due to the coarse dem resolution 10 m 10 m used in the model the slightly higher peak at 02047500 may be due to slopes derived from the dem being milder than the real slopes the much lower peak and lower volume at 02049500 could be due to unrealistically steep slopes derived from the dem compared to real slopes like with 02047500 the much higher peaks at 02050000 may be due to the dem derived slopes which are milder than the real slopes this would explain why the differences in the peaks at stations 02049500 and 02050000 are nearly the same but the one is below and the other is above the observed peak if the slopes of the contributing areas to station 02049500 were milder the peak there would be higher and the peak at the downstream station 02050000 would be lower making both closer to the observed data this might improve if a higher dem resolution is used within the model future work will explore this and the use of nexrad which was unavailable for the study time period to better understand the benefit of this rainfall data for predicting the stage depth peaks 4 3 post processing and automating model output dissemination fig 12 shows the resulting workflow for model output post processing and dissemination of model results this workflow uses different python libraries such as gdal ogr and simple kml library simplekml to generate the visualization of the flooded bridge locations and an email library to automatically email warnings to decision makers the workflow and its products could be used with arcmap google maps google earth geosheets or a custom website such as the one we have configured and hosted on the aws ec2 t2 micro instance fig 12 there are three products for visualization that can be generated from this workflow i an esri shapefile that includes just the flooded bridges ii a kmz file that includes flood information for all bridges that can be visualized through google maps or google earth and iii a dynamic and real time visualization on geosheets created by automatically uploading the bridges with their flooded status to a google sheet using the google drive api fig 13 shows an example of an advanced visualization for the flooded bridges directly on the geosheets permanent url this visualization shows the bridges as not overtopped green nearly overtopped yellow and overtopped red from forecast rain events unlike hosting a website to visualize the kmz file on the ec2 t2 micro instance using geosheets requires no webserver however hosting our own website in the long run will provide much more flexibility and the potential for more capabilities 4 4 automated flood warning system through aws fig 14 shows the design of the automated workflow that meets the design requirements outlined in the methods section this solution uses three aws resources i a low cost ec2 t2 micro instance running a linux operating system ii an gpu instance i e ec2 g2 or p2 instance with windows operating system and iii a s3 bucket the ec2 t2 micro instance has two roles in the workflow first the instance continuously monitors rainfall forecasts to identify an extreme weather event when an extreme weather event is identified the ec2 t2 micro instance starts the gpu instance and a model run is initiated second the ec2 t2 micro instance serves the webpages used to visualize and disseminate the model results computed by the larger gpu instance the gpu instance includes all of the model components and retrieves preprocesses and prepares the forecast rainfall data for the hydrologic models this same instance also executes the 2d hydrologic model after the model runs the gpu instance sends model outputs to the ec2 t2 micro instance for visualization and dissemination the model outputs are also sent along with the processed forecast rainfall data used as model inputs to the s3 bucket for archiving and reproducibility purposes there are two classes of users that can access the model outputs via the webpages running on the ec2 t2 micro instance regular users and power users regular users can access the current flooded locations and can register to receive alerts via email whenever locations are forecasted to flood in the current implementation regular users do not need to authenticate within the system power users have more privileges than the regular users including access to all the archived inundation maps from the s3 bucket and the ability to run the model at any time via a powershell script or through the website hosted by the t2 micro instance aws has the ability to securely control access to services and resources for specific users using the identity and access management iam service this service was used to give permission to the ec2 t2 micro instance to start and stop the other gpu instance a user account was created and given permission for starting and stopping the gpu instance fig 15 using the user credentials the gpu instance id and command lines executed in a scripting language or at the aws command line interface cli the gpu instance can be started and or stopped automatically the main script in the development web framework on the ec2 t2 micro instance is called server py code was added to this python script for monitoring and accessing the other gpu instance in this code a process is run every hour to check the hrrr rainfall data which is updated hourly if the forecasted rainfall is over a certain threshold value it will start the gpu instance that includes the hydrologic model the ec2 t2 micro instance keeps monitoring the gpu instance to make sure that it is fully started this is done by adding additional permissions to the user policy then the ec2 t2 micro instance uses secure shell ssh to initiate a batch file that runs the main workflow for retrieving the data executing the model and generating the output the 2d hydrologic model takes about 10 min to run through a forecasted period 18 h using a model grid resolution of 50 m on the m2 machine while it takes about 38 min using the model grid resolution of 30 m on the m2 machine the running time for the model with 30 m grid resolution is expected to be lower when using the ec2 p2 8xlarge instance using the p2 8xlarge aws instance with five gpus it is expected that the runtime will be 6 3 min for a 50 m grid cell size and 24 min for 30 m grid cell size the batch file that automates the model execution operates as follows first the hrrr data is retrieved and processed following this the hydrologic models are run and the maximum water level at each computational cell is computed and recorded for the duration of the simulation period once the maximum water level output file is available the kmz file is generated which includes information about each bridge and culvert provided by vdot the maximum water level predicted by the model and by how much each bridge would be overtopped the kmz file is sent to the t2 micro instance to be used for visualization using the aws private key generated for the ec2 t2 micro instance another policy added to the iam user is used to access the s3 bucket and archive the processed rainfall data fig 16 a log file is generated that includes a record of the parameters and scripts used in the whole process as a reference for users or decision makers the log file is sent to both the ec2 t2 micro instance and the s3 bucket for archiving finally any files generated from running the whole workflow are deleted to minimize the storage on the gpu instance a power user can use a powershell script to automatically initialize a model run the script gives the user the option of running the workflow either locally or with the gpu instance when the workflow is chosen to run locally the powershell script installs any required dependencies and then runs the batch file to start the workflow if the user chooses to run the workflow through the cloud the script asks for the iam policy credentials and starts the gpu instance once the instance is fully started the script uses ssh to run the batch file to start the main workflow fig 16 shows the different policies used by the ec2 t2 micro and g2 or p2 instances to access the s3 bucket folder that includes the archive information for each run also this figure shows the hierarchy of the s3 bucket folders for archiving the workflow output data the s3 bucket folders receive data from the gpu instance once it starts to give full access for these specific folders and their contents to the gpu instance another policy was added to the iam user fig 16 the gpu instance uses the iam user policy to access the main folder floodwarningmodeldata and archive the output data generated by the workflow in each specific subfolder the ec2 t2 micro instance then retrieves the archived kmz and log files to visualize them on the website this is done by using a separate policy provided by the aws s3 bucket fig 16 the t2 micro instance handles the visualization of the output data using a python based micro web framework flask http flask pocoo org fig 17 when a user accesses the website url https vfis aws uvahydroinformatics org the most recent model output kmz is displayed using the google maps javascript api the output kmz files along with the corresponding log files from only the five most recent model runs are available on the website to save storage space nginx https nginx org en and gunicorn green unicorn http gunicorn org sit in between the flask application and the internet working in tandem to support many users on the website at the same time and handle the distribution of resources the t2 micro instance also triggers a model run when hrrr rainfall forecast data exceeds a given threshold the forecast rainfall data is therefore retrieved every hour if the rainfall exceeds a certain threshold value it will start the gpu instance and initialize a model run with the latest rainfall data an alert on the website will show users whether a model is being run flooding is possible or the model is up to date with no flooding predicted fig 18 shows the architecture of the website on the main view the website contains a navbar allowing the selection of which data to view a link to the log file a login page and a page to register for email alerts the main section of the page is taken up by the google maps javascript api using the google maps javascript api allows us to easily display the map interface using all of google s resources and overlay our output data on top of it when a user clicks on a marker signifying a bridge they are presented with a box containing more information about that bridge and potential flooding events users can sign up and their email will be stored in a secured private structured query language sql database the application will detect when flooding is possible and send an email to everyone on the list through the website power users can display output data archived in the aws s3 bucket without having to store output in the t2 micro instance which has a limited amount of storage 5 conclusions this work described the creation of a cloud based flood forecasting system designed to assist transportation decision makers in time sensitive emergency situations the flood forecasting system was applied for the virginia department of transportation in the hampton roads region of virginia usa to provide decision makers with forecasts of flooded roadways and bridges in near real time based on rainfall forecasts by using gpu resources the model was executed for a 15 day duration up to 80x faster from 120 h compared to 1 5 h compared to using a single cpu an automated cloud based workflow using aws resources was designed and created to link and enhance the three core model components i retrieval and formatting of high resolution gridded hrrr rainfall forecast data ii execution of the 2d model in a short duration to identify flood prone bridges and culverts and iii real time dissemination of model output via generation of an online map with flooded locations and the ability to automatically send alert messages via email using the m2 machine described earlier the 2d hydrodynamic model which is the heart of the flood forecasting system completes an analysis for the upcoming 18 forecast hours in approximately 10 min with a model grid cell size of 50 m and approximately 38 min with a model grid cell size of 30 m using the p2 8xlarge aws instance with five gpus it is expected that the runtime will be 6 3 min for a 50 m grid cell size and 24 min for a 30 m grid cell size for hurricane sandy although the rainfall only lasted 4 days the effects of the rainfall over the study area lasted 15 days assuming a 50 m grid cell size model takes 6 3 min to run for the upcoming 18 forecasted hours on the p2 8xlarge if the model ran every hour through a 15 day period running the workflow would cost about 350 assuming current aws prices for the same scenario changing the grid cell size to 30 m modeling 18 h is expected to take about 24 min to run and cost 1260 for the 15 day duration however this assumes using five gpus further tests are required to identify the optimum number of gpus to run the model with grid cell size of 30 m on the aws ec2 p2 8 instance for this scenario because the tuflow 2d model is expensive to run on a continuous basis it is only used during extreme weather events the t2 micro instance which costs about 10 per month to run continuously monitors the hrrr forecast rainfall data and compares it to rainfall thresholds that represent the amount of rain required to cause potential flooding in the preliminary implementation we used a fixed value for the threshold in the future we plan to find a way to compare the hrrr forecast data against specific thresholds based on the antecedent moisture content of the soil before the start of any upcoming storm a main advantage of the cloud based approach presented here is that it provides a way to strategically utilize computational resources only when flood events are likely to occur additionally the workflow is automated start to finish without the need for any intermediate human interaction this means that a decision maker with little or no experience regarding the details of hydrologic modeling gridded rainfall data pre and post processing procedures and so forth can easily execute the workflow and obtain and visualize model results this work presents a preliminary calibration of the model but additional work is needed to calibrate and evaluate the model across multiple historical flooding events this calibration simply was not feasible before this work given the long model runtime it is important to note that this model has only been tested for hurricane sandy the local m2 machine which was able to run the 15 day hurricane sandy model in 2 4 h could be used for the calibration process without excessive cloud costs results of this study suggest a higher resolution grid will improve model accuracy but this too comes with an increased model runtime a final challenge that needs more investigation is the differences between cpu and gpu generated results this difference may become smaller with updates to the tuflow model software a new version of tuflow was recently released after the completion of this study and includes a significantly enhanced version of the gpu model called tuflow hpc https www tuflow com this version uses 2nd order solution accuracy solvers rather than the 1st order solvers that is used in the tuflow version used in this study it also allows the user to add 2d bridges to the model for better representation within the system and has improvements in the multiple gpu speed performance for executing the model this new version will be used in future work to further enhance calibrate and evaluate the model finally more research is needed to see if improving model input data such as using a finer dem resolution for portions of the study area or nexrad rainfall data will improve the gpu based model results software availability the software created in this research is free and open source the software information and availability are as follows developers mohamed morsy daniel voce gina o neil and jeffrey sadler programming language python bash html javascript powershell css github link https github com uva hydroinformatics floodwarningmodelproject acknowledgments this work was supported by the virginia transportation research council vtrc under grant 107898 we also would like to acknowledge the viz lab a facility for university of virginia students staff and faculty to explore and investigate the power of visualization in research and education for providing us with a local machine with gpus to test our design and perform runs before moving the model to the cloud 
26370,the ability to quickly and accurately forecast flooding is increasingly important as extreme weather events become more common this work focuses on designing a cloud based real time modeling system for supporting decision makers in assessing flood risk the system built using amazon web services aws automates access and pre processing of forecast data execution of a computationally expensive and high resolution 2d hydrodynamic model two dimensional unsteady flow tuflow and map based visualization of model outputs a graphical processing unit gpu version of tuflow was used resulting in an 80x execution time speed up compared to the central processing unit cpu version the system is designed to run automatically to produce near real time results and consume minimal computational resources until triggered by an extreme weather event the system is demonstrated for a case study in the coastal plain of virginia to forecast flooding vulnerability of transportation infrastructure during extreme weather events keywords flood warning applications cloud computing 2d hydrologic model gpus amazon web services reproducibility 1 introduction floods were the number one natural disaster in the us in terms of lives lost and property damage incurred during the 20th century perry 2000 with statistics showing that total flood insurance claims averaged more than 1 9 billion per year from 2006 to 2015 nfip statistics 2016 rainfall events are predicted to become more frequent and intense due to climate change which is expected to cause increased flooding melillo et al 2014 as society faces flooding events with increasing frequency and intensity flood modeling will become an even more important tool for decision makers such models can be used to warn municipalities and communities of forecasted flooding impacts and to test alternative flood mitigation strategies for addressing flood problems the national research council nrc has recommended increased use of two dimensional 2d hydrodynamic models for flood risk management purposes nrc 2009 there are several advantages to using 2d models rather than one dimensional 1d models such as better resolution of velocity localized depth and surface water elevation and determination of floodplain extent directly 2d hydrodynamic models are especially important for cases with complex flows such as in low relief terrains with flat or mild slopes for these low relief terrains 1d models are often not sufficient due to the limitations of assumed uniform water velocity and constant water surface elevation modeled on each cross section garcia et al 2015 executing 2d hydrodynamic models at the regional scale 10 103 100 103 km2 requires parallel computation in order to run in a timeframe reasonable for flood warning applications graphical processing units gpus have recently been shown to effectively execute parallelized 2d hydrodynamic models with observed speed ups of 20x to 100x huxley and syme 2016 garcia et al 2015 vacondio et al 2014 kalyanapu et al 2011 vacondio et al 2014 expect that gpus will continue to be attractive for 2d numerical models compared to clusters of central processing units cpus for several reasons i fast developing gpu hardware ii quickly decreasing costs and iii lower maintenance compared to large cpu clusters with the speed ups provided by gpus regional flood warning systems can now be implemented with 2d hydrodynamic models and the spatial resolution needed to provide targeted and detailed information to decision makers there are currently several related efforts aimed at improving flood warnings the national weather service nws and the united states geological survey usgs have a joint project to generate flood inundation maps at locations where a nws forecast point and a usgs stream gauge exist fowler 2016 at these locations a flood inundation map is created for multiple possible water surface elevations and by using a rating curve and forecasted discharge the data is converted into the corresponding water surface elevation then the corresponding flood inundation map is selected from a precomputed library of flood inundation maps the flood locations and simulated hydrographs flash is a system designed by researchers and developers to improve the ability of the nws to forecast flooding at the weather forecast offices wfos gourley et al 2017 the flash system uses the multi radar multi sensor mrms rainfall data with a spatial resolution of 1 km and temporal resolution of up to 2 min along with a highly efficient distributed hydrologic modeling framework to generate flood forecasts for over 10 8 million grid points across the conterminous united states conus the national flood interoperability experiment nfie is a multiagency effort in collaboration with the academic community to improve river and flood forecasts maidment 2017 a key component of nfie is a model called routing application for parallel computing of discharge rapid http rapid hub org which was developed to operate on the 2 67 million nhdplus catchments and use parallel computing to solve the 1d muskingum flow equations on this large river network maidment 2017 david et al 2013 2011 nfie showed it was possible to increase the spatial density of flooding forecast locations by more than 700x compared to the present nws river forecast system maidment 2017 the rapid model is expected to be replaced by the simulation program for river networks sprint liu and hodges 2014 which has the capability of solving the full nonlinear saint venant equations for 1d unsteady flow and depth for channel networks and promises speed up of the computation time however in some instances a 2d flood model will be necessary to accurately model water transport over large flat areas delft fews is a hydrological forecasting and warning framework that provides a platform through which operational forecasting systems can be constructed allowing for flexibility in the integration of models and data werner et al 2013 delft fews does not contain any inherent hydrologic modeling capabilities within its code base instead it relies on the integration of external hydrologic model components the objective of this research is to design and prototype a cloud based system to support decision makers as they assess flood risk to transportation infrastructure during extreme weather events the system automates access and pre processing of forecast data execution of a high resolution 2d hydrodynamic model and map based visualization of model outputs this work advances on prior approaches described earlier by presenting a cloud based framework for modeling regions with complex flows using a 2d hydrodynamic model rather than relying on precomputed flood maps flood depths and extents this approach allows for water flows to be modeled in real time based on current and forecasted conditions this approach could be adopted in existing decision support systems to leverage cloud and gpu resources within this general framework the study advances on previous work funded by the virginia department of transportation vdot for the hampton roads district of virginia which produced the regional river severe storm model r2s2 hassan water resources plc 2012 the purpose of r2s2 is to help residency administrators to efficiently allocate scarce resources to road closures and to assist first responders with entering and exiting flood prone areas this research advances r2s2 by automating what was previously a manual process of converting forecast rainfall data into model inputs running the model and visualizing the results furthermore this research addresses computational challenges with using r2s2 for real time flood warning and emergency management applications this research also moves r2s2 to the cloud and is one of the first cloud based flood warning applications with i an automated workflow for obtaining the real time forecast rainfall data ii execution of a model to identify flooded bridge and culvert locations in a time duration sufficient for warning and emergency management purposes and iii generation of an online map with locations of the flooded roadways and bridges and the ability to send automated warning messages via email this system can provide vdot with information needed when determining road closures disseminating warning messages for area residents and making other emergency management decisions that affect human safety and property damage although the current case study application of the system is focused on vdot as the primary user the approach could be used as a more general flooding decision support system for other stakeholders cloud computing is gaining attention in environmental applications to satisfy the peak performance needs of applications that use large amounts of processing power granell et al 2016 sun 2013 used google drive a cloud computing service to host an environmental decision support systems edss module that is migrated from the traditional client server based architecture google drive has the capability of providing a number of basic visual analytics features and the collaboration between the decision makers can be increased while decreasing the cost of small scale edss ercan et al 2014 used the windows azure cloud environment to run a created calibration tool built with the modified calibration method a parallel version of the dynamically dimensioned search dds for calibrating the soil and water assessment tool swat using this tool results showed a significant speed up of the model calibration for six different model scenarios wan et al 2014 introduced a public cloud based flood cyber infrastructure cyberflood cyberflood collects organizes manages and visualizes several global flood databases for decision makers and public users in real time this database is expanded by applying a methodology for the data collection in which the public reports new flood events using smartphones or web browsers hu et al 2015 implemented a web based application in the hadoop based cloud computing environment to make enhanced coupled human and natural models publicly available this allows users to access and execute the model without an increase in response time kurtz et al 2017 presented a stochastic cloud based fully operational architecture for a real time prediction and management system related to groundwater management this proposed system allows for data assimilation and is coupled with a physically based hydrologic model hydrogeosphere in a cloud environment to use the generated prediction for groundwater management the work presented here advances on prior studies by demonstrating the ability of using resources in a public cloud including instances with powerful gpus like those provided by aws to build an end to end automated cloud based system for regional scale flood forecasting this system is able to run a computationally expensive 2d hydrodynamic model and is activated automatically during extreme weather events by software that is continuously monitoring forecasted rainfall conditions it is designed to run in a time frame relevant to real time emergency management applications and automatically deliver model outputs to decision makers through online maps and email notifications the remainder of the paper is organized as follows first a study area section is presented to introduce the region where the model is applied second the data and methods section is presented to outline the available data sources the pre processing steps used to translate this data for use in the model steps taken to speed up the model and the post processing steps used to automate the model output dissemination next the results and discussion section presents a prototype of the software and the results of applying the system for an extreme weather event finally the conclusions section provides a summary of the key research outcomes and steps that could be taken to further advance this work 2 study area the study area is in a portion of the chowan river basin that is within vdot s hampton roads district in virginia usa and is approximately 5780 km2 2230 mi2 fig 1 the study area includes the meherrin nottoway and the blackwater rivers the longest flowpath along nhd flowline features is approximately 175 km 109 mi with a slope that varies from nearly 0 to 21 the study area includes 493 georeferenced vdot bridges and culverts due to a high portion of the study area consisting of low relief terrain in the coastal plain especially in the eastern part of the study area fig 2 the system utilizes a 2d hydrodynamic model called two dimensional unsteady flow tuflow https www tuflow com syme 2001 the upstream portion of the project domain consists of relatively higher relief terrain and therefore the hydrologic engineering center hydrologic modeling system hec hms a lumped hydrology model that is less computationally intensive was deemed appropriate to model flows in these areas by using hec hms to generate inflow boundary conditions from the high relief portion to the low relief portion of the study area the overall system runtime is kept smaller including these high relief upstream watersheds the project domain is approximately 11 000 km2 4240 mi2 3 data and methods 3 1 r2s2 system the r2s2 system was first developed by hassan water resources plc to integrate multiple datasets with sophisticated hydrodynamic models to provide flood risk prediction during severe storm events to the hampton roads district of vdot hassan water resources plc 2012 the r2s2 system consists of software that processes the many input files required for the tuflow model runs hec hms to establish boundary conditions for tuflow and processes output files from tuflow to determine inundated bridges and culverts fig 3 constant input data for the r2s2 include a dem with 10 m resolution soil data ssurgo 2012 and land use data with 30 m resolution the observational data must be accessed and processed in real time from federal data providers r2s2 uses real time products for rainfall including national oceanic and atmospheric administration noaa gauges see fig 8 and next generation weather radar nexrad radar data where available for specific storms the rainfall data is used as inputs for both r2s2 hydrologic models first the hec hms model uses the rainfall data to generate the hydrograph for each of the 11 subwatersheds that border the study area then the 11 outlet hydrographs generated are applied to the tuflow model as boundary conditions along with the rainfall data to generate water levels throughout the study area the model is calibrated and evaluated using historic stream gauge data eventually real time stream data will be used to set initial conditions 3 2 rainfall forecast data preparation a preliminary step for building a flood warning system is to identify and automate the pre processing of the forecast rainfall product in this study the procedure to collect and process the forecast rainfall data for model input was automated to reduce human translation errors and decrease the time between when new rainfall forecasts are available and new water level forecasts can be generated both the tuflow and hec hms models in r2s2 require input rainfall data but in different formats tuflow has three approaches for applying the rainfall directly to the computational cells i polygons covering multiple cells assigned as rainfall time series ii gridded rainfall created as ascii files for each time step or as one netcdf file and iii a rainfall control file that allows a user to specify point time series over the model domain and specify how the rainfall is interpolated to the model cells hec hms has two approaches for applying the rainfall data i a rainfall time series for each basin stored in a data storage system dss file that is prepared by hec dssvue a program for viewing editing and manipulating dss files ceiwr hec 2009 and ii gridded rainfall that is prepared by hec gridutil a utility program for managing gridded data with hec dss steissberg and mcpherson 2011 the identification of appropriate forecast datasets focused on noaa rainfall products that are in a grid format and can be quickly accessed for real time flood warning applications several potential forecast datasets were identified for the study region including products from the rapid refresh rap the high resolution rapid refresh hrrr the north american mesoscale forecast system nam and the national digital forecast database ndfd the rap hrrr and nam products are all provided by the national center for environmental prediction ncep and the ndfd is provided by the national weather service nws these forecast products were compared in terms of their spatial resolution temporal resolution and frequency of model initiation i e model cycle results of this comparison and the code written to automate the retrieval and reformatting of the rainfall data to meet the requirements of the tuflow and hec hms models are presented in section 4 1 this code is tested to retrieve the required rainfall data with the desired formats for the two models and with the correct spatial and temporal resolution 3 3 speeding up r2s2 execution tuflow is the computational bottleneck within the overall r2s2 workflow using a cpu for computation takes more than three days to run a 15 day simulation period the duration over which hurricane sandy caused high flows in the study region the use of multiple cpus and gpus has been investigated as a means of speeding up 2d hydrodynamic models kalyanapu et al 2011 brodtkorb et al 2012 rostrup and sterck 2010 castro et al 2011 lacasta et al 2013 sanders et al 2010 garcia et al 2015 as stated in the introduction gpu use offers the performance of smaller clusters at a much lower cost jacobsen et al 2010 therefore gpus were investigated for speeding up the tuflow model rather than using cpu clusters tuflow comes with a gpu module capable of operating on multiple gpus in parallel we explored the use of both local and amazon web services aws resources for gpu computations the tuflow gpu module uses an explicit scheme only while the tuflow cpu solver tuflow classic uses an implicit scheme explicit schemes could be less numerically stable compared to implicit schemes if using the same time step and grid cell size also explicit schemes require a small time step and high resolution grid cell size to compete with well developed implicit schemes boris 1989 anderson and wendt 1995 tóth et al 1998 pau and sanders 2006 zhao 2008 the differences between these two schemes could be large and need to be checked for consistency two local gpu resources with different capabilities were explored table 1 m1 is a machine with a modest gpu and other resources typical of most desktop computers m2 is a high end workstation with 64 gb of ram and two nvidia geforce titan graphics cards there are several types of aws elastic compute cloud ec2 instances designed for gpu based computations there are two sizes of g2 instances which have lower end gpus and three sizes of p2 instances which have higher end gpus table 2 the properties and hourly fees for these instances vary as shown in table 2 several tests were performed to measure the tuflow model execution times using the aws ec2 g2 8xlarge and p2 8xlarge instances the tuflow model with a 50 m grid cell size was used for these tests the g2 8xlarge instance which has 4 gpus was used to execute the model with 1 2 3 and 4 gpus likewise the p2 8xlarge instance which has 8 gpus was used to execute the model with 1 through 8 gpus each of these model runs was performed twice to ensure that model runtimes were consistent following this comparisons of results generated from cpu and gpu solvers were performed lastly using the gpu solver preliminary calibration steps were performed by varying grid cell size and input parameters 3 4 post processing and automating model output dissemination the last main step for the flood warning system is to post process and automatically disseminate the system output one of the most important outputs from the tuflow model for this study is the maximum water level at each computational cell within the study area throughout the simulation duration using these maximum water levels and the vdot bridge locations and deck elevations a post processing workflow was created and tested to automate email notifications providing bridges expected to be overtopped based on model projections in addition to sending email alerts map based visualizations can be created to display flooded bridge locations with flooded depths over the model domain we explored and tested three options for creating such maps table 3 a first and more basic option involved a user manually uploading a keyhole markup language zipped file kmz containing post processed output from the model to the google maps website for visualization providing a quick and simple method to visualize the flooded bridge locations a second more complex option was to use geosheets https www geosheets com an add on to the google sheets app that reads and displays post processed tabular data stored in a google drive account as a google sheet unlike the manual upload of a kmz file to google maps this second method can be automated to dynamically update the flooded bridge locations map this option allows for some customization of map display without needing to configure and deploy a web server the third and most complex option was developing a custom web interface using the google maps api to visualize the output kmz files along with workflow run information see section 4 4 this alternative required the deployment of a web server and was therefore more complex however it provided the highest potential for customization and supported the dissemination of other workflow run information in addition to the flooded bridge locations the post processing workflow and three visualization options were tested to ensure the correct dissemination of the model output and the proper visualization for the decision maker 3 5 design of an automated flood warning system through aws after automating the retrieval of the forecast rainfall data speeding up the 2d model and providing methods for post processing and automating the model output dissemination the final step was to create a seamless workflow using cloud services to link these individual components together without requiring intermediate user action the goal of this automated workflow was to identify the flooded bridges and or culverts in a time duration sufficient for warning and emergency management purposes based on the highest resolution reliable rainfall forecast data and by using publicly available cloud computing resources given that a single cloud instance capable of all of these tasks would be too expensive to continuously run the design of this workflow had to meet several requirements i a smaller low cost instance to monitor the rainfall data for upcoming extreme events and visualize model outputs ii a larger instance with nvidia gpu capabilities to accommodate and execute the hydrologic models and other processing scripts and iii a storage resource to archive model inputs such as the processed rainfall data and model outputs for later analysis the smaller instance would trigger the larger instance when a flood event is forecasted this smaller instance would also assume the role of maintaining the website to display and disseminate the model output so that it can be continuously available to automate these steps of the workflow the gpu instance would need to execute a batch file that i runs the pre processing scripts to prepare the rainfall data ii runs the hydrologic models iii runs the post processing script for preparing the model output for dissemination iv sends outputs to other cloud resources for archiving and visualization and v removes the model output files from the gpu instance each individual step in the workflow i e pre processing the rainfall data speed up the model and post processing and disseminating the model output was tested separately as mentioned in the previous subsections then the entire workflow was tested together locally and remotely using aws resources to test locally the batch file that initiates the workflow was run along with the workflow scripts and hydrologic models that were placed on a local machine with nvidia gpu capabilities then the batch file was tested to run the whole workflow seamlessly then a low cost smaller instance was created to monitor the forecast rainfall data the smaller machine was then set up to monitor forecasted rainfall and to trigger the local machine to run the batch file and receive post processed output for visualization purposes after performing several tests of the designed system locally the batch file and the hydrologic models were placed in a larger aws instance with nvidia gpu capabilities then the smaller machine was linked to this larger machine with an adjustment to the rainfall threshold to start the larger machine the aws based system performance was then monitored and analyzed to ensure it was working as expected 4 results and discussion 4 1 rainfall forecast data preparation comparison of the spatial resolution temporal resolution and model cycle of each dataset table 4 shows that hrrr was the best forecast rainfall product for our purposes hrrr is a weather prediction system composed of a numerical forecast model and an analysis assimilation system to initialize the model hrrr is a higher resolution model nested inside the hourly updated rap although rap can provide upper level analyses and short range forecasts hrrr is best used to examine surface and near surface parameters such as surface precipitation the hrrr model is run every hour of the day and forecasts out to 18 h on a 1 h time step for each cycle it provides a surface total precipitation product in units of mm of precipitation depth at a horizontal resolution of 3 km noaa 2012 surface total precipitation can be accessed as gridded data with dimensions of longitude latitude and time longitude and latitude are provided in the world geodetic system wgs 1984 coordinate system and time is in units of decimal days since 1 1 1 00 00 0 0 noaa 2017a hrrr data are distributed as part of the noaa operational model archive and distribution system nomads project a network of data servers that use the open source project for a network data access protocol opendap noaa 2017a although the hrrr data was selected as the primary input to the model the system could alternatively use the coarser quantitative precipitation forecast qpf from the ndfd dataset which forecasts rainfall for the upcoming 72 h to monitor for large rainfall events beyond the 18 h horizon captured by hrrr and thus allowing for a longer lead time for preparing for severe storms the use of higher resolution rainfall forecast data with a longer lead time will reduce the uncertainty of the model making it a more useful decision support tool and the system is developed in a flexible way that easily enables the application of this better forecast data that may be available in the future fig 4 shows the workflow for downloading and reformatting the forecast rainfall data pydap a pure python library client for opendap servers is used to retrieve the desired forecast data for the study area the automated workflow consists of three main parts i access the latest available forecast data from the hrrr database ii retrieve the forecast surface total precipitation with a horizontal resolution of 3 km in wgs 1984 coordinate system and iii reformat the forecast data for model input in the nad83 utm 18n projected coordinate system these rainfall data are reformatted as gridded rainfall data for tuflow using the geospatial data abstract gdal ogr python library and as subwatershed time series for hec hms using hec dssvue python and java libraries to include these direct rainfall data in tuflow a tuflow event file tef was created to define the storm event properties using the new tef file the user can run the model for a given storm event using either historic or forecast data 4 2 speeding up r2s2 execution the model speed up was evaluated using rainfall from hurricane sandy as input the rainfall lasted for four days and the total modeled time span was 15 days october 28 november 11 2012 table 5 summarizes the results of the three tuflow model scenarios using the m1 and m2 machines see table 1 using the cpu the model took 120 h to execute and using the modest gpu in the m1 machine the model took 11 5 h to execute 10x speed up compared to the cpu using the two more powerful gpus in the m2 machine the model took only 2 4 h to execute 50x speed up compared to the cpu and 5x speed up compared to the m1 machine using the single gpu the input time step did not have a significant effect on the execution time when using gpus which is due to the explicit scheme within the tuflow gpu module that takes the input time step value as an initial value and then optimizes the time step to meet the convergence condition i e courant number 1 bmt wbm 2016 a test was also conducted to determine how increasing the number of gpus influenced model execution time fig 5 as expected running the model by using different numbers of gpus produced the same output results i e no differences in the maximum water levels fig 5 a provides the results of this test using the gpu model and the aws g2 8xlarge instance with different numbers of gpus using the g2 8xlarge instance with one gpu the model takes about 4 6 h to run using the g2 8xlarge instance and increasing the number of gpus the minimum execution time is 3 h when all four gpus are used which costs about 9 per run because only four gpus were available on this instance we were not able to test whether additional gpus would continue to reduce the running time fig 5 b provides the results of this test using the gpu model and the aws p2 8xlarge instance with different numbers of gpus using the p2 8xlarge instance with one gpu the gpu model takes 2 75 h to run which is less than using the g2 8xlarge instance with 4 gpus this shows the benefit of the more modern gpus in the p2 versus g2 ec2 instances using the p2 8xlarge instance and increasing the number of gpus the minimum execution time was found to be 1 5 h which is achieved when five gpus are used this run with the minimum execution time of 1 5 h costs about 13 per run which is about 1 5x more expensive than the g2 8xlarge instance run however it was 2x faster than the g2 8xlarge instance comparing this 1 5 h execution time to the cpu execution time of 120 h shows an 80x speed up for the model using six or more gpus on this instance increases the execution time compared to using five due to known tradeoffs caused by data transfers between parallel gpu units huxley and syme 2016 because the cpu and gpu tuflow solvers use different numerical schemes it is important to understand differences in their outputs fig 6 fig 6 provides the differences in maximum water level max wl generated from executing the model using the cpu and the gpu solvers the maximum difference in max wl across the study area was around 2 5 m 8 ft with 87 of the computational cells having differences in the max wl less than 0 5 m 1 6 ft fig 7 shows the max wls at each bridge location generated by executing the model using the cpu solver versus the gpu solver the mean absolute error mae of 0 48 m 1 6 ft and the root mean square error rmse of 0 78 m 2 6 ft demonstrate a fairly significant difference in this study we did a preliminary sensitivity analysis by changing the model grid cell size and manning coefficient values but future research should investigate this difference more fully the model results using both the cpu and gpu solvers were compared against stream stage observations for the hurricane sandy event fig 8 and table 6 show the usgs stations with data availability for the event the usgs provided unpublished stage data that is considered provisional and therefore may contain erroneous or missing values due to instrument malfunction this data was processed and cleaned to address this issue before being compared to the model output data fig 8 also shows the noaa stations with the available recorded rainfall data for the hurricane sandy storm event hyetographs for this storm event at these stations are shown in fig 9 the finite volume schemes used by the 2d models are heavily dependent on the grid cell shape and size leveque 2002 caviedes voullième et al 2012 the tuflow model gpu solver uses only a cartesian grid with the capability of changing the grid cell size the tuflow model was executed using the gpu solver with grid cell sizes of 50 m 40 m 30 m and 20 m the output data from each of these runs were compared to the observed data at the six usgs stations and model results from cpu solver execution with a cell size of 50 m the modeled peaks using the gpu solver with 50 m grid cell size were significantly higher than the observed data and the model peaks using the cpu solver at four usgs stations 02045500 02047000 02047500 and 02052000 however at one of the usgs stations 02050000 the modeled peak using the gpu solver with 50 m grid cell size was significantly lower than the observed data and the modeled peak using the cpu solver finally at another usgs station 02049500 the modeled peak using the gpu solver with 50 m grid cell size was almost the same as the model peak using the cpu solver however both peaks were significantly lower than the observed data the differences between the modeled and observed peak stages could be due to the lack of adequate bathymetry data in the major rivers and tributaries in all of the minor tributaries and some stretches of the main rivers bathymetry had to be assumed because of a lack of this data this also could be due to the coarse dem resolution 10 m as the tuflow model extracts and utilizes the ground elevation at the model grid cell center for the gpu solver and at the model grid cell center and mid sides for the cpu solver bmt wbm 2016 calibration was also a challenge due to the scarcity of operating river gauges and limited available data for event based calibration over such a large study area in some instances 2d models are not used due to the low resolution of the spatial data available and the difficulties faced when calibrating the model parameters caviedes voullième et al 2012 this large study area includes only six usgs gauges that recorded stream stage during hurricane sandy three of these stations are located on the same main stream at the eastern part of the study area one is in the middle of the study area and the other two are located in the western part of the study area when the cell size of the model using the gpu solver decreases a significant reduction in the peak stages was observed at four of the six usgs stations 02045500 02047000 02047500 and 02052000 at station 02050000 the modeled peak stage using the gpu solver increased with decreasing cell size while at station 02049500 the peak stage remained nearly constant with each cell size decreasing model grid cell size improved the matching of observed peaks at four of the six observation sites and therefore we decided to use a smaller cell size in the model application the drawback of a smaller cell size is an increase in model execution time fig 10 shows the model execution time using the gpu solver with different grid cell sizes 50 m 40 m 30 m and 20 m for the m2 machine see table 1 fig 10 also shows the mae resulting from comparisons of model output generated using the gpu solver at different cell sizes and the model output generated using the cpu solver with the 50 m cell size based on these results we chose the 30 m cell size since there is only a small difference between this scenario and the scenario resulting from the gpu solver with a 20 m grid cell size model and because there is a significant increase in the model runtime 2 8x from 10 2 h to 28 h in addition to decreasing the cell size to 30 m we also adjusted the manning coefficient n to test its sensitivity and ability to improve matching of observed peak stages obtained from the six usgs stations the model initially had manning coefficient values based on the study area land use to assess the sensitivity of the model to changes in the manning coefficient this value was changed to be 0 6n 0 8n 1 0n 1 4n and 1 8n as the manning coefficient value decreased the modeled peak stages became closer to the observed peaks at stations 02045500 02047000 20047500 and 02052000 after reducing the grid cell size from 50 m to 30 m and changing the manning s coefficient from 1 4n to 0 6n the model came the closest to matching observed peak river stage this represents a preliminary calibration of the model that should be more fully explored through additional research the analysis of model response to changing the grid cell size and manning s coefficient was done by applying rainfall time series for hurricane sandy from five rain gauges to polygons that each covered multiple model grid cells tuflfow also has the capability of using direct rainfall data that applies input rainfall values to every cell in the 2d hydrodynamic model when the rainfall is directly applied to the cells the model routes flow based on the cell topography on a cell by cell basis huxley and syme 2016 huxley and syme 2016 investigated using this new method by applying the direct gridded rainfall data and found that gpu direct rainfall hydraulic modeling can be used as an alternative to runoff routing hydrology modeling to check the model behavior using the direct gridded rainfall data method with the chosen grid cell size and manning s coefficient values rainfall data from hurricane sandy was obtained from the tropical rainfall measuring mission trmm this data has resolution of 0 25 0 25 resulting in 16 cells covering the entire study area we hoped to use rainfall data from nexrad provided by noaa but there was no data available for the dates of hurricane sandy for our study area fig 11 and table 7 show the results of using the gridded rainfall data provided by trmm when executing the model using the gpu solver with a grid cell size of 30 m and manning s coefficient value of 0 6n using the gridded rainfall data with this coarse resolution produces results very similar to those found when using the rainfall gauge data and the polygon method the model results almost match the observation peaks at the 02045500 02047000 02047500 and 02052000 usgs stations the other two usgs stations 02049500 and 02050000 where the modeled peaks are further from the observed peaks are located on the same stream at the eastern part of the study area along with station 02047500 this area has the mildest slopes in the study area almost flat see fig 2 the station furthest upstream is 02047500 at this station the model predicts a slightly higher peak than the observed data and the modeled peak using the cpu model the second station 02049500 has a much lower peak than the observed data however the modeled peak using the cpu solver is even lower than the modeled peak using the gpu solver the peak at station 02050000 is much higher than the observed peak and the modeled peak using the cpu solver the variation between the observed and modeled peaks at these three stations could be due to the coarse dem resolution 10 m 10 m used in the model the slightly higher peak at 02047500 may be due to slopes derived from the dem being milder than the real slopes the much lower peak and lower volume at 02049500 could be due to unrealistically steep slopes derived from the dem compared to real slopes like with 02047500 the much higher peaks at 02050000 may be due to the dem derived slopes which are milder than the real slopes this would explain why the differences in the peaks at stations 02049500 and 02050000 are nearly the same but the one is below and the other is above the observed peak if the slopes of the contributing areas to station 02049500 were milder the peak there would be higher and the peak at the downstream station 02050000 would be lower making both closer to the observed data this might improve if a higher dem resolution is used within the model future work will explore this and the use of nexrad which was unavailable for the study time period to better understand the benefit of this rainfall data for predicting the stage depth peaks 4 3 post processing and automating model output dissemination fig 12 shows the resulting workflow for model output post processing and dissemination of model results this workflow uses different python libraries such as gdal ogr and simple kml library simplekml to generate the visualization of the flooded bridge locations and an email library to automatically email warnings to decision makers the workflow and its products could be used with arcmap google maps google earth geosheets or a custom website such as the one we have configured and hosted on the aws ec2 t2 micro instance fig 12 there are three products for visualization that can be generated from this workflow i an esri shapefile that includes just the flooded bridges ii a kmz file that includes flood information for all bridges that can be visualized through google maps or google earth and iii a dynamic and real time visualization on geosheets created by automatically uploading the bridges with their flooded status to a google sheet using the google drive api fig 13 shows an example of an advanced visualization for the flooded bridges directly on the geosheets permanent url this visualization shows the bridges as not overtopped green nearly overtopped yellow and overtopped red from forecast rain events unlike hosting a website to visualize the kmz file on the ec2 t2 micro instance using geosheets requires no webserver however hosting our own website in the long run will provide much more flexibility and the potential for more capabilities 4 4 automated flood warning system through aws fig 14 shows the design of the automated workflow that meets the design requirements outlined in the methods section this solution uses three aws resources i a low cost ec2 t2 micro instance running a linux operating system ii an gpu instance i e ec2 g2 or p2 instance with windows operating system and iii a s3 bucket the ec2 t2 micro instance has two roles in the workflow first the instance continuously monitors rainfall forecasts to identify an extreme weather event when an extreme weather event is identified the ec2 t2 micro instance starts the gpu instance and a model run is initiated second the ec2 t2 micro instance serves the webpages used to visualize and disseminate the model results computed by the larger gpu instance the gpu instance includes all of the model components and retrieves preprocesses and prepares the forecast rainfall data for the hydrologic models this same instance also executes the 2d hydrologic model after the model runs the gpu instance sends model outputs to the ec2 t2 micro instance for visualization and dissemination the model outputs are also sent along with the processed forecast rainfall data used as model inputs to the s3 bucket for archiving and reproducibility purposes there are two classes of users that can access the model outputs via the webpages running on the ec2 t2 micro instance regular users and power users regular users can access the current flooded locations and can register to receive alerts via email whenever locations are forecasted to flood in the current implementation regular users do not need to authenticate within the system power users have more privileges than the regular users including access to all the archived inundation maps from the s3 bucket and the ability to run the model at any time via a powershell script or through the website hosted by the t2 micro instance aws has the ability to securely control access to services and resources for specific users using the identity and access management iam service this service was used to give permission to the ec2 t2 micro instance to start and stop the other gpu instance a user account was created and given permission for starting and stopping the gpu instance fig 15 using the user credentials the gpu instance id and command lines executed in a scripting language or at the aws command line interface cli the gpu instance can be started and or stopped automatically the main script in the development web framework on the ec2 t2 micro instance is called server py code was added to this python script for monitoring and accessing the other gpu instance in this code a process is run every hour to check the hrrr rainfall data which is updated hourly if the forecasted rainfall is over a certain threshold value it will start the gpu instance that includes the hydrologic model the ec2 t2 micro instance keeps monitoring the gpu instance to make sure that it is fully started this is done by adding additional permissions to the user policy then the ec2 t2 micro instance uses secure shell ssh to initiate a batch file that runs the main workflow for retrieving the data executing the model and generating the output the 2d hydrologic model takes about 10 min to run through a forecasted period 18 h using a model grid resolution of 50 m on the m2 machine while it takes about 38 min using the model grid resolution of 30 m on the m2 machine the running time for the model with 30 m grid resolution is expected to be lower when using the ec2 p2 8xlarge instance using the p2 8xlarge aws instance with five gpus it is expected that the runtime will be 6 3 min for a 50 m grid cell size and 24 min for 30 m grid cell size the batch file that automates the model execution operates as follows first the hrrr data is retrieved and processed following this the hydrologic models are run and the maximum water level at each computational cell is computed and recorded for the duration of the simulation period once the maximum water level output file is available the kmz file is generated which includes information about each bridge and culvert provided by vdot the maximum water level predicted by the model and by how much each bridge would be overtopped the kmz file is sent to the t2 micro instance to be used for visualization using the aws private key generated for the ec2 t2 micro instance another policy added to the iam user is used to access the s3 bucket and archive the processed rainfall data fig 16 a log file is generated that includes a record of the parameters and scripts used in the whole process as a reference for users or decision makers the log file is sent to both the ec2 t2 micro instance and the s3 bucket for archiving finally any files generated from running the whole workflow are deleted to minimize the storage on the gpu instance a power user can use a powershell script to automatically initialize a model run the script gives the user the option of running the workflow either locally or with the gpu instance when the workflow is chosen to run locally the powershell script installs any required dependencies and then runs the batch file to start the workflow if the user chooses to run the workflow through the cloud the script asks for the iam policy credentials and starts the gpu instance once the instance is fully started the script uses ssh to run the batch file to start the main workflow fig 16 shows the different policies used by the ec2 t2 micro and g2 or p2 instances to access the s3 bucket folder that includes the archive information for each run also this figure shows the hierarchy of the s3 bucket folders for archiving the workflow output data the s3 bucket folders receive data from the gpu instance once it starts to give full access for these specific folders and their contents to the gpu instance another policy was added to the iam user fig 16 the gpu instance uses the iam user policy to access the main folder floodwarningmodeldata and archive the output data generated by the workflow in each specific subfolder the ec2 t2 micro instance then retrieves the archived kmz and log files to visualize them on the website this is done by using a separate policy provided by the aws s3 bucket fig 16 the t2 micro instance handles the visualization of the output data using a python based micro web framework flask http flask pocoo org fig 17 when a user accesses the website url https vfis aws uvahydroinformatics org the most recent model output kmz is displayed using the google maps javascript api the output kmz files along with the corresponding log files from only the five most recent model runs are available on the website to save storage space nginx https nginx org en and gunicorn green unicorn http gunicorn org sit in between the flask application and the internet working in tandem to support many users on the website at the same time and handle the distribution of resources the t2 micro instance also triggers a model run when hrrr rainfall forecast data exceeds a given threshold the forecast rainfall data is therefore retrieved every hour if the rainfall exceeds a certain threshold value it will start the gpu instance and initialize a model run with the latest rainfall data an alert on the website will show users whether a model is being run flooding is possible or the model is up to date with no flooding predicted fig 18 shows the architecture of the website on the main view the website contains a navbar allowing the selection of which data to view a link to the log file a login page and a page to register for email alerts the main section of the page is taken up by the google maps javascript api using the google maps javascript api allows us to easily display the map interface using all of google s resources and overlay our output data on top of it when a user clicks on a marker signifying a bridge they are presented with a box containing more information about that bridge and potential flooding events users can sign up and their email will be stored in a secured private structured query language sql database the application will detect when flooding is possible and send an email to everyone on the list through the website power users can display output data archived in the aws s3 bucket without having to store output in the t2 micro instance which has a limited amount of storage 5 conclusions this work described the creation of a cloud based flood forecasting system designed to assist transportation decision makers in time sensitive emergency situations the flood forecasting system was applied for the virginia department of transportation in the hampton roads region of virginia usa to provide decision makers with forecasts of flooded roadways and bridges in near real time based on rainfall forecasts by using gpu resources the model was executed for a 15 day duration up to 80x faster from 120 h compared to 1 5 h compared to using a single cpu an automated cloud based workflow using aws resources was designed and created to link and enhance the three core model components i retrieval and formatting of high resolution gridded hrrr rainfall forecast data ii execution of the 2d model in a short duration to identify flood prone bridges and culverts and iii real time dissemination of model output via generation of an online map with flooded locations and the ability to automatically send alert messages via email using the m2 machine described earlier the 2d hydrodynamic model which is the heart of the flood forecasting system completes an analysis for the upcoming 18 forecast hours in approximately 10 min with a model grid cell size of 50 m and approximately 38 min with a model grid cell size of 30 m using the p2 8xlarge aws instance with five gpus it is expected that the runtime will be 6 3 min for a 50 m grid cell size and 24 min for a 30 m grid cell size for hurricane sandy although the rainfall only lasted 4 days the effects of the rainfall over the study area lasted 15 days assuming a 50 m grid cell size model takes 6 3 min to run for the upcoming 18 forecasted hours on the p2 8xlarge if the model ran every hour through a 15 day period running the workflow would cost about 350 assuming current aws prices for the same scenario changing the grid cell size to 30 m modeling 18 h is expected to take about 24 min to run and cost 1260 for the 15 day duration however this assumes using five gpus further tests are required to identify the optimum number of gpus to run the model with grid cell size of 30 m on the aws ec2 p2 8 instance for this scenario because the tuflow 2d model is expensive to run on a continuous basis it is only used during extreme weather events the t2 micro instance which costs about 10 per month to run continuously monitors the hrrr forecast rainfall data and compares it to rainfall thresholds that represent the amount of rain required to cause potential flooding in the preliminary implementation we used a fixed value for the threshold in the future we plan to find a way to compare the hrrr forecast data against specific thresholds based on the antecedent moisture content of the soil before the start of any upcoming storm a main advantage of the cloud based approach presented here is that it provides a way to strategically utilize computational resources only when flood events are likely to occur additionally the workflow is automated start to finish without the need for any intermediate human interaction this means that a decision maker with little or no experience regarding the details of hydrologic modeling gridded rainfall data pre and post processing procedures and so forth can easily execute the workflow and obtain and visualize model results this work presents a preliminary calibration of the model but additional work is needed to calibrate and evaluate the model across multiple historical flooding events this calibration simply was not feasible before this work given the long model runtime it is important to note that this model has only been tested for hurricane sandy the local m2 machine which was able to run the 15 day hurricane sandy model in 2 4 h could be used for the calibration process without excessive cloud costs results of this study suggest a higher resolution grid will improve model accuracy but this too comes with an increased model runtime a final challenge that needs more investigation is the differences between cpu and gpu generated results this difference may become smaller with updates to the tuflow model software a new version of tuflow was recently released after the completion of this study and includes a significantly enhanced version of the gpu model called tuflow hpc https www tuflow com this version uses 2nd order solution accuracy solvers rather than the 1st order solvers that is used in the tuflow version used in this study it also allows the user to add 2d bridges to the model for better representation within the system and has improvements in the multiple gpu speed performance for executing the model this new version will be used in future work to further enhance calibrate and evaluate the model finally more research is needed to see if improving model input data such as using a finer dem resolution for portions of the study area or nexrad rainfall data will improve the gpu based model results software availability the software created in this research is free and open source the software information and availability are as follows developers mohamed morsy daniel voce gina o neil and jeffrey sadler programming language python bash html javascript powershell css github link https github com uva hydroinformatics floodwarningmodelproject acknowledgments this work was supported by the virginia transportation research council vtrc under grant 107898 we also would like to acknowledge the viz lab a facility for university of virginia students staff and faculty to explore and investigate the power of visualization in research and education for providing us with a local machine with gpus to test our design and perform runs before moving the model to the cloud 
26371,this paper presents an empirical study comparing different uncertainty analysis ua and sensitivity analysis sa methods focussing their usefulness for the output analysis of land use land cover change lucc agent based models abms as a result a workflow to integrate ua and sa is presented to evaluate abms outputs we developed a baseline scenario and performed a comprehensive investigation of the impacts that differences in sample sizes sample techniques and sa methods may have on the model output the analysis is done in the context of a particular agent based simulator with a lucc model in a brazilian cerrado case study the experiments indicate that there are known challenges to be overcome by the use of statistical methods even though the presented analysis was done over a particular simulator we intend to contribute to the community that understands the importance of statistical validation techniques to improve the level of confidence in agent based simulation outputs keywords uncertainty analysis sensitivity analysis agent based model spatial simulation land use cover change 1 introduction as cited in the literature the land use land cover change lucc systems are dynamic stochastic and characterized by nonlinear and non monotonic relationships between constant changing entities parker et al 2003 verburg 2006 rindfuss et al 2008 besides agent based models abms have been used as a natural metaphor to model lucc dynamics since they capture emergent phenomena and provide an original description of the modeled system schreinemachers and berger 2011 murray rust et al 2013 ralha et al 2013 however abms are prone to uncertainty because they reflect the intrinsic randomness of environmental physical and social events the uncertainty may also arise because of insufficient knowledge lack of data observation errors measurements used to parametrize the model or from vague premises of the model ligmann zielinska et al oct 2014 lilburne and tarantola 2009 as a result one could argue whether there is any quality in model predictions due to high uncertainty and the considerable number of assumptions imposed by abms models in this scenario uncertainty analysis ua and sensitivity analysis sa are currently popular topics in abms as well as for many other complex systems pappenberger et al 2008 they are valuable tools in understanding lucc models and deriving decisions on strategies to reduce model uncertainty ua provides the variability of model results sa presents which factors are responsible for this variability this variability may be expressed quantitatively in terms of elasticity of performance concerning parameter levels high sensitivities elasticities give cause for concern about the reliability of a model dayananda et al 2002 a factor is any source of uncertainty in the modeling process including model structure initial conditions and input parameters using the terminology proposed by the national research council 2012 uncertainty quantification uq is the process of quantifying uncertainties in a computed quantity of interest qoi with the goals of accounting for all sources of uncertainty and quantifying the contributions of specific sources to the overall uncertainty i e ua and sa applied in tandem although ua and sa applications are rising most abms struggle with a shortage of testing in general mainly due to time and other resource constraints kelly letcher et al 2013 lee et al 2015 argue that while a modeler invests a lot of time and effort in the development of abms the output analysis is not always considered as deserving the same resource intensive attention according to a survey carried out by heath et al 2009 less than 5 of abm publications present any statistical validation techniques angus and hassani mahmooei 2015 argue that one possible cause for this methodological anarchy derives from the fact that with so many possible degrees of freedom within an abm the responsibility to ensure and to demonstrate that a model is structurally sound and the prediction is reliable falls into each modeler we present a uq workflow to integrate ua and sa in the evaluation of agent based simulation outputs we illustrate the use of this workflow in a particular spatial explicit lucc case study in the framework multi agent system for environmental simulation mase bdi we apply general practices that should be a routine to improve the level of confidence in results and to promote more rational and efficient use of abms we may cite that broader and more complete workflows for the application of sa were already proposed such as pianosi et al 2016 and norton 2015 the ua sa integrated proposal is what set our manuscript apart we argue that ua should be used as an input to sa in a broader process of uq also we noticed some conflicting results when we compared relevant studies on sa mainly regarding the experimental setup table 1 summarizes the studies found in the literature vanrolleghem et al 2015 1 gan et al 2014 2 wang et al 2013 3 yang 2011 4 pappenberger et al 2008 5 tang et al 2007 6 some authors have compared different sa methods and experimental setup which are presented in the different lines of the table table 1 illustrates a glimpse of the myriad of possible combinations of strategies for sampling the model parameter space and sa methods to quantify the impacts of sampled parameters on the model qoi we understand that there is no combination of sampling and sa method that fits all applications thus the work of gan et al 2014 shows that different sample strategies can even produce different outputs regarding the same sa method also it seems that there isn t a clear relationship between the number of factors and the number of necessary runs to compute sa furthermore in some cases the number of runs used in the same sampling and sa method is not even in the same order of magnitude for example pianosi et al 2016 recommend 1000 m model runs to calculate variance based sa such as fast where m is the number of input factors subject to sa neither wang et al 2013 nor vanrolleghem et al 2015 nor gan et al 2014 executed this many number of runs the first used a sample of size 2049 for a 47 factor problem instead of 47 000 while the second used a sample size of 3000 for a 17 factor problem instead of 17 000 the third used a sample size of 2777 for a 13 factor problem instead of 13 000 one could ask whether the number of runs should be based on something more than m in this manuscript we will test different experimental strategies for a uq workflow and discuss their relative benefits and limitations a baseline scenario was developed and we performed a comprehensive investigation of the impacts that differences in sample sizes sample techniques and sa methods may have on the qoi in this work we address the research question how ua and sa may be applied to improve users understanding of the uncertainty and relations among input and output responses in lucc agent based simulations we are interested in finding which parameters are responsible for most of the results variability if there is convergence when different sa techniques are applied and finally if there is a minimum sample size to achieve it although the statistical techniques are applied in a specific agent based simulator the methods described are quite general and may illustrate their application in another research in section 2 we provide an overview of the different methods regarding variance stability parameter space exploration ua and sa we also present the proposed uq workflow in section 2 in section 3 we describe the mase bdi framework and lucc model used as a case study followed by the experimental design we present the results compared to related work we discuss challenges and provide some assessment to extrapolate our finding into more general conclusions to produce more robust or parsimonious models as well as to make models more defensible in the face of scientific or technical controversy section 4 finally in section 5 we summarize our findings and outline future work 2 materials and methods the methods we applied in the case study are presented in this section alongside their experimental design the uq experiments have the objective to perform an output analysis on spatial stochastic models to measure uncertainty and to reduce it ultimately we want to understand better how the model behaves and expand our confidence in the response of a lucc model 2 1 variance stability agent based simulations are often stochastic and therefore any analytical exercise requires an outcome pool drawn from a sufficient number of samples it is only possible to draw conclusions if the output mean and variance reaches relative stability otherwise the statistics could harbor too much uncertainty to be reliable lee et al 2015 moreover some abm simulations mase bdi included can take longer run times which makes the execution of large samples prohibitive hence knowing the minimum sample size to reach variance stability can be more compelling to modelers there are many methods to assess variance stability law and kelton 2000 lee et al 2015 we chose to apply the method proposed by lorscheid et al 2012 whose strategy is to assess stability from metrics on an outcome for a sequence of sample sizes the proposed metric relies on the functional ratio between the variance and the sampled mean the coefficient of variation c v is a dimensionless and normalized metric used to measure the uncertainty surrounding the variance i e used for the analysis of experimental error variance it is defined as the ratio of the standard deviation of a number of measurement s to the arithmetic mean μ 1 c v s μ if c v is obtained from a small sample it will vary more than if each sample contained far more runs lorscheid et al 2012 propose a fixed epsilon e to limit c v this is done by calculating the c v s of a different sized set of simulation runs in ascending order of size the sample size at which the difference between consecutive c v s falls below the determined criterion e and remains so is considered a minimum sample size or the minimum number of simulation runs for abms this is the point of variance stability these points should be obtained for all abm outputs thereby the minimum number of runs for the abms is the maximum of these points lee et al 2015 n m i n a r g m a x n c v x n c v x m e x and m n where n is the sample size n m i n is the estimated minimum number of required simulation runs x is a distinct output and m is some sample size for which the c v is calculated thus we apply the lorscheid et al 2012 method to establish the minimum sample size that guarantees that variance stability is achieved 2 2 parameter space exploration sampling methods provide a systematic exploration of the parameter space that guarantees the sample to have specific statistical or structural properties the purpose of these methods is to actively reduce the number of parameter sets that are considered but still chose space filling points in the design space thiele et al 2014 for a complete revision of sampling methods readers can refer to kleijnen et al 2005 saltelli et al 2008 and gong et al 2015 in this manuscript the most common sampling designs are illustrated and applied in the uq process since there are many methods to explore the parameter space readers may have an overview of those sampling methods in appendix a including monte carlo sampling mc latin hypercube lh orthogonal array oa orthogonal array based latin hypercube oalh metis sampling fourier sampling algorithm lpτ lptau sobol extended sobol morris one at a time moat 2 3 uncertainty analysis ua evaluates and quantifies how the variability of input factors propagates through the model and affects the variability of output values ligmann zielinska et al oct 2014 ua can also answer if there are any discontinuities associated with the distribution of results iman and helton 1988 plot the distribution itself calculate the average output the standard deviation the quantiles of its distribution and confidence bounds an overview of the ua process can be found in appendix b for the proposed ua the only parameters considered relevant are the ones related to the qoi and previously selected as input factors of interest all the other model factors and information fed into the model are disregarded i e they do not vary thereby they cannot cause variation in the output however the model outputs y j are non deterministic because of the stochastic component derived from the emergence of the agent s behavior therefore to ensure robustness each vector α j β j of the output must be evaluated regarding the mean and the variance dosi et al 2017 this confirmation is executed by a given number of model runs but with the same parameters configuration ten broeke et al 2016 after the ua quantified the magnitude of the resulting uncertainty in the model predictions due to uncertainties in model inputs the next step in the uq workflow would be to perform sa 2 4 sensitivity analysis sa is the study of how uncertainty in the output of a model can be apportioned to different sources of uncertainty in the model input saltelli et al 2008 the authors show that each measure of sensitivity may produce its ranking of factors by importance there are different methods of sa and each one has advantages and limitations in the particular case of sa in spatial models we incorporated the general guidelines provided by lilburne and tarantola 2009 it is clear from their work that each sa method has sampling and pre processing technique requisites therefore a careless combination of methods will result in inefficient and inappropriate results also not all of the methods are capable of providing sensitivity index for non monotonic input output dependencies typically observed in abms fonoberova et al 2013 ten broeke et al 2016 therefore we selected ten well known methods of qualitative and quantitative sa they were applied in mase bdi to verify if they were capable of providing those indexes for the lucc model in general gradient and linear regression based sa are known as qualitative methods since they use some heuristic to represent the relative sensitivity of the parameters we will assess the morris one of a time screening method moat morris 1991 and some correlation analysis such as spearman spea spearman et al 1904 and the standard regression coefficient src variance based methods are classified as quantitative methods because they tell the sensitivity of a parameter by calculating the impact of this parameter on the total variance of the model outputs saltelli et al 2004 we will assess three variance based sa techniques sobol sobol 1993 fast cukier et al 1973 and mckay 1995 also we compare response surface methods such as sum of trees sot breiman et al 1984 chipman et al 2010 multivariate adaptive regression splines mars friedman 1991 and gaussian process gp gibbs and mackay 1997 other screening methods such as the delta δ test dt pi and peterson 1994 are also assessed the overall mechanisms of each method are discussed in appendix c the implementations of each technique are not provided due to space constraint but readers may refer to tong 2005 gan et al 2014 for details 2 5 ua sa integrated workflow the integration of ua sa has been applied to abms in a few relevant studies ligmann zielinska et al oct 2014 fonoberova et al 2013 parry et al 2013 ligmann zielinska and sun 2016 that argue that a systematic evaluation of abms must comprise of an integrated approach to quantification of model output variability and its sensitivity to inputs hamilton et al 2015 argue that integrated assessmentpresent an opportunity to synthesize diverse knowledge data methods and perspectives into an overarching framework to address complex environmental problems we followed the terminology of the national research council 2012 and called this process uq the process of quantifying uncertainties associated with a model qoi to account for all sources of uncertainty ua and quantifying the contributions of specific sources to the overall uncertainty sa fig 1 presents an overview of the uq integrated workflow with ua and sa as part of the modeling process adapted from the original one proposed by ligmann zielinska et al oct 2014 analyzing the workflow we argue that ua should be used as an input to sa in a broader process of uq abm input factors are often diverse and the stochasticity makes multiple model runs a paramount step of the abm s output evaluation once the modeler defines what is the qoi to be investigated ua should be incorporated in the modeling process to indicate what is the variability of the qoi outcomes the next step would be to test the sensitivity of model response to changes in the factors this discovery could identify interactions among factors factor fixing and prioritization that could lead to a model simplification the reduction of output variance or the improvement of model accuracy this larger uq process involves many smaller tasks so a more detailed workflow is presented in fig 2 pianosi et al 2016 proposed a practical workflow for the application of sa with four fundamental group of activities i experimental setup ii input sampling iii model evaluation and iv post processing this work presents a state of the art review and a very concise guide to good practices for readers however abms have specific characteristics mainly due to stochasticity uncertainties equifinality and because of the complex system applications we took pianosi et al 2016 work as a guideline and tailored the level of effort and estimation to fit abm needs the main difference is the simplification of the sa tasks and the incorporation of the ua tasks because a portion of abm uncertainty is irreducible a comprehensive evaluation of abm uncertainty should assume that code verification model parameter calibration and validation have been successfully accomplished before uq process begins for a qoi the uq workflow for abms fig 2 is composed of three basic steps experimental setup ua and sa we maintained the terminology proposed by pianosi et al 2016 as in fig 2 the first step of the workflow regards the experimental setup with basic choices i defining the qoi the modeler must specify what the qoi for the problem at hand is ii select the input factors of interest and iii specify the range or distribution probability of each factor the fourth task iv is to determine variance stability which represents the minimum number of simulation runs that accurately report the descriptive statistics in the second step of the workflow the ua is composed of three tasks that summarize what is needed to discover what is the variability of the qoi in an abm after choosing the sampling strategy the modeler would run multiple simulations the minimum number of runs is provided by the last task of the experimental setup define variance stability abm modelers usually choose factor values randomly from their respective range distribution as a result ua produces a distribution of the qoi the last task is to use this distribution to quantify the variability of the qoi i e the use of descriptive statistics to analyze the model outputs the third step is a simplification of pianosi et al 2016 original workflow it all begins with the selection of the sa method although the original work proposed a classification system based on the sa purpose the literature shows that for abms this choice is somewhat model specific we decided to leave this decision to the modeler and tested many different methods to see the impacts of the sa method in our case study the next task would be to define the input variability space by choosing the sampling strategy to be applied there are several sampling methods and although mc is still the most used sampling strategy we tested different combinations of well known techniques such as mc and sobol and also tailored sampling strategies to see if there would be an impact on the sensitivities outcomes the number of model runs required to perform sa is usually a rough estimation of a function of the number of factors subject to sa we postulate that this minimum sample size should be equal or larger to the variance stability number of runs defined in the experimental setup step we also test this empiric assumption and discuss it in later sections sections 3 4 and 4 this is what is necessary to obtain the factor s relative importance the workflow s last two steps are checkpoints defined by pianosi et al 2016 to evaluate the model check model behavior and to assess convergence check whether sensitivity estimates are independent of the size of the sample and if they would take similar values if we used independent samples these steps inform us about the reliability of the results we applied the presented uq workflow for abms in a case study we tested several combinations of methods sampling strategies and sample sizes in section 3 we will present the application and the experimental setup designed for this application 3 a land use case study mase 1 1 project website http mase cic unb br software availability https gitlab com infoknow mase mase bdi sourcecode is an agent based simulation tool developed at the university of brasília brazil mase enables modeling and simulations of lucc dynamics using a configurable model and both top down and bottom up grimm 1999 model structures simultaneously mase enables multiple types of agents with different behaviors to represent the interaction between agents with autonomy the physical environment and its relations ralha and abreu 2017 mase has the overall goal of performing medium to long term lucc predictions it also allows assisting decision making processes related to lucc we run the experiments in mase bdi which is a freeware software extension of mase that introduces cognitive reasoning oriented agents through the implementation of the bdi rationality bratman 1987 mase bdi was implemented in jadex multi agent platform braubach et al 2005 in the bdi model agents have beliefs a set of information that about the world it inhabits that changes both the perception and thinking about the world desires represent the motivational attitudes of agents capturing the agent s wishes and driving the course of its actions an agent can also make plans related to its intention to achieve its goals this multi agent reasoning model is defined as means end reasoning wooldridge 2009 the mase bdi architecture is composed of three layers from top to bottom a user interface a utility layer and an agent layer the first provides an optional graphical interface models and simulation parameters can also be defined directly in a configuration file and a jadex control center of the bdi model the utility layer groups a set of modules to control the pre processing of the maps and input of the geographic information it also provides the simulation parameter automatic tuning which is a complex and error prone task in abms the parameter adjustment is performed by employing efficient optimization algorithms to tune the simulation model parameters concerning a user defined single or multi objective function of interest still in the utility layer a module of validation is responsible for evaluating the final simulation output maps and metrics coelho et al 2016 in the agent layer we have an organization of hierarchical agents the grid manager controls the general aspects of the simulation the spatial manager controls the agents responsible for representing and updating the spatial environment the transformation agents are computational entities accountable for moving exploring and reasoning about the space according to their internal goals and beliefs the transformation manager rules and resolves the conflict due to the competition among transformation agents concurring for the same environmental resources readers who are interested in details of the mase bdi architecture agent design and implementation may refer to coelho et al 2016 3 1 the cerrado federal district study area the federal district of brazil 5 789 k m 2 and its cerrado brazilian savanna coverage is the study area in this article the simulations depict the land changes of the region fig 3 the most endangered biome in brazil and the second largest biome in south america harboring significant biodiversity this area has been undergoing severe transformation due to the advance of cattle ranching and soy production being an attractive study area for land use simulations to allow replicability the cerrado lucc simulation model was documented and described using the odd protocol overview design concepts and details protocol grimm et al 2006 the characterization of agent behaviors and attributes in socio ecological systems were applied by empirically grounding abm mechanisms smajgl et al 2011 a complete conceptual and methodological description of the model is available in ralha et al 2013 the initialization data for the simulation is a couple of landsat derived grid raster maps consisting of the land cover of the region from two different time periods an initial and a final map furthermore the user must adjust a set of initialization parameters of the multi agent system such as the number of agents that will explore the landscape transformation agents their typology cattle ranchers and farmers and characteristics of the initial behavior of those agents the simulations are performed in steps where each step corresponds to the measure of time defined by the user in this example one step equals to one week in chronological time the user also determines the size of a plot or cell here the total area of study was divided into plots of one hectare the physical environment is spatially represented by a set of layers of geographical information data shapes or raster files such as rivers lakes slopes building areas highways environmental protected areas and regional zoning maps of the area the aggregation of these geographical features determines the physical environment of any given point in the simulation grid the transformation agents represent humans performing activities of cattle ranchers and farmers with their behavior and beliefs explicitly changing the natural landscape to achieve their internal goals e g production expansion sustainable exploration the simulations are calibrated by the simulation parameter automatic tuning tool adjusting the parameters to best fit the observed change from the two initial maps the outcome of the simulation is a result of the emergence of the agent s action within the duration of a simulation determined by the user the final landscape is a result of the emergence of the agent s effects on the land mase bdi is a spatially explicit framework because the results comprise of the quantity of land cover change and the spatial allocation of the change which plots were chosen by the agents to initiate or expand their cattle ranching or farming business the result of any mase bdi simulation is a couple of predicted maps with the spatial allocation of the land change and the quantity of change a set of metrics calculated during runtime such as the total amount of land change at the end of each simulation the resulting image is submitted to a goodness of fit measurement and the quality and errors of the quantity of change and allocation of land use change are calculated mase bdi produces stochastic simulations which mean that the same input to the model may lead to a different result in the quantity and allocation of change therefore the same set of parameters must be run several times to raise the confidence that the results are representative 3 2 lucc goodness of fit according to thiele et al 2014 there are two strategies for fitting model parameters to observational data best fit and categorical calibration mase bdi applies the first strategy in which we must find the parameter combination that best fit the data the quality measure is one exact value obtained from the observational data so it is easy to determine which parameter set leads to the lowest difference pontius et al 2008 define the most common quality measure for lucc spatial explicit simulations hence it is used in mase bdi although there is not a universally agreed upon criterion to evaluate the goodness of fit of validation maps the performance of the simulation model is done objectively by computing the sources of error of prediction maps a set of map comparisons is responsible for the evaluation of the model pontius et al 2004 indicate that three maps are necessary i a reference map of the initial time t 0 ii a reference map of a subsequent time t 1 and iii a prediction map of the subsequent time t 1 there are three possible two map comparisons picking two maps at a time comparison between the reference map of time t 0 and the reference map of time t 1 characterizes the observed change in the maps which reflects the dynamics of the landscape comparison between the reference map of time t 0 and the prediction map of time t 1 characterizes the model s predicted change which reflects the behavior of the model comparison between the reference map of time t 1 and the prediction map of time t 1 characterizes the accuracy error of the prediction s accuracy error the total disagreement between any two maps that share a categorical variable is computed in terms of quantity disagreement and location disagreement pontius et al 2004 quantity disagreement derives from differences between the maps regarding the number of pixels for each category location disagreement is the difference that could be resolved by rearranging the pixels spatially within one map so that its agreement with the other map is as broad as possible the sum of them both is the total disagreement to illustrate the methodology we present the brazilian federal district map with only two land cover categories natural vegetation the cerrado and developed areas characterized by 30 or greater of constructed materials e g asphalt concrete buildings considering these two categories the comparison of pixels may result in the categories presented in fig 4 error due to observed vegetation predicted as developed correct due to observed developed predicted as developed correct due to observed vegetation predicted as vegetation and error due to observed developed predicted as vegetation according to pontius et al 2008 the most accurate applications are the ones where the amount of observed net change in the reference maps is larger the figure of merit fom is the ratio of the amount of correctly predicted pixels of change to the sum of all pixels f o m r i g h t c h a n g e w r o n g p e r s i s t e n c e r i g h t c h a n g e w r o n g g a i n i n g w r o n g c h a n g e where wrong persistence is the area of error due to observed change predicted as persistence right change is the area of correct due to observed change predicted as change wrong gaining is the area of error due to observed change predicted as wrong gaining category and wrong change is the area of error due to observed persistence predicted as change fom is a statistical measurement that can range from 0 meaning no overlap between observed and predicted change to 100 meaning perfect overlap between observed and predicted change when the amount of correctly predicted change is larger than the sum of the various types of error fom is greater than 50 fom is the best fit quality measure of this manuscript it is also the qoi chosen to illustrate the uq workflow for abm as the first task of the experimental setup step it is worth mentioning that pontius et al 2008 set a testing benchmark based on statistical methods for map comparison of 13 applications of different popular peer reviewed land change models the results show that in 12 of the 13 lucc models predictive maps the amount of error is more significant than the amount of correctly predicted change at the resolution of raw data in contrast mase bdi was able to surpass these statistics presenting results that show high quality in the accuracy of their predictions fom 50 the complete explanation of the mase simulation results using pontius statistical techniques of map comparison to land change models is presented in ralha et al 2013 3 3 mase bdi and uq tool integration previous work demonstrates that the initialization of the agents may have a substantial effect on the land dynamics and into the final simulation outcome lorscheid et al 2012 therefore it was paramount to use a framework to control calculate trace manage uncertainties and finally make the output analysis feasible the mase bdi framework itself does not provide the modeler with the means to statistically analyze the results the difficulty to perform many different samplings ua and sa analysis may lead to a shortage of testing and finally to a perfunctory uq to avoid this pitfall we chose a statistical platform that provided the tools needed to execute both ua and sa steps in the proposed uq workflow for abm fig 2 among the different uq platforms available we chose psuade 2 2 http computation llnl gov casc uncertainty quantification as the best fit to integrate with mase bdi based on its smooth coupling with external models and variability and availability of ua and sa methods psuade is a software package composed of three main components a sample generator with the experimental design techniques a driver to control the simulator execution environment and an analysis toolset tong 2005 the execution environment created by psuade allows sequential or parallel automatic simulation executions we stylized the use of psuade by creating a python driver to provide an interface for linking mase bdi simulation executable code and psuade also we created a graphical user interface gui that clusters all psuade and mase bdi configurations in a straightforward unified interface that encapsulates all the configuration complexity of both psuade and mase bdi users may edit the configurations of the model or the uq analysis without having to handle directly the configuration files fig 5 shows the flow of activities for mase bdi to work autonomously with the psuade tool beginning with the configuration of the simulation and the uq design of experiments following through the generation of samples in psuade that are going to be the input of the multiple mase bdi simulations all the mase outputs are stored and compiled so the ua and sa chosen techniques would be applied the uq integration modules were designed to be model framework independent so that it can be coupled with psuade in any other model and platforms other than mase bdi the codes of the implementation 3 3 https gitlab com infoknow mase mase bdi sourcecode tree master mase psuade are available to the research community 3 4 experimental setup the application of the uq workflow follows a sequence of steps that were presented in general terms in section 2 5 next we describe the individual choices and methods used in a specific abm application the lucc model simulated in mase bdi we will present the choices we made at each step and maybe help other modelers with our example 3 4 1 define the quantity of interest qoi the first task of the experimental setup the definition of the qoi was determined as the output fom as described in section 3 2 fom was chosen as the qoi of our investigation as it represents the quality of our simulation predictions the higher the fom the better fitted is the prediction 3 4 2 select the input factors of interest regarding the simulation data a baseline scenario with fixed variables was selected for the lucc model to investigate the initialization parameters of mase bdi for this purpose there are no alterations in the geographic information in the simulated environment all simulations were performed with only two types of transformation agents cattle ranchers and farmers the input factors of interest refer to the number of agents initialized in a simulation their initial state and their behavior these parameters characterize the instantiation of mase bdi agents and therefore users may lack familiarity with those variables the mase bdi provides a default value for the simulations obtained through the calibration of the model therefore these parameters are often a black box to users and precisely because of this can be an extra source of uncertainty the number of transformation agents ta is a parameter that reflects the number of computational agents in the multi agent system paradigm instantiated in a simulation run in this case study one agent does not represent one single individual ta was derived from data of the brazilian agricultural census of 2006 and comprises a set of producer legal status the range of 1 100 is a percentage representation to the 3407 registered producers in the region the mase bdi user must inform how many agents may be active or inactive in a given period the details of those agent s characterization are thoroughly illustrated in ralha et al 2013 likewise the number of transformation group agents tg is an initial parameter which represents not an individual but an organization cooperative business and so on the range is an abstraction of the 548 group producers ten of which have permanent exploration licenses the potential for exploration individual or of a group represents the impact an agent can produce in the natural vegetation cover of a cell during a step in the cerrado lucc model considering the deforestation process the potential of exploration is again an abstraction for the wood volume per hectare m 3 h a 1 of wood that can be obtained from a particular grid cell until a nominal limit that represents resource depletion the parameters of table 2 will be the input for the uq process 3 4 3 specify the range of the input to illustrate the third task of the experimental setup step table 2 presents the four parameters that will vary in each run of the simulation they were the selected input factors of interest and the specification of the range of the input is presented in table 2 in addition to the final lucc maps a mase bdi simulation generates 11 metrics as results to evaluate the model response to the different parameters fom will be used as the objective function and the output to be analyzed in the uq process nevertheless another five variables were selected to observe the influence of the simulation input configurations on the model outputs the experiments considered the outputs described in table 3 3 4 4 variance stability determination the last task in the experimental setup step of the uq process is to define the minimum sample size through the determination of variance stability from a pool of over 138 800 model runs that were executed 31 815 runs represent the baseline scenario where only the four input variables vary factor fixing of inputs presented in table 2 the s and the μ for this fixed parameter set are already substantially smaller equation 1 we sampled from this fixed set to apply the variance stability methodology proposed by lorscheid et al 2012 in this multivariate setting we compared the c v rounded to 1 1000 of differently sized set of runs increased iteratively n 10 50 100 500 800 1000 5000 10000 the outcome drawn from runs of different sample techniques may affect variance stability for clarification we applied the proposed methodology with random monte carlo table 4 and quasi random sampling table 5 we selected e 0 01 as the limit of c v although both means for fom were roughly the same mc fom μ 50 59 qr fom μ 50 57 the minimum number of runs were somewhat different for almost every output for each outcome of interest fom pa ua wc rc wp the respective point of stability was 5000 50 50 500 5000 100 applying random sampling table 4 and 800 800 50 500 800 800 applying quasi random sampling table 5 the highlighted values italic on tables 4 and 5 are the c v that fall below the defined e therefore the minimum number of runs for the cerrado lucc model would be 5000 mc random samples or 800 qr samples since we are looking for efficiency 800 will be considered the minimum sample size number of runs 3 5 the methods for ua in the second step of the uq workflow there are three tasks the first one to choose a sampling strategy derives from the findings of the variance stability task we chose the quasi random sampling design since it was more effective in the definition of a minimum sample size the second task of the ua step is to run multiple simulations of the model under study again we used the findings of the experimental setup step as the minimum sample size therefore 800 simulation runs were performed the third task is the quantification of variability in qoi we performed descriptive statistics and statistics of dispersion of the outcomes to draw some ua conclusions for the second step of the uq workflow we will present the results only for the qoi the fom output first four initial moments of the sample are derived the first moment μ 50 57 standard error of μ 0 16 summarizing the central tendency of the stochastic model the second moment variance the third moment skewness and the fourth moment kurtosis the results are summarized in table 6 also the data set has σ 4 62 to explore the variability of the simulation results we performed ua by examining the observed distribution of the fom of the sample resulting simulations fig 6 summarizes the empirical density and the cumulative distribution function of the experiment 800 model runs a cullen and frey graph a squared skewness kurtosis plot is presented to illustrate whether the fom followed a particular distribution the data was bootstrapped to consider the uncertainty of the estimated values of kurtosis and skewness fig 7 is a plot with 1000 boot values the diagram indicates that the skewness and kurtosis are consistent with a beta theoretical distribution but the interval of fom not in the interval 0 1 disproves it the data does not necessarily follow any particular distribution which means that the normality assumption and other known distributions do not refer to the observed data rather the assumption is that the process that produces the data is a distributed process so that process likewise can never be precisely normal because of asymmetries discreteness and boundness of the observable data 3 6 sa experimental setup for the last step of our proposed uq workflow multiple combinations of different sample strategies and sensitivity methods were tested to answer our research questions section 1 regarding sa instead of arbitrarily choosing an sa method task 1 choose the sampling based sa method and the sampling strategy task 2 choose the sampling strategy we decided to test multiple combinations of techniques the configuration of the experiments is presented in table 7 following a similar experimental design of what was proposed by fonoberova et al 2013 and followed by gan et al 2014 we established the minimum quasi random sample size of n 800 runs as a guideline for the other sampling techniques the differences among the sample size in table 7 were due to the requisites of each sampling technique the sample size for mc metis and lh was assigned as 800 since there are no prerequisites for these techniques the sample size of oa was set to 841 1 29 2 for moat and sobol 160 and 140 replications were used resulting in samples of size 800 and 840 respectively for the fast technique the maximum harmonic is m s 6 and the maximum frequency ω m a x 41 when n 4 thus the maximum size of the fast sample for four inputs is 493 we decided to keep the fast sample experiment even though it disregards the variance stability calculation as an open question of the experiment to avoid an ad hoc definition on the sample size we applied the same method presented in section 3 4 by fixing all input parameters and choosing an e 0 001 a quasi random sample of 50 runs was determined as sufficient to qualify the model results for this given set of parameters the next tasks of the sa step are to obtain input s relative importance to check model behavior and to assess convergence those are presented and discussed in the following section 4 output analysis results and discussion to continue to execute the following tasks of our sa step we must perform many tests and simulation the global sa of all model outputs was performed using the mase driver psuade integration the primary data obtained from the execution of each of the simulations are available for checking reviewing and replicating the experiments 4 4 simulation results and uq raw data https gitlab com infoknow mase mase bdi sourcecode tree master psuade 2520raw 2520data 4 1 input s relative importance the method of global gradient sa is presented in fig 8 results from both methods of linear regression based sa are presented in fig 9 response surface sa methods are presented in fig 10 the sensitivity scores represent the first order indices i e the contribution to the output variance by every single input alone if the parameters are normalized 0 1 then the most sensitive parameters get a score next to 1 while the least sensitive ones get a score next to 0 the vertical axis in these figures denotes the mase bdi input parameters used in the experiments the simulations were performed according to the experiment design table 7 the color scale of each grid indicates the order of sensitivity from low to high that is light colors for low data values and dark colors for high data values fig 11 presents the compilation of all qualitative sa methods regarding one single output fom fom was chosen as the qoi of our investigation as presented in section 3 4 the results of the variance based quantitative sa methods for the fom output are summarized in table 8 to address the minimum sample size to detect the most sensitive variables efficiently sa was calculated at different sample sizes for each sa method we illustrate the application of mars sa technique exclusively for the fom output with different sampling methods and sampling sizes as presented in fig 12 the final result for minimum sample sizes and sampling methods are compiled in table 9 4 2 check model behavior and assess convergence the application of ua and sa offers a valuable complement to each other and their close relation in abms has been proven by fonoberova et al 2013 ligmann zielinska et al oct 2014 pianosi et al 2016 since the cerrado lucc model is stochastic there is intrinsic uncertainty in the model even when all model parameters are fixed one of the main concerns of our work was to find the minimum number of model evaluations i e the number of simulation runs that were required to secure the stability of output variance we chose to apply the methodology brought by lorscheid et al 2012 and discussed by lee et al 2015 regarding the minimum number of runs in mase bdi the found problem specific point of stability was 800 this result stays in the middle of the typical find in the literature for a small number of inputs the gan et al 2014 analysis is based in the 10 n rule where n number of input factor subject to sa pianosi et al 2016 argue that the number of runs depends on the sa purpose that should be around 1 to 1000 n when the purpose is screening the parameters through variance based methods the theoretical minimum number of runs should be 1000 n from the results it is clear that some statistical estimation must be done before arbitrarily choosing a sample size and calculating descriptive and dispersion statistics to neglect this previous analysis may lead to statistical pitfalls such as results too uncertain to be reliable some other customary approach to determine minimum sample size may presuppose normality and therefore its efficiency becomes sensitive to the shape of the distribution this assumption is particularly relevant for the reason that abms and most real data often don t conform to parametric distributions moreover as sample size increases any theoretical distribution would likely be rejected another interesting discovery found was that the definition of a sampling technique might alter the minimum sample size required to reach variance stability the most common sampling approach involves a ua that summarizes the results of monte carlo simulation based on simple random sampling we investigated one other scenario with quasi random sampling and found that for our particular case the minimum sample size using random sampling is larger than the minimum found using a quasi random sampling design similar findings were described in other areas of application such as financial models peter a acworth 1998 and statistical circuit analysis singhee and rutenbar 2010 these results are in sync with the current trend of the use of quasi random sampling in abm ligmann zielinska et al oct 2014 saltelli et al 2008 as it generates samples more uniformly over the parameter space notwithstanding in our investigation of sa techniques we decided to test a broader combination of sampling techniques and sensitivity methods this exercise is another guideline to be regarded since there are sampling methods that best fit some sa methods and others that are inefficient or inappropriate the design of the sa experiments must consider it to avoid perfunctory sa very distinct results arise from the comparison of different sa methods in the cerrado lucc model not every method was able to identify the most sensitive parameters such as the linear regression based techniques spea and src and the response surface technique dt for the most part every other technique identified ta table 2 as the most critical parameter for all outputs therefore answering the initial question of which parameters are responsible for the most of the results variability almost every technique also identified ge input parameter 4 as an important parameter to most of the outputs the most significant influence of ge is on the producers accuracy and in the pixel wrong change right change and wrong persistence it is also clear across the different methods that tg and ie input parameter 2 and 3 are entirely insensitive hence not essential to explain the variability in the outputs these results show a positive correlation between input and output uncertainties and present consistency of the screening results and physical interpretations since ge and ta describe the amount of land transformation in a simulation high values of these parameters will increase the model output values ge is the most sensitive parameter followed by ta to understand and to reduce uncertainty within these two variables will therefore reduce the uncertainty of the simulation as a whole ge represents the amount of land cover that is transformed by a group of human agents in a cell of the map ge is a sensitive value as it indicates the voracity and velocity of the current land exploitation which will directly affect the result of the simulation ge was found as highly sensitive in every sa method therefore this result proves that the model is coded in such a way that it behaves similarly to reality because the socio economic groups responsible for large scale cattle ranching and permanent agriculture are the principal driver of deforestation in the cerrado mcalpine et al 2009 smith et al 1998 sa is used to prove this similarity between our model and the observed drivers of change for qualitative sa methods both linear regression and gradient based sensitivity were able to identify the non significant parameters regarding the most important parameter there are some discrepancies we can highlight four findings first moat mars sot and gp got similar results for most of the outputs second spea and src presented very similar results but differ from the other methods regarding ta and ge we argue that traditional methods such as correlation and regression analysis are not suitable for nonlinear and non monotonic problems like the mase bdi model third the results from dt appear very different from that of other methods the dt evaluation metrics were not able to screen the parameters correctly fourth gp results were consistent in three of four input parameters the divergences in the importance of ge may be attributed to the gp algorithm optimal configuration but further investigation is required regarding variance based sa methods the results were robust for all methods indicating ta and ge the two parameters that explain almost all the output variation considering the fom output ta was responsible for over 57 of the output variation followed by ge that explains about 42 of the output variation both tg and ie combined are responsible for less than 1 of the variance there is a consensus among variance based results denoting that quantitative sa is more robust than qualitative sa the divergences in qualitative sa may be explained by the use of heuristics to represent the relative sensitivity of the parameters for the sa comparison the general finding on every approach is described moreover the discrepancies and similarities of the related work table 1 are also summarized moat the gradient based sa technique was able to identify the elementary effects of the inputs correctly and it seems to be ideal for screening purposes the downside is that the interaction effects are not included gan et al 2014 found similar results in a study case with three times more parameters we were able to find consistent results with the minimum number of simulation runs but lilburne and tarantola 2009 argue that the sample generation is not straightforward a blind adoption of moat may not be representative since it is not a global sa practice linear regression ten broeke et al 2016 and lilburne and tarantola 2009 agree that regression is a simple technique that can describe relationships which yield insight into model behavior the bad performance of the spea and src regression methods was also found by gan et al 2014 which may demonstrate that for these case studies the regression model does not fit well to the particular abms response surface these qualitative sa methods were very efficient to indicate the sensitive variables at a low computational cost low number of runs a discrepancy was found compared to the work of gan et al 2014 in the cerrado lucc model the dt method performed poorly while in the related work there were no such problems on the contrary gan et al 2014 discarded the use of gp because it was not able to find the sensitive parameters a situation that did not happen in our study case response surface methods are based on heuristics and maybe these heuristics are more problem specific and a general guideline of use of any particular technique should not be endorsed before scrutiny variance based the techniques with the higher computational cost were the ones with more consensus among them they were all capable of finding the most sensitive parameters and this result is corroborated by different works saltelli et al 2008 lilburne and tarantola 2009 gan et al 2014 thiele et al 2014 ten broeke et al 2016 mc and lh were the sampling methods with better efficacy for qualitative sa methods identifying the most sensitive parameters with a sample size of 200 all the quantitative sa achieved the same result with the sample size of 400 from the results we can attest that qualitative methods are more efficient i e find the sensitive parameters in fewer model evaluations the main disadvantage is that there is no consensus among the methods and in some cases the resulting importance ranking of the parameters is quite the opposite fonoberova et al 2013 argue that the use of surrogate models in abms may be an alternative to increase confidence in qualitative sa methods conversely the results of all quantitative methods were broadly the same and the methods seemed more robust they were all based on variance decomposition and were capable of computing parameter first order effects but it takes larger samples to do so quantitative methods such as sobol are indeed more accurate but at a higher computational cost e g gan et al 2014 for models with a larger number of parameters than the cerrado lucc model one must evaluate the trade off between accuracy and cost 5 conclusions we investigated the various impacts that ua and sa experimental design have on abm outputs the results show that although much of the analysis is problem specific there are known challenges that can be overcome by the use of statistical methods related work comparison illustrates general practices that should be a routine both to improve the level of confidence in results derived from abms and to promote more rational and efficient use of abms we suggest performing a specific investigation of the problem aiming to test the robustness of the results one should begin with an investigation of the number of simulation runs required to secure the stability of output variance followed by a design of experiments selection quasi random sampling it was clear that the quantity of samples has several ramifications to experimental design and the quality of the analysis these steps must be done before ua the results of ua should be explored in a global variance based qualitative sa such as sobol we also investigated the impact that sampling techniques sample sizes and sa methods may have on the model output analysis we identified the most significant and non significant parameters of the mase bdi model by applying gradient based variance based and linear regression based sa we verified that ta is the parameter responsible for most of the variability of mase bdi results although the results were similar across the different sa approaches they also showed that not any technique can be used without being tested and compared with others beforehand choice of analysis methods and sampling heavily impact model parameter sensitivities regarding abms it seems that there is no single method able to embrace all models the best fit method is still dependable on the model and the goal of the experiment ua and sa were found to be essential tools for analyzing and evaluating abms in particular in the lucc context on the cerrado lucc model other than assuring the model predictions are correct we believe those methods should be used for model corroboration to help researchers check e g if the assumptions are fragile if the inferences are robust or if the variables are overly dependent regarding this matter we implemented a comprehensive uq through the integration of mase bdi and psuade we were able to improve the cerrado lucc model factor prioritization setting to identify which factor was most deserving of further analysis or measurement and to assess the abm parameter elasticity as a future work we are interested in identifying critical or otherwise interesting regions in the space of the input factors also we search to uncover factors which interact and which may therefore generate extreme values an abm may be used for learning purposes role playing games to understand the dynamics of a process or to investigate different scenarios and configurations despite the research area the number of parameters or the size of the model there is room to apply ua and sa routinely as a part of the modeling process or even in the model s operational use it is time to make the methodology of agent based modeling more robust and the analysis of results collected with abms more scientific to this end all expressions describing the systematic and methodological analysis of the responses and behaviors of the model and the mapping between its inputs and its outputs such as robustness checking variability ua or sa are to be disseminated to the community and to be applied on a regular basis acknowledgments the authors would like to thank m c pereira for helpful discussions professor celia ghedini ralha would like to thank the brazilian national council for scientific and technological development cnpq for the research productivity grant in the computer science area pq 2 process number 303863 2015 3 appendix a parameter space exploration sampling methods provide a systematic exploration of the parameter space that guarantees the sample to have specific statistical or structural properties the purpose of these methods is to actively reduce the number of parameter sets that are considered but still choose space filling points in the design space thiele et al 2014 for a complete revision of sampling methods readers can refer to kleijnen et al 2005 saltelli et al 2008 gong et al 2015 in this manuscript the most common sampling designs are illustrated and applied in the uq process monte carlo sampling mc metropolis and ulam 1949 method is the most common class of computational techniques based on repeated random sampling to obtain n numerical approximations of a specified distribution function of an unknown probabilistic entity however larger sample sizes are required to explore the parameter space fully latin hypercube lh mckay et al 1979 is a 1 dimensionally space filling method also known as stratified sampling method without replacement when sampling a function of n variables the range of each variable is divided into p equally probable intervals with a total of p sample points therefore each sample point is the only one in each interval lh method selects sample points in the interior of the hypercube of p levels lh can capture more variability in the sample space than simple random sampling orthogonal array oa owen 1992 is a 2 dimensionally space filling method that uses a general fractional factorial design to improve lh the oa design extends to t dimensional margins the univariate stratification properties of lh that is for an n dimension p level parameter space a t strength oa sampling generates p t sample points when t n orthogonal array based latin hypercube oalh tang 1993 uses orthogonal arrays to construct latin hypercubes in other words the samples go through a stratification process to produce samples that have been both orthogonalized and stratified this sampling scheme provides more suitable designs for computer experiments and numerical integration than general lh sampling metis sampling karypis and kumar 1998 is an m directional space filling method that is a part of a set of multilevel partitioning algorithms designed for partitioning irregular graphs partitioning large meshes and computing fill reducing ordering of sparse matrices metis can partition an unstructured graph into a user specified number k of parts fourier sampling algorithm cukier et al 1973 was designed specifically for the fourier amplitude sensitivity test fast in this method the parameter space is explored periodically with interference free frequencies it takes a small number of correlated random samples from a signal and processes them efficiently to produce an approximation of the discrete fourier transform dft of the signal the minimum sample size of fast is n 2 m s ω m a x 1 where m s is the maximum harmonic in general 4 or 6 and ω m a x is the maximum frequency which is determined by the number of inputs lpτ lptau statnikov and matusov 2002 is a quasi random qr sampling method i e the samples are generated from a finite subset of low discrepancy sequence of points these samples are not random in the sense of being completely unpredictable however they are like random points in the sense that they are uniformly distributed across an n dimensional space lptau explores the parameter space using partitions of the parameter ranges on the base of two sobol extended sobol sobol 2001 saltelli 2002 is a replicated version of low discrepancy sequences quasi random sobol generates a uniform distribution in probability space a qualitatively random distribution filling previously unsampled regions of the probability function this is done with two random r n sample matrices m 0 and m n 1 therefore the total number of sample points is n 2 r morris one at a time moat morris 1991 sampling was designed specifically for moat sa and is similar to sobol the range of each parameter is divided into p 1 equal intervals next r points are generated from the n dimension p 1 orthogonal grid for each one other sample points are generated by perturbing one dimension at a time until all dimensions have been varied for only one time with a n 1 r total number of sample points appendix b uncertainty analysis the uncertainty analysis assesses a confidence bound on the output estimation by quantifying the uncertainty associated with the model response due to uncertainties in the model input to achieve these results we follow the necessary steps of ua summarized by saltelli et al 2008 1 start from a model parameter α n α σ α which reads after estimation the distribution of α is known with mean α and standard deviation σ α 2 assume that all the parameters β γ are independent of each other 3 draw a sample from the respective distributions of each parameter in other words produce a set of row vectors α j β j in a way that α 1 α 2 α n is a sample from n α σ α likewise for all parameters α 1 β 1 γ 1 α 2 β 2 γ 2 α n 1 β n 1 γ n 1 α n β n γ n 4 run the model for all vectors α j β j thereby producing a set of n values of a model output y j y 1 y 2 y n 1 y n by executing these steps it is possible to quantify the impact of input uncertainties on the model response and assess whether or not the response meets the required standards of precision although monte carlo is the most used method there are many other methods available to generate the samples and estimations required by ua some interesting ua applications and experimental design are described in the literature crosetto et al 2000 lilburne and tarantola 2009 saltelli et al 2008 and fonoberova et al 2013 the expected means and variance are quantified for each parameter additionally a histogram of the output variable can be displayed thus thoroughly describing the stochastic features of the model output the overall computational cost of ua depends basically on the cost of the model evaluations which is linked to the complexity of the model itself appendix c sensitivity analysis methods many techniques for sa have been proposed and a thorough description of the techniques can be found in saltelli et al 2008 regardless of the technique saltelli and annoni 2010 present a guideline on how to avoid perfunctory sa which we applied throughout the manuscript a brief description of the methods applied is found next the morris one of a time screening method moat morris 1991 may be regarded as a gradient based global sa as the final measure is obtained by averaging local measures the elementary effects ee it is composed of individually randomized one at a time experiments that calculate two sensitivity measures of the gradients of each parameter sampled from r local changes the mean μ assesses the overall influence of the factor on the output the standard deviation σ estimates the ensemble of the factors effects whether nonlinear or due to interactions with other factors ee provides the information that the effects for a given parameter may be i negligible ii linear and additive or iii nonlinear or involved with interactions with other factors moat can be much faster than other variance based sa techniques we assessed three variance based sa techniques sobol sobol 1993 fast cukier et al 1973 and mckay 1995 in general they have higher computational cost than qualitative sa but some exciting features to abms are that variance based sa measures are model independent and provide the investigation of interaction effects the first order index represents the main effect contribution of each input factor to the variance output the total effect of a variable would be the total contribution to the output variation that is its first order effect plus all higher order effects due to interaction in the sobol method the variance may be attributed to a single input first order main effect or by the interaction of two or more inputs second order effect the sum of those contributions is the total effect of a parameter to decompose the variance fast varies different parameters at different frequencies and applies a fourier transformation to measure each parameter contribution mckay uses analysis of variance anova to calculate a correlation ratio that is a ratio of the variance of a parameter and the total variance of the output the significance of the parameter increases with the correlation ratio linear regression based sa decomposes the variance of the model outcomes by fitting a regression function of the input parameters to these outcomes therefore the simulation outcomes are described concerning input output relationships which can be validated using standard statistical measures such as r 2 correlation analysis ca measures the parameter sensitivity through correlations coefficients such as spearman spearman et al 1904 regression analysis ra makes the same measures using the standard regression coefficient src to estimate the result from a regression analysis that has been normalized so that the variances of the dependent and independent variables are equal to one the efficacy of this methods relies on the input output being somewhat linear or monotonic the methods sum of trees sot gaussian process gp and multivariate adaptive regression splines mars are considered response surface or surrogate models from which it is possible to obtain relative scores of the total effects of a parameter those methods provide a mapping from parameters to outputs sot breiman et al 1984 chipman et al 2010 is a tree based bayesian method a single regression tree model is obtained by the use of a recursive binary partition of the parameter space the created balanced binary tree in which the variables are split to cause the maximum decrease in the residual sum of squares has each terminal node with a minimum number of sample points the variable with the larger number of splits is considered the most sensitive one mars friedman 1991 is a non parametric regression able to model nonlinearities and interactions between parameters it is considered an extension of the tree method because after partitioning the space it builds localized regressions first and second order for each model a score generalizes cross validation is computed it will remove each parameter and recalculate the model score the larger the score the more important is the removed parameter gp is an implementation of the tpros algorithm proposed by gibbs abd mackay 1997 gp is a method for regression using gaussian process priors which allow an exact bayesian analysis using matrix manipulations the theory behind the method states that points that are close in parameter space give rise to similar response values thus it is possible to identify the influence of the parameters in the model response the dt pi and peterson 1994 is a method that establishes dependencies in continuous functions given a sequence of measurements δ an estimate of noise variance when a subset of variables in the sample are selected for regression the approach is based on calculating conditional probabilities from vector component distances it has been proved that adding unrelated variables or withdrawing related ones will increase δ hence the subset of all variables that minimize noise variance is considered the most sensitive 
26371,this paper presents an empirical study comparing different uncertainty analysis ua and sensitivity analysis sa methods focussing their usefulness for the output analysis of land use land cover change lucc agent based models abms as a result a workflow to integrate ua and sa is presented to evaluate abms outputs we developed a baseline scenario and performed a comprehensive investigation of the impacts that differences in sample sizes sample techniques and sa methods may have on the model output the analysis is done in the context of a particular agent based simulator with a lucc model in a brazilian cerrado case study the experiments indicate that there are known challenges to be overcome by the use of statistical methods even though the presented analysis was done over a particular simulator we intend to contribute to the community that understands the importance of statistical validation techniques to improve the level of confidence in agent based simulation outputs keywords uncertainty analysis sensitivity analysis agent based model spatial simulation land use cover change 1 introduction as cited in the literature the land use land cover change lucc systems are dynamic stochastic and characterized by nonlinear and non monotonic relationships between constant changing entities parker et al 2003 verburg 2006 rindfuss et al 2008 besides agent based models abms have been used as a natural metaphor to model lucc dynamics since they capture emergent phenomena and provide an original description of the modeled system schreinemachers and berger 2011 murray rust et al 2013 ralha et al 2013 however abms are prone to uncertainty because they reflect the intrinsic randomness of environmental physical and social events the uncertainty may also arise because of insufficient knowledge lack of data observation errors measurements used to parametrize the model or from vague premises of the model ligmann zielinska et al oct 2014 lilburne and tarantola 2009 as a result one could argue whether there is any quality in model predictions due to high uncertainty and the considerable number of assumptions imposed by abms models in this scenario uncertainty analysis ua and sensitivity analysis sa are currently popular topics in abms as well as for many other complex systems pappenberger et al 2008 they are valuable tools in understanding lucc models and deriving decisions on strategies to reduce model uncertainty ua provides the variability of model results sa presents which factors are responsible for this variability this variability may be expressed quantitatively in terms of elasticity of performance concerning parameter levels high sensitivities elasticities give cause for concern about the reliability of a model dayananda et al 2002 a factor is any source of uncertainty in the modeling process including model structure initial conditions and input parameters using the terminology proposed by the national research council 2012 uncertainty quantification uq is the process of quantifying uncertainties in a computed quantity of interest qoi with the goals of accounting for all sources of uncertainty and quantifying the contributions of specific sources to the overall uncertainty i e ua and sa applied in tandem although ua and sa applications are rising most abms struggle with a shortage of testing in general mainly due to time and other resource constraints kelly letcher et al 2013 lee et al 2015 argue that while a modeler invests a lot of time and effort in the development of abms the output analysis is not always considered as deserving the same resource intensive attention according to a survey carried out by heath et al 2009 less than 5 of abm publications present any statistical validation techniques angus and hassani mahmooei 2015 argue that one possible cause for this methodological anarchy derives from the fact that with so many possible degrees of freedom within an abm the responsibility to ensure and to demonstrate that a model is structurally sound and the prediction is reliable falls into each modeler we present a uq workflow to integrate ua and sa in the evaluation of agent based simulation outputs we illustrate the use of this workflow in a particular spatial explicit lucc case study in the framework multi agent system for environmental simulation mase bdi we apply general practices that should be a routine to improve the level of confidence in results and to promote more rational and efficient use of abms we may cite that broader and more complete workflows for the application of sa were already proposed such as pianosi et al 2016 and norton 2015 the ua sa integrated proposal is what set our manuscript apart we argue that ua should be used as an input to sa in a broader process of uq also we noticed some conflicting results when we compared relevant studies on sa mainly regarding the experimental setup table 1 summarizes the studies found in the literature vanrolleghem et al 2015 1 gan et al 2014 2 wang et al 2013 3 yang 2011 4 pappenberger et al 2008 5 tang et al 2007 6 some authors have compared different sa methods and experimental setup which are presented in the different lines of the table table 1 illustrates a glimpse of the myriad of possible combinations of strategies for sampling the model parameter space and sa methods to quantify the impacts of sampled parameters on the model qoi we understand that there is no combination of sampling and sa method that fits all applications thus the work of gan et al 2014 shows that different sample strategies can even produce different outputs regarding the same sa method also it seems that there isn t a clear relationship between the number of factors and the number of necessary runs to compute sa furthermore in some cases the number of runs used in the same sampling and sa method is not even in the same order of magnitude for example pianosi et al 2016 recommend 1000 m model runs to calculate variance based sa such as fast where m is the number of input factors subject to sa neither wang et al 2013 nor vanrolleghem et al 2015 nor gan et al 2014 executed this many number of runs the first used a sample of size 2049 for a 47 factor problem instead of 47 000 while the second used a sample size of 3000 for a 17 factor problem instead of 17 000 the third used a sample size of 2777 for a 13 factor problem instead of 13 000 one could ask whether the number of runs should be based on something more than m in this manuscript we will test different experimental strategies for a uq workflow and discuss their relative benefits and limitations a baseline scenario was developed and we performed a comprehensive investigation of the impacts that differences in sample sizes sample techniques and sa methods may have on the qoi in this work we address the research question how ua and sa may be applied to improve users understanding of the uncertainty and relations among input and output responses in lucc agent based simulations we are interested in finding which parameters are responsible for most of the results variability if there is convergence when different sa techniques are applied and finally if there is a minimum sample size to achieve it although the statistical techniques are applied in a specific agent based simulator the methods described are quite general and may illustrate their application in another research in section 2 we provide an overview of the different methods regarding variance stability parameter space exploration ua and sa we also present the proposed uq workflow in section 2 in section 3 we describe the mase bdi framework and lucc model used as a case study followed by the experimental design we present the results compared to related work we discuss challenges and provide some assessment to extrapolate our finding into more general conclusions to produce more robust or parsimonious models as well as to make models more defensible in the face of scientific or technical controversy section 4 finally in section 5 we summarize our findings and outline future work 2 materials and methods the methods we applied in the case study are presented in this section alongside their experimental design the uq experiments have the objective to perform an output analysis on spatial stochastic models to measure uncertainty and to reduce it ultimately we want to understand better how the model behaves and expand our confidence in the response of a lucc model 2 1 variance stability agent based simulations are often stochastic and therefore any analytical exercise requires an outcome pool drawn from a sufficient number of samples it is only possible to draw conclusions if the output mean and variance reaches relative stability otherwise the statistics could harbor too much uncertainty to be reliable lee et al 2015 moreover some abm simulations mase bdi included can take longer run times which makes the execution of large samples prohibitive hence knowing the minimum sample size to reach variance stability can be more compelling to modelers there are many methods to assess variance stability law and kelton 2000 lee et al 2015 we chose to apply the method proposed by lorscheid et al 2012 whose strategy is to assess stability from metrics on an outcome for a sequence of sample sizes the proposed metric relies on the functional ratio between the variance and the sampled mean the coefficient of variation c v is a dimensionless and normalized metric used to measure the uncertainty surrounding the variance i e used for the analysis of experimental error variance it is defined as the ratio of the standard deviation of a number of measurement s to the arithmetic mean μ 1 c v s μ if c v is obtained from a small sample it will vary more than if each sample contained far more runs lorscheid et al 2012 propose a fixed epsilon e to limit c v this is done by calculating the c v s of a different sized set of simulation runs in ascending order of size the sample size at which the difference between consecutive c v s falls below the determined criterion e and remains so is considered a minimum sample size or the minimum number of simulation runs for abms this is the point of variance stability these points should be obtained for all abm outputs thereby the minimum number of runs for the abms is the maximum of these points lee et al 2015 n m i n a r g m a x n c v x n c v x m e x and m n where n is the sample size n m i n is the estimated minimum number of required simulation runs x is a distinct output and m is some sample size for which the c v is calculated thus we apply the lorscheid et al 2012 method to establish the minimum sample size that guarantees that variance stability is achieved 2 2 parameter space exploration sampling methods provide a systematic exploration of the parameter space that guarantees the sample to have specific statistical or structural properties the purpose of these methods is to actively reduce the number of parameter sets that are considered but still chose space filling points in the design space thiele et al 2014 for a complete revision of sampling methods readers can refer to kleijnen et al 2005 saltelli et al 2008 and gong et al 2015 in this manuscript the most common sampling designs are illustrated and applied in the uq process since there are many methods to explore the parameter space readers may have an overview of those sampling methods in appendix a including monte carlo sampling mc latin hypercube lh orthogonal array oa orthogonal array based latin hypercube oalh metis sampling fourier sampling algorithm lpτ lptau sobol extended sobol morris one at a time moat 2 3 uncertainty analysis ua evaluates and quantifies how the variability of input factors propagates through the model and affects the variability of output values ligmann zielinska et al oct 2014 ua can also answer if there are any discontinuities associated with the distribution of results iman and helton 1988 plot the distribution itself calculate the average output the standard deviation the quantiles of its distribution and confidence bounds an overview of the ua process can be found in appendix b for the proposed ua the only parameters considered relevant are the ones related to the qoi and previously selected as input factors of interest all the other model factors and information fed into the model are disregarded i e they do not vary thereby they cannot cause variation in the output however the model outputs y j are non deterministic because of the stochastic component derived from the emergence of the agent s behavior therefore to ensure robustness each vector α j β j of the output must be evaluated regarding the mean and the variance dosi et al 2017 this confirmation is executed by a given number of model runs but with the same parameters configuration ten broeke et al 2016 after the ua quantified the magnitude of the resulting uncertainty in the model predictions due to uncertainties in model inputs the next step in the uq workflow would be to perform sa 2 4 sensitivity analysis sa is the study of how uncertainty in the output of a model can be apportioned to different sources of uncertainty in the model input saltelli et al 2008 the authors show that each measure of sensitivity may produce its ranking of factors by importance there are different methods of sa and each one has advantages and limitations in the particular case of sa in spatial models we incorporated the general guidelines provided by lilburne and tarantola 2009 it is clear from their work that each sa method has sampling and pre processing technique requisites therefore a careless combination of methods will result in inefficient and inappropriate results also not all of the methods are capable of providing sensitivity index for non monotonic input output dependencies typically observed in abms fonoberova et al 2013 ten broeke et al 2016 therefore we selected ten well known methods of qualitative and quantitative sa they were applied in mase bdi to verify if they were capable of providing those indexes for the lucc model in general gradient and linear regression based sa are known as qualitative methods since they use some heuristic to represent the relative sensitivity of the parameters we will assess the morris one of a time screening method moat morris 1991 and some correlation analysis such as spearman spea spearman et al 1904 and the standard regression coefficient src variance based methods are classified as quantitative methods because they tell the sensitivity of a parameter by calculating the impact of this parameter on the total variance of the model outputs saltelli et al 2004 we will assess three variance based sa techniques sobol sobol 1993 fast cukier et al 1973 and mckay 1995 also we compare response surface methods such as sum of trees sot breiman et al 1984 chipman et al 2010 multivariate adaptive regression splines mars friedman 1991 and gaussian process gp gibbs and mackay 1997 other screening methods such as the delta δ test dt pi and peterson 1994 are also assessed the overall mechanisms of each method are discussed in appendix c the implementations of each technique are not provided due to space constraint but readers may refer to tong 2005 gan et al 2014 for details 2 5 ua sa integrated workflow the integration of ua sa has been applied to abms in a few relevant studies ligmann zielinska et al oct 2014 fonoberova et al 2013 parry et al 2013 ligmann zielinska and sun 2016 that argue that a systematic evaluation of abms must comprise of an integrated approach to quantification of model output variability and its sensitivity to inputs hamilton et al 2015 argue that integrated assessmentpresent an opportunity to synthesize diverse knowledge data methods and perspectives into an overarching framework to address complex environmental problems we followed the terminology of the national research council 2012 and called this process uq the process of quantifying uncertainties associated with a model qoi to account for all sources of uncertainty ua and quantifying the contributions of specific sources to the overall uncertainty sa fig 1 presents an overview of the uq integrated workflow with ua and sa as part of the modeling process adapted from the original one proposed by ligmann zielinska et al oct 2014 analyzing the workflow we argue that ua should be used as an input to sa in a broader process of uq abm input factors are often diverse and the stochasticity makes multiple model runs a paramount step of the abm s output evaluation once the modeler defines what is the qoi to be investigated ua should be incorporated in the modeling process to indicate what is the variability of the qoi outcomes the next step would be to test the sensitivity of model response to changes in the factors this discovery could identify interactions among factors factor fixing and prioritization that could lead to a model simplification the reduction of output variance or the improvement of model accuracy this larger uq process involves many smaller tasks so a more detailed workflow is presented in fig 2 pianosi et al 2016 proposed a practical workflow for the application of sa with four fundamental group of activities i experimental setup ii input sampling iii model evaluation and iv post processing this work presents a state of the art review and a very concise guide to good practices for readers however abms have specific characteristics mainly due to stochasticity uncertainties equifinality and because of the complex system applications we took pianosi et al 2016 work as a guideline and tailored the level of effort and estimation to fit abm needs the main difference is the simplification of the sa tasks and the incorporation of the ua tasks because a portion of abm uncertainty is irreducible a comprehensive evaluation of abm uncertainty should assume that code verification model parameter calibration and validation have been successfully accomplished before uq process begins for a qoi the uq workflow for abms fig 2 is composed of three basic steps experimental setup ua and sa we maintained the terminology proposed by pianosi et al 2016 as in fig 2 the first step of the workflow regards the experimental setup with basic choices i defining the qoi the modeler must specify what the qoi for the problem at hand is ii select the input factors of interest and iii specify the range or distribution probability of each factor the fourth task iv is to determine variance stability which represents the minimum number of simulation runs that accurately report the descriptive statistics in the second step of the workflow the ua is composed of three tasks that summarize what is needed to discover what is the variability of the qoi in an abm after choosing the sampling strategy the modeler would run multiple simulations the minimum number of runs is provided by the last task of the experimental setup define variance stability abm modelers usually choose factor values randomly from their respective range distribution as a result ua produces a distribution of the qoi the last task is to use this distribution to quantify the variability of the qoi i e the use of descriptive statistics to analyze the model outputs the third step is a simplification of pianosi et al 2016 original workflow it all begins with the selection of the sa method although the original work proposed a classification system based on the sa purpose the literature shows that for abms this choice is somewhat model specific we decided to leave this decision to the modeler and tested many different methods to see the impacts of the sa method in our case study the next task would be to define the input variability space by choosing the sampling strategy to be applied there are several sampling methods and although mc is still the most used sampling strategy we tested different combinations of well known techniques such as mc and sobol and also tailored sampling strategies to see if there would be an impact on the sensitivities outcomes the number of model runs required to perform sa is usually a rough estimation of a function of the number of factors subject to sa we postulate that this minimum sample size should be equal or larger to the variance stability number of runs defined in the experimental setup step we also test this empiric assumption and discuss it in later sections sections 3 4 and 4 this is what is necessary to obtain the factor s relative importance the workflow s last two steps are checkpoints defined by pianosi et al 2016 to evaluate the model check model behavior and to assess convergence check whether sensitivity estimates are independent of the size of the sample and if they would take similar values if we used independent samples these steps inform us about the reliability of the results we applied the presented uq workflow for abms in a case study we tested several combinations of methods sampling strategies and sample sizes in section 3 we will present the application and the experimental setup designed for this application 3 a land use case study mase 1 1 project website http mase cic unb br software availability https gitlab com infoknow mase mase bdi sourcecode is an agent based simulation tool developed at the university of brasília brazil mase enables modeling and simulations of lucc dynamics using a configurable model and both top down and bottom up grimm 1999 model structures simultaneously mase enables multiple types of agents with different behaviors to represent the interaction between agents with autonomy the physical environment and its relations ralha and abreu 2017 mase has the overall goal of performing medium to long term lucc predictions it also allows assisting decision making processes related to lucc we run the experiments in mase bdi which is a freeware software extension of mase that introduces cognitive reasoning oriented agents through the implementation of the bdi rationality bratman 1987 mase bdi was implemented in jadex multi agent platform braubach et al 2005 in the bdi model agents have beliefs a set of information that about the world it inhabits that changes both the perception and thinking about the world desires represent the motivational attitudes of agents capturing the agent s wishes and driving the course of its actions an agent can also make plans related to its intention to achieve its goals this multi agent reasoning model is defined as means end reasoning wooldridge 2009 the mase bdi architecture is composed of three layers from top to bottom a user interface a utility layer and an agent layer the first provides an optional graphical interface models and simulation parameters can also be defined directly in a configuration file and a jadex control center of the bdi model the utility layer groups a set of modules to control the pre processing of the maps and input of the geographic information it also provides the simulation parameter automatic tuning which is a complex and error prone task in abms the parameter adjustment is performed by employing efficient optimization algorithms to tune the simulation model parameters concerning a user defined single or multi objective function of interest still in the utility layer a module of validation is responsible for evaluating the final simulation output maps and metrics coelho et al 2016 in the agent layer we have an organization of hierarchical agents the grid manager controls the general aspects of the simulation the spatial manager controls the agents responsible for representing and updating the spatial environment the transformation agents are computational entities accountable for moving exploring and reasoning about the space according to their internal goals and beliefs the transformation manager rules and resolves the conflict due to the competition among transformation agents concurring for the same environmental resources readers who are interested in details of the mase bdi architecture agent design and implementation may refer to coelho et al 2016 3 1 the cerrado federal district study area the federal district of brazil 5 789 k m 2 and its cerrado brazilian savanna coverage is the study area in this article the simulations depict the land changes of the region fig 3 the most endangered biome in brazil and the second largest biome in south america harboring significant biodiversity this area has been undergoing severe transformation due to the advance of cattle ranching and soy production being an attractive study area for land use simulations to allow replicability the cerrado lucc simulation model was documented and described using the odd protocol overview design concepts and details protocol grimm et al 2006 the characterization of agent behaviors and attributes in socio ecological systems were applied by empirically grounding abm mechanisms smajgl et al 2011 a complete conceptual and methodological description of the model is available in ralha et al 2013 the initialization data for the simulation is a couple of landsat derived grid raster maps consisting of the land cover of the region from two different time periods an initial and a final map furthermore the user must adjust a set of initialization parameters of the multi agent system such as the number of agents that will explore the landscape transformation agents their typology cattle ranchers and farmers and characteristics of the initial behavior of those agents the simulations are performed in steps where each step corresponds to the measure of time defined by the user in this example one step equals to one week in chronological time the user also determines the size of a plot or cell here the total area of study was divided into plots of one hectare the physical environment is spatially represented by a set of layers of geographical information data shapes or raster files such as rivers lakes slopes building areas highways environmental protected areas and regional zoning maps of the area the aggregation of these geographical features determines the physical environment of any given point in the simulation grid the transformation agents represent humans performing activities of cattle ranchers and farmers with their behavior and beliefs explicitly changing the natural landscape to achieve their internal goals e g production expansion sustainable exploration the simulations are calibrated by the simulation parameter automatic tuning tool adjusting the parameters to best fit the observed change from the two initial maps the outcome of the simulation is a result of the emergence of the agent s action within the duration of a simulation determined by the user the final landscape is a result of the emergence of the agent s effects on the land mase bdi is a spatially explicit framework because the results comprise of the quantity of land cover change and the spatial allocation of the change which plots were chosen by the agents to initiate or expand their cattle ranching or farming business the result of any mase bdi simulation is a couple of predicted maps with the spatial allocation of the land change and the quantity of change a set of metrics calculated during runtime such as the total amount of land change at the end of each simulation the resulting image is submitted to a goodness of fit measurement and the quality and errors of the quantity of change and allocation of land use change are calculated mase bdi produces stochastic simulations which mean that the same input to the model may lead to a different result in the quantity and allocation of change therefore the same set of parameters must be run several times to raise the confidence that the results are representative 3 2 lucc goodness of fit according to thiele et al 2014 there are two strategies for fitting model parameters to observational data best fit and categorical calibration mase bdi applies the first strategy in which we must find the parameter combination that best fit the data the quality measure is one exact value obtained from the observational data so it is easy to determine which parameter set leads to the lowest difference pontius et al 2008 define the most common quality measure for lucc spatial explicit simulations hence it is used in mase bdi although there is not a universally agreed upon criterion to evaluate the goodness of fit of validation maps the performance of the simulation model is done objectively by computing the sources of error of prediction maps a set of map comparisons is responsible for the evaluation of the model pontius et al 2004 indicate that three maps are necessary i a reference map of the initial time t 0 ii a reference map of a subsequent time t 1 and iii a prediction map of the subsequent time t 1 there are three possible two map comparisons picking two maps at a time comparison between the reference map of time t 0 and the reference map of time t 1 characterizes the observed change in the maps which reflects the dynamics of the landscape comparison between the reference map of time t 0 and the prediction map of time t 1 characterizes the model s predicted change which reflects the behavior of the model comparison between the reference map of time t 1 and the prediction map of time t 1 characterizes the accuracy error of the prediction s accuracy error the total disagreement between any two maps that share a categorical variable is computed in terms of quantity disagreement and location disagreement pontius et al 2004 quantity disagreement derives from differences between the maps regarding the number of pixels for each category location disagreement is the difference that could be resolved by rearranging the pixels spatially within one map so that its agreement with the other map is as broad as possible the sum of them both is the total disagreement to illustrate the methodology we present the brazilian federal district map with only two land cover categories natural vegetation the cerrado and developed areas characterized by 30 or greater of constructed materials e g asphalt concrete buildings considering these two categories the comparison of pixels may result in the categories presented in fig 4 error due to observed vegetation predicted as developed correct due to observed developed predicted as developed correct due to observed vegetation predicted as vegetation and error due to observed developed predicted as vegetation according to pontius et al 2008 the most accurate applications are the ones where the amount of observed net change in the reference maps is larger the figure of merit fom is the ratio of the amount of correctly predicted pixels of change to the sum of all pixels f o m r i g h t c h a n g e w r o n g p e r s i s t e n c e r i g h t c h a n g e w r o n g g a i n i n g w r o n g c h a n g e where wrong persistence is the area of error due to observed change predicted as persistence right change is the area of correct due to observed change predicted as change wrong gaining is the area of error due to observed change predicted as wrong gaining category and wrong change is the area of error due to observed persistence predicted as change fom is a statistical measurement that can range from 0 meaning no overlap between observed and predicted change to 100 meaning perfect overlap between observed and predicted change when the amount of correctly predicted change is larger than the sum of the various types of error fom is greater than 50 fom is the best fit quality measure of this manuscript it is also the qoi chosen to illustrate the uq workflow for abm as the first task of the experimental setup step it is worth mentioning that pontius et al 2008 set a testing benchmark based on statistical methods for map comparison of 13 applications of different popular peer reviewed land change models the results show that in 12 of the 13 lucc models predictive maps the amount of error is more significant than the amount of correctly predicted change at the resolution of raw data in contrast mase bdi was able to surpass these statistics presenting results that show high quality in the accuracy of their predictions fom 50 the complete explanation of the mase simulation results using pontius statistical techniques of map comparison to land change models is presented in ralha et al 2013 3 3 mase bdi and uq tool integration previous work demonstrates that the initialization of the agents may have a substantial effect on the land dynamics and into the final simulation outcome lorscheid et al 2012 therefore it was paramount to use a framework to control calculate trace manage uncertainties and finally make the output analysis feasible the mase bdi framework itself does not provide the modeler with the means to statistically analyze the results the difficulty to perform many different samplings ua and sa analysis may lead to a shortage of testing and finally to a perfunctory uq to avoid this pitfall we chose a statistical platform that provided the tools needed to execute both ua and sa steps in the proposed uq workflow for abm fig 2 among the different uq platforms available we chose psuade 2 2 http computation llnl gov casc uncertainty quantification as the best fit to integrate with mase bdi based on its smooth coupling with external models and variability and availability of ua and sa methods psuade is a software package composed of three main components a sample generator with the experimental design techniques a driver to control the simulator execution environment and an analysis toolset tong 2005 the execution environment created by psuade allows sequential or parallel automatic simulation executions we stylized the use of psuade by creating a python driver to provide an interface for linking mase bdi simulation executable code and psuade also we created a graphical user interface gui that clusters all psuade and mase bdi configurations in a straightforward unified interface that encapsulates all the configuration complexity of both psuade and mase bdi users may edit the configurations of the model or the uq analysis without having to handle directly the configuration files fig 5 shows the flow of activities for mase bdi to work autonomously with the psuade tool beginning with the configuration of the simulation and the uq design of experiments following through the generation of samples in psuade that are going to be the input of the multiple mase bdi simulations all the mase outputs are stored and compiled so the ua and sa chosen techniques would be applied the uq integration modules were designed to be model framework independent so that it can be coupled with psuade in any other model and platforms other than mase bdi the codes of the implementation 3 3 https gitlab com infoknow mase mase bdi sourcecode tree master mase psuade are available to the research community 3 4 experimental setup the application of the uq workflow follows a sequence of steps that were presented in general terms in section 2 5 next we describe the individual choices and methods used in a specific abm application the lucc model simulated in mase bdi we will present the choices we made at each step and maybe help other modelers with our example 3 4 1 define the quantity of interest qoi the first task of the experimental setup the definition of the qoi was determined as the output fom as described in section 3 2 fom was chosen as the qoi of our investigation as it represents the quality of our simulation predictions the higher the fom the better fitted is the prediction 3 4 2 select the input factors of interest regarding the simulation data a baseline scenario with fixed variables was selected for the lucc model to investigate the initialization parameters of mase bdi for this purpose there are no alterations in the geographic information in the simulated environment all simulations were performed with only two types of transformation agents cattle ranchers and farmers the input factors of interest refer to the number of agents initialized in a simulation their initial state and their behavior these parameters characterize the instantiation of mase bdi agents and therefore users may lack familiarity with those variables the mase bdi provides a default value for the simulations obtained through the calibration of the model therefore these parameters are often a black box to users and precisely because of this can be an extra source of uncertainty the number of transformation agents ta is a parameter that reflects the number of computational agents in the multi agent system paradigm instantiated in a simulation run in this case study one agent does not represent one single individual ta was derived from data of the brazilian agricultural census of 2006 and comprises a set of producer legal status the range of 1 100 is a percentage representation to the 3407 registered producers in the region the mase bdi user must inform how many agents may be active or inactive in a given period the details of those agent s characterization are thoroughly illustrated in ralha et al 2013 likewise the number of transformation group agents tg is an initial parameter which represents not an individual but an organization cooperative business and so on the range is an abstraction of the 548 group producers ten of which have permanent exploration licenses the potential for exploration individual or of a group represents the impact an agent can produce in the natural vegetation cover of a cell during a step in the cerrado lucc model considering the deforestation process the potential of exploration is again an abstraction for the wood volume per hectare m 3 h a 1 of wood that can be obtained from a particular grid cell until a nominal limit that represents resource depletion the parameters of table 2 will be the input for the uq process 3 4 3 specify the range of the input to illustrate the third task of the experimental setup step table 2 presents the four parameters that will vary in each run of the simulation they were the selected input factors of interest and the specification of the range of the input is presented in table 2 in addition to the final lucc maps a mase bdi simulation generates 11 metrics as results to evaluate the model response to the different parameters fom will be used as the objective function and the output to be analyzed in the uq process nevertheless another five variables were selected to observe the influence of the simulation input configurations on the model outputs the experiments considered the outputs described in table 3 3 4 4 variance stability determination the last task in the experimental setup step of the uq process is to define the minimum sample size through the determination of variance stability from a pool of over 138 800 model runs that were executed 31 815 runs represent the baseline scenario where only the four input variables vary factor fixing of inputs presented in table 2 the s and the μ for this fixed parameter set are already substantially smaller equation 1 we sampled from this fixed set to apply the variance stability methodology proposed by lorscheid et al 2012 in this multivariate setting we compared the c v rounded to 1 1000 of differently sized set of runs increased iteratively n 10 50 100 500 800 1000 5000 10000 the outcome drawn from runs of different sample techniques may affect variance stability for clarification we applied the proposed methodology with random monte carlo table 4 and quasi random sampling table 5 we selected e 0 01 as the limit of c v although both means for fom were roughly the same mc fom μ 50 59 qr fom μ 50 57 the minimum number of runs were somewhat different for almost every output for each outcome of interest fom pa ua wc rc wp the respective point of stability was 5000 50 50 500 5000 100 applying random sampling table 4 and 800 800 50 500 800 800 applying quasi random sampling table 5 the highlighted values italic on tables 4 and 5 are the c v that fall below the defined e therefore the minimum number of runs for the cerrado lucc model would be 5000 mc random samples or 800 qr samples since we are looking for efficiency 800 will be considered the minimum sample size number of runs 3 5 the methods for ua in the second step of the uq workflow there are three tasks the first one to choose a sampling strategy derives from the findings of the variance stability task we chose the quasi random sampling design since it was more effective in the definition of a minimum sample size the second task of the ua step is to run multiple simulations of the model under study again we used the findings of the experimental setup step as the minimum sample size therefore 800 simulation runs were performed the third task is the quantification of variability in qoi we performed descriptive statistics and statistics of dispersion of the outcomes to draw some ua conclusions for the second step of the uq workflow we will present the results only for the qoi the fom output first four initial moments of the sample are derived the first moment μ 50 57 standard error of μ 0 16 summarizing the central tendency of the stochastic model the second moment variance the third moment skewness and the fourth moment kurtosis the results are summarized in table 6 also the data set has σ 4 62 to explore the variability of the simulation results we performed ua by examining the observed distribution of the fom of the sample resulting simulations fig 6 summarizes the empirical density and the cumulative distribution function of the experiment 800 model runs a cullen and frey graph a squared skewness kurtosis plot is presented to illustrate whether the fom followed a particular distribution the data was bootstrapped to consider the uncertainty of the estimated values of kurtosis and skewness fig 7 is a plot with 1000 boot values the diagram indicates that the skewness and kurtosis are consistent with a beta theoretical distribution but the interval of fom not in the interval 0 1 disproves it the data does not necessarily follow any particular distribution which means that the normality assumption and other known distributions do not refer to the observed data rather the assumption is that the process that produces the data is a distributed process so that process likewise can never be precisely normal because of asymmetries discreteness and boundness of the observable data 3 6 sa experimental setup for the last step of our proposed uq workflow multiple combinations of different sample strategies and sensitivity methods were tested to answer our research questions section 1 regarding sa instead of arbitrarily choosing an sa method task 1 choose the sampling based sa method and the sampling strategy task 2 choose the sampling strategy we decided to test multiple combinations of techniques the configuration of the experiments is presented in table 7 following a similar experimental design of what was proposed by fonoberova et al 2013 and followed by gan et al 2014 we established the minimum quasi random sample size of n 800 runs as a guideline for the other sampling techniques the differences among the sample size in table 7 were due to the requisites of each sampling technique the sample size for mc metis and lh was assigned as 800 since there are no prerequisites for these techniques the sample size of oa was set to 841 1 29 2 for moat and sobol 160 and 140 replications were used resulting in samples of size 800 and 840 respectively for the fast technique the maximum harmonic is m s 6 and the maximum frequency ω m a x 41 when n 4 thus the maximum size of the fast sample for four inputs is 493 we decided to keep the fast sample experiment even though it disregards the variance stability calculation as an open question of the experiment to avoid an ad hoc definition on the sample size we applied the same method presented in section 3 4 by fixing all input parameters and choosing an e 0 001 a quasi random sample of 50 runs was determined as sufficient to qualify the model results for this given set of parameters the next tasks of the sa step are to obtain input s relative importance to check model behavior and to assess convergence those are presented and discussed in the following section 4 output analysis results and discussion to continue to execute the following tasks of our sa step we must perform many tests and simulation the global sa of all model outputs was performed using the mase driver psuade integration the primary data obtained from the execution of each of the simulations are available for checking reviewing and replicating the experiments 4 4 simulation results and uq raw data https gitlab com infoknow mase mase bdi sourcecode tree master psuade 2520raw 2520data 4 1 input s relative importance the method of global gradient sa is presented in fig 8 results from both methods of linear regression based sa are presented in fig 9 response surface sa methods are presented in fig 10 the sensitivity scores represent the first order indices i e the contribution to the output variance by every single input alone if the parameters are normalized 0 1 then the most sensitive parameters get a score next to 1 while the least sensitive ones get a score next to 0 the vertical axis in these figures denotes the mase bdi input parameters used in the experiments the simulations were performed according to the experiment design table 7 the color scale of each grid indicates the order of sensitivity from low to high that is light colors for low data values and dark colors for high data values fig 11 presents the compilation of all qualitative sa methods regarding one single output fom fom was chosen as the qoi of our investigation as presented in section 3 4 the results of the variance based quantitative sa methods for the fom output are summarized in table 8 to address the minimum sample size to detect the most sensitive variables efficiently sa was calculated at different sample sizes for each sa method we illustrate the application of mars sa technique exclusively for the fom output with different sampling methods and sampling sizes as presented in fig 12 the final result for minimum sample sizes and sampling methods are compiled in table 9 4 2 check model behavior and assess convergence the application of ua and sa offers a valuable complement to each other and their close relation in abms has been proven by fonoberova et al 2013 ligmann zielinska et al oct 2014 pianosi et al 2016 since the cerrado lucc model is stochastic there is intrinsic uncertainty in the model even when all model parameters are fixed one of the main concerns of our work was to find the minimum number of model evaluations i e the number of simulation runs that were required to secure the stability of output variance we chose to apply the methodology brought by lorscheid et al 2012 and discussed by lee et al 2015 regarding the minimum number of runs in mase bdi the found problem specific point of stability was 800 this result stays in the middle of the typical find in the literature for a small number of inputs the gan et al 2014 analysis is based in the 10 n rule where n number of input factor subject to sa pianosi et al 2016 argue that the number of runs depends on the sa purpose that should be around 1 to 1000 n when the purpose is screening the parameters through variance based methods the theoretical minimum number of runs should be 1000 n from the results it is clear that some statistical estimation must be done before arbitrarily choosing a sample size and calculating descriptive and dispersion statistics to neglect this previous analysis may lead to statistical pitfalls such as results too uncertain to be reliable some other customary approach to determine minimum sample size may presuppose normality and therefore its efficiency becomes sensitive to the shape of the distribution this assumption is particularly relevant for the reason that abms and most real data often don t conform to parametric distributions moreover as sample size increases any theoretical distribution would likely be rejected another interesting discovery found was that the definition of a sampling technique might alter the minimum sample size required to reach variance stability the most common sampling approach involves a ua that summarizes the results of monte carlo simulation based on simple random sampling we investigated one other scenario with quasi random sampling and found that for our particular case the minimum sample size using random sampling is larger than the minimum found using a quasi random sampling design similar findings were described in other areas of application such as financial models peter a acworth 1998 and statistical circuit analysis singhee and rutenbar 2010 these results are in sync with the current trend of the use of quasi random sampling in abm ligmann zielinska et al oct 2014 saltelli et al 2008 as it generates samples more uniformly over the parameter space notwithstanding in our investigation of sa techniques we decided to test a broader combination of sampling techniques and sensitivity methods this exercise is another guideline to be regarded since there are sampling methods that best fit some sa methods and others that are inefficient or inappropriate the design of the sa experiments must consider it to avoid perfunctory sa very distinct results arise from the comparison of different sa methods in the cerrado lucc model not every method was able to identify the most sensitive parameters such as the linear regression based techniques spea and src and the response surface technique dt for the most part every other technique identified ta table 2 as the most critical parameter for all outputs therefore answering the initial question of which parameters are responsible for the most of the results variability almost every technique also identified ge input parameter 4 as an important parameter to most of the outputs the most significant influence of ge is on the producers accuracy and in the pixel wrong change right change and wrong persistence it is also clear across the different methods that tg and ie input parameter 2 and 3 are entirely insensitive hence not essential to explain the variability in the outputs these results show a positive correlation between input and output uncertainties and present consistency of the screening results and physical interpretations since ge and ta describe the amount of land transformation in a simulation high values of these parameters will increase the model output values ge is the most sensitive parameter followed by ta to understand and to reduce uncertainty within these two variables will therefore reduce the uncertainty of the simulation as a whole ge represents the amount of land cover that is transformed by a group of human agents in a cell of the map ge is a sensitive value as it indicates the voracity and velocity of the current land exploitation which will directly affect the result of the simulation ge was found as highly sensitive in every sa method therefore this result proves that the model is coded in such a way that it behaves similarly to reality because the socio economic groups responsible for large scale cattle ranching and permanent agriculture are the principal driver of deforestation in the cerrado mcalpine et al 2009 smith et al 1998 sa is used to prove this similarity between our model and the observed drivers of change for qualitative sa methods both linear regression and gradient based sensitivity were able to identify the non significant parameters regarding the most important parameter there are some discrepancies we can highlight four findings first moat mars sot and gp got similar results for most of the outputs second spea and src presented very similar results but differ from the other methods regarding ta and ge we argue that traditional methods such as correlation and regression analysis are not suitable for nonlinear and non monotonic problems like the mase bdi model third the results from dt appear very different from that of other methods the dt evaluation metrics were not able to screen the parameters correctly fourth gp results were consistent in three of four input parameters the divergences in the importance of ge may be attributed to the gp algorithm optimal configuration but further investigation is required regarding variance based sa methods the results were robust for all methods indicating ta and ge the two parameters that explain almost all the output variation considering the fom output ta was responsible for over 57 of the output variation followed by ge that explains about 42 of the output variation both tg and ie combined are responsible for less than 1 of the variance there is a consensus among variance based results denoting that quantitative sa is more robust than qualitative sa the divergences in qualitative sa may be explained by the use of heuristics to represent the relative sensitivity of the parameters for the sa comparison the general finding on every approach is described moreover the discrepancies and similarities of the related work table 1 are also summarized moat the gradient based sa technique was able to identify the elementary effects of the inputs correctly and it seems to be ideal for screening purposes the downside is that the interaction effects are not included gan et al 2014 found similar results in a study case with three times more parameters we were able to find consistent results with the minimum number of simulation runs but lilburne and tarantola 2009 argue that the sample generation is not straightforward a blind adoption of moat may not be representative since it is not a global sa practice linear regression ten broeke et al 2016 and lilburne and tarantola 2009 agree that regression is a simple technique that can describe relationships which yield insight into model behavior the bad performance of the spea and src regression methods was also found by gan et al 2014 which may demonstrate that for these case studies the regression model does not fit well to the particular abms response surface these qualitative sa methods were very efficient to indicate the sensitive variables at a low computational cost low number of runs a discrepancy was found compared to the work of gan et al 2014 in the cerrado lucc model the dt method performed poorly while in the related work there were no such problems on the contrary gan et al 2014 discarded the use of gp because it was not able to find the sensitive parameters a situation that did not happen in our study case response surface methods are based on heuristics and maybe these heuristics are more problem specific and a general guideline of use of any particular technique should not be endorsed before scrutiny variance based the techniques with the higher computational cost were the ones with more consensus among them they were all capable of finding the most sensitive parameters and this result is corroborated by different works saltelli et al 2008 lilburne and tarantola 2009 gan et al 2014 thiele et al 2014 ten broeke et al 2016 mc and lh were the sampling methods with better efficacy for qualitative sa methods identifying the most sensitive parameters with a sample size of 200 all the quantitative sa achieved the same result with the sample size of 400 from the results we can attest that qualitative methods are more efficient i e find the sensitive parameters in fewer model evaluations the main disadvantage is that there is no consensus among the methods and in some cases the resulting importance ranking of the parameters is quite the opposite fonoberova et al 2013 argue that the use of surrogate models in abms may be an alternative to increase confidence in qualitative sa methods conversely the results of all quantitative methods were broadly the same and the methods seemed more robust they were all based on variance decomposition and were capable of computing parameter first order effects but it takes larger samples to do so quantitative methods such as sobol are indeed more accurate but at a higher computational cost e g gan et al 2014 for models with a larger number of parameters than the cerrado lucc model one must evaluate the trade off between accuracy and cost 5 conclusions we investigated the various impacts that ua and sa experimental design have on abm outputs the results show that although much of the analysis is problem specific there are known challenges that can be overcome by the use of statistical methods related work comparison illustrates general practices that should be a routine both to improve the level of confidence in results derived from abms and to promote more rational and efficient use of abms we suggest performing a specific investigation of the problem aiming to test the robustness of the results one should begin with an investigation of the number of simulation runs required to secure the stability of output variance followed by a design of experiments selection quasi random sampling it was clear that the quantity of samples has several ramifications to experimental design and the quality of the analysis these steps must be done before ua the results of ua should be explored in a global variance based qualitative sa such as sobol we also investigated the impact that sampling techniques sample sizes and sa methods may have on the model output analysis we identified the most significant and non significant parameters of the mase bdi model by applying gradient based variance based and linear regression based sa we verified that ta is the parameter responsible for most of the variability of mase bdi results although the results were similar across the different sa approaches they also showed that not any technique can be used without being tested and compared with others beforehand choice of analysis methods and sampling heavily impact model parameter sensitivities regarding abms it seems that there is no single method able to embrace all models the best fit method is still dependable on the model and the goal of the experiment ua and sa were found to be essential tools for analyzing and evaluating abms in particular in the lucc context on the cerrado lucc model other than assuring the model predictions are correct we believe those methods should be used for model corroboration to help researchers check e g if the assumptions are fragile if the inferences are robust or if the variables are overly dependent regarding this matter we implemented a comprehensive uq through the integration of mase bdi and psuade we were able to improve the cerrado lucc model factor prioritization setting to identify which factor was most deserving of further analysis or measurement and to assess the abm parameter elasticity as a future work we are interested in identifying critical or otherwise interesting regions in the space of the input factors also we search to uncover factors which interact and which may therefore generate extreme values an abm may be used for learning purposes role playing games to understand the dynamics of a process or to investigate different scenarios and configurations despite the research area the number of parameters or the size of the model there is room to apply ua and sa routinely as a part of the modeling process or even in the model s operational use it is time to make the methodology of agent based modeling more robust and the analysis of results collected with abms more scientific to this end all expressions describing the systematic and methodological analysis of the responses and behaviors of the model and the mapping between its inputs and its outputs such as robustness checking variability ua or sa are to be disseminated to the community and to be applied on a regular basis acknowledgments the authors would like to thank m c pereira for helpful discussions professor celia ghedini ralha would like to thank the brazilian national council for scientific and technological development cnpq for the research productivity grant in the computer science area pq 2 process number 303863 2015 3 appendix a parameter space exploration sampling methods provide a systematic exploration of the parameter space that guarantees the sample to have specific statistical or structural properties the purpose of these methods is to actively reduce the number of parameter sets that are considered but still choose space filling points in the design space thiele et al 2014 for a complete revision of sampling methods readers can refer to kleijnen et al 2005 saltelli et al 2008 gong et al 2015 in this manuscript the most common sampling designs are illustrated and applied in the uq process monte carlo sampling mc metropolis and ulam 1949 method is the most common class of computational techniques based on repeated random sampling to obtain n numerical approximations of a specified distribution function of an unknown probabilistic entity however larger sample sizes are required to explore the parameter space fully latin hypercube lh mckay et al 1979 is a 1 dimensionally space filling method also known as stratified sampling method without replacement when sampling a function of n variables the range of each variable is divided into p equally probable intervals with a total of p sample points therefore each sample point is the only one in each interval lh method selects sample points in the interior of the hypercube of p levels lh can capture more variability in the sample space than simple random sampling orthogonal array oa owen 1992 is a 2 dimensionally space filling method that uses a general fractional factorial design to improve lh the oa design extends to t dimensional margins the univariate stratification properties of lh that is for an n dimension p level parameter space a t strength oa sampling generates p t sample points when t n orthogonal array based latin hypercube oalh tang 1993 uses orthogonal arrays to construct latin hypercubes in other words the samples go through a stratification process to produce samples that have been both orthogonalized and stratified this sampling scheme provides more suitable designs for computer experiments and numerical integration than general lh sampling metis sampling karypis and kumar 1998 is an m directional space filling method that is a part of a set of multilevel partitioning algorithms designed for partitioning irregular graphs partitioning large meshes and computing fill reducing ordering of sparse matrices metis can partition an unstructured graph into a user specified number k of parts fourier sampling algorithm cukier et al 1973 was designed specifically for the fourier amplitude sensitivity test fast in this method the parameter space is explored periodically with interference free frequencies it takes a small number of correlated random samples from a signal and processes them efficiently to produce an approximation of the discrete fourier transform dft of the signal the minimum sample size of fast is n 2 m s ω m a x 1 where m s is the maximum harmonic in general 4 or 6 and ω m a x is the maximum frequency which is determined by the number of inputs lpτ lptau statnikov and matusov 2002 is a quasi random qr sampling method i e the samples are generated from a finite subset of low discrepancy sequence of points these samples are not random in the sense of being completely unpredictable however they are like random points in the sense that they are uniformly distributed across an n dimensional space lptau explores the parameter space using partitions of the parameter ranges on the base of two sobol extended sobol sobol 2001 saltelli 2002 is a replicated version of low discrepancy sequences quasi random sobol generates a uniform distribution in probability space a qualitatively random distribution filling previously unsampled regions of the probability function this is done with two random r n sample matrices m 0 and m n 1 therefore the total number of sample points is n 2 r morris one at a time moat morris 1991 sampling was designed specifically for moat sa and is similar to sobol the range of each parameter is divided into p 1 equal intervals next r points are generated from the n dimension p 1 orthogonal grid for each one other sample points are generated by perturbing one dimension at a time until all dimensions have been varied for only one time with a n 1 r total number of sample points appendix b uncertainty analysis the uncertainty analysis assesses a confidence bound on the output estimation by quantifying the uncertainty associated with the model response due to uncertainties in the model input to achieve these results we follow the necessary steps of ua summarized by saltelli et al 2008 1 start from a model parameter α n α σ α which reads after estimation the distribution of α is known with mean α and standard deviation σ α 2 assume that all the parameters β γ are independent of each other 3 draw a sample from the respective distributions of each parameter in other words produce a set of row vectors α j β j in a way that α 1 α 2 α n is a sample from n α σ α likewise for all parameters α 1 β 1 γ 1 α 2 β 2 γ 2 α n 1 β n 1 γ n 1 α n β n γ n 4 run the model for all vectors α j β j thereby producing a set of n values of a model output y j y 1 y 2 y n 1 y n by executing these steps it is possible to quantify the impact of input uncertainties on the model response and assess whether or not the response meets the required standards of precision although monte carlo is the most used method there are many other methods available to generate the samples and estimations required by ua some interesting ua applications and experimental design are described in the literature crosetto et al 2000 lilburne and tarantola 2009 saltelli et al 2008 and fonoberova et al 2013 the expected means and variance are quantified for each parameter additionally a histogram of the output variable can be displayed thus thoroughly describing the stochastic features of the model output the overall computational cost of ua depends basically on the cost of the model evaluations which is linked to the complexity of the model itself appendix c sensitivity analysis methods many techniques for sa have been proposed and a thorough description of the techniques can be found in saltelli et al 2008 regardless of the technique saltelli and annoni 2010 present a guideline on how to avoid perfunctory sa which we applied throughout the manuscript a brief description of the methods applied is found next the morris one of a time screening method moat morris 1991 may be regarded as a gradient based global sa as the final measure is obtained by averaging local measures the elementary effects ee it is composed of individually randomized one at a time experiments that calculate two sensitivity measures of the gradients of each parameter sampled from r local changes the mean μ assesses the overall influence of the factor on the output the standard deviation σ estimates the ensemble of the factors effects whether nonlinear or due to interactions with other factors ee provides the information that the effects for a given parameter may be i negligible ii linear and additive or iii nonlinear or involved with interactions with other factors moat can be much faster than other variance based sa techniques we assessed three variance based sa techniques sobol sobol 1993 fast cukier et al 1973 and mckay 1995 in general they have higher computational cost than qualitative sa but some exciting features to abms are that variance based sa measures are model independent and provide the investigation of interaction effects the first order index represents the main effect contribution of each input factor to the variance output the total effect of a variable would be the total contribution to the output variation that is its first order effect plus all higher order effects due to interaction in the sobol method the variance may be attributed to a single input first order main effect or by the interaction of two or more inputs second order effect the sum of those contributions is the total effect of a parameter to decompose the variance fast varies different parameters at different frequencies and applies a fourier transformation to measure each parameter contribution mckay uses analysis of variance anova to calculate a correlation ratio that is a ratio of the variance of a parameter and the total variance of the output the significance of the parameter increases with the correlation ratio linear regression based sa decomposes the variance of the model outcomes by fitting a regression function of the input parameters to these outcomes therefore the simulation outcomes are described concerning input output relationships which can be validated using standard statistical measures such as r 2 correlation analysis ca measures the parameter sensitivity through correlations coefficients such as spearman spearman et al 1904 regression analysis ra makes the same measures using the standard regression coefficient src to estimate the result from a regression analysis that has been normalized so that the variances of the dependent and independent variables are equal to one the efficacy of this methods relies on the input output being somewhat linear or monotonic the methods sum of trees sot gaussian process gp and multivariate adaptive regression splines mars are considered response surface or surrogate models from which it is possible to obtain relative scores of the total effects of a parameter those methods provide a mapping from parameters to outputs sot breiman et al 1984 chipman et al 2010 is a tree based bayesian method a single regression tree model is obtained by the use of a recursive binary partition of the parameter space the created balanced binary tree in which the variables are split to cause the maximum decrease in the residual sum of squares has each terminal node with a minimum number of sample points the variable with the larger number of splits is considered the most sensitive one mars friedman 1991 is a non parametric regression able to model nonlinearities and interactions between parameters it is considered an extension of the tree method because after partitioning the space it builds localized regressions first and second order for each model a score generalizes cross validation is computed it will remove each parameter and recalculate the model score the larger the score the more important is the removed parameter gp is an implementation of the tpros algorithm proposed by gibbs abd mackay 1997 gp is a method for regression using gaussian process priors which allow an exact bayesian analysis using matrix manipulations the theory behind the method states that points that are close in parameter space give rise to similar response values thus it is possible to identify the influence of the parameters in the model response the dt pi and peterson 1994 is a method that establishes dependencies in continuous functions given a sequence of measurements δ an estimate of noise variance when a subset of variables in the sample are selected for regression the approach is based on calculating conditional probabilities from vector component distances it has been proved that adding unrelated variables or withdrawing related ones will increase δ hence the subset of all variables that minimize noise variance is considered the most sensitive 
26372,an accurate modeling of urban co2 emissions is important for understanding the dynamics of carbon cycle and for designing low carbon policies we develop an improved nightlight based method to model urban co2 emissions and investigate their spatiotemporal patterns differing from the previous methods in processing the pre modeling data we bring forward the existing co2 inventories from national and provincial levels to city level and correct the saturation and blooming problems of nightlight in modeling the correlation between nightlight and statistically accounted co2 emissions we highlight a panel data regression analysis that considers the spatiotemporal heterogeneity across cities and over time simultaneously eleven cities in yangtze river delta of china were selected for a case study testing our method the internal and external validations have proven the predominance of our proposed method for capturing the nightlight co2 correlation and for describing the spatial distribution and heterogeneity of urban co2 emissions keywords urban co2 emissions nighttime light panel data regression yangtze river delta 1 introduction cities are the main contributor to climate change since they are responsible for more than 70 of the global fossil fuel induced carbon emissions while occupying less than 2 of the earth s land area gurney et al 2015 their impacts are expected to grow due to continuous urbanization china has been urbanizing at an unprecedented speed and has become the largest carbon emitter in the world the proportion of residences qualifying as urban increased from 18 in 1978 to 55 in 2013 the number of prefecture level cities with population sizes over one million also expanded to 133 nbs 2015 consequently some chinese mega cities such as shanghai emitted more greenhouse gases than several countries did such as thailand and the netherlands world bank 2010 therefore the reduction in carbon emissions at a city scale becomes increasingly important and urgent a long term monitoring of urban co2 emissions is critical for understanding the dynamic patterns and drivers of the carbon cycle and for helping policymakers to design effective policies to mitigate climate change based on intergovernmental panel on climate change ipcc guidelines for national greenhouse gas accounting a growing number of scholars and research institutes have developed methodologies and tools for quantifying carbon emissions at a city scale including the international local government greenhouse gas emission analysis protocol iclei 2009 the ghg protocol wbcsd and wri 2004 the greenhouse gas regional inventory protocol carney et al 2009 and the sustainable energy action plan seap com 2010 however despite recent advancements in research aimed at estimating the dynamics of urban carbon emissions great challenges remain that are largely due to the lack of comprehensive consistent and comparable statistical data on energy consumption and human activities on a city scale furthermore all of the accounting methods based on energy statistics all treat the city as a homogenous unit but represent the dynamics of the urbanization processes such as the rapid sprawl of urban built up areas poorly albert et al 2015 edward and matthew 2010 as means of addressing the abovementioned challenges nighttime light ntl has been widely used as a useful proxy for economic output chen and nordhaus 2011 urban extent extraction xie and weng 2017 population estimation sutton et al 2001 electricity consumption cao et al 2014 and in use metal stocks liang et al 2014 due to its strong correlation with human activities and its availability at a high spatial resolution for most of the world beginning in 1992 recently this proxy has also been applied to estimate urban co2 emissions at different time and space scales for example oda and maksyutov 2011 downscaled national co2 emissions to global 1 km 1 km grids using nightlight as a proxy and separately allocated the point source emissions based on the global power plant database asefi najafabady et al 2014 built upon a previously developed fossil fuel data assimilation system ffdas and expanded the estimated 1 km gridded co2 emissions from a single year to multi years from 1997 to 2010 by combining with nightlight data gridded population and global power plant database su et al 2014 analyzed the linear correlation between co2 emissions and ntl in provinces and a limited number of cities of china based on a pool data regression analysis without considering the differences across regions and over time and predicted the carbon emissions in cities without direct energy data meng et al 2014 and shi et al 2016 developed a panel data regression model that took into account the city specific coefficient in capturing the relationships between province level energy related co2 emissions in china with ntl and downscaled the emissions to an urban or 1 km scale these studies have reached a consensus that the ntl can be used as a valid and useful proxy for downscaling statistically accounted co2 emissions to scale of interest for example pixel urban city and urban agglomeration and one can obtain estimates that are much more geographically consistent than energy consumption statistics through bypassing the reliance on statistical energy consumption data however at least three aspects can be substantially improved first a suitable carbon accounting method needs to be selected that is compatible with the available energy statistics the ipcc guidelines are based on detailed energy consumption data by sector and fuel type and are the most preferred and commonly used approach in the existing literature oda and maksyutov 2011 meng et al 2014 su et al 2014 shi et al 2016 however detailed energy data are not always available for cities taking china as an example the energy balance table is only available for the whole country provinces and a limited number of cities such as shanghai guangzhou and shenzhen in most cities only certain fragmented information about energy consumption in certain specific sectors e g industrial enterprises and households can be accessed through the city s statistical yearbook thus the scope and methodological complexity should be taken into account in choosing an appropriate accounting method so a more exact correlation between carbon emissions and ntl can be derived second the intrinsic saturation problem of ntl especially in the city centers needs to be addressed otherwise it may constrain ntl s further application and estimation accuracy internationally there are a great number of studies focusing on saturation correction various methods and indices have been developed generally by combining original ntl with other data sources such as normalized difference vegetation index ndvi and population density to increase the variation of ntl in urban cores meng et al 2014 ma et al 2017 however potential improvements still exist in relieving the saturation issue bennett and smith 2017 third the spatiotemporal heterogeneity across cities and over time must be considered the socioeconomic and geographical conditions usually vary in different cities and at different development stages even for the same city these differences may result in significant disparities in both the quantity and change patterns of carbon emissions this study aims to address the abovementioned deficiencies so as to better estimate urban co2 emissions in processing the pre modeling data we bring forward the existing co2 inventories from national and provincial levels to city level which to some extent breaks through the strong requirement on detailed statistical energy data and raise the accuracy of pixel level co2 estimation as it is downscaled from city level rather than from country and province moreover we correct the saturation and blooming problems of ntl by integrating time series ndvi and population density data which reduces the estimation error in the urban core and rural areas in modeling the correlation between ntl and statistically accounted co2 emissions we propose a panel data regression model which considers the spatiotemporal heterogeneities across cities and over time simultaneously eleven cities in the yangtze river delta yrd of china were selected as a case to test the method based on the same dataset our model was internally validated through a 2 fold cross validation process and compared with a pool data regression model and a panel data regression model that only considers city specific coefficient in addition our model was also externally validated with other studies both at city and pixel level by doing so the predominance of our improved method could be seen clearly finally the uncertainties limitations and potential improvements have also been discussed 2 study area and data 2 1 study area yrd is china s largest urban cluster wherein 11 cities were selected for case studies fig 1 there are three reasons for their selection the first is the important role that the yrd plays in socioeconomic development and carbon emissions it is one of the most rapidly urbanizing and wealthiest regions in china with the country s largest urban cluster covering 2 of the country s territory but contributing 20 and 12 to the total gdp and co2 emissions respectively in 2005 cai and xie 2007 the second reason is the relatively lower disparity in societal and natural conditions e g culture lifestyle income level and climate among the cities in the yrd compared to broader areas across china choosing a study area with less regional disparity may reduce the disturbance from spatial heterogeneity in correlation analyses between carbon and ntl the third reason was that the 11 case study cities reflect some generalities of economic structure and transport development of chinese cities as shown in table s1 in the supplementary material some cities in the yrd for example huzhou and suzhou have developed the secondary industry as their leading industry which is consistent with most cities in middle reaches of the yellow river the middle reaches of the yangtze river and in the northeastern regions however there are also cities in the yrd for example shanghai and hangzhou whose tertiary industry is the pillar industry it represents a widely existing situation in the eastern coastal and northern coastal cities in china for the transport sector there is a common phenomenon not only in the yrd cities but also across china that the number of civil automobiles has been increasing sharply at a mean annual rate of more than 13 between 2003 and 2013 nbs 2015 in summary owing to the important role lower disparity and representative generality we believe that the yrd is a good case study area for both testing our proposed method and increasing our understanding of urban co2 emissions in china 2 2 description of the data table 1 outlines the data used for the analysis which generally includes two kinds with a time range from 2003 to 2013 one is the spatial data including ntl produced by the defense meteorological satellite program s operational linescan system dmsp ols land use and land cover data classified based on landsat enhanced thematic mapper plus landsat etm images ndvi data based on moderate resolution imaging spectroradiomet modis from united state geographic survey http www usgs gov and population density data the nighttime light data version 4 have a spatial resolution of 1 km 1 km and its digital number dn values of the artificial nighttime light brightness from cities towns and other sites ranged from 0 to 63 these data can be accessed online from the national geophysical data center at noaa usa http ngdc noaa gov eog the monthly composite ndvi in china after splicing cutting and re projection was download from the resources and environmental sciences international scientific technical data mirror site computer network information center http www gscloud cn the land use data including 6 land types urban built up cropland water forest grassland others as well as the population density data were obtained from the data center for resources and environmental sciences chinese academy of sciences http www resdc cn the other data include the city level statistics for co2 accounting the direct energy consumption data by industry commercial and household sectors together with the vehicle fleet number of all eleven cities were compiled from the statistical yearbooks of each city jiangsu and zhejiang provinces and china overall 3 methodology generally the method that uses ntl to downscale statistically accounted co2 emissions to scale of interest for example pixel urban and etc is based on a hypothesis that the pixel level ntl s brightness has a positive correlation with the energy consumption related co2 emissions from the same pixel in our study in order to downscale city level accounted co2 emissions to urban scale there are basically two major steps the first is to process the pre modeling data which contains the city level co2 accounting and ntl data correction and urban extent extraction the second is to model urban co2 emissions through building the correlation between accounted emission and ntl and estimating pixel level emissions and aggregating them to urban scale 3 1 pre modeling data processing 3 1 1 city level co2 accounting and uncertainty quantification as defined in the corporate accounting and reporting standards which were developed by the world resources institute and the world business council for sustainable development wri wbcsd and are widely used by researchers the city level carbon inventories generally include three scopes of operational boundaries wbcsd and wri 2004 scope 1 is the direct emissions from activities that occurred within the physical boundary of a city such as emissions from factories vehicles and households scope 2 incorporates the emissions outside of a city but is related to energy use within a city which includes the electricity and heat produced elsewhere scope 3 is more comprehensive than scopes 1 and 2 it considers both the direct and embodied emissions during life cycle processes of products and services consumed in the city scope 3 includes the emissions from waste disposal air transport those embodied in food water and construction materials and others usually the basic carbon inventory for most cities refers to scope 1 and 2 emissions for example kennedy et al 2010 yu et al 2012 however those including scope 3 emissions are mostly reported in urban metabolism studies for example kennedy et al 2009 obviously the more scopes included in inventory accounting the more it requires extensive data processing time and expertise whittaker et al 2013 due to the features of appropriate coverage of carbon emission scopes and moderate difficulty in accounting the seap approach yamina et al 2014 is employed in this study since it fits within scope 1 and 2 emissions in which city level co2 emissions include direct emissions generated inside of a city boundary such as the combustion of coal and oil in industrial enterprises and the indirect emissions from electricity consumption and heating that are mainly produced outside of cities in the accounting the seap method estimates the co2 emissions from stationary and mobile sources in a city fig 2 the stationary emissions include those from energy consumption in industrial tertiary and household sectors whereas the mobile emissions are derived from public private and commercial transport sectors specifically the accounted stationary emissions aestationary are estimated by eq 1 1 ae stationary m n t ec m n t ncv m n ef m n where m n and t represent the investigated sector fuel type and year respectively ec represents the amount of energy consumption in metric tons ncv represents the net calorific value in megawatts per ton mwh t ef represents the co2 emission factor in tons co2 per megawatts tco2 mwh the details can be found in table 2 the accounted mobile emissions aemobile are estimated by eq 2 2 ae mobile v n t fn v n t vkt v n t fe v n t d n cr n 44 12 where v n and t represent the vehicle type fuel type gasoline or diesel and year respectively in this study the main types of fuels consumed by on road vehicles are gasoline and diesel fn is the fleet number which consists of public bus and taxi and private vehicles passenger car truck and motorcycle and the passenger car and truck are further divided into light medium and heavy duty classes vkt represents the average kilometers vehicle traveled fe is the fuel economy of vehicles in liters per kilometer l km d is the density of fuel type in kg l which is 0 732 for gasoline and 0 875 for diesel cr n is the carbon ratio of fuel type which is 85 5 for gasoline and 87 for diesel 44 12 is the molecular weight ratio of carbon dioxide to carbon due to the lack of statistics on the fleet number further divided by fuel type we make the following assumptions to enable the estimation 1 all the buses and heavy duty trucks use diesel 2 all the motorcycles taxis and light duty passenger vehicles consume gasoline for the rest of the vehicle classes the proportions of vehicle either using gasoline or diesel are estimated by following the literature yan and crookes 2009 han and hayashi 2008 liu et al 2013 the detailed data are listed in table 3 for the absence of officially published statistics on vkt and fe they are complied from literature and interpolated based on historical trends for those years lack of data the details are listed in table 4 and table 5 though the seap method employed in this paper is compatible with the ipcc guidelines and is applicable to chinese cities that have limited and fragmented energy data the results may be different from those generated with the ipcc guidelines since the former relies partially on city activities for example using fleet number as a proxy to estimate emissions from the transport sector whereas the latter mainly depends on detailed energy consumption statistics moreover even using the same seap method the parameter settings in different literature may also cause diverse results to quantify the uncertainties in co2 accounting a monte carlo simulation approach was employed this approach divides the uncertainties into two sources activity levels als and emission factors efs the als and efs which were assigned with a normal distribution and corresponding coefficients of variation cvs the ratio of the standard deviation to the average were fed into the monte carlo simulations for the als the normal distributions with cvs of 10 20 20 and 16 were recommended for industrial commercial household and transport sectors zhao et al 2012 the cvs for the efs are also derived from zhao et al s 2012 work according to a range of 95 confidence intervals the cvs for different sectors are listed in table s2 in the supplementary material a total of 1000 trials were performed to estimate the uncertainties in co2 emissions accounting 3 1 2 ntl data correction and urban extent extraction since the ntl is monitored with separate satellites dmsp satellites f10 f12 f14 f15 f16 and the dn values are incompatible for direct use we first removed the background noises and lights from the gas flares based on a method from elvidge et al 2009 then calibrated the data by using the methods proposed by liu et al 2012 however unlike the approach of liu et al 2012 who used a city located in northeastern china jixi city as the reference region we chose taizhou city a city in zhejiang province in yrd as the reference city to calibrate the ntl images the reason for the detailed calibration process is described in 1 calibration processes of dmsp ols data in yrd in the supplementary material because of the relatively coarse spatial resolution of the ols sensor the calibrated ntl data are still suffer from the saturation effect where lights detected in the center of large cities are too bright and cannot be distinguished elvidge et al 2007 besides the spatial extent of lighted areas is often larger than the developed areas blooming effect as the diffuse and scattered lights detected by ols sensor small et al 2005 the saturation and blooming effects may cause potential estimation errors especially in city center and suburban areas zhang et al 2013 based on the assumption of that vegetation cover and human activities are inversely spatial correlated an improved vegetation adjusted ntl urban index which combines ntl with time series ndvi and population density meng et al 2017 was employed to reduce the saturation and blooming problems after the calibration of original ntl dataset on the basis of the correction of ntl data the urban extent was extracted from a city boundary through a dynamic threshold method which has been used in a number of studies liu et al 2012 zhou et al 2014 here as shown in fig 3 urban is defined as a built up area where land has been developed and constructed on a large scale including basic municipal public facilities and where most of socioeconomic activities occur its boundary varies over time due to the urbanization process in contrast a city refers to a jurisdictional unit with a fixed administrative boundary urban populations are one of the most important driving forces of urban expansion sutton et al 2001 han et al 2009 thus to increase the efficiency of urban extent extraction we divided the 11 cities into 4 groups based on their urban population size they are the super megacity mega city large city and medium city following a standard division used by the state council of china 2014 then we determined an optimal threshold of ntl s dn value for each city group to separate urban and non urban extents using urban built up areas classified from landsat etm images in 2000 2005 and 2010 as references thresholds from 2000 were applied to the close years 2003 2004 values from 2005 were applied to 2005 2008 and thresholds from 2010 were applied to 2009 2013 table 6 with the support of the landsat data the accuracy could be much improved and those pixels dominated by water and vegetation could be excluded from the urban extent however the variation in the threshold value will definitely influence the final result to quantify the uncertainties of urban extent extraction based on dynamic threshold method on the result of urban co2 emissions we conducted a sensitivity analysis by measuring of the impact of threshold variation 1 5 and 10 increase or decrease on urban co2 emissions while keeping other parameters constant 3 2 urban co2 emission modeling 3 2 1 models for capturing the correlation between accounted co2 and ntl previous studies suggested the relationship between ntl and socio environmental factors such as population gdp and co2 emissions follows a power law relationship where a relative change in one quantity results in a proportional relative change in the other quantity sutton and costanza 2002 to determine an improved method that can accurately and reliably capture the relationship between accounted co2 emissions and ntl we developed the following three econometric models for comparison model 1 is based on pool data regression and considers no spatiotemporal heterogeneities across cities and over time which was also applied by su et al 2014 a double logarithm as illustrated in eq 3 enabled comparison between the explanatory and explained variables and ascertained the elasticity of the dn on carbon emissions values of α and β represent the intercept and slope coefficient ε is the error term 3 model 1 ln ae c i t y i t α β ln dn city i t ε i t to capture the influences of spatiotemporal heterogeneity among the 11 sample cities between 2003 and 2013 panel data regression models were proposed the subscripts i and t denote each city and year respectively model 2 incorporates a city specific coefficient μ in addition to dn which was also used in studies by meng et al 2014 and shi et al 2016 model 3 includes both the city specific coefficient μ and time variable σ as additional explanatory variables the values of i and t represent the specific city and year 4 model 2 ln ae c i t y i t α β ln dn city i t μ i ε i t 5 model 3 ln ae c i t y i t α β ln dn city i t σ t μ i ε i t 3 2 2 internal model validation using all the data for 11 cities covering 11 years 121 sample set in total we performed an internal validation of model 3 based on the following 2 fold cross validation processes and compared the result with that of model 1 and 2 using the same data and validation process i all the data were randomly divided to two groups ii data in each group were used in turn as training set for correlation analysis while the remaining group was removed based on the coefficients derived from the regression models the emissions for the remaining group were predicted iii the data in the removed group were used as references to calculate the difference between the predicted and observed values for each sample set iv the mean squares of the differences for all the sample set were calculated v by repeating the processes 1000 times through the monte carlo simulation the model accuracy could be assessed through the median of root mean squared error the lower the median is the more accurate the model will be 3 2 3 urban co2 emissions estimation based on the correlation between city level accounted co2 emissions and dn that were deduced from either eqs 3 and 4 or 5 the pixel level co2 emissions could be modeled using dn as a predictor as differences exist between the accounted and modeled emissions we used their ratio to further calibrate the co2 emissions at a pixel level which is shown in eq 6 6 ce urban i t k ce urban i t k k ae city i t me city i t me urban i t k where ce urban i t is the calibrated urban co2 emissions in city i for year t the value for k is the pixel within the urban area aecity represents the accounted city level co2 emissions mecity and meurban are the modeled co2 emissions at a city and urban level 4 results 4 1 model predictive power fig 4 presents the correlation between ntl and the accounted co2 emissions that were estimated by the three models and compares each model s predictive power that is measured by r 2 in model 1 the r 2 in both the sectoral and aggregated results was relatively high r 2 0 65 which indicates that a significant correlation between the accounted co2 emissions and ntl and at least 65 of the change in co2 emissions could be explained by ntl meanwhile the scattered points deviated from the fitting curve as also suggested the existence of regional diversity among the cities to eliminate the influence of spatial diversity on the carbon ntl relationship panel data analysis considering the city specific effect as explained in model 2 was conducted it was found that the r 2 in model 2 was significantly enhanced with values over 0 97 the results in model 3 justify the importance of the incorporation of both space and time differences as explanatory variables in estimation the r 2 values in model 3 are all improved over those in models 1 and 2 in the estimation of the relationship between the city s total co2 emissions and corresponding ntl the r 2 in model 3 demonstrated the highest value reaching 0 990 the city specific coefficient μ for each city as estimated by both model 2 and 3 are listed in table s5 in the supplementary material however for the time dummy variable σ in model 3 a strong linear correlation with the year difference for mobile emissions was found along with an inverted u shape curve for stationary and total emissions fig s5 in the supplementary material all of the r 2 of the fitting lines were fairly high which indicates a strong predictive power when using the fitting functions to estimate the time dummy variable σ in addition when looking at the correlations between ntl and the accounted co2 emissions from the secondary industrial tertiary industrial and household sectors as shown in fig s6 in the supplementary material model 3 still performed better than others as its minimum value of r 2 was 0 978 in sum model 3 which considers both the space and time differences across cities and over time was proven to have a better predictive power than the other two models in modeling co2 emissions using ntl as a predictor 4 2 model validation fig 5 shows the results of internal validation for three models using the same data set covering 11 cities from 2003 to 2013 it is found that the median of root mean square error rmse for 1000 times of two fold cross validations was 0 438 0 401 and 0 409 for stationary mobile and total emissions respectively in model 3 which was much lower than that in model 1 and 2 it suggests that model 3 which considers both the space and time differences is more accurate fig 6 shows the external validation of our improved model s results at city level through the comparison of our accounted city level co2 emissions with 95 confidence intervals in some specific cities with previous studies it can be clearly found that the results of the same city can have much difference in different studies for example our estimates are all smaller than those from wang et al 2013 as they accounted the carbon emissions from industrial processes while we didn t and our results are all larger than those from yu et al 2012 as we consider the emissions from heating while they ignored but anyway the changing trend and scale of all the results are similar with each other which suggests the validity of our estimation fig 7 shows the external validation of our results at pixel level by comparing our modeled 1 km pixel co2 emissions with those estimated by oda and maksyutov 2011 available at http odiac org dataset html in yrd in 2010 it is found that there was an extremely high emission point in oda and maksyutov s estimation the blue peak in fig 7d while not in ours it is because in addition to downscaling the national fossil fuel induced co2 emissions to 1 km pixel level using ntl as a proxy oda and maksyutov 2011 also separately estimated emissions from point sources using a global power plant database except the difference in point source emission it is clear that our result can better describe the spatial distribution and heterogeneity of urban co2 emissions as the fluctuation of our estimates along the line transact was more significant the red curve in fig 7d than oda and maksyutov s and the spatial pattern of our modeled urban co2 emissions fig 7c was closer to the distribution of urban extent fig 7a 4 3 urban co2 emissions in yrd based on the results from model 3 fig 8 illustrated the dynamic change in the calibrated co2 emissions from urban and non urban areas in the yrd cities as shown in fig 8a urban co2 emissions increased four fold from 194 million tons in 2003 to 714 million tons in 2013 with its share in total emissions continuously increasing from 37 to 57 when looking at per capita co2 emissions fig 8b although they increased at a similar rate of 10 per annum in urban and non urban areas urban areas had a lower value with an increase from 6 to 15 tons capita furthermore we also probed into the change in carbon density as shown in fig 8b the carbon density of urban land ascended sharply from 17000 tons km2 in 2003 27000 tons km2 in 2007 and then fluctuated around 28000 tons km2 thereafter which is 3 4 times larger than that of non urban lands the faster growth in urban co2 emissions 22 yr compared to that in urban expansion 9 yr from 2003 to 2007 is the main reason for the quick increase in carbon density in urban areas however after 2007 the increase in urban co2 emissions slowed at an annual rate of 9 and resulted in the stabilization of carbon density second we also investigated the spatiotemporal patterns of urban co2 emissions and illustrated the medium value of monte carlo simulation in fig 9 generally the urban co2 emissions demonstrated a significant increase and expansion over the whole yrd region however the spatial distribution of the increase was not even high emissions were found in the northern and northwestern parts of the yrd such as in suzhou and nanjing in contrast cities in eastern yrd had a relatively low urban carbon density fig 9a to observe the dynamic changes and spatial differences of urban carbon emissions clearly we calculated the annual growth rate of carbon density in each 1 km 1 km pixel from 2003 to 2013 and classified the change into five grades according to the natural breaking methods brewer and pickle 2002 as shown in fig 9b two types of significant growth were detected one was the newly built growth in the outer suburbs of each city where it was non urban in 2003 but became urban in 2013 another is the rapid growth in the peri urban areas adjacent to the urban centers notably the growth in the cities close to shanghai such as suzhou nantong and wuxi were particularly sharp moreover the change in urban co2 emissions per capita was also classified into five grades using the same breaking method fig 9c between 2003 and 2013 the fastest growth occurred in suzhou whose urban population was approximately 7 7 million and the population density was 770 cap km2 in 2013 but the per capita carbon increased 4 fold from 10 7 to 41 4 ton cap in contrast shanghai had an urban population of over 13 million and 3800 cap km2 of population density however its per capita urban carbon demonstrates the slowest growth from 8 3 to 12 3 ton cap this suggests that on a per capita basis large and compact cities are usually more carbon efficient than small and sprawling cities world bank 2010 5 discussions 5 1 uncertainties and limitations of our method the urban co2 emissions were calculated from the summation of gridded emissions within the urban extent since we assumed that relationship between 1 km grid level co2 emissions and ntl was consistent with that at city level thus the uncertainty of each gridded emissions value should be the same as that of the accounted city level emissions therefore the uncertainties in urban co2 emissions modeling should derive from the following two approaches first city level co2 emissions were accounted by the seap approach based on a monte carlo simulation the mean uncertainties with 95 confidence intervals of our accounted co2 emissions from a stationary source mobile source and city total for the 11 cities between 2003 and 2013 were 17 5 to 18 0 18 0 19 7 and 17 5 to 18 0 respectively as a comparison to other studies the uncertainty in china s co2 estimation conducted by gregg et al 2008 ranged from 15 to 20 second the urban extent was extracted by a dynamic threshold approach by comparing our estimated urban extent in some of the selected cities for each city group in 2005 2010 and 2013 with the land use and cover data classified from landsat images that were obtained from the data center for resources and environmental sciences at the chinese academy of sciences http www resdc cn and the global institute for urban and regional sustainability http www giurs com the reliability of the dynamics of urban extent could be measured by kappa and overall accuracy oa indices which are two widely used coefficients to assess the classification accuracy of remote sensing images cohen 1960 fitzgerald and lees 1994 as illustrated in fig s7 fig s9 in the supplementary material the average kappa and oa were 0 37 and 89 17 in 2005 0 38 and 88 41 in 2010 and 0 38 and 84 53 in 2013 suggesting that the dynamic threshold method can capture the change in urban extent with a relatively high accuracy fig 10 shows how every 1 5 and 10 increase or decrease of the dn threshold will affect the final urban co2 emissions we found that the sensitivity of the urban co2 emissions to dn threshold differed from years and increased when the variation in threshold became larger specifically every 1 change in the dn threshold would contribute to 4 1 2 5 changes in urban co2 emissions a 5 threshold variation would cause 7 9 9 7 changes in emissions when the threshold variation increased to 10 the urban co2 emissions would change by 15 8 20 1 which was as large as the mean uncertainties in the city level co2 accounting this suggests that urban extent extraction was as important as city level co2 accounting for reducing the uncertainties of urban co2 modeling in the future more emphasis should be put on improving the quality of statistical energy data raising the accuracy of carbon emissions factors in study area and improving the reliability of the extracted urban extent some limitations still remain and require further research first only three years of landsat images were used as a reference in the urban extent extraction which limits the reliability of the results second the significant correlation between city level co2 emissions per sector and ntl as shown in fig 4 and fig s6 in the supplementary material is only from the perspective of statistical analysis it may not mean there is real correlation between pixel level ntl and co2 emissions per sector without the detailed information about the land type of and the human activity upon the pixel our method can only represent mixed co2 emissions from different sectors at pixel level third the external model validation at pixel level is the comparison of our results with other researcher s model results rather than the comparison with the real ground based co2 emission monitoring data in the future possible improvements could include the following aspects a collecting more landsat images to raise the accuracy of urban extent extraction b combining more detailed land use and human activity data for example high resolution land use and cover maps and point of interest data of infrastructure distribution to obtain more information about urban pixel so that pixel level co2 emissions per sector could be estimated c conducting a true external validation by running an atmospheric transport dispersion model based on our modeled 1 km pixel level co2 emission data and comparing results with ground based co2 monitoring data for example the tansat carbon satellite data which has a 1 2 km spatial resolution and was released in 2017 http satellite nsmc org cn portalsite data satellite aspx 5 2 potential uses based on the proposed method for estimating urban co2 emissions several potential uses can be derived they include but are not limited to the following the proposed method can serve as a decision support tool for updating and investigating the dynamic changes in urban co2 emissions at a relatively high spatial resolution they can also be used as input for carbon cycle and climate simulation models the exploration of urban co2 emissions and their variation provides insights for follow up analysis combined with the spatial data of socio economic variables e g gdp population infrastructure distribution and land use cover the socioeconomic and biophysical factors that drive urban co2 change could also be quantified data availability and quality are recognized as the major obstacles for accurately estimating the spatial and temporal patterns of co2 emissions the pixel level co2 estimation by downscaling from city level co2 inventory rather than from nation and sub national regions as shown in our case study provides a possibility to address this gap though it requires not only the statistical energy data but also the activity data for example vehicle fleet number we believe it is feasible to collect similar data for other chinese cities as well similar database for cities in other countries should also be explored via international collaboration 6 conclusions in this study we propose an improved method for quantifying urban co2 emissions using ntl as a proxy in contrast to previous research we bring forward the existing co2 inventories from national and provincial levels to city level correct the saturation problem of ntl and considers the spatiotemporal heterogeneities across cities and over time in modeling the ntl co2 correlation although the correlation parameters derived in the yrd were city specific and cannot be directly used for other cities regions in china the developed method can be applied to other areas even if they do not have a detailed energy balance table furthermore our method is proven to better capture the correlations between ntl and co2 emissions from different sectors that is industrial tertiary household and transport sectors or for city aggregates compared to the methods reported in previous studies our method is a supplement to existing approaches for modeling urban co2 emissions moreover it helps understand the spatiotemporal dynamics and causal factors of urban carbon emissions the major findings are as follows first through the internal and external validations our proposed method was proven to have better performance for capturing the ntl co2 correlation compared to the methods employed in previous studies su et al 2014 meng et al 2014 shi et al 2016 and for describing the spatial distribution and heterogeneity of co2 emissions second between 2003 and 2013 the total co2 emissions in the yrd more than doubled from 524 to 1243 million tons during which time the contribution from urban areas also increased significantly from 37 to 57 on a per capita basis the mean co2 emissions in urban areas increased from 6 to 15 tons cap in the last decade and large cities are usually more carbon efficient than small and medium cities third the urban carbon density increased sharply from 2003 to 2007 and became steady approximately 28000 tons km2 afterward spatially urban sprawl in the outer suburbs and the regional transfer of labor and resource intensive industries from the core cites to peri urban areas in adjacent cities led to rapid growth of urban carbon density in corresponding areas acknowledgments the work of ji han was supported by the national key r d program of china grant number 2017yfc0505703 the national natural science foundation of china grant number 41401638 ministry of education in china project of humanities and social sciences grant number 14yjazh028 shanghai philosophy of social sciences planning project grant number 2014bck001 hanwei liang acknowledges the funding from the startup foundation for introducing talent of nuist grant number 2243141501003 2015r003 and the shanghai key lab for urban ecological processes and eco restoration of china grant number shues2015a04 zhi cao would thank national key r d program of china grant number 2016yfa0602802 for financial support liang dong would appreciate the financial support from the project of smart industrial parks sips in china towards joint design and institutionalization grant number 467 14 003 as well as national social science foundation grant number 15zdb163 appendix a supplementary data the following are the supplementary data related to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 05 008 
26372,an accurate modeling of urban co2 emissions is important for understanding the dynamics of carbon cycle and for designing low carbon policies we develop an improved nightlight based method to model urban co2 emissions and investigate their spatiotemporal patterns differing from the previous methods in processing the pre modeling data we bring forward the existing co2 inventories from national and provincial levels to city level and correct the saturation and blooming problems of nightlight in modeling the correlation between nightlight and statistically accounted co2 emissions we highlight a panel data regression analysis that considers the spatiotemporal heterogeneity across cities and over time simultaneously eleven cities in yangtze river delta of china were selected for a case study testing our method the internal and external validations have proven the predominance of our proposed method for capturing the nightlight co2 correlation and for describing the spatial distribution and heterogeneity of urban co2 emissions keywords urban co2 emissions nighttime light panel data regression yangtze river delta 1 introduction cities are the main contributor to climate change since they are responsible for more than 70 of the global fossil fuel induced carbon emissions while occupying less than 2 of the earth s land area gurney et al 2015 their impacts are expected to grow due to continuous urbanization china has been urbanizing at an unprecedented speed and has become the largest carbon emitter in the world the proportion of residences qualifying as urban increased from 18 in 1978 to 55 in 2013 the number of prefecture level cities with population sizes over one million also expanded to 133 nbs 2015 consequently some chinese mega cities such as shanghai emitted more greenhouse gases than several countries did such as thailand and the netherlands world bank 2010 therefore the reduction in carbon emissions at a city scale becomes increasingly important and urgent a long term monitoring of urban co2 emissions is critical for understanding the dynamic patterns and drivers of the carbon cycle and for helping policymakers to design effective policies to mitigate climate change based on intergovernmental panel on climate change ipcc guidelines for national greenhouse gas accounting a growing number of scholars and research institutes have developed methodologies and tools for quantifying carbon emissions at a city scale including the international local government greenhouse gas emission analysis protocol iclei 2009 the ghg protocol wbcsd and wri 2004 the greenhouse gas regional inventory protocol carney et al 2009 and the sustainable energy action plan seap com 2010 however despite recent advancements in research aimed at estimating the dynamics of urban carbon emissions great challenges remain that are largely due to the lack of comprehensive consistent and comparable statistical data on energy consumption and human activities on a city scale furthermore all of the accounting methods based on energy statistics all treat the city as a homogenous unit but represent the dynamics of the urbanization processes such as the rapid sprawl of urban built up areas poorly albert et al 2015 edward and matthew 2010 as means of addressing the abovementioned challenges nighttime light ntl has been widely used as a useful proxy for economic output chen and nordhaus 2011 urban extent extraction xie and weng 2017 population estimation sutton et al 2001 electricity consumption cao et al 2014 and in use metal stocks liang et al 2014 due to its strong correlation with human activities and its availability at a high spatial resolution for most of the world beginning in 1992 recently this proxy has also been applied to estimate urban co2 emissions at different time and space scales for example oda and maksyutov 2011 downscaled national co2 emissions to global 1 km 1 km grids using nightlight as a proxy and separately allocated the point source emissions based on the global power plant database asefi najafabady et al 2014 built upon a previously developed fossil fuel data assimilation system ffdas and expanded the estimated 1 km gridded co2 emissions from a single year to multi years from 1997 to 2010 by combining with nightlight data gridded population and global power plant database su et al 2014 analyzed the linear correlation between co2 emissions and ntl in provinces and a limited number of cities of china based on a pool data regression analysis without considering the differences across regions and over time and predicted the carbon emissions in cities without direct energy data meng et al 2014 and shi et al 2016 developed a panel data regression model that took into account the city specific coefficient in capturing the relationships between province level energy related co2 emissions in china with ntl and downscaled the emissions to an urban or 1 km scale these studies have reached a consensus that the ntl can be used as a valid and useful proxy for downscaling statistically accounted co2 emissions to scale of interest for example pixel urban city and urban agglomeration and one can obtain estimates that are much more geographically consistent than energy consumption statistics through bypassing the reliance on statistical energy consumption data however at least three aspects can be substantially improved first a suitable carbon accounting method needs to be selected that is compatible with the available energy statistics the ipcc guidelines are based on detailed energy consumption data by sector and fuel type and are the most preferred and commonly used approach in the existing literature oda and maksyutov 2011 meng et al 2014 su et al 2014 shi et al 2016 however detailed energy data are not always available for cities taking china as an example the energy balance table is only available for the whole country provinces and a limited number of cities such as shanghai guangzhou and shenzhen in most cities only certain fragmented information about energy consumption in certain specific sectors e g industrial enterprises and households can be accessed through the city s statistical yearbook thus the scope and methodological complexity should be taken into account in choosing an appropriate accounting method so a more exact correlation between carbon emissions and ntl can be derived second the intrinsic saturation problem of ntl especially in the city centers needs to be addressed otherwise it may constrain ntl s further application and estimation accuracy internationally there are a great number of studies focusing on saturation correction various methods and indices have been developed generally by combining original ntl with other data sources such as normalized difference vegetation index ndvi and population density to increase the variation of ntl in urban cores meng et al 2014 ma et al 2017 however potential improvements still exist in relieving the saturation issue bennett and smith 2017 third the spatiotemporal heterogeneity across cities and over time must be considered the socioeconomic and geographical conditions usually vary in different cities and at different development stages even for the same city these differences may result in significant disparities in both the quantity and change patterns of carbon emissions this study aims to address the abovementioned deficiencies so as to better estimate urban co2 emissions in processing the pre modeling data we bring forward the existing co2 inventories from national and provincial levels to city level which to some extent breaks through the strong requirement on detailed statistical energy data and raise the accuracy of pixel level co2 estimation as it is downscaled from city level rather than from country and province moreover we correct the saturation and blooming problems of ntl by integrating time series ndvi and population density data which reduces the estimation error in the urban core and rural areas in modeling the correlation between ntl and statistically accounted co2 emissions we propose a panel data regression model which considers the spatiotemporal heterogeneities across cities and over time simultaneously eleven cities in the yangtze river delta yrd of china were selected as a case to test the method based on the same dataset our model was internally validated through a 2 fold cross validation process and compared with a pool data regression model and a panel data regression model that only considers city specific coefficient in addition our model was also externally validated with other studies both at city and pixel level by doing so the predominance of our improved method could be seen clearly finally the uncertainties limitations and potential improvements have also been discussed 2 study area and data 2 1 study area yrd is china s largest urban cluster wherein 11 cities were selected for case studies fig 1 there are three reasons for their selection the first is the important role that the yrd plays in socioeconomic development and carbon emissions it is one of the most rapidly urbanizing and wealthiest regions in china with the country s largest urban cluster covering 2 of the country s territory but contributing 20 and 12 to the total gdp and co2 emissions respectively in 2005 cai and xie 2007 the second reason is the relatively lower disparity in societal and natural conditions e g culture lifestyle income level and climate among the cities in the yrd compared to broader areas across china choosing a study area with less regional disparity may reduce the disturbance from spatial heterogeneity in correlation analyses between carbon and ntl the third reason was that the 11 case study cities reflect some generalities of economic structure and transport development of chinese cities as shown in table s1 in the supplementary material some cities in the yrd for example huzhou and suzhou have developed the secondary industry as their leading industry which is consistent with most cities in middle reaches of the yellow river the middle reaches of the yangtze river and in the northeastern regions however there are also cities in the yrd for example shanghai and hangzhou whose tertiary industry is the pillar industry it represents a widely existing situation in the eastern coastal and northern coastal cities in china for the transport sector there is a common phenomenon not only in the yrd cities but also across china that the number of civil automobiles has been increasing sharply at a mean annual rate of more than 13 between 2003 and 2013 nbs 2015 in summary owing to the important role lower disparity and representative generality we believe that the yrd is a good case study area for both testing our proposed method and increasing our understanding of urban co2 emissions in china 2 2 description of the data table 1 outlines the data used for the analysis which generally includes two kinds with a time range from 2003 to 2013 one is the spatial data including ntl produced by the defense meteorological satellite program s operational linescan system dmsp ols land use and land cover data classified based on landsat enhanced thematic mapper plus landsat etm images ndvi data based on moderate resolution imaging spectroradiomet modis from united state geographic survey http www usgs gov and population density data the nighttime light data version 4 have a spatial resolution of 1 km 1 km and its digital number dn values of the artificial nighttime light brightness from cities towns and other sites ranged from 0 to 63 these data can be accessed online from the national geophysical data center at noaa usa http ngdc noaa gov eog the monthly composite ndvi in china after splicing cutting and re projection was download from the resources and environmental sciences international scientific technical data mirror site computer network information center http www gscloud cn the land use data including 6 land types urban built up cropland water forest grassland others as well as the population density data were obtained from the data center for resources and environmental sciences chinese academy of sciences http www resdc cn the other data include the city level statistics for co2 accounting the direct energy consumption data by industry commercial and household sectors together with the vehicle fleet number of all eleven cities were compiled from the statistical yearbooks of each city jiangsu and zhejiang provinces and china overall 3 methodology generally the method that uses ntl to downscale statistically accounted co2 emissions to scale of interest for example pixel urban and etc is based on a hypothesis that the pixel level ntl s brightness has a positive correlation with the energy consumption related co2 emissions from the same pixel in our study in order to downscale city level accounted co2 emissions to urban scale there are basically two major steps the first is to process the pre modeling data which contains the city level co2 accounting and ntl data correction and urban extent extraction the second is to model urban co2 emissions through building the correlation between accounted emission and ntl and estimating pixel level emissions and aggregating them to urban scale 3 1 pre modeling data processing 3 1 1 city level co2 accounting and uncertainty quantification as defined in the corporate accounting and reporting standards which were developed by the world resources institute and the world business council for sustainable development wri wbcsd and are widely used by researchers the city level carbon inventories generally include three scopes of operational boundaries wbcsd and wri 2004 scope 1 is the direct emissions from activities that occurred within the physical boundary of a city such as emissions from factories vehicles and households scope 2 incorporates the emissions outside of a city but is related to energy use within a city which includes the electricity and heat produced elsewhere scope 3 is more comprehensive than scopes 1 and 2 it considers both the direct and embodied emissions during life cycle processes of products and services consumed in the city scope 3 includes the emissions from waste disposal air transport those embodied in food water and construction materials and others usually the basic carbon inventory for most cities refers to scope 1 and 2 emissions for example kennedy et al 2010 yu et al 2012 however those including scope 3 emissions are mostly reported in urban metabolism studies for example kennedy et al 2009 obviously the more scopes included in inventory accounting the more it requires extensive data processing time and expertise whittaker et al 2013 due to the features of appropriate coverage of carbon emission scopes and moderate difficulty in accounting the seap approach yamina et al 2014 is employed in this study since it fits within scope 1 and 2 emissions in which city level co2 emissions include direct emissions generated inside of a city boundary such as the combustion of coal and oil in industrial enterprises and the indirect emissions from electricity consumption and heating that are mainly produced outside of cities in the accounting the seap method estimates the co2 emissions from stationary and mobile sources in a city fig 2 the stationary emissions include those from energy consumption in industrial tertiary and household sectors whereas the mobile emissions are derived from public private and commercial transport sectors specifically the accounted stationary emissions aestationary are estimated by eq 1 1 ae stationary m n t ec m n t ncv m n ef m n where m n and t represent the investigated sector fuel type and year respectively ec represents the amount of energy consumption in metric tons ncv represents the net calorific value in megawatts per ton mwh t ef represents the co2 emission factor in tons co2 per megawatts tco2 mwh the details can be found in table 2 the accounted mobile emissions aemobile are estimated by eq 2 2 ae mobile v n t fn v n t vkt v n t fe v n t d n cr n 44 12 where v n and t represent the vehicle type fuel type gasoline or diesel and year respectively in this study the main types of fuels consumed by on road vehicles are gasoline and diesel fn is the fleet number which consists of public bus and taxi and private vehicles passenger car truck and motorcycle and the passenger car and truck are further divided into light medium and heavy duty classes vkt represents the average kilometers vehicle traveled fe is the fuel economy of vehicles in liters per kilometer l km d is the density of fuel type in kg l which is 0 732 for gasoline and 0 875 for diesel cr n is the carbon ratio of fuel type which is 85 5 for gasoline and 87 for diesel 44 12 is the molecular weight ratio of carbon dioxide to carbon due to the lack of statistics on the fleet number further divided by fuel type we make the following assumptions to enable the estimation 1 all the buses and heavy duty trucks use diesel 2 all the motorcycles taxis and light duty passenger vehicles consume gasoline for the rest of the vehicle classes the proportions of vehicle either using gasoline or diesel are estimated by following the literature yan and crookes 2009 han and hayashi 2008 liu et al 2013 the detailed data are listed in table 3 for the absence of officially published statistics on vkt and fe they are complied from literature and interpolated based on historical trends for those years lack of data the details are listed in table 4 and table 5 though the seap method employed in this paper is compatible with the ipcc guidelines and is applicable to chinese cities that have limited and fragmented energy data the results may be different from those generated with the ipcc guidelines since the former relies partially on city activities for example using fleet number as a proxy to estimate emissions from the transport sector whereas the latter mainly depends on detailed energy consumption statistics moreover even using the same seap method the parameter settings in different literature may also cause diverse results to quantify the uncertainties in co2 accounting a monte carlo simulation approach was employed this approach divides the uncertainties into two sources activity levels als and emission factors efs the als and efs which were assigned with a normal distribution and corresponding coefficients of variation cvs the ratio of the standard deviation to the average were fed into the monte carlo simulations for the als the normal distributions with cvs of 10 20 20 and 16 were recommended for industrial commercial household and transport sectors zhao et al 2012 the cvs for the efs are also derived from zhao et al s 2012 work according to a range of 95 confidence intervals the cvs for different sectors are listed in table s2 in the supplementary material a total of 1000 trials were performed to estimate the uncertainties in co2 emissions accounting 3 1 2 ntl data correction and urban extent extraction since the ntl is monitored with separate satellites dmsp satellites f10 f12 f14 f15 f16 and the dn values are incompatible for direct use we first removed the background noises and lights from the gas flares based on a method from elvidge et al 2009 then calibrated the data by using the methods proposed by liu et al 2012 however unlike the approach of liu et al 2012 who used a city located in northeastern china jixi city as the reference region we chose taizhou city a city in zhejiang province in yrd as the reference city to calibrate the ntl images the reason for the detailed calibration process is described in 1 calibration processes of dmsp ols data in yrd in the supplementary material because of the relatively coarse spatial resolution of the ols sensor the calibrated ntl data are still suffer from the saturation effect where lights detected in the center of large cities are too bright and cannot be distinguished elvidge et al 2007 besides the spatial extent of lighted areas is often larger than the developed areas blooming effect as the diffuse and scattered lights detected by ols sensor small et al 2005 the saturation and blooming effects may cause potential estimation errors especially in city center and suburban areas zhang et al 2013 based on the assumption of that vegetation cover and human activities are inversely spatial correlated an improved vegetation adjusted ntl urban index which combines ntl with time series ndvi and population density meng et al 2017 was employed to reduce the saturation and blooming problems after the calibration of original ntl dataset on the basis of the correction of ntl data the urban extent was extracted from a city boundary through a dynamic threshold method which has been used in a number of studies liu et al 2012 zhou et al 2014 here as shown in fig 3 urban is defined as a built up area where land has been developed and constructed on a large scale including basic municipal public facilities and where most of socioeconomic activities occur its boundary varies over time due to the urbanization process in contrast a city refers to a jurisdictional unit with a fixed administrative boundary urban populations are one of the most important driving forces of urban expansion sutton et al 2001 han et al 2009 thus to increase the efficiency of urban extent extraction we divided the 11 cities into 4 groups based on their urban population size they are the super megacity mega city large city and medium city following a standard division used by the state council of china 2014 then we determined an optimal threshold of ntl s dn value for each city group to separate urban and non urban extents using urban built up areas classified from landsat etm images in 2000 2005 and 2010 as references thresholds from 2000 were applied to the close years 2003 2004 values from 2005 were applied to 2005 2008 and thresholds from 2010 were applied to 2009 2013 table 6 with the support of the landsat data the accuracy could be much improved and those pixels dominated by water and vegetation could be excluded from the urban extent however the variation in the threshold value will definitely influence the final result to quantify the uncertainties of urban extent extraction based on dynamic threshold method on the result of urban co2 emissions we conducted a sensitivity analysis by measuring of the impact of threshold variation 1 5 and 10 increase or decrease on urban co2 emissions while keeping other parameters constant 3 2 urban co2 emission modeling 3 2 1 models for capturing the correlation between accounted co2 and ntl previous studies suggested the relationship between ntl and socio environmental factors such as population gdp and co2 emissions follows a power law relationship where a relative change in one quantity results in a proportional relative change in the other quantity sutton and costanza 2002 to determine an improved method that can accurately and reliably capture the relationship between accounted co2 emissions and ntl we developed the following three econometric models for comparison model 1 is based on pool data regression and considers no spatiotemporal heterogeneities across cities and over time which was also applied by su et al 2014 a double logarithm as illustrated in eq 3 enabled comparison between the explanatory and explained variables and ascertained the elasticity of the dn on carbon emissions values of α and β represent the intercept and slope coefficient ε is the error term 3 model 1 ln ae c i t y i t α β ln dn city i t ε i t to capture the influences of spatiotemporal heterogeneity among the 11 sample cities between 2003 and 2013 panel data regression models were proposed the subscripts i and t denote each city and year respectively model 2 incorporates a city specific coefficient μ in addition to dn which was also used in studies by meng et al 2014 and shi et al 2016 model 3 includes both the city specific coefficient μ and time variable σ as additional explanatory variables the values of i and t represent the specific city and year 4 model 2 ln ae c i t y i t α β ln dn city i t μ i ε i t 5 model 3 ln ae c i t y i t α β ln dn city i t σ t μ i ε i t 3 2 2 internal model validation using all the data for 11 cities covering 11 years 121 sample set in total we performed an internal validation of model 3 based on the following 2 fold cross validation processes and compared the result with that of model 1 and 2 using the same data and validation process i all the data were randomly divided to two groups ii data in each group were used in turn as training set for correlation analysis while the remaining group was removed based on the coefficients derived from the regression models the emissions for the remaining group were predicted iii the data in the removed group were used as references to calculate the difference between the predicted and observed values for each sample set iv the mean squares of the differences for all the sample set were calculated v by repeating the processes 1000 times through the monte carlo simulation the model accuracy could be assessed through the median of root mean squared error the lower the median is the more accurate the model will be 3 2 3 urban co2 emissions estimation based on the correlation between city level accounted co2 emissions and dn that were deduced from either eqs 3 and 4 or 5 the pixel level co2 emissions could be modeled using dn as a predictor as differences exist between the accounted and modeled emissions we used their ratio to further calibrate the co2 emissions at a pixel level which is shown in eq 6 6 ce urban i t k ce urban i t k k ae city i t me city i t me urban i t k where ce urban i t is the calibrated urban co2 emissions in city i for year t the value for k is the pixel within the urban area aecity represents the accounted city level co2 emissions mecity and meurban are the modeled co2 emissions at a city and urban level 4 results 4 1 model predictive power fig 4 presents the correlation between ntl and the accounted co2 emissions that were estimated by the three models and compares each model s predictive power that is measured by r 2 in model 1 the r 2 in both the sectoral and aggregated results was relatively high r 2 0 65 which indicates that a significant correlation between the accounted co2 emissions and ntl and at least 65 of the change in co2 emissions could be explained by ntl meanwhile the scattered points deviated from the fitting curve as also suggested the existence of regional diversity among the cities to eliminate the influence of spatial diversity on the carbon ntl relationship panel data analysis considering the city specific effect as explained in model 2 was conducted it was found that the r 2 in model 2 was significantly enhanced with values over 0 97 the results in model 3 justify the importance of the incorporation of both space and time differences as explanatory variables in estimation the r 2 values in model 3 are all improved over those in models 1 and 2 in the estimation of the relationship between the city s total co2 emissions and corresponding ntl the r 2 in model 3 demonstrated the highest value reaching 0 990 the city specific coefficient μ for each city as estimated by both model 2 and 3 are listed in table s5 in the supplementary material however for the time dummy variable σ in model 3 a strong linear correlation with the year difference for mobile emissions was found along with an inverted u shape curve for stationary and total emissions fig s5 in the supplementary material all of the r 2 of the fitting lines were fairly high which indicates a strong predictive power when using the fitting functions to estimate the time dummy variable σ in addition when looking at the correlations between ntl and the accounted co2 emissions from the secondary industrial tertiary industrial and household sectors as shown in fig s6 in the supplementary material model 3 still performed better than others as its minimum value of r 2 was 0 978 in sum model 3 which considers both the space and time differences across cities and over time was proven to have a better predictive power than the other two models in modeling co2 emissions using ntl as a predictor 4 2 model validation fig 5 shows the results of internal validation for three models using the same data set covering 11 cities from 2003 to 2013 it is found that the median of root mean square error rmse for 1000 times of two fold cross validations was 0 438 0 401 and 0 409 for stationary mobile and total emissions respectively in model 3 which was much lower than that in model 1 and 2 it suggests that model 3 which considers both the space and time differences is more accurate fig 6 shows the external validation of our improved model s results at city level through the comparison of our accounted city level co2 emissions with 95 confidence intervals in some specific cities with previous studies it can be clearly found that the results of the same city can have much difference in different studies for example our estimates are all smaller than those from wang et al 2013 as they accounted the carbon emissions from industrial processes while we didn t and our results are all larger than those from yu et al 2012 as we consider the emissions from heating while they ignored but anyway the changing trend and scale of all the results are similar with each other which suggests the validity of our estimation fig 7 shows the external validation of our results at pixel level by comparing our modeled 1 km pixel co2 emissions with those estimated by oda and maksyutov 2011 available at http odiac org dataset html in yrd in 2010 it is found that there was an extremely high emission point in oda and maksyutov s estimation the blue peak in fig 7d while not in ours it is because in addition to downscaling the national fossil fuel induced co2 emissions to 1 km pixel level using ntl as a proxy oda and maksyutov 2011 also separately estimated emissions from point sources using a global power plant database except the difference in point source emission it is clear that our result can better describe the spatial distribution and heterogeneity of urban co2 emissions as the fluctuation of our estimates along the line transact was more significant the red curve in fig 7d than oda and maksyutov s and the spatial pattern of our modeled urban co2 emissions fig 7c was closer to the distribution of urban extent fig 7a 4 3 urban co2 emissions in yrd based on the results from model 3 fig 8 illustrated the dynamic change in the calibrated co2 emissions from urban and non urban areas in the yrd cities as shown in fig 8a urban co2 emissions increased four fold from 194 million tons in 2003 to 714 million tons in 2013 with its share in total emissions continuously increasing from 37 to 57 when looking at per capita co2 emissions fig 8b although they increased at a similar rate of 10 per annum in urban and non urban areas urban areas had a lower value with an increase from 6 to 15 tons capita furthermore we also probed into the change in carbon density as shown in fig 8b the carbon density of urban land ascended sharply from 17000 tons km2 in 2003 27000 tons km2 in 2007 and then fluctuated around 28000 tons km2 thereafter which is 3 4 times larger than that of non urban lands the faster growth in urban co2 emissions 22 yr compared to that in urban expansion 9 yr from 2003 to 2007 is the main reason for the quick increase in carbon density in urban areas however after 2007 the increase in urban co2 emissions slowed at an annual rate of 9 and resulted in the stabilization of carbon density second we also investigated the spatiotemporal patterns of urban co2 emissions and illustrated the medium value of monte carlo simulation in fig 9 generally the urban co2 emissions demonstrated a significant increase and expansion over the whole yrd region however the spatial distribution of the increase was not even high emissions were found in the northern and northwestern parts of the yrd such as in suzhou and nanjing in contrast cities in eastern yrd had a relatively low urban carbon density fig 9a to observe the dynamic changes and spatial differences of urban carbon emissions clearly we calculated the annual growth rate of carbon density in each 1 km 1 km pixel from 2003 to 2013 and classified the change into five grades according to the natural breaking methods brewer and pickle 2002 as shown in fig 9b two types of significant growth were detected one was the newly built growth in the outer suburbs of each city where it was non urban in 2003 but became urban in 2013 another is the rapid growth in the peri urban areas adjacent to the urban centers notably the growth in the cities close to shanghai such as suzhou nantong and wuxi were particularly sharp moreover the change in urban co2 emissions per capita was also classified into five grades using the same breaking method fig 9c between 2003 and 2013 the fastest growth occurred in suzhou whose urban population was approximately 7 7 million and the population density was 770 cap km2 in 2013 but the per capita carbon increased 4 fold from 10 7 to 41 4 ton cap in contrast shanghai had an urban population of over 13 million and 3800 cap km2 of population density however its per capita urban carbon demonstrates the slowest growth from 8 3 to 12 3 ton cap this suggests that on a per capita basis large and compact cities are usually more carbon efficient than small and sprawling cities world bank 2010 5 discussions 5 1 uncertainties and limitations of our method the urban co2 emissions were calculated from the summation of gridded emissions within the urban extent since we assumed that relationship between 1 km grid level co2 emissions and ntl was consistent with that at city level thus the uncertainty of each gridded emissions value should be the same as that of the accounted city level emissions therefore the uncertainties in urban co2 emissions modeling should derive from the following two approaches first city level co2 emissions were accounted by the seap approach based on a monte carlo simulation the mean uncertainties with 95 confidence intervals of our accounted co2 emissions from a stationary source mobile source and city total for the 11 cities between 2003 and 2013 were 17 5 to 18 0 18 0 19 7 and 17 5 to 18 0 respectively as a comparison to other studies the uncertainty in china s co2 estimation conducted by gregg et al 2008 ranged from 15 to 20 second the urban extent was extracted by a dynamic threshold approach by comparing our estimated urban extent in some of the selected cities for each city group in 2005 2010 and 2013 with the land use and cover data classified from landsat images that were obtained from the data center for resources and environmental sciences at the chinese academy of sciences http www resdc cn and the global institute for urban and regional sustainability http www giurs com the reliability of the dynamics of urban extent could be measured by kappa and overall accuracy oa indices which are two widely used coefficients to assess the classification accuracy of remote sensing images cohen 1960 fitzgerald and lees 1994 as illustrated in fig s7 fig s9 in the supplementary material the average kappa and oa were 0 37 and 89 17 in 2005 0 38 and 88 41 in 2010 and 0 38 and 84 53 in 2013 suggesting that the dynamic threshold method can capture the change in urban extent with a relatively high accuracy fig 10 shows how every 1 5 and 10 increase or decrease of the dn threshold will affect the final urban co2 emissions we found that the sensitivity of the urban co2 emissions to dn threshold differed from years and increased when the variation in threshold became larger specifically every 1 change in the dn threshold would contribute to 4 1 2 5 changes in urban co2 emissions a 5 threshold variation would cause 7 9 9 7 changes in emissions when the threshold variation increased to 10 the urban co2 emissions would change by 15 8 20 1 which was as large as the mean uncertainties in the city level co2 accounting this suggests that urban extent extraction was as important as city level co2 accounting for reducing the uncertainties of urban co2 modeling in the future more emphasis should be put on improving the quality of statistical energy data raising the accuracy of carbon emissions factors in study area and improving the reliability of the extracted urban extent some limitations still remain and require further research first only three years of landsat images were used as a reference in the urban extent extraction which limits the reliability of the results second the significant correlation between city level co2 emissions per sector and ntl as shown in fig 4 and fig s6 in the supplementary material is only from the perspective of statistical analysis it may not mean there is real correlation between pixel level ntl and co2 emissions per sector without the detailed information about the land type of and the human activity upon the pixel our method can only represent mixed co2 emissions from different sectors at pixel level third the external model validation at pixel level is the comparison of our results with other researcher s model results rather than the comparison with the real ground based co2 emission monitoring data in the future possible improvements could include the following aspects a collecting more landsat images to raise the accuracy of urban extent extraction b combining more detailed land use and human activity data for example high resolution land use and cover maps and point of interest data of infrastructure distribution to obtain more information about urban pixel so that pixel level co2 emissions per sector could be estimated c conducting a true external validation by running an atmospheric transport dispersion model based on our modeled 1 km pixel level co2 emission data and comparing results with ground based co2 monitoring data for example the tansat carbon satellite data which has a 1 2 km spatial resolution and was released in 2017 http satellite nsmc org cn portalsite data satellite aspx 5 2 potential uses based on the proposed method for estimating urban co2 emissions several potential uses can be derived they include but are not limited to the following the proposed method can serve as a decision support tool for updating and investigating the dynamic changes in urban co2 emissions at a relatively high spatial resolution they can also be used as input for carbon cycle and climate simulation models the exploration of urban co2 emissions and their variation provides insights for follow up analysis combined with the spatial data of socio economic variables e g gdp population infrastructure distribution and land use cover the socioeconomic and biophysical factors that drive urban co2 change could also be quantified data availability and quality are recognized as the major obstacles for accurately estimating the spatial and temporal patterns of co2 emissions the pixel level co2 estimation by downscaling from city level co2 inventory rather than from nation and sub national regions as shown in our case study provides a possibility to address this gap though it requires not only the statistical energy data but also the activity data for example vehicle fleet number we believe it is feasible to collect similar data for other chinese cities as well similar database for cities in other countries should also be explored via international collaboration 6 conclusions in this study we propose an improved method for quantifying urban co2 emissions using ntl as a proxy in contrast to previous research we bring forward the existing co2 inventories from national and provincial levels to city level correct the saturation problem of ntl and considers the spatiotemporal heterogeneities across cities and over time in modeling the ntl co2 correlation although the correlation parameters derived in the yrd were city specific and cannot be directly used for other cities regions in china the developed method can be applied to other areas even if they do not have a detailed energy balance table furthermore our method is proven to better capture the correlations between ntl and co2 emissions from different sectors that is industrial tertiary household and transport sectors or for city aggregates compared to the methods reported in previous studies our method is a supplement to existing approaches for modeling urban co2 emissions moreover it helps understand the spatiotemporal dynamics and causal factors of urban carbon emissions the major findings are as follows first through the internal and external validations our proposed method was proven to have better performance for capturing the ntl co2 correlation compared to the methods employed in previous studies su et al 2014 meng et al 2014 shi et al 2016 and for describing the spatial distribution and heterogeneity of co2 emissions second between 2003 and 2013 the total co2 emissions in the yrd more than doubled from 524 to 1243 million tons during which time the contribution from urban areas also increased significantly from 37 to 57 on a per capita basis the mean co2 emissions in urban areas increased from 6 to 15 tons cap in the last decade and large cities are usually more carbon efficient than small and medium cities third the urban carbon density increased sharply from 2003 to 2007 and became steady approximately 28000 tons km2 afterward spatially urban sprawl in the outer suburbs and the regional transfer of labor and resource intensive industries from the core cites to peri urban areas in adjacent cities led to rapid growth of urban carbon density in corresponding areas acknowledgments the work of ji han was supported by the national key r d program of china grant number 2017yfc0505703 the national natural science foundation of china grant number 41401638 ministry of education in china project of humanities and social sciences grant number 14yjazh028 shanghai philosophy of social sciences planning project grant number 2014bck001 hanwei liang acknowledges the funding from the startup foundation for introducing talent of nuist grant number 2243141501003 2015r003 and the shanghai key lab for urban ecological processes and eco restoration of china grant number shues2015a04 zhi cao would thank national key r d program of china grant number 2016yfa0602802 for financial support liang dong would appreciate the financial support from the project of smart industrial parks sips in china towards joint design and institutionalization grant number 467 14 003 as well as national social science foundation grant number 15zdb163 appendix a supplementary data the following are the supplementary data related to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 05 008 
26373,environmental data are growing in complexity size and resolution addressing the types of large multidisciplinary problems faced by today s environmental scientists requires the ability to leverage available data and information to inform decision making successfully synthesizing heterogeneous data from multiple sources to support holistic analyses and extraction of new knowledge requires application of data science in this paper we present the origins and a brief history of data science we revisit prior efforts to define data science and provide a more modern working definition we describe the new professional profile of a data scientist and new and emerging applications of data science within environmental sciences we conclude with a discussion of current challenges for environmental data science and suggest a path forward keywords data science environmental science data driven modelling 1 introduction data science is the science of dealing with data naur 1974 in recent years we have observed an increasing popularity of data science methods that seem to be in the focus of many organizations including those interested in a better comprehension or management of environmental systems data science is already widely used in business to design successful strategies and policies and the economic sector is facing a significant transformation as a result of the penetration of data driven innovation in the business core we believe that a similar transformation is underway within many scientific disciplines among them those within the environmental sciences to investigate the benefits that can be realized through use of appropriate data science approaches in this paper we analyze the origins of data science as a new discipline that is diverse enough to be applied to any domain including those within the environmental sciences the potential of data science to advance our knowledge of the laws governing complex environmental phenomena is enormous the technological development requisite for collecting the volume and resolution of data required to study these phenomena is mature but classical data analysis methods are in many cases insufficient to cope with the size speed and diversity of information sources providing evidence under the variety of forms text videos audio recordings numbers images that require global analysis and local tuning to elicit the hidden relevant knowledge to support higher level decision making many investigators are already investigating how data science can address this deficiency we present the contributions of data science together with an analysis of the new specific skills associated with its inherent multidisciplinarity as there is no common definition of data science in the paper we present several definitions that have been used in the past and a propose a new conceptualization of what data science means a discussion is also provided regarding its contact points with other emerging disciplines such as big data analytics emerging opportunities for new applications in environmental sciences are described while not an exhaustive description of the opportunities for data science in environmental science applications a wide perspective in the area is provided being an emergent field a number of open issues envisage fertile areas for new research in the near future the paper also provides some highlights challenges and trends with the aim to push the development of the data science field in general and in environmental sciences in particular where it can be of help the structure of the paper is as follows in section 2 the origins and a brief history of data science are provided in section 3 the added value of applying data science techniques to real problems using real data is discussed section 4 highlights the new skills required to become a qualified data scientist and the need to develop specific new curricula to provide appropriate training section 5 provides a more modern contemporary view of data science and section 6 provides a general overview of how data science is being applied in environmental sciences section 7 identifies the main current challenges in the area section 8 provides a concluding discussion 2 origins and a brief history although data science is a relatively new discipline the term data science is much older than might be expected it is worth noting that there is no clear and agreed upon definition of the term data science this lack of clarity appears in the first use of the term by naur in 1960 sundaresan 2017 naur used the term to mean data processing in the computer science sense however it has also been used at times as a substitute name for the field of statistics or at the very least applied statistics naur refined his earlier definition to data science is the science of dealing with data while the relation of data to what they represent is delegated to other fields and sciences naur 1974 in the same period in the context of statistical sciences there was also a process by which data became the center of interest of the discipline indeed john w tukey 1962 had already envisaged the need for statistics to move its focus from inference to data analysis as an empirical science for a long time i thought i was a statistician interested in inferences from the particular to the general but i have come to feel that my central interest is in data analysis intrinsically an empirical science the development of computer science near that time was opening an opportunity to this end in the late 1970s tukey 1977 published exploratory data analysis promoting a new approach to statistics where more emphasis needs to be placed on using data to suggest hypotheses to test exploratory data analysis and confirmatory data analysis can and should proceed side by side in 1977 the international association for statistical computing iasc http iasc isi org about iasc2 was established as a section of the international statistical institute it is the mission of the iasc to link traditional statistical methodology modern computer technology and the knowledge of domain experts in order to convert data into information and knowledge iasc 1977 rizzi and vichi 2006 davenport and dyche 2013 in 1996 the international federation of classification societies ifcs used for the first time the term data science in the title of their biennial conference data science classification and related methods aligned with this approach jeff wu seems to have been the first to ask whether statistics should change its name to data science in his talk entitled statistics data science which was given first in november 1997 as the inaugural lecture for his appointment to the h c carver professorship at the university of michigan in 1998 this was his first p c mahalanobis memorial lecture in honor of professor mahalanobis the founder of the indian international statistical institute iisi and was archived by wu 1999 in 2001 william s cleveland called for establishing data science as a field to enlarge the major areas of the field of statistics because the plan is ambitious and implies substantial change the altered field will be called data science cleveland put the proposed new discipline in the context of computer science and the contemporary work in data mining one and two years later the first journals in the area were launched the data science journal and the journal of data science respectively these events are largely why the term data science is currently understood by many people to be closely related to data mining and big data analytics rather than the original sense in which the term was used data science has also been approached from the perspectives of artificial intelligence ai and machine learning in the early 1980s there was a clear idea of the importance of using data as the main source of knowledge extraction in 1985 douglas fisher and bill gale founded the artificial intelligence and statistics society http www aistats org past html with the aim of facilitating interactions between researchers in ai and statistics nearly ten years later cheeseman and oldford stated we feel that there is great potential for development at the intersection of artificial intelligence computational science and statistics cheeseman and oldford 1994 in 1989 gregory piatetsky shapiro organized the first knowledge discovery in databases kdd workshop as part of the international joint conferences on artificial intelligence ijcai world conference it soon 1995 became an independent series of conferences acm sigkdd fayyad et al 1996 edited the seminal book advances in knowledge discovery and data mining introducing new techniques and tools for the discovery of knowledge from data as a response to the urgent need to address data flooding they defined the knowledge discovery from databases kdd framework as the process of the non trivial identifying of valid novel potentially useful ultimately understandable patterns in data in the kdd approach data mining was considered a specific data exploitation step one year later the first international journal data mining and knowledge discovery was launched around the mid 1990s data science started to be seen as a new business opportunity at that time most companies were aware of having large volumes of collected data that were not properly analyzed berry 1994 in many current contexts data science can be understood from a business perspective as the process of discovering what we do not know from data it enables us to get predictive actionable insight from data creating data products with business impact communicating relevant business from data and building confidence in decisions that drive business value somohano 2013 more recently data science has started to be seen as an enabler that has the potential to transform scientific inquiries mattmann 2013 identified algorithm integration and data stewardship as two components of data science that are essential for managing the data deluge in earth and space sciences and other fields like physics and genomics mattmann described algorithm integration as including model integration in scientific workflows and interfacing with data repositories and infrastructures mattman also called for integrating data archival with data processing facilities and in the same work highlighted the diversity of science data that involve many formats file types and conventions in fact data science is often based on the analysis of datasets resulting from a previous conversion of videos audio recordings signals data streams or websites into sets of relevant and or sufficient indicators by means of feature extraction techniques thus finding the relationships between several sources of heterogeneous data together and identifying complex hidden patterns useful for decision support in the last several years data science has been challenged to make the next steps in science by enabling in silico scientific discoveries from vast amounts of data where computers are enabled to identify and prove hypotheses not constructed by scientists for example agarwal and dhar 2014 describe the explosion of opportunities for scientific inquiry with readily available large and complex datasets and suggest that computers are now powerful enough to not only verify hypotheses but also to suggest new theories though such claims may seem ambitious advances in machine learning artificial intelligence data integration and stewardship seem very promising similarly caffo et al 2016 warn that the big data data science hype will flame out if it is only about data and not science they argue that data science is only useful when the data are used to answer a question expressing views similar to those that have been expressed by others that there are limits to what can be accomplished using data science methods and tools and that a balance must be struck between newer and more traditional scientific methods other authors lauro et al 2017 characterize data science as the process by which data are transformed into actionable knowledge to perform predictions as well to support and validate decisions lauro uses a metaphor such that computer science represents the language of data science statistics the logics of data science and domain expertise constitutes a catalytic element in the absence of which the transformation cannot be achieved 3 the added value of data science the development of data science promoted a new concept in decision making in general including business where decisions are data driven and the added value to organizations either institutions or companies is not more technology nor capital but information where data is considered to be a primary source of knowledge this transformation is not restricted to business fields and the value that data science processes can add to our understanding of complex phenomena is widely recognized davenport claimed that instead of competing on traditional factors companies are beginning to employ statistical and quantitative analysis and predictive modeling as primary elements of competition davenport and harris 2007 the climate corporation whose motto is data services for yield maximization was acquired by monsanto in 2013 for nearly 1 billion u s monsanto john deere and dupont pioneer are among many companies scrambling to help agricultural producers do more with their exploding volumes of data noyes 2014 and the value of data science for agricultural applications is clear in the obvious market demand for these services in one specific example application that demonstrates this potential value elarab et al 2015 coupled high resolution imagery from an unmanned aerial vehicle remote sensing platform with machine learning algorithms to estimate chlorophyll concentration in crops as an important biophysical parameter for use in precision agriculture their techniques enable farmers to assess the heterogeneity of the plants in their fields at fine resolution in space and time aiding farmers in targeting management actions accordingly e g effectively targeting application of fertilizers or water only where they are needed the economic and environmental implications of enabling this type of precision agriculture could be significant although the value of informed decision making was understood in the 1950s luhn 1958 after the emergence of knowledge discovery in databases fayyad et al 1996 informed decision making became more popular brynjolfsson et al 2011 several authors discuss the relevance of data for decisions for example craig mundi the head of research and strategy at microsoft stated managed well data can unlock new sources of economic value we are in front of a nascent data centered economy cukier 2010 however despite the excitement about the explosion in available data and data science methods the reality is that most available data are under exploited resulting in a loss of potential for decision making in 2004 hammond formulated what he called the fact gap the disconnect between data and decisions hammond 2004 to refer to this phenomenon of low consumption of available data at the decision making level data science is an emergent discipline that focuses on the intensive consumption of available data to extract decisional knowledge relevant for informed decision making and to bridge hammond s fact gap in the environmental sciences united nations agenda 21 has already since 1992 included a section on bridging the data gap highlighting that the gap in the availability quality coherence standardization and accessibility of data between the developed and the developing world has been increasing seriously impairing the capacities of countries to make informed decisions concerning environment and development un 1992 4 data scientist a new professional profile it seems clear that the skills required to perform data science point to a new professional profile and claim that academia start to design new curricula to train this new type of professional hal varian google s chief economist when interviewed by mckinsey referred to the scarce ability to extract wisdom from data cukier 2010 in many situations data scientists are expected to have a broad set of skills eloquently defined by josh wills 2012 a data scientist is a person who is better at statistics than any software engineer and better at software engineering than any statistician acquisition of the appropriate skills for effectively applying data science is critical in order to ensure a solvent extraction of the right value contained in data some authors have reported how in the absence of proper training several data scientists have extracted contradictory conclusions from a single dataset by performing different analytical procedures baeza yates 2017 silberzahn et al 2015 davenport and patil in their 2012 article in the harvard business review provocatively entitled data scientist the sexiest job of the 21st century argue that skills required for being an effective data scientist go beyond statistical or analytical capabilities and should include storytelling with data and excitement with potential for breakthroughs in the particular domain other authors have also described the new profile of a data scientist and the specific skills required to do data science properly e g sooraij shah 2013 contemporarily the shortage of talent with required skills for capturing the whole potential of data was reported by mckinsey in 2011 by quantifying a shortage of 140 000 to 190 000 data scientists by 2018 manyika et al 2011 in his keynote at the campus party europe in september of 2013 a s pentland head of medialab entrepreneurship mit said there are too few data scientists in the world and education needs to change in order to maximize the true potential of data science palmer 2013 at that time 62 of executives realized that the lack of data scientists was causing a real problem one poll survey for soft supplier teradata mckenna 2013 the shortage is especially severe in the u s for example 80 of new data scientist jobs were not filled within the year 2010 2011 harris et al 2014 in the u s the demand for data science and analytics jobs is projected to grow by 15 between 2015 and 2020 with the highest rate of growth rate of 28 expected for the specific job title of data scientist markow et al 2017 the lack of data scientists that can do high quality work with available data contributes to the fact gap burns 2017 and this shortage still persists at the time of this writing business com 2017 in spite of the shortage of skilled professionals data science has become a relevant profession in the last few years in 2013 venture beat reported data science as the second best new job in america in a more recent report they show data scientist as the number one best job in america venturebeat 2017 and piatetski shapiro 2017 reported on kdnuggets that glassdoor again ranked data scientist as the no 1 job in usa and 5 of the top 10 us jobs are related to analytics big data and data science a ranking that was repeated by glassdoor in 2018 as a response to this reality the last few years have seen recommendations and guidance for integration of data science into degree programs e g association for computing machinery 2017 many universities have created qualifications in data science or in some of its related areas such as data analytics business intelligence and so on these new programs tend to be associated with the computer science or mathematics statistics disciplines while earth environmental and life science curricula have not caught up to those of computer science and mathematics statistics in the area of data science it is becoming more common to find specific courses or discipline specific focus areas on data science methods in additional scientific disciplines 5 a modern view of data science so far we have seen that there is no clear agreed definition of the term data science but the intrinsic multidisciplinary nature of the field seems to be clear conway s data science venn diagram fig 1 conway 2013 provides a useful conceptualization for how coding skills conway calls them hacking skills math and statistics knowledge and domain science expertise conway calls this substantive expertise come together to enable data science domain expertise combined with math and statistics knowledge is where most traditional research is conducted coding with math and statistics knowledge may lead to insight through machine learning using data but without driving scientific questions and hypotheses that come through domain expertise mechanistic or process understanding may be limited coding skills combined only with domain expertise may lead to incorrect interpretation of results without knowledge of math and statistics the danger zone in conway s diagram it is only at the intersection between the three elements that data science can be most effective indeed multidisciplinarity has become a main pillar of data science in recent applications it is still possible to see the relationship between data science and computer science statistics data mining and big data analytics essentially data science is broader than these fields but can make use of all of them for example data science does not necessarily concern itself with the size of the data being processed but rather with transforming data into added value for the end user and extracting relevant knowledge from data coming from complex phenomena where data are prolific data science uses techniques from big data analytics to perform parts of its workflow similarly the data mining process works well for the overall development of some data science applications robust statistical methods are needed in most data science applications but the potential heterogeneity complexity and size of data can require innovative solutions from computer science trying to precisely define the relationship between these fields is difficult as their definitions and the boundaries between them are not altogether clear even the terms used to describe data science methods and the methods themselves are often confusing as discussed in section 2 from a practical perspective we are very much in agreement with lauro et al 2017 that the novelty of data science is identified to be the role played by knowledge which is definitely integrated into the process including interpretation issues with the main purpose to give meaning to data in the current context we consider data science as the multidisciplinary field that combines data analysis with data processing methods and domain expertise transforming data into understandable and actionable knowledge relevant for informed decision making thus contributing to bridge hammond s fact gap this modern view and its associated techniques and applications are enabling data scientists to synthesize value added products from diverse datasets that would be otherwise impossible to obtain data science often requires the ability to overcome data complexity and the limitations of classical statistics and machine learning techniques for example dealing simultaneously with heterogeneous data sources e g videos text or streams or coping with non independencies non normalities and few technical hypothesis on variable s distributions when required 6 data science in environmental sciences much of our discussion up to this point has involved mathematics statistics computer science and applications of data science in general including impact in business however there is no shortage of opportunity for applications in environmental sciences gibert et al 2008 indeed as the size and complexity of environmental datasets continue to grow the demand for data science in environmental applications is increasing data science is able to add value to environmental sciences in many different ways data science is at work in examining interpreting and deriving useful and actionable information from environmental sensor data streams athanasiadis and mitkas 2007 reis et al 2015 bifet et al 2017 these datasets produced by in situ and remote sensors of many different types span the environmental sciences from climate and weather observations from satellite and ground based sensors e g hill et al 2011 to air quality sensors e g wiemann et al 2016 hydrologic and water quality sensors installed in aquatic environments e g wong and kerkez 2016 water quality sensors in wastewater treatment processes e g corominas et al 2017 sensors used to track the movement and behaviors of biological organisms e g kranstauber et al 2011 ground based sensors for detecting and quantifying the magnitude of earthquakes and geological events and many other applications the size of these monitoring networks and the datasets they produce is growing as the cost of sensors and related systems is falling producing a greater need for analysts capable of managing the datasets produced and assimilating them with simulation models and other applications sometimes evolutionary and population based algorithms are used to extract relevant parameters of a reference phenomenon based on these sensor data afshar et al 2015 describe an application to water resource management levasseur et al 2008 uses genetic algorithms in soil applications and nagesh kumar et al 2006 describe an application for optimal reservoir operation for irrigation of multiple crops another quickly growing application is that of interpreting data collected from aerial drones sailing or aquatic drones and satellite remote sensing roelofsen et al 2014 elarab et al 2015 gauci et al 2018 remote sensing data of these types are becoming more and more vital for accurate and broad environmental monitoring the scope of these applications is simply too broad for individual sensors or ground based stations to be effective but the volume of data produced by remote sensing drones and satellites can dwarf most other environmental datasets requiring specialized tools techniques and training for data analysts many new applications at the nexus between water and energy are generating high spatial and temporal resolution datasets using smart metering technologies that collect observations of water or power usage at frequencies on the order of seconds or even more frequent to support applications such as end use disaggregation demand estimation and demand management cominola et al 2015 gurung et al 2015 sophisticated algorithms and data management techniques are required for these applications and without them high resolution data can be an unnecessary barrier for water and power system managers who have traditionally used metering data for monthly billing purposes horsburgh et al 2017 preparing datasets designed to support high performance large scale e g continental scale modeling and analyses is another area data science is being used within the environmental sciences these datasets are typically derived from existing geospatial data but are organized over large spatial scales using sophisticated data modeling to provide the backbone for environmental models examples include the national hydrography dataset plus nhdplus being used as the hydrologic network underlying the continental scale national water model in the u s another example includes ongoing efforts by the united states geological survey to link aquatic monitoring sites and features to the network of streams in the u s elfie https opengeospatial github io elfie json ld using concepts from the open geospatial consortium s hyfeatures specification dornblut and atkinson 2014 data science is needed in the design and preparation of these types of datasets to enable high resolution and high performance use model outputs from high resolution long temporal period climate simulations represent another class of environmental data requiring specialized skills for analysis analysts and modelers working with global or continental scale models are often faced with hundreds of terabytes of model generated results that must be reduced to useful products that can be used for downstream analyses ashraf vaghefi et al 2017 the volume of data involved is challenging from not only the data use perspective but also from the perspective of provisioning basic storage hardware and software required for short and longer term uses of these massive datasets in many contexts integrating data and domain knowledge for obtaining better models requires application of data science methods in this sense uusitalo 2007 discusses the use of bayesian networks in environmental modelling blattenberger and fowles 2017 have also used them to evaluate avalanche danger and gibert et al 2010a used prior knowledge to bias a clustering process in waste water treatment plant applications combining disparate datasets from multiple scientific domains for synthesis studies and to create new derived datasets can also be enabled using data science vitolo et al 2015 provide a broad review of many of the techniques and technologies that have been applied in enabling data integration particularly via the internet this is a problem of data fusion where complexity and heterogeneity in data can be more challenging than size nativi et al 2015 often combining datasets from different sources in a single analyses requires specialized skills in data management programming and visualization that are central to data science as in porter et al 2014 geospatial machine learning methods e g kanevski et al 2008 are also helping in this area mccord et al 2017 hengl et al 2017 another emerging area in environmental data science involves linking human health to environmental conditions especially in the cases of natural disasters such as hurricanes flooding or earthquakes e g klise et al 2017 major hurricane events seriously impact the availability of power potable water and other basic human needs relatively little is currently understood about the short and longer term effects events like these have on human health because it is only in the past few years that more detailed datasets have been available for characterizing environmental conditions during and following events along with health impacts that can be attributed to these conditions exposure gómez losada et al 2014 schlink et al 2016 is another new issue that requires attention the impact of air pollution on public health is currently a focus of interest and modelling air quality on the basis of data streams provided by smart sensors currently available in many cities is crucial to understand how exposure affects public health but also to perform real time air pollution forecasting to run preventive and protective healthcare plans for citizens health data science linking environmental and human health data raises multiple challenges first human health data are sensitive restricted and must be anonymized second the format vocabularies and syntax of these two types of data are very different and require the combination of different resources to deal with them simultaneously properly linking environmental and human health data requires unique and careful approaches for both data management and analysis 7 current challenges and trends in environmental data science while the use of data science techniques to enhance research in the environmental sciences is rapidly growing it is not without significant challenges that must be overcome in this section we describe some of these challenges and while likely not exhaustive the list we provide here clearly illustrates that there is much room for improvement challenge 1 shortage of trained data science experts to cover real demand as mentioned many authors identify a relevant shortage of properly trained data scientists to cover the real demand new training programs are being deployed at different levels of academy to increase the number of formally trained data scientists however it is unlikely that the number of personnel formally trained as data scientists will be able to keep up with the growing demand from so many different disciplines in which they are now employed from the environmental science perspective it will remain difficult to attract the best and brightest to work on environmental problems when salaries for data scientists are so much higher elsewhere this is common with environmental informatics as already pointed out by swayne 2003 challenge 2 lack of data science skills within environmental science curricula applying data science approaches to environmental systems and data requires all three skills described by conway 2013 data programming and data modelling skills in general including statistical and machine learning approaches and domain knowledge it will continue to be difficult to find individuals that can effectively do all three with sufficient expertise collaborations between environmental scientists and computer scientists can potentially address this need but are difficult to foster because the needs interests and expertise of collaborators are not always aligned even though curricula in environmental engineering and the environmental sciences are evolving to address this they have not totally caught up with the need yet and sustained efforts are required to resolve this deficiency developing resources within open environmental data repositories accompanied with problem descriptions can provide materials for introducing realistic practicums in education that helps in better training new scientists challenge 3 methodological gaps for designing data science processes in real applications the number and variety of environmental problems suitable for application of data science approaches is high it is currently the responsibility of the data scientist to translate the environmental problem into a data science workflow that encompasses both the goals of the problem and available data this includes identifying the proper preprocessing data mining and knowledge production methods and their proper sequencing and interactions to create a data science workflow that will advance the project gibert et al 2016 provides some guidelines to design preprocessing steps there are currently no established guidelines or standards for how to design data science workflows leaving subjectivity in their design and difficulty in comparing results where different workflows may have been used to address the same or similar problems research is needed in this respect to build a conceptual framework with standard data science processes providing answers to a certain kind of problems challenge 4 guidelines to map families of environmental problems with prototypical data science processes that help in environmental application from a structural point of view there are some commonalities in some families of environmental problems that fit well with a similar kind of data science process as an example analyzing the effect of pollutants may require similar analytics for air pollution or water pollution analogously predicting the survival of certain protected species might require similar methods for forest fauna and freshwater fish even if the related environmental systems are radically different the first example may entail in both cases modelling continuous multi response variables several coexisting pollutants that are never independent and develop in a spatiotemporal space whereas the latter example is in both cases about discrete prediction methods with a single counting response variable very little work has been done on finding families of environmental problems that share structure and building guidelines to map them into standard environmental data science workflows a deep analysis of the different environmental problems suitable for data science is required to produce insights about them and how they map to standard data science solutions providing environmental scientists with tools to help them identify the structure of an environmental problem would be of great benefit to do these mappings and opens the door for more effective application of data science techniques within the environmental sciences challenge 5 data quality and dealing with uncertainty in data data quality is currently one of the hotpoints of the data science process as environmental data always includes mistakes or biases the validity of data science processes that use these data becomes limited at best or even dangerous under worst case scenarios enabling the production of potentially incorrect conclusions that may lead to decisions with dramatic consequences as an example in 2012 the secretary of environmental protection in pennsylvania told congress that there was no evidence the state s water quality had been affected by fracking in august 2014 the same department published a list of 248 incidents of damage to well water due to gas development this discrepancy was caused because six regions in the state had missing data in 2012 and data collection was not the same among regions barrett and greene 2015 with the diversity of information sources combined in data science projects often including videos audio recordings images and real time data streams guaranteeing the quality of data requires development of stronger methodologies that go beyond the current catalogs of unconnected preprocessing operations provided by existing software along with the need for more research in data preprocessing approaches gibert et al 2016 there is also a need for efforts to improve technologies behind smart sensors to reduce failures in measurements eliminate noise and increase the quality of data transmissions in spite of the need for improvement in these areas it has to be noted that data science offers a certain robustness with regard to noise that other approaches cannot achieve more recent approaches like deep learning have shown reasonable performance with noisy data pointing that data science might create useful results even from noisy or lower quality data the potentials in this direction is still under explored challenge 6 privacy and security guaranteeing privacy of personal data circulating in the internet from sources ranging from smart sensors to communication networks data centers or the cloud is another critical issue that requires attention and limits the scope of applying data science to all available environmental data for example federal laws in the u s and eu directives in europe govern the collection management and disclosure of personally identifiable information including health and medical records or farmer s data yet there are many potential opportunities for combining this type of data with those from environmental sensors or samples to learn more about the effects of exposure to environmental conditions e g reis et al 2015 another example is privacy preservation in creating crowdsourced noise maps drosatos et al 2014 challenge 7 methods to choose pertinent correct sufficient and non superfluous data for analysis as the volume and heterogeneity of available data continues to grow no clear criteria have been established to assess which out of all available data is required for an analysis or whether available data is representative enough of the whole target population big is not necessarily linked with sufficient or unbiased and more work is required to provide guidelines for making these decisions this necessarily requires clarification of which possible biases different data sources might include by construction furthermore as the volume speed and diversity of collected data is growing it is hardly possible for scientists to keep on manually preprocessing data i e performing tasks such as data linking cleaning and integration data sharing and discovery need to be performed using methods that make it possible for machines to tackle these tasks with semantic interoperability rather than human experts additionally clear policies on use of big or small data for specific environmental applications is needed challenge 8 need for development of integral data mining methods there is a clear need for data mining methods able to cope with heterogeneous data that might include traditional databases data derived from environmental samples smart sensor data or data from supervisory control and data acquisition scada systems data with intrinsic uncertainty like georadar data data streams geospatial datasets images sounds and free text to provide an integrated overview of a complex system these methods must also address any structural complexities involved e g high order interactions multigranularity spatiotemporality etc challenge 9 guidelines to choose the right analytics method for a given problem challenges 5 to 7 are related to the data used in the data science process next step in the process is analyzing data with suitable methods selection of the proper methods to effectively process data can be equally difficult not much work has been done to establish consensus about which analytics methods are effective and appropriate for specific applications gibert et al 2010b as such there are few clear guidelines for analyzing a certain kind of data when addressing a certain kind of question we have even seen how analyses of the same dataset can provide contradictory conclusions when analyzed by two independent data scientists without a common set of guidelines for conducting the analysis in the proper way challenge 10 clear policies on long term data storage and computational costs in terms of both sustainability and information availability when data has been consumed for a primary analysis it is useful to store it in a long term data repository for future exploitations including combining it with other information sources for reuse or comparative analysis as well as for independent verification of results however all of these data consume storage space and require energy for both storage and processing the costs of storage space and energy consumption associated with data and processing envisaged in the near future present critical sustainability challenges in 2013 data centers in the u s consumed 91 twh of power this is approximately 2 2 of the total power generated in the u s cgo 2014 it is expected that in 2020 this consumption will increase to 3 5 communication networks consumed approximately 5 of total energy generated in 2012 and it is expected that this will increase to 10 by 2020 as another example the penetration and buildout of the internet of things iot is expected to raise data consumption around 27 between 2015 and 2020 cgci 2015 trend 2013 estimates that approximately 4 6 of world energy consumption 9000 twh per year is currently devoted to information and communications ict systems with yearly increase of 7 given these estimates of current consumption and projected growth finding ways to become more efficient with computation and long term storage along with criteria for which data need to be stored versus which data can be discarded will become critical this also includes considerations about the long term effective life of digital objects that may suffer from degeneration along time conway 2010 for example images and audio recordings corrado and moulaison sandy 2017 require specific methodologies to guarantee they will still be usable in the long future in spite of software upgrades de la rosa et al 2010 challenge 11 reproducibility and interoperability some scientific communities have aligned themselves around standard formats for data and standard software for particular analyses examples include the use of the network common data form netcdf file format in the weather and climate community or the use of the quantitative insights into microbial ecology qiime http qiime org software used by many scientists and bioinformatics experts to perform microbiome analysis from raw dna sequencing data such standardization can promote the reusability of data and the reproducibility of analyses however other scientific domains have not achieved this level of standardization and heterogeneity in both data and analysis techniques is still a barrier to rapid scientific progress using data science the sheer volume of data used in some scientific analyses can also be a barrier to reproducibility because it is not always practical or possible to store and maintain the large volumes of data used for a particular analysis see previous challenge also reproducibility is not just about preserving data in an accessible form in the long term but also about retaining the exact conditions of the analysis itself i e the algorithms used the input parameters software versions involved the intermediate data preprocessing and transformation steps and documentation with enough detail and precision to allow the reproduction of exact results using the same data often inadequate details are recorded about the complete data science workflow and some random elements are involved in the middle of the process like initial random class seeds in k means for example making reproducibility difficult and as a consequence the scope of conclusions may also become limited because generality of conclusions is impacted by lack of reproducibility given that it is unlikely that all scientific disciplines will settle on standards for data collection management and analysis better methods are needed for capturing scientific workflows to enhance the reproducibility of data intensive analyses some of the challenges described above may be addressed through the results of future interesting research trends from the side of data science that may provide new techniques and methodologies for using environmental datasets to provide answers to certain environmental problems on the other hand the future will also require research to create new environmental datasets and to make existing environmental data more suitable for use with data science approaches this will likely rely more on future research from environmental science experts who better understand techniques for making measurements formulating domain specific models of environmental phenomena and generating data in our opinion the most productive approach for addressing the problems listed above is to enhance collaborations in multidisciplinary teams where both environmental and data scientists work together to contribute to overcoming of these challenges in the near future 8 conclusions and the path forward in this paper we have provided insight into the origins of the data science field the intrinsic nature of data science and how it can contribute to improving understanding and management of environmental systems the new view of the field we have provided stresses the multidisciplinary nature of data science as a combination of data analytics data processing and knowledge management in order to provide added value for decision making the term is still controverted as it is a high level umbrella encompassing methods and techniques from many interrelated areas with a common ambition of providing global understanding of complex phenomena indeed the magic involved in data science i e extracting wisdom from data as hal varian expected in 2009 requires specific skills that are not frequent yet in professionals corporations realized the potential impact from highly qualified data scientists and underwent a deep transformation in recent years towards the new data centered economy announced by cukier in 2010 the phenomenon is not restricted to corporations and we are encouraged to see that academia has started to deploy new data science curricula however current efforts are still far from covering the existing shortage one approach to addressing the lack of data science professionals challenges 1 and 2 above is to promote a new culture where the keystone for data science is no longer a single multifaceted professional but rather performing data science within a multidisciplinary team such teams could be composed of statisticians machine learners software engineers knowledge engineers and domain experts like environmental scientists for environmental applications that guarantee the highest levels of expertise in all the skills involved in real data science projects to achieve the extraction of wisdom from data already mentioned in that case a common language among such working teams is still required and needs to be part of specific post graduate curricula to be urgently developed by academia our current ideas about what are necessary components of engineering and science curricula may also need to change to create a next generation of engineers and scientists who are better trained in data science and who are more capable of working in collaborative teams this requires development of specific training aimed at transferring the basics of data science to environmental scientists and the basics of environmental sciences to data scientists we anticipate working toward a new generation of professionals with the necessary skills to not only understand the scientific concepts within a domain but who also have the data processing and computer science expertise to be able to work in computationally complex and data intensive fields stronger links may also be needed between academia and corporations to enable students to be actively engaged with corporations as part of their educational experience without leaving their degree program entirely making education more of a partnership between educators and employers the environmental sciences cannot elude the transformation produced by the penetration of data science and data scientists or data scientist teams that has been experienced in many other application fields in fact we have already seen how much potential data science has for analyzing environmental systems providing a broad perspective of the complexity involved in these systems and as a consequence a nice support for deeper understanding of environmental phenomena and enhanced information for decision making however significant challenges remain we envisage and encourage new research trends that contribute to integration between data science and environmental sciences at a high level new tools that understand the structure of environmental problems and can refer analysts to a standardized family of reference environmental problems would be a major help in mapping environmental problems to data science methods and workflows that can provide an appropriate solution standardization of data science methods would also be a major benefit in this scenario addressing challenge 3 above and would contribute to reproducibility and interoperability challenge 11 this is a high level activity very much related to the design of data science processes on the other hand specific criteria have to be developed to properly manage each of the internal steps within a data science workflow including measurement data transmission data storage analysis and sharing challenges 6 to 11 many users who have collected heterogeneous noisy non linear multigranular spatio temporal environmental data face the prospect of not knowing which methods to use how to evaluate their effectiveness or what constitutes an acceptable result challenges 8 and 9 thus further methodological development followed by software development supporting new methods and guidelines for appropriate usage is encouraged to do this a bespoke repository of environmental problems with their associated data science workflows could provide a basis to support research development benchmarking and a complementary platform of typical pre processing methods modelling tools either predictive of descriptive and post processing methods it could provide a focal point in the area of environmental data science to centralize methodological achievements with their corresponding guidelines these tools could be used with a level of intelligent guidance and explanation adapted to the level of expertise of the user and would be useful to train a new generation of data scientists as well the paper elicits that environmental data science is a fruitful research area providing strategic added value to both corporations and environmental systems and merits attention for further developments in the short mid and long term 
26373,environmental data are growing in complexity size and resolution addressing the types of large multidisciplinary problems faced by today s environmental scientists requires the ability to leverage available data and information to inform decision making successfully synthesizing heterogeneous data from multiple sources to support holistic analyses and extraction of new knowledge requires application of data science in this paper we present the origins and a brief history of data science we revisit prior efforts to define data science and provide a more modern working definition we describe the new professional profile of a data scientist and new and emerging applications of data science within environmental sciences we conclude with a discussion of current challenges for environmental data science and suggest a path forward keywords data science environmental science data driven modelling 1 introduction data science is the science of dealing with data naur 1974 in recent years we have observed an increasing popularity of data science methods that seem to be in the focus of many organizations including those interested in a better comprehension or management of environmental systems data science is already widely used in business to design successful strategies and policies and the economic sector is facing a significant transformation as a result of the penetration of data driven innovation in the business core we believe that a similar transformation is underway within many scientific disciplines among them those within the environmental sciences to investigate the benefits that can be realized through use of appropriate data science approaches in this paper we analyze the origins of data science as a new discipline that is diverse enough to be applied to any domain including those within the environmental sciences the potential of data science to advance our knowledge of the laws governing complex environmental phenomena is enormous the technological development requisite for collecting the volume and resolution of data required to study these phenomena is mature but classical data analysis methods are in many cases insufficient to cope with the size speed and diversity of information sources providing evidence under the variety of forms text videos audio recordings numbers images that require global analysis and local tuning to elicit the hidden relevant knowledge to support higher level decision making many investigators are already investigating how data science can address this deficiency we present the contributions of data science together with an analysis of the new specific skills associated with its inherent multidisciplinarity as there is no common definition of data science in the paper we present several definitions that have been used in the past and a propose a new conceptualization of what data science means a discussion is also provided regarding its contact points with other emerging disciplines such as big data analytics emerging opportunities for new applications in environmental sciences are described while not an exhaustive description of the opportunities for data science in environmental science applications a wide perspective in the area is provided being an emergent field a number of open issues envisage fertile areas for new research in the near future the paper also provides some highlights challenges and trends with the aim to push the development of the data science field in general and in environmental sciences in particular where it can be of help the structure of the paper is as follows in section 2 the origins and a brief history of data science are provided in section 3 the added value of applying data science techniques to real problems using real data is discussed section 4 highlights the new skills required to become a qualified data scientist and the need to develop specific new curricula to provide appropriate training section 5 provides a more modern contemporary view of data science and section 6 provides a general overview of how data science is being applied in environmental sciences section 7 identifies the main current challenges in the area section 8 provides a concluding discussion 2 origins and a brief history although data science is a relatively new discipline the term data science is much older than might be expected it is worth noting that there is no clear and agreed upon definition of the term data science this lack of clarity appears in the first use of the term by naur in 1960 sundaresan 2017 naur used the term to mean data processing in the computer science sense however it has also been used at times as a substitute name for the field of statistics or at the very least applied statistics naur refined his earlier definition to data science is the science of dealing with data while the relation of data to what they represent is delegated to other fields and sciences naur 1974 in the same period in the context of statistical sciences there was also a process by which data became the center of interest of the discipline indeed john w tukey 1962 had already envisaged the need for statistics to move its focus from inference to data analysis as an empirical science for a long time i thought i was a statistician interested in inferences from the particular to the general but i have come to feel that my central interest is in data analysis intrinsically an empirical science the development of computer science near that time was opening an opportunity to this end in the late 1970s tukey 1977 published exploratory data analysis promoting a new approach to statistics where more emphasis needs to be placed on using data to suggest hypotheses to test exploratory data analysis and confirmatory data analysis can and should proceed side by side in 1977 the international association for statistical computing iasc http iasc isi org about iasc2 was established as a section of the international statistical institute it is the mission of the iasc to link traditional statistical methodology modern computer technology and the knowledge of domain experts in order to convert data into information and knowledge iasc 1977 rizzi and vichi 2006 davenport and dyche 2013 in 1996 the international federation of classification societies ifcs used for the first time the term data science in the title of their biennial conference data science classification and related methods aligned with this approach jeff wu seems to have been the first to ask whether statistics should change its name to data science in his talk entitled statistics data science which was given first in november 1997 as the inaugural lecture for his appointment to the h c carver professorship at the university of michigan in 1998 this was his first p c mahalanobis memorial lecture in honor of professor mahalanobis the founder of the indian international statistical institute iisi and was archived by wu 1999 in 2001 william s cleveland called for establishing data science as a field to enlarge the major areas of the field of statistics because the plan is ambitious and implies substantial change the altered field will be called data science cleveland put the proposed new discipline in the context of computer science and the contemporary work in data mining one and two years later the first journals in the area were launched the data science journal and the journal of data science respectively these events are largely why the term data science is currently understood by many people to be closely related to data mining and big data analytics rather than the original sense in which the term was used data science has also been approached from the perspectives of artificial intelligence ai and machine learning in the early 1980s there was a clear idea of the importance of using data as the main source of knowledge extraction in 1985 douglas fisher and bill gale founded the artificial intelligence and statistics society http www aistats org past html with the aim of facilitating interactions between researchers in ai and statistics nearly ten years later cheeseman and oldford stated we feel that there is great potential for development at the intersection of artificial intelligence computational science and statistics cheeseman and oldford 1994 in 1989 gregory piatetsky shapiro organized the first knowledge discovery in databases kdd workshop as part of the international joint conferences on artificial intelligence ijcai world conference it soon 1995 became an independent series of conferences acm sigkdd fayyad et al 1996 edited the seminal book advances in knowledge discovery and data mining introducing new techniques and tools for the discovery of knowledge from data as a response to the urgent need to address data flooding they defined the knowledge discovery from databases kdd framework as the process of the non trivial identifying of valid novel potentially useful ultimately understandable patterns in data in the kdd approach data mining was considered a specific data exploitation step one year later the first international journal data mining and knowledge discovery was launched around the mid 1990s data science started to be seen as a new business opportunity at that time most companies were aware of having large volumes of collected data that were not properly analyzed berry 1994 in many current contexts data science can be understood from a business perspective as the process of discovering what we do not know from data it enables us to get predictive actionable insight from data creating data products with business impact communicating relevant business from data and building confidence in decisions that drive business value somohano 2013 more recently data science has started to be seen as an enabler that has the potential to transform scientific inquiries mattmann 2013 identified algorithm integration and data stewardship as two components of data science that are essential for managing the data deluge in earth and space sciences and other fields like physics and genomics mattmann described algorithm integration as including model integration in scientific workflows and interfacing with data repositories and infrastructures mattman also called for integrating data archival with data processing facilities and in the same work highlighted the diversity of science data that involve many formats file types and conventions in fact data science is often based on the analysis of datasets resulting from a previous conversion of videos audio recordings signals data streams or websites into sets of relevant and or sufficient indicators by means of feature extraction techniques thus finding the relationships between several sources of heterogeneous data together and identifying complex hidden patterns useful for decision support in the last several years data science has been challenged to make the next steps in science by enabling in silico scientific discoveries from vast amounts of data where computers are enabled to identify and prove hypotheses not constructed by scientists for example agarwal and dhar 2014 describe the explosion of opportunities for scientific inquiry with readily available large and complex datasets and suggest that computers are now powerful enough to not only verify hypotheses but also to suggest new theories though such claims may seem ambitious advances in machine learning artificial intelligence data integration and stewardship seem very promising similarly caffo et al 2016 warn that the big data data science hype will flame out if it is only about data and not science they argue that data science is only useful when the data are used to answer a question expressing views similar to those that have been expressed by others that there are limits to what can be accomplished using data science methods and tools and that a balance must be struck between newer and more traditional scientific methods other authors lauro et al 2017 characterize data science as the process by which data are transformed into actionable knowledge to perform predictions as well to support and validate decisions lauro uses a metaphor such that computer science represents the language of data science statistics the logics of data science and domain expertise constitutes a catalytic element in the absence of which the transformation cannot be achieved 3 the added value of data science the development of data science promoted a new concept in decision making in general including business where decisions are data driven and the added value to organizations either institutions or companies is not more technology nor capital but information where data is considered to be a primary source of knowledge this transformation is not restricted to business fields and the value that data science processes can add to our understanding of complex phenomena is widely recognized davenport claimed that instead of competing on traditional factors companies are beginning to employ statistical and quantitative analysis and predictive modeling as primary elements of competition davenport and harris 2007 the climate corporation whose motto is data services for yield maximization was acquired by monsanto in 2013 for nearly 1 billion u s monsanto john deere and dupont pioneer are among many companies scrambling to help agricultural producers do more with their exploding volumes of data noyes 2014 and the value of data science for agricultural applications is clear in the obvious market demand for these services in one specific example application that demonstrates this potential value elarab et al 2015 coupled high resolution imagery from an unmanned aerial vehicle remote sensing platform with machine learning algorithms to estimate chlorophyll concentration in crops as an important biophysical parameter for use in precision agriculture their techniques enable farmers to assess the heterogeneity of the plants in their fields at fine resolution in space and time aiding farmers in targeting management actions accordingly e g effectively targeting application of fertilizers or water only where they are needed the economic and environmental implications of enabling this type of precision agriculture could be significant although the value of informed decision making was understood in the 1950s luhn 1958 after the emergence of knowledge discovery in databases fayyad et al 1996 informed decision making became more popular brynjolfsson et al 2011 several authors discuss the relevance of data for decisions for example craig mundi the head of research and strategy at microsoft stated managed well data can unlock new sources of economic value we are in front of a nascent data centered economy cukier 2010 however despite the excitement about the explosion in available data and data science methods the reality is that most available data are under exploited resulting in a loss of potential for decision making in 2004 hammond formulated what he called the fact gap the disconnect between data and decisions hammond 2004 to refer to this phenomenon of low consumption of available data at the decision making level data science is an emergent discipline that focuses on the intensive consumption of available data to extract decisional knowledge relevant for informed decision making and to bridge hammond s fact gap in the environmental sciences united nations agenda 21 has already since 1992 included a section on bridging the data gap highlighting that the gap in the availability quality coherence standardization and accessibility of data between the developed and the developing world has been increasing seriously impairing the capacities of countries to make informed decisions concerning environment and development un 1992 4 data scientist a new professional profile it seems clear that the skills required to perform data science point to a new professional profile and claim that academia start to design new curricula to train this new type of professional hal varian google s chief economist when interviewed by mckinsey referred to the scarce ability to extract wisdom from data cukier 2010 in many situations data scientists are expected to have a broad set of skills eloquently defined by josh wills 2012 a data scientist is a person who is better at statistics than any software engineer and better at software engineering than any statistician acquisition of the appropriate skills for effectively applying data science is critical in order to ensure a solvent extraction of the right value contained in data some authors have reported how in the absence of proper training several data scientists have extracted contradictory conclusions from a single dataset by performing different analytical procedures baeza yates 2017 silberzahn et al 2015 davenport and patil in their 2012 article in the harvard business review provocatively entitled data scientist the sexiest job of the 21st century argue that skills required for being an effective data scientist go beyond statistical or analytical capabilities and should include storytelling with data and excitement with potential for breakthroughs in the particular domain other authors have also described the new profile of a data scientist and the specific skills required to do data science properly e g sooraij shah 2013 contemporarily the shortage of talent with required skills for capturing the whole potential of data was reported by mckinsey in 2011 by quantifying a shortage of 140 000 to 190 000 data scientists by 2018 manyika et al 2011 in his keynote at the campus party europe in september of 2013 a s pentland head of medialab entrepreneurship mit said there are too few data scientists in the world and education needs to change in order to maximize the true potential of data science palmer 2013 at that time 62 of executives realized that the lack of data scientists was causing a real problem one poll survey for soft supplier teradata mckenna 2013 the shortage is especially severe in the u s for example 80 of new data scientist jobs were not filled within the year 2010 2011 harris et al 2014 in the u s the demand for data science and analytics jobs is projected to grow by 15 between 2015 and 2020 with the highest rate of growth rate of 28 expected for the specific job title of data scientist markow et al 2017 the lack of data scientists that can do high quality work with available data contributes to the fact gap burns 2017 and this shortage still persists at the time of this writing business com 2017 in spite of the shortage of skilled professionals data science has become a relevant profession in the last few years in 2013 venture beat reported data science as the second best new job in america in a more recent report they show data scientist as the number one best job in america venturebeat 2017 and piatetski shapiro 2017 reported on kdnuggets that glassdoor again ranked data scientist as the no 1 job in usa and 5 of the top 10 us jobs are related to analytics big data and data science a ranking that was repeated by glassdoor in 2018 as a response to this reality the last few years have seen recommendations and guidance for integration of data science into degree programs e g association for computing machinery 2017 many universities have created qualifications in data science or in some of its related areas such as data analytics business intelligence and so on these new programs tend to be associated with the computer science or mathematics statistics disciplines while earth environmental and life science curricula have not caught up to those of computer science and mathematics statistics in the area of data science it is becoming more common to find specific courses or discipline specific focus areas on data science methods in additional scientific disciplines 5 a modern view of data science so far we have seen that there is no clear agreed definition of the term data science but the intrinsic multidisciplinary nature of the field seems to be clear conway s data science venn diagram fig 1 conway 2013 provides a useful conceptualization for how coding skills conway calls them hacking skills math and statistics knowledge and domain science expertise conway calls this substantive expertise come together to enable data science domain expertise combined with math and statistics knowledge is where most traditional research is conducted coding with math and statistics knowledge may lead to insight through machine learning using data but without driving scientific questions and hypotheses that come through domain expertise mechanistic or process understanding may be limited coding skills combined only with domain expertise may lead to incorrect interpretation of results without knowledge of math and statistics the danger zone in conway s diagram it is only at the intersection between the three elements that data science can be most effective indeed multidisciplinarity has become a main pillar of data science in recent applications it is still possible to see the relationship between data science and computer science statistics data mining and big data analytics essentially data science is broader than these fields but can make use of all of them for example data science does not necessarily concern itself with the size of the data being processed but rather with transforming data into added value for the end user and extracting relevant knowledge from data coming from complex phenomena where data are prolific data science uses techniques from big data analytics to perform parts of its workflow similarly the data mining process works well for the overall development of some data science applications robust statistical methods are needed in most data science applications but the potential heterogeneity complexity and size of data can require innovative solutions from computer science trying to precisely define the relationship between these fields is difficult as their definitions and the boundaries between them are not altogether clear even the terms used to describe data science methods and the methods themselves are often confusing as discussed in section 2 from a practical perspective we are very much in agreement with lauro et al 2017 that the novelty of data science is identified to be the role played by knowledge which is definitely integrated into the process including interpretation issues with the main purpose to give meaning to data in the current context we consider data science as the multidisciplinary field that combines data analysis with data processing methods and domain expertise transforming data into understandable and actionable knowledge relevant for informed decision making thus contributing to bridge hammond s fact gap this modern view and its associated techniques and applications are enabling data scientists to synthesize value added products from diverse datasets that would be otherwise impossible to obtain data science often requires the ability to overcome data complexity and the limitations of classical statistics and machine learning techniques for example dealing simultaneously with heterogeneous data sources e g videos text or streams or coping with non independencies non normalities and few technical hypothesis on variable s distributions when required 6 data science in environmental sciences much of our discussion up to this point has involved mathematics statistics computer science and applications of data science in general including impact in business however there is no shortage of opportunity for applications in environmental sciences gibert et al 2008 indeed as the size and complexity of environmental datasets continue to grow the demand for data science in environmental applications is increasing data science is able to add value to environmental sciences in many different ways data science is at work in examining interpreting and deriving useful and actionable information from environmental sensor data streams athanasiadis and mitkas 2007 reis et al 2015 bifet et al 2017 these datasets produced by in situ and remote sensors of many different types span the environmental sciences from climate and weather observations from satellite and ground based sensors e g hill et al 2011 to air quality sensors e g wiemann et al 2016 hydrologic and water quality sensors installed in aquatic environments e g wong and kerkez 2016 water quality sensors in wastewater treatment processes e g corominas et al 2017 sensors used to track the movement and behaviors of biological organisms e g kranstauber et al 2011 ground based sensors for detecting and quantifying the magnitude of earthquakes and geological events and many other applications the size of these monitoring networks and the datasets they produce is growing as the cost of sensors and related systems is falling producing a greater need for analysts capable of managing the datasets produced and assimilating them with simulation models and other applications sometimes evolutionary and population based algorithms are used to extract relevant parameters of a reference phenomenon based on these sensor data afshar et al 2015 describe an application to water resource management levasseur et al 2008 uses genetic algorithms in soil applications and nagesh kumar et al 2006 describe an application for optimal reservoir operation for irrigation of multiple crops another quickly growing application is that of interpreting data collected from aerial drones sailing or aquatic drones and satellite remote sensing roelofsen et al 2014 elarab et al 2015 gauci et al 2018 remote sensing data of these types are becoming more and more vital for accurate and broad environmental monitoring the scope of these applications is simply too broad for individual sensors or ground based stations to be effective but the volume of data produced by remote sensing drones and satellites can dwarf most other environmental datasets requiring specialized tools techniques and training for data analysts many new applications at the nexus between water and energy are generating high spatial and temporal resolution datasets using smart metering technologies that collect observations of water or power usage at frequencies on the order of seconds or even more frequent to support applications such as end use disaggregation demand estimation and demand management cominola et al 2015 gurung et al 2015 sophisticated algorithms and data management techniques are required for these applications and without them high resolution data can be an unnecessary barrier for water and power system managers who have traditionally used metering data for monthly billing purposes horsburgh et al 2017 preparing datasets designed to support high performance large scale e g continental scale modeling and analyses is another area data science is being used within the environmental sciences these datasets are typically derived from existing geospatial data but are organized over large spatial scales using sophisticated data modeling to provide the backbone for environmental models examples include the national hydrography dataset plus nhdplus being used as the hydrologic network underlying the continental scale national water model in the u s another example includes ongoing efforts by the united states geological survey to link aquatic monitoring sites and features to the network of streams in the u s elfie https opengeospatial github io elfie json ld using concepts from the open geospatial consortium s hyfeatures specification dornblut and atkinson 2014 data science is needed in the design and preparation of these types of datasets to enable high resolution and high performance use model outputs from high resolution long temporal period climate simulations represent another class of environmental data requiring specialized skills for analysis analysts and modelers working with global or continental scale models are often faced with hundreds of terabytes of model generated results that must be reduced to useful products that can be used for downstream analyses ashraf vaghefi et al 2017 the volume of data involved is challenging from not only the data use perspective but also from the perspective of provisioning basic storage hardware and software required for short and longer term uses of these massive datasets in many contexts integrating data and domain knowledge for obtaining better models requires application of data science methods in this sense uusitalo 2007 discusses the use of bayesian networks in environmental modelling blattenberger and fowles 2017 have also used them to evaluate avalanche danger and gibert et al 2010a used prior knowledge to bias a clustering process in waste water treatment plant applications combining disparate datasets from multiple scientific domains for synthesis studies and to create new derived datasets can also be enabled using data science vitolo et al 2015 provide a broad review of many of the techniques and technologies that have been applied in enabling data integration particularly via the internet this is a problem of data fusion where complexity and heterogeneity in data can be more challenging than size nativi et al 2015 often combining datasets from different sources in a single analyses requires specialized skills in data management programming and visualization that are central to data science as in porter et al 2014 geospatial machine learning methods e g kanevski et al 2008 are also helping in this area mccord et al 2017 hengl et al 2017 another emerging area in environmental data science involves linking human health to environmental conditions especially in the cases of natural disasters such as hurricanes flooding or earthquakes e g klise et al 2017 major hurricane events seriously impact the availability of power potable water and other basic human needs relatively little is currently understood about the short and longer term effects events like these have on human health because it is only in the past few years that more detailed datasets have been available for characterizing environmental conditions during and following events along with health impacts that can be attributed to these conditions exposure gómez losada et al 2014 schlink et al 2016 is another new issue that requires attention the impact of air pollution on public health is currently a focus of interest and modelling air quality on the basis of data streams provided by smart sensors currently available in many cities is crucial to understand how exposure affects public health but also to perform real time air pollution forecasting to run preventive and protective healthcare plans for citizens health data science linking environmental and human health data raises multiple challenges first human health data are sensitive restricted and must be anonymized second the format vocabularies and syntax of these two types of data are very different and require the combination of different resources to deal with them simultaneously properly linking environmental and human health data requires unique and careful approaches for both data management and analysis 7 current challenges and trends in environmental data science while the use of data science techniques to enhance research in the environmental sciences is rapidly growing it is not without significant challenges that must be overcome in this section we describe some of these challenges and while likely not exhaustive the list we provide here clearly illustrates that there is much room for improvement challenge 1 shortage of trained data science experts to cover real demand as mentioned many authors identify a relevant shortage of properly trained data scientists to cover the real demand new training programs are being deployed at different levels of academy to increase the number of formally trained data scientists however it is unlikely that the number of personnel formally trained as data scientists will be able to keep up with the growing demand from so many different disciplines in which they are now employed from the environmental science perspective it will remain difficult to attract the best and brightest to work on environmental problems when salaries for data scientists are so much higher elsewhere this is common with environmental informatics as already pointed out by swayne 2003 challenge 2 lack of data science skills within environmental science curricula applying data science approaches to environmental systems and data requires all three skills described by conway 2013 data programming and data modelling skills in general including statistical and machine learning approaches and domain knowledge it will continue to be difficult to find individuals that can effectively do all three with sufficient expertise collaborations between environmental scientists and computer scientists can potentially address this need but are difficult to foster because the needs interests and expertise of collaborators are not always aligned even though curricula in environmental engineering and the environmental sciences are evolving to address this they have not totally caught up with the need yet and sustained efforts are required to resolve this deficiency developing resources within open environmental data repositories accompanied with problem descriptions can provide materials for introducing realistic practicums in education that helps in better training new scientists challenge 3 methodological gaps for designing data science processes in real applications the number and variety of environmental problems suitable for application of data science approaches is high it is currently the responsibility of the data scientist to translate the environmental problem into a data science workflow that encompasses both the goals of the problem and available data this includes identifying the proper preprocessing data mining and knowledge production methods and their proper sequencing and interactions to create a data science workflow that will advance the project gibert et al 2016 provides some guidelines to design preprocessing steps there are currently no established guidelines or standards for how to design data science workflows leaving subjectivity in their design and difficulty in comparing results where different workflows may have been used to address the same or similar problems research is needed in this respect to build a conceptual framework with standard data science processes providing answers to a certain kind of problems challenge 4 guidelines to map families of environmental problems with prototypical data science processes that help in environmental application from a structural point of view there are some commonalities in some families of environmental problems that fit well with a similar kind of data science process as an example analyzing the effect of pollutants may require similar analytics for air pollution or water pollution analogously predicting the survival of certain protected species might require similar methods for forest fauna and freshwater fish even if the related environmental systems are radically different the first example may entail in both cases modelling continuous multi response variables several coexisting pollutants that are never independent and develop in a spatiotemporal space whereas the latter example is in both cases about discrete prediction methods with a single counting response variable very little work has been done on finding families of environmental problems that share structure and building guidelines to map them into standard environmental data science workflows a deep analysis of the different environmental problems suitable for data science is required to produce insights about them and how they map to standard data science solutions providing environmental scientists with tools to help them identify the structure of an environmental problem would be of great benefit to do these mappings and opens the door for more effective application of data science techniques within the environmental sciences challenge 5 data quality and dealing with uncertainty in data data quality is currently one of the hotpoints of the data science process as environmental data always includes mistakes or biases the validity of data science processes that use these data becomes limited at best or even dangerous under worst case scenarios enabling the production of potentially incorrect conclusions that may lead to decisions with dramatic consequences as an example in 2012 the secretary of environmental protection in pennsylvania told congress that there was no evidence the state s water quality had been affected by fracking in august 2014 the same department published a list of 248 incidents of damage to well water due to gas development this discrepancy was caused because six regions in the state had missing data in 2012 and data collection was not the same among regions barrett and greene 2015 with the diversity of information sources combined in data science projects often including videos audio recordings images and real time data streams guaranteeing the quality of data requires development of stronger methodologies that go beyond the current catalogs of unconnected preprocessing operations provided by existing software along with the need for more research in data preprocessing approaches gibert et al 2016 there is also a need for efforts to improve technologies behind smart sensors to reduce failures in measurements eliminate noise and increase the quality of data transmissions in spite of the need for improvement in these areas it has to be noted that data science offers a certain robustness with regard to noise that other approaches cannot achieve more recent approaches like deep learning have shown reasonable performance with noisy data pointing that data science might create useful results even from noisy or lower quality data the potentials in this direction is still under explored challenge 6 privacy and security guaranteeing privacy of personal data circulating in the internet from sources ranging from smart sensors to communication networks data centers or the cloud is another critical issue that requires attention and limits the scope of applying data science to all available environmental data for example federal laws in the u s and eu directives in europe govern the collection management and disclosure of personally identifiable information including health and medical records or farmer s data yet there are many potential opportunities for combining this type of data with those from environmental sensors or samples to learn more about the effects of exposure to environmental conditions e g reis et al 2015 another example is privacy preservation in creating crowdsourced noise maps drosatos et al 2014 challenge 7 methods to choose pertinent correct sufficient and non superfluous data for analysis as the volume and heterogeneity of available data continues to grow no clear criteria have been established to assess which out of all available data is required for an analysis or whether available data is representative enough of the whole target population big is not necessarily linked with sufficient or unbiased and more work is required to provide guidelines for making these decisions this necessarily requires clarification of which possible biases different data sources might include by construction furthermore as the volume speed and diversity of collected data is growing it is hardly possible for scientists to keep on manually preprocessing data i e performing tasks such as data linking cleaning and integration data sharing and discovery need to be performed using methods that make it possible for machines to tackle these tasks with semantic interoperability rather than human experts additionally clear policies on use of big or small data for specific environmental applications is needed challenge 8 need for development of integral data mining methods there is a clear need for data mining methods able to cope with heterogeneous data that might include traditional databases data derived from environmental samples smart sensor data or data from supervisory control and data acquisition scada systems data with intrinsic uncertainty like georadar data data streams geospatial datasets images sounds and free text to provide an integrated overview of a complex system these methods must also address any structural complexities involved e g high order interactions multigranularity spatiotemporality etc challenge 9 guidelines to choose the right analytics method for a given problem challenges 5 to 7 are related to the data used in the data science process next step in the process is analyzing data with suitable methods selection of the proper methods to effectively process data can be equally difficult not much work has been done to establish consensus about which analytics methods are effective and appropriate for specific applications gibert et al 2010b as such there are few clear guidelines for analyzing a certain kind of data when addressing a certain kind of question we have even seen how analyses of the same dataset can provide contradictory conclusions when analyzed by two independent data scientists without a common set of guidelines for conducting the analysis in the proper way challenge 10 clear policies on long term data storage and computational costs in terms of both sustainability and information availability when data has been consumed for a primary analysis it is useful to store it in a long term data repository for future exploitations including combining it with other information sources for reuse or comparative analysis as well as for independent verification of results however all of these data consume storage space and require energy for both storage and processing the costs of storage space and energy consumption associated with data and processing envisaged in the near future present critical sustainability challenges in 2013 data centers in the u s consumed 91 twh of power this is approximately 2 2 of the total power generated in the u s cgo 2014 it is expected that in 2020 this consumption will increase to 3 5 communication networks consumed approximately 5 of total energy generated in 2012 and it is expected that this will increase to 10 by 2020 as another example the penetration and buildout of the internet of things iot is expected to raise data consumption around 27 between 2015 and 2020 cgci 2015 trend 2013 estimates that approximately 4 6 of world energy consumption 9000 twh per year is currently devoted to information and communications ict systems with yearly increase of 7 given these estimates of current consumption and projected growth finding ways to become more efficient with computation and long term storage along with criteria for which data need to be stored versus which data can be discarded will become critical this also includes considerations about the long term effective life of digital objects that may suffer from degeneration along time conway 2010 for example images and audio recordings corrado and moulaison sandy 2017 require specific methodologies to guarantee they will still be usable in the long future in spite of software upgrades de la rosa et al 2010 challenge 11 reproducibility and interoperability some scientific communities have aligned themselves around standard formats for data and standard software for particular analyses examples include the use of the network common data form netcdf file format in the weather and climate community or the use of the quantitative insights into microbial ecology qiime http qiime org software used by many scientists and bioinformatics experts to perform microbiome analysis from raw dna sequencing data such standardization can promote the reusability of data and the reproducibility of analyses however other scientific domains have not achieved this level of standardization and heterogeneity in both data and analysis techniques is still a barrier to rapid scientific progress using data science the sheer volume of data used in some scientific analyses can also be a barrier to reproducibility because it is not always practical or possible to store and maintain the large volumes of data used for a particular analysis see previous challenge also reproducibility is not just about preserving data in an accessible form in the long term but also about retaining the exact conditions of the analysis itself i e the algorithms used the input parameters software versions involved the intermediate data preprocessing and transformation steps and documentation with enough detail and precision to allow the reproduction of exact results using the same data often inadequate details are recorded about the complete data science workflow and some random elements are involved in the middle of the process like initial random class seeds in k means for example making reproducibility difficult and as a consequence the scope of conclusions may also become limited because generality of conclusions is impacted by lack of reproducibility given that it is unlikely that all scientific disciplines will settle on standards for data collection management and analysis better methods are needed for capturing scientific workflows to enhance the reproducibility of data intensive analyses some of the challenges described above may be addressed through the results of future interesting research trends from the side of data science that may provide new techniques and methodologies for using environmental datasets to provide answers to certain environmental problems on the other hand the future will also require research to create new environmental datasets and to make existing environmental data more suitable for use with data science approaches this will likely rely more on future research from environmental science experts who better understand techniques for making measurements formulating domain specific models of environmental phenomena and generating data in our opinion the most productive approach for addressing the problems listed above is to enhance collaborations in multidisciplinary teams where both environmental and data scientists work together to contribute to overcoming of these challenges in the near future 8 conclusions and the path forward in this paper we have provided insight into the origins of the data science field the intrinsic nature of data science and how it can contribute to improving understanding and management of environmental systems the new view of the field we have provided stresses the multidisciplinary nature of data science as a combination of data analytics data processing and knowledge management in order to provide added value for decision making the term is still controverted as it is a high level umbrella encompassing methods and techniques from many interrelated areas with a common ambition of providing global understanding of complex phenomena indeed the magic involved in data science i e extracting wisdom from data as hal varian expected in 2009 requires specific skills that are not frequent yet in professionals corporations realized the potential impact from highly qualified data scientists and underwent a deep transformation in recent years towards the new data centered economy announced by cukier in 2010 the phenomenon is not restricted to corporations and we are encouraged to see that academia has started to deploy new data science curricula however current efforts are still far from covering the existing shortage one approach to addressing the lack of data science professionals challenges 1 and 2 above is to promote a new culture where the keystone for data science is no longer a single multifaceted professional but rather performing data science within a multidisciplinary team such teams could be composed of statisticians machine learners software engineers knowledge engineers and domain experts like environmental scientists for environmental applications that guarantee the highest levels of expertise in all the skills involved in real data science projects to achieve the extraction of wisdom from data already mentioned in that case a common language among such working teams is still required and needs to be part of specific post graduate curricula to be urgently developed by academia our current ideas about what are necessary components of engineering and science curricula may also need to change to create a next generation of engineers and scientists who are better trained in data science and who are more capable of working in collaborative teams this requires development of specific training aimed at transferring the basics of data science to environmental scientists and the basics of environmental sciences to data scientists we anticipate working toward a new generation of professionals with the necessary skills to not only understand the scientific concepts within a domain but who also have the data processing and computer science expertise to be able to work in computationally complex and data intensive fields stronger links may also be needed between academia and corporations to enable students to be actively engaged with corporations as part of their educational experience without leaving their degree program entirely making education more of a partnership between educators and employers the environmental sciences cannot elude the transformation produced by the penetration of data science and data scientists or data scientist teams that has been experienced in many other application fields in fact we have already seen how much potential data science has for analyzing environmental systems providing a broad perspective of the complexity involved in these systems and as a consequence a nice support for deeper understanding of environmental phenomena and enhanced information for decision making however significant challenges remain we envisage and encourage new research trends that contribute to integration between data science and environmental sciences at a high level new tools that understand the structure of environmental problems and can refer analysts to a standardized family of reference environmental problems would be a major help in mapping environmental problems to data science methods and workflows that can provide an appropriate solution standardization of data science methods would also be a major benefit in this scenario addressing challenge 3 above and would contribute to reproducibility and interoperability challenge 11 this is a high level activity very much related to the design of data science processes on the other hand specific criteria have to be developed to properly manage each of the internal steps within a data science workflow including measurement data transmission data storage analysis and sharing challenges 6 to 11 many users who have collected heterogeneous noisy non linear multigranular spatio temporal environmental data face the prospect of not knowing which methods to use how to evaluate their effectiveness or what constitutes an acceptable result challenges 8 and 9 thus further methodological development followed by software development supporting new methods and guidelines for appropriate usage is encouraged to do this a bespoke repository of environmental problems with their associated data science workflows could provide a basis to support research development benchmarking and a complementary platform of typical pre processing methods modelling tools either predictive of descriptive and post processing methods it could provide a focal point in the area of environmental data science to centralize methodological achievements with their corresponding guidelines these tools could be used with a level of intelligent guidance and explanation adapted to the level of expertise of the user and would be useful to train a new generation of data scientists as well the paper elicits that environmental data science is a fruitful research area providing strategic added value to both corporations and environmental systems and merits attention for further developments in the short mid and long term 
26374,background pollution represents the lowest levels of ambient air pollution to which the population is chronically exposed but few studies have focused on thoroughly characterizing this regime this study uses clustering statistical techniques as a modelling approach to characterize this pollution regime while deriving reliable information to be used as estimates of exposure in epidemiological studies the background levels of four key pollutants in five urban areas of andalusia spain were characterized over an 11 year period 2005 2015 using four widely known clustering methods for each pollutant data set the first lowest cluster representative of the background regime was studied using finite mixture models agglomerative hierarchical clustering hidden markov models hmm and k means clustering method hmm outperforms the rest of the techniques used providing important estimates of exposures related to background pollution as its mean acuteness and time incidence values in the ambient air for all the air pollutants and sites studied keywords clustering techniques background pollution air quality time series analysis exposure health risk 1 introduction determining the population s health risks due to ambient air pollution is critical to the development of effective risk management policies and strategies samet and krewski 2007 to better understand the adverse health effects associated with air pollution accurate exposure assessment is essential epidemiological studies have provided a substantial body of evidence linking daily concentrations of outdoor air pollution to adverse effects on a range of health outcomes studies have tended to focus on the mass concentrations of particles and selected gaseous pollutants but more insight is required regarding the most harmful sources and components of the air pollution mixture to inform focused public health protection policies atkinson et al 2016 background concentration is the ambient level of pollution that is not affected by local sources of pollution who 1980 menichini et al 2007 there are two motivations for focusing on this regime i to better understand the contribution of local sources to total pollutant concentrations and ii to allow the assessment of new pollutant sources that are introduced into the area of study and their impact on local air quality however up until now research has not significantly addressed this lowest fraction of pollution as representative of a permanent concentration of ambient air pollution to which the population is chronically exposed this work focuses on this specific fraction of pollution han et al 2015 classify the methods to determine the background pollution using four categories i physical methods to identify the regional and local pollution processes via atmospheric variables ii chemical methods to identify the chemical composition of air pollutants iii numerical simulations methods using trajectory models and iv statistical methods regarding the latter langford et al 2009 used principal component analysis to describe the local background o3 concentrations recorded during 76 days in 30 monitoring sites in texas tchepel et al 2010 study the contributions to background pollution of pm10 from different sources in four monitoring sites in lisbon portugal during two days through air quality time series via spectral analysis other authors have used clustering techniques to characterize regimes in air pollution austin et al 2012 classify air pollution daily data during six years performing k means km and hierarchical clustering for identifying profiles in them beaver and palazoglu 2006 used an aggregated solution of km to characterize classes of ozone episodes occurring in the san francisco bay considerable effort has been made to characterize profiles of key air pollutants carslaw and ropkins 2012 carlsaw and beevers 2013 since the threshold values that can be considered safe for human health is still under debate pioneering research work explored this relationship for o3 and pm10 koop and tole 2006 and for pm2 5 kiesewetter et al 2015 background profiles of co and nox were studied by venegas and mazzeo 2006 in the city of buenos aires and for nox no2 and o3 in the california south coast air basin by pournazery et al 2014 this study proposes the use of statistical clustering techniques as a methodology for the estimation of background pollution in urban environments to that end four well known clustering methods were compared using data obtained from monitoring sites namely finite mixture models fmm agglomerative hierarchical clustering hc hidden markov models hmm and km this study aims to i evaluate the best clustering statistical method to estimate the background pollution and ii provide model derived exposure estimates from the best method as inputs for epidemiological research the best clustering method was assessed according to its ability to cluster the lowest concentrations of ambient air pollution in a consistent manner to that end data sets from key pollutants co no2 o3 and pm10 from five monitoring sites in andalusia south of spain were studied over 11 years 2 data and methods 2 1 air pollution data air quality data hourly average concentrations of co no2 o3 and pm10 were collected from 2005 to 2015 as independent yearly series for each pollutant these data were obtained at five monitoring sites exhibiting different typology suburban urban and predominant emission sources background traffic since monitored data were available on an average hourly basis daily mean concentrations were calculated when at least 80 of the data were available a total of 200 yearly data sets each one consisting of daily average values for a single pollutant and complete years were studied resulting from 40 55 50 and 55 data sets corresponding to the air pollutants co no2 o3 and pm10 respectively table 1 in order to favour the heterogeneity both of data and range of pollutant concentrations to study monitoring sites were selected in three different cities of andalusia córdoba jaén and seville with different meteorological conditions governing the local air pollutant behaviour the standard monitoring methods established in european directive 2008 50 ec directive 2008 were used for air pollutants co no2 and o3 and beta attenuation monitoring was applied for pm10 air quality monitoring networks are subject to an intense maintenance program to ensure accurate values prior to undergoing analysis the data obtained were validated by the regional ministry of environment and land planning of andalusia 2 2 background pollution estimation for each independent yearly data set with measurements of a single pollutant a clustering technique was applied for a clustering result each cluster represents ranges of concentration values profiles or regimes of pollution for a given pollutant that can be associated to an emission source of pollution this view is based on the lenschow approach lenschow et al 2001 that assumes that the air pollutant concentrations at a monitoring site correspond to the sum of regional urban background and local nature contributions this approach has been used as a prior analysis in source apportionment studies belis et al 2013 and may be applied to urban areas with negligible impact from industrial emissions as in case of córdoba seville and jaén the concentration measured at a traffic site corresponds to the sum of local traffic urban and regional background contributions with regard to an urban or suburban background site the contributions that explain the ambient pollution correspond to those from the background levels of the city or metropolitan area respectively and those of the regional background being a univariate clustering process the resulting clusters represent certain categorization of the original variables into a set of ranges determined by each cluster when sorting the cluster according to the associated ranges the first cluster contains the lowest values of the pollutant and it represents the range of minimum concentrations obtained at a monitoring site this work focuses specifically on this first cluster which might represent the magnitude of a kind of chronic exposure concentration experienced by the population along the year one of the most important advantages of this approach is that it allows the estimation of the first cluster representing the background pollution at any monitoring site and for any air pollutant the estimation is affected neither by the main type of pollution source present nor by the classification of the monitoring site according to its location the ability of four clustering techniques to detect the lowest cluster on different air pollutants was compared 2 3 clustering techniques fmm hc hmm and km were used to cluster data obtained from monitoring sites the aim was to study their ability to detect more than one cluster in data and therefore to be able to associate the lowest one to the background pollution regime because clustering via fmm represents the foundational model upon which the rest of the clustering techniques are based it is explained next in the interest of space a description of hc hmm and km is given in supplementary material sm 1 fig 1 illustrates the use of fmm to model the first cluster equivalent in this work to the background pollution in the no2 data distribution histogram from the aljarafe site during 2015 the information regarding the background pollution appearing in blue fmm represents a model based strategy for clustering by assuming that each cluster of data is described by a different probability distribution component these clusters are combined according to the mixing proportions representations or weights that make up the mixture making the modelling of any multi modal data set possible because of fmm s extreme flexibility mclachlan and peel 2000 in this work all the distributions were considered univariate gaussians the dispersion of the components defining each cluster is given by the standard deviation of the gaussian distributions these standard deviations can be constrained as constant across the clusters of the mixture e configuration equal variances or allowed to vary between them v configuration variable variances once the number of clusters has been fixed in advance to model the data the mixing proportion or representation the mean m and standard deviation sd of each component parameterizes fmm estimating the parameters defining a fmm that are most likely to have generated a given data set is referred to as the maximum likelihood estimation mle problem although there are many methods that can be used to estimate the parameters of a fmm the expectation maximization em algorithm dempster et al 1977 is the most widely used mclachlan and krishnan 2008 the em algorithm computes the maximum log likelihood estimates of the mixture iteratively alternating between two steps e the expectation step and m the maximisation step until a convergence criterion is met the e step calculates the log likelihood given the observed data and the current parameters estimate of the mixture and the m step maximizes the expected log likelihood from the previous e step providing a new estimation for each parameter the convergence criterion may be a permitted number of iteration of the algorithm an acceptable minimum difference ε between the parameter estimates at each iteration or both once the em algorithm converges and the parameters of the fmm estimated the log likelihood of the data can be calculated this allows obtaining the bic bayesian information criterion value schwarz 1978 of the data modelled with a specific fmm and for a given number of clusters thus several fmm differing in the number of clusters can be proposed to model a data set and the corresponding bic values calculated the fmm with larger value of the bic obtained provides the more suitable number of clusters for the data set studied the common features shared between hc hmm and km to fmm have simplified the practical implementation of the techniques in this work and remarkably provided all of them a common probabilistic foundation probabilistic clustering henceforth it was possible to address the determination of the most suitable number of clusters k in data using the bic criterion and thus a comparison of the number of clusters was obtained with the different techniques the application of bic to determine the number of clusters in fmm and hmm is well known due to its consistency in mixtures from exponential families frühwirth schnatter 2006 however the number of studies describing the application of bic to hc and km in the air pollution field is scarce in this work the bic approach avoids the pre setting of the number of clusters in hc and km empirically by the user for each clustering technique and data set studied an initial number of clusters were proposed k 1 9 and the bic values calculated for each solution the more suitable number of clusters was chosen according the maximum bic value the optimal solution k 1 was also included to verify that no cluster absence of air pollution profiles was detectable in the data sets under analysis other bayesian model selection criteria are possible general approaches for model selection are akaike s information criteria akaike 1974 the deviance information criterion spiegelhalter et al 2002 the integrated classification likelihood biernacki et al 2000 and the focused information criteria claeskens and hjort 2003 the selection of different criteria remains data dependent and no one criterion is superior to any other in general cases xu and wunsch 2009 unfortunately there seems to be no simple recommendation to guide the use of these criteria as there are no general results on these methods performance that apply to all situations in the short length ts framework as studied in this work bic criterion is a parsimonious solution for determining the number of clusters next to model selection criteria other statistics for goodness of fit can be found in mackay altman 2004 andtitman and sharples 2008 the computational implementation of all the cluster techniques was accomplished using the open source software r r development core team 2015 such implementations are available upon request to determine the optimal number of clusters in data using fmm and the parameters defining each cluster representation mean and standard deviation values the mclust function from the mclust package fraley et al 2012 was used adopting a v configuration and setting the iterations of the algorithm to unlimited this function adopts a default value for relative convergence of the log likelihood in the em algorithm of ε 10 5 2 3 1 k means km algorithm implicitly assumes that the data in each cluster are spherically distributed around the mean venables and ripley 2002 hamerly and elkan 2003 therefore it is possible to derive the km algorithm as a special case from the univariate gaussian fmm used in this work when the variance of the components adopts the e configuration same variance across the components of the mixture data was analysed with km using the kmeans function from the stats package for each k value the representation of each cluster with respect to the data set size permitted obtaining its representation weights as in fmm the common variance was calculated as the sum of the weighted variance of clusters to parameterize the km clustering solutions from a fmm approach the representation mean value from clusters and their common variance were provided as parameters to the e step of the em algorithm estep function from the mclust package e configuration bic values were later calculated using the bic function from this package 2 3 2 hidden markov models hmm belong to the model based clustering methods which provide a convenient way of formulating an extension of fmm to allow for dependent data mclachlan and peel 2000 and the mle problem can be solved using the em algorithm bulla and berzel 2008 using this clustering technique each data point represents the observed value of a time series ts at time t as in fmm the data are drawn from two or more distributions with different parameters forming a mixture which can fit multiple modes in ts an hmm is a doubly stochastic process in which an underlying stochastic process a set of discrete states can only be observed through another stochastic process that generates a sequence of observations ts data only the ts observations are visible to the observer the observations of the ts are dependent on the discrete states such that the marginal distribution of the data is a mixture distribution as in fmm the data in hmm are dependent rather than indepedent draws from the components of the mixture distribution visser 2011 an hmm is characterized by a set of states equivalents to components in fmm an initial probability distribution for the first state a transition probability matrix linking successive states and state dependent probability distributions responsible of generating the ts data however just the information characterizing the first cluster is examined m and sd as in fmm according to the aim of this study the parameters defining the mixtures in ts data was obtained using the depmix function from the depmixs4 package visser and speekenbrink 2010 using a tolerance value for the relative convergence of ε 10 5 unlimited em algorithm iterations and adopting the v configuration to obtain the bic values from each clustering solution an ad hoc r function was designed considering the parameters of the mixtures and the size of the data sets to check the validity of the modelling results obtained with the depmixs4 package the hiddenmarkov harte 2015 and hmm himmelmann 2010 libraries were also used and negligible differences were found in the parameter estimates 2 3 3 agglomerative hierarchical clustering to define the proximity between clusters in hc an approach equivalent to km was used ward s method ward 1963 attempts to minimize the sum of squared distances of data from their cluster means clarke et al 2009 everitt et al 2011 providing homogeneous spherical clusters around the cluster means an approach that is analogous to km when dealing with hierarchical clustering tan et al 2006 to perform hc for the constrained gaussian model e configuration the procedure followed by fraley and raftery 1998 later implemented in venables and ripley 2002 was applied using the function hc from the mclust package adapted to univariate data the parameterization of each candidate cluster solutions was obtained using the em algorithm em function from the same package e configuration and the bic values by using the bic function from this package 3 results and discussion 3 1 background regime study at one site during one year fig 2 illustrates the different graphical results corresponding to the first cluster analyses using the four clustering techniques fmm hc hmm and km for the air pollutants studied at the torneo site during 2015 the corresponding numerical results are given in table 2 in fig 2a below the daily average concentrations the different coloured segments indicate days in which no external contributions are detected according to the clustering technique used daily data grouped into the first cluster the ability to detect clusters in data is manifested through the k value in table 3 in co no2 and pm10 pollutants km red segments does not detect clusters in data as fmm and hc in pm10 blue and orange segments respectively except by hmm green segments the first cluster detected in no2 is markedly unspecific since the data grouped in it corresponds to almost the whole range of concentrations during the whole year therefore hmm reveals a higher resolution for detecting background concentrations in all the pollutants studied the four techniques distinguish clusters in o3 data possibly due to the distinct differences in concentration ranges experienced during the year maximum values in summer and lowest in winter where any of these clustering solutions would have been potentially valid fig 2b is equivalent to the cluster analyses represented in fig 2a following the same approach as in fig 1 except the density of the mixture is not represented by simplicity it reveals the gaussian curves characterizing the first cluster detected by the clustering methods superimposed to the histogram of data in grey coloured circles represent the average value of the first cluster m while the black circle represents the annual average value of the concentrations m as expected when a technique does not detect clusters in data the average value of the first and single cluster coincides with the average value of the data pm10 clustering in fig 1b for fmm hc and km and only one gaussian component models the data table 2 provides more valuable information focusing on the first cluster the gaussian curves provide the spread of the data sd around their means m indicating the strength of the background exposure to the different pollutants in this analysis this information can only be consistently obtained in all the pollutants by means of the hmm clustering the time incidence representation of the background regime cluster over the whole data set is given by the size of this cluster to measure the proportion of the year in which the population is exposed to the background pollution characterized by its m and sd values 3 2 selection of the best clustering technique as seen in the previous section information related to the first cluster of data allows a full description of the background pollution m and sd however this description is possible because it is based on the resolution of clustering techniques that detect more than one cluster in data k 1 otherwise the only detected cluster would parameterize the entire data set with simply one gaussian curve and would therefore not provide any valuable information fig 3 illustrates clustering methods ability to achieve this end with the number above the bars representing the data sets described with a specific number of clusters for a given technique and pollutant table 3 summarizes this information concluding that hmm is the most suitable technique to detect regimes in data and therefore to describe the background pollution in them k 1 case 99 meanwhile km 57 is an unsuitable technique with fmm 82 5 and hc 83 in an intermediate position this superior performance of hmm may be due to its ability to capture the dynamic behaviour of ts governed by the markov property based on the linkage between subsequent and previous values in the ts the order of the observations being important this could suggest that this underlying information contained in ts is not entirely conveyed in fmm hc and in particular in km or at least in those cases when these latter techniques detect just a single cluster in data k 1 fmm 17 5 hc 17 and km 43 of the cases it arises as a natural question if the lower performance of fmm hc and km is due to their use of dependent data ts such as monitored data however the literature which applies the referred techniques to ts data see the review papers and references therein from aghabozorgi et al 2015 and liao 2005 is vast the quality of a clustering solution is difficult to define pereira rodrigues and gama 2007 the focus adopted in this work was simply to select the technique with a consistent ability to partition data in more than one cluster in order to assign the lowest cluster to the background regime beyond the scope of this study a clustering validation based on the comparison of the resulting cluster structures obtained on every data set by the different techniques is currently being considered for further research in this work hmm possesses good clustering properties related to the aim of this work and data sets studied as long as they fulfil the criteria given by han et al 2012 i interpretability and usability ii discovery of clusters with arbitrary shape iii ability to deal with noisy data iv scalability results not shown and v minimum requirement of information provided by the user 3 3 implications for epidemiological research the graphical result of the evolution of the background pollution at the torneo site estimated with hmm over 11 years is represented in fig 4 the numerical characterization of the background pollution of all sites is given in sm 2 and the graphical representation of the remaining sites in sm 3 coinciding with moreno et al 2009 background concentrations in cities experienced daily variations indicating that they may be influenced by regional air quality and indirectly by local sources as seen in the previous section hmm provides two important features related to background pollution exposure namely 1 concentrations as a quantitative expression of this minimum but permanent abundance of pollution in ambient air and in this work analysed for co no2 o3 and pm10 and 2 the interval time throughout the year to which this abundance is present according to the who 2013 with respect to pm10 pollutant there is no evidence of a safe level of exposure or a threshold below which no adverse health effects occur new threshold values and estimates of exposure for this air pollutant or any other could now be used in epidemiological studies after applying hmm to air pollution these estimates can be based on the range of concentration of background pollution their mean or median and standard deviation values as indicative of their acuteness the quantitative relation between the average pollution to background pollution or their time incidence also background pollution can be studied from a single or multi pollutant perspective the background pollution data in this work was estimated on a daily means aggregation basis however the scalability of hmm allows analyses on hourly data increasing its resolution 4 conclusions aiming to propose a valid clustering technique to estimate the background pollution in urban environments four well known clustering techniques were compared under the same probabilistic framework the use of fmm and hmm are widely used to cluster data however the approximation of hc and km to a model based clustering is scarce in the air pollution literature these clustering methods were applied on 200 heterogeneous data sets to evaluate their ability to detect background pollution in a consistent manner hmm outperformed with respect to the rest of clustering techniques studied the information obtained from hmm when analysing background pollution may result of interest for epidemiological research in that it provides a full characterization of the background pollution mean standard deviation and representation of background pollution may be used as estimates of exposure to this fraction of pollution in ambient air and hence to better understand the implications of background pollution on the population s health data and software availability the data used in this study were kindly provided by the regional ministry of environment and land planning of andalusia seville spain please contact the corresponding author for any enquiries models were implemented using the open source programming environment r version 3 2 2 r development core team 2015 this software is available for download from www r project org and runs on unix windows and macos platforms source codes used in this study are available upon request disclaimer the authors declare that they have no actual or potential competing financial interest the views expressed are purely those of the author and may not in any circumstances be regarded as stating an official position of the european commission acknowledgements the authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve the paper but also to the regional ministry of environment and land planning of andalusia for kindly providing the air quality data appendix a supplementary data the following is the supplementary data related to this article supplementary data supplementary data appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 02 011 
26374,background pollution represents the lowest levels of ambient air pollution to which the population is chronically exposed but few studies have focused on thoroughly characterizing this regime this study uses clustering statistical techniques as a modelling approach to characterize this pollution regime while deriving reliable information to be used as estimates of exposure in epidemiological studies the background levels of four key pollutants in five urban areas of andalusia spain were characterized over an 11 year period 2005 2015 using four widely known clustering methods for each pollutant data set the first lowest cluster representative of the background regime was studied using finite mixture models agglomerative hierarchical clustering hidden markov models hmm and k means clustering method hmm outperforms the rest of the techniques used providing important estimates of exposures related to background pollution as its mean acuteness and time incidence values in the ambient air for all the air pollutants and sites studied keywords clustering techniques background pollution air quality time series analysis exposure health risk 1 introduction determining the population s health risks due to ambient air pollution is critical to the development of effective risk management policies and strategies samet and krewski 2007 to better understand the adverse health effects associated with air pollution accurate exposure assessment is essential epidemiological studies have provided a substantial body of evidence linking daily concentrations of outdoor air pollution to adverse effects on a range of health outcomes studies have tended to focus on the mass concentrations of particles and selected gaseous pollutants but more insight is required regarding the most harmful sources and components of the air pollution mixture to inform focused public health protection policies atkinson et al 2016 background concentration is the ambient level of pollution that is not affected by local sources of pollution who 1980 menichini et al 2007 there are two motivations for focusing on this regime i to better understand the contribution of local sources to total pollutant concentrations and ii to allow the assessment of new pollutant sources that are introduced into the area of study and their impact on local air quality however up until now research has not significantly addressed this lowest fraction of pollution as representative of a permanent concentration of ambient air pollution to which the population is chronically exposed this work focuses on this specific fraction of pollution han et al 2015 classify the methods to determine the background pollution using four categories i physical methods to identify the regional and local pollution processes via atmospheric variables ii chemical methods to identify the chemical composition of air pollutants iii numerical simulations methods using trajectory models and iv statistical methods regarding the latter langford et al 2009 used principal component analysis to describe the local background o3 concentrations recorded during 76 days in 30 monitoring sites in texas tchepel et al 2010 study the contributions to background pollution of pm10 from different sources in four monitoring sites in lisbon portugal during two days through air quality time series via spectral analysis other authors have used clustering techniques to characterize regimes in air pollution austin et al 2012 classify air pollution daily data during six years performing k means km and hierarchical clustering for identifying profiles in them beaver and palazoglu 2006 used an aggregated solution of km to characterize classes of ozone episodes occurring in the san francisco bay considerable effort has been made to characterize profiles of key air pollutants carslaw and ropkins 2012 carlsaw and beevers 2013 since the threshold values that can be considered safe for human health is still under debate pioneering research work explored this relationship for o3 and pm10 koop and tole 2006 and for pm2 5 kiesewetter et al 2015 background profiles of co and nox were studied by venegas and mazzeo 2006 in the city of buenos aires and for nox no2 and o3 in the california south coast air basin by pournazery et al 2014 this study proposes the use of statistical clustering techniques as a methodology for the estimation of background pollution in urban environments to that end four well known clustering methods were compared using data obtained from monitoring sites namely finite mixture models fmm agglomerative hierarchical clustering hc hidden markov models hmm and km this study aims to i evaluate the best clustering statistical method to estimate the background pollution and ii provide model derived exposure estimates from the best method as inputs for epidemiological research the best clustering method was assessed according to its ability to cluster the lowest concentrations of ambient air pollution in a consistent manner to that end data sets from key pollutants co no2 o3 and pm10 from five monitoring sites in andalusia south of spain were studied over 11 years 2 data and methods 2 1 air pollution data air quality data hourly average concentrations of co no2 o3 and pm10 were collected from 2005 to 2015 as independent yearly series for each pollutant these data were obtained at five monitoring sites exhibiting different typology suburban urban and predominant emission sources background traffic since monitored data were available on an average hourly basis daily mean concentrations were calculated when at least 80 of the data were available a total of 200 yearly data sets each one consisting of daily average values for a single pollutant and complete years were studied resulting from 40 55 50 and 55 data sets corresponding to the air pollutants co no2 o3 and pm10 respectively table 1 in order to favour the heterogeneity both of data and range of pollutant concentrations to study monitoring sites were selected in three different cities of andalusia córdoba jaén and seville with different meteorological conditions governing the local air pollutant behaviour the standard monitoring methods established in european directive 2008 50 ec directive 2008 were used for air pollutants co no2 and o3 and beta attenuation monitoring was applied for pm10 air quality monitoring networks are subject to an intense maintenance program to ensure accurate values prior to undergoing analysis the data obtained were validated by the regional ministry of environment and land planning of andalusia 2 2 background pollution estimation for each independent yearly data set with measurements of a single pollutant a clustering technique was applied for a clustering result each cluster represents ranges of concentration values profiles or regimes of pollution for a given pollutant that can be associated to an emission source of pollution this view is based on the lenschow approach lenschow et al 2001 that assumes that the air pollutant concentrations at a monitoring site correspond to the sum of regional urban background and local nature contributions this approach has been used as a prior analysis in source apportionment studies belis et al 2013 and may be applied to urban areas with negligible impact from industrial emissions as in case of córdoba seville and jaén the concentration measured at a traffic site corresponds to the sum of local traffic urban and regional background contributions with regard to an urban or suburban background site the contributions that explain the ambient pollution correspond to those from the background levels of the city or metropolitan area respectively and those of the regional background being a univariate clustering process the resulting clusters represent certain categorization of the original variables into a set of ranges determined by each cluster when sorting the cluster according to the associated ranges the first cluster contains the lowest values of the pollutant and it represents the range of minimum concentrations obtained at a monitoring site this work focuses specifically on this first cluster which might represent the magnitude of a kind of chronic exposure concentration experienced by the population along the year one of the most important advantages of this approach is that it allows the estimation of the first cluster representing the background pollution at any monitoring site and for any air pollutant the estimation is affected neither by the main type of pollution source present nor by the classification of the monitoring site according to its location the ability of four clustering techniques to detect the lowest cluster on different air pollutants was compared 2 3 clustering techniques fmm hc hmm and km were used to cluster data obtained from monitoring sites the aim was to study their ability to detect more than one cluster in data and therefore to be able to associate the lowest one to the background pollution regime because clustering via fmm represents the foundational model upon which the rest of the clustering techniques are based it is explained next in the interest of space a description of hc hmm and km is given in supplementary material sm 1 fig 1 illustrates the use of fmm to model the first cluster equivalent in this work to the background pollution in the no2 data distribution histogram from the aljarafe site during 2015 the information regarding the background pollution appearing in blue fmm represents a model based strategy for clustering by assuming that each cluster of data is described by a different probability distribution component these clusters are combined according to the mixing proportions representations or weights that make up the mixture making the modelling of any multi modal data set possible because of fmm s extreme flexibility mclachlan and peel 2000 in this work all the distributions were considered univariate gaussians the dispersion of the components defining each cluster is given by the standard deviation of the gaussian distributions these standard deviations can be constrained as constant across the clusters of the mixture e configuration equal variances or allowed to vary between them v configuration variable variances once the number of clusters has been fixed in advance to model the data the mixing proportion or representation the mean m and standard deviation sd of each component parameterizes fmm estimating the parameters defining a fmm that are most likely to have generated a given data set is referred to as the maximum likelihood estimation mle problem although there are many methods that can be used to estimate the parameters of a fmm the expectation maximization em algorithm dempster et al 1977 is the most widely used mclachlan and krishnan 2008 the em algorithm computes the maximum log likelihood estimates of the mixture iteratively alternating between two steps e the expectation step and m the maximisation step until a convergence criterion is met the e step calculates the log likelihood given the observed data and the current parameters estimate of the mixture and the m step maximizes the expected log likelihood from the previous e step providing a new estimation for each parameter the convergence criterion may be a permitted number of iteration of the algorithm an acceptable minimum difference ε between the parameter estimates at each iteration or both once the em algorithm converges and the parameters of the fmm estimated the log likelihood of the data can be calculated this allows obtaining the bic bayesian information criterion value schwarz 1978 of the data modelled with a specific fmm and for a given number of clusters thus several fmm differing in the number of clusters can be proposed to model a data set and the corresponding bic values calculated the fmm with larger value of the bic obtained provides the more suitable number of clusters for the data set studied the common features shared between hc hmm and km to fmm have simplified the practical implementation of the techniques in this work and remarkably provided all of them a common probabilistic foundation probabilistic clustering henceforth it was possible to address the determination of the most suitable number of clusters k in data using the bic criterion and thus a comparison of the number of clusters was obtained with the different techniques the application of bic to determine the number of clusters in fmm and hmm is well known due to its consistency in mixtures from exponential families frühwirth schnatter 2006 however the number of studies describing the application of bic to hc and km in the air pollution field is scarce in this work the bic approach avoids the pre setting of the number of clusters in hc and km empirically by the user for each clustering technique and data set studied an initial number of clusters were proposed k 1 9 and the bic values calculated for each solution the more suitable number of clusters was chosen according the maximum bic value the optimal solution k 1 was also included to verify that no cluster absence of air pollution profiles was detectable in the data sets under analysis other bayesian model selection criteria are possible general approaches for model selection are akaike s information criteria akaike 1974 the deviance information criterion spiegelhalter et al 2002 the integrated classification likelihood biernacki et al 2000 and the focused information criteria claeskens and hjort 2003 the selection of different criteria remains data dependent and no one criterion is superior to any other in general cases xu and wunsch 2009 unfortunately there seems to be no simple recommendation to guide the use of these criteria as there are no general results on these methods performance that apply to all situations in the short length ts framework as studied in this work bic criterion is a parsimonious solution for determining the number of clusters next to model selection criteria other statistics for goodness of fit can be found in mackay altman 2004 andtitman and sharples 2008 the computational implementation of all the cluster techniques was accomplished using the open source software r r development core team 2015 such implementations are available upon request to determine the optimal number of clusters in data using fmm and the parameters defining each cluster representation mean and standard deviation values the mclust function from the mclust package fraley et al 2012 was used adopting a v configuration and setting the iterations of the algorithm to unlimited this function adopts a default value for relative convergence of the log likelihood in the em algorithm of ε 10 5 2 3 1 k means km algorithm implicitly assumes that the data in each cluster are spherically distributed around the mean venables and ripley 2002 hamerly and elkan 2003 therefore it is possible to derive the km algorithm as a special case from the univariate gaussian fmm used in this work when the variance of the components adopts the e configuration same variance across the components of the mixture data was analysed with km using the kmeans function from the stats package for each k value the representation of each cluster with respect to the data set size permitted obtaining its representation weights as in fmm the common variance was calculated as the sum of the weighted variance of clusters to parameterize the km clustering solutions from a fmm approach the representation mean value from clusters and their common variance were provided as parameters to the e step of the em algorithm estep function from the mclust package e configuration bic values were later calculated using the bic function from this package 2 3 2 hidden markov models hmm belong to the model based clustering methods which provide a convenient way of formulating an extension of fmm to allow for dependent data mclachlan and peel 2000 and the mle problem can be solved using the em algorithm bulla and berzel 2008 using this clustering technique each data point represents the observed value of a time series ts at time t as in fmm the data are drawn from two or more distributions with different parameters forming a mixture which can fit multiple modes in ts an hmm is a doubly stochastic process in which an underlying stochastic process a set of discrete states can only be observed through another stochastic process that generates a sequence of observations ts data only the ts observations are visible to the observer the observations of the ts are dependent on the discrete states such that the marginal distribution of the data is a mixture distribution as in fmm the data in hmm are dependent rather than indepedent draws from the components of the mixture distribution visser 2011 an hmm is characterized by a set of states equivalents to components in fmm an initial probability distribution for the first state a transition probability matrix linking successive states and state dependent probability distributions responsible of generating the ts data however just the information characterizing the first cluster is examined m and sd as in fmm according to the aim of this study the parameters defining the mixtures in ts data was obtained using the depmix function from the depmixs4 package visser and speekenbrink 2010 using a tolerance value for the relative convergence of ε 10 5 unlimited em algorithm iterations and adopting the v configuration to obtain the bic values from each clustering solution an ad hoc r function was designed considering the parameters of the mixtures and the size of the data sets to check the validity of the modelling results obtained with the depmixs4 package the hiddenmarkov harte 2015 and hmm himmelmann 2010 libraries were also used and negligible differences were found in the parameter estimates 2 3 3 agglomerative hierarchical clustering to define the proximity between clusters in hc an approach equivalent to km was used ward s method ward 1963 attempts to minimize the sum of squared distances of data from their cluster means clarke et al 2009 everitt et al 2011 providing homogeneous spherical clusters around the cluster means an approach that is analogous to km when dealing with hierarchical clustering tan et al 2006 to perform hc for the constrained gaussian model e configuration the procedure followed by fraley and raftery 1998 later implemented in venables and ripley 2002 was applied using the function hc from the mclust package adapted to univariate data the parameterization of each candidate cluster solutions was obtained using the em algorithm em function from the same package e configuration and the bic values by using the bic function from this package 3 results and discussion 3 1 background regime study at one site during one year fig 2 illustrates the different graphical results corresponding to the first cluster analyses using the four clustering techniques fmm hc hmm and km for the air pollutants studied at the torneo site during 2015 the corresponding numerical results are given in table 2 in fig 2a below the daily average concentrations the different coloured segments indicate days in which no external contributions are detected according to the clustering technique used daily data grouped into the first cluster the ability to detect clusters in data is manifested through the k value in table 3 in co no2 and pm10 pollutants km red segments does not detect clusters in data as fmm and hc in pm10 blue and orange segments respectively except by hmm green segments the first cluster detected in no2 is markedly unspecific since the data grouped in it corresponds to almost the whole range of concentrations during the whole year therefore hmm reveals a higher resolution for detecting background concentrations in all the pollutants studied the four techniques distinguish clusters in o3 data possibly due to the distinct differences in concentration ranges experienced during the year maximum values in summer and lowest in winter where any of these clustering solutions would have been potentially valid fig 2b is equivalent to the cluster analyses represented in fig 2a following the same approach as in fig 1 except the density of the mixture is not represented by simplicity it reveals the gaussian curves characterizing the first cluster detected by the clustering methods superimposed to the histogram of data in grey coloured circles represent the average value of the first cluster m while the black circle represents the annual average value of the concentrations m as expected when a technique does not detect clusters in data the average value of the first and single cluster coincides with the average value of the data pm10 clustering in fig 1b for fmm hc and km and only one gaussian component models the data table 2 provides more valuable information focusing on the first cluster the gaussian curves provide the spread of the data sd around their means m indicating the strength of the background exposure to the different pollutants in this analysis this information can only be consistently obtained in all the pollutants by means of the hmm clustering the time incidence representation of the background regime cluster over the whole data set is given by the size of this cluster to measure the proportion of the year in which the population is exposed to the background pollution characterized by its m and sd values 3 2 selection of the best clustering technique as seen in the previous section information related to the first cluster of data allows a full description of the background pollution m and sd however this description is possible because it is based on the resolution of clustering techniques that detect more than one cluster in data k 1 otherwise the only detected cluster would parameterize the entire data set with simply one gaussian curve and would therefore not provide any valuable information fig 3 illustrates clustering methods ability to achieve this end with the number above the bars representing the data sets described with a specific number of clusters for a given technique and pollutant table 3 summarizes this information concluding that hmm is the most suitable technique to detect regimes in data and therefore to describe the background pollution in them k 1 case 99 meanwhile km 57 is an unsuitable technique with fmm 82 5 and hc 83 in an intermediate position this superior performance of hmm may be due to its ability to capture the dynamic behaviour of ts governed by the markov property based on the linkage between subsequent and previous values in the ts the order of the observations being important this could suggest that this underlying information contained in ts is not entirely conveyed in fmm hc and in particular in km or at least in those cases when these latter techniques detect just a single cluster in data k 1 fmm 17 5 hc 17 and km 43 of the cases it arises as a natural question if the lower performance of fmm hc and km is due to their use of dependent data ts such as monitored data however the literature which applies the referred techniques to ts data see the review papers and references therein from aghabozorgi et al 2015 and liao 2005 is vast the quality of a clustering solution is difficult to define pereira rodrigues and gama 2007 the focus adopted in this work was simply to select the technique with a consistent ability to partition data in more than one cluster in order to assign the lowest cluster to the background regime beyond the scope of this study a clustering validation based on the comparison of the resulting cluster structures obtained on every data set by the different techniques is currently being considered for further research in this work hmm possesses good clustering properties related to the aim of this work and data sets studied as long as they fulfil the criteria given by han et al 2012 i interpretability and usability ii discovery of clusters with arbitrary shape iii ability to deal with noisy data iv scalability results not shown and v minimum requirement of information provided by the user 3 3 implications for epidemiological research the graphical result of the evolution of the background pollution at the torneo site estimated with hmm over 11 years is represented in fig 4 the numerical characterization of the background pollution of all sites is given in sm 2 and the graphical representation of the remaining sites in sm 3 coinciding with moreno et al 2009 background concentrations in cities experienced daily variations indicating that they may be influenced by regional air quality and indirectly by local sources as seen in the previous section hmm provides two important features related to background pollution exposure namely 1 concentrations as a quantitative expression of this minimum but permanent abundance of pollution in ambient air and in this work analysed for co no2 o3 and pm10 and 2 the interval time throughout the year to which this abundance is present according to the who 2013 with respect to pm10 pollutant there is no evidence of a safe level of exposure or a threshold below which no adverse health effects occur new threshold values and estimates of exposure for this air pollutant or any other could now be used in epidemiological studies after applying hmm to air pollution these estimates can be based on the range of concentration of background pollution their mean or median and standard deviation values as indicative of their acuteness the quantitative relation between the average pollution to background pollution or their time incidence also background pollution can be studied from a single or multi pollutant perspective the background pollution data in this work was estimated on a daily means aggregation basis however the scalability of hmm allows analyses on hourly data increasing its resolution 4 conclusions aiming to propose a valid clustering technique to estimate the background pollution in urban environments four well known clustering techniques were compared under the same probabilistic framework the use of fmm and hmm are widely used to cluster data however the approximation of hc and km to a model based clustering is scarce in the air pollution literature these clustering methods were applied on 200 heterogeneous data sets to evaluate their ability to detect background pollution in a consistent manner hmm outperformed with respect to the rest of clustering techniques studied the information obtained from hmm when analysing background pollution may result of interest for epidemiological research in that it provides a full characterization of the background pollution mean standard deviation and representation of background pollution may be used as estimates of exposure to this fraction of pollution in ambient air and hence to better understand the implications of background pollution on the population s health data and software availability the data used in this study were kindly provided by the regional ministry of environment and land planning of andalusia seville spain please contact the corresponding author for any enquiries models were implemented using the open source programming environment r version 3 2 2 r development core team 2015 this software is available for download from www r project org and runs on unix windows and macos platforms source codes used in this study are available upon request disclaimer the authors declare that they have no actual or potential competing financial interest the views expressed are purely those of the author and may not in any circumstances be regarded as stating an official position of the european commission acknowledgements the authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve the paper but also to the regional ministry of environment and land planning of andalusia for kindly providing the air quality data appendix a supplementary data the following is the supplementary data related to this article supplementary data supplementary data appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 02 011 
