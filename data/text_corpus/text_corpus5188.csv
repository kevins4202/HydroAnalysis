index,text
25940,we present a new strategy for performing global sensitivity analysis capable to estimate main and interaction effects from a generic sampling design the new strategy is based on a meaningful combination of variance and distribution based approaches the strategy is tested on four analytic functions and on a hydrological model results show that the analysis is consistent with the state of the art saltelli jansen formula but to better quantify the interaction effect between the input factors when the output distribution is skewed moreover the estimation of the sensitivity indices is much more robust requiring a smaller number of simulations runs specific settings and alternative methods that can be integrated in the new strategy are also discussed overall the strategy is considered as a new simple and effective tool for performing global sensitivity analysis that can be easily integrated in any environmental modelling framework keywords global sensitivity analysis variance distribution generic sampling design software availability the analysis has been performed with the statistical software r 3 3 x r core team 2019 and the saltelli jansen analysis using the package sensitivity iooss et al 2019 the simulations with the hydrological model have been conducted using the package hydromad andrews et al 2011 an example script on how to implement the new combined variance and distribution based cvd strategy is freely available under https github com baronig gsa cvd 1 introduction global sensitivity analysis gsa refers to a group of diagnostic modelling tools developed to study how the uncertainty in the output of a mathematical model can be apportioned to the uncertainty in the input factors saltelli et al 2000 in this context the term factor is interpreted in a broad sense of anything that can be subject to some degree of uncertainty in the model e g parameters input boundary conditions or model structure ratto et al 2007 in contrast to the local one at the time approach where one single factor is perturbed while keeping the other fixed gsa approaches are applicable independently of the characteristics of the input output response function they cover the entire input space and they identify non linearity and interactions between the factors saltelli et al 2008 for these reasons they have been recognized as a fundamental analysis to support model understanding and improvements in many applications baroni et al 2018 borgonovo et al 2017 demirel et al 2018 guse et al 2016 haghnegahdar et al 2017 pianosi and wagener 2016 reusser et al 2011 rosolem et al 2012 savage et al 2016 schürz et al 2019 tang et al 2007 xie et al 2017 and they should be preferred to local approaches to avoid a perfunctory analysis saltelli et al 2019 saltelli and annoni 2010 several gsa methods have been developed and we refer to review papers and text books for an overview iooss and lemaître 2015 pianosi et al 2016 razavi and gupta 2015 saltelli et al 2008 song et al 2015 wagener and pianosi 2019 wei et al 2015 here we narrow the discussion to two probabilistic approaches based on monte carlo simulations the variance based approaches and the distribution based also called density based or moment independent approaches sensitivity analysis based on variance measures has been introduced in the early 70s bier 1982 cukier et al 1973 iman 1987 major contributions on the approach however are attributed to the russian mathematician i m sobol who generalized the approach and provided a straightforward monte carlo based implementation sobol 2001 at present the most widely used variance based measures are the so called sobol indices and in particular sobol first order sensitivity measure or main effect together with the total sensitivity indices or total effect introduced by homma and saltelli 1996 the use of these indices enjoyed success among practitioners probably due to a clear interpretation of their meaning saltelli et al 2008 specifically as described in ratto et al 2007 high values in the main effect represent the factors that when constrained at their true value would reduce the uncertainty in the model output the most and therefore make the model inference more robust this type of analysis supports the prioritization in model improvements thus it has been identified in literature with the term factor prioritization on the contrary low values for total effect identify those factors that have an irrelevant contribution to the uncertainty in the output and therefore can be constrained to an arbitrary value within their range of uncertainty e g supporting model simplification this type of analysis has been identified with the term factor fixing it should also be noted that the two analyses defined above can be performed independently i e either estimating the main effect for factor prioritization or the total effect for factor fixing however it has been underlined that the differences between total and main effect reveals interactions among the factors saltelli et al 2008 this additional information is very important when the modelers are interested to know if the specific factor is identifiable by e g calibration ghasemizade et al 2017 guillaume et al 2019 namely a factor that shows high interactions is likely not identifiable so for sake of clarity we term this third type of analysis factor identification that in contrast to factor prioritization and factor fixing can be achieved only by estimating both indices despite its wide use it has been argued that variance based approaches rely too heavily on the assumption that the variance is sufficient to describe the uncertainties and sensitivities encountered park and ahn 1994 the variance measure is ill suited particularly to measure the dispersion of a variable with a heavy tail or a multimodal distribution or which contains some outliers auder and iooss 2009 to overcome this limitation some approaches have been developed based on the idea of comparing the entire probabilistic distributions of the model output rather than the variance the first methods have been derived based on the entropy measures liu et al 2006 park and ahn 1994 borgonovo 2007 developed an important measure based on density functions more recently pianosi and wagener 2015 suggested the use of the kolmogorov smirnov test kolmogorov 1933 generalization of the methods have been also presented for comparing the entire distributions veiga 2015 or targeting some of their statistical moments dell oca et al 2017 these distribution based methods have also been used in several studies borgonovo et al 2011 castaings et al 2012 fox et al 2016 gillies et al 2016 hosseini et al 2018 pianosi and wagener 2016 pilz et al 2017 schürz et al 2019 sedighian et al 2015 and they have showed to converge faster than variance based methods pianosi and wagener 2015 zadeh et al 2017 however these methods focus on estimating only one single effect and separating main total effect and interactions is not targeted for this reason the modelers remain without the identification of important features that can be achieved in global sensitivity analysis saltelli and tarantola 2002 to overcome this limitation and exploit the advantages of both variance and distribution based approaches their combined use has been suggested and tested in literature borgonovo et al 2017 massmann and holzmann 2012 pappenberger et al 2008 most of the studies emphasized the complementarity of the different methods however they also highlighted the difficulties to directly compare the different indices because they are based on different quantities they explore different ranges in the input output space and thus they carry different information borgonovo and tarantola 2008 mora et al 2019 in this study we develop and test an effective strategy to combine variance and distribution based sensitivity analysis we refer to this combined strategy with the term cvd strategy this strategy has been developed to take advantages of the two approaches and to allow a meaningful combined use of the different indices the paper is structured as follows in section 2 we first revise variance and distribution based approaches to better identify similarities and complementarities this provides the basis for the development of the new cvd strategy this new strategy is tested with four analytic functions section 3 and on a hydrological model section 4 in all tests the strategy is compared to the state of the art saltelli jansen formula for estimating both main and total effect the discussion is enriched based on specific settings and possible improvements section 5 conclusions are reported in section 6 2 methods 2 1 variance based approaches we start considering a numerical deterministic model that can be written in the form 1 y g x i where x i are the input factors with i 1 k and k the number of factors g is the generic numerical model and y the output of the model x i can be regarded as any type of input factor parameters model structure input and boundary conditions considering that all types of factors can be associated to a scalar discrete value baroni and tarantola 2014 lilburne and tarantola 2009 plischke et al 2013 we proceed also here assuming that the factors x i are a scalar input e g parameters we now consider that the values of x i are not known and their uncertainty can be described by their probability distributions p x i e g fully defined by mean and standard deviation in case of gaussian distributions we underline that this first task distribution assignment is a crucial step in ensuring the quality and consistency of the results baroni et al 2017 haghnegahdar and razavi 2017 plischke et al 2013 shin et al 2013 then the distributions are sampled n times creating a n x k matrix x the model can be now run in a monte carlo approach as follows 2 y g x the probabilistic distribution of the vector y can be now defined as p y and can be seen as the uncertainty in the model output by propagating the uncertainty in k input factors variance based approaches implicitly assume that the variance the second moment here identified with the term v is sufficient to describe the probabilistic distribution p y thus a generic uncertainty importance measure of the factor i in explaining the uncertainty in the model output can be introduced as the reduced variance one would achieve by fixing one source of uncertainty as follow expressed in relative terms 3 v y x i e x i v y where e indicates the mean operator and v y x i e x i indicates the conditional variance in the model output y when the factor i is fixed to its mean e x i dividing this quantity over the unconditional variance v y leads to an index that quantifies the fraction of remaining variance in the model output when the correct value of factor i is known besides possibly having the problem of an index bigger than one conditional variance unconditional variance one should recognize that e x i could be not necessarily the true i e optimal value thus this measure would not be a good estimation of the importance of the factor iman 1987 for this reason it has been suggested to calculate the conditional variance v y x i based on x i fixed to r values in the range p x i and to calculate their mean as follow 4 e v y x i v y in the simplest case the number of conditional values r could be limited to the two extreme values within the range p x i borgonovo 2010 francke et al 2018 iman 1987 most preferable however the values should cover the entire range in the distributions now when this importance measure is small x i is an important factor considering that the model output variance can be decomposed as the sum of the variance of the conditional expectations e v y x i and the residual term v e y x i mood et al 1974 5 v y e v y x i v e y x i the importance measure can be formulated in a complementary way as 6 s i v e y x i v y now when this importance measure is high x i is an important factor this expression is usually referred to as sobol first order sensitivity measure or main effect and it is indicated in literature with the symbol s i as discussed in the introduction this index represents the mean variance that one would reduce if the factor is known for this reason high values in the main effect identify the most important factors that should be considered to decrease the uncertainty in the model or improve the model performance thus this index supports the so called factor prioritization it has been noted that the factor i could have a direct effect on y or its importance could be related to the effect that the factor has due to interactions with other factors for this reason another index has been proposed as follows 7 t i e v y x i v y where v y x i indicates the variance of y fixing all factors but not i this terms is usually referred as total sensitivity index or total effect as introduced by homma and saltelli 1996 please note that the index is generally denoted with the symbol s i t but for simplicity we adopt the notation t i as suggested by glen and isaacs 2012 as discussed in the introduction this index represents the main effect of the factor and its contribution based on interactions with other factors low values of t i identify factors that are not important and can be fixed thus this index can serve for factor fixing finally we explicitly derive the interaction term as a simple difference between the two indices 8 i i t i s i this index is used to quantify the effect of the factor i due to interactions with the other factors and it is used in combination of the main effect to understand a factors identifiability i e if i i is small and s i is high the factor is likely identifiable 2 2 estimation of first and total sensitivity index in principle the computational cost to estimate s i assuming the same number of samples n for approximating the unconditional and conditional distributions is 9 n k 1 r n where n is the required number of model evaluations k is the number of factors i r is the number of conditional values and n the sample size to approximate the distributions saltelli et al 2008 this tailored sampling design is usually referred to as brute force double loop sampling in the first loop for the current factor i x i is set to r prescribed values while in the second loop n realizations of the remaining inputs x i are generated when additionally t i is estimated the cost of the tailored sampling design increases further obviously such an estimation is impracticable when the computational cost of running the model is high e g more than minutes for one single run for this reason several methods have been developed to reduce the computational burden specifically there are several computationally affordable methods proposed in literature to compute the first order sensitivity indices cukier et al 1973 lewandowski et al 2007 mara et al 2017 mara and joseph 2008 mckay et al 1999 oakley and o hagan 2004 plischke 2010 ratto et al 2007 strong et al 2012 tarantola et al 2006 in most of these cases a dedicated sampling design is used e g based on fourier transformation or resampling however the estimation can be also directly performed based on a generic sampling design kucherenko et al 2012 wainwright et al 2014 plischke 2010 li and mahadevan 2016 strong and oakley 2013 in the simplest case kucherenko et al 2012 noted that there is no the need of brute force double loop sampling eq 9 but the analysis can be directly performed based on a filtering process over a generic sampling design of the cost n the strategy is illustrated in fig 1 upper row where the input output space of a three parameter function is shown based on scatterplots based on this strategy the ranges of x i are partitioned into m conditioning intervals of equal size in each interval the conditional mean i e e y x i is calculated these values are represented in fig 1 upper row by the red dots then the main effect is estimated as the variance of these conditional means variance of the red dots these indices are shown in the bar plot on the right side the indices indicate for this case high importance of the first factor x 1 and no main effect for x 2 and x 3 this method has been found to perform well in estimating s i in comparison to other approaches kucherenko and song 2017 li and mahadevan 2016 similar simple approaches have been not found however for estimating the total effect in contrast the estimation of this index relies on specific sampling designs fourier or resampling and it is much more expensive to compute glen and isaacs 2012 saltelli et al 2010 1999 currently one of the most applied and effective method is the one discussed by saltelli et al 2010 it has shown to perform well in several applications with the advantage of being extendable to the analysis of any sources of uncertainty baroni and tarantola 2014 lilburne and tarantola 2009 savage et al 2016 we briefly introduce this approach also within the present study as reference first two independent sets of input sample matrices a and b each of which is an n k matrix containing n sets of k dimensional parameter vectors from monte carlo sampling are generated sobol quasi random sampling sobol 1976 is usually suggested for this purpose because it showed to increase the rate of convergence of the estimators becker et al 2018 kucherenko et al 2011 tarantola et al 2012 but other techniques can be used as well e g latin hypercube sampling from a and b a matrix c i i 1 2 k is created for each factor such that the i th column of c i is the same as the i th column of a and the other columns of c i are the same as b the number of required simulations is then n n k 2 based on these matrices different estimators of the main effect eq 6 and total effect eq 7 can be applied saltelli et al 2010 for ease of interpretation we report here the estimation first presented by jansen 1999 10 s ˆ i 1 1 2 n i 1 k g a g c i 2 v g a 11 t ˆ i 1 2 n i 1 k g b g c i 2 v g a 12 i ˆ i t ˆ i s ˆ i the hat is here used to indicate that these terms are estimation of the indices expressed by eqs 6 8 as discussed by wainwright et al 2014 these estimations offer an intuitive way to understand the meaning of the sobol indices that we report also here for the sake of clarity the equations represent in fact a correlation based measure between two matrices in the first estimation eq 10 the parameter sets in a and c i share the same values only for parameter i perturbing all the other factors except for i includes the total effects involving all the factors except for the first order effect of i if i is very influential its value mainly determines the results so that g a and g c i should be similar and the differences g a g c i small in the second index eq 11 b and c i have the same values except for i when we perturb i with the other parameters fixed the difference g b g c i hence t i accounts not only for the impact of i as a single factor but also for the interaction effects with the other factors if the parameter i is very influential this factor determines the results so that g b and g c i should now be different and the differences g b g c i become high finally the difference i i between total effect and main effect eq 12 quantifies only the contribution of the factor i due to interactions 2 3 distribution based approaches distribution based important measures follow the same derivation as described in section 2 1 for variance based approach they vary only in the statistical operator to compare the conditional and the unconditional probabilistic distributions plischke et al 2013 veiga 2015 specifically some authors liu et al 2006 park and ahn 1994 compared the distributions based on the kullback leibler entropy metric kullback and leibler 1951 in both cases however the analysis has been performed only by fixing x i to its mean value i e as in eq 3 for the case of variance based approach 13 k l i p y x i e x i log p y x i e x i p y d y alternative distance measures have also been proposed chun et al 2000 later borgonovo 2007 developed the so called δ measure based on the absolute difference between the density functions f in addition he extended the previous methods by taking a statistic half of the average e over r conditional values to eliminate the dependency of the conditional point the mean of x i as it has been developed for the variance based approach see eq 4 14 δ i 1 2 e f y x i f y d y more recently pianosi and wagener 2015 introduced the pawn method which uses r conditional values as well but the comparison between conditional and unconditional distributions is performed on the cumulative probabilistic distribution based on the kolmogorov smirnov test ks kolmogorov 1933 15 p a w n i m e d i a n k s p y x i p y where median is often used as statistical operator even if other operators e g mean maximum can also be considered pianosi and wagener 2015 additional measures have also been introduced underlining the strong analogy between them veiga 2015 however a comprehensive comparison of the different operators has not been carried out so far all these distribution based approaches have been proposed based on the same tailored sampling design discussed for estimating the main effect in variance based approach eq 9 however in the same way that it has been tested for estimating the main effect kucherenko et al 2012 later it has also been noted that the estimation of the distribution measures can also be performed based on filtering a generic sampling of the cost n without the need of the double loop brute force sampling design pianosi and wagener 2018 plischke et al 2013 it should be noted however that extending the distribution based approach to quantify both main and total effects as performed for variance based approach i e also comparing conditional distributions when all factors except i are fixed as defined in eq 7 is not practical as the number of simulation greatly increases kucherenko et al 2012 saltelli et al 2010 1999 for this reason the distribution based approach has been used mainly to estimate one index and it has been extended to quantify main and total effects when the number of conditional points r have been limited to one i e the mean of x i liu et al 2006 finally it is interesting to note that despite the difficulties to directly compare variance and distribution based measures it has been underlined that the distribution based index contains global information and should be compared to the total effect index derived in variance based approach auder and iooss 2009 this can be seen by noting that the estimation of the variance based total effect t i in eq 11 i e g b g c i is equivalent to assessing the difference in y when perturbing x i with the other parameters fixed this is like what happens also for the distribution based comparison see eq 13 15 2 4 an effective combined strategy the basis for an effective strategy for combining variance and distribution based global sensitivity analysis starts by recognizing two key aspects of the approaches first both main effect kucherenko et al 2012 kucherenko and song 2017 li and mahadevan 2016 wainwright et al 2014 and distribution indices pianosi and wagener 2018 plischke et al 2013 can be estimated based on a generic sampling design i e n samples are filtered into m intervals to approximate the conditional distributions this reduces the computational cost in comparison to the double loop sampling design in addition it facilitates the analysis because it can be used after any generic monte carlo simulations that has been previously conducted for e g uncertainty propagation assessment second the two indices calculated based on variance and distribution based approaches carry different meanings in the variance based approach the index refers to the main effect while in the distribution based approach it carries global information associated also to interactions for these reasons the combined use has the potential to address based on a generic sampling design all the three purposes of a global sensitivity analysis factor prioritization based on main effect factor fixing based on total effect and factor identifiability based on interaction effect at this stage however the combined use of main effect and distribution based index is not meaningful because the actual values are not comparable to overcome this limitation we proceed as follow first we estimate the main effect s i eq 6 based on a generic sampling design specifically the estimation can be simply performed based on the filtering approach previously described kucherenko et al 2012 kucherenko and song 2017 i e based on the variance of the conditional mean e y x i estimated in m intervals fig 1 upper row red dots the estimation however can be boosted by interpolating the input output model response green dashed lines in fig 1 and calculating the variance over the interpolated values as suggested in other studies saltelli et al 2008 wainwright et al 2014 in the following we applied this approach using a smooth spline for interpolating the one dimensional input output space we refer to this estimation with the term s i to distinguish it from the term s ˆ i that is used to denote the index estimated based on saltelli jansen approach eq 10 a comparison between calculating the variance over the interpolated values green dashed line versus using the conditional means red dots is presented in the discussion section we now remove the main effect from the conditional distributions by centralizing each conditional distribution i e removing the mean 16 p y x i p y x i e y x i these m centralized conditional distributions are shown in fig 1 lower row differences to the upper row result only for factor x 1 for which now the mean of the conditional distributions red dots are equal to zero too by removing the main effect we now define an index based on these centered conditional distributions to have an estimation of the interaction effects alone fig 1 lower right corner namely we can compare the conditional and unconditional distribution as performed by the distribution based approaches see eqs 13 15 however we rather define the interaction index by comparing only all the combinations c m 2 of the centered conditional distributions p to better isolate the effect to the output of changing x i as follow 17 i i s t a t o p y x i r j p y x i r j 1 where r 1 r m represent the m intervals stat is a statistical operator e g mean median and o is the distribution based measure we use here the term i i to distinguish from the term i ˆ i calculated as the difference between total and main effect estimated using variance based approach eq 12 in the following we refer to this general combined variance and distribution based strategy with the term cvd strategy in principle any operator o described above e g entropy based δ measure or kolmogorov smirnov test can be used within the cvd strategy although it is shown that these measures have some analogies veiga 2015 however a comprehensive inter comparison in their use within the distribution based approaches has not been performed so far in the following we proceed with the use of the δ measure of borgonovo 2007 eq 14 specifically we fit the centered conditional distributions p via a kernel density function parzen 1962 and we quantify the difference between all the combination c m 2 of the centered conditional density functions f as follow 18 i i 1 2 c j 1 m 1 p 2 m f y x i r j f y x i r p p j a comparison between this operator and the alternative kolmogorov smirnov test used in the pawn method is presented in the discussion section 3 test functions 3 1 functions and settings we test the cvd strategy described in section 2 4 for estimating main effect s i and interaction effect i i on four analytic functions often used as a benchmark for sensitivity analyses studies borgonovo 2007 cuntz et al 2015 kucherenko et al 2009 kucherenko and song 2017 mai and tolson 2019 pianosi and wagener 2015 plischke et al 2013 saltelli et al 2008 these functions represent model of increasing complexity number of factor k between 3 and 15 and with different output distributions increasing skewness for comparison we also apply the method of saltelli jansen for estimating main effect s ˆ i and interaction effect i ˆ i based on eqs 10 and 11 and the distribution based measure pawn i eq 15 thus for each function we calculate and compare five indices the first function is the ishigami homma function 19 y sin x 1 a 1 sin x 2 2 a 2 x 3 4 sin x 1 where all x i follow a uniform distribution over π π different values of the parameters a1 and a2 are encountered in literatures in the following we use a 1 2 and a 2 1 as used by pianosi and wagener 2015 to allow for a direct comparison the function exhibits strong non linearity and non monotonicity with interactions between the two terms x 1 and x 3 the input output response is also shown in the scatter plots of fig 1 the second function has been introduced by oakley and o hagan 2004 20 y a 1 t x a 2 t sin x a 3 t cos x x t m x where x is a vector of k 15 input factors sampled over a standard normal distribution i e mean 0 and standard deviation 1 and a j j 1 2 3 and m are three k vectors and a matrix k k of parameters respectively the weights a 1 a 2 and a 3 are chosen so that one group of five input factors x i 10 15 accounts for most of the output variance while the remaining factors have smaller effect in contrast interaction contribution is almost equally distributed to all the factors the values of these weights and of the matrix m can be downloaded from www sheffield ac uk st1jeo and are also reported in saltelli et al 2008 the third function is the so called g function 21 y i 1 k 4 x i 2 a i 1 a i where the terms x i are k independent variables uniformly distributed in the unit hypercube 0 1 and the terms a i are positive integers that condition the importance of the factor x i the smaller a i the higher main and total effects of x i on the function are mara 2009 this function is a more complicated nonlinear and non additive function that produces a right skewed output distribution it is characterized by the presence of interactions among the model inputs generated by their multiplication we use a six dimensional version of the function k 6 with coefficients a i equal to 0 0 5 3 9 99 99 as used in other studies glen and isaacs 2012 saltelli et al 2010 resulting in decreasing sensitivity of the output to the six factors finally we consider the function introduced by bratley et al 1992 22 y i 1 k 1 i j 1 i x j where k 10 and all x i follow a uniform distribution over 0 1 like the g function the input factors also have a wide range of main and interaction effect decreasing with j but the function produces a left skewed model output distribution for each function the five sensitivity indices are estimated based on sobol quasi random sampling sobol 1976 saltelli jansen estimations are performed based on n k 2 2 15 simulation runs the pawn and cvd indices are estimated based on n 217 simulation runs all these indices are calculated by increasing sample size n to assess the convergence of the estimation moreover the analyses are repeated 100 times to assess the robustness of the estimation and the approach of owen 1995 is used to introduce randomness to the deterministic sobol sequence the accuracy of the estimation is quantified based on the absolute mean error mae as follow 23 m a e 1 k i 1 k e i r i where k is the respective number of input factors i and e and r represent the estimated and reference sensitivity index respectively for the main effect s ˆ i and s i the analytic values are used as references for the interaction terms i ˆ i and i i however the comparison is not straightforward as the values represent different quantities and the analytic values of the distribution based indices are not available for this reason the assessment of the interaction terms is performed using as references in eq 23 the average indices over the 100 repetitions obtained with the maximum sample size n 4 2 results of the test functions fig 2 shows the five indices obtained for the four test functions the average indices over the 100 repetitions achieved with the maximum number of simulation n are plotted the indices are combined as conducted for other approaches morris 1991 to visualize their relation and their different information content please also note that error bars one standard deviation based on the 100 repetitions are also plotted but they are not always visible indicating the robustness of the estimation achieved with the maximum sample size n the analysis conducted based on saltelli jansen method on the ishigami homma function fig 2 first column second row identifies x 1 as an important factor based on the main effect s ˆ i see also scatter plots of fig 1 the same interaction effects i ˆ i are quantified for the factors x 1 and x 3 the results suggest that x 2 is not relevant and can be fixed to any value e g mean x 1 and x 3 have interactions and are not identifiable the pawn i index fig 2 first column third row ranks the factors consistently x 1 x 3 x 2 however based on this index alone it is not possible to identify whether the factors have interactions or not the comparison between pawn i index and the main effect s ˆ i suggests that the importance of the parameters x 2 and x 3 result from interactions but it is not possible to conclude if these interaction effects are similar or not indicating the difficulties to compare variance and distribution based indices as underlined in other studies borgonovo and tarantola 2008 mora et al 2019 the main effect s i estimated based on the spline interpolation implemented in the cvd strategy fig 2 first column last row is consistent with saltelli jansen method in addition the new interaction term i i eq 18 correctly identifies the same interaction effects on x 1 and x 3 thus the results show how the cvd strategy correctly removes the main effect and the distribution measure is now able to discriminate specifically the contribution due to the interaction effect the same conclusions are derived by looking at the results of the oakley and o hagan function fig 2 column 2 the saltelli jansen method correctly identifies the most important five input factors based on the main effect x i 10 15 moreover it quantifies interaction as evenly distributed over all the factors in contrast the pawn i indices show strong correlation with the main effect but we are not able to conclude if this contribution is due to direct effect of the input factors to the model output or based on interactions in contrast the cvd strategy estimates well the main effect and it quantifies the interaction terms as detected by satelli jansen method some additional considerations arise by looking at the results of the other two functions for which model outputs show strong skewed distributions fig 2 column 3 and 4 all the three approaches rank the factors consistently however pawn i indices and the interactions terms i i of the cvd strategy show higher values for the input factors x 1 in contrast the interaction terms i ˆ i estimated based on saltelli jansen method rank x 1 and x 2 equally while it cannot be concluded by looking at pawn i indices what is the reason of these differences by removing the main effect with the cvd strategy eq 16 the results suggest that the difference is due to stronger interactions of the factor x 1 with the other factors that cannot be detected using the variance as summery statistic we further explore this hypothesis by looking at the input output space of the first two input factors of the bratley function fig 3 the variability of the conditional means calculated in the m intervals red dots in the upper row is larger for the factor x 1 than for x 2 confirming also by visual inspection that the first factor is more important in explaining the model output variability higher main effect we now remove these main effects by centralizing each conditional distribution eq 16 to isolate the interaction effect fig 3 lower row when removing these conditional means the obtained centered conditional distributions of each input factor are very different in the case of x 1 the distribution in the first left interval m is very narrow and then the spread strongly increases in contrast the differences between the centered conditional distributions of the factor x 2 are smaller despite these differences when looking at the two factors it is interesting to note that the variances calculated in each centered distributions are comparable see green square in fig 3 lower row thus the results show how the variance is not an adequate summary statistic in this specific case for this reason the variance based sensitivity analysis fails to reveal the difference between the factors and assigns the same interaction terms to both in contrast these differences can only be distinguished by looking at their entire distributions and for this reason the distribution based measures are better suited to quantify these interaction terms same conclusions are supported by looking at the input output space of the g function data not shown 4 3 convergence and robustness of the estimations the mean absolute error mae eq 23 is shown in fig 4 for increasing sample size n to assess the convergence of the estimation of the indices in addition we visualize the width of the 95 confidence intervals of the performance metric distribution obtained with the 100 repetitions to assess the robustness of the estimation variability over the repetitions please also note that the axes are plotted in log10 base to better illustrate the behavior for small n results show that the numerical estimates converge towards the reference values and the dispersion is constantly reduced as the sample size increases the results confirm the accuracy of saltelli jansen approach in estimating references indices but the need of a relatively high number of simulations n 104 in contrast indices estimated based on the cvd strategy are more robust lower spread over the replicates and converge faster than jansen saltelli method n 103 specifically the spline interpolation method used to estimate the main effect integrated in the cvd strategy outperforms jansen saltelli method in all the test functions therefore the results confirm the availability of efficient alternative methods when we are interested to quantify only the main effect kucherenko and song 2017 plischke et al 2013 strong and oakley 2013 wainwright et al 2014 in contrast the estimation of the interaction terms requires a relative larger sample size also with the cvd strategy still the cvd strategy shows to converge faster to the reference index and to be much more robust smaller spread over the replicates than the saltelli jansen method the only relevant exception is detected for the interaction terms of the ishigami homma function fig 4 second column first row for which the robustness is still much stronger than saltelli jansen method smaller colored envelopes but the convergence to the reference values requires a larger sample size n this behavior is explored in detail by looking at the estimation of the indices of each input factor fig 5 the results confirm that the interaction terms estimated based on saltelli jansen method are well estimated upper plot but the estimation is not robust at a relative low sample size n 104 when looking at the cvd strategy bottom plot the factors are well ranked at relative low samples size too noteworthy however the convergence of the estimation of the interaction term for the factor x 2 is much slower than for the other two factors this low performance can be explained considering that x 2 has a negligible interaction contribution i 0 when a relative low sample size n is used however the distribution based measure eq 18 is affected by the numerical approximation of the conditional distributions and a higher value is quantified some possible improvements to address the estimation of very low values of the interaction indices are discussed in section 5 4 a practical workflow based on a hydrological model 4 1 sac sma model and setting analysis we performed the global sensitivity analysis based on the cvd strategy and on the variance based approach of saltelli jansen to the sacramento soil moisture accounting model sac sma this is an intermediate complexity conceptual rainfall runoff model that represents the soil column by an upper and lower zone of multiple storages burnash 1995 it has been used extensively in both research and operational applications the model has also been used in the context of gsa and parameter identifiability blasone et al 2008 shin et al 2013 van werkhoven et al 2009 2008 the input parametrizations and observations of shin et al 2013 is used within the present study to allow a comparison the 13 parameters and ranges are listed in table a1 the model runs for 10 years the first year was not included in the sensitivity calculations to allow the model states to warm up and remove any impact of uncertain initial conditions the ranges of the parameters are sampled based on sobol sampling design with n 13 2 2 13 122880 model evaluations for the saltelli jansen estimation and n 2 17 131072 for the cvd strategy these sample sizes have also been used in other studies shin et al 2013 van werkhoven et al 2009 simulations and analyses are repeated 100 times and the approach of owen 1995 is used to introduce randomness to the deterministic sobol sequence the modelling results are evaluated based on the following performance metrics for which the sensitivities are assessed 24 n s e 1 t 1 t o t y t 2 t 1 t o t e o 2 25 n s e 1 t 1 t o t y t 2 t 1 t o t e o 2 1 t 1 t o t y t 2 t 1 t o t e o 2 where t is the number of time steps t y and o are simulated and measured river discharge respectively and e o indicates the mean of observation over the time series eq 25 is the nash sutcliffe index and has been commonly used for river discharge assessment its value is within in the range 1 the modified version eq 25 has been introduced by mathevet et al 2006 and it is bounded between 1 1 thus it reduces the influence of large negative values without otherwise changing the interpretation of the objective function shin et al 2013 the two metrices have been selected as example to show the differences in the sensitivity results when model output present different distributions 5 2 results of the hydrological model the results of the sensitivity analysis are shown in fig 6 the indices are plotted to visualize the relation between main and interaction effect of each parameter as conducted for other approaches morris 1991 each plot is also arbitrary divided dashed gray line to better highlight low values in the indices for the case of the modified nse the saltelli jansen approach estimates low main effect and low interaction for most of the parameters fig 3a gray dots thus the values of these parameters can be fixed to any arbitrary values within their ranges and they can be omitted during further analysis in contrast the main effect of the parameters uztwm and pctim related to the soil water capacity and the land surface characteristic respectively see appendix is relatively large 0 26 for this reason these parameters can be targeted for further model improvements by e g calibration the estimated interaction term however is low for pctim while is large for uztwm thus the analysis suggests that pctim is likely identifiable while the optimum value of uztwm can have strong dependencies with the values of the other parameters for this reason this parameter could be not identifiable these conclusions are also supported by the corresponding indices estimated based on the cvd strategy fig 6a gray dots the main difference is only detected for the parameter pctim for which a large interaction term is quantified even larger than the interaction index estimated for the parameter uztwm this difference to the results of the saltelli jansen method can be explained by looking at the input output space for these parameters fig 7 the variability of the conditional means red dots of the two parameters is similar in terms of range and smoothness fig 7a and b however when removing these conditional means to isolate the interaction effect the obtained centered conditional distributions are very different in the case of uztwm fig 7c the distributions are very similar and the main difference is in the variance see green squares in fig 7c for this reason the variance based approach can capture the interaction term in contrast the centered conditional distributions for the parameter pctim are very similar when looking at the variances green squares in fig 7d and they can only be distinguished by looking at their entire distributions for this reason the distribution measure can capture the interaction term for this parameter while the variance based approach does not quantify any interaction these results are particularly interesting in the light of the parameter identifiability as an example the best 1 simulations are filtered and represented in the scatterplots orange color fig 4a and b in the case of the parameter uztwm the best 1 simulations narrow the prior distribution to the lower value of its range in contrast the same best 1 simulation are evenly distributed in the range of the parameter pctim thus the results show how pctim is less identifiable than uztwm for this reason these results underline how the cvd strategy can better capture the interaction effects between the parameters and it is more consistent with the identifiability analysis when the differences in the distributions are not well represented by their variance the same results are obtained when looking at the interaction terms obtained with the standard nse fig 6 lower row in contrast however a lower main effect is estimated for the parameters uztwm and pctim in addition the parameter pctim becomes slightly more influential than uztwm these differences can be explained considering that the standard nse yields strongly skewed distribution with very low performances obtained on a few samples in this case it is hard to compute meaningful statistics to summarize the whole distribution i e mean and variance can be biased by few outliers thus the result confirms the sensitivity of the variance metric to measure the dispersion of a variable with a heavy tail or which contains some outliers auder and iooss 2009 and the need to properly define the performance metrics for the model output it is noteworthy that the confidence intervals calculated based on 100 replicates are very large in the case of saltelli jansen estimations thus these results show the difficulties to apply this method when the output distributions are strongly skewed for this reason we recommend checking for the normality of the output distribution to understand the reliability in the use of this method in contrast the indices estimated based on the cvd strategy are very well estimated even in the face of the skewed distributions in the output response for this reason they represent a more general and robust estimation 5 discussion 5 1 sample size n and the number of intervals m the results have shown that the analysis conducted based on the cvd strategy provides the same or even more detailed information as the state of the art of variance based approach saltelli et al 2010 with also the advantage of using a generic sampling design and converging with a lower sample size still the number of samples n and the number of intervals m used to calculate the conditional distributions are two free parameters of the strategy and they should be selected with caution the sensitivity of the results to these free parameters has been explored for instance for the case of distribution based approaches it has been shown that in the case of pawn results are quite independent of the chosen value of m when n is relatively high and m 5 mora et al 2019 pianosi and wagener 2018 similarly plischke et al 2013 underlined that increasing the number of m beyond 50 classes has negligible effect on the estimation accuracy more recently however a more detailed analysis conducted on these free parameters have shown the risk of achieving perfunctory results puy et al 2020 for this reason assuming the number of simulations n being the maximum achievable based on the specific model run time we advise to test the robustness of the indices and of the factor ranking by modifying the number of intervals m as performed in other studies li and mahadevan 2016 puy et al 2020 we tested this approach by repeating the analyses varying the intervals m in the range 5 45 by increments of 5 and we found negligible differences in the indices results not shown this robustness is explained considering that the spline interpolation well represents the input output space independently from the number of interval m in contrast the interaction index is calculated over the combination of all the conditional distributions leading to a relative high number of pairs even when m is relatively low e g when m 10 the combinations c m 2 45 thus the results of the cvd strategy show to be largely independent from the number of intervals m and all the conclusions reported on the role of the different factors of the tests are considered well supported by the analyses 5 2 alternative methods in the cvd strategy the cvd strategy has been applied using the spline interpolation for the estimation of the main effect and the δ measure for the interaction index in principle however other methods can be applied as well in the present study we repeated the analyses using two different methods the main effect is estimated based on the variance of the conditional mean e y x i kucherenko and song 2017 without the interpolation step and the interaction index is estimated based on kolmogorov smirnov test instead of the δ measure differences between the use of these approaches were negligible in most of the cases some differences in the estimation of the main effect have been identified only in the case the model response was highly skewed for example the main effects of the skewed functions eq 21 and 22 did not reach the analytic references but they show some differences also at large sample size for the case of the distribution approach the ranking was consistent in all the cases however it has been noted how δ measure proposed by borgonovo 2007 better discriminate the factors in comparison to the kolmogorov smirnov test thus we suggest applying the cvd strategy based on the spline interpolation and the δ measure still further settings can be also tested for instance the spline interpolation can be optimized based on an iterative step as proposed by ratto and pagano 2010 similar optimization could be also envisioned for the kernel density estimation for this reason while we consider the cvd strategy as a new effective strategy for combining variance and distribution based approaches we leave to further studies comparing different settings or other alternative specific methods that can be integrated in the strategy and can perform better in specific conditions cukier et al 1973 lewandowski et al 2007 mara et al 2017 mara and joseph 2008 mckay et al 1999 oakley and o hagan 2004 plischke 2010 ratto et al 2007 tarantola et al 2006 veiga 2015 5 3 correlated factors in principle it is straightforward to apply the cvd strategy also in the presence of correlated input factors the only difference would be to sample from joint probability distributions before estimating the sensitivity indices however it should be noted that these indices lose their interpretability when factors are correlated saltelli and tarantola 2002 the main effect still is used in the context of identifying the model inputs that when fixed lead to the greatest reduction in output variance however it contains also interactions information carried over by correlation for this reason removing this main contribution by centralizing the conditional distributions eq 16 does not isolate anymore the interaction effects and the distribution based index eq 17 loses its information content thus the main effect can be estimated as suggested by kucherenko and song 2017 however we are not able to estimate the interaction effect with eq 18 for this reason we advise to work with uncorrelated samples whenever possible e g by treating dependencies as explicit relationships with a noise term saltelli et al 2008 we leave possible improvements to future studies and we refer to the following references for a deeper discussion on global sensitivity analysis applied to correlated input factors borgonovo and tarantola 2008 kucherenko et al 2017 mara et al 2015 tarantola and mara 2017 zhao et al 2015 5 4 integrating good practices in the cvd strategy most of the gsa relies on monte carlo simulations for this reason a good practice is to repeat the analysis to assess the robustness of the estimation as conducted within the present study i e 100 replicates when this is not possible most likely due to computational burden to run the model complementary approaches can be integrated in the cvd strategy to assist the interpretation of the results as it has been performed for other methods bootstrapping efron and tibshirani 1994 is a widely applied approach to provide confidence intervals based on resampling the original sample set with replacement this approach has extensively been used in global sensitivity analysis nossent et al 2011 sarrazin et al 2016 this method is however inappropriate with small sample sizes for this reason the use of dummy variables mai and tolson 2019 plischke et al 2013 zadeh et al 2017 or bias controlling statistical test plischke et al 2013 has been introduced to support the assessment of the indices and the ranking of the different factors further improvements can also be performed by iteratively decreasing the input space to be sampled by discarding the factors that are well identified in an iterative screening approach cuntz et al 2015 lo piano et al 2017 working with groups by perturbing all factors of the same group simultaneously is also very advantageous for models containing a high number of factors hundreds or thousands this method allows for the reduction of the number of model executions required at the cost of losing information on the relative strength of the inputs belonging to the same group campolongo et al 2007 the groups are generally defined a priori introducing some subjectivities into the analysis more recently however an automatic selection of the groups has also been presented to overcome this limitation sheikholeslami et al 2019 comparison of all these different auxiliary methods should be performed in future studies to identify their advantages and to better guide their use in specific applications 6 conclusions we developed a new strategy called cvd that combines the strength of variance and distribution based global sensitivity analysis in a meaningful and effective way this new strategy enables to estimate main and interaction effects directly from a generic sampling design random latin hypercube quasi monte carlo etc for these reasons it provides a comprehensive analysis of the model response that can be easily implemented in any modelling framework and assessment baroni and tarantola 2014 uusitalo et al 2015 the new combined strategy has been tested on four analytical functions and on a hydrological model the strategy has been implemented based on a spline interpolation saltelli et al 2008 and the δ measure borgonovo 2007 for the estimation of the main and interaction term respectively however other methods can be easily integrated and tested in future studies kucherenko and song 2017 liu et al 2006 pianosi and wagener 2015 ratto and pagano 2010 the results are compared to the state of the art of variance based approach for global sensitivity analysis saltelli et al 2010 the results showed that the new cvd strategy quantifies main and interaction effects correctly and with a lower sample size the strategy is also better able to capture the interactions term when distributions are not gaussian i e the variance does not well represent the distributions thus the strategy combines the strength of variance and distribution based approaches to explore the input output space and the role of the different factors overall the new strategy provides a new simple and comprehensive basis for performing a global sensitivity analysis that can be useful to improve and to facilitate the use of these diagnostic tools for environmental models and to avoid perfunctory analysis that are still very common in many modelling studies saltelli et al 2019 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank razi sheikholeslami and two anonymous reviewers for their constructive comments on previous version of this manuscript this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors g baroni is indebted to the students of the graduate course uncertainty and sensitivity analysis at environmental science and geography at the university of potsdam germany who provided always good questions and motivations to better study and explain the topic appendix table a1 parameters description and ranges as taken from shin et al 2013 all the parameters follow a uniform distribution table a1 parameter name unit range description uztwm mm 1 150 upper zone tension water maximum capacity uzfwm mm 1 150 upper zone free water maximum capacity uzk 1 day 0 1 0 5 upper zone free water lateral depletion rate pctim 0 000001 0 1 fraction of the impervious area adimp 0 0 4 fraction of the additional impervious area zperc 1 250 maximum percolation rate coefficient rexp 0 5 exponent of the percolation equation lztwm mm 1 500 lower zone tension water maximum capacity lzfsm mm 1 1000 lower zone supplementary free water maximum capacity lzfpm mm 1 1000 lower zone primary free water maximum capacity lzsk 1 day 0 01 0 25 lower zone supplementary free water depletion rate lzpk 1 day 0 0001 0 25 lower zone primary free water depletion rate pfree 0 0 6 direct percolation fraction from upper to lower zone free water storage side 0 0 fixed fraction of base flow that is draining to areas other than the observed channel rserv 0 3 fixed fraction of the lower zone free water that is unavailable for transpiration purposes riva 0 0 fixed fraction of the riparian vegetation area 
25940,we present a new strategy for performing global sensitivity analysis capable to estimate main and interaction effects from a generic sampling design the new strategy is based on a meaningful combination of variance and distribution based approaches the strategy is tested on four analytic functions and on a hydrological model results show that the analysis is consistent with the state of the art saltelli jansen formula but to better quantify the interaction effect between the input factors when the output distribution is skewed moreover the estimation of the sensitivity indices is much more robust requiring a smaller number of simulations runs specific settings and alternative methods that can be integrated in the new strategy are also discussed overall the strategy is considered as a new simple and effective tool for performing global sensitivity analysis that can be easily integrated in any environmental modelling framework keywords global sensitivity analysis variance distribution generic sampling design software availability the analysis has been performed with the statistical software r 3 3 x r core team 2019 and the saltelli jansen analysis using the package sensitivity iooss et al 2019 the simulations with the hydrological model have been conducted using the package hydromad andrews et al 2011 an example script on how to implement the new combined variance and distribution based cvd strategy is freely available under https github com baronig gsa cvd 1 introduction global sensitivity analysis gsa refers to a group of diagnostic modelling tools developed to study how the uncertainty in the output of a mathematical model can be apportioned to the uncertainty in the input factors saltelli et al 2000 in this context the term factor is interpreted in a broad sense of anything that can be subject to some degree of uncertainty in the model e g parameters input boundary conditions or model structure ratto et al 2007 in contrast to the local one at the time approach where one single factor is perturbed while keeping the other fixed gsa approaches are applicable independently of the characteristics of the input output response function they cover the entire input space and they identify non linearity and interactions between the factors saltelli et al 2008 for these reasons they have been recognized as a fundamental analysis to support model understanding and improvements in many applications baroni et al 2018 borgonovo et al 2017 demirel et al 2018 guse et al 2016 haghnegahdar et al 2017 pianosi and wagener 2016 reusser et al 2011 rosolem et al 2012 savage et al 2016 schürz et al 2019 tang et al 2007 xie et al 2017 and they should be preferred to local approaches to avoid a perfunctory analysis saltelli et al 2019 saltelli and annoni 2010 several gsa methods have been developed and we refer to review papers and text books for an overview iooss and lemaître 2015 pianosi et al 2016 razavi and gupta 2015 saltelli et al 2008 song et al 2015 wagener and pianosi 2019 wei et al 2015 here we narrow the discussion to two probabilistic approaches based on monte carlo simulations the variance based approaches and the distribution based also called density based or moment independent approaches sensitivity analysis based on variance measures has been introduced in the early 70s bier 1982 cukier et al 1973 iman 1987 major contributions on the approach however are attributed to the russian mathematician i m sobol who generalized the approach and provided a straightforward monte carlo based implementation sobol 2001 at present the most widely used variance based measures are the so called sobol indices and in particular sobol first order sensitivity measure or main effect together with the total sensitivity indices or total effect introduced by homma and saltelli 1996 the use of these indices enjoyed success among practitioners probably due to a clear interpretation of their meaning saltelli et al 2008 specifically as described in ratto et al 2007 high values in the main effect represent the factors that when constrained at their true value would reduce the uncertainty in the model output the most and therefore make the model inference more robust this type of analysis supports the prioritization in model improvements thus it has been identified in literature with the term factor prioritization on the contrary low values for total effect identify those factors that have an irrelevant contribution to the uncertainty in the output and therefore can be constrained to an arbitrary value within their range of uncertainty e g supporting model simplification this type of analysis has been identified with the term factor fixing it should also be noted that the two analyses defined above can be performed independently i e either estimating the main effect for factor prioritization or the total effect for factor fixing however it has been underlined that the differences between total and main effect reveals interactions among the factors saltelli et al 2008 this additional information is very important when the modelers are interested to know if the specific factor is identifiable by e g calibration ghasemizade et al 2017 guillaume et al 2019 namely a factor that shows high interactions is likely not identifiable so for sake of clarity we term this third type of analysis factor identification that in contrast to factor prioritization and factor fixing can be achieved only by estimating both indices despite its wide use it has been argued that variance based approaches rely too heavily on the assumption that the variance is sufficient to describe the uncertainties and sensitivities encountered park and ahn 1994 the variance measure is ill suited particularly to measure the dispersion of a variable with a heavy tail or a multimodal distribution or which contains some outliers auder and iooss 2009 to overcome this limitation some approaches have been developed based on the idea of comparing the entire probabilistic distributions of the model output rather than the variance the first methods have been derived based on the entropy measures liu et al 2006 park and ahn 1994 borgonovo 2007 developed an important measure based on density functions more recently pianosi and wagener 2015 suggested the use of the kolmogorov smirnov test kolmogorov 1933 generalization of the methods have been also presented for comparing the entire distributions veiga 2015 or targeting some of their statistical moments dell oca et al 2017 these distribution based methods have also been used in several studies borgonovo et al 2011 castaings et al 2012 fox et al 2016 gillies et al 2016 hosseini et al 2018 pianosi and wagener 2016 pilz et al 2017 schürz et al 2019 sedighian et al 2015 and they have showed to converge faster than variance based methods pianosi and wagener 2015 zadeh et al 2017 however these methods focus on estimating only one single effect and separating main total effect and interactions is not targeted for this reason the modelers remain without the identification of important features that can be achieved in global sensitivity analysis saltelli and tarantola 2002 to overcome this limitation and exploit the advantages of both variance and distribution based approaches their combined use has been suggested and tested in literature borgonovo et al 2017 massmann and holzmann 2012 pappenberger et al 2008 most of the studies emphasized the complementarity of the different methods however they also highlighted the difficulties to directly compare the different indices because they are based on different quantities they explore different ranges in the input output space and thus they carry different information borgonovo and tarantola 2008 mora et al 2019 in this study we develop and test an effective strategy to combine variance and distribution based sensitivity analysis we refer to this combined strategy with the term cvd strategy this strategy has been developed to take advantages of the two approaches and to allow a meaningful combined use of the different indices the paper is structured as follows in section 2 we first revise variance and distribution based approaches to better identify similarities and complementarities this provides the basis for the development of the new cvd strategy this new strategy is tested with four analytic functions section 3 and on a hydrological model section 4 in all tests the strategy is compared to the state of the art saltelli jansen formula for estimating both main and total effect the discussion is enriched based on specific settings and possible improvements section 5 conclusions are reported in section 6 2 methods 2 1 variance based approaches we start considering a numerical deterministic model that can be written in the form 1 y g x i where x i are the input factors with i 1 k and k the number of factors g is the generic numerical model and y the output of the model x i can be regarded as any type of input factor parameters model structure input and boundary conditions considering that all types of factors can be associated to a scalar discrete value baroni and tarantola 2014 lilburne and tarantola 2009 plischke et al 2013 we proceed also here assuming that the factors x i are a scalar input e g parameters we now consider that the values of x i are not known and their uncertainty can be described by their probability distributions p x i e g fully defined by mean and standard deviation in case of gaussian distributions we underline that this first task distribution assignment is a crucial step in ensuring the quality and consistency of the results baroni et al 2017 haghnegahdar and razavi 2017 plischke et al 2013 shin et al 2013 then the distributions are sampled n times creating a n x k matrix x the model can be now run in a monte carlo approach as follows 2 y g x the probabilistic distribution of the vector y can be now defined as p y and can be seen as the uncertainty in the model output by propagating the uncertainty in k input factors variance based approaches implicitly assume that the variance the second moment here identified with the term v is sufficient to describe the probabilistic distribution p y thus a generic uncertainty importance measure of the factor i in explaining the uncertainty in the model output can be introduced as the reduced variance one would achieve by fixing one source of uncertainty as follow expressed in relative terms 3 v y x i e x i v y where e indicates the mean operator and v y x i e x i indicates the conditional variance in the model output y when the factor i is fixed to its mean e x i dividing this quantity over the unconditional variance v y leads to an index that quantifies the fraction of remaining variance in the model output when the correct value of factor i is known besides possibly having the problem of an index bigger than one conditional variance unconditional variance one should recognize that e x i could be not necessarily the true i e optimal value thus this measure would not be a good estimation of the importance of the factor iman 1987 for this reason it has been suggested to calculate the conditional variance v y x i based on x i fixed to r values in the range p x i and to calculate their mean as follow 4 e v y x i v y in the simplest case the number of conditional values r could be limited to the two extreme values within the range p x i borgonovo 2010 francke et al 2018 iman 1987 most preferable however the values should cover the entire range in the distributions now when this importance measure is small x i is an important factor considering that the model output variance can be decomposed as the sum of the variance of the conditional expectations e v y x i and the residual term v e y x i mood et al 1974 5 v y e v y x i v e y x i the importance measure can be formulated in a complementary way as 6 s i v e y x i v y now when this importance measure is high x i is an important factor this expression is usually referred to as sobol first order sensitivity measure or main effect and it is indicated in literature with the symbol s i as discussed in the introduction this index represents the mean variance that one would reduce if the factor is known for this reason high values in the main effect identify the most important factors that should be considered to decrease the uncertainty in the model or improve the model performance thus this index supports the so called factor prioritization it has been noted that the factor i could have a direct effect on y or its importance could be related to the effect that the factor has due to interactions with other factors for this reason another index has been proposed as follows 7 t i e v y x i v y where v y x i indicates the variance of y fixing all factors but not i this terms is usually referred as total sensitivity index or total effect as introduced by homma and saltelli 1996 please note that the index is generally denoted with the symbol s i t but for simplicity we adopt the notation t i as suggested by glen and isaacs 2012 as discussed in the introduction this index represents the main effect of the factor and its contribution based on interactions with other factors low values of t i identify factors that are not important and can be fixed thus this index can serve for factor fixing finally we explicitly derive the interaction term as a simple difference between the two indices 8 i i t i s i this index is used to quantify the effect of the factor i due to interactions with the other factors and it is used in combination of the main effect to understand a factors identifiability i e if i i is small and s i is high the factor is likely identifiable 2 2 estimation of first and total sensitivity index in principle the computational cost to estimate s i assuming the same number of samples n for approximating the unconditional and conditional distributions is 9 n k 1 r n where n is the required number of model evaluations k is the number of factors i r is the number of conditional values and n the sample size to approximate the distributions saltelli et al 2008 this tailored sampling design is usually referred to as brute force double loop sampling in the first loop for the current factor i x i is set to r prescribed values while in the second loop n realizations of the remaining inputs x i are generated when additionally t i is estimated the cost of the tailored sampling design increases further obviously such an estimation is impracticable when the computational cost of running the model is high e g more than minutes for one single run for this reason several methods have been developed to reduce the computational burden specifically there are several computationally affordable methods proposed in literature to compute the first order sensitivity indices cukier et al 1973 lewandowski et al 2007 mara et al 2017 mara and joseph 2008 mckay et al 1999 oakley and o hagan 2004 plischke 2010 ratto et al 2007 strong et al 2012 tarantola et al 2006 in most of these cases a dedicated sampling design is used e g based on fourier transformation or resampling however the estimation can be also directly performed based on a generic sampling design kucherenko et al 2012 wainwright et al 2014 plischke 2010 li and mahadevan 2016 strong and oakley 2013 in the simplest case kucherenko et al 2012 noted that there is no the need of brute force double loop sampling eq 9 but the analysis can be directly performed based on a filtering process over a generic sampling design of the cost n the strategy is illustrated in fig 1 upper row where the input output space of a three parameter function is shown based on scatterplots based on this strategy the ranges of x i are partitioned into m conditioning intervals of equal size in each interval the conditional mean i e e y x i is calculated these values are represented in fig 1 upper row by the red dots then the main effect is estimated as the variance of these conditional means variance of the red dots these indices are shown in the bar plot on the right side the indices indicate for this case high importance of the first factor x 1 and no main effect for x 2 and x 3 this method has been found to perform well in estimating s i in comparison to other approaches kucherenko and song 2017 li and mahadevan 2016 similar simple approaches have been not found however for estimating the total effect in contrast the estimation of this index relies on specific sampling designs fourier or resampling and it is much more expensive to compute glen and isaacs 2012 saltelli et al 2010 1999 currently one of the most applied and effective method is the one discussed by saltelli et al 2010 it has shown to perform well in several applications with the advantage of being extendable to the analysis of any sources of uncertainty baroni and tarantola 2014 lilburne and tarantola 2009 savage et al 2016 we briefly introduce this approach also within the present study as reference first two independent sets of input sample matrices a and b each of which is an n k matrix containing n sets of k dimensional parameter vectors from monte carlo sampling are generated sobol quasi random sampling sobol 1976 is usually suggested for this purpose because it showed to increase the rate of convergence of the estimators becker et al 2018 kucherenko et al 2011 tarantola et al 2012 but other techniques can be used as well e g latin hypercube sampling from a and b a matrix c i i 1 2 k is created for each factor such that the i th column of c i is the same as the i th column of a and the other columns of c i are the same as b the number of required simulations is then n n k 2 based on these matrices different estimators of the main effect eq 6 and total effect eq 7 can be applied saltelli et al 2010 for ease of interpretation we report here the estimation first presented by jansen 1999 10 s ˆ i 1 1 2 n i 1 k g a g c i 2 v g a 11 t ˆ i 1 2 n i 1 k g b g c i 2 v g a 12 i ˆ i t ˆ i s ˆ i the hat is here used to indicate that these terms are estimation of the indices expressed by eqs 6 8 as discussed by wainwright et al 2014 these estimations offer an intuitive way to understand the meaning of the sobol indices that we report also here for the sake of clarity the equations represent in fact a correlation based measure between two matrices in the first estimation eq 10 the parameter sets in a and c i share the same values only for parameter i perturbing all the other factors except for i includes the total effects involving all the factors except for the first order effect of i if i is very influential its value mainly determines the results so that g a and g c i should be similar and the differences g a g c i small in the second index eq 11 b and c i have the same values except for i when we perturb i with the other parameters fixed the difference g b g c i hence t i accounts not only for the impact of i as a single factor but also for the interaction effects with the other factors if the parameter i is very influential this factor determines the results so that g b and g c i should now be different and the differences g b g c i become high finally the difference i i between total effect and main effect eq 12 quantifies only the contribution of the factor i due to interactions 2 3 distribution based approaches distribution based important measures follow the same derivation as described in section 2 1 for variance based approach they vary only in the statistical operator to compare the conditional and the unconditional probabilistic distributions plischke et al 2013 veiga 2015 specifically some authors liu et al 2006 park and ahn 1994 compared the distributions based on the kullback leibler entropy metric kullback and leibler 1951 in both cases however the analysis has been performed only by fixing x i to its mean value i e as in eq 3 for the case of variance based approach 13 k l i p y x i e x i log p y x i e x i p y d y alternative distance measures have also been proposed chun et al 2000 later borgonovo 2007 developed the so called δ measure based on the absolute difference between the density functions f in addition he extended the previous methods by taking a statistic half of the average e over r conditional values to eliminate the dependency of the conditional point the mean of x i as it has been developed for the variance based approach see eq 4 14 δ i 1 2 e f y x i f y d y more recently pianosi and wagener 2015 introduced the pawn method which uses r conditional values as well but the comparison between conditional and unconditional distributions is performed on the cumulative probabilistic distribution based on the kolmogorov smirnov test ks kolmogorov 1933 15 p a w n i m e d i a n k s p y x i p y where median is often used as statistical operator even if other operators e g mean maximum can also be considered pianosi and wagener 2015 additional measures have also been introduced underlining the strong analogy between them veiga 2015 however a comprehensive comparison of the different operators has not been carried out so far all these distribution based approaches have been proposed based on the same tailored sampling design discussed for estimating the main effect in variance based approach eq 9 however in the same way that it has been tested for estimating the main effect kucherenko et al 2012 later it has also been noted that the estimation of the distribution measures can also be performed based on filtering a generic sampling of the cost n without the need of the double loop brute force sampling design pianosi and wagener 2018 plischke et al 2013 it should be noted however that extending the distribution based approach to quantify both main and total effects as performed for variance based approach i e also comparing conditional distributions when all factors except i are fixed as defined in eq 7 is not practical as the number of simulation greatly increases kucherenko et al 2012 saltelli et al 2010 1999 for this reason the distribution based approach has been used mainly to estimate one index and it has been extended to quantify main and total effects when the number of conditional points r have been limited to one i e the mean of x i liu et al 2006 finally it is interesting to note that despite the difficulties to directly compare variance and distribution based measures it has been underlined that the distribution based index contains global information and should be compared to the total effect index derived in variance based approach auder and iooss 2009 this can be seen by noting that the estimation of the variance based total effect t i in eq 11 i e g b g c i is equivalent to assessing the difference in y when perturbing x i with the other parameters fixed this is like what happens also for the distribution based comparison see eq 13 15 2 4 an effective combined strategy the basis for an effective strategy for combining variance and distribution based global sensitivity analysis starts by recognizing two key aspects of the approaches first both main effect kucherenko et al 2012 kucherenko and song 2017 li and mahadevan 2016 wainwright et al 2014 and distribution indices pianosi and wagener 2018 plischke et al 2013 can be estimated based on a generic sampling design i e n samples are filtered into m intervals to approximate the conditional distributions this reduces the computational cost in comparison to the double loop sampling design in addition it facilitates the analysis because it can be used after any generic monte carlo simulations that has been previously conducted for e g uncertainty propagation assessment second the two indices calculated based on variance and distribution based approaches carry different meanings in the variance based approach the index refers to the main effect while in the distribution based approach it carries global information associated also to interactions for these reasons the combined use has the potential to address based on a generic sampling design all the three purposes of a global sensitivity analysis factor prioritization based on main effect factor fixing based on total effect and factor identifiability based on interaction effect at this stage however the combined use of main effect and distribution based index is not meaningful because the actual values are not comparable to overcome this limitation we proceed as follow first we estimate the main effect s i eq 6 based on a generic sampling design specifically the estimation can be simply performed based on the filtering approach previously described kucherenko et al 2012 kucherenko and song 2017 i e based on the variance of the conditional mean e y x i estimated in m intervals fig 1 upper row red dots the estimation however can be boosted by interpolating the input output model response green dashed lines in fig 1 and calculating the variance over the interpolated values as suggested in other studies saltelli et al 2008 wainwright et al 2014 in the following we applied this approach using a smooth spline for interpolating the one dimensional input output space we refer to this estimation with the term s i to distinguish it from the term s ˆ i that is used to denote the index estimated based on saltelli jansen approach eq 10 a comparison between calculating the variance over the interpolated values green dashed line versus using the conditional means red dots is presented in the discussion section we now remove the main effect from the conditional distributions by centralizing each conditional distribution i e removing the mean 16 p y x i p y x i e y x i these m centralized conditional distributions are shown in fig 1 lower row differences to the upper row result only for factor x 1 for which now the mean of the conditional distributions red dots are equal to zero too by removing the main effect we now define an index based on these centered conditional distributions to have an estimation of the interaction effects alone fig 1 lower right corner namely we can compare the conditional and unconditional distribution as performed by the distribution based approaches see eqs 13 15 however we rather define the interaction index by comparing only all the combinations c m 2 of the centered conditional distributions p to better isolate the effect to the output of changing x i as follow 17 i i s t a t o p y x i r j p y x i r j 1 where r 1 r m represent the m intervals stat is a statistical operator e g mean median and o is the distribution based measure we use here the term i i to distinguish from the term i ˆ i calculated as the difference between total and main effect estimated using variance based approach eq 12 in the following we refer to this general combined variance and distribution based strategy with the term cvd strategy in principle any operator o described above e g entropy based δ measure or kolmogorov smirnov test can be used within the cvd strategy although it is shown that these measures have some analogies veiga 2015 however a comprehensive inter comparison in their use within the distribution based approaches has not been performed so far in the following we proceed with the use of the δ measure of borgonovo 2007 eq 14 specifically we fit the centered conditional distributions p via a kernel density function parzen 1962 and we quantify the difference between all the combination c m 2 of the centered conditional density functions f as follow 18 i i 1 2 c j 1 m 1 p 2 m f y x i r j f y x i r p p j a comparison between this operator and the alternative kolmogorov smirnov test used in the pawn method is presented in the discussion section 3 test functions 3 1 functions and settings we test the cvd strategy described in section 2 4 for estimating main effect s i and interaction effect i i on four analytic functions often used as a benchmark for sensitivity analyses studies borgonovo 2007 cuntz et al 2015 kucherenko et al 2009 kucherenko and song 2017 mai and tolson 2019 pianosi and wagener 2015 plischke et al 2013 saltelli et al 2008 these functions represent model of increasing complexity number of factor k between 3 and 15 and with different output distributions increasing skewness for comparison we also apply the method of saltelli jansen for estimating main effect s ˆ i and interaction effect i ˆ i based on eqs 10 and 11 and the distribution based measure pawn i eq 15 thus for each function we calculate and compare five indices the first function is the ishigami homma function 19 y sin x 1 a 1 sin x 2 2 a 2 x 3 4 sin x 1 where all x i follow a uniform distribution over π π different values of the parameters a1 and a2 are encountered in literatures in the following we use a 1 2 and a 2 1 as used by pianosi and wagener 2015 to allow for a direct comparison the function exhibits strong non linearity and non monotonicity with interactions between the two terms x 1 and x 3 the input output response is also shown in the scatter plots of fig 1 the second function has been introduced by oakley and o hagan 2004 20 y a 1 t x a 2 t sin x a 3 t cos x x t m x where x is a vector of k 15 input factors sampled over a standard normal distribution i e mean 0 and standard deviation 1 and a j j 1 2 3 and m are three k vectors and a matrix k k of parameters respectively the weights a 1 a 2 and a 3 are chosen so that one group of five input factors x i 10 15 accounts for most of the output variance while the remaining factors have smaller effect in contrast interaction contribution is almost equally distributed to all the factors the values of these weights and of the matrix m can be downloaded from www sheffield ac uk st1jeo and are also reported in saltelli et al 2008 the third function is the so called g function 21 y i 1 k 4 x i 2 a i 1 a i where the terms x i are k independent variables uniformly distributed in the unit hypercube 0 1 and the terms a i are positive integers that condition the importance of the factor x i the smaller a i the higher main and total effects of x i on the function are mara 2009 this function is a more complicated nonlinear and non additive function that produces a right skewed output distribution it is characterized by the presence of interactions among the model inputs generated by their multiplication we use a six dimensional version of the function k 6 with coefficients a i equal to 0 0 5 3 9 99 99 as used in other studies glen and isaacs 2012 saltelli et al 2010 resulting in decreasing sensitivity of the output to the six factors finally we consider the function introduced by bratley et al 1992 22 y i 1 k 1 i j 1 i x j where k 10 and all x i follow a uniform distribution over 0 1 like the g function the input factors also have a wide range of main and interaction effect decreasing with j but the function produces a left skewed model output distribution for each function the five sensitivity indices are estimated based on sobol quasi random sampling sobol 1976 saltelli jansen estimations are performed based on n k 2 2 15 simulation runs the pawn and cvd indices are estimated based on n 217 simulation runs all these indices are calculated by increasing sample size n to assess the convergence of the estimation moreover the analyses are repeated 100 times to assess the robustness of the estimation and the approach of owen 1995 is used to introduce randomness to the deterministic sobol sequence the accuracy of the estimation is quantified based on the absolute mean error mae as follow 23 m a e 1 k i 1 k e i r i where k is the respective number of input factors i and e and r represent the estimated and reference sensitivity index respectively for the main effect s ˆ i and s i the analytic values are used as references for the interaction terms i ˆ i and i i however the comparison is not straightforward as the values represent different quantities and the analytic values of the distribution based indices are not available for this reason the assessment of the interaction terms is performed using as references in eq 23 the average indices over the 100 repetitions obtained with the maximum sample size n 4 2 results of the test functions fig 2 shows the five indices obtained for the four test functions the average indices over the 100 repetitions achieved with the maximum number of simulation n are plotted the indices are combined as conducted for other approaches morris 1991 to visualize their relation and their different information content please also note that error bars one standard deviation based on the 100 repetitions are also plotted but they are not always visible indicating the robustness of the estimation achieved with the maximum sample size n the analysis conducted based on saltelli jansen method on the ishigami homma function fig 2 first column second row identifies x 1 as an important factor based on the main effect s ˆ i see also scatter plots of fig 1 the same interaction effects i ˆ i are quantified for the factors x 1 and x 3 the results suggest that x 2 is not relevant and can be fixed to any value e g mean x 1 and x 3 have interactions and are not identifiable the pawn i index fig 2 first column third row ranks the factors consistently x 1 x 3 x 2 however based on this index alone it is not possible to identify whether the factors have interactions or not the comparison between pawn i index and the main effect s ˆ i suggests that the importance of the parameters x 2 and x 3 result from interactions but it is not possible to conclude if these interaction effects are similar or not indicating the difficulties to compare variance and distribution based indices as underlined in other studies borgonovo and tarantola 2008 mora et al 2019 the main effect s i estimated based on the spline interpolation implemented in the cvd strategy fig 2 first column last row is consistent with saltelli jansen method in addition the new interaction term i i eq 18 correctly identifies the same interaction effects on x 1 and x 3 thus the results show how the cvd strategy correctly removes the main effect and the distribution measure is now able to discriminate specifically the contribution due to the interaction effect the same conclusions are derived by looking at the results of the oakley and o hagan function fig 2 column 2 the saltelli jansen method correctly identifies the most important five input factors based on the main effect x i 10 15 moreover it quantifies interaction as evenly distributed over all the factors in contrast the pawn i indices show strong correlation with the main effect but we are not able to conclude if this contribution is due to direct effect of the input factors to the model output or based on interactions in contrast the cvd strategy estimates well the main effect and it quantifies the interaction terms as detected by satelli jansen method some additional considerations arise by looking at the results of the other two functions for which model outputs show strong skewed distributions fig 2 column 3 and 4 all the three approaches rank the factors consistently however pawn i indices and the interactions terms i i of the cvd strategy show higher values for the input factors x 1 in contrast the interaction terms i ˆ i estimated based on saltelli jansen method rank x 1 and x 2 equally while it cannot be concluded by looking at pawn i indices what is the reason of these differences by removing the main effect with the cvd strategy eq 16 the results suggest that the difference is due to stronger interactions of the factor x 1 with the other factors that cannot be detected using the variance as summery statistic we further explore this hypothesis by looking at the input output space of the first two input factors of the bratley function fig 3 the variability of the conditional means calculated in the m intervals red dots in the upper row is larger for the factor x 1 than for x 2 confirming also by visual inspection that the first factor is more important in explaining the model output variability higher main effect we now remove these main effects by centralizing each conditional distribution eq 16 to isolate the interaction effect fig 3 lower row when removing these conditional means the obtained centered conditional distributions of each input factor are very different in the case of x 1 the distribution in the first left interval m is very narrow and then the spread strongly increases in contrast the differences between the centered conditional distributions of the factor x 2 are smaller despite these differences when looking at the two factors it is interesting to note that the variances calculated in each centered distributions are comparable see green square in fig 3 lower row thus the results show how the variance is not an adequate summary statistic in this specific case for this reason the variance based sensitivity analysis fails to reveal the difference between the factors and assigns the same interaction terms to both in contrast these differences can only be distinguished by looking at their entire distributions and for this reason the distribution based measures are better suited to quantify these interaction terms same conclusions are supported by looking at the input output space of the g function data not shown 4 3 convergence and robustness of the estimations the mean absolute error mae eq 23 is shown in fig 4 for increasing sample size n to assess the convergence of the estimation of the indices in addition we visualize the width of the 95 confidence intervals of the performance metric distribution obtained with the 100 repetitions to assess the robustness of the estimation variability over the repetitions please also note that the axes are plotted in log10 base to better illustrate the behavior for small n results show that the numerical estimates converge towards the reference values and the dispersion is constantly reduced as the sample size increases the results confirm the accuracy of saltelli jansen approach in estimating references indices but the need of a relatively high number of simulations n 104 in contrast indices estimated based on the cvd strategy are more robust lower spread over the replicates and converge faster than jansen saltelli method n 103 specifically the spline interpolation method used to estimate the main effect integrated in the cvd strategy outperforms jansen saltelli method in all the test functions therefore the results confirm the availability of efficient alternative methods when we are interested to quantify only the main effect kucherenko and song 2017 plischke et al 2013 strong and oakley 2013 wainwright et al 2014 in contrast the estimation of the interaction terms requires a relative larger sample size also with the cvd strategy still the cvd strategy shows to converge faster to the reference index and to be much more robust smaller spread over the replicates than the saltelli jansen method the only relevant exception is detected for the interaction terms of the ishigami homma function fig 4 second column first row for which the robustness is still much stronger than saltelli jansen method smaller colored envelopes but the convergence to the reference values requires a larger sample size n this behavior is explored in detail by looking at the estimation of the indices of each input factor fig 5 the results confirm that the interaction terms estimated based on saltelli jansen method are well estimated upper plot but the estimation is not robust at a relative low sample size n 104 when looking at the cvd strategy bottom plot the factors are well ranked at relative low samples size too noteworthy however the convergence of the estimation of the interaction term for the factor x 2 is much slower than for the other two factors this low performance can be explained considering that x 2 has a negligible interaction contribution i 0 when a relative low sample size n is used however the distribution based measure eq 18 is affected by the numerical approximation of the conditional distributions and a higher value is quantified some possible improvements to address the estimation of very low values of the interaction indices are discussed in section 5 4 a practical workflow based on a hydrological model 4 1 sac sma model and setting analysis we performed the global sensitivity analysis based on the cvd strategy and on the variance based approach of saltelli jansen to the sacramento soil moisture accounting model sac sma this is an intermediate complexity conceptual rainfall runoff model that represents the soil column by an upper and lower zone of multiple storages burnash 1995 it has been used extensively in both research and operational applications the model has also been used in the context of gsa and parameter identifiability blasone et al 2008 shin et al 2013 van werkhoven et al 2009 2008 the input parametrizations and observations of shin et al 2013 is used within the present study to allow a comparison the 13 parameters and ranges are listed in table a1 the model runs for 10 years the first year was not included in the sensitivity calculations to allow the model states to warm up and remove any impact of uncertain initial conditions the ranges of the parameters are sampled based on sobol sampling design with n 13 2 2 13 122880 model evaluations for the saltelli jansen estimation and n 2 17 131072 for the cvd strategy these sample sizes have also been used in other studies shin et al 2013 van werkhoven et al 2009 simulations and analyses are repeated 100 times and the approach of owen 1995 is used to introduce randomness to the deterministic sobol sequence the modelling results are evaluated based on the following performance metrics for which the sensitivities are assessed 24 n s e 1 t 1 t o t y t 2 t 1 t o t e o 2 25 n s e 1 t 1 t o t y t 2 t 1 t o t e o 2 1 t 1 t o t y t 2 t 1 t o t e o 2 where t is the number of time steps t y and o are simulated and measured river discharge respectively and e o indicates the mean of observation over the time series eq 25 is the nash sutcliffe index and has been commonly used for river discharge assessment its value is within in the range 1 the modified version eq 25 has been introduced by mathevet et al 2006 and it is bounded between 1 1 thus it reduces the influence of large negative values without otherwise changing the interpretation of the objective function shin et al 2013 the two metrices have been selected as example to show the differences in the sensitivity results when model output present different distributions 5 2 results of the hydrological model the results of the sensitivity analysis are shown in fig 6 the indices are plotted to visualize the relation between main and interaction effect of each parameter as conducted for other approaches morris 1991 each plot is also arbitrary divided dashed gray line to better highlight low values in the indices for the case of the modified nse the saltelli jansen approach estimates low main effect and low interaction for most of the parameters fig 3a gray dots thus the values of these parameters can be fixed to any arbitrary values within their ranges and they can be omitted during further analysis in contrast the main effect of the parameters uztwm and pctim related to the soil water capacity and the land surface characteristic respectively see appendix is relatively large 0 26 for this reason these parameters can be targeted for further model improvements by e g calibration the estimated interaction term however is low for pctim while is large for uztwm thus the analysis suggests that pctim is likely identifiable while the optimum value of uztwm can have strong dependencies with the values of the other parameters for this reason this parameter could be not identifiable these conclusions are also supported by the corresponding indices estimated based on the cvd strategy fig 6a gray dots the main difference is only detected for the parameter pctim for which a large interaction term is quantified even larger than the interaction index estimated for the parameter uztwm this difference to the results of the saltelli jansen method can be explained by looking at the input output space for these parameters fig 7 the variability of the conditional means red dots of the two parameters is similar in terms of range and smoothness fig 7a and b however when removing these conditional means to isolate the interaction effect the obtained centered conditional distributions are very different in the case of uztwm fig 7c the distributions are very similar and the main difference is in the variance see green squares in fig 7c for this reason the variance based approach can capture the interaction term in contrast the centered conditional distributions for the parameter pctim are very similar when looking at the variances green squares in fig 7d and they can only be distinguished by looking at their entire distributions for this reason the distribution measure can capture the interaction term for this parameter while the variance based approach does not quantify any interaction these results are particularly interesting in the light of the parameter identifiability as an example the best 1 simulations are filtered and represented in the scatterplots orange color fig 4a and b in the case of the parameter uztwm the best 1 simulations narrow the prior distribution to the lower value of its range in contrast the same best 1 simulation are evenly distributed in the range of the parameter pctim thus the results show how pctim is less identifiable than uztwm for this reason these results underline how the cvd strategy can better capture the interaction effects between the parameters and it is more consistent with the identifiability analysis when the differences in the distributions are not well represented by their variance the same results are obtained when looking at the interaction terms obtained with the standard nse fig 6 lower row in contrast however a lower main effect is estimated for the parameters uztwm and pctim in addition the parameter pctim becomes slightly more influential than uztwm these differences can be explained considering that the standard nse yields strongly skewed distribution with very low performances obtained on a few samples in this case it is hard to compute meaningful statistics to summarize the whole distribution i e mean and variance can be biased by few outliers thus the result confirms the sensitivity of the variance metric to measure the dispersion of a variable with a heavy tail or which contains some outliers auder and iooss 2009 and the need to properly define the performance metrics for the model output it is noteworthy that the confidence intervals calculated based on 100 replicates are very large in the case of saltelli jansen estimations thus these results show the difficulties to apply this method when the output distributions are strongly skewed for this reason we recommend checking for the normality of the output distribution to understand the reliability in the use of this method in contrast the indices estimated based on the cvd strategy are very well estimated even in the face of the skewed distributions in the output response for this reason they represent a more general and robust estimation 5 discussion 5 1 sample size n and the number of intervals m the results have shown that the analysis conducted based on the cvd strategy provides the same or even more detailed information as the state of the art of variance based approach saltelli et al 2010 with also the advantage of using a generic sampling design and converging with a lower sample size still the number of samples n and the number of intervals m used to calculate the conditional distributions are two free parameters of the strategy and they should be selected with caution the sensitivity of the results to these free parameters has been explored for instance for the case of distribution based approaches it has been shown that in the case of pawn results are quite independent of the chosen value of m when n is relatively high and m 5 mora et al 2019 pianosi and wagener 2018 similarly plischke et al 2013 underlined that increasing the number of m beyond 50 classes has negligible effect on the estimation accuracy more recently however a more detailed analysis conducted on these free parameters have shown the risk of achieving perfunctory results puy et al 2020 for this reason assuming the number of simulations n being the maximum achievable based on the specific model run time we advise to test the robustness of the indices and of the factor ranking by modifying the number of intervals m as performed in other studies li and mahadevan 2016 puy et al 2020 we tested this approach by repeating the analyses varying the intervals m in the range 5 45 by increments of 5 and we found negligible differences in the indices results not shown this robustness is explained considering that the spline interpolation well represents the input output space independently from the number of interval m in contrast the interaction index is calculated over the combination of all the conditional distributions leading to a relative high number of pairs even when m is relatively low e g when m 10 the combinations c m 2 45 thus the results of the cvd strategy show to be largely independent from the number of intervals m and all the conclusions reported on the role of the different factors of the tests are considered well supported by the analyses 5 2 alternative methods in the cvd strategy the cvd strategy has been applied using the spline interpolation for the estimation of the main effect and the δ measure for the interaction index in principle however other methods can be applied as well in the present study we repeated the analyses using two different methods the main effect is estimated based on the variance of the conditional mean e y x i kucherenko and song 2017 without the interpolation step and the interaction index is estimated based on kolmogorov smirnov test instead of the δ measure differences between the use of these approaches were negligible in most of the cases some differences in the estimation of the main effect have been identified only in the case the model response was highly skewed for example the main effects of the skewed functions eq 21 and 22 did not reach the analytic references but they show some differences also at large sample size for the case of the distribution approach the ranking was consistent in all the cases however it has been noted how δ measure proposed by borgonovo 2007 better discriminate the factors in comparison to the kolmogorov smirnov test thus we suggest applying the cvd strategy based on the spline interpolation and the δ measure still further settings can be also tested for instance the spline interpolation can be optimized based on an iterative step as proposed by ratto and pagano 2010 similar optimization could be also envisioned for the kernel density estimation for this reason while we consider the cvd strategy as a new effective strategy for combining variance and distribution based approaches we leave to further studies comparing different settings or other alternative specific methods that can be integrated in the strategy and can perform better in specific conditions cukier et al 1973 lewandowski et al 2007 mara et al 2017 mara and joseph 2008 mckay et al 1999 oakley and o hagan 2004 plischke 2010 ratto et al 2007 tarantola et al 2006 veiga 2015 5 3 correlated factors in principle it is straightforward to apply the cvd strategy also in the presence of correlated input factors the only difference would be to sample from joint probability distributions before estimating the sensitivity indices however it should be noted that these indices lose their interpretability when factors are correlated saltelli and tarantola 2002 the main effect still is used in the context of identifying the model inputs that when fixed lead to the greatest reduction in output variance however it contains also interactions information carried over by correlation for this reason removing this main contribution by centralizing the conditional distributions eq 16 does not isolate anymore the interaction effects and the distribution based index eq 17 loses its information content thus the main effect can be estimated as suggested by kucherenko and song 2017 however we are not able to estimate the interaction effect with eq 18 for this reason we advise to work with uncorrelated samples whenever possible e g by treating dependencies as explicit relationships with a noise term saltelli et al 2008 we leave possible improvements to future studies and we refer to the following references for a deeper discussion on global sensitivity analysis applied to correlated input factors borgonovo and tarantola 2008 kucherenko et al 2017 mara et al 2015 tarantola and mara 2017 zhao et al 2015 5 4 integrating good practices in the cvd strategy most of the gsa relies on monte carlo simulations for this reason a good practice is to repeat the analysis to assess the robustness of the estimation as conducted within the present study i e 100 replicates when this is not possible most likely due to computational burden to run the model complementary approaches can be integrated in the cvd strategy to assist the interpretation of the results as it has been performed for other methods bootstrapping efron and tibshirani 1994 is a widely applied approach to provide confidence intervals based on resampling the original sample set with replacement this approach has extensively been used in global sensitivity analysis nossent et al 2011 sarrazin et al 2016 this method is however inappropriate with small sample sizes for this reason the use of dummy variables mai and tolson 2019 plischke et al 2013 zadeh et al 2017 or bias controlling statistical test plischke et al 2013 has been introduced to support the assessment of the indices and the ranking of the different factors further improvements can also be performed by iteratively decreasing the input space to be sampled by discarding the factors that are well identified in an iterative screening approach cuntz et al 2015 lo piano et al 2017 working with groups by perturbing all factors of the same group simultaneously is also very advantageous for models containing a high number of factors hundreds or thousands this method allows for the reduction of the number of model executions required at the cost of losing information on the relative strength of the inputs belonging to the same group campolongo et al 2007 the groups are generally defined a priori introducing some subjectivities into the analysis more recently however an automatic selection of the groups has also been presented to overcome this limitation sheikholeslami et al 2019 comparison of all these different auxiliary methods should be performed in future studies to identify their advantages and to better guide their use in specific applications 6 conclusions we developed a new strategy called cvd that combines the strength of variance and distribution based global sensitivity analysis in a meaningful and effective way this new strategy enables to estimate main and interaction effects directly from a generic sampling design random latin hypercube quasi monte carlo etc for these reasons it provides a comprehensive analysis of the model response that can be easily implemented in any modelling framework and assessment baroni and tarantola 2014 uusitalo et al 2015 the new combined strategy has been tested on four analytical functions and on a hydrological model the strategy has been implemented based on a spline interpolation saltelli et al 2008 and the δ measure borgonovo 2007 for the estimation of the main and interaction term respectively however other methods can be easily integrated and tested in future studies kucherenko and song 2017 liu et al 2006 pianosi and wagener 2015 ratto and pagano 2010 the results are compared to the state of the art of variance based approach for global sensitivity analysis saltelli et al 2010 the results showed that the new cvd strategy quantifies main and interaction effects correctly and with a lower sample size the strategy is also better able to capture the interactions term when distributions are not gaussian i e the variance does not well represent the distributions thus the strategy combines the strength of variance and distribution based approaches to explore the input output space and the role of the different factors overall the new strategy provides a new simple and comprehensive basis for performing a global sensitivity analysis that can be useful to improve and to facilitate the use of these diagnostic tools for environmental models and to avoid perfunctory analysis that are still very common in many modelling studies saltelli et al 2019 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we thank razi sheikholeslami and two anonymous reviewers for their constructive comments on previous version of this manuscript this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors g baroni is indebted to the students of the graduate course uncertainty and sensitivity analysis at environmental science and geography at the university of potsdam germany who provided always good questions and motivations to better study and explain the topic appendix table a1 parameters description and ranges as taken from shin et al 2013 all the parameters follow a uniform distribution table a1 parameter name unit range description uztwm mm 1 150 upper zone tension water maximum capacity uzfwm mm 1 150 upper zone free water maximum capacity uzk 1 day 0 1 0 5 upper zone free water lateral depletion rate pctim 0 000001 0 1 fraction of the impervious area adimp 0 0 4 fraction of the additional impervious area zperc 1 250 maximum percolation rate coefficient rexp 0 5 exponent of the percolation equation lztwm mm 1 500 lower zone tension water maximum capacity lzfsm mm 1 1000 lower zone supplementary free water maximum capacity lzfpm mm 1 1000 lower zone primary free water maximum capacity lzsk 1 day 0 01 0 25 lower zone supplementary free water depletion rate lzpk 1 day 0 0001 0 25 lower zone primary free water depletion rate pfree 0 0 6 direct percolation fraction from upper to lower zone free water storage side 0 0 fixed fraction of base flow that is draining to areas other than the observed channel rserv 0 3 fixed fraction of the lower zone free water that is unavailable for transpiration purposes riva 0 0 fixed fraction of the riparian vegetation area 
25941,geospatial referenced environmental data are extensively used in environmental assessment prediction and management data are commonly obtained by nonrandom surveys or monitoring networks whereas spatial sampling and inference affect the accuracy of subsequent applications design based and model based procedures db and mb for short both allow one to address the gap between statistical inference and spatial data creating independence by sampling implies that db may neglect spatial autocorrelation sac if the sampling interval is beyond the sac range in mb however a particular sampling design can be irrelevant for inferential results empirical studies further showed that mse mean squared error values for both db and mb are affected by sac and spatial stratified heterogeneity ssh we propose a novel framework for integrating sac and ssh into db and mb we do so by distinguishing the spatial population from the spatial sample we show that spatial independence in a spatial population results in independence in a spatial sample whereas sac in a spatial population is reflected in a spatial sample if sampling distances are within the range of dependence otherwise sac is absent in the spatial sample similarly ssh in a population may or may not be inherited in data and this depends on the sampling method thus the population sample and inference constitute a so called spatial statistic trinity sst providing a new framework for spatial statistics including sampling and inference this paper shows that it greatly simplifies the choice of method in spatial sampling and inferences two empirical examples and various citations illustrate the theory keywords population and sample spatial autocorrelation sac spatial stratified heterogeneity ssh variable and random variable spatial statistic trinity sst 1 introduction spatial sampling and inference are widely used in the survey and assessment of soil atmosphere and water environments gao et al 2015 mindham and tych 2019 wang and xuan 2020 wang et al 2013b chen et al 2019 inappropriate application of the methods however may introduce bias in modelling and analysis the model based mb and the design based db statistical procedures are two common ways to address this problem db originated from bowley 1906 and it was further shaped in the 1930s e g neyman 1934 whereas mb originated from kiaer 1896 and it was shaped by fisher 1922 in section 2 and later by krige 1951 and matheron 1963 1971 the two statistical frameworks constitute two mainstream approaches in spatial statistics fig 1 cassel et al 1977 de gruijter and ter braak 1990 brus and guijiter 1997 in db the population is regarded as fixed and stochasticity in the data is introduced by random sampling estimation procedures are based on the known probabilities of sampling spatial variation within a population mainly described as either spatial autocorrelation sac or spatial stratified heterogeneity ssh is of little concern in db de gruijter and ter braak 1990 conversely in mb the population is regarded as a single realization of a joint distribution model and predictions are based on the model chosen to represent the population joint design and model based frameworks dmfs provide a general basis for statistical sampling and inference the distinction between db and mb raises some concerns 1 will the spatial variation sac and ssh of a population affect the mse of estimation when applying db 2 can information contained in the sampling design be used to improve the mse of estimation when applying mb 3 there is some overlap between the two approaches when aiming at the same target de gruijter et al 2006 p 68 to address the first issue we note that in db stratified sampling is favored over simple random sampling if the population exhibits ssh cochran 1978 wang et al 2016 for the second issue it is well known that in mb the empirical error of kriging interpolation i e the difference between the predicted value and the true value at a prediction point varies with the sampling design minasny and mcbratney 2016 for the third issue we note that overlapping makes it difficult for users to make the best choice of approach model assisted sampling and hybrid sampling overcome part of the limitations of the dmf särndal et al 1992 sterba 2009 in hybrid sampling the spatial variation of a population can be included in the joint stochastic distribution model and a sampling design of any random sample can be used in the estimation of parameters accordingly brus and guijiter s 1997 table of sampling strategies can be updated as shown in table 1 the objective of this paper is to explore the root of the inconsistency between theoretical assumptions and empirical studies for both db and mb that is db neglects the spatial variation of a population and mb neglects the sampling design while in empirical studies both of the neglects influence the errors of their estimates from the real value consequently we propose a unified framework named the spatial statistic trinity sst that will greatly simplify the model and method choice in the spatial sampling and spatial inference of environments to address the problems above the remainder of this paper is organized as follows section 2 refines the common standard of performance for various samplings and inferences section 3 proposes the generic sst framework to address the problems arisen in section 1 using the standard of performance set in section 2 then sst is compared to the existing dmf and others in section 4 finally we illustrate the key concepts of sst using empirical studies in section 5 and we draw conclusions in section 6 these sections and sub sections are supported and logically linked by sequential figures as well study background fig 1 motivation of sst with an example figs 2 and 3 connections between the components in sst using a data flow fig 4 computation issues fig 5 model efficiency comparison fig 6 sst model choice for practical applications fig 7 key concepts in sst are empirically illustrated figs 8 10 2 the performance of spatial sampling and statistical inference the gold standard of the performance of an estimator or predictor ψ is its difference from the true value ψ0 1 ψ ψ 0 where ψ is a parameter of a population ℜ or superpopulation r a superpopulation refers to a collection of random variables one for each location and a population is one of the realizations of the superpopulation in practice however ψ0 is unknown and usually only a single realization of a superpopulation or one sample i of a population is available the performance of an estimator ψ ˆ is then measured via the theoretical expectation of the p mse under the assumption of many instances of sampling of the target population by a sampling scheme p equation 2a likewise the performance of a predictor ψ is measured by the theoretical expectation of the ξ mse under the assumption of many instances of realizations of the superpopulation ξ equation 2b or the joint p and ξ mse equation 2c 2a mse p ψ i e p ψ ψ 0 2 2b mse ξ ψ ℜ e ξ ψ ψ 0 2 2c mse ξp ψ e ξp ψ ψ 0 2 where the mse refers to the mean square error and ψ may be either an estimator ψ ˆ or a predictor ψ the mse reduces to a variance in the case of an unbiased estimation 3a e p ψ i r i 1 r p i ψ i r def m ℜ 3b e ξ ψ r r r 1 r p r ψ r r def m r in this study we focus on an unbiased estimation for such a population parameter as the spatial mean p inference fails to account for the sac in a population särndal 1978 de gruijter and ter braak 1990 for a superpopulation parameter such as the model mean ξ inference only concerns the distances between sampling sites in contrast it fails to account for their specific locations note that the theoretical measures in statistics originating from gambling based on random and repeated sampling rrs and widely applied in experimental sciences seem too far from the reality in geoscience where both random and repeated sampling are rare 3 the spatial statistic trinity sst framework 3 1 motivation of sst in mb sampling is done on a population variable whereas spatial data refer to a sample cochran 1977 anselin 1988 cressie 1992 cressie 1990 haining 2003 while in db one assumes a fixed population both the population and sample can be referred to as spatial data to improve estimation for spatial data we have to deal with sac defined as the coincidence of locational and attribute similarities tobler 1970 matheron 1963 anselin 1988 goodchild 2004 sui 2004 although sac is not explicitly included in db inference sac is a property of either the population or the sample with the former being independent of the latter kriging interpolation will be useful if the population has sac particularly if distances between sampled sites are less than the range of spatial dependence instead of the p t spatial statistical dual brus and guijiter 1997 brus 2019 where p represents a sampling design and t represents an estimator we put forward an sst r i ψ composed of a population r sampling design ℑ and estimator ψ each of the three dimensions has many options consequently there are more than 100 combinations of triples fig 2 only a few of these are efficient however where the population properties assumption of estimators and sampling conditions match fig 3 gives a simple example of sst an i i d population random sampling and sample mean in which the triple of the population parameters number of units n and variance σ 2 of the statistic parameters sample mean y and its variance v and of the sampling parameters sample size n are connected by a function having an analytic solution 3 2 sst components and their relationships fig 4 illustrates the spatial data flow in sst from a generator usually a spatial process or superpopulation r to its single realization over a geographical space population ℜ which is observed by a single sampling i we are interested in predicting the values at unsampled sites the spatial mean of the population r or even the parameters of the superpopulation r based on the observed sample ℑ and an estimator or predictor ψ under specific assumptions drawing a sample from a population the values of the sample are the result of data propagation from superpopulation r to population r under sampling scheme i the spatial variation of a superpopulation such as i i d sac or ssh is inherited by the population haining 1988 p 575 eqs 1 1 1 2 cressie 1993 pp 13 15 these properties however may or may not be inherited by the sample due to the sampling method isaaks and srivastava 1989 chapter 7 haining 1988 p 576 and they may or may not be utilized by the chosen estimator because of the propagation and inheritance of the data properties table 2 we coin either spatial i i d or sac or ssh for a population instead of for a superpopulation as linear regression modelling is interpreted under the framework of population and sample without recourse to the word of superpopulation gujarati and porter 2009 p 35 43 when the distinction between population and superpopulation causes little difference for simplification we use population to refer to both population and the mechanism generating the population i e superpopulation consequently an estimation using the sample data ℑ and an estimator ψ are determined by the sampling trinity sst r i ψ sst considers the uncertainties in superpopulation modelling and population sampling as well as the match between the assumption of a model or estimator for inference and the property of a population the elements and linkages of the spatial data flow in fig 4 and table 2 are further explained in the following subsections 3 2 1 population ℜ and estimator ψ a spatial process refers to a superpopulation where its properties are inherited by its populations for example disease risk superpopulation r is to be distinguished from an exhaustive observation population ℜ of disease incidence or prevalence if we are concerned with its causation one commonly assumes that a disease occurrence follows a poisson process haining 2003 p 308 o i poisson e i r i where o i is the observed number of deaths population in the i th subarea and e i denotes the expected number of deaths from the disease then the relative risk of dying from the disease r i with r i o i e i as its maximum likelihood estimator is a superpopulation parameter that can be estimated and mapped for each subarea the risk r i may be further modeled by a prior distribution or covariates banerjee et al 2015 based on different perspectives population and superpopulation may be defined as interchangeable if it concerns individual locations then there are two concepts for estimation which are as follows p disease occurrence locations and p locations disease occurrence these concepts represent the probability of the diseases p y and the probability function of the locations p locations they are related to each other through bayes law that is p y x p x y p y p x commonly the term p x is ignored because it is irrelevant for y and we have that p y x p x y p y the spatial mean of a population ℜ and the model mean of a superpopulation r are defined as 4a m r 1 n i 1 n z i r 4b m r 1 r r 1 r m r respectively where ℜ 1 r and r stands for either a superpopulation or the number of all possible populations of a superpopulation both m ℜ and m r can be estimated by a weighted sample mean 5 m r i 1 n w i z i r where w i is adopted by a sampling design horvitz and thompson 1952 särndal et al 1992 such as 1 n if it is simple random sampling or it is calibrated if the spatial population variation is modeled 3 2 2 population ℜ and sampling ℑ the spatial variation of a population may or may not be reflected in a sample a population ℜ or superpopulation ℝ can be i i d such as a poisson process or white noise deterministic such as a plane or sinuous time series or hybrid that is between completely random and completely deterministic such as an ssh or sac given a specific estimator ψ and sampling method ℑ the mse varies with the properties of the target populations table 3 we consider three cases below case 1 deterministic population dp if a population is fully deterministic such as the path of a bouncing ball it follows newton s law ψ from a low number of sampling points of the path one can reconstruct the exact path population using newton s law as an estimator isaaks and srivastava 1989 p 199 a second example is the detail of urban planning the third example is the population y which is determined by the covariate x such as the first soil map for the united states which is strongly influenced by geology and rivers whitney 1909 soil landscape models allow for the prediction of soil properties based on landscape position branham 1989 pollution emissions by vehicles differ between steady speed and unsteady allowing for the prediction of air pollution along a road the fourth example is the temporal transmission of a communicable disease which can be well reflected by the temporal process of the susceptive exposed infective recovered seir model a few sample units randomly drawn from the seir process can calibrate the seir model to simulate the exact time series of an infectious disease wang et al 2006 sac can be derived from the spatial version of seir and then used for spatial interpolation angulo et al 2013 kolovos et al 2013 case 2 semi random population srp for ssh populations such as climate zones the annual mean temperature of an area can be estimated using a stratified sampling cochran 1977 if a population exhibits sac with a known shape of the variogram which can be calibrated by a sample even if it is i i d and small then kriging can be implemented matheron 1963 rodriguez iturbe and mejia 1974 haining 2003 context effects and uncertainty can then be observed kwan and schwanen 2018 case 3 an i i d population iid against a population containing sac or ssh an i i d population looks like a white noise picture which may be generated by a spatially independently and identically distributed superpopulation an example is a piece of farmland of seedlings that was sowed with problematic corn seeds only 80 can sprout due to variety or spoiling for example it is virtually impossible to reconstruct the i i d population using a finite sample and even advanced estimators there is no better approach than random sampling for surveying an i i d population spatial interpolation is impossible in this case table 3 a population ℜ or superpopulation r could be estimated by a mechanistic model with little data as in case 1 a weak model with much data as in case 3 or a balance between the model and data as in case 2 any choice of estimation method depends on the properties of the population as well as the available sample the properties of a population or superpopulation may be known in advance via the general and specific knowledge of the process christakos 2005 study area relevant determinants or prior exploratory sampling if no prior knowledge of the target population is available simple random or systematic sampling is applicable although this is done at the possible cost of inefficiency 3 2 3 sampling ℑ and estimator ψ sampling is usually followed by applying a method to make an inference of a population or superpopulation by the analytical solution of a sampling design we mean that the variance of an estimator is a function of sample size and vice versa for example the variances of the superpopulation mean and population mean of a simple random sampling are ripley 1981 haining 1988 griffith et al 1994 6a v r 1 r 1 n n s 2 n 6b v r 1 r 1 n n s 2 n respectively where n is the sample size n n is the sampling fraction s 2 is the variance of the population and r is the average of pairwise correlations of the study area computed by the covariance between two randomly selected points for an i i d population r 0 in 6a 6b and the right side becomes the same as the db one after a simple transform the sample size n is a function of the required variance of the sample mean an effective sample size is n n 1 r for an sac superpopulation and n n 1 r for an sac population griffith 2005 for most estimators e g kriging predictors there are no analytic solutions to find the optimal sampling design given their variances thus they must be found by simulations as fig 5 shows for a given population ℜ and sample size how simple random sampling is carried out sample ℑ1 is used by estimator ψ to estimate the spatial mean m ˆ i 1 with variance v i 1 another sampling ℑ2 is carried out to obtain m ˆ i 2 and v i 2 this process is repeated until all combinations c n n are enumerated where n and n are the numbers of units in the population and sample respectively the optimal sampling is the sampling that results into the smallest variance the population mean m ˆ can be estimated by either a db estimator ψ i ℜ of an average over all samplings ℑs in one realized population ℜ or an mb predictor ψ ℜ i of an average over many realized populations ℜs giving one sampling ℑ 3 3 model selection with sst fig 4 shows that the properties of a spatial sample are the result of information propagation from a process that is a superpopulation to a population the population is a realization of the superpopulation and is then captured by the sample and operated in an estimator or predictor sac and ssh are generic features of a spatial process or population they may or may not be reflected by a sample table 4 if the superpopulation is i i d the population will be i i d in most cases whereas autocorrelation in a superpopulation will be preserved in its populations and be present in the observed spatial data if the distance between sampling units is within the sac range the parameters of a population or superpopulation may be obtained from priors like newton s law previous surveys or a survey of a similar attribute in the same area or surveys of the attribute in a similar area in practice sac can be tested by moran s i moran 1950 or semivariogram matheron 1963 ssh can be tested by q statistic wang et al 2016 and a population is i i d if it is neither sac nor ssh many sampling methods and many estimators can be used hence any sampling and estimation can be one of the combinations of the triples fig 2 table 5 and fig 6 illustrate the efficiency of the sst estimation method selections according to sac and ssh are illustrated in fig 7 for an i i d population no approach is better than simple random sampling for inferring about a parameter when a population only presents sac without ssh kriging is preferred for an ssh population stratified sampling or sandwich wang et al 2013a is superior to simple random sampling because it requires fewer sample units to reach a given inference precision when both sac and ssh exist in a population the population is first partitioned according to its ssh and it can then be estimated by kriging in strata stein et al 1988 goovaerts 1997 or more precisely by the mean of the surface with stratified non homogeneity msn wang et al 2009 gao et al 2020 if the sample cannot cover all strata with an ssh population bias correction should be adopted such as the correction method of heckman 1979 for an ssh population and bshade wang et al 2011 and spa wang et al 2012 for a population that presents both sac and ssh in summary db p t and mb ξ brus and guijiter 1997 p 5 can be integrated by the sst r i ψ compared with a complicated dmf based decision tree for choosing between db and mb sampling and statistical strategies brus and guijiter 1997 figure 11 the sst covers the essentials of spatial statistics from sampling choice to statistical inference table 5 4 reinterpretation of key concepts under the sst in sst the mse is determined by the sampling design estimator and population properties this is different from dmf which only admits two of the triples following an influential paper interpreting dmf brus and guijter 1997 we reinterpreted some key concepts under sst table 6 lists the key notations and definitions which are compared with those in authoritative statistics books we have outlined the properties below r1 determinants of the estimates in sst prediction estimation and their variances are determined by sst see fig 2 this contrasts with dmf which considers mb to be unaffected by the sampling design and db to be unaffected by spatial variation however as an mb method kriging has an empirical error that varies according to the sampling scheme random and stratified sampling have developed as db methods and the variances of the sample means are proportional to the variance and ssh of the population r2 ξ unbiasedness and minimum ξ variance in sst fig 2 the mse is the gold standard for assessing the efficiency of spatial sampling and of an estimator this is determined by sst that is the population property sampling plan and estimator regardless of whether a db or mb method is used in dmf ξ unbiasedness and minimal ξ variance are not that useful because in mb there are no restrictions on the selection of the sampling locations r3 random variable in sst the location selection function i x is a random variable which equals 1 if site x is selected and 0 otherwise x i refers to a location i which is fixed once selected thus it is considered a variable but not a random variable this contrasts with dmf where the i th location x i is considered a random variable r4 dependence between observations functional independence is generated by the following theorem if two random variables x and y are independent and g h ℝ ℝ then g x and h y are also independent grimmitt and stirzaker 2001 p 49 theorem 6a in parzen 1960 p 295 theorem 2 in ash 1970 p 84 in sst the sampled sites x 1 and x 2 are two outcomes of an indicator random variable i x therefore application of the function independence theorem for spatial sampling is incorrect and independence between two spatial locations cannot be created by selecting the two locations independently this contrasts with the dmf where it is asserted that if x 1 and x 2 are random locations selected independently from each other then the variables z x 1 and z x 2 are also independent no matter how close the two locations are this definition falls short because two randomly selected locations are not two random variables thus the theorem of function independence is not applicable to this case r5 independence in sst samples are independent if the population is i i d or the sampling interval is beyond the sac range of the population in truth the properties of a population rather than the sample data determine the choice of methods for spatial statistics and spatial sampling sac is not included in the estimation of db cochran 1978 this is no problem if the target population is i i d but it may be less efficient than mb sampling haining 1988 cressie 1993 pp 13 15 if the data are spatially autocorrelated this is in contrast to dmf where the ξ independence of sample data is determined by the modeler and p independence is determined by the sampling design 5 case study according to sst sac in a population affects the efficiency of a sampling and the accuracy of an inference in db sac of a population is neglected or cannot be counted by a quantitative parameter särndal 1978 to demonstrate the advantage of sst to db two case studies using a simulated dataset and a climate dataset respectively were carried out in both cases the same sampling method and estimator were adopted to draw samples and to estimate the means of several populations with different sacs the sampling method and estimator are typical for db and the sample size remains the same the mse is obtained by repeated sampling 7 m s e 1 t t 1 t m ˆ t m 2 where t is the total number of sampling times for each sample size m is the true population mean and m ˆ t is the estimation of m with the sample of time t using a db estimator to make the sampling errors of different populations comparable the normalized mse a v e m s e is used 8 a v e m s e m s e m if a v e m s e varies with the sac of a population sst is a reasonable framework in both cases the finding is consistent with different sample sizes 5 1 simulated dataset five populations with sizes of 50 50 pixels whose values represent the pb concentration in soil ppm were generated using sequential gaussian simulation remy et al 2009 simple kriging with a spherical variogram was applied with the same sill and nugget but at different ranges as shown in table 7 according to the variogram the degree of sac in the five populations is range40 range20 range10 range5 range1 see table 7 and fig 8 each of the five populations was divided into four square strata to be comparable to brus and guijter s 1997 paper samples with different sizes 16 24 32 40 48 56 and 64 were drawn using simple random sampling db for each sample size 500 instances were randomly drawn to compute the a v e m s e via equation 8 the results are shown in fig 9 the a v e m s e values of simulated populations for all sample sizes take exactly the opposite order range40 range20 range10 range5 range1 to that of the sac for all sample sizes this means that the larger the sac of the population the smaller the variance inferenced from samples drawn by simple random sampling given the same sample size the sac in a population is one of the determinants of the variance of db and its impact increases with reductions in sample size 5 2 climate dataset the area aggregated meteorological data of 363 cities from the summer of 2015 june 1st august 31st 92 days obtained from the china meteorological data sharing service system data cma cn were used as populations four meteorological indicators that is the daily mean temperature tem daily mean relative humidity rhu sum of sunshine duration ssd and max wind speed wind have different sacs and represent four populations their sacs r are calculated using the following equation 9 r 1 n 2 i 1 n j 1 n c o v v i v j v a r v i v a r v j where n is the number of cities c o v v i v j is the covariance function of a meteorological factor between city i and city j and v a r v i and v a r v j are variance functions of a meteorological factor of city i and city j for tem the covariance is calculated using the following equation 10 c o v t e m i t e m j t 1 t t e m i t t e m i t e m j t t e m j t 1 where t is the days of temperature data t e m i t represents the daily mean temperature of city i on day t and t e m i is the mean temperature of city i on all t days the variance can be calculated using the following equation 11 v a r t e m i t 1 t t e m i t t e m i 2 t 1 the sac coefficients of different meteorological factors populations are listed in table 8 and tem rhu win ssd for each meteorological indicator the observed values of all the cities on each day are composed of a population the sample sizes are 16 24 32 40 48 56 and 64 five hundred samples were randomly drawn for each sample size the a v e m s e of the meteorological factors of each day were calculated and the results are plotted in fig 10 clearly the order of the a v e m s e of tem rhu win ssd is opposite that of the sacs table 8 for all sample sizes that is the larger the sac of the population the smaller the variance inferred from the samples drawn by simple random sampling the sac in a population is one determinant of the variance of db and its effect increases with reductions in the sample size besides the above two case studies liu et al 2018 tested the ssh and sac of rodent density in a study area and they found that the former is significant and the latter is weak then both the ssh based sandwich estimator wang et al 2013a and sac based kriging were applied to the same sample to map the population respectively the sandwich map has a smaller absolute error than the kriging map as expected by sst 6 discussion and conclusion according to the gold standard of the performance of an estimator or predictor either mb or db alone is incomplete in principle db is applicable but it does not guarantee optimization in all cases because it neglects the mechanism generating the population when a population is not i i d and prior knowledge about the spatial data process superpopulation or population is available there are more efficient strategies for spatial sampling and statistics mb neglects the sampling design so it fails to account for its resulting bias moreover from a technical perspective different configurations of sampling sites will produce different covariance matrices thus generating different estimation results sampling is often incorrectly neglected in spatial statistics of environmental problems statistics originating from gambling usually assumes an rrs which allows the mathematical expectation to be determined this assumption however is often far from reality in the context of spatial data where sampling is usually performed once and non randomly and the accuracy of statistical inference varies with the adopted sampling strategy the accuracy of applying spatial statistics using field data depends on the sampling including its pattern and density brevik et al 2016 sampling efficiency and the precision of inference are determined by the sst which provides a simple and clear sampling and inference decision tree with the following characteristics table 4 1 if the population is deterministic a few sample units plus a deterministic estimator are sufficient for recovering a complete picture of the population in this case db is less efficient 2 if the population is sac or ssh or both sampling and inference should be based upon sac or ssh or both regardless of whether the sampled data are sac or ssh 3 if the population is i i d then the data must be i i d in this case md is useless 4 a db sampling strategy will be fine if the population is i i d but it will lose efficiency if the population is ssh equation 5 28 in cochran 1977 p 99 fig 1 in wang et al 2013 or sac haining 2003 p 118 v opt v prop v ran eq 5 28 in cochran 1977 p 99 and 5 when sac is present db is less efficient than kriging in the absence of sac kriging fails to work sandwich interpolation wang et al 2013a works if the population is ssh for sst we distinguish the population and sample when mentioning data kriging is applicable if the population is sac even if the sampled data are i i d if we can obtain the variogram in other ways to assess predict and manage environmental data this paper shows that it is important to deal carefully with the properties of spatial sampling including how these affect the estimations of environmental properties that are important for humans being and for society at large therefore spatial statistical methods should be as accurate and efficient as possible to obtain the highest accuracy of subsequent research and applications in these cases sst provides a useful framework to guide the choice of the proper method declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study is supported by the national natural science foundation of china grant no 41531179 and 42071375 
25941,geospatial referenced environmental data are extensively used in environmental assessment prediction and management data are commonly obtained by nonrandom surveys or monitoring networks whereas spatial sampling and inference affect the accuracy of subsequent applications design based and model based procedures db and mb for short both allow one to address the gap between statistical inference and spatial data creating independence by sampling implies that db may neglect spatial autocorrelation sac if the sampling interval is beyond the sac range in mb however a particular sampling design can be irrelevant for inferential results empirical studies further showed that mse mean squared error values for both db and mb are affected by sac and spatial stratified heterogeneity ssh we propose a novel framework for integrating sac and ssh into db and mb we do so by distinguishing the spatial population from the spatial sample we show that spatial independence in a spatial population results in independence in a spatial sample whereas sac in a spatial population is reflected in a spatial sample if sampling distances are within the range of dependence otherwise sac is absent in the spatial sample similarly ssh in a population may or may not be inherited in data and this depends on the sampling method thus the population sample and inference constitute a so called spatial statistic trinity sst providing a new framework for spatial statistics including sampling and inference this paper shows that it greatly simplifies the choice of method in spatial sampling and inferences two empirical examples and various citations illustrate the theory keywords population and sample spatial autocorrelation sac spatial stratified heterogeneity ssh variable and random variable spatial statistic trinity sst 1 introduction spatial sampling and inference are widely used in the survey and assessment of soil atmosphere and water environments gao et al 2015 mindham and tych 2019 wang and xuan 2020 wang et al 2013b chen et al 2019 inappropriate application of the methods however may introduce bias in modelling and analysis the model based mb and the design based db statistical procedures are two common ways to address this problem db originated from bowley 1906 and it was further shaped in the 1930s e g neyman 1934 whereas mb originated from kiaer 1896 and it was shaped by fisher 1922 in section 2 and later by krige 1951 and matheron 1963 1971 the two statistical frameworks constitute two mainstream approaches in spatial statistics fig 1 cassel et al 1977 de gruijter and ter braak 1990 brus and guijiter 1997 in db the population is regarded as fixed and stochasticity in the data is introduced by random sampling estimation procedures are based on the known probabilities of sampling spatial variation within a population mainly described as either spatial autocorrelation sac or spatial stratified heterogeneity ssh is of little concern in db de gruijter and ter braak 1990 conversely in mb the population is regarded as a single realization of a joint distribution model and predictions are based on the model chosen to represent the population joint design and model based frameworks dmfs provide a general basis for statistical sampling and inference the distinction between db and mb raises some concerns 1 will the spatial variation sac and ssh of a population affect the mse of estimation when applying db 2 can information contained in the sampling design be used to improve the mse of estimation when applying mb 3 there is some overlap between the two approaches when aiming at the same target de gruijter et al 2006 p 68 to address the first issue we note that in db stratified sampling is favored over simple random sampling if the population exhibits ssh cochran 1978 wang et al 2016 for the second issue it is well known that in mb the empirical error of kriging interpolation i e the difference between the predicted value and the true value at a prediction point varies with the sampling design minasny and mcbratney 2016 for the third issue we note that overlapping makes it difficult for users to make the best choice of approach model assisted sampling and hybrid sampling overcome part of the limitations of the dmf särndal et al 1992 sterba 2009 in hybrid sampling the spatial variation of a population can be included in the joint stochastic distribution model and a sampling design of any random sample can be used in the estimation of parameters accordingly brus and guijiter s 1997 table of sampling strategies can be updated as shown in table 1 the objective of this paper is to explore the root of the inconsistency between theoretical assumptions and empirical studies for both db and mb that is db neglects the spatial variation of a population and mb neglects the sampling design while in empirical studies both of the neglects influence the errors of their estimates from the real value consequently we propose a unified framework named the spatial statistic trinity sst that will greatly simplify the model and method choice in the spatial sampling and spatial inference of environments to address the problems above the remainder of this paper is organized as follows section 2 refines the common standard of performance for various samplings and inferences section 3 proposes the generic sst framework to address the problems arisen in section 1 using the standard of performance set in section 2 then sst is compared to the existing dmf and others in section 4 finally we illustrate the key concepts of sst using empirical studies in section 5 and we draw conclusions in section 6 these sections and sub sections are supported and logically linked by sequential figures as well study background fig 1 motivation of sst with an example figs 2 and 3 connections between the components in sst using a data flow fig 4 computation issues fig 5 model efficiency comparison fig 6 sst model choice for practical applications fig 7 key concepts in sst are empirically illustrated figs 8 10 2 the performance of spatial sampling and statistical inference the gold standard of the performance of an estimator or predictor ψ is its difference from the true value ψ0 1 ψ ψ 0 where ψ is a parameter of a population ℜ or superpopulation r a superpopulation refers to a collection of random variables one for each location and a population is one of the realizations of the superpopulation in practice however ψ0 is unknown and usually only a single realization of a superpopulation or one sample i of a population is available the performance of an estimator ψ ˆ is then measured via the theoretical expectation of the p mse under the assumption of many instances of sampling of the target population by a sampling scheme p equation 2a likewise the performance of a predictor ψ is measured by the theoretical expectation of the ξ mse under the assumption of many instances of realizations of the superpopulation ξ equation 2b or the joint p and ξ mse equation 2c 2a mse p ψ i e p ψ ψ 0 2 2b mse ξ ψ ℜ e ξ ψ ψ 0 2 2c mse ξp ψ e ξp ψ ψ 0 2 where the mse refers to the mean square error and ψ may be either an estimator ψ ˆ or a predictor ψ the mse reduces to a variance in the case of an unbiased estimation 3a e p ψ i r i 1 r p i ψ i r def m ℜ 3b e ξ ψ r r r 1 r p r ψ r r def m r in this study we focus on an unbiased estimation for such a population parameter as the spatial mean p inference fails to account for the sac in a population särndal 1978 de gruijter and ter braak 1990 for a superpopulation parameter such as the model mean ξ inference only concerns the distances between sampling sites in contrast it fails to account for their specific locations note that the theoretical measures in statistics originating from gambling based on random and repeated sampling rrs and widely applied in experimental sciences seem too far from the reality in geoscience where both random and repeated sampling are rare 3 the spatial statistic trinity sst framework 3 1 motivation of sst in mb sampling is done on a population variable whereas spatial data refer to a sample cochran 1977 anselin 1988 cressie 1992 cressie 1990 haining 2003 while in db one assumes a fixed population both the population and sample can be referred to as spatial data to improve estimation for spatial data we have to deal with sac defined as the coincidence of locational and attribute similarities tobler 1970 matheron 1963 anselin 1988 goodchild 2004 sui 2004 although sac is not explicitly included in db inference sac is a property of either the population or the sample with the former being independent of the latter kriging interpolation will be useful if the population has sac particularly if distances between sampled sites are less than the range of spatial dependence instead of the p t spatial statistical dual brus and guijiter 1997 brus 2019 where p represents a sampling design and t represents an estimator we put forward an sst r i ψ composed of a population r sampling design ℑ and estimator ψ each of the three dimensions has many options consequently there are more than 100 combinations of triples fig 2 only a few of these are efficient however where the population properties assumption of estimators and sampling conditions match fig 3 gives a simple example of sst an i i d population random sampling and sample mean in which the triple of the population parameters number of units n and variance σ 2 of the statistic parameters sample mean y and its variance v and of the sampling parameters sample size n are connected by a function having an analytic solution 3 2 sst components and their relationships fig 4 illustrates the spatial data flow in sst from a generator usually a spatial process or superpopulation r to its single realization over a geographical space population ℜ which is observed by a single sampling i we are interested in predicting the values at unsampled sites the spatial mean of the population r or even the parameters of the superpopulation r based on the observed sample ℑ and an estimator or predictor ψ under specific assumptions drawing a sample from a population the values of the sample are the result of data propagation from superpopulation r to population r under sampling scheme i the spatial variation of a superpopulation such as i i d sac or ssh is inherited by the population haining 1988 p 575 eqs 1 1 1 2 cressie 1993 pp 13 15 these properties however may or may not be inherited by the sample due to the sampling method isaaks and srivastava 1989 chapter 7 haining 1988 p 576 and they may or may not be utilized by the chosen estimator because of the propagation and inheritance of the data properties table 2 we coin either spatial i i d or sac or ssh for a population instead of for a superpopulation as linear regression modelling is interpreted under the framework of population and sample without recourse to the word of superpopulation gujarati and porter 2009 p 35 43 when the distinction between population and superpopulation causes little difference for simplification we use population to refer to both population and the mechanism generating the population i e superpopulation consequently an estimation using the sample data ℑ and an estimator ψ are determined by the sampling trinity sst r i ψ sst considers the uncertainties in superpopulation modelling and population sampling as well as the match between the assumption of a model or estimator for inference and the property of a population the elements and linkages of the spatial data flow in fig 4 and table 2 are further explained in the following subsections 3 2 1 population ℜ and estimator ψ a spatial process refers to a superpopulation where its properties are inherited by its populations for example disease risk superpopulation r is to be distinguished from an exhaustive observation population ℜ of disease incidence or prevalence if we are concerned with its causation one commonly assumes that a disease occurrence follows a poisson process haining 2003 p 308 o i poisson e i r i where o i is the observed number of deaths population in the i th subarea and e i denotes the expected number of deaths from the disease then the relative risk of dying from the disease r i with r i o i e i as its maximum likelihood estimator is a superpopulation parameter that can be estimated and mapped for each subarea the risk r i may be further modeled by a prior distribution or covariates banerjee et al 2015 based on different perspectives population and superpopulation may be defined as interchangeable if it concerns individual locations then there are two concepts for estimation which are as follows p disease occurrence locations and p locations disease occurrence these concepts represent the probability of the diseases p y and the probability function of the locations p locations they are related to each other through bayes law that is p y x p x y p y p x commonly the term p x is ignored because it is irrelevant for y and we have that p y x p x y p y the spatial mean of a population ℜ and the model mean of a superpopulation r are defined as 4a m r 1 n i 1 n z i r 4b m r 1 r r 1 r m r respectively where ℜ 1 r and r stands for either a superpopulation or the number of all possible populations of a superpopulation both m ℜ and m r can be estimated by a weighted sample mean 5 m r i 1 n w i z i r where w i is adopted by a sampling design horvitz and thompson 1952 särndal et al 1992 such as 1 n if it is simple random sampling or it is calibrated if the spatial population variation is modeled 3 2 2 population ℜ and sampling ℑ the spatial variation of a population may or may not be reflected in a sample a population ℜ or superpopulation ℝ can be i i d such as a poisson process or white noise deterministic such as a plane or sinuous time series or hybrid that is between completely random and completely deterministic such as an ssh or sac given a specific estimator ψ and sampling method ℑ the mse varies with the properties of the target populations table 3 we consider three cases below case 1 deterministic population dp if a population is fully deterministic such as the path of a bouncing ball it follows newton s law ψ from a low number of sampling points of the path one can reconstruct the exact path population using newton s law as an estimator isaaks and srivastava 1989 p 199 a second example is the detail of urban planning the third example is the population y which is determined by the covariate x such as the first soil map for the united states which is strongly influenced by geology and rivers whitney 1909 soil landscape models allow for the prediction of soil properties based on landscape position branham 1989 pollution emissions by vehicles differ between steady speed and unsteady allowing for the prediction of air pollution along a road the fourth example is the temporal transmission of a communicable disease which can be well reflected by the temporal process of the susceptive exposed infective recovered seir model a few sample units randomly drawn from the seir process can calibrate the seir model to simulate the exact time series of an infectious disease wang et al 2006 sac can be derived from the spatial version of seir and then used for spatial interpolation angulo et al 2013 kolovos et al 2013 case 2 semi random population srp for ssh populations such as climate zones the annual mean temperature of an area can be estimated using a stratified sampling cochran 1977 if a population exhibits sac with a known shape of the variogram which can be calibrated by a sample even if it is i i d and small then kriging can be implemented matheron 1963 rodriguez iturbe and mejia 1974 haining 2003 context effects and uncertainty can then be observed kwan and schwanen 2018 case 3 an i i d population iid against a population containing sac or ssh an i i d population looks like a white noise picture which may be generated by a spatially independently and identically distributed superpopulation an example is a piece of farmland of seedlings that was sowed with problematic corn seeds only 80 can sprout due to variety or spoiling for example it is virtually impossible to reconstruct the i i d population using a finite sample and even advanced estimators there is no better approach than random sampling for surveying an i i d population spatial interpolation is impossible in this case table 3 a population ℜ or superpopulation r could be estimated by a mechanistic model with little data as in case 1 a weak model with much data as in case 3 or a balance between the model and data as in case 2 any choice of estimation method depends on the properties of the population as well as the available sample the properties of a population or superpopulation may be known in advance via the general and specific knowledge of the process christakos 2005 study area relevant determinants or prior exploratory sampling if no prior knowledge of the target population is available simple random or systematic sampling is applicable although this is done at the possible cost of inefficiency 3 2 3 sampling ℑ and estimator ψ sampling is usually followed by applying a method to make an inference of a population or superpopulation by the analytical solution of a sampling design we mean that the variance of an estimator is a function of sample size and vice versa for example the variances of the superpopulation mean and population mean of a simple random sampling are ripley 1981 haining 1988 griffith et al 1994 6a v r 1 r 1 n n s 2 n 6b v r 1 r 1 n n s 2 n respectively where n is the sample size n n is the sampling fraction s 2 is the variance of the population and r is the average of pairwise correlations of the study area computed by the covariance between two randomly selected points for an i i d population r 0 in 6a 6b and the right side becomes the same as the db one after a simple transform the sample size n is a function of the required variance of the sample mean an effective sample size is n n 1 r for an sac superpopulation and n n 1 r for an sac population griffith 2005 for most estimators e g kriging predictors there are no analytic solutions to find the optimal sampling design given their variances thus they must be found by simulations as fig 5 shows for a given population ℜ and sample size how simple random sampling is carried out sample ℑ1 is used by estimator ψ to estimate the spatial mean m ˆ i 1 with variance v i 1 another sampling ℑ2 is carried out to obtain m ˆ i 2 and v i 2 this process is repeated until all combinations c n n are enumerated where n and n are the numbers of units in the population and sample respectively the optimal sampling is the sampling that results into the smallest variance the population mean m ˆ can be estimated by either a db estimator ψ i ℜ of an average over all samplings ℑs in one realized population ℜ or an mb predictor ψ ℜ i of an average over many realized populations ℜs giving one sampling ℑ 3 3 model selection with sst fig 4 shows that the properties of a spatial sample are the result of information propagation from a process that is a superpopulation to a population the population is a realization of the superpopulation and is then captured by the sample and operated in an estimator or predictor sac and ssh are generic features of a spatial process or population they may or may not be reflected by a sample table 4 if the superpopulation is i i d the population will be i i d in most cases whereas autocorrelation in a superpopulation will be preserved in its populations and be present in the observed spatial data if the distance between sampling units is within the sac range the parameters of a population or superpopulation may be obtained from priors like newton s law previous surveys or a survey of a similar attribute in the same area or surveys of the attribute in a similar area in practice sac can be tested by moran s i moran 1950 or semivariogram matheron 1963 ssh can be tested by q statistic wang et al 2016 and a population is i i d if it is neither sac nor ssh many sampling methods and many estimators can be used hence any sampling and estimation can be one of the combinations of the triples fig 2 table 5 and fig 6 illustrate the efficiency of the sst estimation method selections according to sac and ssh are illustrated in fig 7 for an i i d population no approach is better than simple random sampling for inferring about a parameter when a population only presents sac without ssh kriging is preferred for an ssh population stratified sampling or sandwich wang et al 2013a is superior to simple random sampling because it requires fewer sample units to reach a given inference precision when both sac and ssh exist in a population the population is first partitioned according to its ssh and it can then be estimated by kriging in strata stein et al 1988 goovaerts 1997 or more precisely by the mean of the surface with stratified non homogeneity msn wang et al 2009 gao et al 2020 if the sample cannot cover all strata with an ssh population bias correction should be adopted such as the correction method of heckman 1979 for an ssh population and bshade wang et al 2011 and spa wang et al 2012 for a population that presents both sac and ssh in summary db p t and mb ξ brus and guijiter 1997 p 5 can be integrated by the sst r i ψ compared with a complicated dmf based decision tree for choosing between db and mb sampling and statistical strategies brus and guijiter 1997 figure 11 the sst covers the essentials of spatial statistics from sampling choice to statistical inference table 5 4 reinterpretation of key concepts under the sst in sst the mse is determined by the sampling design estimator and population properties this is different from dmf which only admits two of the triples following an influential paper interpreting dmf brus and guijter 1997 we reinterpreted some key concepts under sst table 6 lists the key notations and definitions which are compared with those in authoritative statistics books we have outlined the properties below r1 determinants of the estimates in sst prediction estimation and their variances are determined by sst see fig 2 this contrasts with dmf which considers mb to be unaffected by the sampling design and db to be unaffected by spatial variation however as an mb method kriging has an empirical error that varies according to the sampling scheme random and stratified sampling have developed as db methods and the variances of the sample means are proportional to the variance and ssh of the population r2 ξ unbiasedness and minimum ξ variance in sst fig 2 the mse is the gold standard for assessing the efficiency of spatial sampling and of an estimator this is determined by sst that is the population property sampling plan and estimator regardless of whether a db or mb method is used in dmf ξ unbiasedness and minimal ξ variance are not that useful because in mb there are no restrictions on the selection of the sampling locations r3 random variable in sst the location selection function i x is a random variable which equals 1 if site x is selected and 0 otherwise x i refers to a location i which is fixed once selected thus it is considered a variable but not a random variable this contrasts with dmf where the i th location x i is considered a random variable r4 dependence between observations functional independence is generated by the following theorem if two random variables x and y are independent and g h ℝ ℝ then g x and h y are also independent grimmitt and stirzaker 2001 p 49 theorem 6a in parzen 1960 p 295 theorem 2 in ash 1970 p 84 in sst the sampled sites x 1 and x 2 are two outcomes of an indicator random variable i x therefore application of the function independence theorem for spatial sampling is incorrect and independence between two spatial locations cannot be created by selecting the two locations independently this contrasts with the dmf where it is asserted that if x 1 and x 2 are random locations selected independently from each other then the variables z x 1 and z x 2 are also independent no matter how close the two locations are this definition falls short because two randomly selected locations are not two random variables thus the theorem of function independence is not applicable to this case r5 independence in sst samples are independent if the population is i i d or the sampling interval is beyond the sac range of the population in truth the properties of a population rather than the sample data determine the choice of methods for spatial statistics and spatial sampling sac is not included in the estimation of db cochran 1978 this is no problem if the target population is i i d but it may be less efficient than mb sampling haining 1988 cressie 1993 pp 13 15 if the data are spatially autocorrelated this is in contrast to dmf where the ξ independence of sample data is determined by the modeler and p independence is determined by the sampling design 5 case study according to sst sac in a population affects the efficiency of a sampling and the accuracy of an inference in db sac of a population is neglected or cannot be counted by a quantitative parameter särndal 1978 to demonstrate the advantage of sst to db two case studies using a simulated dataset and a climate dataset respectively were carried out in both cases the same sampling method and estimator were adopted to draw samples and to estimate the means of several populations with different sacs the sampling method and estimator are typical for db and the sample size remains the same the mse is obtained by repeated sampling 7 m s e 1 t t 1 t m ˆ t m 2 where t is the total number of sampling times for each sample size m is the true population mean and m ˆ t is the estimation of m with the sample of time t using a db estimator to make the sampling errors of different populations comparable the normalized mse a v e m s e is used 8 a v e m s e m s e m if a v e m s e varies with the sac of a population sst is a reasonable framework in both cases the finding is consistent with different sample sizes 5 1 simulated dataset five populations with sizes of 50 50 pixels whose values represent the pb concentration in soil ppm were generated using sequential gaussian simulation remy et al 2009 simple kriging with a spherical variogram was applied with the same sill and nugget but at different ranges as shown in table 7 according to the variogram the degree of sac in the five populations is range40 range20 range10 range5 range1 see table 7 and fig 8 each of the five populations was divided into four square strata to be comparable to brus and guijter s 1997 paper samples with different sizes 16 24 32 40 48 56 and 64 were drawn using simple random sampling db for each sample size 500 instances were randomly drawn to compute the a v e m s e via equation 8 the results are shown in fig 9 the a v e m s e values of simulated populations for all sample sizes take exactly the opposite order range40 range20 range10 range5 range1 to that of the sac for all sample sizes this means that the larger the sac of the population the smaller the variance inferenced from samples drawn by simple random sampling given the same sample size the sac in a population is one of the determinants of the variance of db and its impact increases with reductions in sample size 5 2 climate dataset the area aggregated meteorological data of 363 cities from the summer of 2015 june 1st august 31st 92 days obtained from the china meteorological data sharing service system data cma cn were used as populations four meteorological indicators that is the daily mean temperature tem daily mean relative humidity rhu sum of sunshine duration ssd and max wind speed wind have different sacs and represent four populations their sacs r are calculated using the following equation 9 r 1 n 2 i 1 n j 1 n c o v v i v j v a r v i v a r v j where n is the number of cities c o v v i v j is the covariance function of a meteorological factor between city i and city j and v a r v i and v a r v j are variance functions of a meteorological factor of city i and city j for tem the covariance is calculated using the following equation 10 c o v t e m i t e m j t 1 t t e m i t t e m i t e m j t t e m j t 1 where t is the days of temperature data t e m i t represents the daily mean temperature of city i on day t and t e m i is the mean temperature of city i on all t days the variance can be calculated using the following equation 11 v a r t e m i t 1 t t e m i t t e m i 2 t 1 the sac coefficients of different meteorological factors populations are listed in table 8 and tem rhu win ssd for each meteorological indicator the observed values of all the cities on each day are composed of a population the sample sizes are 16 24 32 40 48 56 and 64 five hundred samples were randomly drawn for each sample size the a v e m s e of the meteorological factors of each day were calculated and the results are plotted in fig 10 clearly the order of the a v e m s e of tem rhu win ssd is opposite that of the sacs table 8 for all sample sizes that is the larger the sac of the population the smaller the variance inferred from the samples drawn by simple random sampling the sac in a population is one determinant of the variance of db and its effect increases with reductions in the sample size besides the above two case studies liu et al 2018 tested the ssh and sac of rodent density in a study area and they found that the former is significant and the latter is weak then both the ssh based sandwich estimator wang et al 2013a and sac based kriging were applied to the same sample to map the population respectively the sandwich map has a smaller absolute error than the kriging map as expected by sst 6 discussion and conclusion according to the gold standard of the performance of an estimator or predictor either mb or db alone is incomplete in principle db is applicable but it does not guarantee optimization in all cases because it neglects the mechanism generating the population when a population is not i i d and prior knowledge about the spatial data process superpopulation or population is available there are more efficient strategies for spatial sampling and statistics mb neglects the sampling design so it fails to account for its resulting bias moreover from a technical perspective different configurations of sampling sites will produce different covariance matrices thus generating different estimation results sampling is often incorrectly neglected in spatial statistics of environmental problems statistics originating from gambling usually assumes an rrs which allows the mathematical expectation to be determined this assumption however is often far from reality in the context of spatial data where sampling is usually performed once and non randomly and the accuracy of statistical inference varies with the adopted sampling strategy the accuracy of applying spatial statistics using field data depends on the sampling including its pattern and density brevik et al 2016 sampling efficiency and the precision of inference are determined by the sst which provides a simple and clear sampling and inference decision tree with the following characteristics table 4 1 if the population is deterministic a few sample units plus a deterministic estimator are sufficient for recovering a complete picture of the population in this case db is less efficient 2 if the population is sac or ssh or both sampling and inference should be based upon sac or ssh or both regardless of whether the sampled data are sac or ssh 3 if the population is i i d then the data must be i i d in this case md is useless 4 a db sampling strategy will be fine if the population is i i d but it will lose efficiency if the population is ssh equation 5 28 in cochran 1977 p 99 fig 1 in wang et al 2013 or sac haining 2003 p 118 v opt v prop v ran eq 5 28 in cochran 1977 p 99 and 5 when sac is present db is less efficient than kriging in the absence of sac kriging fails to work sandwich interpolation wang et al 2013a works if the population is ssh for sst we distinguish the population and sample when mentioning data kriging is applicable if the population is sac even if the sampled data are i i d if we can obtain the variogram in other ways to assess predict and manage environmental data this paper shows that it is important to deal carefully with the properties of spatial sampling including how these affect the estimations of environmental properties that are important for humans being and for society at large therefore spatial statistical methods should be as accurate and efficient as possible to obtain the highest accuracy of subsequent research and applications in these cases sst provides a useful framework to guide the choice of the proper method declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study is supported by the national natural science foundation of china grant no 41531179 and 42071375 
25942,serious games are increasingly used as tools to facilitate stakeholder participation and stimulate social learning in environmental management we present the virtual river game that aims to support stakeholders in collaboratively exploring the complexity of a changed river management paradigm in the netherlands the game uses a novel hybrid interface design that features a bidirectional coupling of a physical game board to computer models we ran five game sessions involving both domain experts and non experts to assess the game s value as a participatory tool the results show that the game was effective in enabling participants to collaboratively experiment with various river interventions and in stimulating social learning as a participatory tool the game appears to be valuable to introduce non expert stakeholders to dutch river management we further discuss how the hybrid interface combines qualities usually found in board and computer games that are beneficial in engaging stakeholders and stimulating learning keywords serious gaming social learning water management stakeholder participation participatory decision making tangible interaction software availability software name virtual river developers robert jan den haan fedor baart year first official release 2020 hardware requirements pc game table drawings available including a webcam touchscreen monitor and projector a test version can be run on just a pc system requirements windows or linux delft3d fm suite 2019 01 1 5 1 41875 program language python program size 1 7 gb availability https github com erjeetje virtual river prototype license gpl 3 0 documentation readme in the github repository 1 introduction stakeholder involvement and participatory approaches are increasingly important in environmental decision making pahl wostl et al 2008 reed 2008 voinov et al 2016 a recent shift from sectoral towards more integrated natural resources management has made stakeholder participation essential to the pursuit of cross disciplinary objectives berkes 2009 pahl wostl 2007 reed 2008 at the same time stakeholder participation is recognized as an effective way to improve the quality of and acceptance in decision making cundill and rodela 2012 reed 2008 voinov et al 2016 one method that is receiving increasing attention to facilitate stakeholder participation in environmental management is serious gaming aubert et al 2018 rusca et al 2012 voinov et al 2016 serious games are generally referred to as games that have a primary purpose other than mere entertainment michael and chen 2005 susi et al 2007 in the context of environmental management and participation serious games are defined by mayer 2009 as experi m ent i al rule based interactive environments where players learn by taking actions and by experiencing their effects through feedback mechanisms that are deliberately built into and around the game to provide feedback on actions such serious games include a simplified representation of reality in terms of both the environmental system and its stakeholders harteveld 2011 redpath et al 2018 rodela et al 2019 in this way serious games enable stakeholders to explore environmental challenges interventions and the effects of such interventions in an environment in which it is safe to experiment furthermore serious games enable stakeholders to experience the strategic interactions between stakeholders by explicitly including interaction rules and assuming stakeholder roles in the game therefore serious games facilitate stakeholders learning about both the physical technical and the inherent socio political complexities bekebrede 2010 de caluwé et al 2012 geurts et al 2007 mayer 2009 the lessons learned while playing serious games can be both relevant and transferable to real world decision making geurts et al 2007 mayer 2009 one promising opportunity for serious games in the context of stakeholder participation in environmental management is their use as what rodela et al 2019 categorize as learning based interventions such games are developed to engage stakeholders in dialogue and activity in order to contribute to what is commonly referred to as social learning changes in understanding through interaction in collaborative and participatory processes that go beyond the individual see e g cundill and rodela 2012 muro and jeffrey 2008 reed et al 2010 rodela 2011 as learning based interventions games are therefore developed under the assumptions that these 1 provide stakeholders with participatory environments that facilitate the stakeholder interactions and collaborative experimentation that are essential to establish social learning and 2 contribute to individuals or groups experiencing a change in understanding as a result of the game s collaborative activity ampatzidou et al 2018 flood et al 2018 medema et al 2016 in the literature social learning is usually operationalized as 1 cognitive learning acquiring new or restructuring existing knowledge 2 normative learning changing viewpoints values or paradigms and 3 relational learning increasing the understanding of the mind set and perspectives of other stakeholders as well as fostering the ability to cooperate among stakeholders baird et al 2014 ensor and harvey 2015 in the context of environmental management there are various examples of serious games as learning based interventions see e g reviews by aubert et al 2018 den haan and van der voort 2018 flood et al 2018 for games on water management sustainability and climate change respectively as one of a few recent examples from a far longer list craven et al 2017 developed simbasin to bring stakeholders together with the aim of developing a shared understanding and sense of urgency around the management of the magdalena cauca river basin in colombia they showed that the game was successful in creating an open discussion space to bring stakeholders and scientists together the sustainable delta game valkering et al 2013 van der wal et al 2016 challenges stakeholders to develop collective strategies to manage a fictional stretch of a dutch river and aims to help them to learn about the complex interactions between river management climate change and changes in society results from twelve sessions showed that playing the game led to the convergence of the players perspectives van der wal et al 2016 becu et al 2017 developed littosim to enable social learning among local authority managers on oléron island in france about prevention measures to reduce the risk of coastal flooding they showed that littosim facilitated stakeholder experimentation with and learning about risk prevention measures in relation to possible flood events van hardeveld et al 2019 developed the re peat game to explore collaborative management strategies to help reduce soil subsidence in the netherlands results from ten sessions showed that re peat improved cooperation among peatland stakeholders increased their understanding of the problems and led them to possible strategies for reducing soil subsidence recent changes in dutch river management provide a valuable case study for designing and evaluating a serious game as a participatory tool traditionally river management has been dominated by dike strengthening and was therefore within the field of hydraulic engineering a new management paradigm involving spatial measures and multifunctional design brings in many other stakeholders that are traditionally not usually involved with river management the challenge of collaboration between these diverse groups of stakeholders in what is still a rather technical field demands a shared understanding of the physical system in this paper we present the virtual river game a serious game to collaboratively explore river management complexity in the netherlands the game was played in five sessions involving both domain experts and non experts to assess its value as a participatory tool it was guided by the research questions 1 to what extent does the game facilitate stakeholders collaboratively exploring and experimenting with river interventions and 2 to what extent does playing the game lead to social learning outcomes the paper is structured as follows the next section discusses the game and particularly the fact that it facilitates stakeholders to apply river interventions on a game board that has a bidirectional link to computer models section 3 presents the assessment approach of the game including an overview of the sessions and the methods to collect and analyze the data in order to address the research questions section 4 reports on the factual game output of the sessions observations of the in game discussions and the participants self reported learning outcomes the paper ends with a discussion of the value of the game as a participatory tool as well as on how the game s hybrid interface combines those qualities found in board and computer games that benefit and support stakeholder participation and social learning processes 2 game description 2 1 background and aim to protect the deltaic floodplains from flooding the traditional approach in the netherlands has been to build and reinforce dikes however near flood events in the 1990s shifted the approach of dutch river management towards applying so called spatial measures that aim to create more space for rivers to safely discharge water rijke et al 2012 warner et al 2012 for example by digging side channels lowering floodplains and moving back dikes see e g berends et al 2019 straatsma et al 2019 van stokkom et al 2005 in addition to lowering peak water levels such spatial measures also aimed to restore the local ecology fliervoet et al 2013 klijn et al 2013 straatsma et al 2017 this paradigm shift while still retaining flood safety as its primary focus led to an increasingly more integrated river management approach as a result it attracted new stakeholders to river management verbrugge et al 2019 and emphasized the importance of stakeholder participation in decision making edelenbos et al 2017 fliervoet et al 2013 zevenbergen et al 2015 in the context of the new dutch river management paradigm we set out to develop the virtual river game as a tool to increase and support stakeholder participation the game can beneficially be played at an early stage of a project as an icebreaker activity but that is disconnected from the project s actual decision making the game enables stakeholders to collaboratively experiment with river interventions in order to increase their understanding of dutch river management including both the physical system and the effects and trade offs of specific interventions and the perspectives and interests of other stakeholders in relation to such interventions during the game s design process we found that some stakeholders particularly those introduced to river management as a result of the paradigm change view the hydrodynamic models central to dutch river management decision making as mysterious black boxes den haan et al 2018 therefore to support stakeholder participation we set out to develop the virtual river game to enable stakeholders regardless of background and expertise to work with a hydrodynamic model that is widely used in practice to that end we developed a hybrid interface based on tangible interaction linking physical forms to digital information hornecker and buur 2006 ishii 2008 the interface features a bidirectional coupling of a physical game board to a hydrodynamic ecological and cost model an impression of the virtual river game and its interface is shown in fig 1 the virtual river game uses hexagonal tiles to represent a typical stretch of a dutch river that includes a main channel and floodplains and dikes on both sides of the channel the changing of the tiles on the game board provides input to the models while the models output is visualized both on the game board through projection and in a game engine shown on a touchscreen monitor an overview of the game s software and hardware components is shown in fig 2 in the following sections we first introduce the models and their integration in the virtual river game followed by a description of the game itself 2 2 monodisciplinary models integrated in the game 2 2 1 delft3d flexible mesh hydrodynamic model to model water flow and water levels in the game area we incorporated the delft3d flexible mesh fm hydrodynamic model to compute the hydrodynamic response to system change berends et al 2019 kernkamp et al 2011 we use a rectangular numerical grid of cell size 20 m by 20 m initial bed levels and chézy friction coefficients are determined at the start of a game and are updated as the game progresses the boundary conditions are given by an upstream constant discharge and a downstream constant water level we use default parameter settings si table 1 water levels and flow velocities are the model outputs of interest 2 2 2 biosafe biodiversity model to model the potential biodiversity of the game area we integrated the biosafe model as developed by lenders et al 2001 de nooij et al 2004 and straatsma et al 2017 the model calculates biodiversity scores based on the potential occurrence of protected and endangered species in each ecotope the laws and regulations protecting the species and the surface area distribution of the main channel and floodplain ecotopes between the dikes ecotopes are defined as spatial landscape units that are homogeneous as to vegetation structure succession stage and the main abiotic factors that are relevant to plant growth klijn and de haes 1994 as input the model needs the ecotopes and their surface areas as output the model provides potential biodiversity scores for seven taxonomic groups mammals birds herpetofauna fish butterflies dragonflies and damselflies and higher plants 2 2 3 vrcost cost model to model the costs of interventions we created a model in python by translating unit prices for costs for interventions in dutch river management straatsma et al 2019 to interventions in the game unit prices relate to costs per volume area or length hexagon cross section for example a volume of soil may have to be excavated to construct a side channel in the river s floodplain the model distinguishes four cost categories excavations construction of hydraulic structures land use changes and land acquisition as input the model needs the elevation and land use change as output the model calculates the total costs for changes on the board as well as the costs per type 2 3 interface design model integration for the virtual river game s hybrid interface we designed and built a physical table fig 1 that includes the game board plus an off the shelf webcam touchscreen monitor and projector the game board consists of 143 hexagonal tiles representing a stretch of river the tabletop has an open aluminum mesh into which the tiles slot leaving the bottom side of each tile visible to the webcam each tile contains information on terrain height and land use which can be independently varied table 1 shows the five elevation levels and twelve land use types and their potential combinations we chose the five elevation levels as a representation of the varying elevations found in dutch rivers for the land use types we took inspiration from the classification of vegetation types used by the dutch public works authority rijkswaterstaat 2012 markers on the bottom of the tiles enable the conversion of the physical board to a digital board defining each tile s elevation and land use in the game s software based on a picture taken by the webcam throughout the game the system can be updated triggering the software to process the latest board state in the following subsection we explain the additional processing steps in the software needed to link the board to the models 2 3 1 model interface the game software converts the digital board to a digital elevation model dem and a roughness distribution as input to delft3d fm the dem is created by an inverse distance interpolation power of 2 of the terrain height at the center of the three nearest tiles to a regular grid indexed to the computational grid used in delft3d fm the roughness classes are based on a lookup table from land use to roughness class table a1 subsequently the software calculates the hydraulic roughness by retrieving the water levels of locations from delft3d fm and applying the vegetation friction model of klopstra et al 1996 the software sets the elevation and roughness coefficients and also retrieves water levels and flow velocity from delft3d fm through the basic model interface bmi peckham et al 2013 using the python bmi baart 2017 for biosafe the software converts the digital board to an ecotope distribution through a lookup table that links terrain height and land use to ecotopes table a1 a subset of 15 ecotopes out of the 82 fluvial ecotopes defined in the dutch ecotope classification van der molen et al 2003 are included composite land use such as main channel with a longitudinal training dam are split up into their pure ecotope classes in order to calculate the surface areas of all ecotopes the python version of biosafe straatsma et al 2017 is integrated in the game s software to enable sending of the ecotopes and retrieving the potential biodiversity score for the taxonomic groups for vrcost the software compares the previous and new board states to detect changes in elevation and land use changes on the board are sent directly to the model and the costs of changes including their breakdown into the four categories are retrieved 2 3 2 model feedback the virtual river game offers model output in two locations on the game board itself and on the touchscreen on the physical game board the software visualizes the dem water flow patterns from delft3d fm and hydraulic roughness coefficients by converting these into colormaps which are subsequently projected on the board the flow pattern is visualized as diffusive paint blobs that follow the flow lines the hydrodynamic effects of changing tiles on the board are therefore visualized based on the model s output and projected on the same location as which changes take place through these choices we aimed at making the hydrodynamic model more accessible and transparent by providing a tangible easy to use interface and by enabling players to link their actions to the model s output on the touchscreen the game shows the tygron geodesign platform a 3d spatial planning modeling tool that is also used as a virtual game engine for those serious games that have a spatial development component bekebrede et al 2015 van hardeveld et al 2019 warmerdam et al 2006 the game interfaces with the tygron game engine through its api tygron 2018 the inclusion of the engine serves two purposes first the dem and land use types of the digital board are converted to a virtual game world that matches the game board for example the engine shows trees for tiles on the game board that represent the forest land use second interactive panels in the engine provide players with the output from and information about all three models we deliberately developed the game in such a way that players are able to fully control the interface without needing the game s facilitator players change tiles on the board switch visualizations through the graphical user interface gui displayed on the touchscreen and inspect the virtual world as well as open panels in the game engine updating the board state which involves processing changes on the board running the models and updating both the visualizations and information in the tygron engine only takes between 15 and 30 s depending on the time needed for water levels in the hydrodynamic model to stabilize the update times are based on a dedicated portable computer utilizing an amd ryzen x3700 desktop processor to run the game and models locally 2 4 virtual river game in the virtual river game players are challenged to manage a 3500 m long deltaic stretch of river incorporating a navigable main channel that has floodplains and dikes on both sides the game scenario reflects a high river discharge as result of which the floodplains are inundated three teams each consisting of one or two players play the roles of flood manager nature manager and financial manager collectively the players are given a budget and are tasked to improve the flood safety status and ecological value of the area each team is given additional objectives as well as special abilities to block implementations of interventions based on real world stakeholders legislation and european union development targets the flood manager mirrors the dutch public works authority which is responsible for ensuring and maintaining adequate flood safety levels the flood manager can block interventions if these decrease flood safety levels or if land use is changed to a type leading to high hydraulic roughness reed and brushwood shrubs forest and mixtype which reflects the legislative power of the public works authority the nature manager represents larger nature management organizations such as the dutch state forestry agency which own and manage a large percentage of the dutch floodplains and aim to develop their ecological value the nature manager can block interventions if those decrease the area s ecological value to reflect the common view that nature organizations hold less power than for example the dutch public works authority in real world decision making this ability can only be used once during the game the financial manager represents a combination of the dutch national government which allocates budget for river projects and regional governments which are the commissioners of river projects and responsible for managing the floodplains as natura 2000 areas under the eu birds and habitats directives the financial manager can block interventions if those would take more than half of the initial budget or if these require expensive land acquisition buildings and agricultural land which reflects the interest of governments to pursue cost effective win win solutions if players are part of any of these or similar organizations they are assigned a role that is different from their regular role to let them experience river management from another point of view each game consists of a maximum of four rounds during each round players apply one of six interventions side channel construction floodplain lowering grading floodplain smoothing changing the floodplain vegetation to lower roughness replacement of groins alternatively termed wing dikes or spur dikes with longitudinal training dams dike relocation or dike reinforcement players first discuss and agree which intervention they wish to apply they subsequently implement the chosen intervention by changing tiles on the game board while following the intervention s rules during this implementation phase players can continually change tiles and collectively evaluate the intervention s effects before agreeing on a final implementation 2 4 1 in game scoring in the virtual river game we included performance indicators on flood safety biodiversity and budget therefore interventions are evaluated on hydrodynamics ecology and costs as in the delft3d fm biosafe and vrcost models each indicator has its own progress bar across a 0 100 score range minimum 50 good 65 and excellent 80 scores are provided for both the flood safety and biodiversity indicators we determined these scores as a balance between the game s representation of reality and its playability see harteveld 2011 that 1 reflects dutch river management practice in terms of performance and 2 corresponds to the difficulty of attaining good scores in the game players can click each indicator s progress bar on the touchscreen to see graphics on the effects of interventions on that indicator in addition separate information panels are available to players where they can see simplified scores after each tile change to anticipate the effects of interventions the flood safety score is based on the water levels along the river axis in comparison to the crest height of the dike at each tile location on the board the water levels can be at or above just below or well below the dike s crest height corresponding to that location being considered unsafe moderately safe and substantially safe respectively unsafe moderately safe and substantially safe locations each contribute to the flood safety score as zero half and full score respectively consequently the overall flood safety score is 0 when all dike locations are considered unsafe and 100 when all dike locations are considered substantially safe flood safety is visualized in the indicator panel as a longitudinal profile of the stretch of river that includes initial and current water levels and as a top view with each dike tile colored red yellow or green representing unsafe moderately safe and substantially safe respectively the biodiversity score is based on the potential biodiversity the sum of the scores on the seven taxonomic groups over the whole river reach this sum is converted to a percentage in the 0 100 range for simplicity we determined the 0 and 100 scores for biodiversity by running a monte carlo simulation to find the board layouts that result in the lowest and highest potential biodiversity scores respectively that can be achieved in the game in the indicator panel the potential biodiversity of the taxonomic groups the sum and the corresponding score are provided furthermore a bar graph shows the scores on the seven taxonomic groups for both the initial and the current game board a second bar graph shows the change for each taxonomic group expressed as a percentage the budget score reflects the remaining budget that players have available as a percentage of the initial budget that they received at the start of a game the initial budget is therefore equivalent to a 100 score spending the whole budget results in a 0 score and spending more than the budget results in a negative score a graph shows the budget spent per round as well as the remaining budget in the budget indicator panel expressed both in euros and a percentage of the initial budget in a second graph the breakdown of the costs incurred per round is shown as stacked bars 2 4 2 in game objectives teams are scored on each indicator separately throughout the game as a collective objective the players have to achieve the minimum score 50 for both flood safety and biodiversity in the four rounds while preferably staying within the budget collectively the teams may decide not to play all rounds if they reach the collective objective before the fourth round additionally each team is given one main and two secondary objectives the teams receive the instruction that in order to win the game they have to reach both the collective objective and their role specific main objective the role specific secondary objectives are presented as bonus points that they can earn the flood manager is given the main objective to achieve a good flood safety score 65 the secondary objectives are to achieve an excellent flood safety score 80 and ending the game without any unsafe dike locations the nature manager has the main objective of achieving a the good biodiversity score 65 and the secondary objectives of achieving an excellent biodiversity score 80 and to end the game with five or more forest locations within the floodplains the budget manager is tasked to limit spending to the collective budget the secondary objectives are to achieve good scores for flood safety and biodiversity 65 the collective objective is known to all players each team is given its main and secondary objectives at the start of the game by blindly drawing one of four role specific objective cards players do not know that these four cards all list the same main and secondary objectives for their team players are not told whether or not to share their team s objectives with other players 3 virtual river game evaluation we developed the virtual river game to support stakeholder participation in the new dutch river management paradigm in this study we set out to assess the potential of the game as a participatory tool guided by the research questions 1 to what extent does the game facilitate stakeholders collaboratively exploring and experimenting with river interventions and 2 to what extent does playing the game lead to social learning outcomes the research questions address what aubert et al 2019 refer to as the process oriented how are these outcomes achieved and variance oriented what outcomes are achieved assessment of serious games the first research question focused on the game itself to evaluate its ability to engage stakeholders in dialogue and activity a prerequisite to stimulating social learning following the scope of the game and its interface design we were particularly interested in evaluating the extent to which the game facilitates both domain experts and non experts to engage in collaborative experimentation with river interventions we considered the game to be successful as a participatory tool when participants regardless of their background and expertise 1 developed a shared understanding of the problem 2 developed a collaborative strategy to address that problem 3 engaged in discussions on how interventions affect indicators and role objectives and 4 applied and tested various implementations of interventions the second research question focused on evaluating to what extent playing the game led to social learning by individual participants following the game s scope we focused on learning outcomes related to cognitive and relational learning baird et al 2014 ensor and harvey 2015 cognitive learning outcomes that were assessed included gaining an improved understanding of 1 the functioning of the river system 2 the effects of interventions and their trade offs 3 how hydrodynamic models work and are used in decision making and 4 the conflicts and opportunities for cooperation between the various stakeholder roles relational learning outcomes that were assessed relate to gaining an improved understanding of the mind sets and perspectives of other participants we considered the game to be successful when both domain experts and non experts achieve cognitive or relational learning outcomes or both as experts bring their knowledge and experience to the game we expected that they would achieve fewer cognitive learning outcomes than non experts to address the research questions we organized five sessions playing the virtual river game in the following subsection we describe the setup of the sessions and their participants in subsection 3 2 we describe the data collection methods and the data analysis used to address both research questions 3 1 sessions and participants the description of a session playing the virtual river game is provided in box 1 we invited both domain experts professionals working in dutch river management and non experts participants without river management expertise to the sessions none of the sessions were linked to a real world river project this made it difficult to engage with non experts who are also real world stakeholders in dutch river management therefore we invited design researchers and game designers to represent non expert stakeholders we organized two sessions that included only expert participants and three sessions of experts and non experts table 2 each session had between four and six participants in a total of 26 participants the length of each session including the explanations and debriefings was around 3 h the actual length of gameplay ranged between 78 and 109 min all sessions had the same experienced facilitator and a trained observer we prepared an initial board fig 3 as a scenario that we used in every session the scenario reflected a stretch of river with a hydrodynamic bottleneck formed by narrow floodplains higher floodplain terrain and vegetation that causes high hydraulic roughness resulting in unsafe water levels upstream from the bottleneck after the second session we changed one parameter of the scenario by lowering the initial budget by 30 to 17 5 million because the initial budget was found to be too high although reducing the budget between sessions was undesirable we aimed to stimulate more discussions that also addressed the cost effectiveness of interventions one of the main criteria in real world decision making we chose the 30 reduction as this aligned with the budget spent during the first session 3 2 data collection and analysis we applied a multi method approach to collecting and analyzing both quantitative and qualitative data our approach consisted of 1 a pre game questionnaire 2 in game data logging 3 in game observations 4 a post game questionnaire and 5 a post game debriefing before starting the game the participants filled out a short questionnaire stating their age expertise experience related to river management and experience with serious gaming all so as to separate expert from non expert participants si table 2 during the game the participants decisions and performance for each update were stored in a log file in addition we observed the participants discussions during the game we used an observation recording form to document a timeline of each session by linking discussions to the three indicators as well as to intermediate and final decisions taken in the game si table 3 we used this timeline to analyze each session s outcomes e g interventions applied final layout score progression and processes e g discussions consideration of options contributions of participants in order to address the first research question directly after playing the game the participants completed a second questionnaire that focused on their overall impressions of the game and the insights gained by playing the game si table 4 in closed questions the participants were asked to self report their experiences by rating their agreement with statements on a 5 point likert scale that ranged from strongly agree to strongly disagree a common approach in game studies bekebrede et al 2018 keijser et al 2018 mayer et al 2013 in one open question the participants were asked to list a maximum of three main insights obtained by playing the game we categorized the open question answers in relation to cognitive and relational learning to determine what experts and non experts identify as the main insights from playing the game we used the data from the completed questionnaires to address the second research question one non expert participant did not complete the post game questionnaire and was therefore excluded from the results ntotal 25 for the questionnaire to conclude each session we conducted a debriefing to collectively reflect on the game activity the debriefing was set up along the lines proposed by kriz 2010 six phases to structurally reflect on the game activity to confirm that the discussions and considerations in the game reflect those encountered in practice we expanded the third phase of the debriefing which includes reflecting on the game s external validity van den hoogen et al 2014 in the two expert sessions following informed consent we recorded the debriefings to transcribe these for later analysis we used the debriefing data to further interpret the analyses to help address both research questions in addition we compared the data to address the two research questions with each other to strive for data triangulation 4 results 4 1 collaborative experimentation with river interventions rq1 the first research question was to what extent the game facilitates stakeholders to collaboratively exploring and experimenting with river interventions here we present the chosen interventions and indicator scores as the factual game output as well as the observations of the in game discussions about and experimentations with those interventions 4 1 1 interventions and scores the choice and implementations of interventions during the game sessions showed that good scores between 65 and 79 for both flood safety and biodiversity can be achieved in several ways fig 4 for flood safety only the participants in the fourth session 71 did not achieve an excellent score 80 or higher excellent scores for biodiversity were not achieved in any session in all sessions the participants first focused on increasing the flood safety scores generally spending most of the budget during the first two rounds increasing the biodiversity scores became the focus during the later rounds participants in the third and fifth sessions used their remaining budget to achieve good biodiversity scores between 65 and 79 but only after achieving satisfactory flood safety scores during all sessions participants applied the floodplain smoothing intervention to increase their biodiversity scores while at times also optimizing their flood safety scores during the second and third sessions participants also chose the side channel construction and replaced groins with longitudinal training dams to increase their biodiversity scores 4 1 2 strategies and discussions at the start of each session participants explored the game scenario by changing the board visualizations inspecting the information on the touchscreen and linking the information from these two sources initial discussions focused on establishing a shared understanding of the flood safety bottleneck and on how flood safety could be improved during the mixed sessions expert participants were observed taking prominent roles in these early discussions these experts supplemented the in game information by explaining what they saw as problems for example that the bottleneck caused higher water levels upstream and suggested what could be done to improve the indicator scores non experts initially followed their lead but then started to make their own suggestions as the games progressed during the sessions that included only professionals experts who regularly work with hydrodynamic models were similarly observed to take more prominent roles at the start of the game the discussions during these sessions moved more quickly from identifying the bottleneck to possible interventions than in the mixed sessions during these discussions flood safety specialists were observed to start the games as if they were in their real world roles even though they had not been assigned to the flood safety manager team game discussions during the early rounds did include how tackling the flood safety bottleneck could simultaneously improve biodiversity without the costs of interventions rising unacceptably however no change in general direction of these discussions was observed as a result of lowering the budget after the second session after establishing a shared understanding of the problem the discussions during all sessions focused on deciding which intervention to implement in the first round in no session did the participants develop a collaborative strategy at the start of the game during the sessions the participants were observed to plan ahead by discussing how the intervention they were implementing could be made more effective by applying a different intervention in the next round experts continued to supplement the in game information for example by explaining that changing land use to a type with high hydraulic roughness to increase biodiversity e g to a forest is best done at locations where there is a low flow velocity e g next to a dike to limit its effect on flood safety discussions during the later stages of the game featured more bargaining between the teams each seeking to pursue their main and secondary objectives for example discussions during the third and fifth sessions included teams agreeing on the choice of intervention in one round only if the other teams agreed beforehand on the intervention to be made in the subsequent round during these bargaining discussions participants started to share their team objectives only in the fifth session did the participants share their team objectives at the start of the game to establish their own common objectives guided mostly by one expert participant who advocated forgetting about the politics no notable differences in late game discussions were observed between the expert and mixed sessions non experts were as active in proposing implementations of interventions as experts were and their suggestions were also applied on the board in the three sessions after reducing the budget the costs of interventions were emphasized more during the late game discussions in particular by the budget manager teams during the debriefings participants confirmed that they did not have a collective strategy for the game but that it felt natural to them to plan ahead while testing implementations of interventions additionally participants in the second third and fifth sessions explicitly indicated that it seemed logical to them to start with the most drastic intervention and to subsequently optimize from there before asking about the connection of the game activity to reality during the third session expert session the following discussion took place expert a and that the game more or less reflects how things work in reality i am working together with expert d on an area in which we have the same type of discussions expert b i really liked that you could quickly update and inspect the results like how does this work out how far are we now that is very useful the risk is that you start micro managing the environment but yeah expert c but that is also realistic expert b yes true expert d the game feels very realistic because the discussions we were having about the board where we were working with those are the same discussions you have in practice and the advantage is that you can quickly update the situation and inspect the effects so you really get a feel of what the interventions do the discussion illustrates that these experts found that using the board to apply interventions felt sufficiently realistic and elicited discussions similar to those occurring in practice experts in other sessions provided feedback that supported these findings during the debriefings and in written statements made in the post game questionnaire experts in the first third and fifth sessions did mention that they missed a role that directly represents the interest of agriculture in the game as a result although facing a financial penalty for the compulsory purchase of land they mentioned that they could convert agricultural land into other land use types without the emotional discussions associated with the compulsory purchase of land by the government that would occur in real life 4 1 3 experimentation approach in all sessions participants used the opportunity to experiment with interventions in sessions 1 to 4 the participants applied and evaluated between 17 and 20 implementations of interventions while playing the game fig 4 the number of applied interventions in the fifth session was notably lower which was probably caused by internet problems during the session that resulted in a loss of time to play the game to help propose interventions participants used the board as a map to suggest interventions by hand gestures and used the information both from the visualizations as well as on the touchscreen to formulate arguments participants experimented with interventions both to test implementations at several locations e g constructing a side channel on the other side of the main channel comparison and to improve the implementation at a chosen location optimization especially the floodplain smoothing intervention led to experimentation with participants in the second and third sessions trying eight and eleven implementations respectively in these experimentations non experts were observed to realize that lowering the hydraulic roughness from locations with higher water flow velocity is effective in increasing the flood safety score furthermore participants were observed to realize through experimentation that changing a production meadow intensively managed grassland to a more natural setting less managed is most effective at increasing biodiversity 4 2 experience and social learning outcomes rq2 the second research question was to what extent does playing the game lead to social learning outcomes here we describe the participants overall impression on the game and their self reported learning outcomes with respect to both cognitive and relational learning the questionnaire results indicate that the game was well received by both domain experts and non experts fig 5 most participants indicated that the game s goal was clear and that they enjoyed playing the game during the debriefings participants also frequently mentioned that they regarded the game as a fun activity regarding the learning outcomes the questionnaire results indicate that participants gained insights into both cognitive and relational learning fig 6 compared to experts an equal or higher percentage of the non expert participants gave answers to the statements that showed strong agreement below we discuss the results for experts and non experts separately fig 6 shows that non experts mostly agreed or strongly agreed with the twelve statements non experts agreed most with statements 11 and 12 μ 4 40 and 4 60 respectively related to relational learning in relation to cognitive learning non experts agreed most with statements 1 and 10 both μ 4 20 only on statement 4 μ 3 40 about the costs of interventions did two non experts give answers that showed strong disagreement these participants explained that they had difficulty interpreting information about costs during the game in the open question non experts most frequently mentioned insights related to the functioning of the river system 10 and interventions and trade offs 6 categories si table 5 both associated with cognitive learning most of these insights were broadly formulated such as the relation between flow velocity and flood safety and trade offs between decisions experts gave more mixed responses to the statements than non experts experts agreed most strongly with statement 5 μ 3 87 related to the trade offs between interventions they also rated statements 11 and 12 μ 3 67 and 3 80 respectively on relational learning positively with most experts only agreeing but a few strongly agreeing statements 6 and 7 μ 2 67 and 2 93 respectively on hydrodynamic models were rated poorly with more experts disagreeing than agreeing with both statements this was not an unexpected result as experts regularly work with these models anyway in the open question experts most frequently mentioned insights related to interventions and trade offs 8 and player perspectives 7 si table 5 associated with cognitive learning and relational learning respectively on the interventions and trade offs insights were formulated more specifically than by non experts such as insight into the ratio of costs of different interventions and more insights into the costs of interventions in relation to their effectiveness experts indicated in both the questionnaires and the debriefings that they gained such insights as a result of playing a role different from their day to day role during the debriefings participants collectively reflected on what helped them to obtain their main insights and mentioned the game s feedback all sessions the shared exploration and experimentation with interventions sessions 2 3 4 and 5 explanations from other participants sessions 1 2 4 and 5 and finding it easier to interpret information from the game board than from a screen session 5 in all three mixed sessions non experts explicitly pointed out that the experts had helped them to understand the problem as well as to predict the consequences of interventions experts in turn indicated that collaborative experimentation with interventions in combination with the game s feedback helped them to explain river management principles to non experts 5 discussion in this section we first reflect on the potential value of the virtual river game as a participatory tool based on the study s results next we discuss the design of the game and its interface we end the section by reflecting on the study and outlining its limitations 5 1 game as a participatory tool the results of the sessions indicate that the game was successful at engaging both domain experts and non experts in collaboratively exploring and experimenting with river interventions in all sessions the participants developed a shared understanding of the problem to address the problem they applied and tested various implementations of interventions although the participants did not develop a collective strategy from the start they did collaboratively plan ahead while applying interventions discussing how future interventions could further improve the indicator scores and contribute to achieving the various objectives experts and non experts were both active in the games discussions with experts taking more of a leading role especially at the start of the games furthermore experts noted that the discussions and considerations that they had during the game reflect well those found in real world projects the results further indicate that the virtual river game stimulated social learning for both experts n 15 and non experts n 10 considering the game s scope and the operationalization of social learning we looked specifically at cognitive learning acquiring new or restructuring existing knowledge and relational learning increasing the understanding of the mind set and perspectives of others baird et al 2014 ensor and harvey 2015 with only a few exceptions non experts rated all statements in relation to the game s learning outcomes positively according to their comments the main insights that non experts emphasized were almost exclusively associated with cognitive learning however experts rated the statements in a more mixed way with some statements rated positively but some statements associated with cognitive learning neutrally to negatively we expected that experts would report less cognitive learning given their background knowledge the statements on relational learning were rated positively by experts which was also reflected in their reported main insights taken together we conclude that both domain experts and non experts learned by playing the game however the extent to which they learned and emphasis they placed on it varied the game appears to be particularly valuable as a participatory tool to introduce non expert stakeholders to river management in the netherlands keijser et al 2018 reported similar results from their game on marine spatial planning showing that their game worked well as an introductory game to engage non experts on the topic on learning more generally our results are in line with evaluations of other serious games see e g reviews by aubert et al 2018 den haan and van der voort 2018 flood et al 2018 which have shown that serious games are an effective way of increasing the understanding of physical systems becu et al 2017 carson et al 2018 keijser et al 2018 of raising awareness onencan and van de walle 2018 stefanska et al 2011 van pelt et al 2015 and of increasing the understanding of alternative views and perspectives douven et al 2014 jean et al 2018 souchère et al 2010 combining the results on collaborative exploration and learning this study offers further evidence that serious games are effective ways of improving social learning hofstede et al 2010 medema et al 2016 savic et al 2016 5 2 game and interface design serious games are increasingly explored as learning based interventions in environmental management aubert et al 2019 flood et al 2018 rodela et al 2019 in terms of the design of such games this work contributes to the growing body of literature by proposing a new hybrid interface design concept the bidirectional coupling of a physical game board to computer models whereas other serious games on environmental management tend to use simplified custom computer models that are location specific e g craven et al 2017 valkering et al 2013 our focus was on creating a simplified yet realistic 3d representation of a typical stretch of a dutch river in combination with models currently used in practice designing serious games always requires some form of simplification to strike a good balance between an accurate representation of reality the playability of reality and the meaning of the game harteveld 2011 in our approach the biggest simplification of the physical reality was not in the models but in the fact that the board uses a fixed hexagonal grid we applied this simplification mainly to increase playability the fixed number and shape of the tiles reduces the level of detail in relation to a real stretch of river and provides a manageable structure by limiting the number of possible arrangements in the game the sessions showed that the game board design facilitated participants working together and as an interface enabled especially non experts to work with models currently used in practice moreover experts determined that the game triggered discussions that also occur in practice therefore simplifying a river stretch to a hexagonal grid proved to be a suitable approach to balancing a good representation of reality with playability increasing the level of detail in the game could be further explored but is not necessary to facilitate the collaborative exploration of and experimentation with river interventions in addition to serving its specific purpose the hybrid interface offers qualities that we see as beneficial to facilitating stakeholder participation in environmental management and to improving social learning serious games have been developed as board games hertzog et al 2014 keijser et al 2018 speelman et al 2014 as digital games ayadi et al 2014 carson et al 2018 craven et al 2017 hill et al 2014 and as hybrid games combining elements of board and digital games cleland et al 2012 magnuszewski et al 2018 valkering et al 2013 van der wal et al 2016 such games have a common feature that participants are provided with an experimentation environment in which they can collaboratively and safely explore both the techno physical and socio political complexity of an environmental system with the hybrid interface design of the virtual river game we combined three qualities that are separately found in board and computer games these qualities are described below first the hybrid interface provides participants with a physical tangible object to engage with in shared exploration through the game board and touchscreen control of the game is shared and not limited to a facilitator or one participant controlling an input device participants engage in discussions by moving game tiles or pointing to locations on the game board making their views explicit to other participants stanton et al 2001 suzuki and kato 1995 second as an interface the game board appeared to lower the threshold for participating while allowing participants to collaboratively work with computer models by using tangible objects removing the need to have specific expertise in using these models computer games have been developed as interaction layers to models based on a gui chew et al 2013 craven et al 2017 van hardeveld et al 2019 but without the use of tangible objects as a direct source of input in turn games have been developed that use a non digital board in combination with computer models magnuszewski et al 2018 stefanska et al 2011 but without either a direct connection to the models or the models output being visualized on the board in the virtual river game the bidirectional link between the board and models provides participants with visualizations of their actions at the same location as where they made them third for a serious game in general being able to experiment with interventions is a valuable way of triggering learning becu et al 2017 ferrero et al 2018 multiplayer serious games that focus on managing a spatial area generally include interventions as binary options that can be turned on or off for defined locations carson et al 2018 craven et al 2017 rusca et al 2012 this can include a few options of the same interventions at the same locations such as small medium and large versions onencan et al 2016 savic et al 2016 valkering et al 2013 in the virtual river game participants determine the location direction shape and size of interventions by replacing tiles on the board while following rules defined outside the software therefore the interface increases the experimentation potential by including the design of interventions rather than choosing predefined intervention options in serious games similar functionality is found in tile based computer games e g becu et al 2017 chew et al 2013 but not in combination with tangible tiles this type of functionality is also found in planning support systems in which participants can draw and apply interventions on a tabletop surface see e g leskens et al 2014 vonk and ligtenberg 2010 to summarize the hybrid interface combines the strengths of board games accessibility tangibility with the power and flexibility of computer games modeling visualizations which in combination are beneficial to supporting stakeholder participation and improving social learning 5 3 reflection and limitations in all sessions the participants focused mainly on increasing the flood safety score in the early game rounds and only focused on the biodiversity score during later rounds this approach is consistent with current dutch river management practice in which improving flood safety is the primary objective see e g deltaprogramma 2019 however some choices in the virtual river game s design and the choice of participants invited to the sessions may have influenced the participants focus on improving their flood safety scores although no distinction between primary and secondary objectives is made in the game the in game interventions are based on interventions implemented in reality that do have the improvement of flood safety as the primary objective the sessions starting conditions may have further nudged participants to first focus on the flood safety indicator as its initial score 29 was lower than the biodiversity score 47 in addition some flood safety specialists were observed to approach the game as if they were initially in their real world roles besides these possible influences on the results the study has two other limitations in general a serious game aims to provide participants with a safe space to experiment to take on another role and to defend positions that they may not take in reality de caluwé et al 2012 geurts et al 2007 mayer 2009 the results of this study suggest that the game provides this sense of safety as participants experimented with interventions to pursue the objectives of their assigned role in particular domain experts were observed to propose and experiment with interventions that ran counter to objectives that they would normally pursue in their real world roles however the sessions were organized outside of a real world policy making setting therefore the first limitation of the study is that we do not know if the game would also establish the same sense of safety in experimenting with interventions and defending positions of other stakeholder roles when played in the context of a real world river project this needs to be explored in future research before the game is used in a real river project the roles in the game must be reviewed in particular based on the results of this study adding an additional role that actively defends the interests of the agricultural sector must be considered this role was initially left out to limit the game s complexity in order not to increase the game s complexity an alternative could be to consider the current game as an introductory level and add a second game level with additional objectives indicators roles and interventions the second limitation lies in the limited number of sessions and participants conducting studies on serious games is time consuming both for the participants and for the researchers as a result the number of expert and non expert participants was too low to be able to draw statistically significant conclusions consequently we looked at these two groups separately and have not made comparisons between the learning outcomes of experts and non experts however despite the limitations the results indicate that the game may well be useful as a tool to help support stakeholder participation in dutch river management 6 conclusion in this paper we presented the design of the virtual river game and investigated its potential value as a participatory tool in dutch river management based on the results of five sessions we showed that the game was successful in 1 facilitating domain experts and non experts to collaboratively experiment with river interventions that are commonly applied in the netherlands and to elicit discussions similar to those that occur in real world river projects and 2 establishing social learning outcomes as both experts and non experts indicated that they had learned by playing the game through self reporting non experts indicated that most gained insights into the game s learning objectives and emphasized their learning about the functioning of the river system and about river interventions and their inevitable trade offs experts in turn reported more mixed results regarding the learning objectives and emphasized learning about river interventions and their inevitable trade offs as well as about the views and perspectives of other participants as a participatory tool we conclude that the substantial value of the game lies in introducing non expert stakeholders to dutch river management funding this research is part of the research program rivercare towards self sustaining multifunctional rivers supported by the netherlands organization for scientific research nwo and partly funded by the ministry of economic affairs under grant number p12 14 perspective program declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix table a1 land use options in the game and the translation to specific roughness classes scholten and stout 2013 and ecotopes van der molen et al 2003 including their fractional surface area in a tile for example the tile representing the main channel that includes a longitudinal training dam is assigned roughness class 102 and has a fractional surface area of 0 6 in the ecotope labeled as deep main channel rzd in the dutch ecotope system of large water bodies and 0 4 of the surface area is taken by the ecotope for medium depth side channel rnm table a1 land use of hexagon roughness class of hexagon dominant ecotope in hexagon subdominant ecotope in hexagon fraction dominant ecotope fraction subdominant ecotope elevation level 0 on the game board main channel main channel with groins 102 rzd ii 2 0 75 0 25 main channel with ltd 102 rzd rnm 0 6 0 4 elevation level 1 secondary channel secondary channel 105 rnm rnm 1 0 elevation level 2 lower floodplain built up land floodplain 114 ua 2 ua 2 1 0 production meadow floodplain 1201 ug 2 ug 2 1 0 natural grassland floodplain 1202 ug 1 ug 1 1 0 reed floodplain 1215 iv 9 iv 9 1 0 shrubs floodplain 1231 ub 2 ub 2 1 0 forest floodplain 1245 ub 1 ub 1 1 0 mixtype 70 natural grassland 30 forest floodplain 1202 1245 ug 1 ub 1 0 7 0 3 elevation level 3 higher floodplain built up land natural levee 114 oa 2 oa 2 1 0 production meadow natural levee 1201 og 2 og 2 1 0 natural grassland natural levee 1202 og 1 og 1 1 0 reed natural levee 1215 hm 1 hm 1 1 0 shrubs natural levee 1231 ob 2 ob 2 1 0 forest natural levee 1245 ob 1 ob 1 1 0 mixtype 70 natural grassland 30 forest natural levee 1202 1245 og 1 ob 1 0 7 0 3 elevation level 4 dikes dike 1201 reinforced dike 1201 the groins and ltds could only be included in delft3d fm through the bmi by manipulating the dem to include their shapes in the elevation as a result the main channel is used as a roughness class for both dikes are not considered part of the area evaluated for biodiversity and thus are not assigned an ecotope appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 multimedia component 4 multimedia component 4 multimedia component 5 multimedia component 5 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104855 
25942,serious games are increasingly used as tools to facilitate stakeholder participation and stimulate social learning in environmental management we present the virtual river game that aims to support stakeholders in collaboratively exploring the complexity of a changed river management paradigm in the netherlands the game uses a novel hybrid interface design that features a bidirectional coupling of a physical game board to computer models we ran five game sessions involving both domain experts and non experts to assess the game s value as a participatory tool the results show that the game was effective in enabling participants to collaboratively experiment with various river interventions and in stimulating social learning as a participatory tool the game appears to be valuable to introduce non expert stakeholders to dutch river management we further discuss how the hybrid interface combines qualities usually found in board and computer games that are beneficial in engaging stakeholders and stimulating learning keywords serious gaming social learning water management stakeholder participation participatory decision making tangible interaction software availability software name virtual river developers robert jan den haan fedor baart year first official release 2020 hardware requirements pc game table drawings available including a webcam touchscreen monitor and projector a test version can be run on just a pc system requirements windows or linux delft3d fm suite 2019 01 1 5 1 41875 program language python program size 1 7 gb availability https github com erjeetje virtual river prototype license gpl 3 0 documentation readme in the github repository 1 introduction stakeholder involvement and participatory approaches are increasingly important in environmental decision making pahl wostl et al 2008 reed 2008 voinov et al 2016 a recent shift from sectoral towards more integrated natural resources management has made stakeholder participation essential to the pursuit of cross disciplinary objectives berkes 2009 pahl wostl 2007 reed 2008 at the same time stakeholder participation is recognized as an effective way to improve the quality of and acceptance in decision making cundill and rodela 2012 reed 2008 voinov et al 2016 one method that is receiving increasing attention to facilitate stakeholder participation in environmental management is serious gaming aubert et al 2018 rusca et al 2012 voinov et al 2016 serious games are generally referred to as games that have a primary purpose other than mere entertainment michael and chen 2005 susi et al 2007 in the context of environmental management and participation serious games are defined by mayer 2009 as experi m ent i al rule based interactive environments where players learn by taking actions and by experiencing their effects through feedback mechanisms that are deliberately built into and around the game to provide feedback on actions such serious games include a simplified representation of reality in terms of both the environmental system and its stakeholders harteveld 2011 redpath et al 2018 rodela et al 2019 in this way serious games enable stakeholders to explore environmental challenges interventions and the effects of such interventions in an environment in which it is safe to experiment furthermore serious games enable stakeholders to experience the strategic interactions between stakeholders by explicitly including interaction rules and assuming stakeholder roles in the game therefore serious games facilitate stakeholders learning about both the physical technical and the inherent socio political complexities bekebrede 2010 de caluwé et al 2012 geurts et al 2007 mayer 2009 the lessons learned while playing serious games can be both relevant and transferable to real world decision making geurts et al 2007 mayer 2009 one promising opportunity for serious games in the context of stakeholder participation in environmental management is their use as what rodela et al 2019 categorize as learning based interventions such games are developed to engage stakeholders in dialogue and activity in order to contribute to what is commonly referred to as social learning changes in understanding through interaction in collaborative and participatory processes that go beyond the individual see e g cundill and rodela 2012 muro and jeffrey 2008 reed et al 2010 rodela 2011 as learning based interventions games are therefore developed under the assumptions that these 1 provide stakeholders with participatory environments that facilitate the stakeholder interactions and collaborative experimentation that are essential to establish social learning and 2 contribute to individuals or groups experiencing a change in understanding as a result of the game s collaborative activity ampatzidou et al 2018 flood et al 2018 medema et al 2016 in the literature social learning is usually operationalized as 1 cognitive learning acquiring new or restructuring existing knowledge 2 normative learning changing viewpoints values or paradigms and 3 relational learning increasing the understanding of the mind set and perspectives of other stakeholders as well as fostering the ability to cooperate among stakeholders baird et al 2014 ensor and harvey 2015 in the context of environmental management there are various examples of serious games as learning based interventions see e g reviews by aubert et al 2018 den haan and van der voort 2018 flood et al 2018 for games on water management sustainability and climate change respectively as one of a few recent examples from a far longer list craven et al 2017 developed simbasin to bring stakeholders together with the aim of developing a shared understanding and sense of urgency around the management of the magdalena cauca river basin in colombia they showed that the game was successful in creating an open discussion space to bring stakeholders and scientists together the sustainable delta game valkering et al 2013 van der wal et al 2016 challenges stakeholders to develop collective strategies to manage a fictional stretch of a dutch river and aims to help them to learn about the complex interactions between river management climate change and changes in society results from twelve sessions showed that playing the game led to the convergence of the players perspectives van der wal et al 2016 becu et al 2017 developed littosim to enable social learning among local authority managers on oléron island in france about prevention measures to reduce the risk of coastal flooding they showed that littosim facilitated stakeholder experimentation with and learning about risk prevention measures in relation to possible flood events van hardeveld et al 2019 developed the re peat game to explore collaborative management strategies to help reduce soil subsidence in the netherlands results from ten sessions showed that re peat improved cooperation among peatland stakeholders increased their understanding of the problems and led them to possible strategies for reducing soil subsidence recent changes in dutch river management provide a valuable case study for designing and evaluating a serious game as a participatory tool traditionally river management has been dominated by dike strengthening and was therefore within the field of hydraulic engineering a new management paradigm involving spatial measures and multifunctional design brings in many other stakeholders that are traditionally not usually involved with river management the challenge of collaboration between these diverse groups of stakeholders in what is still a rather technical field demands a shared understanding of the physical system in this paper we present the virtual river game a serious game to collaboratively explore river management complexity in the netherlands the game was played in five sessions involving both domain experts and non experts to assess its value as a participatory tool it was guided by the research questions 1 to what extent does the game facilitate stakeholders collaboratively exploring and experimenting with river interventions and 2 to what extent does playing the game lead to social learning outcomes the paper is structured as follows the next section discusses the game and particularly the fact that it facilitates stakeholders to apply river interventions on a game board that has a bidirectional link to computer models section 3 presents the assessment approach of the game including an overview of the sessions and the methods to collect and analyze the data in order to address the research questions section 4 reports on the factual game output of the sessions observations of the in game discussions and the participants self reported learning outcomes the paper ends with a discussion of the value of the game as a participatory tool as well as on how the game s hybrid interface combines those qualities found in board and computer games that benefit and support stakeholder participation and social learning processes 2 game description 2 1 background and aim to protect the deltaic floodplains from flooding the traditional approach in the netherlands has been to build and reinforce dikes however near flood events in the 1990s shifted the approach of dutch river management towards applying so called spatial measures that aim to create more space for rivers to safely discharge water rijke et al 2012 warner et al 2012 for example by digging side channels lowering floodplains and moving back dikes see e g berends et al 2019 straatsma et al 2019 van stokkom et al 2005 in addition to lowering peak water levels such spatial measures also aimed to restore the local ecology fliervoet et al 2013 klijn et al 2013 straatsma et al 2017 this paradigm shift while still retaining flood safety as its primary focus led to an increasingly more integrated river management approach as a result it attracted new stakeholders to river management verbrugge et al 2019 and emphasized the importance of stakeholder participation in decision making edelenbos et al 2017 fliervoet et al 2013 zevenbergen et al 2015 in the context of the new dutch river management paradigm we set out to develop the virtual river game as a tool to increase and support stakeholder participation the game can beneficially be played at an early stage of a project as an icebreaker activity but that is disconnected from the project s actual decision making the game enables stakeholders to collaboratively experiment with river interventions in order to increase their understanding of dutch river management including both the physical system and the effects and trade offs of specific interventions and the perspectives and interests of other stakeholders in relation to such interventions during the game s design process we found that some stakeholders particularly those introduced to river management as a result of the paradigm change view the hydrodynamic models central to dutch river management decision making as mysterious black boxes den haan et al 2018 therefore to support stakeholder participation we set out to develop the virtual river game to enable stakeholders regardless of background and expertise to work with a hydrodynamic model that is widely used in practice to that end we developed a hybrid interface based on tangible interaction linking physical forms to digital information hornecker and buur 2006 ishii 2008 the interface features a bidirectional coupling of a physical game board to a hydrodynamic ecological and cost model an impression of the virtual river game and its interface is shown in fig 1 the virtual river game uses hexagonal tiles to represent a typical stretch of a dutch river that includes a main channel and floodplains and dikes on both sides of the channel the changing of the tiles on the game board provides input to the models while the models output is visualized both on the game board through projection and in a game engine shown on a touchscreen monitor an overview of the game s software and hardware components is shown in fig 2 in the following sections we first introduce the models and their integration in the virtual river game followed by a description of the game itself 2 2 monodisciplinary models integrated in the game 2 2 1 delft3d flexible mesh hydrodynamic model to model water flow and water levels in the game area we incorporated the delft3d flexible mesh fm hydrodynamic model to compute the hydrodynamic response to system change berends et al 2019 kernkamp et al 2011 we use a rectangular numerical grid of cell size 20 m by 20 m initial bed levels and chézy friction coefficients are determined at the start of a game and are updated as the game progresses the boundary conditions are given by an upstream constant discharge and a downstream constant water level we use default parameter settings si table 1 water levels and flow velocities are the model outputs of interest 2 2 2 biosafe biodiversity model to model the potential biodiversity of the game area we integrated the biosafe model as developed by lenders et al 2001 de nooij et al 2004 and straatsma et al 2017 the model calculates biodiversity scores based on the potential occurrence of protected and endangered species in each ecotope the laws and regulations protecting the species and the surface area distribution of the main channel and floodplain ecotopes between the dikes ecotopes are defined as spatial landscape units that are homogeneous as to vegetation structure succession stage and the main abiotic factors that are relevant to plant growth klijn and de haes 1994 as input the model needs the ecotopes and their surface areas as output the model provides potential biodiversity scores for seven taxonomic groups mammals birds herpetofauna fish butterflies dragonflies and damselflies and higher plants 2 2 3 vrcost cost model to model the costs of interventions we created a model in python by translating unit prices for costs for interventions in dutch river management straatsma et al 2019 to interventions in the game unit prices relate to costs per volume area or length hexagon cross section for example a volume of soil may have to be excavated to construct a side channel in the river s floodplain the model distinguishes four cost categories excavations construction of hydraulic structures land use changes and land acquisition as input the model needs the elevation and land use change as output the model calculates the total costs for changes on the board as well as the costs per type 2 3 interface design model integration for the virtual river game s hybrid interface we designed and built a physical table fig 1 that includes the game board plus an off the shelf webcam touchscreen monitor and projector the game board consists of 143 hexagonal tiles representing a stretch of river the tabletop has an open aluminum mesh into which the tiles slot leaving the bottom side of each tile visible to the webcam each tile contains information on terrain height and land use which can be independently varied table 1 shows the five elevation levels and twelve land use types and their potential combinations we chose the five elevation levels as a representation of the varying elevations found in dutch rivers for the land use types we took inspiration from the classification of vegetation types used by the dutch public works authority rijkswaterstaat 2012 markers on the bottom of the tiles enable the conversion of the physical board to a digital board defining each tile s elevation and land use in the game s software based on a picture taken by the webcam throughout the game the system can be updated triggering the software to process the latest board state in the following subsection we explain the additional processing steps in the software needed to link the board to the models 2 3 1 model interface the game software converts the digital board to a digital elevation model dem and a roughness distribution as input to delft3d fm the dem is created by an inverse distance interpolation power of 2 of the terrain height at the center of the three nearest tiles to a regular grid indexed to the computational grid used in delft3d fm the roughness classes are based on a lookup table from land use to roughness class table a1 subsequently the software calculates the hydraulic roughness by retrieving the water levels of locations from delft3d fm and applying the vegetation friction model of klopstra et al 1996 the software sets the elevation and roughness coefficients and also retrieves water levels and flow velocity from delft3d fm through the basic model interface bmi peckham et al 2013 using the python bmi baart 2017 for biosafe the software converts the digital board to an ecotope distribution through a lookup table that links terrain height and land use to ecotopes table a1 a subset of 15 ecotopes out of the 82 fluvial ecotopes defined in the dutch ecotope classification van der molen et al 2003 are included composite land use such as main channel with a longitudinal training dam are split up into their pure ecotope classes in order to calculate the surface areas of all ecotopes the python version of biosafe straatsma et al 2017 is integrated in the game s software to enable sending of the ecotopes and retrieving the potential biodiversity score for the taxonomic groups for vrcost the software compares the previous and new board states to detect changes in elevation and land use changes on the board are sent directly to the model and the costs of changes including their breakdown into the four categories are retrieved 2 3 2 model feedback the virtual river game offers model output in two locations on the game board itself and on the touchscreen on the physical game board the software visualizes the dem water flow patterns from delft3d fm and hydraulic roughness coefficients by converting these into colormaps which are subsequently projected on the board the flow pattern is visualized as diffusive paint blobs that follow the flow lines the hydrodynamic effects of changing tiles on the board are therefore visualized based on the model s output and projected on the same location as which changes take place through these choices we aimed at making the hydrodynamic model more accessible and transparent by providing a tangible easy to use interface and by enabling players to link their actions to the model s output on the touchscreen the game shows the tygron geodesign platform a 3d spatial planning modeling tool that is also used as a virtual game engine for those serious games that have a spatial development component bekebrede et al 2015 van hardeveld et al 2019 warmerdam et al 2006 the game interfaces with the tygron game engine through its api tygron 2018 the inclusion of the engine serves two purposes first the dem and land use types of the digital board are converted to a virtual game world that matches the game board for example the engine shows trees for tiles on the game board that represent the forest land use second interactive panels in the engine provide players with the output from and information about all three models we deliberately developed the game in such a way that players are able to fully control the interface without needing the game s facilitator players change tiles on the board switch visualizations through the graphical user interface gui displayed on the touchscreen and inspect the virtual world as well as open panels in the game engine updating the board state which involves processing changes on the board running the models and updating both the visualizations and information in the tygron engine only takes between 15 and 30 s depending on the time needed for water levels in the hydrodynamic model to stabilize the update times are based on a dedicated portable computer utilizing an amd ryzen x3700 desktop processor to run the game and models locally 2 4 virtual river game in the virtual river game players are challenged to manage a 3500 m long deltaic stretch of river incorporating a navigable main channel that has floodplains and dikes on both sides the game scenario reflects a high river discharge as result of which the floodplains are inundated three teams each consisting of one or two players play the roles of flood manager nature manager and financial manager collectively the players are given a budget and are tasked to improve the flood safety status and ecological value of the area each team is given additional objectives as well as special abilities to block implementations of interventions based on real world stakeholders legislation and european union development targets the flood manager mirrors the dutch public works authority which is responsible for ensuring and maintaining adequate flood safety levels the flood manager can block interventions if these decrease flood safety levels or if land use is changed to a type leading to high hydraulic roughness reed and brushwood shrubs forest and mixtype which reflects the legislative power of the public works authority the nature manager represents larger nature management organizations such as the dutch state forestry agency which own and manage a large percentage of the dutch floodplains and aim to develop their ecological value the nature manager can block interventions if those decrease the area s ecological value to reflect the common view that nature organizations hold less power than for example the dutch public works authority in real world decision making this ability can only be used once during the game the financial manager represents a combination of the dutch national government which allocates budget for river projects and regional governments which are the commissioners of river projects and responsible for managing the floodplains as natura 2000 areas under the eu birds and habitats directives the financial manager can block interventions if those would take more than half of the initial budget or if these require expensive land acquisition buildings and agricultural land which reflects the interest of governments to pursue cost effective win win solutions if players are part of any of these or similar organizations they are assigned a role that is different from their regular role to let them experience river management from another point of view each game consists of a maximum of four rounds during each round players apply one of six interventions side channel construction floodplain lowering grading floodplain smoothing changing the floodplain vegetation to lower roughness replacement of groins alternatively termed wing dikes or spur dikes with longitudinal training dams dike relocation or dike reinforcement players first discuss and agree which intervention they wish to apply they subsequently implement the chosen intervention by changing tiles on the game board while following the intervention s rules during this implementation phase players can continually change tiles and collectively evaluate the intervention s effects before agreeing on a final implementation 2 4 1 in game scoring in the virtual river game we included performance indicators on flood safety biodiversity and budget therefore interventions are evaluated on hydrodynamics ecology and costs as in the delft3d fm biosafe and vrcost models each indicator has its own progress bar across a 0 100 score range minimum 50 good 65 and excellent 80 scores are provided for both the flood safety and biodiversity indicators we determined these scores as a balance between the game s representation of reality and its playability see harteveld 2011 that 1 reflects dutch river management practice in terms of performance and 2 corresponds to the difficulty of attaining good scores in the game players can click each indicator s progress bar on the touchscreen to see graphics on the effects of interventions on that indicator in addition separate information panels are available to players where they can see simplified scores after each tile change to anticipate the effects of interventions the flood safety score is based on the water levels along the river axis in comparison to the crest height of the dike at each tile location on the board the water levels can be at or above just below or well below the dike s crest height corresponding to that location being considered unsafe moderately safe and substantially safe respectively unsafe moderately safe and substantially safe locations each contribute to the flood safety score as zero half and full score respectively consequently the overall flood safety score is 0 when all dike locations are considered unsafe and 100 when all dike locations are considered substantially safe flood safety is visualized in the indicator panel as a longitudinal profile of the stretch of river that includes initial and current water levels and as a top view with each dike tile colored red yellow or green representing unsafe moderately safe and substantially safe respectively the biodiversity score is based on the potential biodiversity the sum of the scores on the seven taxonomic groups over the whole river reach this sum is converted to a percentage in the 0 100 range for simplicity we determined the 0 and 100 scores for biodiversity by running a monte carlo simulation to find the board layouts that result in the lowest and highest potential biodiversity scores respectively that can be achieved in the game in the indicator panel the potential biodiversity of the taxonomic groups the sum and the corresponding score are provided furthermore a bar graph shows the scores on the seven taxonomic groups for both the initial and the current game board a second bar graph shows the change for each taxonomic group expressed as a percentage the budget score reflects the remaining budget that players have available as a percentage of the initial budget that they received at the start of a game the initial budget is therefore equivalent to a 100 score spending the whole budget results in a 0 score and spending more than the budget results in a negative score a graph shows the budget spent per round as well as the remaining budget in the budget indicator panel expressed both in euros and a percentage of the initial budget in a second graph the breakdown of the costs incurred per round is shown as stacked bars 2 4 2 in game objectives teams are scored on each indicator separately throughout the game as a collective objective the players have to achieve the minimum score 50 for both flood safety and biodiversity in the four rounds while preferably staying within the budget collectively the teams may decide not to play all rounds if they reach the collective objective before the fourth round additionally each team is given one main and two secondary objectives the teams receive the instruction that in order to win the game they have to reach both the collective objective and their role specific main objective the role specific secondary objectives are presented as bonus points that they can earn the flood manager is given the main objective to achieve a good flood safety score 65 the secondary objectives are to achieve an excellent flood safety score 80 and ending the game without any unsafe dike locations the nature manager has the main objective of achieving a the good biodiversity score 65 and the secondary objectives of achieving an excellent biodiversity score 80 and to end the game with five or more forest locations within the floodplains the budget manager is tasked to limit spending to the collective budget the secondary objectives are to achieve good scores for flood safety and biodiversity 65 the collective objective is known to all players each team is given its main and secondary objectives at the start of the game by blindly drawing one of four role specific objective cards players do not know that these four cards all list the same main and secondary objectives for their team players are not told whether or not to share their team s objectives with other players 3 virtual river game evaluation we developed the virtual river game to support stakeholder participation in the new dutch river management paradigm in this study we set out to assess the potential of the game as a participatory tool guided by the research questions 1 to what extent does the game facilitate stakeholders collaboratively exploring and experimenting with river interventions and 2 to what extent does playing the game lead to social learning outcomes the research questions address what aubert et al 2019 refer to as the process oriented how are these outcomes achieved and variance oriented what outcomes are achieved assessment of serious games the first research question focused on the game itself to evaluate its ability to engage stakeholders in dialogue and activity a prerequisite to stimulating social learning following the scope of the game and its interface design we were particularly interested in evaluating the extent to which the game facilitates both domain experts and non experts to engage in collaborative experimentation with river interventions we considered the game to be successful as a participatory tool when participants regardless of their background and expertise 1 developed a shared understanding of the problem 2 developed a collaborative strategy to address that problem 3 engaged in discussions on how interventions affect indicators and role objectives and 4 applied and tested various implementations of interventions the second research question focused on evaluating to what extent playing the game led to social learning by individual participants following the game s scope we focused on learning outcomes related to cognitive and relational learning baird et al 2014 ensor and harvey 2015 cognitive learning outcomes that were assessed included gaining an improved understanding of 1 the functioning of the river system 2 the effects of interventions and their trade offs 3 how hydrodynamic models work and are used in decision making and 4 the conflicts and opportunities for cooperation between the various stakeholder roles relational learning outcomes that were assessed relate to gaining an improved understanding of the mind sets and perspectives of other participants we considered the game to be successful when both domain experts and non experts achieve cognitive or relational learning outcomes or both as experts bring their knowledge and experience to the game we expected that they would achieve fewer cognitive learning outcomes than non experts to address the research questions we organized five sessions playing the virtual river game in the following subsection we describe the setup of the sessions and their participants in subsection 3 2 we describe the data collection methods and the data analysis used to address both research questions 3 1 sessions and participants the description of a session playing the virtual river game is provided in box 1 we invited both domain experts professionals working in dutch river management and non experts participants without river management expertise to the sessions none of the sessions were linked to a real world river project this made it difficult to engage with non experts who are also real world stakeholders in dutch river management therefore we invited design researchers and game designers to represent non expert stakeholders we organized two sessions that included only expert participants and three sessions of experts and non experts table 2 each session had between four and six participants in a total of 26 participants the length of each session including the explanations and debriefings was around 3 h the actual length of gameplay ranged between 78 and 109 min all sessions had the same experienced facilitator and a trained observer we prepared an initial board fig 3 as a scenario that we used in every session the scenario reflected a stretch of river with a hydrodynamic bottleneck formed by narrow floodplains higher floodplain terrain and vegetation that causes high hydraulic roughness resulting in unsafe water levels upstream from the bottleneck after the second session we changed one parameter of the scenario by lowering the initial budget by 30 to 17 5 million because the initial budget was found to be too high although reducing the budget between sessions was undesirable we aimed to stimulate more discussions that also addressed the cost effectiveness of interventions one of the main criteria in real world decision making we chose the 30 reduction as this aligned with the budget spent during the first session 3 2 data collection and analysis we applied a multi method approach to collecting and analyzing both quantitative and qualitative data our approach consisted of 1 a pre game questionnaire 2 in game data logging 3 in game observations 4 a post game questionnaire and 5 a post game debriefing before starting the game the participants filled out a short questionnaire stating their age expertise experience related to river management and experience with serious gaming all so as to separate expert from non expert participants si table 2 during the game the participants decisions and performance for each update were stored in a log file in addition we observed the participants discussions during the game we used an observation recording form to document a timeline of each session by linking discussions to the three indicators as well as to intermediate and final decisions taken in the game si table 3 we used this timeline to analyze each session s outcomes e g interventions applied final layout score progression and processes e g discussions consideration of options contributions of participants in order to address the first research question directly after playing the game the participants completed a second questionnaire that focused on their overall impressions of the game and the insights gained by playing the game si table 4 in closed questions the participants were asked to self report their experiences by rating their agreement with statements on a 5 point likert scale that ranged from strongly agree to strongly disagree a common approach in game studies bekebrede et al 2018 keijser et al 2018 mayer et al 2013 in one open question the participants were asked to list a maximum of three main insights obtained by playing the game we categorized the open question answers in relation to cognitive and relational learning to determine what experts and non experts identify as the main insights from playing the game we used the data from the completed questionnaires to address the second research question one non expert participant did not complete the post game questionnaire and was therefore excluded from the results ntotal 25 for the questionnaire to conclude each session we conducted a debriefing to collectively reflect on the game activity the debriefing was set up along the lines proposed by kriz 2010 six phases to structurally reflect on the game activity to confirm that the discussions and considerations in the game reflect those encountered in practice we expanded the third phase of the debriefing which includes reflecting on the game s external validity van den hoogen et al 2014 in the two expert sessions following informed consent we recorded the debriefings to transcribe these for later analysis we used the debriefing data to further interpret the analyses to help address both research questions in addition we compared the data to address the two research questions with each other to strive for data triangulation 4 results 4 1 collaborative experimentation with river interventions rq1 the first research question was to what extent the game facilitates stakeholders to collaboratively exploring and experimenting with river interventions here we present the chosen interventions and indicator scores as the factual game output as well as the observations of the in game discussions about and experimentations with those interventions 4 1 1 interventions and scores the choice and implementations of interventions during the game sessions showed that good scores between 65 and 79 for both flood safety and biodiversity can be achieved in several ways fig 4 for flood safety only the participants in the fourth session 71 did not achieve an excellent score 80 or higher excellent scores for biodiversity were not achieved in any session in all sessions the participants first focused on increasing the flood safety scores generally spending most of the budget during the first two rounds increasing the biodiversity scores became the focus during the later rounds participants in the third and fifth sessions used their remaining budget to achieve good biodiversity scores between 65 and 79 but only after achieving satisfactory flood safety scores during all sessions participants applied the floodplain smoothing intervention to increase their biodiversity scores while at times also optimizing their flood safety scores during the second and third sessions participants also chose the side channel construction and replaced groins with longitudinal training dams to increase their biodiversity scores 4 1 2 strategies and discussions at the start of each session participants explored the game scenario by changing the board visualizations inspecting the information on the touchscreen and linking the information from these two sources initial discussions focused on establishing a shared understanding of the flood safety bottleneck and on how flood safety could be improved during the mixed sessions expert participants were observed taking prominent roles in these early discussions these experts supplemented the in game information by explaining what they saw as problems for example that the bottleneck caused higher water levels upstream and suggested what could be done to improve the indicator scores non experts initially followed their lead but then started to make their own suggestions as the games progressed during the sessions that included only professionals experts who regularly work with hydrodynamic models were similarly observed to take more prominent roles at the start of the game the discussions during these sessions moved more quickly from identifying the bottleneck to possible interventions than in the mixed sessions during these discussions flood safety specialists were observed to start the games as if they were in their real world roles even though they had not been assigned to the flood safety manager team game discussions during the early rounds did include how tackling the flood safety bottleneck could simultaneously improve biodiversity without the costs of interventions rising unacceptably however no change in general direction of these discussions was observed as a result of lowering the budget after the second session after establishing a shared understanding of the problem the discussions during all sessions focused on deciding which intervention to implement in the first round in no session did the participants develop a collaborative strategy at the start of the game during the sessions the participants were observed to plan ahead by discussing how the intervention they were implementing could be made more effective by applying a different intervention in the next round experts continued to supplement the in game information for example by explaining that changing land use to a type with high hydraulic roughness to increase biodiversity e g to a forest is best done at locations where there is a low flow velocity e g next to a dike to limit its effect on flood safety discussions during the later stages of the game featured more bargaining between the teams each seeking to pursue their main and secondary objectives for example discussions during the third and fifth sessions included teams agreeing on the choice of intervention in one round only if the other teams agreed beforehand on the intervention to be made in the subsequent round during these bargaining discussions participants started to share their team objectives only in the fifth session did the participants share their team objectives at the start of the game to establish their own common objectives guided mostly by one expert participant who advocated forgetting about the politics no notable differences in late game discussions were observed between the expert and mixed sessions non experts were as active in proposing implementations of interventions as experts were and their suggestions were also applied on the board in the three sessions after reducing the budget the costs of interventions were emphasized more during the late game discussions in particular by the budget manager teams during the debriefings participants confirmed that they did not have a collective strategy for the game but that it felt natural to them to plan ahead while testing implementations of interventions additionally participants in the second third and fifth sessions explicitly indicated that it seemed logical to them to start with the most drastic intervention and to subsequently optimize from there before asking about the connection of the game activity to reality during the third session expert session the following discussion took place expert a and that the game more or less reflects how things work in reality i am working together with expert d on an area in which we have the same type of discussions expert b i really liked that you could quickly update and inspect the results like how does this work out how far are we now that is very useful the risk is that you start micro managing the environment but yeah expert c but that is also realistic expert b yes true expert d the game feels very realistic because the discussions we were having about the board where we were working with those are the same discussions you have in practice and the advantage is that you can quickly update the situation and inspect the effects so you really get a feel of what the interventions do the discussion illustrates that these experts found that using the board to apply interventions felt sufficiently realistic and elicited discussions similar to those occurring in practice experts in other sessions provided feedback that supported these findings during the debriefings and in written statements made in the post game questionnaire experts in the first third and fifth sessions did mention that they missed a role that directly represents the interest of agriculture in the game as a result although facing a financial penalty for the compulsory purchase of land they mentioned that they could convert agricultural land into other land use types without the emotional discussions associated with the compulsory purchase of land by the government that would occur in real life 4 1 3 experimentation approach in all sessions participants used the opportunity to experiment with interventions in sessions 1 to 4 the participants applied and evaluated between 17 and 20 implementations of interventions while playing the game fig 4 the number of applied interventions in the fifth session was notably lower which was probably caused by internet problems during the session that resulted in a loss of time to play the game to help propose interventions participants used the board as a map to suggest interventions by hand gestures and used the information both from the visualizations as well as on the touchscreen to formulate arguments participants experimented with interventions both to test implementations at several locations e g constructing a side channel on the other side of the main channel comparison and to improve the implementation at a chosen location optimization especially the floodplain smoothing intervention led to experimentation with participants in the second and third sessions trying eight and eleven implementations respectively in these experimentations non experts were observed to realize that lowering the hydraulic roughness from locations with higher water flow velocity is effective in increasing the flood safety score furthermore participants were observed to realize through experimentation that changing a production meadow intensively managed grassland to a more natural setting less managed is most effective at increasing biodiversity 4 2 experience and social learning outcomes rq2 the second research question was to what extent does playing the game lead to social learning outcomes here we describe the participants overall impression on the game and their self reported learning outcomes with respect to both cognitive and relational learning the questionnaire results indicate that the game was well received by both domain experts and non experts fig 5 most participants indicated that the game s goal was clear and that they enjoyed playing the game during the debriefings participants also frequently mentioned that they regarded the game as a fun activity regarding the learning outcomes the questionnaire results indicate that participants gained insights into both cognitive and relational learning fig 6 compared to experts an equal or higher percentage of the non expert participants gave answers to the statements that showed strong agreement below we discuss the results for experts and non experts separately fig 6 shows that non experts mostly agreed or strongly agreed with the twelve statements non experts agreed most with statements 11 and 12 μ 4 40 and 4 60 respectively related to relational learning in relation to cognitive learning non experts agreed most with statements 1 and 10 both μ 4 20 only on statement 4 μ 3 40 about the costs of interventions did two non experts give answers that showed strong disagreement these participants explained that they had difficulty interpreting information about costs during the game in the open question non experts most frequently mentioned insights related to the functioning of the river system 10 and interventions and trade offs 6 categories si table 5 both associated with cognitive learning most of these insights were broadly formulated such as the relation between flow velocity and flood safety and trade offs between decisions experts gave more mixed responses to the statements than non experts experts agreed most strongly with statement 5 μ 3 87 related to the trade offs between interventions they also rated statements 11 and 12 μ 3 67 and 3 80 respectively on relational learning positively with most experts only agreeing but a few strongly agreeing statements 6 and 7 μ 2 67 and 2 93 respectively on hydrodynamic models were rated poorly with more experts disagreeing than agreeing with both statements this was not an unexpected result as experts regularly work with these models anyway in the open question experts most frequently mentioned insights related to interventions and trade offs 8 and player perspectives 7 si table 5 associated with cognitive learning and relational learning respectively on the interventions and trade offs insights were formulated more specifically than by non experts such as insight into the ratio of costs of different interventions and more insights into the costs of interventions in relation to their effectiveness experts indicated in both the questionnaires and the debriefings that they gained such insights as a result of playing a role different from their day to day role during the debriefings participants collectively reflected on what helped them to obtain their main insights and mentioned the game s feedback all sessions the shared exploration and experimentation with interventions sessions 2 3 4 and 5 explanations from other participants sessions 1 2 4 and 5 and finding it easier to interpret information from the game board than from a screen session 5 in all three mixed sessions non experts explicitly pointed out that the experts had helped them to understand the problem as well as to predict the consequences of interventions experts in turn indicated that collaborative experimentation with interventions in combination with the game s feedback helped them to explain river management principles to non experts 5 discussion in this section we first reflect on the potential value of the virtual river game as a participatory tool based on the study s results next we discuss the design of the game and its interface we end the section by reflecting on the study and outlining its limitations 5 1 game as a participatory tool the results of the sessions indicate that the game was successful at engaging both domain experts and non experts in collaboratively exploring and experimenting with river interventions in all sessions the participants developed a shared understanding of the problem to address the problem they applied and tested various implementations of interventions although the participants did not develop a collective strategy from the start they did collaboratively plan ahead while applying interventions discussing how future interventions could further improve the indicator scores and contribute to achieving the various objectives experts and non experts were both active in the games discussions with experts taking more of a leading role especially at the start of the games furthermore experts noted that the discussions and considerations that they had during the game reflect well those found in real world projects the results further indicate that the virtual river game stimulated social learning for both experts n 15 and non experts n 10 considering the game s scope and the operationalization of social learning we looked specifically at cognitive learning acquiring new or restructuring existing knowledge and relational learning increasing the understanding of the mind set and perspectives of others baird et al 2014 ensor and harvey 2015 with only a few exceptions non experts rated all statements in relation to the game s learning outcomes positively according to their comments the main insights that non experts emphasized were almost exclusively associated with cognitive learning however experts rated the statements in a more mixed way with some statements rated positively but some statements associated with cognitive learning neutrally to negatively we expected that experts would report less cognitive learning given their background knowledge the statements on relational learning were rated positively by experts which was also reflected in their reported main insights taken together we conclude that both domain experts and non experts learned by playing the game however the extent to which they learned and emphasis they placed on it varied the game appears to be particularly valuable as a participatory tool to introduce non expert stakeholders to river management in the netherlands keijser et al 2018 reported similar results from their game on marine spatial planning showing that their game worked well as an introductory game to engage non experts on the topic on learning more generally our results are in line with evaluations of other serious games see e g reviews by aubert et al 2018 den haan and van der voort 2018 flood et al 2018 which have shown that serious games are an effective way of increasing the understanding of physical systems becu et al 2017 carson et al 2018 keijser et al 2018 of raising awareness onencan and van de walle 2018 stefanska et al 2011 van pelt et al 2015 and of increasing the understanding of alternative views and perspectives douven et al 2014 jean et al 2018 souchère et al 2010 combining the results on collaborative exploration and learning this study offers further evidence that serious games are effective ways of improving social learning hofstede et al 2010 medema et al 2016 savic et al 2016 5 2 game and interface design serious games are increasingly explored as learning based interventions in environmental management aubert et al 2019 flood et al 2018 rodela et al 2019 in terms of the design of such games this work contributes to the growing body of literature by proposing a new hybrid interface design concept the bidirectional coupling of a physical game board to computer models whereas other serious games on environmental management tend to use simplified custom computer models that are location specific e g craven et al 2017 valkering et al 2013 our focus was on creating a simplified yet realistic 3d representation of a typical stretch of a dutch river in combination with models currently used in practice designing serious games always requires some form of simplification to strike a good balance between an accurate representation of reality the playability of reality and the meaning of the game harteveld 2011 in our approach the biggest simplification of the physical reality was not in the models but in the fact that the board uses a fixed hexagonal grid we applied this simplification mainly to increase playability the fixed number and shape of the tiles reduces the level of detail in relation to a real stretch of river and provides a manageable structure by limiting the number of possible arrangements in the game the sessions showed that the game board design facilitated participants working together and as an interface enabled especially non experts to work with models currently used in practice moreover experts determined that the game triggered discussions that also occur in practice therefore simplifying a river stretch to a hexagonal grid proved to be a suitable approach to balancing a good representation of reality with playability increasing the level of detail in the game could be further explored but is not necessary to facilitate the collaborative exploration of and experimentation with river interventions in addition to serving its specific purpose the hybrid interface offers qualities that we see as beneficial to facilitating stakeholder participation in environmental management and to improving social learning serious games have been developed as board games hertzog et al 2014 keijser et al 2018 speelman et al 2014 as digital games ayadi et al 2014 carson et al 2018 craven et al 2017 hill et al 2014 and as hybrid games combining elements of board and digital games cleland et al 2012 magnuszewski et al 2018 valkering et al 2013 van der wal et al 2016 such games have a common feature that participants are provided with an experimentation environment in which they can collaboratively and safely explore both the techno physical and socio political complexity of an environmental system with the hybrid interface design of the virtual river game we combined three qualities that are separately found in board and computer games these qualities are described below first the hybrid interface provides participants with a physical tangible object to engage with in shared exploration through the game board and touchscreen control of the game is shared and not limited to a facilitator or one participant controlling an input device participants engage in discussions by moving game tiles or pointing to locations on the game board making their views explicit to other participants stanton et al 2001 suzuki and kato 1995 second as an interface the game board appeared to lower the threshold for participating while allowing participants to collaboratively work with computer models by using tangible objects removing the need to have specific expertise in using these models computer games have been developed as interaction layers to models based on a gui chew et al 2013 craven et al 2017 van hardeveld et al 2019 but without the use of tangible objects as a direct source of input in turn games have been developed that use a non digital board in combination with computer models magnuszewski et al 2018 stefanska et al 2011 but without either a direct connection to the models or the models output being visualized on the board in the virtual river game the bidirectional link between the board and models provides participants with visualizations of their actions at the same location as where they made them third for a serious game in general being able to experiment with interventions is a valuable way of triggering learning becu et al 2017 ferrero et al 2018 multiplayer serious games that focus on managing a spatial area generally include interventions as binary options that can be turned on or off for defined locations carson et al 2018 craven et al 2017 rusca et al 2012 this can include a few options of the same interventions at the same locations such as small medium and large versions onencan et al 2016 savic et al 2016 valkering et al 2013 in the virtual river game participants determine the location direction shape and size of interventions by replacing tiles on the board while following rules defined outside the software therefore the interface increases the experimentation potential by including the design of interventions rather than choosing predefined intervention options in serious games similar functionality is found in tile based computer games e g becu et al 2017 chew et al 2013 but not in combination with tangible tiles this type of functionality is also found in planning support systems in which participants can draw and apply interventions on a tabletop surface see e g leskens et al 2014 vonk and ligtenberg 2010 to summarize the hybrid interface combines the strengths of board games accessibility tangibility with the power and flexibility of computer games modeling visualizations which in combination are beneficial to supporting stakeholder participation and improving social learning 5 3 reflection and limitations in all sessions the participants focused mainly on increasing the flood safety score in the early game rounds and only focused on the biodiversity score during later rounds this approach is consistent with current dutch river management practice in which improving flood safety is the primary objective see e g deltaprogramma 2019 however some choices in the virtual river game s design and the choice of participants invited to the sessions may have influenced the participants focus on improving their flood safety scores although no distinction between primary and secondary objectives is made in the game the in game interventions are based on interventions implemented in reality that do have the improvement of flood safety as the primary objective the sessions starting conditions may have further nudged participants to first focus on the flood safety indicator as its initial score 29 was lower than the biodiversity score 47 in addition some flood safety specialists were observed to approach the game as if they were initially in their real world roles besides these possible influences on the results the study has two other limitations in general a serious game aims to provide participants with a safe space to experiment to take on another role and to defend positions that they may not take in reality de caluwé et al 2012 geurts et al 2007 mayer 2009 the results of this study suggest that the game provides this sense of safety as participants experimented with interventions to pursue the objectives of their assigned role in particular domain experts were observed to propose and experiment with interventions that ran counter to objectives that they would normally pursue in their real world roles however the sessions were organized outside of a real world policy making setting therefore the first limitation of the study is that we do not know if the game would also establish the same sense of safety in experimenting with interventions and defending positions of other stakeholder roles when played in the context of a real world river project this needs to be explored in future research before the game is used in a real river project the roles in the game must be reviewed in particular based on the results of this study adding an additional role that actively defends the interests of the agricultural sector must be considered this role was initially left out to limit the game s complexity in order not to increase the game s complexity an alternative could be to consider the current game as an introductory level and add a second game level with additional objectives indicators roles and interventions the second limitation lies in the limited number of sessions and participants conducting studies on serious games is time consuming both for the participants and for the researchers as a result the number of expert and non expert participants was too low to be able to draw statistically significant conclusions consequently we looked at these two groups separately and have not made comparisons between the learning outcomes of experts and non experts however despite the limitations the results indicate that the game may well be useful as a tool to help support stakeholder participation in dutch river management 6 conclusion in this paper we presented the design of the virtual river game and investigated its potential value as a participatory tool in dutch river management based on the results of five sessions we showed that the game was successful in 1 facilitating domain experts and non experts to collaboratively experiment with river interventions that are commonly applied in the netherlands and to elicit discussions similar to those that occur in real world river projects and 2 establishing social learning outcomes as both experts and non experts indicated that they had learned by playing the game through self reporting non experts indicated that most gained insights into the game s learning objectives and emphasized their learning about the functioning of the river system and about river interventions and their inevitable trade offs experts in turn reported more mixed results regarding the learning objectives and emphasized learning about river interventions and their inevitable trade offs as well as about the views and perspectives of other participants as a participatory tool we conclude that the substantial value of the game lies in introducing non expert stakeholders to dutch river management funding this research is part of the research program rivercare towards self sustaining multifunctional rivers supported by the netherlands organization for scientific research nwo and partly funded by the ministry of economic affairs under grant number p12 14 perspective program declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix table a1 land use options in the game and the translation to specific roughness classes scholten and stout 2013 and ecotopes van der molen et al 2003 including their fractional surface area in a tile for example the tile representing the main channel that includes a longitudinal training dam is assigned roughness class 102 and has a fractional surface area of 0 6 in the ecotope labeled as deep main channel rzd in the dutch ecotope system of large water bodies and 0 4 of the surface area is taken by the ecotope for medium depth side channel rnm table a1 land use of hexagon roughness class of hexagon dominant ecotope in hexagon subdominant ecotope in hexagon fraction dominant ecotope fraction subdominant ecotope elevation level 0 on the game board main channel main channel with groins 102 rzd ii 2 0 75 0 25 main channel with ltd 102 rzd rnm 0 6 0 4 elevation level 1 secondary channel secondary channel 105 rnm rnm 1 0 elevation level 2 lower floodplain built up land floodplain 114 ua 2 ua 2 1 0 production meadow floodplain 1201 ug 2 ug 2 1 0 natural grassland floodplain 1202 ug 1 ug 1 1 0 reed floodplain 1215 iv 9 iv 9 1 0 shrubs floodplain 1231 ub 2 ub 2 1 0 forest floodplain 1245 ub 1 ub 1 1 0 mixtype 70 natural grassland 30 forest floodplain 1202 1245 ug 1 ub 1 0 7 0 3 elevation level 3 higher floodplain built up land natural levee 114 oa 2 oa 2 1 0 production meadow natural levee 1201 og 2 og 2 1 0 natural grassland natural levee 1202 og 1 og 1 1 0 reed natural levee 1215 hm 1 hm 1 1 0 shrubs natural levee 1231 ob 2 ob 2 1 0 forest natural levee 1245 ob 1 ob 1 1 0 mixtype 70 natural grassland 30 forest natural levee 1202 1245 og 1 ob 1 0 7 0 3 elevation level 4 dikes dike 1201 reinforced dike 1201 the groins and ltds could only be included in delft3d fm through the bmi by manipulating the dem to include their shapes in the elevation as a result the main channel is used as a roughness class for both dikes are not considered part of the area evaluated for biodiversity and thus are not assigned an ecotope appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 multimedia component 4 multimedia component 4 multimedia component 5 multimedia component 5 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104855 
25943,households are responsible for a significant share of global greenhouse emissions hence academic and policy discourses highlight behavioral changes among households as an essential strategy for combating climate change however formal models used to assess economic impacts of energy policies face limitations in tracing cumulative impacts of adaptive behavior of diverse households the past decade has witnessed a proliferation of agent based simulation models that quantify behavioral climate change mitigation relying on social science theories and micro level survey data yet these behaviorally rich models usually operate on a small scale of neighborhoods towns and small regions ignoring macro scale social institutions such as international markets and rarely covering large areas relevant for climate change mitigation policy this paper presents a methodology to scale up behavioral changes among heterogeneous individuals regarding energy choices while tracing their macroeconomic and cross sectoral impacts to achieve this goal we combine the strengths of top down computable general equilibrium models and bottom up agent based models we illustrate the integration process of these two alien modeling approaches by linking data rich macroeconomic with micro behavioral models following a three step approach we investigate the dynamics of cumulative impacts of changes in individual energy use under three behavioral scenarios our findings demonstrate that the regional dimension is important in a low carbon economy transition heterogeneity in individual socio demographics e g education and age structural characteristics e g type and size of dwellings behavioral and social traits e g awareness and personal norms and social interactions amplify these differences causing nonlinearities in diffusion of green investments among households and macro economic dynamics keywords behavior change grassroots dynamics soft linking environmental modeling upscaling computational economics 1 introduction energy consumption is the primary culprit behind anthropogenic global warming humanity s demand for energy is satisfied by consuming fossil fuels as well as renewable energy sources leading to varied greenhouse gas emission ghgs footprints households are responsible for 70 of global ghgs hertwich and peters 2009 in europe one quarter of direct total energy consumption and ghgs comes from households 1 1 https climatepolicyinfohub eu node 71 pdf academic and policy discourses highlight behavioral changes among households as an essential strategy for reducing ghg emissions and combating climate change dietz et al 2013 doppelt et al 2009 faber et al 2012 mckinsey 2009 nielsen et al 2020 importantly an individual s decision making is known to deviate from rational and perfectly informed optimization process calling for a thorough understanding of behavioral aspects abrahamse and steg 2011 bamberg et al 2015 2007 poortinga et al 2004 stern 2016 van raaij 2017 policy makers rely on decision support tools to assess future changes in energy markets and the economy as a whole macroeconomic computable general equilibrium cge models serve as standard tools for quantitative policy assessments in climate change mitigation babatunde et al 2017 fujimori et al 2017 ipcc 2014 jrc 2014 rive et al 2006 vandyck et al 2016 cge models are popular among governments and academia for ex ante policy analysis they rely on advancements in micro based macro economic theory that represent the aggregate behavior of rational and fully informed economic agents households and firms and their trade interactions via supply chains behavioral changes including behavioral climate change mitigation actions driven by the increased level of knowledge about climate change in society and shifts in preferences are difficult to model directly with cge models this is one of the critics regarding their capacity to support climate change mitigation policy creutzig et al 2018 farmer et al 2015 farmer and foley 2009 isley et al 2015 niamir et al 2018b stern 2016 in contrast to this macroeconomic top down approach bottom up agent based models abms focus on behaviorally rich representation of energy consumers integrate technological learning out of equilibrium dynamics and social interactions bhattacharyya 2011 farmer et al 2015 hunt and evans 2009 niamir and filatova 2015 niamir et al 2018b tesfatsion 2006 agents in abms follow a set of if else rules sometimes combined with equations that guide their actions interactions with other actors or institutions e g markets and learning abms could compliment macro economic models by accommodating heterogeneity adaptive behavior and interactions bounded rationality and imperfect information filatova and niamir 2019 however their use for climate policy is hindered by high data intensity for individual behavioral rules and interactions when energy abms are grounded in empirical data their upscaling remains limited humphreys and imbert 2013 lamperti et al 2019 preventing the assessment of economy wide impacts effects of national or eu policies and generalization of abms results there is a long history in bridging top down cge models with bottom up models krook riekkola et al 2017 usually non abm specifically for energy macroeconomic models are linked with engineering micro simulation models focusing on the technological processes of electricity generation sue wing 2008 scholars either establish a soft link between micro and macro models or complement one by a reduced form of the other or combine them directly through hybrid modeling böhringer and rutherford 2009 since engineering bottom up models often rely on mathematical programming the latter approach focuses on resolving mixed complementarity problems bohringer and rutherford 2008 besides linking to engineering micro simulations national level cges rely on complimentary micro simulation models for environmental analysis taxation peichl and schaefer 2009 fiscal analyses debowicz 2016 and labor market analysis benczúr et al 2018 however an integration of micro macro approaches at the regional sub national level is scarce verikios and zhang 2015 in parallel as inequality and distributional impacts of climate change policies come into a spotlight internationally introducing heterogeneity into cge models becomes increasingly important bijl et al 2017 kulmer and seebauer 2019 melnikov et al 2017 rao et al 2017 van ruijven et al 2015 this is commonly done by disaggregating the representative agent in macro models with micro level survey data rausch et al 2011 duarte et al 2016 provide an excellent example on modeling of pro environmental consumer behavior in a regional cge model for spain using micro level data this study evaluates the impact of improving environmental awareness by specifying drivers of behavioral changes adoption of household appliances with different energy efficiency levels for different income levels using household survey data duarte et al 2016 while using survey data in cges is a major step in accommodating heterogeneity the choices that economic agents pursue remain fixed and are still assumed to be taken under conditions of perfect information it hinders the representation of behavioral changes bounded rationality and social influences so prominent in understanding pro environmental choices niamir et al 2020a steg and vlek 2009 linking macroeconomic cge models with micro level behaviorally rich abms can operationalize behavioral changes in formal policy analysis and open new synergies between micro and macro approaches krook riekkola et al 2017 melnikov et al 2017 parris 2005 safarzyńska et al 2013 smajgl et al 2009 earlier attempts to integrate abm and cge models include the work of safarzyńska et al 2013 who propose an elegant way to integrate the evolutionary dynamics of abms into a cge model yet authors leave it at the conceptual level without an implementation smajgl et al 2009 discuss a farm level integration of abm cge for fishery policy impact assessment with no integration results to the best of our knowledge there is no empirical example of resolving the key methodological differences between abm and cge modeling while aligning with survey data on behavioral heterogeneity the current paper addresses this methodological gap by demonstrating how aggregated impacts of household energy behavior changes emerging from an empirical abm could be scaled up and linked to the macroeconomic dynamics of a cge model to demonstrate the feasibility of the method we employ a soft linkage between the two empirical models future work will focus on a hard link integration following our earlier pilot on using software wrappers to assure a real time data exchange between toy abm and cge models belete et al 2019 here we ensure models consistency by aligning functional forms and by using the same database and economic scenarios the objective of this paper is twofold 1 to investigate feasibility of an original approach to link empirical abm and cge models while targeting individuals heterogeneity social interactions and behavioral changes and 2 to explore the impacts of climate change mitigation behavior across scales from individuals to the eu regions towards this end we propose a three step upscaling approach that goes beyond our specific application and may serve as a systematic way to link abm and cge models section 2 our results demonstrate that it permits tracing the macro economic and cross sectoral impacts and indirect effects of individual energy behavioral changes section 3 section 4 concludes with a discussion and outlining future work 2 methods to explore economy wide impacts of behavioral changes and the role of social interactions the current paper employs the strengths of micro and macro socio economic models we use an empirical behavioral abm bench v 3 originally developed to study cumulative impacts of individual changes in energy use niamir et al 2020b 2018a to trace indirect effects and cross sectoral impacts of shifts in residential energy demand and changes in households consumption behavior we employ an empirically calibrated cge model eu ems ivanova et al 2019 the scientific challenge is in aligning the two models that differ in key assumptions namely representative vs heterogeneous agents cge models work with a representative agent group while abms assume heterogeneity in attributes and behavior perfect vs bounded rationality agents in cge are assumed to be fully rational while abms proliferate in tackling research problems where bounded rationality is relevant static vs adaptive behavior households in cge have fixed preferences and perfect information while abm are designed to explicitly model adaptive expectations since abm agents do not have full information they learn over the course of a simulation either from their own experience from their social network or through market signals unique one shot equilibrium vs out of equilibrium dynamics cge models are solved via the assumption of a unique equilibrium occurring in one shot when markets clear in contrast abms trace the process of out of equilibrium dynamics and transitions between multiple equilibria while eliciting path dependencies 2 1 models and scenarios 2 1 1 the bench agent based model originally the bench abm niamir et al 2020b 2018a niamir and filatova 2017 was developed to investigate the role of behavioral changes with respect to an individual energy use in the transition to a low carbon economy households in bench abm are heterogeneous in socio demographic characteristics e g income age education dwelling characteristics e g type size age energy consumption patterns e g electricity and gas consumption energy provider and behavioral factors e g awareness personal norms social norms bench is spatially explicit with behavioral rules of agents calibrated based on the survey data for two eu nuts2 2 2 the nomenclature of territorial units for statistics abbreviated nuts is a geographical nomenclature subdividing the economic territory of the european union eu into regions at three different levels nuts 1 2 and 3 respectively moving from larger to smaller territorial units regions navarre spain and overijssel the netherlands niamir et al 2020a we advance this abm further to permit integration with the eu ems cge both in terms of the theoretical consistency of functional forms used in abm and cge as well as the datasets and scenario assumptions we start aligning the abm model with its macro counterpart by including the empirically estimated discrete choice functions for the representation of households investment decisions these functions stem from the utility optimization approach that is also used for the derivation of demand functions in the cge model and are further relaxed in the abm to accommodate bounded rationality namely agents utility functions are modified to align with empirically grounded energy decisions from the households survey niamir et al 2020a social interactions and learning with macroeconomic dynamics in our data driven cge model in particular benchv 3 focuses on energy investments that households may decide to undertake significant investments in house insulation i1 or moderate investment in solar panels i2 and modest investments in energy efficient appliances i3 fig 1 cognitive process behind individual behavioral changes in accordance with the theory of planned behavior and norm activation theory from psychology we assume that boundedly rational individuals in bench v 3 make decisions following a number of cognitive steps knowledge activation motivation and consideration niamir et al 2020a 2018a fig 2 shows heterogonous households in sociodemographic characteristics dwelling conditions electricity and gas consumption follow a cognitive process to decide whether to pursue any energy investment i1 i3 niamir et al 2018a describes how each individuals knowledge activation and motivation are measured and calculated at the model initialization stage based on the survey data in summary an individual knowledge activation level is calculated based on the average of three types of knowledge person s climate energy environment knowledge k awareness about climate environment and energy issues a c and energy decision a e if this average for an individual is above the empirical threshold then the person is tagged as feeling guilt and proceeds to the next step to assess his her motivation for actions i1 i3 such individuals proceed to evaluate the motivational factors personal and social norms n p n s for each action i1 i3 if individuals are highly motivated and feel responsible the perceived behavior controls 4 3 photo sources i1 by tracey nicholls cc by 3 0 i2 by enrix knuth cc by sa 4 0 i3 by tommaso sansone91 cc0 available from https commons wikimedia org 4 own perception of their ability to perform an action or change behavior pbc and the dwelling ownership status owner or renter are evaluated to assess intentions individuals with a high level of intention proceed to estimate utilities which are formulated as a discrete choice problem here household agents follow these stages for each action when deciding whether to invest in insulation solar panels or energy efficient appliances households in bench v 3 make choices based on the indirect utility function eq 1 as the inverse of the expenditure function when prices are constant it reflects individual preferences for different energy actions under budget constraints eq 1 v i j x i j β i ε i j the utility of individual j associated with choice i v i j is calculated based on the vector of explanatory observed and latent variables x i j including socio economic characteristics of the individuals dwelling characteristics and financial and ownership situation as well as behavioral factors and the parameter vector β i estimated using a probit regression niamir et al 2020a finally ε i j is the vector of error terms an individual chooses a particular sub action i when their utility is non negative eq 2 i f v i j 0 i i j t r u e e l s e i i j f a l s e social interactions and learning the speed of green investments diffusion does not depend only on social interactions that affect updating of knowledge awareness and norms it depends also on the individual heterogeneity socio economic characteristics or dwelling characteristics which affect utility of taking an action i1 i3 i e serve as proxy for the perceived behavior control pbc in bench v 3 agents exchange information following a simple opinion dynamics model moussaïd et al 2015 when a neighbor takes an action i1 i3 it may alter knowledge awareness and the motivational factors regarding energy choices of others in this peer group namely individuals compare own behavioral factors k a c a e n p n s pbc with those of their closest neighbors and gradually adjust them fig 3 eq 3 we run various scenarios of this social learning see section 2 1 3 our abm uses the same baseline scenario of regional demographic and economic development as the cge model ensuring the consistency between the scenario analysis in two models further the abm takes as inputs data on the regional gdp projections estimated for 2015 2050 by the cge model the detailed description of the bench agent based model is presented in appendix 1 2 1 2 computable general equilibrium model eu ems ivanova et al 2019 is a spatial cge model developed by the pbl netherlands environmental assessment agency for policy impact assessments the current version of eu ems covers 276 nuts2 regions across the eu28 member states goods and services are produced by firms and consumed by households or other firms and exchanged on competitive markets spatial interactions between regions are captured through the trade in goods and services factor mobility and knowledge spill overs following the tradition of comprehensive empirical cge models eu ems uses large datasets of real economic data in combination with complex computational algorithms to assess how the economy reacts to changes in governmental policy technology availability of resources and other external macro economic factors the eu ems model consists of a the system of non linear equations which describes the behavior of various economic actors and b a very detailed database of economic trade environmental and physical data the core part of the model database is the social accounting matrix which represents in a consistent way all annual economic transactions the database 5 5 http themasites pbl nl winnaars verliezers regionale concurrentie of the model has been constructed by pbl using the combination of national european and international data sources and represents a detailed regional level nuts2 for eu28 plus 34 non eu countries multi regional input output mrio table for the world the main datasets used for the construction of this mrio include the 2013 oecd database baci trade data eurostat regional statistics and national supply and use tables as well as the detailed regional level transport database of dg move called etis plus 6 6 http viewer etisplus net the later dataset allows us to estimate the inter regional trade flows at the level of nuts2 regions that are currently not available from official statistical sources the aggregated groups of the sectors can be directly linked to the panel data econometric analysis and estimations that have been done for total factor productivity tfp projections using the eu klems database 7 7 http www euklems net we have used panel data techniques on eu klems data in order to model the development of tfp according to the technological catch up theory the detailed description of our cge model is presented in appendix 2 measuring economic inequality economists often measure regional disparities using theil s t inequality index eq 3 the absolute value of which indicates the distance from equality eq 3a t h e i l t θ i i θ i i 1 n log γ i μ where θ o is the gdp of each nuts2 region γ i is the gdp per capita in each region as a measure of regional income and μ is the average gdp per capita across the eu28 nuts2 regions the eu ems cge model estimates the cross sectoral aggregated impacts of individual behavioral changes produced by the abm and traces the consequent changes across the eu regions triggered by the macro economy the cge receives measures a the diffusion of each of the three types of actions i1 i3 among heterogeneous households classified in 12 age and education groups b the changes in electricity and gas consumption c saved co2 emissions and d the amount of investment from bench model results 2 1 3 scenarios micro level end user behavioral scenarios besides being heterogeneous in terms of sociodemographic characteristics e g age income education housing they reside in e g tenure status size energy label and psychological factors e g attitudes and beliefs personal norms agents in the bench v 3 abm exhibit heterogeneous behavioral characteristics such knowledge and awareness engage in social interactions and learn bench v3 abm introduces three end user behavioral scenarios baseline fd id by differentiating between the intensity of social interactions and the speed of learning see table 1 based on the neighborhood size this social learning may occur at either a slow or fast pace see scenarios in appendix 1 macro level scenarios in addition to these three behavioral scenarios the eu ems cge model relies on the demographic projections from eurostat until 2050 and total factor productivity tfp projections by economic sector based on our own econometric analysis hence the macroeconomic and demographic scenarios are combined with the slow fast informative dynamics scenarios of micro level behavior with respect to energy related investments of heterogeneous households 2 2 upscaling behavioral changes abm and cge models each have their own assumptions strength and weaknesses we attempt to overcome the latter by linking the two models to pursue this in a systematic manner we take a step wise approach to bridge the abm with the cge model fig 4 2 2 1 step 1 from individual households to regional shifts in energy use bench v 3 abm calculates the extent of behavioral changes among heterogeneous household agents who evolve through a cognitive process section 2 1 1 fig 2 before reaching a more rational stage where the discrete choice utility maximization is activated section 2 1 1 eqs 1 and 2 given the stochastic nature of abms we use the mean values from 100 abm simulations run for each scenario and case study to feed them further into the cge model the main outcomes of the bench v 3 abm used in the eu ems cge model are the relative changes in electricity and gas use and the total investments made by various individuals i1 i3 the eu ems cge model however operates at the level of all 276 eu28 nuts2 regions and needs regional changes in energy consumption and investments of the representative households as an input hence the behavioral patterns emerging at the overijssel and navarre provinces for different households need to be scaled not only up to the national level but up to the entire eu see next steps and fig 4 2 2 2 step 2 dynamic socio demographic groups with similar behavioral patterns we take an intermediate step to derive the changes in investments gas and electricity consumption across households of different age and education levels for all 276 eu28 nuts2 regions based on the outcomes of two regional abms economic theory suggests that investment choices depend on households incomes however our survey on behavioral changes regarding energy use niamir et al 2020a reveals that age and education are the most important factors explaining households preparedness to invest in low carbon energy i1 i3 8 8 with the help of our empirical data we examined the impact socio demographic factors namely income gender education and age on households energy bahavior changes in two provinces overijssel nl and navarre es particulary our analysis shows the probability of households energy behavior increases with the level of eduction 95 confidential interval niamir et al 2020a thus we define behavioral patterns for a group of households in the dutch and spanish regional abms separately aggregating by age and education level following the eurostat classification we work with 12 age education groups table 2 for all 12 groups we estimate a number of households pursuing an action i1 i3 and calculate the corresponding average gas and electricity savings and investments the behavioral factors awareness motivations intentions and likely actions across 12 groups differ between the two countries in our survey sample and so do the patterns of behavioral climate change mitigation emerging in the abms to utilize the information regarding regional differences in patterns of behavioral change we create the mapping between nuts2 regions of the eu28 with the two abm regions according to their perceived cultural distance social structure wealth and lifestyle religion institutional and economic conditions and natural environment play a role in assessing cultural distance gobel et al 2018 hofstede 2011 2001 kaasa et al 2016 schwartz 2014 vignoles et al 2018 specifically in the absence of more granular data we use the dutch case to approximate how the behavioral patterns may evolve in the north west eu states and the spanish case for the south east eu states see table a3 1 in appendix 3 we acknowledge that this approach does not fully capture all the cultural differences but it for example accounts for the role of social network higher among the spanish respondents compared to the dutch in behavioral climate change mitigation ideally one should use native survey data regarding the modeled behavior or employ secondary data on revealed empirical differences on behavioral changes across regions furthermore differences in policy institutional technological and environmental conditions across eu countries are indirectly accounted for in our cge model and the databases it relies upon since behavioral changes vary primarily among households with different age and education levels the changes in these characteristics over time are crucial hence we employ demographic projections for the period until 2050 the only regional nuts2 level projections that have been done for the eu28 are europop2008 9 9 https ec europa eu eurostat documents 3433488 5564440 ks sf 10 001 en pdf d5b8bf54 6979 4834 998a f7d1a61aa82d projections of eurostat population projections of eurostat provide information about the development of the population until 2050 detailed by age and gender groups furthermore eurostat population projections at nuts2 level are combined with iiasa global education trends scenario projections 10 10 http www iiasa ac at web home research researchprograms worldpopulation projections 2014 html related to the share of high medium and low educated persons in each eu country this allows us to construct population projections by age and education level for the period 2020 2050 for each nuts2 region of the eu28 these nuts2 level population projections till 2050 match with the scaled up mapping of behavioral patterns of 12 groups in our abm hence now we use age and education information to linked it with the emerging behavioral patterns of the agent based bench v 3 model when creating nuts2 specific that is corresponding to the population structure of that region inputs into the spatial eu ems cge model 2 2 3 step 3 cumulative economy wide impacts of behavioral changes finally we use the predicted population structure by age and education level for the period 2020 2050 to calculate aggregated changes in the residential use of gas and electricity for each nuts2 regions of eu28 on the basis of calculated averages for each of the 12 individual groups the eu ems cge model estimates the cross sectoral impacts of these shifts in the aggregated residential energy demand that impacts gdp projects the linked abm cge model quantifies the cumulative impacts of behavioral changes among heterogeneous households at the level of 276 eu28 nuts2 regions this allows us to understand the impacts of various behavioral scenarios within the cge framework including distributional effects across these eu regions an important direction of future work would be to develop direct two way linkages between the two models with the cge generated gdp projections feeding back into the abm data flows between two models are presented in fig 4 this step wise approach to linking the abm and cge models allows us to address the key methodological challenges from representative to heterogeneous agents heterogeneous households in the abm are matched with representative households in the cge model aggregation occurs along the two dimensions that impact relevant behavioral changes among households most age and education levels this is done using detailed information about the structure of the population by age and education in each nuts2 region for the period 2020 2050 while keeping behavior heterogeneous across the 12 groups from perfect to bounded rationality agents in our abm are boundedly rational due to the presence of behavior factors k a c a e n p n s pbc that precede discrete choice utility estimate subjective knowledge and awareness motivation and intention to consider a change in behavior which are all prone to social influence the use of the abm allows us to assess the impacts of pure behavioral changes in the cge model and calculate their broader economic impacts the rest of the economy in the cge model e g households decisions on a labor market decisions of firms clearing of the markets still operates in line with the rationality principles allowing for the coherent treatment of macro economic processes in the cge model from static to adaptive agents agents in the abm are prone to social influence and learn from their neighbors as their behavior attributes knowledge and awareness evolve they go through various cognitive stages of knowledge activation motivation and consideration and may eventually decide to invest in low carbon energy by scaling up these behavioral patterns through age education groups we are able to link to the architecture of a cge by default cge models assume perfect information and rational expectations omitting a variety of behavioral strategies through which adaptive behavior can be channeled into macro dynamics from an equilibrium to adaptive dynamics with social learning the cge model is based on assumptions of market equilibrium and interlinkages between different agents sectors and markets in the economy the abm treats agents decisions as a cognitive process in the presence of social interactions and fast slow informative learning before discussing the results it may be useful to be explicit about the limitations of the current study the presented cge to abm link is currently indirect operationalized via the eu gdp growth rates scenarios the dotted curve in fig 4 furthermore to demonstrate the applicability of method we work with two survey datasets for a real policy analysis it is essential to work with a richer representation of regions that may also account for differences in climatic and institutional conditions across countries while our abm relies on households surveys niamir et al 2020b 2020a 2018a for micro validation macro validation against regional level panel data remains a subject of future work we believe that micro validation is sufficient for the methodological demonstration of the applicability of this approach for upscaling behavioral climate change mitigation complementing it with macro validation would be essential when performing a real policy analysis 3 results and discussion given the stochastic nature of abms we run bench multiple times under the same parameter settings for each scenario the abm results presented below plot the means across 100 random runs therefore we use the mean values from each abm scenario and case study to scale up the observed behavioral patterns and to estimate their cross sectoral impacts in the cge model 3 1 step 1 from behavioral patterns in survey data to cumulative impacts in two provinces firstly we run the bench v3 abm for two eu provinces overijssel and navarre under the three behavioral scenarios baseline fd and id we report the regional impacts of the energy behavior choices of heterogeneous households the diffusion of each of the three types of behavioral actions among heterogeneous households over time the changes in electricity and gas consumption saved co2 emissions and the amount of investment fig 5 illustrates the dynamics of electricity and gas saving in the two eu provinces as a result of households energy investments the general trend is as expected faster learning boosted by an information campaign leads to more investments in solar panels i2 and in appliances i3 and consequently to higher electricity savings in both provinces intensive social learning boosts electricity savings by 40 and 100 in overijssel and navarre fd vs baseline fig 5 a and table 3 in addition electricity savings increase by 14 and 22 in two provinces if pro environmental awareness is raised through an information policy id vs fd fig 5 a and table 3 however these trends do not hold for investments in insulation i1 and corresponding gas savings informative strategy id has a mixed impact on insulation investments in navarre crossing of fd and id curves in fig 5b and the opposite effect in overijssel id delivers 26 lower gas savings compared to fd fig 5b the difference between cases may be driven by initial conditions climate institutional settings gas prices in the two countries in addition comparing fd and id scenarios shows that an information policy and social interactions among neighbors impact households insulation decisions in a non linear way table 3 shows the amount of co2 emission savings that households energy behavior changes could deliver and at what investment cost intensive social interaction fd scenario leads to 1 4 and 2 times more saved co2 emissions in overijssel and navarre compared to the baseline as expected information policy along with social interactions id scenario amplify the impact 1 1 and 1 2 times more on top of the fd scenario in overijssel and navarre respectively we observe a non linear pattern in total investments euro per households under behavioral scenarios over time when information policy id scenario is activated dutch households invest 17 more compared to the fd scenario in 2020 and this then drops in 2050 20 less than the fd scenario spanish household investments in the id scenario increases up to 33 in 2030 and then drops by 5 compared to the fd scenario these nonlinearities emerge from households preferred actions i1 i3 unequally distributed over time and space these results are a pure effect of individual changes driven by behavioral factors we do not include any price based scenarios subsidies for green or taxes on grey energy or changes in technological costs in this article our analysis confirms that faster learning boosted by an information campaign fd vs baseline scenarios leads to more investments i2 i3 and consequently to higher electricity savings 40 100 in both provinces in addition electricity savings increase by 14 22 in two provinces if pro environmental awareness is raised through an information policy id vs fd scenarios however id has a mixed impact on insulation investments i1 and gas consumption in navarre and the opposite effect in overijssel id delivers 26 lower gas savings compared to fd 3 2 step 2 scaling up behavioral scenarios to national and eu level after analyzing the dynamics in households behavioral changes in two provinces over time we switch to understanding how they change over space using the population projection scenarios for the eu28 see section 2 2 step 2 we scale the dynamics in household energy behavioral changes in two provinces over time up to national and eu levels namely we define behavioral patterns for a heterogeneous group of households in the dutch and spanish regional abms for each of the 12 age education groups table 2 a number of households perusing an action i1 i3 is estimated together with the average investments and gas and electricity savings the analysis reveals that in the netherlands and spain that the majority of households 75 9 and 68 1 intend to invest in energy efficient appliances i3 by 2050 the minority 4 9 and 9 4 want to invest in insulation i1 this trend is stable over time 2020 2050 electricity consumption resulting from individual behavioral changes decreases between 51 and 71 the netherlands and 51 66 spain by 2050 see appendix 4 table a4 1 fig 6 shows percentage changes in residential electricity consumption as a result of scaling up the output of the empirical abm with the population change scenario electricity consumption resulting from individual behavioral changes decreases between 56 2 69 5 and 13 8 63 8 by 2050 in the netherlands and spain correspondingly importantly there is significant spatial heterogeneity in how behavioral changes diffuse and what regions emerge as laggers or pioneers in bottom up investments in energy efficiency if behavioral patterns elicited through our survey hold in the next few decades it could be expected that the limburg drenthe and zeeland provinces in the netherlands and the castile leon and asturias regions in spain will be pioneers compared to others in respective countries 3 3 step 3 from regional to the national and eu28 economy scaled up outputs of the abm are used as input to the simulation setup of the spatial cge model namely information from bench v 3 on the decrease in households use of electricity and gas is used in order to exogenously modify the minimum subsistence level of households consumption of the respective services in eu ems see appendix 2 the abm cge results indicate that households with higher education levels are more likely to change their behavior compared to less educated people importantly among these higher educated households younger people 20 40 are more active in particular dutch youth saves up to 17 and 74 more electricity and gas compared to 40 households under the fd scenario fig 7 among the pioneers g6 8 i e middle educated and 20 age see table 2 spanish households save 1 9 2 8 and 1 0 1 4 times more gas and electricity compared to dutch households depending on groups and behavioral scenarios intensive social dynamics fd scenario has a stronger impact on saving gas while the informative id scenario activates more households in saving electricity appendix 4 presents a more detailed abm cge analysis on diffusion of households investment per capita per action among sociodemographic groups a reduction in the consumption of gas and electricity by households results in a higher budget share that becomes available for other types of consumption depending on households consumption patterns such shifts in consumption might result in higher values of gdp over time the eu ems model operates at the level of nuts2 regions of the eu28 and hence enables the calculation of the regional impacts of various behavioral scenarios on real gdp that is gdp that includes only quantity effects we choose to use gdp in our analysis instead of welfare indicators such as equivalent variation measure because the monetary indicator such as gdp can be easily compared with the outcomes of the abm model in terms of monetized energy savings and investments the focus of the present study is in illustrating the added value of the use of cge model and the degree of the indirect and economy wide effects calculated by the cge which justifies the choice of monetary gdp indicator for our analysis fig 8 illustrates the difference in regional real gdp levels in 2050 between the baseline and fd scenarios most of the eu28 regions benefit from the behavioral changes which leads to a decrease in energy consumption with a few regions affected negatively the level of overall real gdp impacts depends on the size of the region in terms of population and its share of highly educated youth appendix 4 presents the percentage changes on the level of regional gdp relative to the baseline scenario see figures a4 2 fig 9 presents the effects in relative terms scenario as of the baseline which already accounts for whether a region is rural or urban and relate them to gdp per capita it implies there is a statistical relationship between the two variables the baseline gdp per capita which is also positively correlated with the share of highly educated persons and the benefits in terms of additional economic growth per capita from the modeled behavioral changes though the relationship is non linear the trend indicates that rich and economically well developed regions receive higher benefits from promoting behavioral changes in the long run compared to the lagging regions this phenomena raises the question of whether the distribution of economic benefits skewed towards rich and well developed regions increases the overall interregional inequality in europe to understand how behavioral changes under our scenarios impact eu28 regional disparities we calculate economic inequality index for the period 2015 2050 section 2 1 2 eq 3 the dynamics of theil s t inequality index demonstrate that the inequality between regions decreases in the period of large investments in energy savings 2025 2035 and then starts to increase again over time indicating the non linear nature of the process fig 10 however the regional inequality in 2050 does not reach the level of 2015 indicating the positive overall impact of behavioral changes on equality despite this changes in inequality due to the implementation of behavioral scenarios remain modest 4 conclusions and outlook the potential of individual behavioral changes in reducing carbon emissions attracts considerable attention as one of the climate change mitigation strategies creutzig et al 2016 ipcc 2014 niamir 2019 comprehensive empirical cges which support quantitative climate change mitigation policy assessments are strong in tracing cross sectoral impacts feedback in the economy as a whole and in linking to readily available datasets however their econometrically estimated equations reflect past behavior making it difficult to integrate behavioral changes babatunde et al 2017 farmer and foley 2009 moreover while empirical evidence suggests that individual decision making deviates from a rational and perfectly informed optimization process the latter is the core of cge models farmer et al 2015 stern 2016 wilkerson jerde and wilensky 2015 abms compliment macroeconomic models by accommodating heterogeneity adaptive behavior and interactions bounded rationality and imperfect information rai and henry 2016 while there are few largely non empirical abms in policy and institutional domain that take a macro e g country and global scale perspective castro et al 2020 gerst et al 2013 behaviorally rich empirical abms mostly operate on small scales of neighborhoods cities and regions although these micro abms are strong in aggregating heterogeneous adaptive behavior they omit feedbacks with the rest of the economy and cross sectoral impacts survey data is increasingly used to specify individual agent s rules yet this behavioral data is not always compatible with the data used in macro models linking abms and cge models could ameliorate their weaknesses yet the models should be aligned coherently conceptually and data wise to benefit from their strengths voinov and shugart 2013 methodologically this article contributes to the ongoing debate krook riekkola et al 2017 parris 2005 safarzyńska et al 2013 smajgl et al 2009 on linking these two alien approaches by presenting a method of systematic upscaling of individual heterogeneity and social dynamics to combine abm and cge models the insights from this methodological exercise offer three conclusions firstly we demonstrate the feasibility and importance of introducing heterogeneity and behavioral rich dynamics in assessing climate change mitigation policies we develop a transparent step wise process to integrate an empirical behaviorally rich abm and a spatial cge model to the best of our knowledge this is the first attempt to link empirical abm and cge models to estimate the macroeconomic impacts of individual energy behavioral changes in the absence of this integration one should twist the cge parameters and structure in an ad hoc manner to permit some representation of a behavioral change instead an abm that relies strongly on the theoretical and empirical micro foundations from surveys quantifies the patterns of behavioral change across heterogeneous households in a transparent way accounting for non monetary aspects of individual energy choices secondly this article demonstrates that scaling up behavioral change dynamics has policy relevant consequences at large scales our abm grounded in theory and survey data quantifies the patterns of behavioral change which could further be channeled into the cge models that traces macroeconomic and cross sectoral dynamics specifically here we find that the regional dimension is important in a low carbon economy transition driven by individual behavioral change some regions lag behind while others are pioneers due to the heterogeneity in individuals socio demographics e g education and age structural characteristics e g type and size of dwellings behavioral and social traits and spatial characteristics e g urban vs rural which produce incremental differences at small scales yet when aggregated they cumulatively create disparities which are amplified by macro economic forces importantly the inequality between regions decreases in the period of large investments 2015 2035 and starts to increase over time following it finally as behavioral barriers to climate change mitigation in designing policies gain attention policy makers would benefit from decision support tool that go beyond a stylized representation of households as perfectly informed optimizers individual awareness diversity in norms and knowledge play a key role in a green economy transition and climate change mitigation policies should ideally combine the conventional macroeconomic analysis with these behavioral barriers and drivers considering bottom up behavioral patterns would not easily change over time to see substantial changes we need a mix of external intervention from soft information policies aimed to raise awareness bottom up to financial incentives altering the macro landscape of energy markets and technological transitions at times information and price based policies create a non linear effect on cumulative behavioral changes regarding energy use niamir et al 2020b our approach demonstrates that with computational abm directly linked to survey data and macroeconomic cge models individual behavioral heterogeneity and social influences can now be considered when designing implementable and politically feasible policy options the future work can go in two main directions advancing the modeling approach and improving the models dataset from the modeling perspective future work could focus on introducing direct feedbacks between cge abm enabling the evaluation of price based and information policies jointly at multiple scales the feedbacks between the two empirical models may be enabled through software wrappers and modern web interfaces for integration belete et al 2019 in addition due to the large number of parameters and multidimensionality of the generated data from any abm lee et al 2015 the global sensitivity and uncertainty analysis was out of scope of this article future work should focus on quantifying uncertainties that this integration of abm and cge models may impose including for example exploratory analysis kwakkel and pruyt 2013 to understand the integrated model s behavior and its sensitivity to initial configurations of its parameters from the dataset perspective running surveys in more eu countries would improve the model accuracy especially vital when predicting policy impacts also data wise the behaviorally rich demand side modeling could benefit from endogenizing the dynamics of dwelling stock static and aging housing should be replaced by scenarios of structural and technological progress in new urban development e g zero carbon footprint buildings and refurbishing old housing stock in cities data availability the extensive description of the models and data is presented in the appendix of this manuscript the bench model is calibrated based on the empirical dataset we designed and conducted the survey in two provinces in europe for the purpose of this research niamir et al 2020a the agent based bench model is parameterized using the survey data on socio demographic economic structural and behavioral attributes of households and their dwelling characteristic table a1 1 the bench agent based model is open source and available on comses https www comses net the main database of eu ems model is the pbl jrc world wide mrio database documented in https ec europa eu jrc sites jrcsh files jrc115439 pdf and available to download from https data overheid nl dataset pbl euregio database 2000 2010 besides this mrio database we have also used the national accounts data from eurostat research project rpp 342 2016 csis eu silc hbs lfs and oecd for the construction of social accounting matrices used to calibrate the model according to the terms of use authors are not allowed to redistribute the eurostat micro data the derived intermediate result are available from the corresponding author upon reasonable request declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the eu fp7 complex knowledge based climate mitigation systems for a low carbon economy project no 308601 the scalar european research council erc project grant agreement no 758014 and the iiasa rite distinguished young scientist award recipient leila niamir we are thankful to prof dr alexey voinov university of technology sydney australia and prof dr hans bressers university of twente the netherlands for their feedback and support the authors would like to thank the tns nipo team for their collaboration in conducting the household survey we also appreciate the participation of the survey respondents and the constructive feedback we received from the three anonymous reviewers appendix 1 bench agent based model the bench abm niamir et al 2020b 2018a is developed to study shifts in residential energy use and corresponding emissions driven by behavioral changes among individuals main processes of the model odd protocol table a1 1 bench v 3 abm odd protocol table a1 1 guiding protocol the bench v 3 model a overview a 1 purpose the bench v 3 agent based model is designed to study shifts in residential energy use and corresponding emissions at the regional level driven by behavioral changes among heterogeneous individuals this empirically grounded model is of interest to i environmental scientists interested in modeling human behavior and economic institutions ii energy economists working on micro aspects iii scholars integrating individuals behavioral change in climate change mitigation modeling a 2 entities state variables and scales agents individuals in bench v 3 model are heterogeneous in socio demographic and dwelling characteristics energy consumption and patterns source of energy and energy provider and behavioral factors the bench v 3 simulations 1035 and 755 individual households in the overijssel province the netherlands and navarre province spain over 34 years 2016 2050 one time step represents one round in the behavioral experiments each run consist of 34 time steps aligning to the 34 rounds in the behavioral experiments a 3 process overview one time step represents one year in each time step a household goes through several processes 1 asses behavioral factors knowledge activation motivation consideration 2 calculate utilities 3 pursue an action or not 4 calculate saved energy and co2 emission 5 social dynamics and learning process 6 satisfaction and regret 7 updates see fig 2 for algorithm and decision making proccess in the bench v 3 agent based model b design concept b 1 theoretical and empirical background in application to environmental and energy related choices three behavioral change theories are commonly applied theory of planned behavior tpb norm activation theory nat and value belief norm vbn theory tpb formulated by ajzen 1980 and based on the theory of reasoned action is one of the most influential theories in social and health psychology and has been used in many environmental studies armitage and conner 2001 onwezen et al 2013 nat originally developed by schwartz 1977 operates in the context of altruistic and environmentally friendly behavior it is mostly focused on anticipating pride in doing the right thing and on studying the evolution of feelings of guilt vbn theory stern et al 1999 stern 2000 explains environmental behavior and good intentions such as willingness to change behavior nordlund and garvill 2003 steg and vlek 2009 stern et al 1999 environmental citizenship stern et al 1999 and policy acceptability de groot and steg 2009 steg et al 2005 b 2 individual decision making we introduce a framework that combines the strengths of the three key behavioral theories see figure a1 1 b 3 heterogeneity agents are heterogeneous in respect of the following variables see table a1 2 socio demographic dwelling energy consumption energy provider behavioral factors b 4 interactions social dynamics and learning agents heterogeneous individual households engage in interactions and learn from each other in particular they can exchange information with neighbors which may alter own knowledge awareness and motivation regarding energy related behavior we employ a simple opinion dynamics model acemoglu and ozdaglar 2011 degroot 1974 hegselmann 2002 moussaïd et al 2015 assuming that each agent interacts with a fixed set of nearby neighbors the bench v 3 model is a spatially explicit model that takes the raster maps of the two nuts2 regions as an input hence an agent who is in active neighborhood where at least one out of eight nearest spatial neighbors within 1 raster cell moor neighborhood concept undertakes an energy related action will interact and exchange opinions the idea of the moore neighborhood comes from cellular automata literature and used only to enable opinion exchange between neighbors about climate and environmental awareness and compare norms agents compare values of their own behavioral factors knowledge awareness and motivation with those of their eight closest neighbors and adjust their values for a closer match see fig 3 and eq 3 however the agents heterogeneity beyond their spatial location income age education and economic factors affect individual choices of undertaking any of energy actions i1 i3 or not b 5 spatial scale lowest scale individualshighest scale nuts2the focus of this research is on overijssel the netherlands nl21 and navarre spain es22 nuts2 regions which consist of 25 and 10 main cities municipalities respectively b 6 individual prediction individuals do not predict future condition b 7 stochasticity there are various sources of stochasticity in the model 1 initial setting agents attributes initialization are partly random 2 during the process social dynamics and learning process is partly random b 8 observation bench v 3 estimates cumulative impacts of energy related behavioral changes of individual households on electricity and gas consumption and co2 emissions reports number of energy related actions per year investment conservation switching saved electricity and gas per action year investment conservation switching avoided co2 emission per action year investment conservation switching across socioeconomic age and education groups see table 1 and cases nl vs es b 9 implementation details the model is coded in netlogo 6 0 4 open source and available on comses https www comses net r is used for the result visualizations c details c 1 initialization the variations in socio demographic dwelling and psychological factors among our survey respondents are used to initialize a population of heterogeneous agents in the bench v 3 model see table a1 1 and a1 3 c 2 input data the data on the behavioral and economic factors affecting household energy choices were collected using an online questionnaire n 1790 households and serve as empirical micro foundation of agent rules in the bench v 3 model fig a1 1 bench v 3 conceptual behavioral framework source niamir et al 2020a fig a1 1 table a1 2 overview of main variables and parameters used in bench v 3 table a1 2 factors variables value range socio demographic income 1000 150 000 education primary doctoral dwelling energy label a f ownership status owner renter energy consumption 500 5000 provider grey brown green energy saving habit 0 3 behavioral knowledge 1 7 cee awareness 1 7 ed awareness 1 7 personal norms 1 7 social norms 1 7 intention a1 1 7 intention a2 1 7 intention a3 1 7 data the bench v 3 model is calibrated based on an empirical dataset we designed and conducted the survey in two provinces in europe for the purpose of this research in 2016 1035 households in the overijssel province the netherlands and 755 households in the navarre province spain filled out our online questionnaire niamir 2019 niamir et al 2020a niamir and filatova 2017 2016 the agent based bench v 3 model is parameterized using the survey data on socio demographic economic structural and behavioral attributes of households and their dwelling characteristic table a1 3 table a1 3 survey data on households characteristics and behavioral intentions the data is used to parameterize households behavior in the bench v 3 abm source niamir et al 2020a 2018a table a1 3 factors overijssel navarre socio demographic characteristics gender female 46 4 male 53 6 female 57 1 male 42 9 age years 53 41 education isced image 5 image 6 annual income in thousand euros per year image 7 image 8 dwelling characteristics type of residence apartment 14 9 house 85 1 apartment 77 8 house 22 2 tenure status owner 71 renter 29 owner 80 3 renter 19 7 size of residence image 9 image 10 age of residence image 11 image 12 behavioral characteristics value on the 1 7 scale knowledge k 4 2 0 7 5 0 0 8 awareness climate a c 4 9 0 8 5 4 0 8 awareness energy decision a e 4 5 1 0 5 3 1 1 personal norms n p 4 6 0 9 5 4 1 0 social norms n s 3 3 1 1 4 5 1 2 perceived behavior control pbc 4 4 1 1 5 0 1 3 https ec europa eu eurostat statistics explained index php international standard classification of education isced outputs the agent based bench v 3 model tracks the individual and cumulative impacts of three energy behavioral changes investments on insulation pvs installation and energy efficient appliances among heterogeneous individuals in the overijssel and navarre provinces over 34 years 2016 2050 we report the number of individuals pursuing a particular action i1 i3 the cumulative electricity and gas consumption and saved carbon emissions given the stochastic nature of abms we perform multiple n 100 repetitive runs of each simulation experiment lee et al 2015 appendix 2 spatial eu ems cge model general description eu ems is a spatial computable general equilibrium scge model developed by pbl netherlands environmental assessment agency the sectoral and geographical dimensions of the model are flexible and can be adjusted to the needs of a specific policy or research question the model is used for policy impact assessment and provides sector region and time specific model based support to dutch and eu policy makers on structural reforms growth innovation human capital and infrastructure policies the current version of eu ems covers 276 nuts2 regions of the eu28 member states and each regional economy is disaggregated into 63 nace rev 2 economic sectors 11 11 https ec europa eu eurostat documents 3859598 5902521 ks ra 07 015 en pdf goods and services are consumed by households government and firms and are produced in markets that can be perfectly or imperfectly competitive spatial interactions between regions are captured through trade of goods and services factor mobility and knowledge spill overs this makes eu ems particularly well suited for analyzing policies related to human capital transport infrastructure r i and innovation in the current application of the model we have aggregated the economic sectors to the following six large groups following the eurostat classification of the economic sectors according to their r d intensity 1 traditional 2 low tech industry 3 medium tech industry 4 high tech industry 5 knowledge intensive services and 6 other services main processes of the model eu ems accounts for the a feedback between price and demand supply quantities and b interactions between economic agents at the macro and sectorial level therefore it gives the economic relations between all industry sectors via their intermediate use the eu ems model is a dynamic recursive over time model involving dynamics of capital accumulation and technology progress stock and flow relationships and adaptive expectations the model equations are neo classical in spirit assuming cost minimizing behavior by producers average cost pricing and household demands based on optimizing behavior the cge model database consists of tables of transaction values and elasticities dimensionless parameters that capture behavioral response the database is presented as a social accounting matrix which covers an entire national economy and distinguishes a number of sectors commodities primary factors and types of households as a classical cge model eu ems represents the behavior of the whole population group or of the whole industrial sector as the behavior of one single aggregate agent it is further assumed that the behavior of each such aggregate agent is driven by certain optimization criteria such as maximization of utility or minimization of costs in following detailed representation of the eu ems model and its main equations are presented fig a2 1 circular economic flow in the cge eu ems model source ivanova et al 2019 fig a2 1 regional structure of the model regions differ by the type of production sectors which dominate overall production activities in the region some specialize in traditional sectors such as agriculture whereas others specialize in modern sectors such as finance and industry those sectors are characterized by different levels of agglomeration and its importance traditional sectors do not experience any agglomeration effects whereas modern sectors do this allows some sectors to grow faster than other the prototype model will incorporate the regional difference in sectoral specialization and hence the difference of agglomeration economies between the regions table a2 1 regions in eu ems cge models source ivanova et al 2019 table a2 1 code name code name aus australia arg argentina aut austria bgr bulgaria bel belgium bra brazil can canada brn brunei darussalam chl chile chn china cze czech republic chn dom china domestic sales only dnk denmark chn pro china processing est estonia chn npr china non processing goods exporters fin finland col colombia fra france cri costa rica deu germany cyp cyprus grc greece hkg hong kong sar hun hungary hrv croatia isl iceland idn indonesia irl ireland ind india isr israel khm cambodia ita italy ltu lithuania jpn japan lva latvia kor korea mlt malta lux luxembourg mys malaysia mex mexico phl philippines mex gmf mexico global manufacturing rou romania mex ngm mexico non global manufacturing rus russian federation nld netherlands sau saudi arabia nzl new zealand sgp singapore nor norway tha thailand pol poland tun tunisia prt portugal twn chinese taipei svk slovak republic vnm viet nam svn slovenia zaf south africa esp spain row rest of the world swe sweden che switzerland tur turkey gbr united kingdom usa united states household preferences and governmental sector the households and governmental demand for goods and services is represented by the linear expenditure system les that is derived as a solution to the stone geary utility maximization problem eq a2 1 u r i c r i μ r i γ r i the resulting demand system where i r denotes households disposable income and p r i are consumer prices of goods and services that include taxes subsidies transport and trade margins can be written as follows eq a2 2 c r i μ r i γ r i 1 p r i i r j μ r j p r j households always consume a certain minimum level of each good and services where this level reflects the necessity or price elasticity of the good or service necessities such as food have low price elasticity and hence a higher minimum level of consumption the disposable income of the households consists of wages return to capital and social transfers from the government minus the income taxes and households savings the government collects production consumptions and income taxes the tax revenue is further used to pay social transfers and buy goods and services for public consumption the governmental savings can be either endogenous or exogenous in the model depending on the type of simulation and the type of chosen macro economic closure firms production domestic production x r i d is obtained using the nested ces production technology of capital labour energy materials klem type where k is the capital l is the labour e is the energy and m is the materials figure a2 2 represents the nests in the klem production function used in the model with services between used according to the fixed leontief input coefficients in the production process the energy in the model is differentiated between electricity and other types of energy with some substitution possibilities between them the labour is differentiated according to three education levels according to international labour organisation ilo classification the domestic production is generated according to nested production ces function which is described by the following set of composite ces functions that follow the production structure from top to the bottom nest eq a2 3 x r i d a r i m r i ρ m k l e 1 a r i k l e r i ρ m k l e 1 ρ m k l e eq a2 4 k l e r i b r i e r i ρ e k l 1 b r i k l r i ρ e k l 1 ρ m k l e eq a2 5 k l r i c r i k r i ρ k l 1 c r i l r i ρ k l 1 ρ k l eq a2 6 e r i d r i e r i n e l e c ρ e 1 d r i e r i e l e c ρ e 1 ρ e eq a2 7 l r i e f r i e l r i e e d ρ l 1 ρ l where a r i b r i c r i d r i and f r i e are the share parameters of the corresponding production function nests and ρ m k l e ρ e k l ρ k l ρ e and ρ l represent the substitution possibilities for each of the production function nests the inputs into the production are denoted as m r i input of materials k l e r i composite capital labor energy nest e r i energy inputs k l r i composite capital labor nest k r i capital input l r i labor input e r i n e l e c input of non electric energy e r i e l e c input of electric energy and l r i e e d inputs of labor by type of education e fig a2 2 structure of klem production functions in the model source ivanova et al 2019 fig a2 2 international and inter regional trade the total sales x r i of tradable goods and services i in region r in the model is an armington constant elasticity of substitution ces ref composite between domestic output x r i d and imports x r i m such that eq a2 8 x r i α r i d x r i d ρ i α r i m x r i m ρ i 1 ρ i where α r i d and α r i m are the calibrated share parameters of the ces function and ρ i σ i 1 σ i with σ i being the armington elasticity of substitution between domestic and imported tradable goods and services the elasticity of substitution varies between different types of goods and services depending on the available empirical estimates in case of non tradable the composite is equal to the domestically produced product imported goods can come from various regions and countries represented in the model and the composite imported goods and services are represented by the ces composite that uses a higher armington elasticity of substitution as compared to the upper armington nest we assume as in the gtap model that the elasticity of substitution between the same type of goods and services coming from different countries is twice as large as the elasticity of substitution between domestic and aggregate imported goods and services the aggregate imported good is calculated according to the following ces composite function eq a2 9 x r i m s α s r i t x s r i t ρ i t 1 ρ i t where α s r i t is the calibrated share coefficient of the ces production function x s r i t is the flow of trade in commodity i from country s to country r the coefficient ρ i t σ i t 1 σ i t where σ i t is the elasticity of substitution between commodities produced in different countries labour capital and goods markets market equilibrium in the economy results in equalization of both monetary values and quantities of supply and demand market equilibrium results in equilibrium prices that represent in the case of cge models the solution to the system of nonlinear equations that include both intermediate and final demand equations as well as accounting constraints that calculate households and government incomes savings and investments as well as trade balance eu ems model represents a closed economic system meaning that nothing appears from nowhere or disappears into nowhere in it this feature of the cge model constitutes the core of the walrasian equilibrium and ensures that even if one excludes any single equation of the model it will still hold this is the property of cge models called walras law that tells us that in the closed economic system if n 1 markets are in equilibrium the last nth market will also be in equilibrium in our eu ems model the static equilibrium is described by the set of commodity and factor prices total outputs final demands of households and government investments savings and net transfers from abroad such that 1 markets for goods and services clear 2 total investments are equal to total savings 3 total households consumption is equal to their disposable income minus savings 4 total governmental consumption is equal to its net tax revenues minus transfers to households minus savings 5 total revenue of each economic sector is equal to its total production costs and 6 difference between imports and exports is equal to the net transfers from abroad recursive dynamics eu ems is a dynamic model and allows for the analysis of each period of the simulation time horizon this horizon is currently set at 2050 but it can be extended to longer time periods for each year of the time horizon eu ems calculates a set of various economic social and environmental indicators the economic growth rate in eu ems depends positively on investments in r d and education by investing in r d and education each region is able to catch up faster with the technological leader region and better adopt its technologies time periods in eu ems are linked by savings and investments by the end of each time period households firms and government in the model save a certain amount of money this money goes to the investment bank distributing it as investments between the production sectors of the various regions the allocation decisions of the investment bank sectors depend on the sector s financial profitability the model runs in time steps of five years for the period 2015 2050 the capital stocks evolve according to the dynamic rule presented below where the capital stock in period t is equal to the capital stock in period t 1 minus the depreciation plus the new investments into the capital stock eq a2 10 k t r i k t 1 r i 1 δ i i t r i at the end of each period there is a pool of savings s r available for investments into additional capital stocks of the sectors this pool of savings comes from households firms and foreign investors the sector investments i t r i are derived as a share of the total savings in the economy according to the discrete choice formula eq a2 11 i t r i s t t 1 r b r i k t 1 r i e ϑ w k r t 1 r i j b r j k t 1 r j e ϑ w k r t 1 r j eq a2 12 w k r t 1 r i r t 1 r i p i t 1 r g r δ r i where w k r t 1 r i denotes the capital remuneration rate g r the steady state growth rate b r i the calibrated gravity attraction parameter and ϑ the speed of investment adjustment outputs the eu ems model produces detailed dynamics of regional gdp production and value added by region and by economic sector interregional trade flows by the type of commodity electricity and gas consumption per region and sector employment by regional and economic sector household income and consumption and governmental revenues and spending for the purpose of this article we limit the presentation of the main cge output to gross domestic product gdp percentage change in the electricity consumption per nuts2 region country and the entire eu appendix 3 upscaling distance between countries is not only the geographical and therefore the regional economic integration should not happen regardless other local factors social structure wealth and lifestyle religion institutional and economic conditions and natural environment play a role in assessing cultural distance gobel et al 2018 hofstede 2011 2001 kaasa et al 2016 schwartz 2014 vignoles et al 2018 table a3 1 summarized the value of cultural dimensions in this study due to the absence of more granular data we use the dutch case to approximate how the behavioral patterns may evolve in the north west eu states and the spanish case for the south east eu states which is in line with the values presented below table a3 1 values of cultural dimensions for all eu countries sources čuhlová 2018 table a3 1 country pdi inv austria 11 55 belgium 65 75 bulgaria 65 75 croatia 73 33 cyprus a czech republic 57 58 denmark 18 74 estonia 40 60 finland 33 63 france 68 71 germany 35 67 greece 60 35 hungary 46 80 ireland 28 70 italy 50 76 latvia 44 70 lithuania 42 60 luxembourg 40 60 malta 56 59 netherlands 38 80 poland 68 60 portugal 63 27 romania 90 30 slovakia 104 52 slovenia 71 27 spain 57 51 sweden 31 71 uk 35 89 pdi power distance index inv individualism a complete data for cyprus are not available appendix 4 results and discussions step 2 scaling up behavioral scenarios to national and eu level using the population projection scenarios for the eu28 we scale the dynamics in household energy behavioral changes in two provinces over time up to national and eu levels table a4 1 table a4 1 share of actions in two countries over time source scaled up bench v 3 results table a4 1 step 3 from regional to the national and eu28 economy to estimate the macroeconomic and cross sectoral impacts of individual energy behavioral changes we link the up scaled abm output to the cge eu ems model the bench v 3 behavioral patterns in each of the 12 age education groups changes in heterogeneous households electricity and gas consumption exogenously modify the minimum subsistence level of households consumption of the respective services in eu ems the analysis of eu ems results indicates that most of the eu28 regions benefit from the behavioral changes and lead to the decrease in energy consumption with a small number of regions being affected negatively importantly regions with larger population as well as the regions with higher share of highly educated people benefit more from the behavioral changes since they save more electricity and gas fig a4 1 diffusion of households investments per capita and per action insulation pvs installation energy efficient appliances among 12 sociodemographic groups under the informative dynamics scenario in two province source eu ems and bench v 3 fig a4 1 as expected pvs get more of a share of the investments in both countries figure a4 1 households in groups 6 8 invest 110 160 and 160 180 euros per capita on pvs in netherlands and spain respectively while insulation in spain 82 euros per capita and ee appliances in netherlands 37 euros per capita are second in household investments fig a4 2 percentage changes in the levels of regional real gdp relative to the baseline under the fd scenario in 2050 as an aggregated effect of households behavioral changes in millions of euros source eu ems and bench v03 fig a4 2 the eu ems model operates at the level of nuts2 regions of the eu28 and hence enables the calculation of the regional impacts of various behavioral scenarios on changes in the gdp and income the changes in income presents similar patterns as changes in real gdp see fig 6 however it is interesting that different pattern in percentage changes in regional gdp levels from the absolute monetary changes in regional gdp is captured see figure a4 2 the majority of relatively large changes in gdp are located in great britain italy and central europe this might be related to the assumed population and education level developments which influence the upscaling of the results of the bench abm model 
25943,households are responsible for a significant share of global greenhouse emissions hence academic and policy discourses highlight behavioral changes among households as an essential strategy for combating climate change however formal models used to assess economic impacts of energy policies face limitations in tracing cumulative impacts of adaptive behavior of diverse households the past decade has witnessed a proliferation of agent based simulation models that quantify behavioral climate change mitigation relying on social science theories and micro level survey data yet these behaviorally rich models usually operate on a small scale of neighborhoods towns and small regions ignoring macro scale social institutions such as international markets and rarely covering large areas relevant for climate change mitigation policy this paper presents a methodology to scale up behavioral changes among heterogeneous individuals regarding energy choices while tracing their macroeconomic and cross sectoral impacts to achieve this goal we combine the strengths of top down computable general equilibrium models and bottom up agent based models we illustrate the integration process of these two alien modeling approaches by linking data rich macroeconomic with micro behavioral models following a three step approach we investigate the dynamics of cumulative impacts of changes in individual energy use under three behavioral scenarios our findings demonstrate that the regional dimension is important in a low carbon economy transition heterogeneity in individual socio demographics e g education and age structural characteristics e g type and size of dwellings behavioral and social traits e g awareness and personal norms and social interactions amplify these differences causing nonlinearities in diffusion of green investments among households and macro economic dynamics keywords behavior change grassroots dynamics soft linking environmental modeling upscaling computational economics 1 introduction energy consumption is the primary culprit behind anthropogenic global warming humanity s demand for energy is satisfied by consuming fossil fuels as well as renewable energy sources leading to varied greenhouse gas emission ghgs footprints households are responsible for 70 of global ghgs hertwich and peters 2009 in europe one quarter of direct total energy consumption and ghgs comes from households 1 1 https climatepolicyinfohub eu node 71 pdf academic and policy discourses highlight behavioral changes among households as an essential strategy for reducing ghg emissions and combating climate change dietz et al 2013 doppelt et al 2009 faber et al 2012 mckinsey 2009 nielsen et al 2020 importantly an individual s decision making is known to deviate from rational and perfectly informed optimization process calling for a thorough understanding of behavioral aspects abrahamse and steg 2011 bamberg et al 2015 2007 poortinga et al 2004 stern 2016 van raaij 2017 policy makers rely on decision support tools to assess future changes in energy markets and the economy as a whole macroeconomic computable general equilibrium cge models serve as standard tools for quantitative policy assessments in climate change mitigation babatunde et al 2017 fujimori et al 2017 ipcc 2014 jrc 2014 rive et al 2006 vandyck et al 2016 cge models are popular among governments and academia for ex ante policy analysis they rely on advancements in micro based macro economic theory that represent the aggregate behavior of rational and fully informed economic agents households and firms and their trade interactions via supply chains behavioral changes including behavioral climate change mitigation actions driven by the increased level of knowledge about climate change in society and shifts in preferences are difficult to model directly with cge models this is one of the critics regarding their capacity to support climate change mitigation policy creutzig et al 2018 farmer et al 2015 farmer and foley 2009 isley et al 2015 niamir et al 2018b stern 2016 in contrast to this macroeconomic top down approach bottom up agent based models abms focus on behaviorally rich representation of energy consumers integrate technological learning out of equilibrium dynamics and social interactions bhattacharyya 2011 farmer et al 2015 hunt and evans 2009 niamir and filatova 2015 niamir et al 2018b tesfatsion 2006 agents in abms follow a set of if else rules sometimes combined with equations that guide their actions interactions with other actors or institutions e g markets and learning abms could compliment macro economic models by accommodating heterogeneity adaptive behavior and interactions bounded rationality and imperfect information filatova and niamir 2019 however their use for climate policy is hindered by high data intensity for individual behavioral rules and interactions when energy abms are grounded in empirical data their upscaling remains limited humphreys and imbert 2013 lamperti et al 2019 preventing the assessment of economy wide impacts effects of national or eu policies and generalization of abms results there is a long history in bridging top down cge models with bottom up models krook riekkola et al 2017 usually non abm specifically for energy macroeconomic models are linked with engineering micro simulation models focusing on the technological processes of electricity generation sue wing 2008 scholars either establish a soft link between micro and macro models or complement one by a reduced form of the other or combine them directly through hybrid modeling böhringer and rutherford 2009 since engineering bottom up models often rely on mathematical programming the latter approach focuses on resolving mixed complementarity problems bohringer and rutherford 2008 besides linking to engineering micro simulations national level cges rely on complimentary micro simulation models for environmental analysis taxation peichl and schaefer 2009 fiscal analyses debowicz 2016 and labor market analysis benczúr et al 2018 however an integration of micro macro approaches at the regional sub national level is scarce verikios and zhang 2015 in parallel as inequality and distributional impacts of climate change policies come into a spotlight internationally introducing heterogeneity into cge models becomes increasingly important bijl et al 2017 kulmer and seebauer 2019 melnikov et al 2017 rao et al 2017 van ruijven et al 2015 this is commonly done by disaggregating the representative agent in macro models with micro level survey data rausch et al 2011 duarte et al 2016 provide an excellent example on modeling of pro environmental consumer behavior in a regional cge model for spain using micro level data this study evaluates the impact of improving environmental awareness by specifying drivers of behavioral changes adoption of household appliances with different energy efficiency levels for different income levels using household survey data duarte et al 2016 while using survey data in cges is a major step in accommodating heterogeneity the choices that economic agents pursue remain fixed and are still assumed to be taken under conditions of perfect information it hinders the representation of behavioral changes bounded rationality and social influences so prominent in understanding pro environmental choices niamir et al 2020a steg and vlek 2009 linking macroeconomic cge models with micro level behaviorally rich abms can operationalize behavioral changes in formal policy analysis and open new synergies between micro and macro approaches krook riekkola et al 2017 melnikov et al 2017 parris 2005 safarzyńska et al 2013 smajgl et al 2009 earlier attempts to integrate abm and cge models include the work of safarzyńska et al 2013 who propose an elegant way to integrate the evolutionary dynamics of abms into a cge model yet authors leave it at the conceptual level without an implementation smajgl et al 2009 discuss a farm level integration of abm cge for fishery policy impact assessment with no integration results to the best of our knowledge there is no empirical example of resolving the key methodological differences between abm and cge modeling while aligning with survey data on behavioral heterogeneity the current paper addresses this methodological gap by demonstrating how aggregated impacts of household energy behavior changes emerging from an empirical abm could be scaled up and linked to the macroeconomic dynamics of a cge model to demonstrate the feasibility of the method we employ a soft linkage between the two empirical models future work will focus on a hard link integration following our earlier pilot on using software wrappers to assure a real time data exchange between toy abm and cge models belete et al 2019 here we ensure models consistency by aligning functional forms and by using the same database and economic scenarios the objective of this paper is twofold 1 to investigate feasibility of an original approach to link empirical abm and cge models while targeting individuals heterogeneity social interactions and behavioral changes and 2 to explore the impacts of climate change mitigation behavior across scales from individuals to the eu regions towards this end we propose a three step upscaling approach that goes beyond our specific application and may serve as a systematic way to link abm and cge models section 2 our results demonstrate that it permits tracing the macro economic and cross sectoral impacts and indirect effects of individual energy behavioral changes section 3 section 4 concludes with a discussion and outlining future work 2 methods to explore economy wide impacts of behavioral changes and the role of social interactions the current paper employs the strengths of micro and macro socio economic models we use an empirical behavioral abm bench v 3 originally developed to study cumulative impacts of individual changes in energy use niamir et al 2020b 2018a to trace indirect effects and cross sectoral impacts of shifts in residential energy demand and changes in households consumption behavior we employ an empirically calibrated cge model eu ems ivanova et al 2019 the scientific challenge is in aligning the two models that differ in key assumptions namely representative vs heterogeneous agents cge models work with a representative agent group while abms assume heterogeneity in attributes and behavior perfect vs bounded rationality agents in cge are assumed to be fully rational while abms proliferate in tackling research problems where bounded rationality is relevant static vs adaptive behavior households in cge have fixed preferences and perfect information while abm are designed to explicitly model adaptive expectations since abm agents do not have full information they learn over the course of a simulation either from their own experience from their social network or through market signals unique one shot equilibrium vs out of equilibrium dynamics cge models are solved via the assumption of a unique equilibrium occurring in one shot when markets clear in contrast abms trace the process of out of equilibrium dynamics and transitions between multiple equilibria while eliciting path dependencies 2 1 models and scenarios 2 1 1 the bench agent based model originally the bench abm niamir et al 2020b 2018a niamir and filatova 2017 was developed to investigate the role of behavioral changes with respect to an individual energy use in the transition to a low carbon economy households in bench abm are heterogeneous in socio demographic characteristics e g income age education dwelling characteristics e g type size age energy consumption patterns e g electricity and gas consumption energy provider and behavioral factors e g awareness personal norms social norms bench is spatially explicit with behavioral rules of agents calibrated based on the survey data for two eu nuts2 2 2 the nomenclature of territorial units for statistics abbreviated nuts is a geographical nomenclature subdividing the economic territory of the european union eu into regions at three different levels nuts 1 2 and 3 respectively moving from larger to smaller territorial units regions navarre spain and overijssel the netherlands niamir et al 2020a we advance this abm further to permit integration with the eu ems cge both in terms of the theoretical consistency of functional forms used in abm and cge as well as the datasets and scenario assumptions we start aligning the abm model with its macro counterpart by including the empirically estimated discrete choice functions for the representation of households investment decisions these functions stem from the utility optimization approach that is also used for the derivation of demand functions in the cge model and are further relaxed in the abm to accommodate bounded rationality namely agents utility functions are modified to align with empirically grounded energy decisions from the households survey niamir et al 2020a social interactions and learning with macroeconomic dynamics in our data driven cge model in particular benchv 3 focuses on energy investments that households may decide to undertake significant investments in house insulation i1 or moderate investment in solar panels i2 and modest investments in energy efficient appliances i3 fig 1 cognitive process behind individual behavioral changes in accordance with the theory of planned behavior and norm activation theory from psychology we assume that boundedly rational individuals in bench v 3 make decisions following a number of cognitive steps knowledge activation motivation and consideration niamir et al 2020a 2018a fig 2 shows heterogonous households in sociodemographic characteristics dwelling conditions electricity and gas consumption follow a cognitive process to decide whether to pursue any energy investment i1 i3 niamir et al 2018a describes how each individuals knowledge activation and motivation are measured and calculated at the model initialization stage based on the survey data in summary an individual knowledge activation level is calculated based on the average of three types of knowledge person s climate energy environment knowledge k awareness about climate environment and energy issues a c and energy decision a e if this average for an individual is above the empirical threshold then the person is tagged as feeling guilt and proceeds to the next step to assess his her motivation for actions i1 i3 such individuals proceed to evaluate the motivational factors personal and social norms n p n s for each action i1 i3 if individuals are highly motivated and feel responsible the perceived behavior controls 4 3 photo sources i1 by tracey nicholls cc by 3 0 i2 by enrix knuth cc by sa 4 0 i3 by tommaso sansone91 cc0 available from https commons wikimedia org 4 own perception of their ability to perform an action or change behavior pbc and the dwelling ownership status owner or renter are evaluated to assess intentions individuals with a high level of intention proceed to estimate utilities which are formulated as a discrete choice problem here household agents follow these stages for each action when deciding whether to invest in insulation solar panels or energy efficient appliances households in bench v 3 make choices based on the indirect utility function eq 1 as the inverse of the expenditure function when prices are constant it reflects individual preferences for different energy actions under budget constraints eq 1 v i j x i j β i ε i j the utility of individual j associated with choice i v i j is calculated based on the vector of explanatory observed and latent variables x i j including socio economic characteristics of the individuals dwelling characteristics and financial and ownership situation as well as behavioral factors and the parameter vector β i estimated using a probit regression niamir et al 2020a finally ε i j is the vector of error terms an individual chooses a particular sub action i when their utility is non negative eq 2 i f v i j 0 i i j t r u e e l s e i i j f a l s e social interactions and learning the speed of green investments diffusion does not depend only on social interactions that affect updating of knowledge awareness and norms it depends also on the individual heterogeneity socio economic characteristics or dwelling characteristics which affect utility of taking an action i1 i3 i e serve as proxy for the perceived behavior control pbc in bench v 3 agents exchange information following a simple opinion dynamics model moussaïd et al 2015 when a neighbor takes an action i1 i3 it may alter knowledge awareness and the motivational factors regarding energy choices of others in this peer group namely individuals compare own behavioral factors k a c a e n p n s pbc with those of their closest neighbors and gradually adjust them fig 3 eq 3 we run various scenarios of this social learning see section 2 1 3 our abm uses the same baseline scenario of regional demographic and economic development as the cge model ensuring the consistency between the scenario analysis in two models further the abm takes as inputs data on the regional gdp projections estimated for 2015 2050 by the cge model the detailed description of the bench agent based model is presented in appendix 1 2 1 2 computable general equilibrium model eu ems ivanova et al 2019 is a spatial cge model developed by the pbl netherlands environmental assessment agency for policy impact assessments the current version of eu ems covers 276 nuts2 regions across the eu28 member states goods and services are produced by firms and consumed by households or other firms and exchanged on competitive markets spatial interactions between regions are captured through the trade in goods and services factor mobility and knowledge spill overs following the tradition of comprehensive empirical cge models eu ems uses large datasets of real economic data in combination with complex computational algorithms to assess how the economy reacts to changes in governmental policy technology availability of resources and other external macro economic factors the eu ems model consists of a the system of non linear equations which describes the behavior of various economic actors and b a very detailed database of economic trade environmental and physical data the core part of the model database is the social accounting matrix which represents in a consistent way all annual economic transactions the database 5 5 http themasites pbl nl winnaars verliezers regionale concurrentie of the model has been constructed by pbl using the combination of national european and international data sources and represents a detailed regional level nuts2 for eu28 plus 34 non eu countries multi regional input output mrio table for the world the main datasets used for the construction of this mrio include the 2013 oecd database baci trade data eurostat regional statistics and national supply and use tables as well as the detailed regional level transport database of dg move called etis plus 6 6 http viewer etisplus net the later dataset allows us to estimate the inter regional trade flows at the level of nuts2 regions that are currently not available from official statistical sources the aggregated groups of the sectors can be directly linked to the panel data econometric analysis and estimations that have been done for total factor productivity tfp projections using the eu klems database 7 7 http www euklems net we have used panel data techniques on eu klems data in order to model the development of tfp according to the technological catch up theory the detailed description of our cge model is presented in appendix 2 measuring economic inequality economists often measure regional disparities using theil s t inequality index eq 3 the absolute value of which indicates the distance from equality eq 3a t h e i l t θ i i θ i i 1 n log γ i μ where θ o is the gdp of each nuts2 region γ i is the gdp per capita in each region as a measure of regional income and μ is the average gdp per capita across the eu28 nuts2 regions the eu ems cge model estimates the cross sectoral aggregated impacts of individual behavioral changes produced by the abm and traces the consequent changes across the eu regions triggered by the macro economy the cge receives measures a the diffusion of each of the three types of actions i1 i3 among heterogeneous households classified in 12 age and education groups b the changes in electricity and gas consumption c saved co2 emissions and d the amount of investment from bench model results 2 1 3 scenarios micro level end user behavioral scenarios besides being heterogeneous in terms of sociodemographic characteristics e g age income education housing they reside in e g tenure status size energy label and psychological factors e g attitudes and beliefs personal norms agents in the bench v 3 abm exhibit heterogeneous behavioral characteristics such knowledge and awareness engage in social interactions and learn bench v3 abm introduces three end user behavioral scenarios baseline fd id by differentiating between the intensity of social interactions and the speed of learning see table 1 based on the neighborhood size this social learning may occur at either a slow or fast pace see scenarios in appendix 1 macro level scenarios in addition to these three behavioral scenarios the eu ems cge model relies on the demographic projections from eurostat until 2050 and total factor productivity tfp projections by economic sector based on our own econometric analysis hence the macroeconomic and demographic scenarios are combined with the slow fast informative dynamics scenarios of micro level behavior with respect to energy related investments of heterogeneous households 2 2 upscaling behavioral changes abm and cge models each have their own assumptions strength and weaknesses we attempt to overcome the latter by linking the two models to pursue this in a systematic manner we take a step wise approach to bridge the abm with the cge model fig 4 2 2 1 step 1 from individual households to regional shifts in energy use bench v 3 abm calculates the extent of behavioral changes among heterogeneous household agents who evolve through a cognitive process section 2 1 1 fig 2 before reaching a more rational stage where the discrete choice utility maximization is activated section 2 1 1 eqs 1 and 2 given the stochastic nature of abms we use the mean values from 100 abm simulations run for each scenario and case study to feed them further into the cge model the main outcomes of the bench v 3 abm used in the eu ems cge model are the relative changes in electricity and gas use and the total investments made by various individuals i1 i3 the eu ems cge model however operates at the level of all 276 eu28 nuts2 regions and needs regional changes in energy consumption and investments of the representative households as an input hence the behavioral patterns emerging at the overijssel and navarre provinces for different households need to be scaled not only up to the national level but up to the entire eu see next steps and fig 4 2 2 2 step 2 dynamic socio demographic groups with similar behavioral patterns we take an intermediate step to derive the changes in investments gas and electricity consumption across households of different age and education levels for all 276 eu28 nuts2 regions based on the outcomes of two regional abms economic theory suggests that investment choices depend on households incomes however our survey on behavioral changes regarding energy use niamir et al 2020a reveals that age and education are the most important factors explaining households preparedness to invest in low carbon energy i1 i3 8 8 with the help of our empirical data we examined the impact socio demographic factors namely income gender education and age on households energy bahavior changes in two provinces overijssel nl and navarre es particulary our analysis shows the probability of households energy behavior increases with the level of eduction 95 confidential interval niamir et al 2020a thus we define behavioral patterns for a group of households in the dutch and spanish regional abms separately aggregating by age and education level following the eurostat classification we work with 12 age education groups table 2 for all 12 groups we estimate a number of households pursuing an action i1 i3 and calculate the corresponding average gas and electricity savings and investments the behavioral factors awareness motivations intentions and likely actions across 12 groups differ between the two countries in our survey sample and so do the patterns of behavioral climate change mitigation emerging in the abms to utilize the information regarding regional differences in patterns of behavioral change we create the mapping between nuts2 regions of the eu28 with the two abm regions according to their perceived cultural distance social structure wealth and lifestyle religion institutional and economic conditions and natural environment play a role in assessing cultural distance gobel et al 2018 hofstede 2011 2001 kaasa et al 2016 schwartz 2014 vignoles et al 2018 specifically in the absence of more granular data we use the dutch case to approximate how the behavioral patterns may evolve in the north west eu states and the spanish case for the south east eu states see table a3 1 in appendix 3 we acknowledge that this approach does not fully capture all the cultural differences but it for example accounts for the role of social network higher among the spanish respondents compared to the dutch in behavioral climate change mitigation ideally one should use native survey data regarding the modeled behavior or employ secondary data on revealed empirical differences on behavioral changes across regions furthermore differences in policy institutional technological and environmental conditions across eu countries are indirectly accounted for in our cge model and the databases it relies upon since behavioral changes vary primarily among households with different age and education levels the changes in these characteristics over time are crucial hence we employ demographic projections for the period until 2050 the only regional nuts2 level projections that have been done for the eu28 are europop2008 9 9 https ec europa eu eurostat documents 3433488 5564440 ks sf 10 001 en pdf d5b8bf54 6979 4834 998a f7d1a61aa82d projections of eurostat population projections of eurostat provide information about the development of the population until 2050 detailed by age and gender groups furthermore eurostat population projections at nuts2 level are combined with iiasa global education trends scenario projections 10 10 http www iiasa ac at web home research researchprograms worldpopulation projections 2014 html related to the share of high medium and low educated persons in each eu country this allows us to construct population projections by age and education level for the period 2020 2050 for each nuts2 region of the eu28 these nuts2 level population projections till 2050 match with the scaled up mapping of behavioral patterns of 12 groups in our abm hence now we use age and education information to linked it with the emerging behavioral patterns of the agent based bench v 3 model when creating nuts2 specific that is corresponding to the population structure of that region inputs into the spatial eu ems cge model 2 2 3 step 3 cumulative economy wide impacts of behavioral changes finally we use the predicted population structure by age and education level for the period 2020 2050 to calculate aggregated changes in the residential use of gas and electricity for each nuts2 regions of eu28 on the basis of calculated averages for each of the 12 individual groups the eu ems cge model estimates the cross sectoral impacts of these shifts in the aggregated residential energy demand that impacts gdp projects the linked abm cge model quantifies the cumulative impacts of behavioral changes among heterogeneous households at the level of 276 eu28 nuts2 regions this allows us to understand the impacts of various behavioral scenarios within the cge framework including distributional effects across these eu regions an important direction of future work would be to develop direct two way linkages between the two models with the cge generated gdp projections feeding back into the abm data flows between two models are presented in fig 4 this step wise approach to linking the abm and cge models allows us to address the key methodological challenges from representative to heterogeneous agents heterogeneous households in the abm are matched with representative households in the cge model aggregation occurs along the two dimensions that impact relevant behavioral changes among households most age and education levels this is done using detailed information about the structure of the population by age and education in each nuts2 region for the period 2020 2050 while keeping behavior heterogeneous across the 12 groups from perfect to bounded rationality agents in our abm are boundedly rational due to the presence of behavior factors k a c a e n p n s pbc that precede discrete choice utility estimate subjective knowledge and awareness motivation and intention to consider a change in behavior which are all prone to social influence the use of the abm allows us to assess the impacts of pure behavioral changes in the cge model and calculate their broader economic impacts the rest of the economy in the cge model e g households decisions on a labor market decisions of firms clearing of the markets still operates in line with the rationality principles allowing for the coherent treatment of macro economic processes in the cge model from static to adaptive agents agents in the abm are prone to social influence and learn from their neighbors as their behavior attributes knowledge and awareness evolve they go through various cognitive stages of knowledge activation motivation and consideration and may eventually decide to invest in low carbon energy by scaling up these behavioral patterns through age education groups we are able to link to the architecture of a cge by default cge models assume perfect information and rational expectations omitting a variety of behavioral strategies through which adaptive behavior can be channeled into macro dynamics from an equilibrium to adaptive dynamics with social learning the cge model is based on assumptions of market equilibrium and interlinkages between different agents sectors and markets in the economy the abm treats agents decisions as a cognitive process in the presence of social interactions and fast slow informative learning before discussing the results it may be useful to be explicit about the limitations of the current study the presented cge to abm link is currently indirect operationalized via the eu gdp growth rates scenarios the dotted curve in fig 4 furthermore to demonstrate the applicability of method we work with two survey datasets for a real policy analysis it is essential to work with a richer representation of regions that may also account for differences in climatic and institutional conditions across countries while our abm relies on households surveys niamir et al 2020b 2020a 2018a for micro validation macro validation against regional level panel data remains a subject of future work we believe that micro validation is sufficient for the methodological demonstration of the applicability of this approach for upscaling behavioral climate change mitigation complementing it with macro validation would be essential when performing a real policy analysis 3 results and discussion given the stochastic nature of abms we run bench multiple times under the same parameter settings for each scenario the abm results presented below plot the means across 100 random runs therefore we use the mean values from each abm scenario and case study to scale up the observed behavioral patterns and to estimate their cross sectoral impacts in the cge model 3 1 step 1 from behavioral patterns in survey data to cumulative impacts in two provinces firstly we run the bench v3 abm for two eu provinces overijssel and navarre under the three behavioral scenarios baseline fd and id we report the regional impacts of the energy behavior choices of heterogeneous households the diffusion of each of the three types of behavioral actions among heterogeneous households over time the changes in electricity and gas consumption saved co2 emissions and the amount of investment fig 5 illustrates the dynamics of electricity and gas saving in the two eu provinces as a result of households energy investments the general trend is as expected faster learning boosted by an information campaign leads to more investments in solar panels i2 and in appliances i3 and consequently to higher electricity savings in both provinces intensive social learning boosts electricity savings by 40 and 100 in overijssel and navarre fd vs baseline fig 5 a and table 3 in addition electricity savings increase by 14 and 22 in two provinces if pro environmental awareness is raised through an information policy id vs fd fig 5 a and table 3 however these trends do not hold for investments in insulation i1 and corresponding gas savings informative strategy id has a mixed impact on insulation investments in navarre crossing of fd and id curves in fig 5b and the opposite effect in overijssel id delivers 26 lower gas savings compared to fd fig 5b the difference between cases may be driven by initial conditions climate institutional settings gas prices in the two countries in addition comparing fd and id scenarios shows that an information policy and social interactions among neighbors impact households insulation decisions in a non linear way table 3 shows the amount of co2 emission savings that households energy behavior changes could deliver and at what investment cost intensive social interaction fd scenario leads to 1 4 and 2 times more saved co2 emissions in overijssel and navarre compared to the baseline as expected information policy along with social interactions id scenario amplify the impact 1 1 and 1 2 times more on top of the fd scenario in overijssel and navarre respectively we observe a non linear pattern in total investments euro per households under behavioral scenarios over time when information policy id scenario is activated dutch households invest 17 more compared to the fd scenario in 2020 and this then drops in 2050 20 less than the fd scenario spanish household investments in the id scenario increases up to 33 in 2030 and then drops by 5 compared to the fd scenario these nonlinearities emerge from households preferred actions i1 i3 unequally distributed over time and space these results are a pure effect of individual changes driven by behavioral factors we do not include any price based scenarios subsidies for green or taxes on grey energy or changes in technological costs in this article our analysis confirms that faster learning boosted by an information campaign fd vs baseline scenarios leads to more investments i2 i3 and consequently to higher electricity savings 40 100 in both provinces in addition electricity savings increase by 14 22 in two provinces if pro environmental awareness is raised through an information policy id vs fd scenarios however id has a mixed impact on insulation investments i1 and gas consumption in navarre and the opposite effect in overijssel id delivers 26 lower gas savings compared to fd 3 2 step 2 scaling up behavioral scenarios to national and eu level after analyzing the dynamics in households behavioral changes in two provinces over time we switch to understanding how they change over space using the population projection scenarios for the eu28 see section 2 2 step 2 we scale the dynamics in household energy behavioral changes in two provinces over time up to national and eu levels namely we define behavioral patterns for a heterogeneous group of households in the dutch and spanish regional abms for each of the 12 age education groups table 2 a number of households perusing an action i1 i3 is estimated together with the average investments and gas and electricity savings the analysis reveals that in the netherlands and spain that the majority of households 75 9 and 68 1 intend to invest in energy efficient appliances i3 by 2050 the minority 4 9 and 9 4 want to invest in insulation i1 this trend is stable over time 2020 2050 electricity consumption resulting from individual behavioral changes decreases between 51 and 71 the netherlands and 51 66 spain by 2050 see appendix 4 table a4 1 fig 6 shows percentage changes in residential electricity consumption as a result of scaling up the output of the empirical abm with the population change scenario electricity consumption resulting from individual behavioral changes decreases between 56 2 69 5 and 13 8 63 8 by 2050 in the netherlands and spain correspondingly importantly there is significant spatial heterogeneity in how behavioral changes diffuse and what regions emerge as laggers or pioneers in bottom up investments in energy efficiency if behavioral patterns elicited through our survey hold in the next few decades it could be expected that the limburg drenthe and zeeland provinces in the netherlands and the castile leon and asturias regions in spain will be pioneers compared to others in respective countries 3 3 step 3 from regional to the national and eu28 economy scaled up outputs of the abm are used as input to the simulation setup of the spatial cge model namely information from bench v 3 on the decrease in households use of electricity and gas is used in order to exogenously modify the minimum subsistence level of households consumption of the respective services in eu ems see appendix 2 the abm cge results indicate that households with higher education levels are more likely to change their behavior compared to less educated people importantly among these higher educated households younger people 20 40 are more active in particular dutch youth saves up to 17 and 74 more electricity and gas compared to 40 households under the fd scenario fig 7 among the pioneers g6 8 i e middle educated and 20 age see table 2 spanish households save 1 9 2 8 and 1 0 1 4 times more gas and electricity compared to dutch households depending on groups and behavioral scenarios intensive social dynamics fd scenario has a stronger impact on saving gas while the informative id scenario activates more households in saving electricity appendix 4 presents a more detailed abm cge analysis on diffusion of households investment per capita per action among sociodemographic groups a reduction in the consumption of gas and electricity by households results in a higher budget share that becomes available for other types of consumption depending on households consumption patterns such shifts in consumption might result in higher values of gdp over time the eu ems model operates at the level of nuts2 regions of the eu28 and hence enables the calculation of the regional impacts of various behavioral scenarios on real gdp that is gdp that includes only quantity effects we choose to use gdp in our analysis instead of welfare indicators such as equivalent variation measure because the monetary indicator such as gdp can be easily compared with the outcomes of the abm model in terms of monetized energy savings and investments the focus of the present study is in illustrating the added value of the use of cge model and the degree of the indirect and economy wide effects calculated by the cge which justifies the choice of monetary gdp indicator for our analysis fig 8 illustrates the difference in regional real gdp levels in 2050 between the baseline and fd scenarios most of the eu28 regions benefit from the behavioral changes which leads to a decrease in energy consumption with a few regions affected negatively the level of overall real gdp impacts depends on the size of the region in terms of population and its share of highly educated youth appendix 4 presents the percentage changes on the level of regional gdp relative to the baseline scenario see figures a4 2 fig 9 presents the effects in relative terms scenario as of the baseline which already accounts for whether a region is rural or urban and relate them to gdp per capita it implies there is a statistical relationship between the two variables the baseline gdp per capita which is also positively correlated with the share of highly educated persons and the benefits in terms of additional economic growth per capita from the modeled behavioral changes though the relationship is non linear the trend indicates that rich and economically well developed regions receive higher benefits from promoting behavioral changes in the long run compared to the lagging regions this phenomena raises the question of whether the distribution of economic benefits skewed towards rich and well developed regions increases the overall interregional inequality in europe to understand how behavioral changes under our scenarios impact eu28 regional disparities we calculate economic inequality index for the period 2015 2050 section 2 1 2 eq 3 the dynamics of theil s t inequality index demonstrate that the inequality between regions decreases in the period of large investments in energy savings 2025 2035 and then starts to increase again over time indicating the non linear nature of the process fig 10 however the regional inequality in 2050 does not reach the level of 2015 indicating the positive overall impact of behavioral changes on equality despite this changes in inequality due to the implementation of behavioral scenarios remain modest 4 conclusions and outlook the potential of individual behavioral changes in reducing carbon emissions attracts considerable attention as one of the climate change mitigation strategies creutzig et al 2016 ipcc 2014 niamir 2019 comprehensive empirical cges which support quantitative climate change mitigation policy assessments are strong in tracing cross sectoral impacts feedback in the economy as a whole and in linking to readily available datasets however their econometrically estimated equations reflect past behavior making it difficult to integrate behavioral changes babatunde et al 2017 farmer and foley 2009 moreover while empirical evidence suggests that individual decision making deviates from a rational and perfectly informed optimization process the latter is the core of cge models farmer et al 2015 stern 2016 wilkerson jerde and wilensky 2015 abms compliment macroeconomic models by accommodating heterogeneity adaptive behavior and interactions bounded rationality and imperfect information rai and henry 2016 while there are few largely non empirical abms in policy and institutional domain that take a macro e g country and global scale perspective castro et al 2020 gerst et al 2013 behaviorally rich empirical abms mostly operate on small scales of neighborhoods cities and regions although these micro abms are strong in aggregating heterogeneous adaptive behavior they omit feedbacks with the rest of the economy and cross sectoral impacts survey data is increasingly used to specify individual agent s rules yet this behavioral data is not always compatible with the data used in macro models linking abms and cge models could ameliorate their weaknesses yet the models should be aligned coherently conceptually and data wise to benefit from their strengths voinov and shugart 2013 methodologically this article contributes to the ongoing debate krook riekkola et al 2017 parris 2005 safarzyńska et al 2013 smajgl et al 2009 on linking these two alien approaches by presenting a method of systematic upscaling of individual heterogeneity and social dynamics to combine abm and cge models the insights from this methodological exercise offer three conclusions firstly we demonstrate the feasibility and importance of introducing heterogeneity and behavioral rich dynamics in assessing climate change mitigation policies we develop a transparent step wise process to integrate an empirical behaviorally rich abm and a spatial cge model to the best of our knowledge this is the first attempt to link empirical abm and cge models to estimate the macroeconomic impacts of individual energy behavioral changes in the absence of this integration one should twist the cge parameters and structure in an ad hoc manner to permit some representation of a behavioral change instead an abm that relies strongly on the theoretical and empirical micro foundations from surveys quantifies the patterns of behavioral change across heterogeneous households in a transparent way accounting for non monetary aspects of individual energy choices secondly this article demonstrates that scaling up behavioral change dynamics has policy relevant consequences at large scales our abm grounded in theory and survey data quantifies the patterns of behavioral change which could further be channeled into the cge models that traces macroeconomic and cross sectoral dynamics specifically here we find that the regional dimension is important in a low carbon economy transition driven by individual behavioral change some regions lag behind while others are pioneers due to the heterogeneity in individuals socio demographics e g education and age structural characteristics e g type and size of dwellings behavioral and social traits and spatial characteristics e g urban vs rural which produce incremental differences at small scales yet when aggregated they cumulatively create disparities which are amplified by macro economic forces importantly the inequality between regions decreases in the period of large investments 2015 2035 and starts to increase over time following it finally as behavioral barriers to climate change mitigation in designing policies gain attention policy makers would benefit from decision support tool that go beyond a stylized representation of households as perfectly informed optimizers individual awareness diversity in norms and knowledge play a key role in a green economy transition and climate change mitigation policies should ideally combine the conventional macroeconomic analysis with these behavioral barriers and drivers considering bottom up behavioral patterns would not easily change over time to see substantial changes we need a mix of external intervention from soft information policies aimed to raise awareness bottom up to financial incentives altering the macro landscape of energy markets and technological transitions at times information and price based policies create a non linear effect on cumulative behavioral changes regarding energy use niamir et al 2020b our approach demonstrates that with computational abm directly linked to survey data and macroeconomic cge models individual behavioral heterogeneity and social influences can now be considered when designing implementable and politically feasible policy options the future work can go in two main directions advancing the modeling approach and improving the models dataset from the modeling perspective future work could focus on introducing direct feedbacks between cge abm enabling the evaluation of price based and information policies jointly at multiple scales the feedbacks between the two empirical models may be enabled through software wrappers and modern web interfaces for integration belete et al 2019 in addition due to the large number of parameters and multidimensionality of the generated data from any abm lee et al 2015 the global sensitivity and uncertainty analysis was out of scope of this article future work should focus on quantifying uncertainties that this integration of abm and cge models may impose including for example exploratory analysis kwakkel and pruyt 2013 to understand the integrated model s behavior and its sensitivity to initial configurations of its parameters from the dataset perspective running surveys in more eu countries would improve the model accuracy especially vital when predicting policy impacts also data wise the behaviorally rich demand side modeling could benefit from endogenizing the dynamics of dwelling stock static and aging housing should be replaced by scenarios of structural and technological progress in new urban development e g zero carbon footprint buildings and refurbishing old housing stock in cities data availability the extensive description of the models and data is presented in the appendix of this manuscript the bench model is calibrated based on the empirical dataset we designed and conducted the survey in two provinces in europe for the purpose of this research niamir et al 2020a the agent based bench model is parameterized using the survey data on socio demographic economic structural and behavioral attributes of households and their dwelling characteristic table a1 1 the bench agent based model is open source and available on comses https www comses net the main database of eu ems model is the pbl jrc world wide mrio database documented in https ec europa eu jrc sites jrcsh files jrc115439 pdf and available to download from https data overheid nl dataset pbl euregio database 2000 2010 besides this mrio database we have also used the national accounts data from eurostat research project rpp 342 2016 csis eu silc hbs lfs and oecd for the construction of social accounting matrices used to calibrate the model according to the terms of use authors are not allowed to redistribute the eurostat micro data the derived intermediate result are available from the corresponding author upon reasonable request declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was supported by the eu fp7 complex knowledge based climate mitigation systems for a low carbon economy project no 308601 the scalar european research council erc project grant agreement no 758014 and the iiasa rite distinguished young scientist award recipient leila niamir we are thankful to prof dr alexey voinov university of technology sydney australia and prof dr hans bressers university of twente the netherlands for their feedback and support the authors would like to thank the tns nipo team for their collaboration in conducting the household survey we also appreciate the participation of the survey respondents and the constructive feedback we received from the three anonymous reviewers appendix 1 bench agent based model the bench abm niamir et al 2020b 2018a is developed to study shifts in residential energy use and corresponding emissions driven by behavioral changes among individuals main processes of the model odd protocol table a1 1 bench v 3 abm odd protocol table a1 1 guiding protocol the bench v 3 model a overview a 1 purpose the bench v 3 agent based model is designed to study shifts in residential energy use and corresponding emissions at the regional level driven by behavioral changes among heterogeneous individuals this empirically grounded model is of interest to i environmental scientists interested in modeling human behavior and economic institutions ii energy economists working on micro aspects iii scholars integrating individuals behavioral change in climate change mitigation modeling a 2 entities state variables and scales agents individuals in bench v 3 model are heterogeneous in socio demographic and dwelling characteristics energy consumption and patterns source of energy and energy provider and behavioral factors the bench v 3 simulations 1035 and 755 individual households in the overijssel province the netherlands and navarre province spain over 34 years 2016 2050 one time step represents one round in the behavioral experiments each run consist of 34 time steps aligning to the 34 rounds in the behavioral experiments a 3 process overview one time step represents one year in each time step a household goes through several processes 1 asses behavioral factors knowledge activation motivation consideration 2 calculate utilities 3 pursue an action or not 4 calculate saved energy and co2 emission 5 social dynamics and learning process 6 satisfaction and regret 7 updates see fig 2 for algorithm and decision making proccess in the bench v 3 agent based model b design concept b 1 theoretical and empirical background in application to environmental and energy related choices three behavioral change theories are commonly applied theory of planned behavior tpb norm activation theory nat and value belief norm vbn theory tpb formulated by ajzen 1980 and based on the theory of reasoned action is one of the most influential theories in social and health psychology and has been used in many environmental studies armitage and conner 2001 onwezen et al 2013 nat originally developed by schwartz 1977 operates in the context of altruistic and environmentally friendly behavior it is mostly focused on anticipating pride in doing the right thing and on studying the evolution of feelings of guilt vbn theory stern et al 1999 stern 2000 explains environmental behavior and good intentions such as willingness to change behavior nordlund and garvill 2003 steg and vlek 2009 stern et al 1999 environmental citizenship stern et al 1999 and policy acceptability de groot and steg 2009 steg et al 2005 b 2 individual decision making we introduce a framework that combines the strengths of the three key behavioral theories see figure a1 1 b 3 heterogeneity agents are heterogeneous in respect of the following variables see table a1 2 socio demographic dwelling energy consumption energy provider behavioral factors b 4 interactions social dynamics and learning agents heterogeneous individual households engage in interactions and learn from each other in particular they can exchange information with neighbors which may alter own knowledge awareness and motivation regarding energy related behavior we employ a simple opinion dynamics model acemoglu and ozdaglar 2011 degroot 1974 hegselmann 2002 moussaïd et al 2015 assuming that each agent interacts with a fixed set of nearby neighbors the bench v 3 model is a spatially explicit model that takes the raster maps of the two nuts2 regions as an input hence an agent who is in active neighborhood where at least one out of eight nearest spatial neighbors within 1 raster cell moor neighborhood concept undertakes an energy related action will interact and exchange opinions the idea of the moore neighborhood comes from cellular automata literature and used only to enable opinion exchange between neighbors about climate and environmental awareness and compare norms agents compare values of their own behavioral factors knowledge awareness and motivation with those of their eight closest neighbors and adjust their values for a closer match see fig 3 and eq 3 however the agents heterogeneity beyond their spatial location income age education and economic factors affect individual choices of undertaking any of energy actions i1 i3 or not b 5 spatial scale lowest scale individualshighest scale nuts2the focus of this research is on overijssel the netherlands nl21 and navarre spain es22 nuts2 regions which consist of 25 and 10 main cities municipalities respectively b 6 individual prediction individuals do not predict future condition b 7 stochasticity there are various sources of stochasticity in the model 1 initial setting agents attributes initialization are partly random 2 during the process social dynamics and learning process is partly random b 8 observation bench v 3 estimates cumulative impacts of energy related behavioral changes of individual households on electricity and gas consumption and co2 emissions reports number of energy related actions per year investment conservation switching saved electricity and gas per action year investment conservation switching avoided co2 emission per action year investment conservation switching across socioeconomic age and education groups see table 1 and cases nl vs es b 9 implementation details the model is coded in netlogo 6 0 4 open source and available on comses https www comses net r is used for the result visualizations c details c 1 initialization the variations in socio demographic dwelling and psychological factors among our survey respondents are used to initialize a population of heterogeneous agents in the bench v 3 model see table a1 1 and a1 3 c 2 input data the data on the behavioral and economic factors affecting household energy choices were collected using an online questionnaire n 1790 households and serve as empirical micro foundation of agent rules in the bench v 3 model fig a1 1 bench v 3 conceptual behavioral framework source niamir et al 2020a fig a1 1 table a1 2 overview of main variables and parameters used in bench v 3 table a1 2 factors variables value range socio demographic income 1000 150 000 education primary doctoral dwelling energy label a f ownership status owner renter energy consumption 500 5000 provider grey brown green energy saving habit 0 3 behavioral knowledge 1 7 cee awareness 1 7 ed awareness 1 7 personal norms 1 7 social norms 1 7 intention a1 1 7 intention a2 1 7 intention a3 1 7 data the bench v 3 model is calibrated based on an empirical dataset we designed and conducted the survey in two provinces in europe for the purpose of this research in 2016 1035 households in the overijssel province the netherlands and 755 households in the navarre province spain filled out our online questionnaire niamir 2019 niamir et al 2020a niamir and filatova 2017 2016 the agent based bench v 3 model is parameterized using the survey data on socio demographic economic structural and behavioral attributes of households and their dwelling characteristic table a1 3 table a1 3 survey data on households characteristics and behavioral intentions the data is used to parameterize households behavior in the bench v 3 abm source niamir et al 2020a 2018a table a1 3 factors overijssel navarre socio demographic characteristics gender female 46 4 male 53 6 female 57 1 male 42 9 age years 53 41 education isced image 5 image 6 annual income in thousand euros per year image 7 image 8 dwelling characteristics type of residence apartment 14 9 house 85 1 apartment 77 8 house 22 2 tenure status owner 71 renter 29 owner 80 3 renter 19 7 size of residence image 9 image 10 age of residence image 11 image 12 behavioral characteristics value on the 1 7 scale knowledge k 4 2 0 7 5 0 0 8 awareness climate a c 4 9 0 8 5 4 0 8 awareness energy decision a e 4 5 1 0 5 3 1 1 personal norms n p 4 6 0 9 5 4 1 0 social norms n s 3 3 1 1 4 5 1 2 perceived behavior control pbc 4 4 1 1 5 0 1 3 https ec europa eu eurostat statistics explained index php international standard classification of education isced outputs the agent based bench v 3 model tracks the individual and cumulative impacts of three energy behavioral changes investments on insulation pvs installation and energy efficient appliances among heterogeneous individuals in the overijssel and navarre provinces over 34 years 2016 2050 we report the number of individuals pursuing a particular action i1 i3 the cumulative electricity and gas consumption and saved carbon emissions given the stochastic nature of abms we perform multiple n 100 repetitive runs of each simulation experiment lee et al 2015 appendix 2 spatial eu ems cge model general description eu ems is a spatial computable general equilibrium scge model developed by pbl netherlands environmental assessment agency the sectoral and geographical dimensions of the model are flexible and can be adjusted to the needs of a specific policy or research question the model is used for policy impact assessment and provides sector region and time specific model based support to dutch and eu policy makers on structural reforms growth innovation human capital and infrastructure policies the current version of eu ems covers 276 nuts2 regions of the eu28 member states and each regional economy is disaggregated into 63 nace rev 2 economic sectors 11 11 https ec europa eu eurostat documents 3859598 5902521 ks ra 07 015 en pdf goods and services are consumed by households government and firms and are produced in markets that can be perfectly or imperfectly competitive spatial interactions between regions are captured through trade of goods and services factor mobility and knowledge spill overs this makes eu ems particularly well suited for analyzing policies related to human capital transport infrastructure r i and innovation in the current application of the model we have aggregated the economic sectors to the following six large groups following the eurostat classification of the economic sectors according to their r d intensity 1 traditional 2 low tech industry 3 medium tech industry 4 high tech industry 5 knowledge intensive services and 6 other services main processes of the model eu ems accounts for the a feedback between price and demand supply quantities and b interactions between economic agents at the macro and sectorial level therefore it gives the economic relations between all industry sectors via their intermediate use the eu ems model is a dynamic recursive over time model involving dynamics of capital accumulation and technology progress stock and flow relationships and adaptive expectations the model equations are neo classical in spirit assuming cost minimizing behavior by producers average cost pricing and household demands based on optimizing behavior the cge model database consists of tables of transaction values and elasticities dimensionless parameters that capture behavioral response the database is presented as a social accounting matrix which covers an entire national economy and distinguishes a number of sectors commodities primary factors and types of households as a classical cge model eu ems represents the behavior of the whole population group or of the whole industrial sector as the behavior of one single aggregate agent it is further assumed that the behavior of each such aggregate agent is driven by certain optimization criteria such as maximization of utility or minimization of costs in following detailed representation of the eu ems model and its main equations are presented fig a2 1 circular economic flow in the cge eu ems model source ivanova et al 2019 fig a2 1 regional structure of the model regions differ by the type of production sectors which dominate overall production activities in the region some specialize in traditional sectors such as agriculture whereas others specialize in modern sectors such as finance and industry those sectors are characterized by different levels of agglomeration and its importance traditional sectors do not experience any agglomeration effects whereas modern sectors do this allows some sectors to grow faster than other the prototype model will incorporate the regional difference in sectoral specialization and hence the difference of agglomeration economies between the regions table a2 1 regions in eu ems cge models source ivanova et al 2019 table a2 1 code name code name aus australia arg argentina aut austria bgr bulgaria bel belgium bra brazil can canada brn brunei darussalam chl chile chn china cze czech republic chn dom china domestic sales only dnk denmark chn pro china processing est estonia chn npr china non processing goods exporters fin finland col colombia fra france cri costa rica deu germany cyp cyprus grc greece hkg hong kong sar hun hungary hrv croatia isl iceland idn indonesia irl ireland ind india isr israel khm cambodia ita italy ltu lithuania jpn japan lva latvia kor korea mlt malta lux luxembourg mys malaysia mex mexico phl philippines mex gmf mexico global manufacturing rou romania mex ngm mexico non global manufacturing rus russian federation nld netherlands sau saudi arabia nzl new zealand sgp singapore nor norway tha thailand pol poland tun tunisia prt portugal twn chinese taipei svk slovak republic vnm viet nam svn slovenia zaf south africa esp spain row rest of the world swe sweden che switzerland tur turkey gbr united kingdom usa united states household preferences and governmental sector the households and governmental demand for goods and services is represented by the linear expenditure system les that is derived as a solution to the stone geary utility maximization problem eq a2 1 u r i c r i μ r i γ r i the resulting demand system where i r denotes households disposable income and p r i are consumer prices of goods and services that include taxes subsidies transport and trade margins can be written as follows eq a2 2 c r i μ r i γ r i 1 p r i i r j μ r j p r j households always consume a certain minimum level of each good and services where this level reflects the necessity or price elasticity of the good or service necessities such as food have low price elasticity and hence a higher minimum level of consumption the disposable income of the households consists of wages return to capital and social transfers from the government minus the income taxes and households savings the government collects production consumptions and income taxes the tax revenue is further used to pay social transfers and buy goods and services for public consumption the governmental savings can be either endogenous or exogenous in the model depending on the type of simulation and the type of chosen macro economic closure firms production domestic production x r i d is obtained using the nested ces production technology of capital labour energy materials klem type where k is the capital l is the labour e is the energy and m is the materials figure a2 2 represents the nests in the klem production function used in the model with services between used according to the fixed leontief input coefficients in the production process the energy in the model is differentiated between electricity and other types of energy with some substitution possibilities between them the labour is differentiated according to three education levels according to international labour organisation ilo classification the domestic production is generated according to nested production ces function which is described by the following set of composite ces functions that follow the production structure from top to the bottom nest eq a2 3 x r i d a r i m r i ρ m k l e 1 a r i k l e r i ρ m k l e 1 ρ m k l e eq a2 4 k l e r i b r i e r i ρ e k l 1 b r i k l r i ρ e k l 1 ρ m k l e eq a2 5 k l r i c r i k r i ρ k l 1 c r i l r i ρ k l 1 ρ k l eq a2 6 e r i d r i e r i n e l e c ρ e 1 d r i e r i e l e c ρ e 1 ρ e eq a2 7 l r i e f r i e l r i e e d ρ l 1 ρ l where a r i b r i c r i d r i and f r i e are the share parameters of the corresponding production function nests and ρ m k l e ρ e k l ρ k l ρ e and ρ l represent the substitution possibilities for each of the production function nests the inputs into the production are denoted as m r i input of materials k l e r i composite capital labor energy nest e r i energy inputs k l r i composite capital labor nest k r i capital input l r i labor input e r i n e l e c input of non electric energy e r i e l e c input of electric energy and l r i e e d inputs of labor by type of education e fig a2 2 structure of klem production functions in the model source ivanova et al 2019 fig a2 2 international and inter regional trade the total sales x r i of tradable goods and services i in region r in the model is an armington constant elasticity of substitution ces ref composite between domestic output x r i d and imports x r i m such that eq a2 8 x r i α r i d x r i d ρ i α r i m x r i m ρ i 1 ρ i where α r i d and α r i m are the calibrated share parameters of the ces function and ρ i σ i 1 σ i with σ i being the armington elasticity of substitution between domestic and imported tradable goods and services the elasticity of substitution varies between different types of goods and services depending on the available empirical estimates in case of non tradable the composite is equal to the domestically produced product imported goods can come from various regions and countries represented in the model and the composite imported goods and services are represented by the ces composite that uses a higher armington elasticity of substitution as compared to the upper armington nest we assume as in the gtap model that the elasticity of substitution between the same type of goods and services coming from different countries is twice as large as the elasticity of substitution between domestic and aggregate imported goods and services the aggregate imported good is calculated according to the following ces composite function eq a2 9 x r i m s α s r i t x s r i t ρ i t 1 ρ i t where α s r i t is the calibrated share coefficient of the ces production function x s r i t is the flow of trade in commodity i from country s to country r the coefficient ρ i t σ i t 1 σ i t where σ i t is the elasticity of substitution between commodities produced in different countries labour capital and goods markets market equilibrium in the economy results in equalization of both monetary values and quantities of supply and demand market equilibrium results in equilibrium prices that represent in the case of cge models the solution to the system of nonlinear equations that include both intermediate and final demand equations as well as accounting constraints that calculate households and government incomes savings and investments as well as trade balance eu ems model represents a closed economic system meaning that nothing appears from nowhere or disappears into nowhere in it this feature of the cge model constitutes the core of the walrasian equilibrium and ensures that even if one excludes any single equation of the model it will still hold this is the property of cge models called walras law that tells us that in the closed economic system if n 1 markets are in equilibrium the last nth market will also be in equilibrium in our eu ems model the static equilibrium is described by the set of commodity and factor prices total outputs final demands of households and government investments savings and net transfers from abroad such that 1 markets for goods and services clear 2 total investments are equal to total savings 3 total households consumption is equal to their disposable income minus savings 4 total governmental consumption is equal to its net tax revenues minus transfers to households minus savings 5 total revenue of each economic sector is equal to its total production costs and 6 difference between imports and exports is equal to the net transfers from abroad recursive dynamics eu ems is a dynamic model and allows for the analysis of each period of the simulation time horizon this horizon is currently set at 2050 but it can be extended to longer time periods for each year of the time horizon eu ems calculates a set of various economic social and environmental indicators the economic growth rate in eu ems depends positively on investments in r d and education by investing in r d and education each region is able to catch up faster with the technological leader region and better adopt its technologies time periods in eu ems are linked by savings and investments by the end of each time period households firms and government in the model save a certain amount of money this money goes to the investment bank distributing it as investments between the production sectors of the various regions the allocation decisions of the investment bank sectors depend on the sector s financial profitability the model runs in time steps of five years for the period 2015 2050 the capital stocks evolve according to the dynamic rule presented below where the capital stock in period t is equal to the capital stock in period t 1 minus the depreciation plus the new investments into the capital stock eq a2 10 k t r i k t 1 r i 1 δ i i t r i at the end of each period there is a pool of savings s r available for investments into additional capital stocks of the sectors this pool of savings comes from households firms and foreign investors the sector investments i t r i are derived as a share of the total savings in the economy according to the discrete choice formula eq a2 11 i t r i s t t 1 r b r i k t 1 r i e ϑ w k r t 1 r i j b r j k t 1 r j e ϑ w k r t 1 r j eq a2 12 w k r t 1 r i r t 1 r i p i t 1 r g r δ r i where w k r t 1 r i denotes the capital remuneration rate g r the steady state growth rate b r i the calibrated gravity attraction parameter and ϑ the speed of investment adjustment outputs the eu ems model produces detailed dynamics of regional gdp production and value added by region and by economic sector interregional trade flows by the type of commodity electricity and gas consumption per region and sector employment by regional and economic sector household income and consumption and governmental revenues and spending for the purpose of this article we limit the presentation of the main cge output to gross domestic product gdp percentage change in the electricity consumption per nuts2 region country and the entire eu appendix 3 upscaling distance between countries is not only the geographical and therefore the regional economic integration should not happen regardless other local factors social structure wealth and lifestyle religion institutional and economic conditions and natural environment play a role in assessing cultural distance gobel et al 2018 hofstede 2011 2001 kaasa et al 2016 schwartz 2014 vignoles et al 2018 table a3 1 summarized the value of cultural dimensions in this study due to the absence of more granular data we use the dutch case to approximate how the behavioral patterns may evolve in the north west eu states and the spanish case for the south east eu states which is in line with the values presented below table a3 1 values of cultural dimensions for all eu countries sources čuhlová 2018 table a3 1 country pdi inv austria 11 55 belgium 65 75 bulgaria 65 75 croatia 73 33 cyprus a czech republic 57 58 denmark 18 74 estonia 40 60 finland 33 63 france 68 71 germany 35 67 greece 60 35 hungary 46 80 ireland 28 70 italy 50 76 latvia 44 70 lithuania 42 60 luxembourg 40 60 malta 56 59 netherlands 38 80 poland 68 60 portugal 63 27 romania 90 30 slovakia 104 52 slovenia 71 27 spain 57 51 sweden 31 71 uk 35 89 pdi power distance index inv individualism a complete data for cyprus are not available appendix 4 results and discussions step 2 scaling up behavioral scenarios to national and eu level using the population projection scenarios for the eu28 we scale the dynamics in household energy behavioral changes in two provinces over time up to national and eu levels table a4 1 table a4 1 share of actions in two countries over time source scaled up bench v 3 results table a4 1 step 3 from regional to the national and eu28 economy to estimate the macroeconomic and cross sectoral impacts of individual energy behavioral changes we link the up scaled abm output to the cge eu ems model the bench v 3 behavioral patterns in each of the 12 age education groups changes in heterogeneous households electricity and gas consumption exogenously modify the minimum subsistence level of households consumption of the respective services in eu ems the analysis of eu ems results indicates that most of the eu28 regions benefit from the behavioral changes and lead to the decrease in energy consumption with a small number of regions being affected negatively importantly regions with larger population as well as the regions with higher share of highly educated people benefit more from the behavioral changes since they save more electricity and gas fig a4 1 diffusion of households investments per capita and per action insulation pvs installation energy efficient appliances among 12 sociodemographic groups under the informative dynamics scenario in two province source eu ems and bench v 3 fig a4 1 as expected pvs get more of a share of the investments in both countries figure a4 1 households in groups 6 8 invest 110 160 and 160 180 euros per capita on pvs in netherlands and spain respectively while insulation in spain 82 euros per capita and ee appliances in netherlands 37 euros per capita are second in household investments fig a4 2 percentage changes in the levels of regional real gdp relative to the baseline under the fd scenario in 2050 as an aggregated effect of households behavioral changes in millions of euros source eu ems and bench v03 fig a4 2 the eu ems model operates at the level of nuts2 regions of the eu28 and hence enables the calculation of the regional impacts of various behavioral scenarios on changes in the gdp and income the changes in income presents similar patterns as changes in real gdp see fig 6 however it is interesting that different pattern in percentage changes in regional gdp levels from the absolute monetary changes in regional gdp is captured see figure a4 2 the majority of relatively large changes in gdp are located in great britain italy and central europe this might be related to the assumed population and education level developments which influence the upscaling of the results of the bench abm model 
25944,equifinality a situation in which multiple plausible explanations exist for a single outcome presents a challenge for socio environmental systems modeling when equifinality is ignored in model calibration subsequent policy analyses may mis estimate the range of potential policy effects in this paper we present and demonstrate an approach called dmc rpa for generating a set of diverse model calibrations dmc to enable more robust policy analysis rpa the optimization based approach maximizes diversity in the model parameters and or structural configurations to efficiently represent any equifinality in the model set we demonstrate the approach for an agent based model that is used to compare resilience enhancing strategies in a smallholder farming system results over the set of diverse model calibrations demonstrate consistent policy effects enabling stronger conclusions than a single model analysis going forward this approach can be applied in the development of socio environmental systems models to facilitate more robust policy analysis and inference graphical abstract image 1 keywords model calibration equifinality policy analysis evolutionary algorithm pattern oriented modeling agent based modeling software availability information software name dmc rpa version 1 1 developer contact address tim williams tgw umich edu year first available 2020 hardware required no specific hardware software required python 3 license gpl 3 0 availability fully available on open abm https www comses net codebases 5c7710b4 f9c1 47cc ad61 8734febdb2f0 releases 1 1 0 program language python cost free for non commercial use 1 introduction process based models are regularly used for ex ante evaluation of policy interventions verburg et al 2016 schulze et al 2017 kremmydas et al 2018 schmolke et al 2010 however model outputs and hence any policy recommendations derived from model analysis are dependent on both the chosen model structure and parameterization van vliet et al 2016 in particular the complexity of socio environmental systems ses and the corresponding uncertainty inherent in modeling them hornberger and spear 1981 liu et al 2007 ligmann zielinska et al 2014 means that there may exist multiple plausible model configurations that reasonably fit observed outcomes axtell and epstein 1994 oreskes et al 1994 beven 2006 this is known as equifinality under this condition it is possible that a single model s behavior does not represent the full range of plausible outcomes leading to biased policy recommendations this can have large implications when adaptation actions are limited in their reversibility or are costly to counteract potentially locking systems into maladaptive pathways leclère et al 2014 however the vast majority of ses modeling studies employ single best fit models to conduct policy analysis parker et al 2003 huber et al 2018 brown et al 2013 o sullivan et al 2016 to address equifinality in the development of ses models and prioritize robust policies for sustainable development therefore requires that two questions be answered 1 do multiple plausible structural and or parameter representations exist for a given ses model if so how do they vary 2 when applying a set of plausible models to new conditions e g a policy analysis do they lead to qualitatively consistent results if not what can we learn from this to attend to these questions we present an optimization based approach for identifying equifinal models and exploring their implications for policy analysis we name the approach dmc rpa diverse model calibration for robust policy analysis first we systematically identify multiple parameter sets and or model structural characteristics that each match calibration data within a specified level of fitness yet are as diverse as possible dmc brill et al 1982 zechman and ranjithan 2004 next we conduct a policy analysis and explore the consistency of policy effects over the equifinal model set rpa there are two main contributions in this approach first by explicitly maximizing diversity within the model set our approach enables an efficient representation of equifinality in a small number of models this assists in the communication and understanding of equifinal models as well as reduces the computational complexity of subsequent model experimentation second we focus on the implications of equifinality for policy analysis which allows for more robust policy assessments and inference in ses modeling we demonstrate our approach using a case study in which an agent based model abm is used to compare strategies for enhancing climate resilience of smallholder farmers in an ethiopian context we measure climate resilience with respect to the effect of drought on household food security and examine which of two policy interventions provides the greatest resilience enhancing benefit using the dmc rpa approach we identify a set of plausible diverse model configurations and explore the implications for policy recommendations we seek to examine what identifying equifinal models can mean for inferences drawn throughout the modeling process and what implications it might have for model based policy studies 2 background equifinality in socio environmental systems models equifinality describes a situation in which a given set of observed patterns or outcomes can be produced by multiple distinct explanations beven and freer 2001 this is equivalent to the terms nonidentifiability nonuniqueness multi realizability and the parameter identification problem in which multiple parameterizations or generative process descriptions are observationally equivalent oreskes et al 1994 conte and paolucci 2014 walter 2014 guillaume et al 2019 a particularly strong case of equifinality is known as structural nonidentifiability which is most easily imagined in the case of a fully parametric equation based model if the model has more free parameters than the number of data points used to calibrate it an infinite number of observationally equivalent parameterizations may exist schmidt et al 2020 equifinality is relevant to ses modeling many ses models are highly complicated sun et al 2016 lee et al 2015 that is they contain a large number of parameters and structural assumptions in many cases there is limited knowledge of the processes driving the modeled system ligmann zielinska et al 2014 as well as limited empirical data against which to compare model outputs augusiak et al 2014 given this potential mismatch between the dimensionality of the model and the data multiple structures and or parameterizations may exist that generate outputs consistent with the data in other words the mapping from micro rules to macro structures may be many to one axtell and epstein 1994 due to stochasticity feedbacks and the non analytical nature i e no fixed structural form of many ses models windrum et al 2007 such equifinality may be difficult to identify yet due to this very nature ses models can exhibit high degrees of path dependence and non linear dynamics which may lead to significant implications if the potential for equifinality is not acknowledged equifinality is most pertinent to the calibration stage in the iterative model development cycle grimm and railsback 2005 the purpose of model calibration is to improve a model s fit to real world conditions by adjusting its parameter values and or structural representations van vliet et al 2016 national research council 2012 typically this involves finding the single best fit model to utilize for subsequent analysis proponents of the equifinality thesis argue that the possibility for multiple acceptable models should not be rejected and the model calibration process should instead constitute a mapping of the landscape into a space of feasible models beven 2006 there exist various approaches for model calibration and analysis that either explicitly or implicitly acknowledge equifinality table 1 gives a non exhaustive overview approaches explicitly dealing with equifinality have been most extensively discussed and developed in the field of hydrology e g beven and freer 2001 blazkova and beven 2009 smith et al 2008 efstratiadis and koutsoyiannis 2010 yen et al 2014 vrugt et al 2008 in these contexts the objective of allowing for multiple model configurations is typically to produce a wider uncertainty band on model predictions that is more likely to contain the true value in contrast acknowledgement of equifinality is surprisingly absent in process based model evaluations of policy impacts in ses this literature could therefore benefit from an approach that builds from frequently used methods in ses modeling to 1 identify equifinality and 2 explore its implications for policy analysis 3 approach diverse model calibration for robust policy analysis dmc rpa 3 1 overview we propose an approach for incorporating equifinality into the model development cycle to enable more robust policy analysis in socio environmental systems fig 1 fig 2 the approach consists of two steps which together we refer to as dmc rpa diverse model calibration for robust policy analysis first an optimization based model calibration procedure seeks to identify the maximally diverse set of model configurations that can explain the observed data diverse model calibration dmc second this small set of maximally different models is applied to a policy analysis robust policy analysis rpa if policy recommendations are qualitatively different over the set of diverse plausible model configurations this is evidence to suggest that these policies may not be robust in reality or that further information is needed to reduce equifinality conversely if results are consistent this provides strength to any model driven inferences and policy recommendations beyond a best fit model analysis in either case our approach enables a more robust policy analysis 1 1 we note the distinction in the use of the word robust between our approach and robust decision making rdm in rdm a robust policy is one that is beneficial over a wide range of uncertain exogenous conditions e g input data future trajectories kasprzyk et al 2013 whereas we refer to a robust policy as one that is beneficial over a range of equifinal model configurations the more general phrase robust policy analysis rpa refers to our overall approach whether the policy itself is robust or not the motivation underlying our approach is that given the complexity of ses and the paucity of empirical data in many situations we cannot claim to have all potentially relevant data for model calibration i e there are inherently objectives that are unmeasured in the calibration process thus it is not instructive to focus only on the single optimal calibration to this imperfect set of data rather it is more useful to generate a number of calibrated solutions that each perform well with respect to modeled issues and are significantly different with respect to the decisions they specify brill et al 1982 these diverse solutions may consequently behave considerably differently under conditions not modeled in the calibration process such as a policy analysis by focusing on a small number of diverse plausible solutions this approach efficiently encompasses any equifinality enabling each calibration to be individually examined and reducing the computational burden of subsequent policy experiments this kind of approach with an explicit focus on solution diversity was first proposed as a method for generating a set of diverse decision options in the context of land use planning brill et al 1982 and has since been applied as a decision support tool in other areas decarolis et al 2017 ligmann zielinska et al 2008 harrison et al 2001 in this paper we extend this work to the context of model calibration and its implications for policy analysis we note that the approach as presented in this paper focuses on abms but is also applicable to any process driven model that requires calibration e g cellular automata partial or general equilibrium models system dynamic models or other simulation based biophysical models we also note that the dmc rpa approach focuses only on model calibration and application it does not attend to other stages of the model development cycle such as model validation rather our approach simply suggests that the modeling process be modified to allow for the possibility of multiple plausible model configurations through the model analysis stage fig 2 the equifinal calibrated models could go through a further validation refinement before being applied to assess policy we discuss this in section 6 3 2 diverse model calibration dmc we implemented the dmc approach in python 3 pseudocode is given in online appendix b and the code for the case study application is hosted on comses net 2 2 https www comses net codebases 5c7710b4 f9c1 47cc ad61 8734febdb2f0 releases 1 1 0 our approach for diverse model calibration uses eaga evolutionary algorithm to generate alternatives zechman and ranjithan 2004 which is an extension of a conventional genetic algorithm ga 3 3 gas are a form of optimization inspired by darwin s theory of evolution a population of individuals is evolved toward better solutions each individual is characterized by a vector of parameters i e their genes selection and reproduction occur within the population and are mediated by each individual s fitness which in this case represents the degree to which an individual s output matches calibration data using this approach we are solving a multimodal multiobjective optimization problem efstratiadis and koutsoyiannis 2010 singh and deb 2006 in which the uncertain model parameters and or structures constitute the decision variables the objective function has two components the first representing the degree to which a model s outputs match empirical data and the second representing the degree to which a model s configuration is different from other candidate models for abms of ses which are stochastic and can exhibit nonlinear dynamics evolutionary approaches are a useful heuristic method for searching parameter spaces reed et al 2013 thiele et al 2014 so are appropriate in this context our operational definitions for various terms are given in table 2 3 2 1 optimization procedure genetic algorithm in an extension to a regular ga which consists of a single population of individuals the eaga consists of multiple subpopulations sps of individuals i e potential model configurations that coexist in the decision space algorithm 1 each sp is analogous to the population of individuals in a regular ga but each individual evolves to both increase its fitness to the empirical data i e reduce the loss in equation 1 below and increase its difference from the models in the other sps equation 4 below evolution and genetic selection occur within each sp i e there is no genetic spillover between sps and utilize standard ga evolution procedures one sp is defined a priori as the master sp the master sp seeks to find the globally optimal solution and each individual in the master sp evolves solely based on fitness i e diversity is not important solutions in other sps are considered feasible when their fitness given by equation 1 below lies within some tolerance of the best solution in the master sp e g up to 20 larger the feasibility of solutions affects the selection of parents in the eaga the binary tournament selection process selects individuals to act as parents by randomly pairing two individuals and selecting one of these to pass on its genetic material using the following heuristic if both potential parents are feasible select the more diverse of the two i e higher d q k in equation 4 below if only one potential parent is feasible select this one and if both potential parents are infeasible select the one with the better fitness equation 1 below algorithm 1 dmc procedure adapted from zechman and ranjithan 2004 image 2 3 2 2 decision variables uncertain parameters and structures each continuous uncertain model parameter is defined within some specified bounds i e constraints and potential process descriptions and categorical parameters are represented using categorical decision variables in this sense uncertain structural characteristics are treated no differently than model level parameters with the specification of both falling under the general term configuration specifically each candidate model k is characterized by a set of s configuration elements x k x 1 x s which for these calculations are each normalized to the 0 1 unit interval in the case of a categorical configuration element it can be assumed that all categories are equidistant from each other by specifying a single distance e g 1 for models that are different with respect to this element 3 2 3 objectives matching patterns and increasing disparity the first objective is to minimize the discrepancy between a set of model generated and empirically observed patterns as such our approach is a form of pattern oriented modeling for each individual k discrepancies are weighted and combined over the r patterns to give a single measure of fit 1 l k r 1 r w e i g h t r d i s c r e p a n c y m o d e l k r d a t a r the discrepancy measure or loss l is an informal measure of likelihood hartig et al 2011 smith et al 2008 similar to those used in other studies calvez and hutzler 2006 stonedahl and wilensky 2010 chica et al 2017 the discrepancy measure could take a number of forms depending on the type of pattern to be fit table 3 the second objective is a measure of difference between a given model configuration and the other candidate models which is measured as a distance in the configuration space because the goal is to evolve increasingly disparate sps this difference is calculated between each individual k in sp q and the centroid of each of the other sps s p p p q the centroid of the pth sp c p is calculated as a fitness weighted average over its individuals configuration elements i e parameters and or structures 2 c p 1 k k 1 k w e i g h t k x p k where the best fitting individual in each sp receives a weight of one the worst fitting individual receives a weight of zero and there is a linear scaling in between based on fitness the distance d between model k in sp q and s p p s centroid is calculated as the sum of the absolute differences manhattan distance in the normalized configuration space 3 d q k p s 1 s w e i g h t s x q k s c p s different weighting schemes or distance calculations may be chosen if desired finally for each model k in sp q the second component of the objective function d q k is then evaluated as the distance to the closest sp centroid 4 d q k min p p q d q k p 3 2 4 selecting models given the stochasticity in both the model and the ga the objectives equations 1 and 4 are likely to be non monotonic and will fluctuate as each sp evolves to assess convergence the modeler can visually assess the two components of the objective function and discern whether they have stabilized because the dmc evolution is likely highly influenced by the dynamics of the particular socio environmental model we do not present a quantitative measure for assessing its convergence once convergence has been reached a single solution is chosen from each sp not including the master sp to prioritize diversity among the selected models select the most diverse feasible solution within each sp 3 2 5 selecting dmc hyperparameters the dmc procedure contains several hyperparameters that need to be specified by the modeler for example as the number of sps n s p is increased the well fitting regions of the parameter space will become more crowded and in turn the solutions less diverse there is hence a tradeoff whereby if n s p is too low plausible regions of the parameter space may not be discovered and if n s p is too high the solutions lose their diversity and begin to collapse on top of each other applications of this approach should therefore explore the effect of varying n s p on the solutions reached by the dmc procedure alternatively practical considerations may drive the choice of n s p for example if the individual models are to be presented to decision makers or if subsequent policy related computational requirements are high the number has to be manageable the example presented in the original description of the eaga included four sps zechman and ranjithan 2004 other hyperparameters such as the number of generations and the population size within each sp will also affect the solutions reached again the effect of these values on the results should be assessed to encourage appropriate choices we present an example assessment of hyperparameter values for our case study application in appendix a 3 3 robust policy analysis rpa following the identification of a diverse set of model calibrations the second step is to assess the implications for system behavior under policy intervention this simply involves conducting the same policy assessment for each selected model there are two general possible classes of outcome 1 if results are consistent over the set of models we can have greater confidence in any policy recommendations and 2 if results are qualitatively different between the selected models our approach has exposed sensitivities that may have been missed in an analysis using a single best fit model in this case the equifinal models can be explored to suggest the potential socio environmental conditions or mechanisms that may give rise to the success or failure of a policy intervention or additional data can be included to attempt to restrict equifinality see the discussion in section 6 in either case we achieve a more robust policy analysis 4 case study description smallholder climate resilience using an abm of smallholder farmer resilience we apply the dmc rpa approach to generate a diverse set of models and explore whether these lead to policy related assessments that are qualitatively consistent we give a brief overview of the abm here an odd d protocol müller et al 2013 is provided in online appendix a and in williams et al 2020 4 1 abm description smallholder agricultural systems are highly vulnerable to climatic variability vermeulen et al 2012 it is therefore important to identify ways through which their climate resilience can be supported hansen et al 2019 they also exhibit key properties of complex adaptive systems de vos et al 2019 smallholder populations are highly heterogeneous in their attributes and access to capital and household level mechanisms to cope with shocks e g selling of livestock or assets can lead to path dependencies and poverty traps haider et al 2018 additionally interactions between smallholder households and agroecosystems can give rise to dynamically evolving system trajectories giller et al 2011 tittonell 2014 thus agent based modeling is an appropriate tool through which to assess these resilience dynamics bitterman and bennett 2018 schlüter et al 2019a the purpose of the abm is to provide temporal and distributional assessments of smallholder drought vulnerability and the potential household and community level effects of selected resilience enhancing strategies the abm is designed to represent an ethiopian smallholder mixed crop livestock farming system it draws from several sources of empirical data to represent the conditions of amhara in the ethiopian highlands however the model is not intended to produce policy recommendations for a specific location rather it serves as an experimental platform to evaluate the potential effects of resilience enhancing strategies in smallholder systems more generally each agent represents a single smallholder household the modeled livelihood activities include farming livestock rearing and wage based employment fig 3 agents are heterogeneous with respect to their household size land holding and risk aversion additionally each agent has preference for either maximizing wealth or leisure livestock are grazed on a combination of on farm crop residues and a communal rangeland system which each provide amounts of fodder that vary over time based on both climate and endogenously driven rangeland demand the availability of wage based employment is exogenous and does not vary over time climate affects the following model components crop yields which are calculated at an agent level on an annual basis rangeland dynamics which is simulated at the regional level on an annual basis and crop prices which vary each month at the regional level but are exogenous to the modeled system at the beginning of each year agents make decisions about how to manage their farmland fertilizer application planting date whether to buy sell livestock from their herd and how much labor to allocate to non farm wage based employment these options are represented as a finite set of livelihood options a single crop type maize is modeled these start of year decisions are made under a degree of uncertainty about the future climate and market conditions agent level beliefs about these conditions are formed from their previous experiences as well as interaction with neighboring agents following these decisions crop yields are calculated and monthly wage employment allocations are made crop yields are influenced by both agent decisions and the exogenous climate and wage employment allocations depend on the regional demand for labor fig 3 at each month throughout the year agents attempt to satisfy their food and cash consumption requirements through their own crop production food stores and cash holdings agents can buy and sell crops from the market each month as well as sell livestock as a coping measure to smooth cash and food consumption each month that an agent cannot satisfy their food requirements they are classified as food insecure the primary output of the model is this binary monthly household level representation of food security which emerges as a result of interactions between the different modeled components of the agricultural system 4 2 calibrating diverse models 4 2 1 uncertain model characteristics the abm contains a variety of uncertain parameters and structures table 4 although the differences in alternative model structures are relatively minor they are sufficient to demonstrate how the dmc procedure works to evaluate potential model structures other applications could integrate dmc more thoroughly into model structure development in addition to identifying model parameter values critically what table 4 demonstrates is that uncertain model structural representations although fundamentally different from parameters in how they affect the abm are treated no differently by the dmc approach and are simply coded as binary or categorical switches for example we allow the possibility for two alternative decision making representations expected utility maximization and satisficing e x p u t i l d m in table 4 in doing so we apply our approach to contrast alternative theories grimm et al 2005 under expected utility maximization each agent chooses the livelihood option each year that maximizes either their wealth or their leisure time depending on their preference under satisficing all agents have two levels of hierarchical objectives kaufman 1990 1 to choose the option that leads to the lowest food insecurity and 2 to choose the option that maximizes expected wealth or leisure time depending on their preference the second level objective only activates if multiple options tie with respect to the first objective these two alternatives represent different functional representations in the abm yet their reduction to a binary switch is more an issue of model design than a qualitative difference in the dmc procedure 4 2 2 patterns using data from the 2015 world bank s living standards measurement study lsms we identified eight emergent outcomes i e patterns that we wish the model to match to generate these patterns the abm was run from 2003 to 2015 and outputs from the final year of the simulation were compared against the calibration data which represent the empirical conditions in amhara in 2015 five of the patterns represent agent livelihood characteristics we represent these using histograms thus combining information from the household level into a regional pattern these distributions include non farm labor allocation agricultural labor allocation farming and livestock months of food insecurity subsistence fraction i e percent of production consumed and large livestock holdings in addition to these distributions we include three binary indicators representing desirable qualitative model level characteristics the first specifies that at least 70 of the agents choose to farm their land on average this indicator is included to encourage the generation of models in which farming is the dominant livelihood activity which is consistent with empirical data for the modeled region csa 2017 4 4 we note that the lsms reports higher farming percentages than this 89 in amhara in 2015 csa 2017 we use a more permissive value because we do not model land rental dynamics which is a common practice in the modeled region but would unnecessarily complicate the model the second indicator specifies that the grass biomass in the communal rangeland does not decrease to zero at any time to discourage unrealistic rangeland dynamics the third indicator specifies that no agent should ever have more than 80 head of livestock which further encourages livestock holdings to be consistent with the empirical lsms data to measure the discrepancy between the model outputs and the histograms we convert each histogram into its empirical cumulative distribution function ecdf and calculate the average squared difference between each abm generated and empirical ecdf step this represents a discretized version of the distribution distribution pattern type in table 3 we chose this as it bounds the maximum possible loss for each distributional pattern between zero and one meaning that each distribution exerts comparable influence on the total loss we weight all distributions equally as each distribution was chosen to represent a relevant independent livelihood characteristic an additional value of one is added to the total loss for each qualitative pattern that the model does not generate thus the overall loss is bounded between zero and eight 4 2 3 genetic algorithm we conducted an experiment with four sps five total including the master each comprised of 30 individuals run for 300 generations table 5 we chose to present the results for four sps in this article for visual clarity but we also experimented with alternative numbers of sps and population sizes see appendix a to select models we selected the feasible individual from each sp that is most distant from any other sp s centroid although the abm is stochastic for the calibration we ran a single simulation replication for each model configuration and calculated the loss from this single set of outputs we found the variability in model outputs to be much larger between model configurations than within each model configuration so opted for this approach due to computational feasibility to assess the sensitivity of each parameterization we conducted a local sensitivity analysis we systematically perturbed each parameter from its calibrated value and assessed the effect on the fit to the empirical data see appendix e 4 3 policy analysis the objective of the case study is to examine which of two resilience enhancing strategies provides the greatest benefit to climate resilience in the smallholder agricultural system the strategies include the provision of seasonal climate forecasts and a 20 increase in the availability of non farm employment opportunities these are not necessarily explicit policies at a government or institutional level but represent potential policy relevant interventions to the system federal democratic republic of ethiopia 2019 the climate forecasts give the agents information at the start of each year about the upcoming climate conditions this information is not perfect but enables the agents to make better informed agricultural decisions e g shifting their planting date or choosing to not apply fertilizer to their fields potentially increasing their climate resilience increased job availability in contrast provides an opportunity for agents to diversify their livelihoods and can act as an important source of income for agents that do not otherwise have access to land or livestock based capital given the different mechanisms through which these strategies operate they may differently affect the households and in turn the system s collective ability to respond to drought under alternative socio environmental conditions we quantify specific resilience carpenter et al 2001 using a measure of the effect of drought on the agents food security we represent droughts by imposing reductions in rainfall a 50 drought represents a year in which the rainfall is reduced by 50 this affects the crop prices rangeland dynamics and crop yields the effects on prices and rangeland dynamics are at the regional level while the effects on crop yields are both nonlinear in rainfall and spatially explicit to isolate the overall effect of a 50 drought we run two simulations one for 30 years under regular climatic variability and one for 30 years under the same climatic variability but with a 50 drought imposed in the fifth year of simulation in every month of each simulation we record the percent of agents that are food insecure i e are unable to satisfy their food consumption requirements we then take the difference between food insecurity in these two simulations giving a monthly measure of the additional percentage of the abm agents that are food insecure as a result of the drought when analyzed over time this incorporates both the initial impact of the drought and the long term recovery of food security in the years following because food security varies throughout the year this will exhibit an annual cycle to isolate and compare the strategies resilience enhancing benefits we evaluate this measure of resilience both under baseline conditions and with each of the two strategies in place we assess a strategy s overall benefit as the cumulative amount of food insecurity that it avoids in the wake of the drought i e the total number of agent insecurity months avoided in the 25 years following the drought finally we calculate the difference between the two strategies resilience benefits δ r e s as 5 δ r e s a 1 n y 5 30 m 1 12 f i a y m c l i m a t e f o r e c a s t f i a y m j o b a v a i l a b i l i t y where a indexes the n agents y indexes the years m indexes the months and f i denotes the incidence of household level food insecurity there are two levels of stochasticity relevant for this policy assessment both of which will affect the quantity in equation 5 the first level represents within model stochasticity introduced in the assignment of the agents to the landscape crop yield calculation and allocation of regional wage labor and livestock reproduction mortality to account for this we replicate each simulation 50 times 5 5 a convergence analysis law 2008 pg 502 determined that 50 replications were sufficient to achieve a relative error of 0 1 i e x n μ μ 0 1 if x n is the estimate based on n replications and μ e x with a confidence level of 90 the second level represents uncertainty associated with the drought as already described we define our droughts using single year reductions in rainfall however the ultimate effects of this on the smallholder system will depend on both the preceding and succeeding climatic conditions 6 6 for example the effects of the recent ethiopian drought in which some parts of the country only experienced 50 75 of the regular rainfall were in part exacerbated by continued dry conditions in the following year singh et al 2016 to account for this we generated 40 climate timeseries each 30 years long by repeatedly randomly sampling years from the 2000 2015 observational climate record we impose a drought in the fifth year of each timeseries finally to assess sensitivity in the policy comparison we conduct two additional experiments one with nine sps ten including the master and one assessing the effect of a 20 drought 5 case study results 5 1 diverse model calibration the master sp s solutions converge in their fit to the empirical data after approximately 25 generations of the genetic algorithm fig 4 a after approximately 50 generations the solutions in all other sps begin to become feasible fig 4a and c i e within 30 of the master sp s best solution once the solutions are feasible their diversity begins to increase fig 4b and stabilizes after approximately 175 generations overall the abm does well at matching the empirical patterns the total summed loss in the master sp is approximately 0 08 fig 4a out of a maximum possible value of 8 and all other sps are within 30 of this indicating that all qualitative patterns are matched and the levels of fit to the empirical histograms are high however all calibrated models overestimate the percentage of households with no livestock herds fig 5 and appendix b livestock represent an important coping mechanism both in reality and in the abm given this our models may underestimate resilience or lead to biased policy assessments for example climate forecasts provide the agents with information that can aid their livestock stocking destocking decisions because our model underestimates the number of households with livestock it may underestimate the benefits of climate forecasts through this mechanism in addition the calibrated models exhibit different levels of fit to the non farm labor distribution fig 5 and appendix b sp2 underestimates and sp4 overestimates the proportion of agents engaging in non farm labor some variation is to be expected in the performance of any set of equifinal models and that variation is important to provide structure to the variation in any policy relevant conclusions it is therefore important to incorporate understanding of the variations in level of fit into policy analysis based on the calibrated models as different levels of fit may imply different degrees of credibility over the model set future work could expand the range of structural representations included in the model calibration to improve the level of fit to the livestock distribution as well as include a validation step to filter out models from the calibrated set that less adequately represent reality in ways that might significantly affect the policy analysis and in turn bias conclusions 5 2 equifinality in the calibrated models the selected model configurations are diverse fig 6 suggesting that the complexity in the model allows for very different parameter sets to produce similar levels of fit to the data specifically the distance between each of the sps in the normalized parameter space is approximately 6 fig 4b given that there are 18 uncertain model elements table 4 the maximum possible distance equation 3 between any two models is 18 different parameters are more or less consistent over the model set for example the r i s k a v e r s i o n m u l t parameter which represents a dimensionless multiplier on the agents risk aversion coefficient is present over almost its entire range in the four models fig 6 this implies that the model is insensitive to changes in this parameter in contrast parameters such as p l a n t i n g f r a c t i o n are only present over a smaller range in the calibrated model set fig 6 this implies that the model is more sensitive to changes in these parameters and that only a narrow range of values produce plausible model outputs our analysis of the sensitivity of the model calibrations appendix e confirms these observations and additionally shows that the sp3 and sp4 calibrations are less stable slight perturbations in their parameters result in some of the qualitative fitting patterns not being matched thus degrading the calculated fit we observe an interesting result in the uncertain model structural representations all four models contain agents that utilize expected utility maximization to make their livelihood decisions e x p u t i l d m in fig 6 i e the satisficing decision making representation is never selected by the calibration procedure 5 3 policy analysis enhancing climate resilience with no policy in place the effect of the drought on household food security differs over the selected models fig 7 for example the model selected from sp1 exhibits the smallest drought vulnerability with only a maximum of 6 of households affected by the drought at the median simulation output in comparison in sp3 food insecurity is at the median 14 higher in the year following the drought sp3 also exhibits the highest levels of food insecurity under baseline conditions appendix b in terms of recovery in no model does food security recover completely to its level in the no drought counterfactual fig 7 showing that in all cases the drought permanently alters the livelihood trajectory of some households the differences between the sps suggest that in this case there are implications of equifinality as each of the equifinal models exhibits different behavior when applied to a situation drought event not used in the calibration when comparing the effects of the two interventions on the system s resilience all four models yield the same directional result climate forecasts offer larger potential benefits to food security in the wake of a drought than an increase in job availability figs 8 and 9 thus in spite of the differing model configurations fig 6 and baseline levels of vulnerability fig 7 there are no large implications of equifinality for our policy analysis in this case the dmc rpa approach has therefore yielded a conclusion that is likely similar to that utilizing a single model calibration but the consistency of this result over the diverse calibrated models increases the robustness of this conclusion 5 4 sensitivity analyses reducing the magnitude of the drought to 20 does not affect these conclusions the 20 drought has a smaller yet similar effect on household level food security and the climate forecast still consistently provides larger benefits under these drought conditions appendix c in an experiment with nine sps not all sps become feasible after 300 generations and due to the larger number of solutions are more closely located within the uncertain parameter space appendix d however the overall diversity of the feasible solutions is higher for example in contrast to the results above in which all models specified expected utility maximization two sps in this experiment specify satisficing as the decision making framework additionally the overall range of many continuous variables over these models is higher this higher diversity between parameterizations contributes to a higher diversity in drought responses and policy comparisons appendix d however the result from above that climate forecasts provide greater resilience benefits is demonstrated in seven of the eight retained parameterizations thus we conclude that this result is a robust one 6 discussion 6 1 dmc rpa for model development and inference the dmc rpa approach allows the researcher to retain multiple hypotheses represented by multiple disparate model configurations through the analysis stage of the model development cycle fig 2 doing so is in accordance with the notion of strong inference platt 1964 although we have focused on the implications for policy analysis the dmc approach also presents opportunities for model development model driven theory development and systems inference first the parameters and structures of the calibrated models themselves i e the conditions measured in the calibration may suggest different socio demographic and environmental contexts under which similar outcomes may be possible parameters that are especially variable or consistent may represent critical or sensitive factors in driving the empirical outcomes alternatively if one process representation is consistently selected over another this constitutes a form of combined model based and empirical evidence supporting its appropriateness for describing reality in the modeled context this could be compared against existing theory and evidence to aid in developing more generalized knowledge magliocca et al 2018 schlüter et al 2019a b second the calibrated models could be compared along some unmeasured axes i e information not assessed in the calibration process yet contained within the calibration simulations wiegand et al 2003 khatami et al 2019 if these unmeasured axes represent factors that are unobservable or unobserved in reality they could provide new information about the diverse mechanisms that may give rise to the observed outcomes acting as a complement to studies that use statistical methods to explore causal mechanisms in empirical data ferraro and hanauer 2014 alternatively unobserved factors that differ over the set of models could be identified to prioritize the collection of new empirical observations a key context within which model equifinality is salient is when model complexity outstrips the availability of calibration data schmidt et al 2020 in these contexts a single calibrated model runs the risk of being overfit to the data i e it could fit the available data well but have low ability to recreate patterns not used in its calibration van vliet et al 2016 sun et al 2016 by using multiple information rich patterns pom attempts to reduce this concern latombe et al 2011 grimm and railsback 2012 sensitivity analysis such as in appendix e can indicate the stability of each parameterization but does not indicate the degree to which the calibrated model may be overfit to the data if the model outputs exhibit an extremely high variability under conditions not used for the calibration e g a policy analysis this may indicate that overfitting is occurring calvez and hutzler 2006 however it is indeed possible that there are multiple plausible discordant model representations in any case acknowledging that there need not be a single optimal solution reduces the risk that incorrect inferences are made even if each individual solution is overfit it is possible that including new tests of emergent model characteristics i e testing against data that are observable in reality yet unmeasured in the original calibration procedure would reduce equifinality platt 1964 latombe et al 2011 guillaume et al 2019 this could involve for example a model output corroboration augusiak et al 2014 or cross validation procedure that employs patterns or tests unmeasured in the calibration process itself wang et al 2018 wiegand et al 2003 if the model is spatial it could be validated by applying it or comparing its outputs to those from a different region magliocca et al 2015 brown et al 2005 alternatively consultation with domain experts could be used to filter out unreasonable model configurations van vliet et al 2016 with respect to model development our approach is similar to the virtual laboratory and building block approaches magliocca and ellis 2016 which use pattern oriented modeling to systematically evaluate hypothesized model structures potentially representing contrasting theories or differing levels of complicatedness sun et al 2016 against empirical data our approach builds on this by identifying multiple diverse model configurations systematic procedures for model structure and parameter specification that allow for equifinality have been more extensively developed in the field of hydrology khatami et al 2019 touhidul mustafa et al 2020 and could be turned to for future inspiration in the ses modeling community in principle dmca rpa is a formalization of the pattern oriented modeling approach and we situate it within this body of research finally as we have previously stated the policy assessment can lead to two general classes of outcome either the conclusions are similar over the set of models or they are not in our case study example we observed remarkably consistent effects over the four model configurations this implies a greater level of robustness of our conclusions than if we had used a single model parameterization had we instead observed inconsistent effects these results could be used to shed light on the socio environmental conditions under which different interventions may be more or less effective helping to inform the targeting of policy interventions giller et al 2011 6 2 comparison to alternative calibration approaches there are three main features that set our approach apart from alternative model calibration methods 1 identifying a small set of n models 2 retaining these models as separate and 3 maximizing model diversity these features have both philosophical and practical implications in interpreting the model configurations and policy analysis results first our approach identifies a small set of n models in the case study application we chose n 4 as we have discussed this allows for equifinality and is more appropriate than any best fit model calibration procedure by calibrating a small set of distinct models each model configuration can be individually examined enabling enhanced inference and communication with decision makers schwartz 2012 having a small number of models is also particularly advantageous in situations when subsequent policy related experiments are computationally expensive in these situations it is desirable to not only have efficient sampling over the prior distributions for the parameters vrugt and beven 2018 yen et al 2014 but also to efficiently encompass equifinality in a small set of models mcmc based calibration approaches require repeated sampling from the posterior distribution and in high dimensional cases a large number of samples may be necessary for example the application in an initial presentation of the mcmc based dream algorithm used 2500 draws from the posterior distribution vrugt et al 2009 other implementations of multimodal evolutionary algorithms for model calibration use diversity based filtering to reduce the number of solutions chica et al 2017 moya et al 2019 so are comparable to our approach in this respect second and related to the first we do not assign a probability or relative likelihood to each model but present each model configuration and its policy assessment separately this differs from bayesian model calibration methods that estimate a posterior distribution for the model parameters structures and use this to produce a single predictive distribution ajami et al 2007 vrugt et al 2008 hartig et al 2011 touhidul mustafa et al 2020 similarly monte carlo calibration approaches typically aggregate outputs over the entire set of behavioral models beven and freer 2001 although an interesting exception exists in khatami et al 2019 conceptually our approach is motivated by the deep uncertainty in modeling socio environmental systems which makes it problematic to assign a probability to each model polasky et al 2011 in this regard the dmc rpa approach is similar to robust decision making lempert 2003 and we employ it to identify policies that are beneficial over a wide range of potential states without assigning a probability to these states if a single predictive distribution is desired however our approach does not preclude using the model fits equation 1 or preferably the success of each model in some independent validation exercise to develop informal weights for each model that we do not assign a probability to each model configuration also demonstrates some similarities and differences to both the limits of acceptability loa beven 2006 and the pom concepts grimm et al 2006 in loa models that satisfy a number of predetermined acceptable limits e g consistently produce outputs within 20 of an observed value are characterized as behavioral no behavioral model is considered to be more or less behavioral than another pom also employs an equivalent behavioral notion similarly in our approach the final model configurations are each equally considered however our approach differs in that model configurations are defined as feasible based on their fit relative to the master sp s best solution this feasibility criterion is not defined a priori but evolves with the algorithm e g dashed line in fig 4a this helps to maintain the balance between fit and diversity during the genetic algorithm s evolution but is not as strongly based on theory or domain expertise as is required for loa or pom beven 2006 third our approach is unique in how it maximizes diversity within the feasible model set while staying within a specified tolerance of the optimal model other set theoretic and mcmc based calibration approaches generally aim to maintain model diversity which is accomplished by removing or penalizing solutions that are similar in the configuration space i e the parameters and or structures that they specify singh and deb 2006 olalotiti lawal et al 2015 because of the dual objectives i e model fit and diversity our approach is technically a bi objective optimization these are not combined into a single objective nor do we seek to find a pareto optimal set of solutions our approach does not maintain that diverse models are more desirable rather that the diverse set of plausible models most efficiently encompasses equifinality the diversity component enters the objective function only in models that are feasible e g with an error within 30 of the master sp s best solution due to this treatment of the two objectives combined with the fact that we do not seek to identify a posterior distribution our approach could not to the best of our knowledge be directly integrated into an mcmc procedure at least without modification thus the dmc procedure constitutes a distinct approach to model calibration while providing important computational and inferential advantages these features of the dmc rpa approach may also present tradeoffs in certain situations importantly because the number of models n s p must be specified a priori the algorithm is limited in its ability to characterize the structure of unknown objective spaces other evolutionary approaches such as niching genetic algorithms goldberg et al 1987 miller and shaw 1996 deb et al 2002 can more flexibly identify an unknown number of local optima these can be filtered to a smaller number ex post if desired chica et al 2017 additionally although monte carlo methods are inefficient in exploring the parameter space vrugt and beven 2018 they also do not place any restrictions on the number of desired solutions in our case study example we conducted a sensitivity analysis to the n s p hyperparameter appendix a and found that increasing the number of models did not significantly affect the fitness and diversity of the solutions this showed that in this case there are many possible equifinal model representations and that this can have implications for the policy analysis appendix d thus applications of dmc rpa should carefully consider the choice of n s p additionally our approach does not consider the sensitivity of each parameterization this sensitivity can exist on at least two levels first model stochasticity and uncertainty in the input data can lead to drastically different system behavior under a single model configuration and our approach does not allow for this variability when evaluating a model configuration in contrast to probabilistic approaches olalotiti lawal et al 2015 second due to the complexity of socio environmental systems small parameter or structural changes can massively affect system behavior lempert 2002 liu et al 2007 by identifying n discrete models each defined by a fixed set of parameter structure values our approach does not give information about this sensitivity in our case study example we conducted a local sensitivity analysis to the resultant model configurations and found two of them to be highly sensitive to small changes in some of the parameters appendix e thus we recommend that sensitivity analyses are integrated with future applications of dmc rpa 6 3 potential extensions our intention has been to demonstrate diverse model calibration for robust policy analysis we did not conduct computational experiments to compare eaga to alternative optimization procedures so we do not claim that eaga represents the most computationally efficient or appropriate method for diverse model calibration in all contexts niching genetic algorithms are an alternative evolutionary approach that model a single population of solutions that evolve based on a combination of feasibility and the density of other solutions in the parametric neighborhood miller and shaw 1996 goldberg et al 1987 singh and deb 2006 multiobjective genetic algorithms have also been developed in other contexts to identify sets of pareto optimal solutions deb et al 2002 park et al 2013 komuro et al 2006 turley and ford 2009 alternatively the characteristics of this approach could potentially be integrated into mcmc based methods for example by including diversity requirements as a penalty in mcmc likelihood functions olalotiti lawal et al 2015 prioritizing diversity when sampling from mcmc generated posterior distributions e g some form of latin hypercube sampling or through multi objective mcmc algorithms although dmc rpa is not directly comparable to these other optimization approaches in particular due to the way it treats the fit and diversity objectives future work could 1 compare the effectiveness of the eaga with other adaptive sampling procedures in identifying a set of plausible diverse model configurations and 2 integrate diversity objectives more explicitly into other calibration methods within the dmc rpa approach there also exist promising avenues for future extension some of these relate to the evaluation of model fit for example for our case study we combined the loss calculated for each histogram and system level pattern into a single measure of fit equation 1 however it has been demonstrated that such aggregation can inefficiently explore the entire parameter space park et al 2013 deb et al 2002 thus a multiobjective loss measure may help to identify better calibrations additionally to prioritize the generation of parsimonious model structures a complicatedness based penalty could be integrated into the objective function magliocca and ellis 2016 when experimenting with the algorithm we noticed a tendency for non influential parameters to diverge to the extreme ends of the prescribed bounds i e 0 and 1 in fig 6 this is entirely a result of the diversity objective the algorithm exploits non influential parameters to increase the assessed diversity of the model configurations without measurably affecting the models fit to the data to reduce this effect and focus on diversity where it matters most we encourage iterative model development fig 2 integrated with sensitivity analysis as in appendix e to sequentially refine the parameters structures included in the genetic algorithm ligmann zielinska et al 2014 to more comprehensively evaluate the policies robustness using the equifinal models the dmc rpa approach could benefit from tools developed in the rdm literature for example the policy analysis could also consider uncertainty related to model inputs future exogenous conditions or elements of model configuration that cannot be fixed during the calibration process kasprzyk et al 2013 efforts could also be made to more explicitly map the calibrated parameter values and structural states to the scenario performance to identify robust regions within the configuration space lempert 2002 bryant and lempert 2010 finally in this paper we have applied dmc rpa to calibrate an agent based model using distributional data from a single point in time however the approach could be applied to any calibrated process based model in the most general sense it requires simply a model m that produces an output y that is dependent on some input parameters and or structure x i e y m x many other types of models e g system dynamics economic equilibrium biophysical simulation in many different fields e g land system science ecological economics natural resource management fit this description additionally the loss function equation 1 is very flexible it need not satisfy any statistical properties and only requires that a set of model configurations can be cardinally evaluated according to their level of acceptability thus it would be possible to integrate for example 1 timeseries data by either summing or multiplying the discrepancy measure in equation 1 at each time point vrugt and beven 2018 2 spatial features using a measure of landscape pattern similarity parker and meretsky 2004 brown et al 2005 or 3 other levels of uncertainty by either averaging losses over stochastic simulations or assessing robustness over multiple types of uncertainty lempert 2003 6 4 case study results smallholder resilience our results to the case study suggest that under the conditions of the modeled system and noting the inaccuracy in the models abilities to recreate the empirical livestock herd size distribution climate forecasts may provide superior direct benefits to smallholder drought resilience than an increase in non farm job availability this result was consistent over all four model parameterizations fig 9 and over seven of the eight parameterizations in the sensitivity analysis appendix d this suggests that dramatic improvements to resilience could be realized without the significant infrastructural investment that would be required to increase non farm employment opportunities thus informational forms of support like climate forecasting could play an important role in supporting smallholder resilience under a changing climate vermeulen et al 2012 hansen et al 2019 however appropriate communication of forecasts and integration into farmer decision making would be necessary to achieve these benefits in reality hansen et al 2011 in conjunction with adequate accuracy of the climate forecasts themselves ziervogel et al 2005 future modeling work could more thoroughly represent these elements and or extend the structural breadth and empirical grounding of the abm to increase the policy relevance of these results expected utility maximization and the rational actor constitute a common approach for representing human decision making in agent based models kremmydas et al 2018 schlüter et al 2017 klabunde and willekens 2016 groeneveld et al 2017 however this type of approach has long been criticized as not realistically representing how individuals actually make decisions simon 1955 interestingly our results fig 6 showed that models in which agents maximize their expected utility produced better levels of fit to the data than models in which agents behave as satisfiers that first attempt to ensure their food security then maximize their utility beyond this kaufman 1990 because some agents utility in the abm is represented by leisure time even under utility maximization these agents do not behave as maximizers in the traditional economic sense potentially explaining this result the dmc approach could be used in future work to evaluate and compare alternative approaches for modeling decision making based on the degree to which they generate empirically consistent behavior schlüter et al 2017 7 conclusions we have argued in this paper that given the prevalence of complex process based models in socio environmental policy analysis and the paucity of empirical data with which to calibrate these models for their intended purposes equifinality is an issue of general concern to this community we do not claim that process based models are too sensitive to be useful rather we advocate that modelers seriously consider the implications that model structure and parameterization may have on any model generated inferences the dmc rpa approach that we outline and demonstrate in this paper can be used to identify policies that perform well over an entire set of equifinal models thus supporting robust decision making alternatively the approach also can expose inconsistencies that more completely represent uncertainty in the relative benefits of policy interventions in this case divergent results may give information about the socio environmental conditions under which certain policies may be more or less beneficial in either case the dmc rpa approach facilitates more robust model development policy analysis and inference declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this project was supported by a u s national science foundation nsf infews grant 1639214 and an nsf cnh grant deb 1617364 the authors would like to thank allison kelly and uriah israel for their help with the lsms data and genetic algorithm respectively as well as the helpful comments from five anonymous reviewers that greatly improved the paper s quality and clarity appendix asupplementary data the following are the supplementary data to this article eaga pseudocodemultimedia component 1 eaga pseudocode odd d model description odd d model description appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104831 appendices a eaga hyperparameter experiments the specification of hyperparameters for the eaga presents tradeoffs between model fit model diversity and computational burden practical considerations will likely influence decisions in most applications and different abms will respond differently to hyperparameter changes thus we do not present concrete guidelines for hyperparameter selection here but qualitatively explore the effects of hyperparameter changes in our case study application the experiments we conducted are summarized in table a 1 due to stochasticity in the initialization of the eaga solutions we repeated each experiment three times i e three different random number seeds computational requirements prohibited us from running more replications but we believe our results reveal the general trends table a 1 number of model replications required for hyperparameter experiments table a 1 experiment gens n sp pop size seeds model runs n s p 250 3 20 3 45 000 5 75 000 10 150 000 15 225 000 20 300 000 25 375 000 30 450 000 1 575 000 p o p s i z e 250 5 10 3 37 500 20 75 000 30 112 500 40 150 000 375 000 note that this value includes the master sp a 1 number of subpopulations n s p the primary effect that we expect as n s p is increased is that the solutions will reduce in diversity i e the configuration space becomes more crowded we observe this effect with a dramatic decrease in diversity above three sps and then a progressive decrease beyond this fig a 1a increasing beyond 20 sps has little effect on the solution diversity suggesting that the additional solutions in the 25sp and 30sp experiments populate different regions of the configuration space varying n s p had little effect on the fitness to the empirical data in the selected solutions fig a 1b or the spacing within each sp fig a 1c this suggests that for all n s p considered the eaga is able to find a set of reasonable solutions and that 250 generations are sufficient to converge in the objective function value for these selected solutions finally at all n s p above three there were some sps that contained no feasible solutions fig a 1d thus reducing the number of model configurations that can be selected this could suggest issues with the genetic algorithm getting stuck in local minima that do not fit the data well enough to be considered feasible a larger number of ga replications may help these sps to escape these regions because the fraction is not strictly increasing in n s p this demonstrates that the configuration space is not yet fully saturated even with 30 sps there likely exist more model configurations that could satisfy the feasibility requirements choosing the appropriate number of sps given this information is clearly a subjective decision as stated in the main body of the paper practical considerations also have to be considered e g computational requirements or ease of communication for ease of display and to enable investigation of each individual sp we chose four sps five including the master in the main body of the paper however policy relevant applications investigating the robustness of policy interventions may choose to use a larger number of sps to more fully cover the model configuration space fig a 1 influence of the number of subpopulations n s p on eaga convergence the plotted values in a b and c represent the minimum maximum mean over the selected final solutions i e the most diverse feasible solution in each sp d shows the fraction of sps that contained no feasible solutions fig a 1 a 2 population size the results reveal a slight tradeoff between solution diversity and objective function value as population size is varied small population sizes are unable to achieve as good fits to the data within the modeled number of generations fig a 2b and d yet foster slightly more diverse solutions fig a2a the progressive reduction in diversity is because larger population sizes result in a greater degree of smoothing of the centroid locations as the parameter values of a larger more diverse fig a 2c population of solutions are averaged again choosing an appropriate population size is a subjective decision we chose to use a population size of 30 in our application because it achieves a good balance between diversity and fitness yet does not have issues with invalid sps fig a 2d fig a 2 influence of the population size within each sp on eaga convergence the plotted values represent the minimum maximum mean over the selected final solutions i e the closest feasible solution to each sp centroid fig a 2 b comparison of abm generated and empirical patterns fig b 1 abm empirical comparison for sp 2 fig b 1 fig b 2 abm empirical comparison for sp 3 fig b 2 fig b 3 abm empirical comparison for sp 4 fig b 3 c resilience analysis with a 20 drought fig c 1 effect of a 20 drought on household food security fig c 1 fig c 2 relative benefit of the two interventions under a 20 drought fig c 2 d case study experiments with nine sps d 1 diverse model calibration an experiment with nine sps yielded similar results but one of the sps did not become feasible after 300 generations of the eaga fig d 1 as a result only eight parameterizations were selected for the resilience analysis the final parameterizations in this experiment are on average around five units apart in the normalized parameter space fig d 1b which is slightly less diverse than the experiment with four sps and indicates a crowding of the parameter space however there are now two models that contain the satisficing decision making representation sp1 and sp3 fig d 2 and a greater overall diversity in some parameters such as the planting fraction fig d 1 eaga convergence measures with nine sps fig d 1 fig d 2 normalized parameter values with nine sps note that there were no feasible solutions in sp2 so it is not displayed here fig d 2 d 2 resilience analysis the greater overall model diversity leads to a greater diversity in the effects of drought on household food security fig d 3 in particular the maximum effect is not experienced until several years after the drought in sp3 and there is a permanently decreased level of food security in sp1 the relative benefits of the interventions are also more variable over the eight selected models fig d 3 five of these models show results similar to those in fig 9 in the main body of the paper while sps 3 4 and 9 show some evidence of a greater benefit in increased job availability however in seven of the eight retained parameterizations provision of climate forecasts provides larger benefits to resilience than increased job availability sp3 displays the most discordant behavior of the sps the main way in which sp3 differs from the other sps is in the livestock reproduction rate l s g r o w t h r a t e fig d 2 sp3 s lower livestock reproduction rate contributes to a slower recovery time in the wake of a drought fig d 3 as more livestock must be purchased to recover herd sizes and in turn food security as a result of this the additional financial capital provided by an increased job availability yields larger benefits to food security than the climate forecasts which only provide financial capital indirectly through better agricultural management fig d 3 effect of a 50 drought on household food security with nine sps note that no solutions in sp2 were feasible so it was not included in the resilience analysis fig d 3 fig d 4 comparison of policy effects with nine sps fig d 4 e sensitivity of selected model configurations we conducted a univariate sensitivity analysis to assess the stability of the selected model configurations to do so we sequentially perturbed each of the continuous parameters from their calibrated values and assessed the effect of this on the fit to the empirical data we did not include the model structural elements or the categorical parameter in this assessment we note that a univariate sensitivity analysis does not capture potential dependencies between parameters lee et al 2015 and as such we view these results in an exploratory manner the results reveal a wide variability in the stability of the models to parameter perturbations fig e 1 sp1 and sp2 are in general more stable parameterizations than sp3 and sp4 i e their shaded bands in fig e 1 are wider additional experimentation confirmed that the points at which the decline in fit increased above 20 were generally due to the model failing to generate one of the qualitative patterns thus having a value of one added to its loss sp3 and sp4 were each on the verge of not recreating one of these patterns so small deviations in many of the parameters resulted in a dramatic decrease in calculated fit some parameters particularly r f s l o p e r f i n t e r c e p t and r i s k a v e r s i o n m u l t exert very little influence on the models fit to the data given these results these parameters could likely be excluded from the dmc process with little effect on the results this kind of sensitivity analysis could inform the iterative development of a model by sequentially excluding the parameters that the model is not sensitive to fig e 1 the stability of each sp configuration the shaded areas encompass the amount of deviation each parameter can undergo before the model s fit declines by the specified amount wider shaded bands indicate more stable configurations a value of 0 5 on the horizontal axis represents a parameter being set to 50 of its scaled calibrated value fig e 1 
25944,equifinality a situation in which multiple plausible explanations exist for a single outcome presents a challenge for socio environmental systems modeling when equifinality is ignored in model calibration subsequent policy analyses may mis estimate the range of potential policy effects in this paper we present and demonstrate an approach called dmc rpa for generating a set of diverse model calibrations dmc to enable more robust policy analysis rpa the optimization based approach maximizes diversity in the model parameters and or structural configurations to efficiently represent any equifinality in the model set we demonstrate the approach for an agent based model that is used to compare resilience enhancing strategies in a smallholder farming system results over the set of diverse model calibrations demonstrate consistent policy effects enabling stronger conclusions than a single model analysis going forward this approach can be applied in the development of socio environmental systems models to facilitate more robust policy analysis and inference graphical abstract image 1 keywords model calibration equifinality policy analysis evolutionary algorithm pattern oriented modeling agent based modeling software availability information software name dmc rpa version 1 1 developer contact address tim williams tgw umich edu year first available 2020 hardware required no specific hardware software required python 3 license gpl 3 0 availability fully available on open abm https www comses net codebases 5c7710b4 f9c1 47cc ad61 8734febdb2f0 releases 1 1 0 program language python cost free for non commercial use 1 introduction process based models are regularly used for ex ante evaluation of policy interventions verburg et al 2016 schulze et al 2017 kremmydas et al 2018 schmolke et al 2010 however model outputs and hence any policy recommendations derived from model analysis are dependent on both the chosen model structure and parameterization van vliet et al 2016 in particular the complexity of socio environmental systems ses and the corresponding uncertainty inherent in modeling them hornberger and spear 1981 liu et al 2007 ligmann zielinska et al 2014 means that there may exist multiple plausible model configurations that reasonably fit observed outcomes axtell and epstein 1994 oreskes et al 1994 beven 2006 this is known as equifinality under this condition it is possible that a single model s behavior does not represent the full range of plausible outcomes leading to biased policy recommendations this can have large implications when adaptation actions are limited in their reversibility or are costly to counteract potentially locking systems into maladaptive pathways leclère et al 2014 however the vast majority of ses modeling studies employ single best fit models to conduct policy analysis parker et al 2003 huber et al 2018 brown et al 2013 o sullivan et al 2016 to address equifinality in the development of ses models and prioritize robust policies for sustainable development therefore requires that two questions be answered 1 do multiple plausible structural and or parameter representations exist for a given ses model if so how do they vary 2 when applying a set of plausible models to new conditions e g a policy analysis do they lead to qualitatively consistent results if not what can we learn from this to attend to these questions we present an optimization based approach for identifying equifinal models and exploring their implications for policy analysis we name the approach dmc rpa diverse model calibration for robust policy analysis first we systematically identify multiple parameter sets and or model structural characteristics that each match calibration data within a specified level of fitness yet are as diverse as possible dmc brill et al 1982 zechman and ranjithan 2004 next we conduct a policy analysis and explore the consistency of policy effects over the equifinal model set rpa there are two main contributions in this approach first by explicitly maximizing diversity within the model set our approach enables an efficient representation of equifinality in a small number of models this assists in the communication and understanding of equifinal models as well as reduces the computational complexity of subsequent model experimentation second we focus on the implications of equifinality for policy analysis which allows for more robust policy assessments and inference in ses modeling we demonstrate our approach using a case study in which an agent based model abm is used to compare strategies for enhancing climate resilience of smallholder farmers in an ethiopian context we measure climate resilience with respect to the effect of drought on household food security and examine which of two policy interventions provides the greatest resilience enhancing benefit using the dmc rpa approach we identify a set of plausible diverse model configurations and explore the implications for policy recommendations we seek to examine what identifying equifinal models can mean for inferences drawn throughout the modeling process and what implications it might have for model based policy studies 2 background equifinality in socio environmental systems models equifinality describes a situation in which a given set of observed patterns or outcomes can be produced by multiple distinct explanations beven and freer 2001 this is equivalent to the terms nonidentifiability nonuniqueness multi realizability and the parameter identification problem in which multiple parameterizations or generative process descriptions are observationally equivalent oreskes et al 1994 conte and paolucci 2014 walter 2014 guillaume et al 2019 a particularly strong case of equifinality is known as structural nonidentifiability which is most easily imagined in the case of a fully parametric equation based model if the model has more free parameters than the number of data points used to calibrate it an infinite number of observationally equivalent parameterizations may exist schmidt et al 2020 equifinality is relevant to ses modeling many ses models are highly complicated sun et al 2016 lee et al 2015 that is they contain a large number of parameters and structural assumptions in many cases there is limited knowledge of the processes driving the modeled system ligmann zielinska et al 2014 as well as limited empirical data against which to compare model outputs augusiak et al 2014 given this potential mismatch between the dimensionality of the model and the data multiple structures and or parameterizations may exist that generate outputs consistent with the data in other words the mapping from micro rules to macro structures may be many to one axtell and epstein 1994 due to stochasticity feedbacks and the non analytical nature i e no fixed structural form of many ses models windrum et al 2007 such equifinality may be difficult to identify yet due to this very nature ses models can exhibit high degrees of path dependence and non linear dynamics which may lead to significant implications if the potential for equifinality is not acknowledged equifinality is most pertinent to the calibration stage in the iterative model development cycle grimm and railsback 2005 the purpose of model calibration is to improve a model s fit to real world conditions by adjusting its parameter values and or structural representations van vliet et al 2016 national research council 2012 typically this involves finding the single best fit model to utilize for subsequent analysis proponents of the equifinality thesis argue that the possibility for multiple acceptable models should not be rejected and the model calibration process should instead constitute a mapping of the landscape into a space of feasible models beven 2006 there exist various approaches for model calibration and analysis that either explicitly or implicitly acknowledge equifinality table 1 gives a non exhaustive overview approaches explicitly dealing with equifinality have been most extensively discussed and developed in the field of hydrology e g beven and freer 2001 blazkova and beven 2009 smith et al 2008 efstratiadis and koutsoyiannis 2010 yen et al 2014 vrugt et al 2008 in these contexts the objective of allowing for multiple model configurations is typically to produce a wider uncertainty band on model predictions that is more likely to contain the true value in contrast acknowledgement of equifinality is surprisingly absent in process based model evaluations of policy impacts in ses this literature could therefore benefit from an approach that builds from frequently used methods in ses modeling to 1 identify equifinality and 2 explore its implications for policy analysis 3 approach diverse model calibration for robust policy analysis dmc rpa 3 1 overview we propose an approach for incorporating equifinality into the model development cycle to enable more robust policy analysis in socio environmental systems fig 1 fig 2 the approach consists of two steps which together we refer to as dmc rpa diverse model calibration for robust policy analysis first an optimization based model calibration procedure seeks to identify the maximally diverse set of model configurations that can explain the observed data diverse model calibration dmc second this small set of maximally different models is applied to a policy analysis robust policy analysis rpa if policy recommendations are qualitatively different over the set of diverse plausible model configurations this is evidence to suggest that these policies may not be robust in reality or that further information is needed to reduce equifinality conversely if results are consistent this provides strength to any model driven inferences and policy recommendations beyond a best fit model analysis in either case our approach enables a more robust policy analysis 1 1 we note the distinction in the use of the word robust between our approach and robust decision making rdm in rdm a robust policy is one that is beneficial over a wide range of uncertain exogenous conditions e g input data future trajectories kasprzyk et al 2013 whereas we refer to a robust policy as one that is beneficial over a range of equifinal model configurations the more general phrase robust policy analysis rpa refers to our overall approach whether the policy itself is robust or not the motivation underlying our approach is that given the complexity of ses and the paucity of empirical data in many situations we cannot claim to have all potentially relevant data for model calibration i e there are inherently objectives that are unmeasured in the calibration process thus it is not instructive to focus only on the single optimal calibration to this imperfect set of data rather it is more useful to generate a number of calibrated solutions that each perform well with respect to modeled issues and are significantly different with respect to the decisions they specify brill et al 1982 these diverse solutions may consequently behave considerably differently under conditions not modeled in the calibration process such as a policy analysis by focusing on a small number of diverse plausible solutions this approach efficiently encompasses any equifinality enabling each calibration to be individually examined and reducing the computational burden of subsequent policy experiments this kind of approach with an explicit focus on solution diversity was first proposed as a method for generating a set of diverse decision options in the context of land use planning brill et al 1982 and has since been applied as a decision support tool in other areas decarolis et al 2017 ligmann zielinska et al 2008 harrison et al 2001 in this paper we extend this work to the context of model calibration and its implications for policy analysis we note that the approach as presented in this paper focuses on abms but is also applicable to any process driven model that requires calibration e g cellular automata partial or general equilibrium models system dynamic models or other simulation based biophysical models we also note that the dmc rpa approach focuses only on model calibration and application it does not attend to other stages of the model development cycle such as model validation rather our approach simply suggests that the modeling process be modified to allow for the possibility of multiple plausible model configurations through the model analysis stage fig 2 the equifinal calibrated models could go through a further validation refinement before being applied to assess policy we discuss this in section 6 3 2 diverse model calibration dmc we implemented the dmc approach in python 3 pseudocode is given in online appendix b and the code for the case study application is hosted on comses net 2 2 https www comses net codebases 5c7710b4 f9c1 47cc ad61 8734febdb2f0 releases 1 1 0 our approach for diverse model calibration uses eaga evolutionary algorithm to generate alternatives zechman and ranjithan 2004 which is an extension of a conventional genetic algorithm ga 3 3 gas are a form of optimization inspired by darwin s theory of evolution a population of individuals is evolved toward better solutions each individual is characterized by a vector of parameters i e their genes selection and reproduction occur within the population and are mediated by each individual s fitness which in this case represents the degree to which an individual s output matches calibration data using this approach we are solving a multimodal multiobjective optimization problem efstratiadis and koutsoyiannis 2010 singh and deb 2006 in which the uncertain model parameters and or structures constitute the decision variables the objective function has two components the first representing the degree to which a model s outputs match empirical data and the second representing the degree to which a model s configuration is different from other candidate models for abms of ses which are stochastic and can exhibit nonlinear dynamics evolutionary approaches are a useful heuristic method for searching parameter spaces reed et al 2013 thiele et al 2014 so are appropriate in this context our operational definitions for various terms are given in table 2 3 2 1 optimization procedure genetic algorithm in an extension to a regular ga which consists of a single population of individuals the eaga consists of multiple subpopulations sps of individuals i e potential model configurations that coexist in the decision space algorithm 1 each sp is analogous to the population of individuals in a regular ga but each individual evolves to both increase its fitness to the empirical data i e reduce the loss in equation 1 below and increase its difference from the models in the other sps equation 4 below evolution and genetic selection occur within each sp i e there is no genetic spillover between sps and utilize standard ga evolution procedures one sp is defined a priori as the master sp the master sp seeks to find the globally optimal solution and each individual in the master sp evolves solely based on fitness i e diversity is not important solutions in other sps are considered feasible when their fitness given by equation 1 below lies within some tolerance of the best solution in the master sp e g up to 20 larger the feasibility of solutions affects the selection of parents in the eaga the binary tournament selection process selects individuals to act as parents by randomly pairing two individuals and selecting one of these to pass on its genetic material using the following heuristic if both potential parents are feasible select the more diverse of the two i e higher d q k in equation 4 below if only one potential parent is feasible select this one and if both potential parents are infeasible select the one with the better fitness equation 1 below algorithm 1 dmc procedure adapted from zechman and ranjithan 2004 image 2 3 2 2 decision variables uncertain parameters and structures each continuous uncertain model parameter is defined within some specified bounds i e constraints and potential process descriptions and categorical parameters are represented using categorical decision variables in this sense uncertain structural characteristics are treated no differently than model level parameters with the specification of both falling under the general term configuration specifically each candidate model k is characterized by a set of s configuration elements x k x 1 x s which for these calculations are each normalized to the 0 1 unit interval in the case of a categorical configuration element it can be assumed that all categories are equidistant from each other by specifying a single distance e g 1 for models that are different with respect to this element 3 2 3 objectives matching patterns and increasing disparity the first objective is to minimize the discrepancy between a set of model generated and empirically observed patterns as such our approach is a form of pattern oriented modeling for each individual k discrepancies are weighted and combined over the r patterns to give a single measure of fit 1 l k r 1 r w e i g h t r d i s c r e p a n c y m o d e l k r d a t a r the discrepancy measure or loss l is an informal measure of likelihood hartig et al 2011 smith et al 2008 similar to those used in other studies calvez and hutzler 2006 stonedahl and wilensky 2010 chica et al 2017 the discrepancy measure could take a number of forms depending on the type of pattern to be fit table 3 the second objective is a measure of difference between a given model configuration and the other candidate models which is measured as a distance in the configuration space because the goal is to evolve increasingly disparate sps this difference is calculated between each individual k in sp q and the centroid of each of the other sps s p p p q the centroid of the pth sp c p is calculated as a fitness weighted average over its individuals configuration elements i e parameters and or structures 2 c p 1 k k 1 k w e i g h t k x p k where the best fitting individual in each sp receives a weight of one the worst fitting individual receives a weight of zero and there is a linear scaling in between based on fitness the distance d between model k in sp q and s p p s centroid is calculated as the sum of the absolute differences manhattan distance in the normalized configuration space 3 d q k p s 1 s w e i g h t s x q k s c p s different weighting schemes or distance calculations may be chosen if desired finally for each model k in sp q the second component of the objective function d q k is then evaluated as the distance to the closest sp centroid 4 d q k min p p q d q k p 3 2 4 selecting models given the stochasticity in both the model and the ga the objectives equations 1 and 4 are likely to be non monotonic and will fluctuate as each sp evolves to assess convergence the modeler can visually assess the two components of the objective function and discern whether they have stabilized because the dmc evolution is likely highly influenced by the dynamics of the particular socio environmental model we do not present a quantitative measure for assessing its convergence once convergence has been reached a single solution is chosen from each sp not including the master sp to prioritize diversity among the selected models select the most diverse feasible solution within each sp 3 2 5 selecting dmc hyperparameters the dmc procedure contains several hyperparameters that need to be specified by the modeler for example as the number of sps n s p is increased the well fitting regions of the parameter space will become more crowded and in turn the solutions less diverse there is hence a tradeoff whereby if n s p is too low plausible regions of the parameter space may not be discovered and if n s p is too high the solutions lose their diversity and begin to collapse on top of each other applications of this approach should therefore explore the effect of varying n s p on the solutions reached by the dmc procedure alternatively practical considerations may drive the choice of n s p for example if the individual models are to be presented to decision makers or if subsequent policy related computational requirements are high the number has to be manageable the example presented in the original description of the eaga included four sps zechman and ranjithan 2004 other hyperparameters such as the number of generations and the population size within each sp will also affect the solutions reached again the effect of these values on the results should be assessed to encourage appropriate choices we present an example assessment of hyperparameter values for our case study application in appendix a 3 3 robust policy analysis rpa following the identification of a diverse set of model calibrations the second step is to assess the implications for system behavior under policy intervention this simply involves conducting the same policy assessment for each selected model there are two general possible classes of outcome 1 if results are consistent over the set of models we can have greater confidence in any policy recommendations and 2 if results are qualitatively different between the selected models our approach has exposed sensitivities that may have been missed in an analysis using a single best fit model in this case the equifinal models can be explored to suggest the potential socio environmental conditions or mechanisms that may give rise to the success or failure of a policy intervention or additional data can be included to attempt to restrict equifinality see the discussion in section 6 in either case we achieve a more robust policy analysis 4 case study description smallholder climate resilience using an abm of smallholder farmer resilience we apply the dmc rpa approach to generate a diverse set of models and explore whether these lead to policy related assessments that are qualitatively consistent we give a brief overview of the abm here an odd d protocol müller et al 2013 is provided in online appendix a and in williams et al 2020 4 1 abm description smallholder agricultural systems are highly vulnerable to climatic variability vermeulen et al 2012 it is therefore important to identify ways through which their climate resilience can be supported hansen et al 2019 they also exhibit key properties of complex adaptive systems de vos et al 2019 smallholder populations are highly heterogeneous in their attributes and access to capital and household level mechanisms to cope with shocks e g selling of livestock or assets can lead to path dependencies and poverty traps haider et al 2018 additionally interactions between smallholder households and agroecosystems can give rise to dynamically evolving system trajectories giller et al 2011 tittonell 2014 thus agent based modeling is an appropriate tool through which to assess these resilience dynamics bitterman and bennett 2018 schlüter et al 2019a the purpose of the abm is to provide temporal and distributional assessments of smallholder drought vulnerability and the potential household and community level effects of selected resilience enhancing strategies the abm is designed to represent an ethiopian smallholder mixed crop livestock farming system it draws from several sources of empirical data to represent the conditions of amhara in the ethiopian highlands however the model is not intended to produce policy recommendations for a specific location rather it serves as an experimental platform to evaluate the potential effects of resilience enhancing strategies in smallholder systems more generally each agent represents a single smallholder household the modeled livelihood activities include farming livestock rearing and wage based employment fig 3 agents are heterogeneous with respect to their household size land holding and risk aversion additionally each agent has preference for either maximizing wealth or leisure livestock are grazed on a combination of on farm crop residues and a communal rangeland system which each provide amounts of fodder that vary over time based on both climate and endogenously driven rangeland demand the availability of wage based employment is exogenous and does not vary over time climate affects the following model components crop yields which are calculated at an agent level on an annual basis rangeland dynamics which is simulated at the regional level on an annual basis and crop prices which vary each month at the regional level but are exogenous to the modeled system at the beginning of each year agents make decisions about how to manage their farmland fertilizer application planting date whether to buy sell livestock from their herd and how much labor to allocate to non farm wage based employment these options are represented as a finite set of livelihood options a single crop type maize is modeled these start of year decisions are made under a degree of uncertainty about the future climate and market conditions agent level beliefs about these conditions are formed from their previous experiences as well as interaction with neighboring agents following these decisions crop yields are calculated and monthly wage employment allocations are made crop yields are influenced by both agent decisions and the exogenous climate and wage employment allocations depend on the regional demand for labor fig 3 at each month throughout the year agents attempt to satisfy their food and cash consumption requirements through their own crop production food stores and cash holdings agents can buy and sell crops from the market each month as well as sell livestock as a coping measure to smooth cash and food consumption each month that an agent cannot satisfy their food requirements they are classified as food insecure the primary output of the model is this binary monthly household level representation of food security which emerges as a result of interactions between the different modeled components of the agricultural system 4 2 calibrating diverse models 4 2 1 uncertain model characteristics the abm contains a variety of uncertain parameters and structures table 4 although the differences in alternative model structures are relatively minor they are sufficient to demonstrate how the dmc procedure works to evaluate potential model structures other applications could integrate dmc more thoroughly into model structure development in addition to identifying model parameter values critically what table 4 demonstrates is that uncertain model structural representations although fundamentally different from parameters in how they affect the abm are treated no differently by the dmc approach and are simply coded as binary or categorical switches for example we allow the possibility for two alternative decision making representations expected utility maximization and satisficing e x p u t i l d m in table 4 in doing so we apply our approach to contrast alternative theories grimm et al 2005 under expected utility maximization each agent chooses the livelihood option each year that maximizes either their wealth or their leisure time depending on their preference under satisficing all agents have two levels of hierarchical objectives kaufman 1990 1 to choose the option that leads to the lowest food insecurity and 2 to choose the option that maximizes expected wealth or leisure time depending on their preference the second level objective only activates if multiple options tie with respect to the first objective these two alternatives represent different functional representations in the abm yet their reduction to a binary switch is more an issue of model design than a qualitative difference in the dmc procedure 4 2 2 patterns using data from the 2015 world bank s living standards measurement study lsms we identified eight emergent outcomes i e patterns that we wish the model to match to generate these patterns the abm was run from 2003 to 2015 and outputs from the final year of the simulation were compared against the calibration data which represent the empirical conditions in amhara in 2015 five of the patterns represent agent livelihood characteristics we represent these using histograms thus combining information from the household level into a regional pattern these distributions include non farm labor allocation agricultural labor allocation farming and livestock months of food insecurity subsistence fraction i e percent of production consumed and large livestock holdings in addition to these distributions we include three binary indicators representing desirable qualitative model level characteristics the first specifies that at least 70 of the agents choose to farm their land on average this indicator is included to encourage the generation of models in which farming is the dominant livelihood activity which is consistent with empirical data for the modeled region csa 2017 4 4 we note that the lsms reports higher farming percentages than this 89 in amhara in 2015 csa 2017 we use a more permissive value because we do not model land rental dynamics which is a common practice in the modeled region but would unnecessarily complicate the model the second indicator specifies that the grass biomass in the communal rangeland does not decrease to zero at any time to discourage unrealistic rangeland dynamics the third indicator specifies that no agent should ever have more than 80 head of livestock which further encourages livestock holdings to be consistent with the empirical lsms data to measure the discrepancy between the model outputs and the histograms we convert each histogram into its empirical cumulative distribution function ecdf and calculate the average squared difference between each abm generated and empirical ecdf step this represents a discretized version of the distribution distribution pattern type in table 3 we chose this as it bounds the maximum possible loss for each distributional pattern between zero and one meaning that each distribution exerts comparable influence on the total loss we weight all distributions equally as each distribution was chosen to represent a relevant independent livelihood characteristic an additional value of one is added to the total loss for each qualitative pattern that the model does not generate thus the overall loss is bounded between zero and eight 4 2 3 genetic algorithm we conducted an experiment with four sps five total including the master each comprised of 30 individuals run for 300 generations table 5 we chose to present the results for four sps in this article for visual clarity but we also experimented with alternative numbers of sps and population sizes see appendix a to select models we selected the feasible individual from each sp that is most distant from any other sp s centroid although the abm is stochastic for the calibration we ran a single simulation replication for each model configuration and calculated the loss from this single set of outputs we found the variability in model outputs to be much larger between model configurations than within each model configuration so opted for this approach due to computational feasibility to assess the sensitivity of each parameterization we conducted a local sensitivity analysis we systematically perturbed each parameter from its calibrated value and assessed the effect on the fit to the empirical data see appendix e 4 3 policy analysis the objective of the case study is to examine which of two resilience enhancing strategies provides the greatest benefit to climate resilience in the smallholder agricultural system the strategies include the provision of seasonal climate forecasts and a 20 increase in the availability of non farm employment opportunities these are not necessarily explicit policies at a government or institutional level but represent potential policy relevant interventions to the system federal democratic republic of ethiopia 2019 the climate forecasts give the agents information at the start of each year about the upcoming climate conditions this information is not perfect but enables the agents to make better informed agricultural decisions e g shifting their planting date or choosing to not apply fertilizer to their fields potentially increasing their climate resilience increased job availability in contrast provides an opportunity for agents to diversify their livelihoods and can act as an important source of income for agents that do not otherwise have access to land or livestock based capital given the different mechanisms through which these strategies operate they may differently affect the households and in turn the system s collective ability to respond to drought under alternative socio environmental conditions we quantify specific resilience carpenter et al 2001 using a measure of the effect of drought on the agents food security we represent droughts by imposing reductions in rainfall a 50 drought represents a year in which the rainfall is reduced by 50 this affects the crop prices rangeland dynamics and crop yields the effects on prices and rangeland dynamics are at the regional level while the effects on crop yields are both nonlinear in rainfall and spatially explicit to isolate the overall effect of a 50 drought we run two simulations one for 30 years under regular climatic variability and one for 30 years under the same climatic variability but with a 50 drought imposed in the fifth year of simulation in every month of each simulation we record the percent of agents that are food insecure i e are unable to satisfy their food consumption requirements we then take the difference between food insecurity in these two simulations giving a monthly measure of the additional percentage of the abm agents that are food insecure as a result of the drought when analyzed over time this incorporates both the initial impact of the drought and the long term recovery of food security in the years following because food security varies throughout the year this will exhibit an annual cycle to isolate and compare the strategies resilience enhancing benefits we evaluate this measure of resilience both under baseline conditions and with each of the two strategies in place we assess a strategy s overall benefit as the cumulative amount of food insecurity that it avoids in the wake of the drought i e the total number of agent insecurity months avoided in the 25 years following the drought finally we calculate the difference between the two strategies resilience benefits δ r e s as 5 δ r e s a 1 n y 5 30 m 1 12 f i a y m c l i m a t e f o r e c a s t f i a y m j o b a v a i l a b i l i t y where a indexes the n agents y indexes the years m indexes the months and f i denotes the incidence of household level food insecurity there are two levels of stochasticity relevant for this policy assessment both of which will affect the quantity in equation 5 the first level represents within model stochasticity introduced in the assignment of the agents to the landscape crop yield calculation and allocation of regional wage labor and livestock reproduction mortality to account for this we replicate each simulation 50 times 5 5 a convergence analysis law 2008 pg 502 determined that 50 replications were sufficient to achieve a relative error of 0 1 i e x n μ μ 0 1 if x n is the estimate based on n replications and μ e x with a confidence level of 90 the second level represents uncertainty associated with the drought as already described we define our droughts using single year reductions in rainfall however the ultimate effects of this on the smallholder system will depend on both the preceding and succeeding climatic conditions 6 6 for example the effects of the recent ethiopian drought in which some parts of the country only experienced 50 75 of the regular rainfall were in part exacerbated by continued dry conditions in the following year singh et al 2016 to account for this we generated 40 climate timeseries each 30 years long by repeatedly randomly sampling years from the 2000 2015 observational climate record we impose a drought in the fifth year of each timeseries finally to assess sensitivity in the policy comparison we conduct two additional experiments one with nine sps ten including the master and one assessing the effect of a 20 drought 5 case study results 5 1 diverse model calibration the master sp s solutions converge in their fit to the empirical data after approximately 25 generations of the genetic algorithm fig 4 a after approximately 50 generations the solutions in all other sps begin to become feasible fig 4a and c i e within 30 of the master sp s best solution once the solutions are feasible their diversity begins to increase fig 4b and stabilizes after approximately 175 generations overall the abm does well at matching the empirical patterns the total summed loss in the master sp is approximately 0 08 fig 4a out of a maximum possible value of 8 and all other sps are within 30 of this indicating that all qualitative patterns are matched and the levels of fit to the empirical histograms are high however all calibrated models overestimate the percentage of households with no livestock herds fig 5 and appendix b livestock represent an important coping mechanism both in reality and in the abm given this our models may underestimate resilience or lead to biased policy assessments for example climate forecasts provide the agents with information that can aid their livestock stocking destocking decisions because our model underestimates the number of households with livestock it may underestimate the benefits of climate forecasts through this mechanism in addition the calibrated models exhibit different levels of fit to the non farm labor distribution fig 5 and appendix b sp2 underestimates and sp4 overestimates the proportion of agents engaging in non farm labor some variation is to be expected in the performance of any set of equifinal models and that variation is important to provide structure to the variation in any policy relevant conclusions it is therefore important to incorporate understanding of the variations in level of fit into policy analysis based on the calibrated models as different levels of fit may imply different degrees of credibility over the model set future work could expand the range of structural representations included in the model calibration to improve the level of fit to the livestock distribution as well as include a validation step to filter out models from the calibrated set that less adequately represent reality in ways that might significantly affect the policy analysis and in turn bias conclusions 5 2 equifinality in the calibrated models the selected model configurations are diverse fig 6 suggesting that the complexity in the model allows for very different parameter sets to produce similar levels of fit to the data specifically the distance between each of the sps in the normalized parameter space is approximately 6 fig 4b given that there are 18 uncertain model elements table 4 the maximum possible distance equation 3 between any two models is 18 different parameters are more or less consistent over the model set for example the r i s k a v e r s i o n m u l t parameter which represents a dimensionless multiplier on the agents risk aversion coefficient is present over almost its entire range in the four models fig 6 this implies that the model is insensitive to changes in this parameter in contrast parameters such as p l a n t i n g f r a c t i o n are only present over a smaller range in the calibrated model set fig 6 this implies that the model is more sensitive to changes in these parameters and that only a narrow range of values produce plausible model outputs our analysis of the sensitivity of the model calibrations appendix e confirms these observations and additionally shows that the sp3 and sp4 calibrations are less stable slight perturbations in their parameters result in some of the qualitative fitting patterns not being matched thus degrading the calculated fit we observe an interesting result in the uncertain model structural representations all four models contain agents that utilize expected utility maximization to make their livelihood decisions e x p u t i l d m in fig 6 i e the satisficing decision making representation is never selected by the calibration procedure 5 3 policy analysis enhancing climate resilience with no policy in place the effect of the drought on household food security differs over the selected models fig 7 for example the model selected from sp1 exhibits the smallest drought vulnerability with only a maximum of 6 of households affected by the drought at the median simulation output in comparison in sp3 food insecurity is at the median 14 higher in the year following the drought sp3 also exhibits the highest levels of food insecurity under baseline conditions appendix b in terms of recovery in no model does food security recover completely to its level in the no drought counterfactual fig 7 showing that in all cases the drought permanently alters the livelihood trajectory of some households the differences between the sps suggest that in this case there are implications of equifinality as each of the equifinal models exhibits different behavior when applied to a situation drought event not used in the calibration when comparing the effects of the two interventions on the system s resilience all four models yield the same directional result climate forecasts offer larger potential benefits to food security in the wake of a drought than an increase in job availability figs 8 and 9 thus in spite of the differing model configurations fig 6 and baseline levels of vulnerability fig 7 there are no large implications of equifinality for our policy analysis in this case the dmc rpa approach has therefore yielded a conclusion that is likely similar to that utilizing a single model calibration but the consistency of this result over the diverse calibrated models increases the robustness of this conclusion 5 4 sensitivity analyses reducing the magnitude of the drought to 20 does not affect these conclusions the 20 drought has a smaller yet similar effect on household level food security and the climate forecast still consistently provides larger benefits under these drought conditions appendix c in an experiment with nine sps not all sps become feasible after 300 generations and due to the larger number of solutions are more closely located within the uncertain parameter space appendix d however the overall diversity of the feasible solutions is higher for example in contrast to the results above in which all models specified expected utility maximization two sps in this experiment specify satisficing as the decision making framework additionally the overall range of many continuous variables over these models is higher this higher diversity between parameterizations contributes to a higher diversity in drought responses and policy comparisons appendix d however the result from above that climate forecasts provide greater resilience benefits is demonstrated in seven of the eight retained parameterizations thus we conclude that this result is a robust one 6 discussion 6 1 dmc rpa for model development and inference the dmc rpa approach allows the researcher to retain multiple hypotheses represented by multiple disparate model configurations through the analysis stage of the model development cycle fig 2 doing so is in accordance with the notion of strong inference platt 1964 although we have focused on the implications for policy analysis the dmc approach also presents opportunities for model development model driven theory development and systems inference first the parameters and structures of the calibrated models themselves i e the conditions measured in the calibration may suggest different socio demographic and environmental contexts under which similar outcomes may be possible parameters that are especially variable or consistent may represent critical or sensitive factors in driving the empirical outcomes alternatively if one process representation is consistently selected over another this constitutes a form of combined model based and empirical evidence supporting its appropriateness for describing reality in the modeled context this could be compared against existing theory and evidence to aid in developing more generalized knowledge magliocca et al 2018 schlüter et al 2019a b second the calibrated models could be compared along some unmeasured axes i e information not assessed in the calibration process yet contained within the calibration simulations wiegand et al 2003 khatami et al 2019 if these unmeasured axes represent factors that are unobservable or unobserved in reality they could provide new information about the diverse mechanisms that may give rise to the observed outcomes acting as a complement to studies that use statistical methods to explore causal mechanisms in empirical data ferraro and hanauer 2014 alternatively unobserved factors that differ over the set of models could be identified to prioritize the collection of new empirical observations a key context within which model equifinality is salient is when model complexity outstrips the availability of calibration data schmidt et al 2020 in these contexts a single calibrated model runs the risk of being overfit to the data i e it could fit the available data well but have low ability to recreate patterns not used in its calibration van vliet et al 2016 sun et al 2016 by using multiple information rich patterns pom attempts to reduce this concern latombe et al 2011 grimm and railsback 2012 sensitivity analysis such as in appendix e can indicate the stability of each parameterization but does not indicate the degree to which the calibrated model may be overfit to the data if the model outputs exhibit an extremely high variability under conditions not used for the calibration e g a policy analysis this may indicate that overfitting is occurring calvez and hutzler 2006 however it is indeed possible that there are multiple plausible discordant model representations in any case acknowledging that there need not be a single optimal solution reduces the risk that incorrect inferences are made even if each individual solution is overfit it is possible that including new tests of emergent model characteristics i e testing against data that are observable in reality yet unmeasured in the original calibration procedure would reduce equifinality platt 1964 latombe et al 2011 guillaume et al 2019 this could involve for example a model output corroboration augusiak et al 2014 or cross validation procedure that employs patterns or tests unmeasured in the calibration process itself wang et al 2018 wiegand et al 2003 if the model is spatial it could be validated by applying it or comparing its outputs to those from a different region magliocca et al 2015 brown et al 2005 alternatively consultation with domain experts could be used to filter out unreasonable model configurations van vliet et al 2016 with respect to model development our approach is similar to the virtual laboratory and building block approaches magliocca and ellis 2016 which use pattern oriented modeling to systematically evaluate hypothesized model structures potentially representing contrasting theories or differing levels of complicatedness sun et al 2016 against empirical data our approach builds on this by identifying multiple diverse model configurations systematic procedures for model structure and parameter specification that allow for equifinality have been more extensively developed in the field of hydrology khatami et al 2019 touhidul mustafa et al 2020 and could be turned to for future inspiration in the ses modeling community in principle dmca rpa is a formalization of the pattern oriented modeling approach and we situate it within this body of research finally as we have previously stated the policy assessment can lead to two general classes of outcome either the conclusions are similar over the set of models or they are not in our case study example we observed remarkably consistent effects over the four model configurations this implies a greater level of robustness of our conclusions than if we had used a single model parameterization had we instead observed inconsistent effects these results could be used to shed light on the socio environmental conditions under which different interventions may be more or less effective helping to inform the targeting of policy interventions giller et al 2011 6 2 comparison to alternative calibration approaches there are three main features that set our approach apart from alternative model calibration methods 1 identifying a small set of n models 2 retaining these models as separate and 3 maximizing model diversity these features have both philosophical and practical implications in interpreting the model configurations and policy analysis results first our approach identifies a small set of n models in the case study application we chose n 4 as we have discussed this allows for equifinality and is more appropriate than any best fit model calibration procedure by calibrating a small set of distinct models each model configuration can be individually examined enabling enhanced inference and communication with decision makers schwartz 2012 having a small number of models is also particularly advantageous in situations when subsequent policy related experiments are computationally expensive in these situations it is desirable to not only have efficient sampling over the prior distributions for the parameters vrugt and beven 2018 yen et al 2014 but also to efficiently encompass equifinality in a small set of models mcmc based calibration approaches require repeated sampling from the posterior distribution and in high dimensional cases a large number of samples may be necessary for example the application in an initial presentation of the mcmc based dream algorithm used 2500 draws from the posterior distribution vrugt et al 2009 other implementations of multimodal evolutionary algorithms for model calibration use diversity based filtering to reduce the number of solutions chica et al 2017 moya et al 2019 so are comparable to our approach in this respect second and related to the first we do not assign a probability or relative likelihood to each model but present each model configuration and its policy assessment separately this differs from bayesian model calibration methods that estimate a posterior distribution for the model parameters structures and use this to produce a single predictive distribution ajami et al 2007 vrugt et al 2008 hartig et al 2011 touhidul mustafa et al 2020 similarly monte carlo calibration approaches typically aggregate outputs over the entire set of behavioral models beven and freer 2001 although an interesting exception exists in khatami et al 2019 conceptually our approach is motivated by the deep uncertainty in modeling socio environmental systems which makes it problematic to assign a probability to each model polasky et al 2011 in this regard the dmc rpa approach is similar to robust decision making lempert 2003 and we employ it to identify policies that are beneficial over a wide range of potential states without assigning a probability to these states if a single predictive distribution is desired however our approach does not preclude using the model fits equation 1 or preferably the success of each model in some independent validation exercise to develop informal weights for each model that we do not assign a probability to each model configuration also demonstrates some similarities and differences to both the limits of acceptability loa beven 2006 and the pom concepts grimm et al 2006 in loa models that satisfy a number of predetermined acceptable limits e g consistently produce outputs within 20 of an observed value are characterized as behavioral no behavioral model is considered to be more or less behavioral than another pom also employs an equivalent behavioral notion similarly in our approach the final model configurations are each equally considered however our approach differs in that model configurations are defined as feasible based on their fit relative to the master sp s best solution this feasibility criterion is not defined a priori but evolves with the algorithm e g dashed line in fig 4a this helps to maintain the balance between fit and diversity during the genetic algorithm s evolution but is not as strongly based on theory or domain expertise as is required for loa or pom beven 2006 third our approach is unique in how it maximizes diversity within the feasible model set while staying within a specified tolerance of the optimal model other set theoretic and mcmc based calibration approaches generally aim to maintain model diversity which is accomplished by removing or penalizing solutions that are similar in the configuration space i e the parameters and or structures that they specify singh and deb 2006 olalotiti lawal et al 2015 because of the dual objectives i e model fit and diversity our approach is technically a bi objective optimization these are not combined into a single objective nor do we seek to find a pareto optimal set of solutions our approach does not maintain that diverse models are more desirable rather that the diverse set of plausible models most efficiently encompasses equifinality the diversity component enters the objective function only in models that are feasible e g with an error within 30 of the master sp s best solution due to this treatment of the two objectives combined with the fact that we do not seek to identify a posterior distribution our approach could not to the best of our knowledge be directly integrated into an mcmc procedure at least without modification thus the dmc procedure constitutes a distinct approach to model calibration while providing important computational and inferential advantages these features of the dmc rpa approach may also present tradeoffs in certain situations importantly because the number of models n s p must be specified a priori the algorithm is limited in its ability to characterize the structure of unknown objective spaces other evolutionary approaches such as niching genetic algorithms goldberg et al 1987 miller and shaw 1996 deb et al 2002 can more flexibly identify an unknown number of local optima these can be filtered to a smaller number ex post if desired chica et al 2017 additionally although monte carlo methods are inefficient in exploring the parameter space vrugt and beven 2018 they also do not place any restrictions on the number of desired solutions in our case study example we conducted a sensitivity analysis to the n s p hyperparameter appendix a and found that increasing the number of models did not significantly affect the fitness and diversity of the solutions this showed that in this case there are many possible equifinal model representations and that this can have implications for the policy analysis appendix d thus applications of dmc rpa should carefully consider the choice of n s p additionally our approach does not consider the sensitivity of each parameterization this sensitivity can exist on at least two levels first model stochasticity and uncertainty in the input data can lead to drastically different system behavior under a single model configuration and our approach does not allow for this variability when evaluating a model configuration in contrast to probabilistic approaches olalotiti lawal et al 2015 second due to the complexity of socio environmental systems small parameter or structural changes can massively affect system behavior lempert 2002 liu et al 2007 by identifying n discrete models each defined by a fixed set of parameter structure values our approach does not give information about this sensitivity in our case study example we conducted a local sensitivity analysis to the resultant model configurations and found two of them to be highly sensitive to small changes in some of the parameters appendix e thus we recommend that sensitivity analyses are integrated with future applications of dmc rpa 6 3 potential extensions our intention has been to demonstrate diverse model calibration for robust policy analysis we did not conduct computational experiments to compare eaga to alternative optimization procedures so we do not claim that eaga represents the most computationally efficient or appropriate method for diverse model calibration in all contexts niching genetic algorithms are an alternative evolutionary approach that model a single population of solutions that evolve based on a combination of feasibility and the density of other solutions in the parametric neighborhood miller and shaw 1996 goldberg et al 1987 singh and deb 2006 multiobjective genetic algorithms have also been developed in other contexts to identify sets of pareto optimal solutions deb et al 2002 park et al 2013 komuro et al 2006 turley and ford 2009 alternatively the characteristics of this approach could potentially be integrated into mcmc based methods for example by including diversity requirements as a penalty in mcmc likelihood functions olalotiti lawal et al 2015 prioritizing diversity when sampling from mcmc generated posterior distributions e g some form of latin hypercube sampling or through multi objective mcmc algorithms although dmc rpa is not directly comparable to these other optimization approaches in particular due to the way it treats the fit and diversity objectives future work could 1 compare the effectiveness of the eaga with other adaptive sampling procedures in identifying a set of plausible diverse model configurations and 2 integrate diversity objectives more explicitly into other calibration methods within the dmc rpa approach there also exist promising avenues for future extension some of these relate to the evaluation of model fit for example for our case study we combined the loss calculated for each histogram and system level pattern into a single measure of fit equation 1 however it has been demonstrated that such aggregation can inefficiently explore the entire parameter space park et al 2013 deb et al 2002 thus a multiobjective loss measure may help to identify better calibrations additionally to prioritize the generation of parsimonious model structures a complicatedness based penalty could be integrated into the objective function magliocca and ellis 2016 when experimenting with the algorithm we noticed a tendency for non influential parameters to diverge to the extreme ends of the prescribed bounds i e 0 and 1 in fig 6 this is entirely a result of the diversity objective the algorithm exploits non influential parameters to increase the assessed diversity of the model configurations without measurably affecting the models fit to the data to reduce this effect and focus on diversity where it matters most we encourage iterative model development fig 2 integrated with sensitivity analysis as in appendix e to sequentially refine the parameters structures included in the genetic algorithm ligmann zielinska et al 2014 to more comprehensively evaluate the policies robustness using the equifinal models the dmc rpa approach could benefit from tools developed in the rdm literature for example the policy analysis could also consider uncertainty related to model inputs future exogenous conditions or elements of model configuration that cannot be fixed during the calibration process kasprzyk et al 2013 efforts could also be made to more explicitly map the calibrated parameter values and structural states to the scenario performance to identify robust regions within the configuration space lempert 2002 bryant and lempert 2010 finally in this paper we have applied dmc rpa to calibrate an agent based model using distributional data from a single point in time however the approach could be applied to any calibrated process based model in the most general sense it requires simply a model m that produces an output y that is dependent on some input parameters and or structure x i e y m x many other types of models e g system dynamics economic equilibrium biophysical simulation in many different fields e g land system science ecological economics natural resource management fit this description additionally the loss function equation 1 is very flexible it need not satisfy any statistical properties and only requires that a set of model configurations can be cardinally evaluated according to their level of acceptability thus it would be possible to integrate for example 1 timeseries data by either summing or multiplying the discrepancy measure in equation 1 at each time point vrugt and beven 2018 2 spatial features using a measure of landscape pattern similarity parker and meretsky 2004 brown et al 2005 or 3 other levels of uncertainty by either averaging losses over stochastic simulations or assessing robustness over multiple types of uncertainty lempert 2003 6 4 case study results smallholder resilience our results to the case study suggest that under the conditions of the modeled system and noting the inaccuracy in the models abilities to recreate the empirical livestock herd size distribution climate forecasts may provide superior direct benefits to smallholder drought resilience than an increase in non farm job availability this result was consistent over all four model parameterizations fig 9 and over seven of the eight parameterizations in the sensitivity analysis appendix d this suggests that dramatic improvements to resilience could be realized without the significant infrastructural investment that would be required to increase non farm employment opportunities thus informational forms of support like climate forecasting could play an important role in supporting smallholder resilience under a changing climate vermeulen et al 2012 hansen et al 2019 however appropriate communication of forecasts and integration into farmer decision making would be necessary to achieve these benefits in reality hansen et al 2011 in conjunction with adequate accuracy of the climate forecasts themselves ziervogel et al 2005 future modeling work could more thoroughly represent these elements and or extend the structural breadth and empirical grounding of the abm to increase the policy relevance of these results expected utility maximization and the rational actor constitute a common approach for representing human decision making in agent based models kremmydas et al 2018 schlüter et al 2017 klabunde and willekens 2016 groeneveld et al 2017 however this type of approach has long been criticized as not realistically representing how individuals actually make decisions simon 1955 interestingly our results fig 6 showed that models in which agents maximize their expected utility produced better levels of fit to the data than models in which agents behave as satisfiers that first attempt to ensure their food security then maximize their utility beyond this kaufman 1990 because some agents utility in the abm is represented by leisure time even under utility maximization these agents do not behave as maximizers in the traditional economic sense potentially explaining this result the dmc approach could be used in future work to evaluate and compare alternative approaches for modeling decision making based on the degree to which they generate empirically consistent behavior schlüter et al 2017 7 conclusions we have argued in this paper that given the prevalence of complex process based models in socio environmental policy analysis and the paucity of empirical data with which to calibrate these models for their intended purposes equifinality is an issue of general concern to this community we do not claim that process based models are too sensitive to be useful rather we advocate that modelers seriously consider the implications that model structure and parameterization may have on any model generated inferences the dmc rpa approach that we outline and demonstrate in this paper can be used to identify policies that perform well over an entire set of equifinal models thus supporting robust decision making alternatively the approach also can expose inconsistencies that more completely represent uncertainty in the relative benefits of policy interventions in this case divergent results may give information about the socio environmental conditions under which certain policies may be more or less beneficial in either case the dmc rpa approach facilitates more robust model development policy analysis and inference declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this project was supported by a u s national science foundation nsf infews grant 1639214 and an nsf cnh grant deb 1617364 the authors would like to thank allison kelly and uriah israel for their help with the lsms data and genetic algorithm respectively as well as the helpful comments from five anonymous reviewers that greatly improved the paper s quality and clarity appendix asupplementary data the following are the supplementary data to this article eaga pseudocodemultimedia component 1 eaga pseudocode odd d model description odd d model description appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104831 appendices a eaga hyperparameter experiments the specification of hyperparameters for the eaga presents tradeoffs between model fit model diversity and computational burden practical considerations will likely influence decisions in most applications and different abms will respond differently to hyperparameter changes thus we do not present concrete guidelines for hyperparameter selection here but qualitatively explore the effects of hyperparameter changes in our case study application the experiments we conducted are summarized in table a 1 due to stochasticity in the initialization of the eaga solutions we repeated each experiment three times i e three different random number seeds computational requirements prohibited us from running more replications but we believe our results reveal the general trends table a 1 number of model replications required for hyperparameter experiments table a 1 experiment gens n sp pop size seeds model runs n s p 250 3 20 3 45 000 5 75 000 10 150 000 15 225 000 20 300 000 25 375 000 30 450 000 1 575 000 p o p s i z e 250 5 10 3 37 500 20 75 000 30 112 500 40 150 000 375 000 note that this value includes the master sp a 1 number of subpopulations n s p the primary effect that we expect as n s p is increased is that the solutions will reduce in diversity i e the configuration space becomes more crowded we observe this effect with a dramatic decrease in diversity above three sps and then a progressive decrease beyond this fig a 1a increasing beyond 20 sps has little effect on the solution diversity suggesting that the additional solutions in the 25sp and 30sp experiments populate different regions of the configuration space varying n s p had little effect on the fitness to the empirical data in the selected solutions fig a 1b or the spacing within each sp fig a 1c this suggests that for all n s p considered the eaga is able to find a set of reasonable solutions and that 250 generations are sufficient to converge in the objective function value for these selected solutions finally at all n s p above three there were some sps that contained no feasible solutions fig a 1d thus reducing the number of model configurations that can be selected this could suggest issues with the genetic algorithm getting stuck in local minima that do not fit the data well enough to be considered feasible a larger number of ga replications may help these sps to escape these regions because the fraction is not strictly increasing in n s p this demonstrates that the configuration space is not yet fully saturated even with 30 sps there likely exist more model configurations that could satisfy the feasibility requirements choosing the appropriate number of sps given this information is clearly a subjective decision as stated in the main body of the paper practical considerations also have to be considered e g computational requirements or ease of communication for ease of display and to enable investigation of each individual sp we chose four sps five including the master in the main body of the paper however policy relevant applications investigating the robustness of policy interventions may choose to use a larger number of sps to more fully cover the model configuration space fig a 1 influence of the number of subpopulations n s p on eaga convergence the plotted values in a b and c represent the minimum maximum mean over the selected final solutions i e the most diverse feasible solution in each sp d shows the fraction of sps that contained no feasible solutions fig a 1 a 2 population size the results reveal a slight tradeoff between solution diversity and objective function value as population size is varied small population sizes are unable to achieve as good fits to the data within the modeled number of generations fig a 2b and d yet foster slightly more diverse solutions fig a2a the progressive reduction in diversity is because larger population sizes result in a greater degree of smoothing of the centroid locations as the parameter values of a larger more diverse fig a 2c population of solutions are averaged again choosing an appropriate population size is a subjective decision we chose to use a population size of 30 in our application because it achieves a good balance between diversity and fitness yet does not have issues with invalid sps fig a 2d fig a 2 influence of the population size within each sp on eaga convergence the plotted values represent the minimum maximum mean over the selected final solutions i e the closest feasible solution to each sp centroid fig a 2 b comparison of abm generated and empirical patterns fig b 1 abm empirical comparison for sp 2 fig b 1 fig b 2 abm empirical comparison for sp 3 fig b 2 fig b 3 abm empirical comparison for sp 4 fig b 3 c resilience analysis with a 20 drought fig c 1 effect of a 20 drought on household food security fig c 1 fig c 2 relative benefit of the two interventions under a 20 drought fig c 2 d case study experiments with nine sps d 1 diverse model calibration an experiment with nine sps yielded similar results but one of the sps did not become feasible after 300 generations of the eaga fig d 1 as a result only eight parameterizations were selected for the resilience analysis the final parameterizations in this experiment are on average around five units apart in the normalized parameter space fig d 1b which is slightly less diverse than the experiment with four sps and indicates a crowding of the parameter space however there are now two models that contain the satisficing decision making representation sp1 and sp3 fig d 2 and a greater overall diversity in some parameters such as the planting fraction fig d 1 eaga convergence measures with nine sps fig d 1 fig d 2 normalized parameter values with nine sps note that there were no feasible solutions in sp2 so it is not displayed here fig d 2 d 2 resilience analysis the greater overall model diversity leads to a greater diversity in the effects of drought on household food security fig d 3 in particular the maximum effect is not experienced until several years after the drought in sp3 and there is a permanently decreased level of food security in sp1 the relative benefits of the interventions are also more variable over the eight selected models fig d 3 five of these models show results similar to those in fig 9 in the main body of the paper while sps 3 4 and 9 show some evidence of a greater benefit in increased job availability however in seven of the eight retained parameterizations provision of climate forecasts provides larger benefits to resilience than increased job availability sp3 displays the most discordant behavior of the sps the main way in which sp3 differs from the other sps is in the livestock reproduction rate l s g r o w t h r a t e fig d 2 sp3 s lower livestock reproduction rate contributes to a slower recovery time in the wake of a drought fig d 3 as more livestock must be purchased to recover herd sizes and in turn food security as a result of this the additional financial capital provided by an increased job availability yields larger benefits to food security than the climate forecasts which only provide financial capital indirectly through better agricultural management fig d 3 effect of a 50 drought on household food security with nine sps note that no solutions in sp2 were feasible so it was not included in the resilience analysis fig d 3 fig d 4 comparison of policy effects with nine sps fig d 4 e sensitivity of selected model configurations we conducted a univariate sensitivity analysis to assess the stability of the selected model configurations to do so we sequentially perturbed each of the continuous parameters from their calibrated values and assessed the effect of this on the fit to the empirical data we did not include the model structural elements or the categorical parameter in this assessment we note that a univariate sensitivity analysis does not capture potential dependencies between parameters lee et al 2015 and as such we view these results in an exploratory manner the results reveal a wide variability in the stability of the models to parameter perturbations fig e 1 sp1 and sp2 are in general more stable parameterizations than sp3 and sp4 i e their shaded bands in fig e 1 are wider additional experimentation confirmed that the points at which the decline in fit increased above 20 were generally due to the model failing to generate one of the qualitative patterns thus having a value of one added to its loss sp3 and sp4 were each on the verge of not recreating one of these patterns so small deviations in many of the parameters resulted in a dramatic decrease in calculated fit some parameters particularly r f s l o p e r f i n t e r c e p t and r i s k a v e r s i o n m u l t exert very little influence on the models fit to the data given these results these parameters could likely be excluded from the dmc process with little effect on the results this kind of sensitivity analysis could inform the iterative development of a model by sequentially excluding the parameters that the model is not sensitive to fig e 1 the stability of each sp configuration the shaded areas encompass the amount of deviation each parameter can undergo before the model s fit declines by the specified amount wider shaded bands indicate more stable configurations a value of 0 5 on the horizontal axis represents a parameter being set to 50 of its scaled calibrated value fig e 1 
