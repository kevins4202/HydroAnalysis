index,text
475,lattice boltzmann method lbm is widely adopted in simulating flow and reactive transport in porous media due to its easy treatment of boundaries and high computational efficiency there have been various boundary schemes developed in the lbm due to their vital roles when dealing with fluid solid interface in the present work we aim to review and intercompare all the single node boundary schemes including the existing schemes and our newly developed schemes in the lbm through a series of numerical experiments from simple benchmarking cases to realistic applications both the accuracy and computational efficiency of all the schemes are evaluated and analyzed among the existing schemes it is known that the bounce back scheme bb is commonly employed at the boundaries in the lbm however the bb is of only first order spatial accuracy when zero velocity is not exactly located in the middle of the fluid and solid nodes which cannot be guaranteed in porous media flows due to the complex pore structures therefore fine grids are usually generated to obtain satisfactory results yet computationally expensive recently a second order single node boundary scheme ssn which is more flexible than the bb while retaining the locality has been proposed zhao et al j comput phys 329 1 15 2017 zhao and yong 2017 unfortunately the ssn fails to provide a viscosity independent permeability i e the predicted permeability is related to the fluid viscosity which is unphysical based on the ssn we first developed a magic second order single node boundary scheme mssn to overcome this shortcoming based on the chapman enskog analysis of the two relaxation time lattice boltzmann equation trt lbe model comparative study of the bb ssn mssn and another single node boundary scheme i e the improved bounce back scheme ibb the magic improved bounce back scheme mibb is then performed both theoretically and numerically numerical simulations of porous media flows in both two and three dimensions confirm that the bb mibb and mssn can obtain the viscosity independent permeability while the ibb and ssn fail to do so first of all as benchmarking tests flow through a 2d fracture is studied using different schemes the results of which show that the ibb and ssn cannot provide viscosity independent permeability while such drawbacks can be overcome by the bb mibb mssn furthermore simulations of flows through a hexagonal array of cylinders and a 3d sphere packing demonstrate that the mssn is more accurate than both the bb and mibb finally for porous media samples generated from the micro ct images where the locations of the boundaries are not exactly known the mssn is more flexible to locate the positions of the fluid solid interface than the bb and mibb which in turn can provide more accurate descriptions of the fluid flow the current intercomparison study will benefit the community by providing comprehensive review and evaluation of the performance when using different single node boundary schemes in the lbm keywords lattice boltzmann method flow in porous media single node boundary scheme benchmarking intercomparison 2010 msc 00 01 99 00 1 introduction microscopic simulations of flow and reactive transport in porous media can provide detailed information for macroscopic properties e g permeability effective diffusion coefficient etc which is essential to advance our understanding of fundamental processes in various applications such as soil groundwater contamination oil recovery co2 sequestration and so on blunt et al 2013 meng and guo 2015 among all the state of the art numerical methods shabro et al 2012 silva et al 2017 aksnes and elster 2009 narváez et al 2010 gerke et al 2018 manwart et al 2002 mostaghimi et al 2013 yang et al 2013 2016 molins et al 2014 tartakovsky and meakin 2005 such as the finite difference method fdm finite volume method fvm lattice boltzmann method lbm and smoothed particle hydrodynamics sph the lbm is one of the mostly used due to its advantage of easy treatment of complex boundaries succi et al 1989 succi 2001 zarghami et al 2014 o donnell et al 2007 when using the lbm to simulate porous media flows boundary schemes imposed at the fluid solid interface are critical for the accuracy of the results as known the bounce back bb scheme which can easily treat complex boundaries is the most widely adopted single node scheme for imposing the no slip boundary condition at the fluid solid interface in the lbm when simulating porous media flows it has been demonstrated that the lbm is of second order accuracy for simulating the bulk fluid flows in the pore space maier and bernard 2010 which however has a major constraint that zero velocity has to be exactly located in the middle of the fluid and solid nodes if using the bb i e q 1 2 in fig 1 otherwise only first order accuracy can be obtained for the bb khirevich et al 2015 such flaw is unavoidable in simulating flows in complex pore geometries e g spheres packing q 0 1 yang et al 2013 dissolution precipitation induced flows q 1 yoon et al 2015 kang et al 2002 2003 etc hence refined grids are commonly used to reduce the computational errors yet increase the computational costs this defect indeed prevents the bb based lbm from more practical applications various boundary schemes for no slip boundary conditions have been developed in recent years to overcome such issues such as the bouzidi yu model and multi reflection type boundary schemes filippova and hänel 1998 bouzidi et al 2001 yu et al 2003 guo et al 2002 ginzburg and d humieres 2003 kao and yang 2008 however most of these interpolation extrapolation based schemes are not localized which will introduce extra numerical errors in those narrow pore throats where only few grids exist that are not fine enough for interpolations extrapolations pan et al 2006 therefore the above mentioned interpolation extrapolation based methods are not feasible for simulating flows in porous media in other words boundary schemes that are more accurate while retaining the locality are still desirable for simulating flows in complex geometries recently zhao et al developed a second order single node boundary scheme for no slip boundary conditions in the two relaxation time lattice boltzmann equation trt lbe model with asymptotic analysis zhao and yong 2017 this boundary scheme can achieve second order accuracy for any q 0 1 while retaining the locality which is promising for simulating flow in porous media what s more the bouzidi yu model is also a localized scheme for special cases with q 1 named as the improved bounce back scheme ibb in the present study bouzidi et al 2001 yu et al 2003 which may be more accurate than the original bb for dissolution precipitation induced flows to our best knowledge the accuracy of both the ssn and ibb for simulating porous media flows has not been evaluated yet another crucial issue for simulating porous media flows using the lbm is to overcome the defect that the computed permeability is related to the fluid viscosity pan et al 2006 according to refs d humiéres and ginzburg 2009 ginzburg et al 2008 both the multiple relaxation time lattice boltzmann equation model mrt lbe and the two relaxation time lattice boltzmann equation trt lbe model can overcome this drawback if the free relaxation parameters are properly selected since the trt lbe model is easier to implement and is computationally more efficient than the mrt lbe it is thus adopted in the present study in the trt lbe model the steady solution of ν u ν is the kinematic fluid viscosity and u is the fluid velocity in the pore space for the bulk fluid region only depends on the magic parameter λ λ λ where λ 3 ν and λ is related to the free relaxation rate in the trt lbe model which is defined in appendix a ginzburg et al 2008 based on darcy s law 1 ν u d k f p d where u d is the average velocity f is the external force and pd is the average pressure the permeability k yields the same solution by keeping λ as a constant which is independent from the fluid viscosity ginzburg et al 2008 except for the bulk fluid region the numerical errors induced from selected boundary schemes e g the bouzidi yu model etc depend on the fluid viscosity leading to a viscosity dependent permeability even when λ is kept as constant ginzburg et al 2008 it is noted that the numerical errors in the bb are independent from the fluid viscosity if λ is kept as constant which means that the trt lbe bb can provide a viscosity independent permeability correlation nevertheless for the localized ssn and ibb it is still not clear whether these two methods can provide a viscosity independent permeability which needs further assessments in the present work we first analyze the numerical errors of the existing single node boundary schemes i e the bb ibb and ssn using the chapman enskog analysis based on which the ibb and ssn are then improved to obtain the viscosity independent permeability comparative study on all the single node boundary schemes for simulating porous media flows is further conducted through a series of numerical experiments from simple benchmarking cases to realistic porous media samples the rest of the paper is organized as follows the single node boundary schemes are explained in section 2 with numerical tests and discussions presented in section 3 finally a brief summary is given in section 4 2 lattice boltzmann equation model 2 1 governing equations in the present study pore scale flow in porous media can be described by the incompressible stokes equations as follows khirevich et al 2015 pan et al 2006 talon et al 2012 2 u 0 ν 2 u p f where u is the velocity vector i e u u v in two dimensions and u u v w in three dimensions in the pore space p denotes the pressure ν is the kinetic viscosity and f is the external force generally the no slip boundary condition is imposed at the fluid solid interface 2 2 two relaxation time lattice boltzmann equation model the incompressible trt lbe model used for solving eq 2 is presented here in the lbm the space is discretized into regular lattices all particle distribution functions pdfs are assumed to move with a series of discrete velocities on the nodes in two dimensions the most popular discrete velocity model is the d2q9 two dimension nine velocity which is defined as 3 c i c 0 0 i 0 c cos i 1 π 2 sin i 1 π 2 i 1 2 3 4 2 c cos i 5 π 2 π 4 sin i 5 π 2 π 4 i 5 6 7 8 where c δ x δ t is the lattice speed δx and δt are the lattice spacing and time step respectively in addition the d3q19 three dimension nineteen velocity model is the mostly used in 3d scenarios which is expressed as 4 c i c 0 0 0 i 0 c 1 0 0 c 0 0 c 0 0 1 i 1 6 c 1 1 0 c 1 0 1 c 0 1 1 i 7 18 the evolution equation in the present trt lbe model consists of two steps i e collision and streaming which reads as 5 c o l l i s i o n f i x t f i x t g i g i δ t f i s t r e a m i n g f i x c i δ t t δ t f i x t where fi x t is the pdf at position x and time t with the velocity ci f i x t is the post collision distribution function and fi is the forcing term which is expressed as 6 f i ω i c i f c s 2 where c s 1 3 c is the lattice sound speed in addition g i λ n i where λ τ 1 λ and λ are respectively relaxation times related to the symmetric f i and anti symmetric f i terms in the pdf and n i is the non equilibrium distribution function expressed as 7 n i f i e i in eq 7 f i is defined as 8 f i 1 2 f i f i f i 1 2 f i f i where i denotes the opposite direction of i e i and e i are the symmetric and anti symmetric terms in the equilibrium distribution function which is defined as 9 e i ω i ρ e i ω i c i u c s 2 where ρ is related to the pressure as p c s 2 ρ based on the discrete velocity model employed in the present study if ϕ is a variable denoting g f e and n and ϕ ϕ f g n e is equivalent with ϕ x t then it is noted that ϕ i ϕ i and ϕ i ϕ i in this sense f i f i finally it is worth mentioning that δ t 1 and c 1 khirevich et al 2015 ginzburg et al 2008 talon et al 2012 are employed in the present study 2 3 boundary schemes in the lbm for simulating flow in porous media three different single node boundary schemes in lbm i e the bounce back scheme bb the improved bounce back scheme ibb and the single node second order accuracy scheme ssn developed for the no slip boundary condition in simulating porous media flows are introduced in this section in addition specific parameters for the viscosity independent permeability in each boundary scheme are analyzed based on the trt lbe model mentioned above 2 3 1 bounce back bb scheme as shown in fig 1 f i at x f is unknown after the streaming step which needs to be determined by the boundary condition in the bounce back scheme the unknown distribution function can be obtained following 10 f i x f t δ t f i x f t where x f in fig 1 represents any fluid node whose neighboring node is solid it is also noticed that f i fi and f i can be written as 11a f i e i e i n i n i 11b f i e i e i n i n i 11c f i e i e i 1 1 λ g i 1 1 λ g i f i substituting eq 11 into eq 10 we can obtain the closure relation as follows 12 e i 1 2 f i 1 2 g i λ g i 0 which is equivalent to the eq 32 presented in refs khirevich et al 2015 in addition g i and g i can be obtained analytically based on the chapman enskog analysis details can be found in appendix a which are expressed as 13 g i ω i β u α c i α c i β c s 2 g i ω i c i α α p f α λ β 2 u α c i β 2 c s 2 λ 3 β 2 u α e where e ω i 3 c i α c i β 2 c i α c s 2 it is clear that the bounce back scheme satisfies the closure relation automatically when c i α 0 for ciαciβ 0 the following equation can be further obtained by substituting eqs 9 and 13 into eq 12 14 j α 1 2 β j α c i β 2 3 λ β 2 j α c i β 2 ω i c i α 0 which is noticed to be the second order taylor series at x f as 2λ 3 is equal to the corresponding coefficient in the taylor expansion furthermore we should note that 1 the zero velocity location i e the fluid solid interface is half a lattice away from x f i e q 1 2 fig 1 in this scheme 2 if jα has a linear profile the last term becomes zero meaning this scheme is accurate as for the parabolic flow the bb is accurate if 2λ 3 is equal to the corresponding coefficient in the taylor expansion otherwise numerical errors would occur for this method i e the velocity at the fluid solid interface is not exactly zero however we can see clearly from eq 14 that this type of error only depends on λ which is the reason why the bb can provide a viscosity independent permeability correlation as λ is kept constant 3 for cases with q 1 2 the numerical error for this method is of first order suggesting that the bb has first order accuracy in space in addition the second order accuracy can be achieved as q 1 2 4 j α u α f α 2 is used for the output instead of the uα based on eq 14 we can see that the value of λ for different flows is not identical e g flow in straight and rotated channel khirevich et al 2015 talon et al 2012 considering the complex pore geometries the exact value of λ for porous media flows is difficult to obtain however λ calculated from the steady poiseuille flow in a channel with the rotated angle ranging from 0 to π 4 by taking the tortuosity into consideration has been validated to work quite well even for cases like flow through sandstone khirevich et al 2015 talon et al 2012 to implement the boundary condition accurately in a rotated channel fig 2 we can similarly obtain the following equation in the rotated coordinate system x y 15 λ λ λ 3 θ 8 3 θ 1 θ c i n y 2 1 2 1 where θ 0 π 4 is the rotated angle n y sin θ cos θ is the normal unit vector of the wall the limits for λ are 3 16 for the straight channel and 3 8 for the channel with θ π 4 respectively eq 15 is noticed to be the same as reported in refs khirevich et al 2015 2 3 2 improved bounce back ibb scheme for dissolution precipitation induced flows the fluid solid interface is located on lattices i e q 1 in fig 1 as mentioned only first order accuracy can be obtained if the bb is adopted for this problem we further introduce an improved bounce back scheme which is a reduced case of the bouzidi yu model as q 1 to achieve second order accuracy bouzidi et al 2001 yu et al 2003 based on fig 3 the implementation of this scheme consists of the following steps 1 at time t post collision pdfs f are known at x y 1 which will then stream to its neighboring nodes after the streaming step f i x y 0 t δ t and f i x y 2 t δ t can be obtained as 16 f i x y 0 t δ t f i x y 1 t f i x y 2 t δ t f i x y 1 t 2 for the node x y 0 at time t δ t f i x y 0 t δ t can be obtained using the modified bounce back scheme he et al 1997 as f i x y 0 t δ t f i x y 0 t δ t up to now both the pdfs in the i direction at x y 0 and x y 2 are known 3 to obtain the unknown f i x y 1 t δ t we employ the following interpolation as 17 f i x y 1 t δ t 1 2 f i x y 0 t δ t f i x y 2 t δ t based on the last two steps the above equation can be rewritten as 18 f i x f t δ t 1 2 f i x f t f i x f t considering that the post collision pdfs in eq 18 are determined locally the present scheme is localized in addition the modified bounce back scheme employed in step 2 and the interpolation in step 3 are of second order accuracy he et al 1997 hence the ibb is expected to have second order accuracy in space we would like to point out that this scheme can be viewed as a special case of the boundary schemes proposed in bouzidi et al 2001 yu et al 2003 with q 1 however to distinguish the present scheme with these two schemes we denote it as the improved bounce back scheme here we then analyze the parameters for the viscosity independent permeability in this scheme similarly f i can be written as f i e i e i 1 1 λ g i 1 1 λ g i f i based on eq 18 we can obtain the closure relation for the ibb as 19 e i g i λ g i 1 2 g i 0 with the aid of eqs 9 and 13 eq 19 can be rewritten as ciαciβ 0 20 j α β j α c i β 2 3 λ β 2 j α c i β 2 1 6 λ β j α c i β 2 ω i c i α 0 it is clear that the velocity at the fluid solid interface is related to the fluid viscosity in this scheme even as λ is constant suggesting the viscosity dependent permeability for this scheme in addition it should be noted that j α u α in this model which is slightly different from that in the bb motivated by the works in refs d humiéres and ginzburg 2009 ginzburg et al 2008 a magic improved bounce back scheme mibb is proposed based on the ibb as 21 f i x f t δ t 1 2 f i x f t f i x f t 1 2 g i where g i can be calculated locally as g i λ i f i e i where f i and e i are known variables at the previous time step x f t the closure relation for the mibb then becomes 22 j α β j α c i β 2 3 λ β 2 j α c i β 2 ω i c i α 0 we can see that the velocity at the fluid solid interface is now independent from the fluid viscosity as λ is kept constant leading to a viscosity independent permeability in this scheme similarly we note that 1 the zero velocity location is one lattice away from x f i e q 1 fig 1 2 for the linear flow this scheme is exactly accurate as for the parabolic flow the method is accurate if 2λ 3 is equal to the corresponding coefficient in the taylor expansion otherwise the velocity at the fluid solid interface is not exactly zero 3 for cases with q 1 the numerical error from this method is of first order suggesting that the mibb has first order accuracy in space in addition the second order accuracy can be achieved as q 1 finally the value of λ for the rotated channel ranging from 0 to π 4 is obtained as based on eq 22 23 λ λ λ 3 θ 2 3 θ 1 θ c i n y 2 1 2 1 the limits for λ are 3 4 for the straight channel and 3 2 for the channel with θ π 4 respectively 2 3 3 single node second order accuracy ssn scheme here the second order single node ssn scheme proposed in ref zhao and yong 2017 zhao 2017 for no slip boundary conditions is introduced which can be expressed as 24 f i x f t δ t 2 q 1 2 q f i x f t f i 1 1 2 q f i x f t we then derive the closure relation for this scheme in the same way as presented above which can be expressed as 25 e i q g i λ g i g i 0 based on eqs 9 and 13 we can rewrite eq 25 as 26 j α q β j α c i β 2 3 λ β 2 j α c i β 2 1 3 λ β 2 j α c i β 2 ω i c i α 0 similarly the velocity at the fluid solid interface in this scheme is also related to the fluid viscosity as λ is constant in addition it is noted that the velocity at the fluid solid interface is larger than that in the ibb for the same λ and λ as q 1 to obtain viscosity independent permeability we further propose the following magic second order single node boundary scheme mssn which can be expressed as 27 f i x f t δ t 2 q 1 2 q f i x f t f i 1 1 2 q f i x f t g i here g i can also be computed locally i e g i λ i f i e i where f i and e i are known variables at the previous time step x f t which is the same as in section 2 3 2 the closure relation for the mssn reads as 28 j α q β j α c i β 2 3 λ β 2 j α c i β 2 ω i c i α 0 where j α u α this method is also expected to obtain a viscosity independent permeability correlation due to the fact that the velocity at the fluid solid interface is independent from the fluid viscosity as λ is kept constant we would like to mention that 1 the zero velocity location can be any distance from x f ranging from 0 to 1 i e q 0 1 fig 1 2 for the linear flow this scheme is exactly accurate as for the parabolic flow the method is exactly accurate if 2λ 3 is equal to the corresponding coefficient in the taylor expansion otherwise the velocity at the fluid solid interface is not exactly zero 3 the second order accuracy can be achieved for q 0 1 which indicates that the mssn can be more accurate and flexible than the bb and ibb mibb for simulating flows in porous media with curved boundaries 4 equation 28 is noticed to be the same as the closure relation for the linear interpolation extrapolation based methods in ref ginzburg et al 2008 detailed study on the effect of λ on the computed permeability for the linear interpolation extrapolation based methods has been conducted in ref khirevich et al 2015 which can be directly used in the present method 5 both the correction terms in the mibb and mssn can be computed using local non equilibrium distribution functions in the reverse direction at the previous time step therefore the direction of solid surface is not required which is the same as in the bb finally the value of λ for the rotated channel ranging from 0 to π 4 is expressed as by using eq 28 29 λ λ λ 3 q 2 θ 2 3 θ 1 θ c i n y 2 1 2 1 q 0 1 the limits for λ are 3q 2 4 for the straight channel and 3q 2 2 for the channel with θ π 4 respectively a brief summary on two important parameters q and λ and order of accuracy for all the single node boundary schemes are displayed in table 1 as shown we can see that 1 the bb ibb mibb can achieve second order accuracy only if q 1 2 1 1 respectively while the ssn mssn has second order accuracy for all q 0 1 2 the bb mibb mssn can obtain a viscosity independent permeability relation as λ is properly selected following table 1 while ibb and ssn cannot 3 results and discussions a series of numerical experiments are conducted to evaluate the performance of all the single node boundary schemes based on simple benchmarking cases and realistic porous media applications first flow through a 2d fracture which consists of two parallel plates is simulated furthermore flows through two dimensional heterogeneous porous media cylinder array and sandstone are studied finally numerical simulations of pore scale flow through a 3d sphere packing are further performed permeability and the permeability viscosity relations calculated from all the boundary schemes are presented for comparison the performances i e accuracy permeability viscosity relation etc of all the methods are then evaluated and intercompared 3 1 permeability calculations as known flow through porous media with small reynolds numbers are described by darcy s law as 30 u d k μ p d g where u d is the darcy velocity defined as the volume averaged velocity in the flow field μ is the dynamic viscosity of the fluid pd represents the pressure and g is the external force thus permeability of the pore structure can be evaluated numerically based on eq 30 as 31 k μ u d α p d g α where gα is the external force in the α direction both the permeability of the two dimensional and three dimensional porous media with specified structures are computed by this method in the following tests 3 2 flow through a 2d fracture to validate the theoretical analysis for the accurate implementation of the no slip boundary condition in section 2 3 the pressure gradient driven flow through a 2d fracture that includes two parallel plates is simulated we first consider a horizontally located channel with θ 0 the computational domain is defined as 0 y h 0 x l where h 2 l the initial and boundary conditions employed in this case are expressed as follows 32 u x y 0 v x y 0 0 u x 0 t u x h t v x 0 t v x h t 0 based on eq 32 an analytic solution for this problem can be obtained as 33 u y g ν h 2 2 y h y 2 h 2 where g p x is the pressure gradient ν is the kinematic viscosity of the fluid with eqs 31 and 33 we can then obtain the permeability of the fracture as talon et al 2012 34 k a h 2 12 where ka represents the analytic permeability which is served as the reference solution in this problem the dimensionless characteristic parameter reynolds number re is defined as re u max h ν where u max g h 2 8 ν is the maximum velocity obtained from eq 33 here a constant external force in the x direction is employed to drive the flow with re 1 boundary conditions at the inlet outlet are periodic while no slip boundary conditions in eq 32 are imposed on the upper and bottom walls the convergence criterion in our simulations is 35 e u i j u x t 1000 δ t u x t i j u x t 1000 δ t 10 6 where eu is the global relative error gre according to the theoretical analyses in section 2 3 the no slip boundary condition can be accurately implemented as we select values of q and λ in our simulations following table 2 which will be first validated considering that the boundary location can be within any distance between 0 and 1 in the ssn mssn two different q i e q 1 2 and 1 are tested here as shown in fig 4 the errors between the calculated permeability and the analytic results for the bb mibb and mssn are up to the machine accuracy suggesting the accurate implementation of the no slip boundary condition in these boundary schemes we proceed to investigate the relation between the permeability and fluid viscosity the permeability computed from the ibb and ssn is observed to increase with the fluid viscosity even as λ is kept as constant fig 4 in addition it is also noted that the permeability calculated from the ssn with q 1 increases faster with the fluid viscosity than that from the ibb as mentioned in section 2 3 the ssn with q 1 has a larger numerical error for the velocity at the fluid solid interface than the ibb for the same fluid and λ these results clearly confirm the theoretical analyses presented in section 2 3 it is worth mentioning that similar results can be obtained for flows in other rotated fractures representative results for the flow in a channel with θ π 4 are presented in figs 4 c 4 d which again justify the theoretical analyses in section 2 3 we further investigate the convergence rates of the bb mibb and mssn for the no slip boundary condition which are all of second order accuracy when λ is not consistent with the value in table 2 according to the results in section 2 3 here we also select the channel flow with θ 0 and π 4 for demonstration specifically we simulate flows with τ 0 8 for all cases numerical results are displayed in fig 5 which indicate that all the three methods can achieve second order accuracy suggesting the validation of the results in section 2 3 due to the complexity of the porous structure the zero velocity boundary location is not always exactly located at q 1 2 and 1 for the bb and mibb respectively to further investigate the effect of the boundary location i e zero velocity location on the accuracy of the bb mibb and mssn different values of q i e q 1 4 and 3 4 are then tested for simplicity we only present results for the case with θ 0 in simulations the value of λ for each boundary scheme also follows table 2 here we simulate fluid flow with a viscosity 1 10 i e τ 0 8 as the representative as displayed in fig 6 it can be observed that both the bb and mibb are of first order accuracy now as for the mssn the computational error approaches to the machine accuracy as λ 3 q 2 4 due to the accurate implementation of the no slip boundary condition which is not presented in fig 6 we further consider λ 3q 2 4 i e λ 3 16 and 3 4 for q 1 4 and 3 4 in our simulations respectively in the mssn this method is observed to be of second order accuracy fig 6 which again confirms the analysis in section 2 3 in summary the ibb and ssn cannot provide viscosity independent permeability which on the other hand is overcome by the bb mibb mssn in addition the bb and mibb can achieve second order accuracy only if q 1 2 and q 1 respectively otherwise only first order accuracy can be obtained for both the bb and mibb however the mssn is of second order accuracy for any q ranging from 0 to 1 which makes it more accurate than bb and mibb especially for simulating flow in porous media with curved boundaries 3 3 flow through a hexagonal array of cylinders flow through two dimensional porous media with curved boundaries i e hexagonal array of cylinders hac is simulated in this section as displayed in fig 7 h is the length of the computational domain and a is the radius of the cylinder the flow is driven by a constant external force in the x direction with re u d d ν 0 1 where d 2 a here we set a h 0 3 leading to a porosity ϵ 0 43 no slip boundary conditions are imposed at the fluid solid interface while periodic boundary conditions are employed on all the other boundaries the relation between the computed permeability and fluid viscosity is first investigated considering that the permeability viscosity relations for different λ are similar only the results for two special values of λ i e λ 3 16 and 3 4 are presented in fig 8 as shown the permeability from the bb mibb and mssn are independent from the fluid viscosity as λ is kept constant while which increase with the fluid viscosity in the ibb and ssn in addition it is also noted that the permeability calculated from the ssn with q 1 increases faster with the fluid viscosity than that of the ibb as mentioned in section 2 3 the ssn has a larger error for the velocity at the fluid solid interface than the ibb for the same λ and fluid all these results again confirm the theoretical analyses in section 2 3 we further investigate the accuracy of all the single node boundary schemes since the ibb ssn cannot obtain viscosity independent permeability the results from these two methods will not be presented in what follows due to the fact that there is no analytical solution for this case a reference solution is first calculated as noticed in refs khirevich et al 2015 talon et al 2012 the bb with λ 1 8 3 8 can provide relatively more accurate predictions of permeability for porous media the permeability computed from the bb with two fine grid simulations i e h 1000 δ x and 2000δx are displayed in fig 9 as we can see the permeability calculated from different resolutions and λ almost converge to the same value which is then served as the reference solution the computed permeability for different grid sizes and λ are then presented in fig 10 as shown we can see that 1 the bb is generally more accurate than the mibb due to the fact the average boundary location is closer to 0 5 than 1 for this case considering that the curved boundary location is exactly resolved in the mssn it is found to be the most accurate 2 the calculated permeability from the bb and mibb fluctuate with the increase of grid size which can be attributed to the fact the geometry varies with the grid size due to the zig zag cartesian grids in these two methods no fluctuations are observed for the results from the mssn in other words the computed permeability from the mssn converges to the reference solution with the increase of the grid resolution for all λ 3 the mssn can achieve almost second order accuracy only if λ 1 8 and 3 16 what s more results from the mssn with λ 3 4 can be less accurate than those of the bb as mentioned in ref khirevich et al 2015 λ 3 q 2 4 3 4 is proper for curved boundaries due to the fact that 0 q 1 as λ 3 4 all links in the mssn are second order inaccurate leading to larger computational errors therefore results from λ 1 8 and 3 16 are more accurate than λ 3 4 and 1 for the mssn similar results are also reported in refs khirevich et al 2015 for more information about the effect of λ on the accuracy of the computed permeability one can refer to refs khirevich et al 2015 talon et al 2012 3 4 flow through a berea micromodel in this section we simulate flow through a more complicated porous media sample i e berea micromodel fig 11 with a porosity ϵ 0 31 boek and venturoli 2010 the permeability for this structure obtained from the experiment is 445 35md boek and venturoli 2010 which is served as the reference solution for the following numerical results in our simulations the flow is driven by a constant external force f y 10 5 lattice unit in the y direction periodic boundary conditions are employed on the inlet outlet and left right boundaries while no slip boundary conditions are imposed at the fluid solid interface the grid size used in our simulations is h l 301 376 the permeability viscosity relations for all the boundary schemes are first tested and displayed in fig 12 due to the fact that the permeability viscosity relations for different λ are similar only the results with λ 3 16 are presented again we can observe that the computed permeability is independent from the fluid viscosity for the bb mibb and mssn while the ibb and ssn cannot provide the viscosity independent permeability to investigate the accuracy of the bb mibb and mssn the computed permeability are then compared with the experimental data based on the case ν 1 10 i e τ 0 8 in simulations two different values of q in the mssn i e q 1 2 and 1 are first employed the effect of λ on the computed permeability are studied based on the three boundary schemes as shown in fig 13 it is found that 1 the results from mssn with q 1 2 and 1 are almost the same as those from the bb and mibb for the same λ respectively similar results are also observed in section 3 2 2 the computed permeability from all boundary schemes increase with λ specifically the relative error for results from the bb mssn with q 1 2 mssn 1 and the experimental data are less than 15 as λ 1 8 3 16 which then increases as λ 3 16 however the relative errors from the mibb and mssn with q 1 mssn 2 are much larger than those from the bb and mssn with q 1 2 e g the smallest error from the mibb and the mssn with q 1 is about 70 for λ 1 8 1 for the bb and mssn 1 the zero velocity location is imposed at the middle of the fluid and solid lattices while in the mibb and mssn 2 the zero velocity location is put on the fluid solid interface as pointed out in ref talon et al 2012 the position of the interface for micro ct images is not precisely known since it depends on the resolution and segmentation procedure applied to separate the voids from the solids therefore the bb and mssn 1 can be more accurate than the mibb and mssn 2 if the average boundary location is close to 0 5 rather than 1 while for situations in which the average boundary location is close to 1 the mibb is believed to be more accurate than the bb finally it is worth mentioning that the bb can only locate the solid interface half way between the two grid points located in the fluid and the solid which cannot always be accurate for all the micro ct images while we can further employ the mssn with q 0 4 to calculate the permeability of the berea micromodel suggesting the zero velocity location is 0 4 lattice away from the fluid node the value of λ adopted in simulations is λ 3q 2 4 3q 2 2 in which q 0 4 as we can see in fig 14 the relative errors between the computed permeability and the experimental data are less than 20 for the employed λ which is much better than the bb and mssn 1 specifically the numerical results fall in the experimental interval as λ 0 12 0 192 in summary the mssn with q 1 2 and 1 can provide identical results with the bb and mibb for the same λ respectively in addition the mssn can locate the position of the solid interface within any distance from 0 to 1 by adjusting q the permeability for the porous media which is generated from the micro ct images can be more accurate than the bb and mibb to this end we would like to point out the best value of q strongly depends on the resolution of the micro ct images which can be approximated by analyzing the experimentally measured permeability for instance assuming that we have a stack of images with the same resolution for a rock sample and we also have experimentally measured permeability for a portion of the images we can then tune q based on the available experimental data and employ the same q to calculate the permeability for the remaining images furthermore q 0 5 is suggested in simulations if no experimental measured permeability is available meaning that the solid wall is assumed to locate at the middle between a void and solid pixel 3 5 flow through a random sphere packing a more realistic application i e flow through a column packed with mono dispersed micro beads fig 15 yang et al 2013 2016 is simulated in this section as displayed in fig 15 the column is packed with 6864 mono dispersed micro beads the diameter of the column is d 8 8 m m with a length l 16 8 m m in addition all the micro beads have the same size i e d p 500 μ m leading to a porosity ϵ 0 4267 the flow is driven by a constant velocity at the inlet i e x 0 m m with re ρ p u 0 d p μ 0 2563 where ρp and μ are respectively the density and dynamic viscosity of water ρ p 997 561 k g m 3 μ 8 887 10 4 p a s the pressure at the outlet is kept constant and the no slip boundary condition is imposed at the fluid solid interface similarly the permeability viscosity relation is studied based on the grid resolution d 110 δ x the λ derived analytically for the straight channel is found to be quite accurate for the sphere packing according to ref khirevich et al 2015 hence λ 3 16 and 3 4 are taken for bb and mibb respectively in addition λ 3 16 is also found to be quite accurate for the linear interpolation extrapolation based methods khirevich et al 2015 since the numerical error in the mssn is the same as the linear interpolation extrapolation based methods khirevich et al 2015 ginzburg et al 2008 here we also take λ 3 16 for the mssn as displayed in fig 16 the bb mibb mssn can overcome the defect of viscosity dependent permeability while the ibb and ssn cannot furthermore the permeability predicted by the ssn with q 1 increases faster with the fluid viscosity than the ibb the same results have been presented in the above sections which will not be analyzed here for simplicity we proceed to investigate the accuracy of the single node boundary schemes since the results from the ibb and ssn are unphysical they are not presented here in our simulations four different grid resolutions i e d 110 δ x 132δx 165δx and 220δx are employed again we take ν 1 10 τ 0 8 as the representative case due to the fact that the calculated permeability is independent from ν in those boundary schemes permabilities calculated from the bb mibb and mssn are then compared to the results in ref yang et al 2016 considering that the boundary scheme adopted for the lbm in yang et al 2016 cannot provide a viscosity independent permeability either ginzburg et al 2008 we use the results from the finite volume method yang et al 2016 as the reference solution i e results simulated by the tethys using a structured mesh with a resolution d 440 δ x the reference permeability from the tethys i e k r e f 4 25 10 6 c m 2 is noticed to be the same as that predicted by the well known carman kozeny equation carman 1937 as shown in fig 17 it can be observed that 1 the calculated permeability for the bb and mssn converge to the reference solution with the increase of grid size it is noticed that the results from the bb and the mssn are similar as d 165δx which agree well with the reference solution the relative error is less than 3 while the mssn is more accurate than the bb for coarse grid simulations which is attributed to its higher accuracy specifically the relative error for d 110 δ x in the mssn are about 5 less than that from the bb 2 the relative error between the permeability predicted by the mibb and the reference solution decreases with the increase of grid resolution but it is still about 100 larger than the reference solution for d 220 δ x it is reasonable that the mibb with λ 3 4 locates the fluid solid interface one lattice away from the fluid node leading to the inaccurate description for almost all the boundary locations therefore large computational errors occur in this method while the bb with λ 3 16 puts the zero velocity location half a lattice away from the fluid node which can approximate the curved boundary much better than the mibb hence the results from the bb is more accurate than the mibb as mentioned the boundary location is accurately resolved in the mssn which leads to the best results among all the single node boundary schemes in addition it is observed that the relative error of the bb for d 220 δ x is larger than d 165 δ x in the bb since the computational domain is discretized into lattices a curved surface is approximated by zig zag staircases thus its morphology cannot be precisely resolved unlike the body fitted mesh in addition the no slip boundary is located half a lattice away from the solid wall suggesting that the actual location of the curved surface may vary if using different resolutions in general the computational error of the permeability will decrease with increased resolutions but the monotonous convergence cannot be guaranteed similar results are also presented in section 3 3 finally we would like to mention that our results and conclusions are consistent with those reported in pan et al 2006 to accelerate the computations an in house developed gpu scheme is implemented using the sparse matrix mode proposed in huang et al 2015 all simulations are run on a single workstation with two cpus intel xeon e5 2643 and one gpu titan xp the comparison on the performance of the bb mssn and mibb is illustrated in table 3 as shown 1 the bb is the most efficient among the three methods and 2 the mssn and mibb have comparable efficiency considering that both f i x f t i e the post collision distribution function in the same direction as the unknown distribution function at the boundary node and g i x f t i e the correction term need to be computed in the mssn as well as the mibb the computational costs are similar between the two but more expensive than the bb finally we would like to point out that the effect of λ on the accuracy for the permeability prediction for flows in sphere packing and tomographic geometries has been discussed in detail for both the bb and linear interpolation extrapolation based methods in refs khirevich et al 2015 talon et al 2012 since the numerical error in the mssn is the same as the linear interpolation extrapolation based methods khirevich et al 2015 ginzburg et al 2008 the choice of λ in the linear interpolation extrapolation based methods can be directly used for the mssn for more information on the choice of λ for porous media flows interested readers can refer to ref khirevich et al 2015 ginzburg et al 2008 talon et al 2012 4 conclusion single node boundary schemes are of great importance in simulating porous media flows using the lattice boltzmann method in this work three existing single node boundary schemes i e the bounce back bb scheme the improved bounce back ibb scheme and the single node second order ssn boundary scheme are first analyzed using the chapman enskog analysis theoretical results indicate that the ibb and ssn cannot obtain viscosity independent permeability which are then improved by using the magic versions of the ibb and ssn mibb and mssn to overcome this defect intercomparisons among all the single node schemes i e the bb ibb mibb ssn mssn for simulating porous media flows are conducted numerical simulations of flows through two and three dimensional porous media confirm that the predicted permeability is independent from the fluid viscosity in the bb mibb and mssn while the unphysical result i e the viscosity dependent permeability relation is observed in the ibb and ssn the theoretical and numerical results also demonstrate that 1 the bb and ibb are of second order accuracy when the boundary location are half a q 1 2 and one lattice q 1 away from the fluid node while the mssn can achieve second order accuray for any q 0 1 furthermore the mssn can obtain the same results as the bb and mibb with the same λ and fluid viscosity by setting q 1 2 and 1 respectively 2 according to the theoretical analysis e g eqs 14 and 22 in section 2 3 both the bb and mibb have only first order for 0 5 q 1 in addition the computational errors are proportional to q 0 5 and q 1 for the bb and mibb respectively hence the mibb is more accurate than the bb as q 0 75 for flows with curved boundaries the bb is generally more accurate than the mibb which can be attributed to the fact that the average distance between the fluid node and the solid boundary location is close to 0 5 rather than 1 in this type of flows while the mibb can be more accurate than the bb for situations where q is close to 1 q 0 75 if used in porous media samples such as those generated from high resolution micro ct images in which the solid boundaries can be accurately resolved i e q is close to 1 in such cases 3 the mssn is more flexible than the bb and mibb to approximate the boundary location which is found to be the most accurate for simulations of flows through porous media with curved boundaries generated from the micro ct images the present study is expected to advance our understanding of the existing single node boundary schemes and serve as a comprehensive review evaluation of their performances which can help to choose appropriate boundary schemes in the lbm for more applications credit authorship contribution statement xuhui meng conceptualization methodology software validation formal analysis investigation writing original draft writing review editing visualization liang wang methodology writing original draft weifeng zhao methodology xiaofan yang conceptualization methodology investigation writing original draft writing review editing supervision project administration funding acquisition declaration of competing interest the authors declare no conflict of interest to report acknowledgement xuhui meng and xiaofan yang are funded by the national natural science foundation of china nsfc grant no 41877183 and the special program for applied research on super computation of the nsfc guangdong joint fund the second phase liang wang is funded by the national natural science foundation of china nsfc grant no 51776068 the authors would like to express their thanks to prof zhenhua chai for fruitful discussions appendix a chapman enskog analysis of the trt lbe model in order to obtain the closure relations for the single node boundary schemes the solutions for g i and g i in eqs 12 19 and 25 are needed here we first employ the taylor series expansion to f i x c i δ t δ t around x t as follows a 1 f i x c i δ t δ t f i x t δ t d i f i x t δ t 2 2 d i 2 f i x t o δ t 3 where d i t c i we then adopt the following expansions to the space and time for multi scale analysis a 2 α ε α 1 t ε 2 t 2 where ε is a small parameter in addition fi x t can be expanded as a 3 f i f i 0 ε f i 1 ε 2 f i 2 o ε 3 where f i 0 e i e i substitutiong eqs a 1 a 3 to eq 5 we can obtain the following equations a 4 ε 1 δ t c i α α 1 e i e i p i 1 m i 1 δ t f i 1 a 5 ε 2 t 2 f i 0 δ t d i f i 1 δ t 2 2 f i 0 p i 2 m i 2 where p i g i and m i g i rewrite eq a 4 in the opposite direction i e i and sum these two equations we can obtain a 6 p i 1 δ t c i α α 1 e i m i 1 δ t c i α α 1 e i δ t f i 1 in addition f i 1 f i 1 f i 1 the following equation can be obtained a 7 f i 1 δ t p i 1 λ δ t m i 1 λ substituting eq a 7 into eq a 5 we can obtain the solutions of p i 2 and m i 2 as a 8 p i 2 δ t t 2 e i δ t 2 λ c i α α 1 2 e i a 9 m i 2 δ t t 2 e i δ t 2 λ c i α α 1 2 e i where λ 1 λ 1 2 and λ 1 λ 1 2 for the steady state considered in the present study we can obtain the expressions of g i and g i as δ t 1 a 10 g i ε p i 1 ε 2 p i 2 ω i β u α c i α c i β c s 2 a 11 g i ε m i 1 ε 2 m i 2 ω i c i α α p f α λ β 2 u α c i β 2 c s 2 λ 3 β 2 u α e in which a 12 α p f α λ 3 β 2 u α 
475,lattice boltzmann method lbm is widely adopted in simulating flow and reactive transport in porous media due to its easy treatment of boundaries and high computational efficiency there have been various boundary schemes developed in the lbm due to their vital roles when dealing with fluid solid interface in the present work we aim to review and intercompare all the single node boundary schemes including the existing schemes and our newly developed schemes in the lbm through a series of numerical experiments from simple benchmarking cases to realistic applications both the accuracy and computational efficiency of all the schemes are evaluated and analyzed among the existing schemes it is known that the bounce back scheme bb is commonly employed at the boundaries in the lbm however the bb is of only first order spatial accuracy when zero velocity is not exactly located in the middle of the fluid and solid nodes which cannot be guaranteed in porous media flows due to the complex pore structures therefore fine grids are usually generated to obtain satisfactory results yet computationally expensive recently a second order single node boundary scheme ssn which is more flexible than the bb while retaining the locality has been proposed zhao et al j comput phys 329 1 15 2017 zhao and yong 2017 unfortunately the ssn fails to provide a viscosity independent permeability i e the predicted permeability is related to the fluid viscosity which is unphysical based on the ssn we first developed a magic second order single node boundary scheme mssn to overcome this shortcoming based on the chapman enskog analysis of the two relaxation time lattice boltzmann equation trt lbe model comparative study of the bb ssn mssn and another single node boundary scheme i e the improved bounce back scheme ibb the magic improved bounce back scheme mibb is then performed both theoretically and numerically numerical simulations of porous media flows in both two and three dimensions confirm that the bb mibb and mssn can obtain the viscosity independent permeability while the ibb and ssn fail to do so first of all as benchmarking tests flow through a 2d fracture is studied using different schemes the results of which show that the ibb and ssn cannot provide viscosity independent permeability while such drawbacks can be overcome by the bb mibb mssn furthermore simulations of flows through a hexagonal array of cylinders and a 3d sphere packing demonstrate that the mssn is more accurate than both the bb and mibb finally for porous media samples generated from the micro ct images where the locations of the boundaries are not exactly known the mssn is more flexible to locate the positions of the fluid solid interface than the bb and mibb which in turn can provide more accurate descriptions of the fluid flow the current intercomparison study will benefit the community by providing comprehensive review and evaluation of the performance when using different single node boundary schemes in the lbm keywords lattice boltzmann method flow in porous media single node boundary scheme benchmarking intercomparison 2010 msc 00 01 99 00 1 introduction microscopic simulations of flow and reactive transport in porous media can provide detailed information for macroscopic properties e g permeability effective diffusion coefficient etc which is essential to advance our understanding of fundamental processes in various applications such as soil groundwater contamination oil recovery co2 sequestration and so on blunt et al 2013 meng and guo 2015 among all the state of the art numerical methods shabro et al 2012 silva et al 2017 aksnes and elster 2009 narváez et al 2010 gerke et al 2018 manwart et al 2002 mostaghimi et al 2013 yang et al 2013 2016 molins et al 2014 tartakovsky and meakin 2005 such as the finite difference method fdm finite volume method fvm lattice boltzmann method lbm and smoothed particle hydrodynamics sph the lbm is one of the mostly used due to its advantage of easy treatment of complex boundaries succi et al 1989 succi 2001 zarghami et al 2014 o donnell et al 2007 when using the lbm to simulate porous media flows boundary schemes imposed at the fluid solid interface are critical for the accuracy of the results as known the bounce back bb scheme which can easily treat complex boundaries is the most widely adopted single node scheme for imposing the no slip boundary condition at the fluid solid interface in the lbm when simulating porous media flows it has been demonstrated that the lbm is of second order accuracy for simulating the bulk fluid flows in the pore space maier and bernard 2010 which however has a major constraint that zero velocity has to be exactly located in the middle of the fluid and solid nodes if using the bb i e q 1 2 in fig 1 otherwise only first order accuracy can be obtained for the bb khirevich et al 2015 such flaw is unavoidable in simulating flows in complex pore geometries e g spheres packing q 0 1 yang et al 2013 dissolution precipitation induced flows q 1 yoon et al 2015 kang et al 2002 2003 etc hence refined grids are commonly used to reduce the computational errors yet increase the computational costs this defect indeed prevents the bb based lbm from more practical applications various boundary schemes for no slip boundary conditions have been developed in recent years to overcome such issues such as the bouzidi yu model and multi reflection type boundary schemes filippova and hänel 1998 bouzidi et al 2001 yu et al 2003 guo et al 2002 ginzburg and d humieres 2003 kao and yang 2008 however most of these interpolation extrapolation based schemes are not localized which will introduce extra numerical errors in those narrow pore throats where only few grids exist that are not fine enough for interpolations extrapolations pan et al 2006 therefore the above mentioned interpolation extrapolation based methods are not feasible for simulating flows in porous media in other words boundary schemes that are more accurate while retaining the locality are still desirable for simulating flows in complex geometries recently zhao et al developed a second order single node boundary scheme for no slip boundary conditions in the two relaxation time lattice boltzmann equation trt lbe model with asymptotic analysis zhao and yong 2017 this boundary scheme can achieve second order accuracy for any q 0 1 while retaining the locality which is promising for simulating flow in porous media what s more the bouzidi yu model is also a localized scheme for special cases with q 1 named as the improved bounce back scheme ibb in the present study bouzidi et al 2001 yu et al 2003 which may be more accurate than the original bb for dissolution precipitation induced flows to our best knowledge the accuracy of both the ssn and ibb for simulating porous media flows has not been evaluated yet another crucial issue for simulating porous media flows using the lbm is to overcome the defect that the computed permeability is related to the fluid viscosity pan et al 2006 according to refs d humiéres and ginzburg 2009 ginzburg et al 2008 both the multiple relaxation time lattice boltzmann equation model mrt lbe and the two relaxation time lattice boltzmann equation trt lbe model can overcome this drawback if the free relaxation parameters are properly selected since the trt lbe model is easier to implement and is computationally more efficient than the mrt lbe it is thus adopted in the present study in the trt lbe model the steady solution of ν u ν is the kinematic fluid viscosity and u is the fluid velocity in the pore space for the bulk fluid region only depends on the magic parameter λ λ λ where λ 3 ν and λ is related to the free relaxation rate in the trt lbe model which is defined in appendix a ginzburg et al 2008 based on darcy s law 1 ν u d k f p d where u d is the average velocity f is the external force and pd is the average pressure the permeability k yields the same solution by keeping λ as a constant which is independent from the fluid viscosity ginzburg et al 2008 except for the bulk fluid region the numerical errors induced from selected boundary schemes e g the bouzidi yu model etc depend on the fluid viscosity leading to a viscosity dependent permeability even when λ is kept as constant ginzburg et al 2008 it is noted that the numerical errors in the bb are independent from the fluid viscosity if λ is kept as constant which means that the trt lbe bb can provide a viscosity independent permeability correlation nevertheless for the localized ssn and ibb it is still not clear whether these two methods can provide a viscosity independent permeability which needs further assessments in the present work we first analyze the numerical errors of the existing single node boundary schemes i e the bb ibb and ssn using the chapman enskog analysis based on which the ibb and ssn are then improved to obtain the viscosity independent permeability comparative study on all the single node boundary schemes for simulating porous media flows is further conducted through a series of numerical experiments from simple benchmarking cases to realistic porous media samples the rest of the paper is organized as follows the single node boundary schemes are explained in section 2 with numerical tests and discussions presented in section 3 finally a brief summary is given in section 4 2 lattice boltzmann equation model 2 1 governing equations in the present study pore scale flow in porous media can be described by the incompressible stokes equations as follows khirevich et al 2015 pan et al 2006 talon et al 2012 2 u 0 ν 2 u p f where u is the velocity vector i e u u v in two dimensions and u u v w in three dimensions in the pore space p denotes the pressure ν is the kinetic viscosity and f is the external force generally the no slip boundary condition is imposed at the fluid solid interface 2 2 two relaxation time lattice boltzmann equation model the incompressible trt lbe model used for solving eq 2 is presented here in the lbm the space is discretized into regular lattices all particle distribution functions pdfs are assumed to move with a series of discrete velocities on the nodes in two dimensions the most popular discrete velocity model is the d2q9 two dimension nine velocity which is defined as 3 c i c 0 0 i 0 c cos i 1 π 2 sin i 1 π 2 i 1 2 3 4 2 c cos i 5 π 2 π 4 sin i 5 π 2 π 4 i 5 6 7 8 where c δ x δ t is the lattice speed δx and δt are the lattice spacing and time step respectively in addition the d3q19 three dimension nineteen velocity model is the mostly used in 3d scenarios which is expressed as 4 c i c 0 0 0 i 0 c 1 0 0 c 0 0 c 0 0 1 i 1 6 c 1 1 0 c 1 0 1 c 0 1 1 i 7 18 the evolution equation in the present trt lbe model consists of two steps i e collision and streaming which reads as 5 c o l l i s i o n f i x t f i x t g i g i δ t f i s t r e a m i n g f i x c i δ t t δ t f i x t where fi x t is the pdf at position x and time t with the velocity ci f i x t is the post collision distribution function and fi is the forcing term which is expressed as 6 f i ω i c i f c s 2 where c s 1 3 c is the lattice sound speed in addition g i λ n i where λ τ 1 λ and λ are respectively relaxation times related to the symmetric f i and anti symmetric f i terms in the pdf and n i is the non equilibrium distribution function expressed as 7 n i f i e i in eq 7 f i is defined as 8 f i 1 2 f i f i f i 1 2 f i f i where i denotes the opposite direction of i e i and e i are the symmetric and anti symmetric terms in the equilibrium distribution function which is defined as 9 e i ω i ρ e i ω i c i u c s 2 where ρ is related to the pressure as p c s 2 ρ based on the discrete velocity model employed in the present study if ϕ is a variable denoting g f e and n and ϕ ϕ f g n e is equivalent with ϕ x t then it is noted that ϕ i ϕ i and ϕ i ϕ i in this sense f i f i finally it is worth mentioning that δ t 1 and c 1 khirevich et al 2015 ginzburg et al 2008 talon et al 2012 are employed in the present study 2 3 boundary schemes in the lbm for simulating flow in porous media three different single node boundary schemes in lbm i e the bounce back scheme bb the improved bounce back scheme ibb and the single node second order accuracy scheme ssn developed for the no slip boundary condition in simulating porous media flows are introduced in this section in addition specific parameters for the viscosity independent permeability in each boundary scheme are analyzed based on the trt lbe model mentioned above 2 3 1 bounce back bb scheme as shown in fig 1 f i at x f is unknown after the streaming step which needs to be determined by the boundary condition in the bounce back scheme the unknown distribution function can be obtained following 10 f i x f t δ t f i x f t where x f in fig 1 represents any fluid node whose neighboring node is solid it is also noticed that f i fi and f i can be written as 11a f i e i e i n i n i 11b f i e i e i n i n i 11c f i e i e i 1 1 λ g i 1 1 λ g i f i substituting eq 11 into eq 10 we can obtain the closure relation as follows 12 e i 1 2 f i 1 2 g i λ g i 0 which is equivalent to the eq 32 presented in refs khirevich et al 2015 in addition g i and g i can be obtained analytically based on the chapman enskog analysis details can be found in appendix a which are expressed as 13 g i ω i β u α c i α c i β c s 2 g i ω i c i α α p f α λ β 2 u α c i β 2 c s 2 λ 3 β 2 u α e where e ω i 3 c i α c i β 2 c i α c s 2 it is clear that the bounce back scheme satisfies the closure relation automatically when c i α 0 for ciαciβ 0 the following equation can be further obtained by substituting eqs 9 and 13 into eq 12 14 j α 1 2 β j α c i β 2 3 λ β 2 j α c i β 2 ω i c i α 0 which is noticed to be the second order taylor series at x f as 2λ 3 is equal to the corresponding coefficient in the taylor expansion furthermore we should note that 1 the zero velocity location i e the fluid solid interface is half a lattice away from x f i e q 1 2 fig 1 in this scheme 2 if jα has a linear profile the last term becomes zero meaning this scheme is accurate as for the parabolic flow the bb is accurate if 2λ 3 is equal to the corresponding coefficient in the taylor expansion otherwise numerical errors would occur for this method i e the velocity at the fluid solid interface is not exactly zero however we can see clearly from eq 14 that this type of error only depends on λ which is the reason why the bb can provide a viscosity independent permeability correlation as λ is kept constant 3 for cases with q 1 2 the numerical error for this method is of first order suggesting that the bb has first order accuracy in space in addition the second order accuracy can be achieved as q 1 2 4 j α u α f α 2 is used for the output instead of the uα based on eq 14 we can see that the value of λ for different flows is not identical e g flow in straight and rotated channel khirevich et al 2015 talon et al 2012 considering the complex pore geometries the exact value of λ for porous media flows is difficult to obtain however λ calculated from the steady poiseuille flow in a channel with the rotated angle ranging from 0 to π 4 by taking the tortuosity into consideration has been validated to work quite well even for cases like flow through sandstone khirevich et al 2015 talon et al 2012 to implement the boundary condition accurately in a rotated channel fig 2 we can similarly obtain the following equation in the rotated coordinate system x y 15 λ λ λ 3 θ 8 3 θ 1 θ c i n y 2 1 2 1 where θ 0 π 4 is the rotated angle n y sin θ cos θ is the normal unit vector of the wall the limits for λ are 3 16 for the straight channel and 3 8 for the channel with θ π 4 respectively eq 15 is noticed to be the same as reported in refs khirevich et al 2015 2 3 2 improved bounce back ibb scheme for dissolution precipitation induced flows the fluid solid interface is located on lattices i e q 1 in fig 1 as mentioned only first order accuracy can be obtained if the bb is adopted for this problem we further introduce an improved bounce back scheme which is a reduced case of the bouzidi yu model as q 1 to achieve second order accuracy bouzidi et al 2001 yu et al 2003 based on fig 3 the implementation of this scheme consists of the following steps 1 at time t post collision pdfs f are known at x y 1 which will then stream to its neighboring nodes after the streaming step f i x y 0 t δ t and f i x y 2 t δ t can be obtained as 16 f i x y 0 t δ t f i x y 1 t f i x y 2 t δ t f i x y 1 t 2 for the node x y 0 at time t δ t f i x y 0 t δ t can be obtained using the modified bounce back scheme he et al 1997 as f i x y 0 t δ t f i x y 0 t δ t up to now both the pdfs in the i direction at x y 0 and x y 2 are known 3 to obtain the unknown f i x y 1 t δ t we employ the following interpolation as 17 f i x y 1 t δ t 1 2 f i x y 0 t δ t f i x y 2 t δ t based on the last two steps the above equation can be rewritten as 18 f i x f t δ t 1 2 f i x f t f i x f t considering that the post collision pdfs in eq 18 are determined locally the present scheme is localized in addition the modified bounce back scheme employed in step 2 and the interpolation in step 3 are of second order accuracy he et al 1997 hence the ibb is expected to have second order accuracy in space we would like to point out that this scheme can be viewed as a special case of the boundary schemes proposed in bouzidi et al 2001 yu et al 2003 with q 1 however to distinguish the present scheme with these two schemes we denote it as the improved bounce back scheme here we then analyze the parameters for the viscosity independent permeability in this scheme similarly f i can be written as f i e i e i 1 1 λ g i 1 1 λ g i f i based on eq 18 we can obtain the closure relation for the ibb as 19 e i g i λ g i 1 2 g i 0 with the aid of eqs 9 and 13 eq 19 can be rewritten as ciαciβ 0 20 j α β j α c i β 2 3 λ β 2 j α c i β 2 1 6 λ β j α c i β 2 ω i c i α 0 it is clear that the velocity at the fluid solid interface is related to the fluid viscosity in this scheme even as λ is constant suggesting the viscosity dependent permeability for this scheme in addition it should be noted that j α u α in this model which is slightly different from that in the bb motivated by the works in refs d humiéres and ginzburg 2009 ginzburg et al 2008 a magic improved bounce back scheme mibb is proposed based on the ibb as 21 f i x f t δ t 1 2 f i x f t f i x f t 1 2 g i where g i can be calculated locally as g i λ i f i e i where f i and e i are known variables at the previous time step x f t the closure relation for the mibb then becomes 22 j α β j α c i β 2 3 λ β 2 j α c i β 2 ω i c i α 0 we can see that the velocity at the fluid solid interface is now independent from the fluid viscosity as λ is kept constant leading to a viscosity independent permeability in this scheme similarly we note that 1 the zero velocity location is one lattice away from x f i e q 1 fig 1 2 for the linear flow this scheme is exactly accurate as for the parabolic flow the method is accurate if 2λ 3 is equal to the corresponding coefficient in the taylor expansion otherwise the velocity at the fluid solid interface is not exactly zero 3 for cases with q 1 the numerical error from this method is of first order suggesting that the mibb has first order accuracy in space in addition the second order accuracy can be achieved as q 1 finally the value of λ for the rotated channel ranging from 0 to π 4 is obtained as based on eq 22 23 λ λ λ 3 θ 2 3 θ 1 θ c i n y 2 1 2 1 the limits for λ are 3 4 for the straight channel and 3 2 for the channel with θ π 4 respectively 2 3 3 single node second order accuracy ssn scheme here the second order single node ssn scheme proposed in ref zhao and yong 2017 zhao 2017 for no slip boundary conditions is introduced which can be expressed as 24 f i x f t δ t 2 q 1 2 q f i x f t f i 1 1 2 q f i x f t we then derive the closure relation for this scheme in the same way as presented above which can be expressed as 25 e i q g i λ g i g i 0 based on eqs 9 and 13 we can rewrite eq 25 as 26 j α q β j α c i β 2 3 λ β 2 j α c i β 2 1 3 λ β 2 j α c i β 2 ω i c i α 0 similarly the velocity at the fluid solid interface in this scheme is also related to the fluid viscosity as λ is constant in addition it is noted that the velocity at the fluid solid interface is larger than that in the ibb for the same λ and λ as q 1 to obtain viscosity independent permeability we further propose the following magic second order single node boundary scheme mssn which can be expressed as 27 f i x f t δ t 2 q 1 2 q f i x f t f i 1 1 2 q f i x f t g i here g i can also be computed locally i e g i λ i f i e i where f i and e i are known variables at the previous time step x f t which is the same as in section 2 3 2 the closure relation for the mssn reads as 28 j α q β j α c i β 2 3 λ β 2 j α c i β 2 ω i c i α 0 where j α u α this method is also expected to obtain a viscosity independent permeability correlation due to the fact that the velocity at the fluid solid interface is independent from the fluid viscosity as λ is kept constant we would like to mention that 1 the zero velocity location can be any distance from x f ranging from 0 to 1 i e q 0 1 fig 1 2 for the linear flow this scheme is exactly accurate as for the parabolic flow the method is exactly accurate if 2λ 3 is equal to the corresponding coefficient in the taylor expansion otherwise the velocity at the fluid solid interface is not exactly zero 3 the second order accuracy can be achieved for q 0 1 which indicates that the mssn can be more accurate and flexible than the bb and ibb mibb for simulating flows in porous media with curved boundaries 4 equation 28 is noticed to be the same as the closure relation for the linear interpolation extrapolation based methods in ref ginzburg et al 2008 detailed study on the effect of λ on the computed permeability for the linear interpolation extrapolation based methods has been conducted in ref khirevich et al 2015 which can be directly used in the present method 5 both the correction terms in the mibb and mssn can be computed using local non equilibrium distribution functions in the reverse direction at the previous time step therefore the direction of solid surface is not required which is the same as in the bb finally the value of λ for the rotated channel ranging from 0 to π 4 is expressed as by using eq 28 29 λ λ λ 3 q 2 θ 2 3 θ 1 θ c i n y 2 1 2 1 q 0 1 the limits for λ are 3q 2 4 for the straight channel and 3q 2 2 for the channel with θ π 4 respectively a brief summary on two important parameters q and λ and order of accuracy for all the single node boundary schemes are displayed in table 1 as shown we can see that 1 the bb ibb mibb can achieve second order accuracy only if q 1 2 1 1 respectively while the ssn mssn has second order accuracy for all q 0 1 2 the bb mibb mssn can obtain a viscosity independent permeability relation as λ is properly selected following table 1 while ibb and ssn cannot 3 results and discussions a series of numerical experiments are conducted to evaluate the performance of all the single node boundary schemes based on simple benchmarking cases and realistic porous media applications first flow through a 2d fracture which consists of two parallel plates is simulated furthermore flows through two dimensional heterogeneous porous media cylinder array and sandstone are studied finally numerical simulations of pore scale flow through a 3d sphere packing are further performed permeability and the permeability viscosity relations calculated from all the boundary schemes are presented for comparison the performances i e accuracy permeability viscosity relation etc of all the methods are then evaluated and intercompared 3 1 permeability calculations as known flow through porous media with small reynolds numbers are described by darcy s law as 30 u d k μ p d g where u d is the darcy velocity defined as the volume averaged velocity in the flow field μ is the dynamic viscosity of the fluid pd represents the pressure and g is the external force thus permeability of the pore structure can be evaluated numerically based on eq 30 as 31 k μ u d α p d g α where gα is the external force in the α direction both the permeability of the two dimensional and three dimensional porous media with specified structures are computed by this method in the following tests 3 2 flow through a 2d fracture to validate the theoretical analysis for the accurate implementation of the no slip boundary condition in section 2 3 the pressure gradient driven flow through a 2d fracture that includes two parallel plates is simulated we first consider a horizontally located channel with θ 0 the computational domain is defined as 0 y h 0 x l where h 2 l the initial and boundary conditions employed in this case are expressed as follows 32 u x y 0 v x y 0 0 u x 0 t u x h t v x 0 t v x h t 0 based on eq 32 an analytic solution for this problem can be obtained as 33 u y g ν h 2 2 y h y 2 h 2 where g p x is the pressure gradient ν is the kinematic viscosity of the fluid with eqs 31 and 33 we can then obtain the permeability of the fracture as talon et al 2012 34 k a h 2 12 where ka represents the analytic permeability which is served as the reference solution in this problem the dimensionless characteristic parameter reynolds number re is defined as re u max h ν where u max g h 2 8 ν is the maximum velocity obtained from eq 33 here a constant external force in the x direction is employed to drive the flow with re 1 boundary conditions at the inlet outlet are periodic while no slip boundary conditions in eq 32 are imposed on the upper and bottom walls the convergence criterion in our simulations is 35 e u i j u x t 1000 δ t u x t i j u x t 1000 δ t 10 6 where eu is the global relative error gre according to the theoretical analyses in section 2 3 the no slip boundary condition can be accurately implemented as we select values of q and λ in our simulations following table 2 which will be first validated considering that the boundary location can be within any distance between 0 and 1 in the ssn mssn two different q i e q 1 2 and 1 are tested here as shown in fig 4 the errors between the calculated permeability and the analytic results for the bb mibb and mssn are up to the machine accuracy suggesting the accurate implementation of the no slip boundary condition in these boundary schemes we proceed to investigate the relation between the permeability and fluid viscosity the permeability computed from the ibb and ssn is observed to increase with the fluid viscosity even as λ is kept as constant fig 4 in addition it is also noted that the permeability calculated from the ssn with q 1 increases faster with the fluid viscosity than that from the ibb as mentioned in section 2 3 the ssn with q 1 has a larger numerical error for the velocity at the fluid solid interface than the ibb for the same fluid and λ these results clearly confirm the theoretical analyses presented in section 2 3 it is worth mentioning that similar results can be obtained for flows in other rotated fractures representative results for the flow in a channel with θ π 4 are presented in figs 4 c 4 d which again justify the theoretical analyses in section 2 3 we further investigate the convergence rates of the bb mibb and mssn for the no slip boundary condition which are all of second order accuracy when λ is not consistent with the value in table 2 according to the results in section 2 3 here we also select the channel flow with θ 0 and π 4 for demonstration specifically we simulate flows with τ 0 8 for all cases numerical results are displayed in fig 5 which indicate that all the three methods can achieve second order accuracy suggesting the validation of the results in section 2 3 due to the complexity of the porous structure the zero velocity boundary location is not always exactly located at q 1 2 and 1 for the bb and mibb respectively to further investigate the effect of the boundary location i e zero velocity location on the accuracy of the bb mibb and mssn different values of q i e q 1 4 and 3 4 are then tested for simplicity we only present results for the case with θ 0 in simulations the value of λ for each boundary scheme also follows table 2 here we simulate fluid flow with a viscosity 1 10 i e τ 0 8 as the representative as displayed in fig 6 it can be observed that both the bb and mibb are of first order accuracy now as for the mssn the computational error approaches to the machine accuracy as λ 3 q 2 4 due to the accurate implementation of the no slip boundary condition which is not presented in fig 6 we further consider λ 3q 2 4 i e λ 3 16 and 3 4 for q 1 4 and 3 4 in our simulations respectively in the mssn this method is observed to be of second order accuracy fig 6 which again confirms the analysis in section 2 3 in summary the ibb and ssn cannot provide viscosity independent permeability which on the other hand is overcome by the bb mibb mssn in addition the bb and mibb can achieve second order accuracy only if q 1 2 and q 1 respectively otherwise only first order accuracy can be obtained for both the bb and mibb however the mssn is of second order accuracy for any q ranging from 0 to 1 which makes it more accurate than bb and mibb especially for simulating flow in porous media with curved boundaries 3 3 flow through a hexagonal array of cylinders flow through two dimensional porous media with curved boundaries i e hexagonal array of cylinders hac is simulated in this section as displayed in fig 7 h is the length of the computational domain and a is the radius of the cylinder the flow is driven by a constant external force in the x direction with re u d d ν 0 1 where d 2 a here we set a h 0 3 leading to a porosity ϵ 0 43 no slip boundary conditions are imposed at the fluid solid interface while periodic boundary conditions are employed on all the other boundaries the relation between the computed permeability and fluid viscosity is first investigated considering that the permeability viscosity relations for different λ are similar only the results for two special values of λ i e λ 3 16 and 3 4 are presented in fig 8 as shown the permeability from the bb mibb and mssn are independent from the fluid viscosity as λ is kept constant while which increase with the fluid viscosity in the ibb and ssn in addition it is also noted that the permeability calculated from the ssn with q 1 increases faster with the fluid viscosity than that of the ibb as mentioned in section 2 3 the ssn has a larger error for the velocity at the fluid solid interface than the ibb for the same λ and fluid all these results again confirm the theoretical analyses in section 2 3 we further investigate the accuracy of all the single node boundary schemes since the ibb ssn cannot obtain viscosity independent permeability the results from these two methods will not be presented in what follows due to the fact that there is no analytical solution for this case a reference solution is first calculated as noticed in refs khirevich et al 2015 talon et al 2012 the bb with λ 1 8 3 8 can provide relatively more accurate predictions of permeability for porous media the permeability computed from the bb with two fine grid simulations i e h 1000 δ x and 2000δx are displayed in fig 9 as we can see the permeability calculated from different resolutions and λ almost converge to the same value which is then served as the reference solution the computed permeability for different grid sizes and λ are then presented in fig 10 as shown we can see that 1 the bb is generally more accurate than the mibb due to the fact the average boundary location is closer to 0 5 than 1 for this case considering that the curved boundary location is exactly resolved in the mssn it is found to be the most accurate 2 the calculated permeability from the bb and mibb fluctuate with the increase of grid size which can be attributed to the fact the geometry varies with the grid size due to the zig zag cartesian grids in these two methods no fluctuations are observed for the results from the mssn in other words the computed permeability from the mssn converges to the reference solution with the increase of the grid resolution for all λ 3 the mssn can achieve almost second order accuracy only if λ 1 8 and 3 16 what s more results from the mssn with λ 3 4 can be less accurate than those of the bb as mentioned in ref khirevich et al 2015 λ 3 q 2 4 3 4 is proper for curved boundaries due to the fact that 0 q 1 as λ 3 4 all links in the mssn are second order inaccurate leading to larger computational errors therefore results from λ 1 8 and 3 16 are more accurate than λ 3 4 and 1 for the mssn similar results are also reported in refs khirevich et al 2015 for more information about the effect of λ on the accuracy of the computed permeability one can refer to refs khirevich et al 2015 talon et al 2012 3 4 flow through a berea micromodel in this section we simulate flow through a more complicated porous media sample i e berea micromodel fig 11 with a porosity ϵ 0 31 boek and venturoli 2010 the permeability for this structure obtained from the experiment is 445 35md boek and venturoli 2010 which is served as the reference solution for the following numerical results in our simulations the flow is driven by a constant external force f y 10 5 lattice unit in the y direction periodic boundary conditions are employed on the inlet outlet and left right boundaries while no slip boundary conditions are imposed at the fluid solid interface the grid size used in our simulations is h l 301 376 the permeability viscosity relations for all the boundary schemes are first tested and displayed in fig 12 due to the fact that the permeability viscosity relations for different λ are similar only the results with λ 3 16 are presented again we can observe that the computed permeability is independent from the fluid viscosity for the bb mibb and mssn while the ibb and ssn cannot provide the viscosity independent permeability to investigate the accuracy of the bb mibb and mssn the computed permeability are then compared with the experimental data based on the case ν 1 10 i e τ 0 8 in simulations two different values of q in the mssn i e q 1 2 and 1 are first employed the effect of λ on the computed permeability are studied based on the three boundary schemes as shown in fig 13 it is found that 1 the results from mssn with q 1 2 and 1 are almost the same as those from the bb and mibb for the same λ respectively similar results are also observed in section 3 2 2 the computed permeability from all boundary schemes increase with λ specifically the relative error for results from the bb mssn with q 1 2 mssn 1 and the experimental data are less than 15 as λ 1 8 3 16 which then increases as λ 3 16 however the relative errors from the mibb and mssn with q 1 mssn 2 are much larger than those from the bb and mssn with q 1 2 e g the smallest error from the mibb and the mssn with q 1 is about 70 for λ 1 8 1 for the bb and mssn 1 the zero velocity location is imposed at the middle of the fluid and solid lattices while in the mibb and mssn 2 the zero velocity location is put on the fluid solid interface as pointed out in ref talon et al 2012 the position of the interface for micro ct images is not precisely known since it depends on the resolution and segmentation procedure applied to separate the voids from the solids therefore the bb and mssn 1 can be more accurate than the mibb and mssn 2 if the average boundary location is close to 0 5 rather than 1 while for situations in which the average boundary location is close to 1 the mibb is believed to be more accurate than the bb finally it is worth mentioning that the bb can only locate the solid interface half way between the two grid points located in the fluid and the solid which cannot always be accurate for all the micro ct images while we can further employ the mssn with q 0 4 to calculate the permeability of the berea micromodel suggesting the zero velocity location is 0 4 lattice away from the fluid node the value of λ adopted in simulations is λ 3q 2 4 3q 2 2 in which q 0 4 as we can see in fig 14 the relative errors between the computed permeability and the experimental data are less than 20 for the employed λ which is much better than the bb and mssn 1 specifically the numerical results fall in the experimental interval as λ 0 12 0 192 in summary the mssn with q 1 2 and 1 can provide identical results with the bb and mibb for the same λ respectively in addition the mssn can locate the position of the solid interface within any distance from 0 to 1 by adjusting q the permeability for the porous media which is generated from the micro ct images can be more accurate than the bb and mibb to this end we would like to point out the best value of q strongly depends on the resolution of the micro ct images which can be approximated by analyzing the experimentally measured permeability for instance assuming that we have a stack of images with the same resolution for a rock sample and we also have experimentally measured permeability for a portion of the images we can then tune q based on the available experimental data and employ the same q to calculate the permeability for the remaining images furthermore q 0 5 is suggested in simulations if no experimental measured permeability is available meaning that the solid wall is assumed to locate at the middle between a void and solid pixel 3 5 flow through a random sphere packing a more realistic application i e flow through a column packed with mono dispersed micro beads fig 15 yang et al 2013 2016 is simulated in this section as displayed in fig 15 the column is packed with 6864 mono dispersed micro beads the diameter of the column is d 8 8 m m with a length l 16 8 m m in addition all the micro beads have the same size i e d p 500 μ m leading to a porosity ϵ 0 4267 the flow is driven by a constant velocity at the inlet i e x 0 m m with re ρ p u 0 d p μ 0 2563 where ρp and μ are respectively the density and dynamic viscosity of water ρ p 997 561 k g m 3 μ 8 887 10 4 p a s the pressure at the outlet is kept constant and the no slip boundary condition is imposed at the fluid solid interface similarly the permeability viscosity relation is studied based on the grid resolution d 110 δ x the λ derived analytically for the straight channel is found to be quite accurate for the sphere packing according to ref khirevich et al 2015 hence λ 3 16 and 3 4 are taken for bb and mibb respectively in addition λ 3 16 is also found to be quite accurate for the linear interpolation extrapolation based methods khirevich et al 2015 since the numerical error in the mssn is the same as the linear interpolation extrapolation based methods khirevich et al 2015 ginzburg et al 2008 here we also take λ 3 16 for the mssn as displayed in fig 16 the bb mibb mssn can overcome the defect of viscosity dependent permeability while the ibb and ssn cannot furthermore the permeability predicted by the ssn with q 1 increases faster with the fluid viscosity than the ibb the same results have been presented in the above sections which will not be analyzed here for simplicity we proceed to investigate the accuracy of the single node boundary schemes since the results from the ibb and ssn are unphysical they are not presented here in our simulations four different grid resolutions i e d 110 δ x 132δx 165δx and 220δx are employed again we take ν 1 10 τ 0 8 as the representative case due to the fact that the calculated permeability is independent from ν in those boundary schemes permabilities calculated from the bb mibb and mssn are then compared to the results in ref yang et al 2016 considering that the boundary scheme adopted for the lbm in yang et al 2016 cannot provide a viscosity independent permeability either ginzburg et al 2008 we use the results from the finite volume method yang et al 2016 as the reference solution i e results simulated by the tethys using a structured mesh with a resolution d 440 δ x the reference permeability from the tethys i e k r e f 4 25 10 6 c m 2 is noticed to be the same as that predicted by the well known carman kozeny equation carman 1937 as shown in fig 17 it can be observed that 1 the calculated permeability for the bb and mssn converge to the reference solution with the increase of grid size it is noticed that the results from the bb and the mssn are similar as d 165δx which agree well with the reference solution the relative error is less than 3 while the mssn is more accurate than the bb for coarse grid simulations which is attributed to its higher accuracy specifically the relative error for d 110 δ x in the mssn are about 5 less than that from the bb 2 the relative error between the permeability predicted by the mibb and the reference solution decreases with the increase of grid resolution but it is still about 100 larger than the reference solution for d 220 δ x it is reasonable that the mibb with λ 3 4 locates the fluid solid interface one lattice away from the fluid node leading to the inaccurate description for almost all the boundary locations therefore large computational errors occur in this method while the bb with λ 3 16 puts the zero velocity location half a lattice away from the fluid node which can approximate the curved boundary much better than the mibb hence the results from the bb is more accurate than the mibb as mentioned the boundary location is accurately resolved in the mssn which leads to the best results among all the single node boundary schemes in addition it is observed that the relative error of the bb for d 220 δ x is larger than d 165 δ x in the bb since the computational domain is discretized into lattices a curved surface is approximated by zig zag staircases thus its morphology cannot be precisely resolved unlike the body fitted mesh in addition the no slip boundary is located half a lattice away from the solid wall suggesting that the actual location of the curved surface may vary if using different resolutions in general the computational error of the permeability will decrease with increased resolutions but the monotonous convergence cannot be guaranteed similar results are also presented in section 3 3 finally we would like to mention that our results and conclusions are consistent with those reported in pan et al 2006 to accelerate the computations an in house developed gpu scheme is implemented using the sparse matrix mode proposed in huang et al 2015 all simulations are run on a single workstation with two cpus intel xeon e5 2643 and one gpu titan xp the comparison on the performance of the bb mssn and mibb is illustrated in table 3 as shown 1 the bb is the most efficient among the three methods and 2 the mssn and mibb have comparable efficiency considering that both f i x f t i e the post collision distribution function in the same direction as the unknown distribution function at the boundary node and g i x f t i e the correction term need to be computed in the mssn as well as the mibb the computational costs are similar between the two but more expensive than the bb finally we would like to point out that the effect of λ on the accuracy for the permeability prediction for flows in sphere packing and tomographic geometries has been discussed in detail for both the bb and linear interpolation extrapolation based methods in refs khirevich et al 2015 talon et al 2012 since the numerical error in the mssn is the same as the linear interpolation extrapolation based methods khirevich et al 2015 ginzburg et al 2008 the choice of λ in the linear interpolation extrapolation based methods can be directly used for the mssn for more information on the choice of λ for porous media flows interested readers can refer to ref khirevich et al 2015 ginzburg et al 2008 talon et al 2012 4 conclusion single node boundary schemes are of great importance in simulating porous media flows using the lattice boltzmann method in this work three existing single node boundary schemes i e the bounce back bb scheme the improved bounce back ibb scheme and the single node second order ssn boundary scheme are first analyzed using the chapman enskog analysis theoretical results indicate that the ibb and ssn cannot obtain viscosity independent permeability which are then improved by using the magic versions of the ibb and ssn mibb and mssn to overcome this defect intercomparisons among all the single node schemes i e the bb ibb mibb ssn mssn for simulating porous media flows are conducted numerical simulations of flows through two and three dimensional porous media confirm that the predicted permeability is independent from the fluid viscosity in the bb mibb and mssn while the unphysical result i e the viscosity dependent permeability relation is observed in the ibb and ssn the theoretical and numerical results also demonstrate that 1 the bb and ibb are of second order accuracy when the boundary location are half a q 1 2 and one lattice q 1 away from the fluid node while the mssn can achieve second order accuray for any q 0 1 furthermore the mssn can obtain the same results as the bb and mibb with the same λ and fluid viscosity by setting q 1 2 and 1 respectively 2 according to the theoretical analysis e g eqs 14 and 22 in section 2 3 both the bb and mibb have only first order for 0 5 q 1 in addition the computational errors are proportional to q 0 5 and q 1 for the bb and mibb respectively hence the mibb is more accurate than the bb as q 0 75 for flows with curved boundaries the bb is generally more accurate than the mibb which can be attributed to the fact that the average distance between the fluid node and the solid boundary location is close to 0 5 rather than 1 in this type of flows while the mibb can be more accurate than the bb for situations where q is close to 1 q 0 75 if used in porous media samples such as those generated from high resolution micro ct images in which the solid boundaries can be accurately resolved i e q is close to 1 in such cases 3 the mssn is more flexible than the bb and mibb to approximate the boundary location which is found to be the most accurate for simulations of flows through porous media with curved boundaries generated from the micro ct images the present study is expected to advance our understanding of the existing single node boundary schemes and serve as a comprehensive review evaluation of their performances which can help to choose appropriate boundary schemes in the lbm for more applications credit authorship contribution statement xuhui meng conceptualization methodology software validation formal analysis investigation writing original draft writing review editing visualization liang wang methodology writing original draft weifeng zhao methodology xiaofan yang conceptualization methodology investigation writing original draft writing review editing supervision project administration funding acquisition declaration of competing interest the authors declare no conflict of interest to report acknowledgement xuhui meng and xiaofan yang are funded by the national natural science foundation of china nsfc grant no 41877183 and the special program for applied research on super computation of the nsfc guangdong joint fund the second phase liang wang is funded by the national natural science foundation of china nsfc grant no 51776068 the authors would like to express their thanks to prof zhenhua chai for fruitful discussions appendix a chapman enskog analysis of the trt lbe model in order to obtain the closure relations for the single node boundary schemes the solutions for g i and g i in eqs 12 19 and 25 are needed here we first employ the taylor series expansion to f i x c i δ t δ t around x t as follows a 1 f i x c i δ t δ t f i x t δ t d i f i x t δ t 2 2 d i 2 f i x t o δ t 3 where d i t c i we then adopt the following expansions to the space and time for multi scale analysis a 2 α ε α 1 t ε 2 t 2 where ε is a small parameter in addition fi x t can be expanded as a 3 f i f i 0 ε f i 1 ε 2 f i 2 o ε 3 where f i 0 e i e i substitutiong eqs a 1 a 3 to eq 5 we can obtain the following equations a 4 ε 1 δ t c i α α 1 e i e i p i 1 m i 1 δ t f i 1 a 5 ε 2 t 2 f i 0 δ t d i f i 1 δ t 2 2 f i 0 p i 2 m i 2 where p i g i and m i g i rewrite eq a 4 in the opposite direction i e i and sum these two equations we can obtain a 6 p i 1 δ t c i α α 1 e i m i 1 δ t c i α α 1 e i δ t f i 1 in addition f i 1 f i 1 f i 1 the following equation can be obtained a 7 f i 1 δ t p i 1 λ δ t m i 1 λ substituting eq a 7 into eq a 5 we can obtain the solutions of p i 2 and m i 2 as a 8 p i 2 δ t t 2 e i δ t 2 λ c i α α 1 2 e i a 9 m i 2 δ t t 2 e i δ t 2 λ c i α α 1 2 e i where λ 1 λ 1 2 and λ 1 λ 1 2 for the steady state considered in the present study we can obtain the expressions of g i and g i as δ t 1 a 10 g i ε p i 1 ε 2 p i 2 ω i β u α c i α c i β c s 2 a 11 g i ε m i 1 ε 2 m i 2 ω i c i α α p f α λ β 2 u α c i β 2 c s 2 λ 3 β 2 u α e in which a 12 α p f α λ 3 β 2 u α 
476,ground water flow and the transport of contaminants in groundwater systems are strongly controlled by geologic heterogeneities because the data available to model these subsurface heterogeneities are generally sparse the observed groundwater flow piezometric head at well locations or the concentration profile of solutes provide valuable additional information about these complex subsurface systems regardless there is likely to be considerable uncertainty associated with the predictions of the subsurface heterogeneities this has motivated the development of several ensemble based schemes for assimilating flow response data into predictions of subsurface heterogeneities most ensemble based data assimilation methods including ensemble kalman filter enkf or indicator based data assimilation inda used for assimilation of dynamic data into geologic models utilize statistics in the form of covariances calculated using the ensemble of models to perform model updates these covariance based updates do not preserve the complex spatial characteristics of geologic structures that are better represented using multiple point statistics additionally in enkf the mismatch between the observed and the simulated values are assumed to be linearly related to the parameter updates and the distribution of the state variable is assumed to be multi gaussian the spatial distribution of the primary variables like facies can be non gaussian because of the spatial continuity exhibited by channel facies sands and non channel facies clay that have distinctly different properties also the relationship between the mismatch and the parameter updates can be strongly non linear in indicator based data assimilation inda method the indicator transforms of parameter and mismatch variables are used which affords the treatment of these variables as bi variate non gaussian furthermore in the indicator transformed space restrictive linear assumption can also be lifted as the indicator transform is invariant under non linear transformations however the updates are still governed by the bivariate interactions between the state variables a multiple point extension of inda is proposed in this paper which utilizes existing multiple point simulation algorithms like single normal equation simulation snesim in combination with inda to preserve the spatial characteristics of the geologic model while at the same time honor the observed flow or transport response the proposed method is applied to a synthetic groundwater system with complex channel distributions the channel features of the final updated ensemble of models is shown to converge towards the reference model and the ensemble flow responses are also shown to match the reference flow response keywords data assimilation multipoint geostatistics inda enkf non gaussian parameters flow data integration probability map indicator transform methods 1 introduction modeling of ground water flow entails an understanding of the subsurface distribution of different facies having different conductivities that controls the flow of fluids therefore a proper understanding of ground water flow in an aquifer requires detailed characterization of the subsurface conductivites within the area under consideration the inability to directly measure underground conductivity compounded by the fact that it is spatially distributed along complex channel like structures make its characterization very difficult with the help of advanced simulation algorithms like snesim strebelle 2002 direct sampling mariethoz et al 2010 and simpat arpat and caers 2007 filtersim zhang et al 2006 and using rock facies data at limited number of locations within the modeling domain it is often possible to come up with an ensemble of models for channel distributions representing the uncertainty associated with the prediction of facies distribution in a given aquifer although the initial uncertainty in conductivity distribution is often a good starting point for the characterization of an aquifer this uncertainty is often high because the available hard data is often sparse and thus some secondary data is assimilated to reduce this uncertainty data assimilation algorithms are suitable for this task as these algorithms use some observed variables like fluid flow rates and heads at well locations to update the subsurface facies or the conductivity distribution as the main variables of interest for subsurface characterization are parameters like facies and conductivities these are called primary variables while flow responses like flow rates and head are termed secondary variables the secondary variables are generally related indirectly to the primary variables and this relationship is often non linear data assimilation algorithms like ensemble kalman filter enkf evensen 1994 and burgers et al 1998 utilize the ensemble of model realizations along with the observed secondary variables to update the uncertainty in the primary variable distributions it is known that kalman filter is a linear least squares estimator llse that is the estimator is by definition a linear function of the observations this can also be said about enkf as it is applying the same update using sample statistics such a linear update is optimal only if the multivariate distribution describing the variables involved in the update is multi gaussian these simplifications render the applicability of enkf to be limited especially in cases where the ground water flow and transport of chemical species in the groundwater are strongly controlled by complex spatial patterns of geologic facies that reflect non gaussianity efforts to handle the non gaussian distribution of variables involved in the update equation led to the development of the normal score ensemble kalman filter ns enkf as discussed in zhou et al 2011 and schoniger et al 2012 although the method does propose a remedy by first applying normal score transform to the variable distribution and then performing an enkf update the transform procedure is univariate and does not ensure multi gaussianity of the multivariate distribution describing the relationship between state variables another issue that requires attention in data assimilation is that of the non linear linear nature of the transfer function while enkf requires a full ensemble of flow simulations to propagate the covariance of state variables in time other methods like extended kalman filter leng and yeh 2003 and yeh and huang 2005 use a linearized transfer function relating model parameters and the flow response variables this linearization is achieved by applying a taylor expansion and then using this simplified transfer function to propagate the state covariances in time for complex non linear transfer functions this method can be both time consuming and complicated to apply in another method called the indicator based data assimilation inda kumar and srinivasan 2019 indicator transforms of the state variables are used to update the primary variable uncertainty directly see appendix c it has been shown that inda addresses the issue of univariate non gaussian distribution of variables and the bivariate non gaussian relationship between the secondary data mismatch and primary variables since this bivariate non gaussian relationship is a result of non linear mapping of the state variables to the data mismatch by the non linear transfer function inda is designed to handle this non linearity although all these techniques show advancement over other ensemble based data assimilation methods they do not address the update of spatial distributions that are characterized by complex non linear features that are best described using multiple point statistics the current ensemble based methods utilize covariances two point statistics in the update equation that is only adequate for representing the statistical characteristics of multi gaussian distributions since the initial ensemble of models are often generated using multiple point statistics based simulation algorithms the application of two point update methods often degrade the geological realism of the final updated models the need for multiple point statistics mps based data assimilation techniques in ground water modeling becomes even more evident when a deeper look is taken into the mechanisms of flow and transport in complex porous media for instance the well fluid rates and or the well head values are not only dependent on the conductivity values at individual locations but also depend on the conductivity of the flow path for fluids i e the conductivity at several locations taken jointly this leads to the following observations the mismatch between the observed and simulated values should be associated to a pattern of conductivity rather than the value at an individual location the update procedure should yield the updated higher order statistics describing the pattern of variability of conductivity these observations motivate the discussion of multipoint based conductivity updating beyond the two point covariance with a goal to preserve the initial multiple point features throughout the update process an interesting data assimilation method introduced by zhou et al 2012 utilizes the ensemble of models as a repertoir of multiple point statistics that is used to perform mps update of reservoir variables a detailed evolution of data integration methods to improve parameter estimation and model forecasting in hydrogeology is provided in zhou et al 2014 several methods like gradual deformation method gdm hu 2000 and caers 2003 aiming at mps updates have also been proposed in the past in gdm an optimization aimed at minimizing the mismatch between observed flow responses secondary data and the ones simulated from a model derived as a weighted linear combination of the realizations in the ensemble of models the weights resulting in the minimum mismatch are then used to define the most representative model honoring the observed data since the entire model is treated as an update variable and not broken down into pixel based updates the multiple point distributions of the geologic properties are conserved after the updates are performed however the method works consistently only if the variables being updated are multivariate gaussian probability perturbation methods ppm hoffman and caers 2005 tarun and sanjay 2003 yadav et al 2005 are another set of algorithms that are aimed at performing mps updates in ppm the probability of the model parameters conditional to the given flow response data are updated by optimization of a single parameter called the deformation parameter since the matching is done directly in the probability space there is no multi gaussian assumption underlying this method an interesting approach to mps updates is the probability conditioning method pcm jafarpour and khodabakhshi 2011 which uses probability distribution maps of occurrence of certain parameter categories conditioned to some flow response data and then combines this information with a mps simulation algorithm like snesim strebelle 2002 in form of secondary data in addition to the methods discussed in this section a repository of methods aimed at combining conditional probabilities from different sources to get a final conditional probability are discussed in great detail by allard et al 2012 one advantage of the pcm method is that it does not explicitly require any assumptions related to the distribution of parameters or the relationship between the observed variables and the variables being updated however these assumptions might be implicitly imposed depending upon the technique that is used to deduce the secondary probabilities used in the method for instance in jafarpour and khodabakhshi 2011 the enkf method is used for the evaluation of secondary probabilities meaning the assumptions related to enkf also apply to the these probabilities additionally this implementation of pcm assumes no redundancy in information from the training image ti used in the mps simulation and the secondary probability calculated from enkf this is because in the τ model journel 2002 used for combining the probabilities from training image and that from production data the τ value is assumed to be equal to 1 0 in a another work by ma and jafarpour 2019 a modification of the pcm is proposed where a sensitivity based evaluation of spatially varrying τ values is proposed for proper probability conditioning of probability based on the distance between the points in the reservoir and the secondry data source locations further this method also investigates the reasons which lead to snesim method resulting in almost determininstic output once few initial locations are simulated in the reservoir some of the limitations assumed within the aforementioned methods are overcome in this paper that introduces a new mps update algorithm that combines the snesim algorithm stanford center for earth resources forecasting 2015 with the inda algorithm as inda algorithm kumar and srinivasan 2019 is free of any gaussian assumptions for the variables being updated and permits non linear mapping of the data mismatch to the update variable the secondary probabilities are more reliable than the ones deduced from enkf further the proposed method calculates the redundancy in the information obtained from secondary information and the training image used within snesim it is shown that the redundancy or τ parameter changes with each update step and follows a trend that is speculated in jafarpour and khodabakhshi 2011 this method also considers uncertainty in the ti and updates the ti statistics at each update step in the next few sections the proposed method called the indicator based data assimilation with multiple point statistics inda mps is introduced the method for evaluation of secondary probabilities is explained in detail then the results obtained from the application of the proposed method to a synthetic aquifer model are analyzed and finally these results are also compared with the results obtained from the application of enkf and inda to the same model further some relevant discussions are also added in the appendix sections 2 proposed multiple point extension to inda in this section a data assimilation method called indicator based data assimilation to update multiple point statistics inda mps is proposed by extending the existing inda kumar and srinivasan 2019 see appendix c algorithm this extension uses the update procedure to yield updates of multiple point statistics corresponding to pattern templates describing subsurface conductivity variations represented within a training image ti these updated multiple point statistics are used within an mps simulation algorithm like snesim strebelle 2002 as the exhaustive update of higher order statistics for an entire aquifer is computationally impractical for most real cases the statistics updates are restricted only to a few selective templates within the ti in the two point scheme of inda the cumulative probabilities are updated one location at a time by using some secondary data but in the multiple point counterpart the updates of higher order statistics are made only for the aforementioned selective templates then the updated template statistics can serve as an input to a multiple point simulation scheme like single normal equation simulation snesim strebelle 2002 that relies on multi point statistics drawn from training images ti maharaja 2008 this update step addresses the uncertainty in the ti by conditioning it with some observed secondary data in the subsequent sections a method for isolating the most relevant or selective templates in the training image describing the channel connectivity in the ensemble of models is presented updates to the statistics for those selective templates are performed using inda with the flow response data at well locations as the secondary data it should be noted that although the ensemble of conductivity models generated using the updated ti statistics will have different spatial connectivity than the one generated using the initial ti statistics it will not capture the local update of variables to achieve the local update of variables the information from the conventional application two point inda updates to each location in the model is provided as secondary data to snesim in addition to the update ti statistics it is shown that models thus generated are conditioned to the production data and honor the spatial continuity of geologic properties captured by the ti at the same time this results in updated models that represent complex channel features governed by multiple point statistics in summary the inda mps update process comprises of two steps update of the global multiple point statistics of the primary variables by updating the ti statistics for a selective set of templates using the observed secondary data local update of the primary variables using the observed secondary data an important remark the update of the multiple point statistics mps for the selected templates is performed by the two point inda update method using the observed flow responses as secondary data this is done by treating the mps for a given template in the ti as an update parameter and using the available repeats of this mps from the ensemble of models to create an ensemble of these statistics indicator transforms of the ensemble of mps data and ensemble of mismatch between the observed and simulated flow response data are used within the two point inda this process updates the global mps from the ti in order to render models that accurately reflect local updates from flow response data in the vicinity of wells the global mp statistics will have to be combined with local information such as soft probabilities at each location in the reservoir inferred by assimilating flow response data the process is discussed in detail in the following sections 2 1 global mps update by ti statistics update as discussed in section 2 the first step in the inda mps update method is the update of ti statistics for selective templates used in the snesim algorithm a brief recap of the snesim algorithm is presented in appendix b so that the process of updating the multiple point statistics using dynamic data can be better understood within the data assimilation framework proposed in this paper the ti statistics for some selective templates are updated in the search tree using the inda procedure these selective templates are chosen based on their number of repetitions within the training image the templates appearing more frequently in a training image are considered to be a key spatial characteristic of the ensemble of models generated using the training image for the example case of data assimilation presented in some later sections the templates appearing more than 200 times in a training image are considered as selective templates the multiple point statistics corresponding to these templates are updated using the inda procedure this update is performed by using the inda update equation discussed in eq 12 of kumar and srinivasan 2019 which treats the mutiple point statistics of the selective templates from the ti as the primary variable ψf that is updated by the indicator transformed secondary data yp where p 1 2 n from n observation locations 1 e i ψ f ψ k y 1 1 y 2 1 y 3 1 y n 1 e i ψ f ψ k i 1 i n λ i 1 e y i the thresholds ψk required for the selective templates ψf in eq 1 are considered to be percentile values of 1 through 100 in the intervals of 5 calculated by pooling the multiple point statistics of all the selective templates for all the models in the ensemble in this paper the ti statistics treated as the primary variable ψf are the number of repetitions of a categorical variable clay or sand at the center of a selective template while the absolute mismatch between the simulated and observed flow responses tracer fluid rate at p 1 2 n well locations at a given time step are considered the secondary variables yp it should be noted that any update scheme other than inda can also be used to update the ti statistics if desired the benefit of inda over other schemes is because of its ability to handle non gaussian distribution of state variables naturally the ensemble of models is re simulated using the updated multiple point statistics and the hard conditionning data clay and sand information at well locations in the reservoir using this updated ensemble the updated probability p a b at any spatial location can be estimated by pooling the categorical values of the variable at that location from the entire ensemble as mentioned in section 2 the inda based procedure yields updated global multiple point statistics thus the updated p a b do not ensure that the simulations reflect the local flow characteristics in the vicinity of the wells providing flow response data for the update to render the updates locally accurate the integration of secondary flow response data c is required 2 2 secondary data integration into snesim for local updates as mentioned in section 2 the integration of secondary data c is required to achieve local uncertainty updates of model parameters the inference of local information at a location u from the flow response data c can be designated as p a u c while the informtion from the updated training image 2 1 is designated as p a u b the updated local probabilities combining both geologic information b as well as information c from production data is annotated as p a u b c this section describes a way of combining information from aforementioned sources by using the tau model bordley 1982 journel 2002 krishnan 2008 this method requires at each update location u the probability values p a u b p a u c and a pair of global tau values τ 1 τ 2 describing the redundacy in the information sources b and c impacting the outcome of a u as shown later in this section that τ 1 and τ 2 are arithmetic averages of τ 1 u and τ 2 u respectively τ 1 u and τ 2 u at any location u can be written as a functions of p a u b and p a u c respectively as described in section 2 1 the p a u b are known at the begining of the local update step thus τ 1 u values can be easily evaluated for any location u for the evaluation of p a u c a non linear function in p a u b p a u c p a u b c and τ 1 u is solved iteratively since the goal of the local step is to solve for p a u b c this is not known at this stage thus an approximate value of p a u b c is used to solve for p a u c the probability of the outcome of a u given the geologic information b and conditioned by secondary data c using the two point inda with localization is used as the approximate value of p a u b c the localization embedded in inda loc ensures that resultant p a u c is impacted more by secondary data sources that have smaller spatial lags from u both primary conductivity and secondary flow response data are indicator transformed for the inda step as described in kumar and srinivasan 2019 once the p a u c values are solved they are used to solve for τ 2 u values at each location u next the arithmetic average τ 1 and τ 2 of τ 1 u and τ 2 u respectively are calculated to get an average representation of the redundancy in information for the two data sources b and c at the given timestep for the entire reservoir finally the p a u b p a u c τ 1 and τ 2 values are used as input to the snesim algorithm to generate an updated ensemble of models that represent the updated multiple point statistics the equations involved for the achieving the aforementioned calculations are discussed in detail in the follwing paragraphs in snesim stanford center for earth resources forecasting 2015 the probability p a u b c at any spatial location u x y z is calculated by the integration of p a u b and p a u c using the tau τ bordley 1982 journel 2002 krishnan 2008 model according to this model 2 x a b a τ 1 c a τ 2 w h e r e x p a b c 1 p a b c b p a b 1 p a b c p a c 1 p a c a p a 1 p a τ 1 a n d τ 2 a r e p a r a m e t e r s u s e d i n t h i s e q u a t i o n a brief description of terms used in the tau model are provided in appendix d it is clear from eq 2 that secondary data integration at any location u x y z using this approach requires p a p a u b p a u c τ 1 u and τ 2 u p a is usually known in the form of prior proportion of category k and the values of p a u b are calculated using the multiple point statistics section 2 1 from the training image within the snesim algorithm appendix b so the only remaining unknown quantities are p a u c τ 1 u and τ 2 u one approach to compute the τ values accounting for redundancy of different data sources is implemented within the snesim algorithm stanford center for earth resources forecasting 2015 and is given by the following equation 3 τ 1 u p a u b p a 1 p a if p a u b p a p a p a u b p a otherwise τ 2 u p a u c p a 1 p a if p a u c p a p a p a u c p a otherwise in the following for simplicity p a b p a c τ 1 and τ 2 are assumed to designate p a u b p a u c τ 1 u and τ 2 u respectively unless specified otherwise throughout the rest of the paper now applying the natural logarithm of the first line in eq 2 and substitute for τ 2 as a function of p a c from eq 3 the resultant expression is 4 ln x a τ 1 ln b a p a c p a 1 p a ln c a if p a c p a τ 1 ln b a p a p a c p a ln c a otherwise further if we replace c in eq 4 as a function of p a c from the third line in eq 2 then the following can be written as 5 ln x a τ 1 ln b a p a c p a 1 p a ln p a c 1 p a c a if p a c p a τ 1 ln b a p a p a c p a ln p a c 1 p a c a otherwise taking all the terms to the l h s a non linear objective function g in p a c can be written as shown in eq 6 6 g p a c ln x a τ 1 ln b a p a c p a 1 p a ln p a c 1 p a c a if p a c p a ln x a τ 1 ln b a p a p a c p a ln p a c 1 p a c a otherwise in a root finding sense the objective would be to update guesses of τ 1 and p a c in order to converge to a value close to 0 in this case convergence corresponds to an objective function value of 10 3 for g eq 6 with an initial guess value for p a c o the converged value of p a c can be evaluated using a scheme such as the newton raphson iterative method assuming τ 1 o τ 2 o 1 the p a c o values are evaluated using eq 2 the assumption of τ 1 o τ 2 o 1 ensures that the initial guess for p a c o disregards data redundancy between information sources b and c after this stage all further calculations are performed with τ 1 u values calculated using eq 3 the additional variable required for the calculation of p a c o is x which is a function of p a b c it should be noted that the goal of inda mps itself is to integrate information from training image and hard conditioning data b with secondary data c to calculate p a b c using mps based updates so these probability values are not yet available the two point inda kumar and srinivasan 2019 updated ensemble of models can be used to get an approximate measure of p a b c this is because the two point inda updates evaluate probabilities p a b c by combining secondary data c into an ensemble of models generated using training image and hard conditioning data b thus the p a b c calculated using inda are a good estimate of the actual p a b c values and are used for the calculation p a c o in theory the final converged values of p a c for each category at every location should add up to one to ensure that they represent legitimate set of probability values but practically they often add up to values very close but not exactly one to ensure this legitimacy the probability values for each category are normalized at every location and are then used to evaluate final τ 2 values for secondary data integration in snesim the p a u c values are required at all spatial locations u x y z and can be readily provided in form of the converged and normalized p a u c values the snesim algorithm also requires scalar values of τ 1 and τ 2 values that are representative of all locations within the model the application of eq 3 results in a different value of τ 1 and τ 2 at each location u x y z in the model average values of τ 1 u and τ 2 u respectively are calculated over the entire spatial domain u x y z and specified as the unique τ values for the entire simulation the ensemble of models thus generated have updated global multiple point statistics for the selected templates and the secondary data integration results in reduced local uncertainty in summary the two point inda loc provides a way to calculate approximate values of local conditional probabilities for the spatial parameter k at a given location p a k c conditioned to the secondary data c the redundancy of that conditional probability to the conditional probability due to other information sources b is described by the parameters τ 1 and τ 2 in order to determine these redundancy parameters an iterative scheme is implemented a flow chart of this data assimilation scheme implemented in inda mps is shown in fig 1 3 demonstration of indicator based data assimilation with multiple point statistics inda mps updates using a synthetic channel aquifer the indicator based data assimilation with multiple point statistics inda mps method has been explained in detail in the previous sections in this section an example of this data assimilation method is demonstrated with an aritficial but realistic working example the inda mps requires p a c values for the generation of mps updates section 2 2 and the p a c calculations in turn require inda with localization updates localization ensures that observation data points located far from the update parameter location do not contribute significantly towards the magnitude of update this may happen because of spurious covariance relationships occurring at large length scales due to the use of a limited ensemble size the localization method employed in this research is detailed in greybush et al 2011 where the contributions of the observed data toward the update of a parameter are weighed according to their spatial lag from the location to be updated in greybush et al 2011 the elements of the diagonal observation error matrix in the enkf analysis equation are weighed with the weights increasing exponentially as the spatial lag between the update parameter and the observation data source increase for the application of localization in the context of inda the diagonal elements of the secondary data indicator auto covariance matrix lhs of eq 13 in kumar and srinivasan 2019 were weighed using the same exponential weight function as in greybush et al 2011 3 1 description of reference model the model used in this example is a two dimensional model with a total of 13 000 100 130 1 grid blocks the grid dimensions in the x and y direction are same for all grid blocks with a value of 25m while the thickness of the grid blocks vary in space as shown in fig 2 a additional model description is provided in table 1 the reference model has sand channels high conductivity allowing easy flow of fluid and clay areas low conductivity fluid flow is not easy with conductivity values of sand and clay facies in m d 1 sampled from the distributions given by n 1 31 0 026 2 and n 0 026 0 013 2 respectively fig 2 b shows the reference conductivity distribution of the model used in this exercise the tracer rate production injection data at the well locations shown in fig 2 b are used as observed data for data assimilation all maps representing the distribution of sand and clay facies and the corresponding statistical attributes like mean and variance are reported in the units of natural logarithm of conductivity lnk 3 2 generation of initial ensemble of models the ensemble of initial models is generated using the single normal equation simulation snesim strebelle 2002 available in gslib format stanford center for earth resources forecasting 2015 the snesim algorithm produces realizations that are modeled by multiple point statistics which enables representation of channel like features that are signature characteristics of a fluvial depositional environment the algorithm relies on a training image maharaja 2008 to calculate the higher order statistics the training image generated for the purposes of this exercise is shown in fig 3 a in order to replicate a realistic scenario the conditioning data used for the generation of the initial ensemble of 100 conductivity models were restricted only to the well locations for sand facies and additional six locations with clay facies assumed to be dry well locations fig 3 b the conductivity distribution of some models ensemble members 10 20 50 and the reference conductivity distribution are shown in fig 4 the cumulative probability distribution cdf of conductivity values of the entire reference model shows a non gaussian characteristic fig 5 the same characteristic is captured well by the initial ensemble of conductivity models fig 5 for flow simulation purposes the top and bottom five grid rows are set to zero transmissibility to avoid edge effects which might have impacted the snesim algorithm 3 3 assimilation of secondary data to update ensemble of conductivity models the initial ensemble of conductivity models represents the initial uncertainty associated with the distribution of conductivity in the aquifer the tracer fluid rate production or injection data from the reference model is used as secondary data for updating the ensemble of models the cmg black oil with seawater simulation option with default parameters for the sea water properties is used in for generating flow response data in this paper the sea water saline water serves as a tracer in this work the initial reservoir pressure is 3 600psi with a water saturation of 1 0 a zero tracer concentration and all four boundaries as no flow all injection wells inject tracers from initial time to the end of simulation time into the reservoir the primary data cut off is defined as shown in fig 5 it is evident that more cut off values are specified in the parts of the cdf where there is a steeper change in probability than in parts with gradual change for calculation of the secondary thresholds the procedure discussed in kumar and srinivasan 2019 is used an uncorrelated gaussian error with 0m 3 day mean and a standard deviation of 48m 3 day is used for all observations the tracer fluid rate data is assimilated every 1 month for 14 months resulting in a total of 14 model update steps the conductivity values are updated using the inda mps while in the absence of training images the state variables like tracer fluid fraction and head values in the grid block locations are updated using inda with localization as discussed in section 2 2 each of these assimilation steps requires calculations for spatial distribution of p a c values and average τ values the plot of τ 1 and τ 2 as they vary with time is shown in fig 6 where τ 1 describes the redundancy of the conditioning data and the multiple point statistics collectively b with the other information available to model the parameter variable a the information from conditioning data is constant throughout the assimilation process but the pattern statistics information retrieved from the training image is updated at each assimilation step by updating the global training image statistics fig 1 however because the pattern statistics corresponding to only a selective number of templates are updated the mp statistics only change by a small amount this is indicated by the τ 1 value remaining almost constant with time the τ 2 describes the redundancy of the tracer fluid rate response data c with other data sources as they pertain to the parameter variable a as the information contained in c is changing throughout the history matching process the τ 2 values also change the values increase with time at a decreasing rate this means that although the information in c tracer fluid rate responses related to the facies distribution in the model increases with time the additional information contained in c with each additional assimilation becomes smaller this diminishing value of information in the secondary information source c is also reflected clearly by the update exhibited by a single model ensemble member 50 with each update step as shown in fig 7 starting from the initial model fig 7 a the first few update steps figs 7 b to 7 e result in large changes in conductivity distribution in the model while the updates to the conductivity distribution after the 7 th fig 7 e and the 14 th update fig 7 f is minimal as expected from the profile of τ values shown in fig 6 for the assimilation of data for the first month the maps of p a sand b p a sand c and the combined probability of p a sand b c is shown in fig 8 which clearly shows how the combined probability information weighs the information from the two sources b and c with their respective τ values for that time step the updated models are then run to the next time step where observed data is available for assimilation 3 4 analysis of results the ensemble of models once updated are then flow simulated for a period of 47 months from the initial time as the models are calibrated only for the first 14 months section 3 3 the ability of the models to match future rates for the next 33 months is tested comparisons of tracer fluid rate forecasts with respect to reference rate values are made both for the initial ensemble of models and the final inda mps updated models fig 9 these comparisons bring out the effectiveness of the proposed data assimilation technique it is evident that post assimilation the updated models predict the tracer fluid rate responses that are very close to the reference rate the impact of the update process on the spatial distribution of conductivity is clearly seen in fig 10 where the final updated models figs 10 a 10 b and 10 c show channel continuity and location that are very similar to the reference model fig 10 d the channels in the initial models figs 4 a 4 b 4 c on the other hand exhibit characteristics quite different from the reference model fig 4 d in addition to matching tracer fluid rates the inda mps update scheme preserves the initial non gaussian distribution of the conductivity fig 11 this is a direct implication of the indicator coding of the data and the subsequent probability update that eliminate the multi gaussian assumption implicit within traditional enkf this approach also results in the range of conductivity values falling within desired or realistic limits as discussed in section 2 1 the inda mps method updates the global training image statistics and the local conductivity values the update of training image statistics is a unique feature of the inda mps method which ensures that the initial uncertainty in the training image is addressed during the update process selective templates are chosen from all the possible templates in the training image by using the criteria mentioned in section 2 1 some of these templates are shown in fig 12 the frequency of occurrence of sand or clay facies at the center of these templates is then updated using inda fig 13 these new frequencies are then used within the snesim algorithm along with additional local conditioning information to generate a new ensemble of models section 2 2 fig 14 shows the comparison of the initial conductivity realizations with the realizations generated using the updated training image statistics after 14 update steps this gives an idea about the contribution of global multiple point statistics update alone towards the description of final models as can be seen from the figure the updating of the multiple point statistics results in better depiction of the curvilinear characteristics of the channel especially in the regions highlighted in fig 14 however the updated regions do not neccessarily reflect the position of the channels around wells where the production information is available this motivates the assimilation of the conditional probability p a c derived using the dynamic data discussed in section 2 2 4 comparison of results from inda mps inda with localization inda loc and enkf with localization enkf loc this section compares the models and the responses from models generated using different data assimilation methods like inda mps inda using two point covariance with localization inda loc and enkf with localization enkf loc as is the case in section 3 the two point update encoded within inda mps in order to infer the conditional probability p a c is inda with localization the updates are implemented using a filtering approach meaning after updating the parameters and the state variables flow simulations are conducted using the ensemble of models until the next update time step the other methods which are being compared to inda mps are inda with localization inda loc and enkf with localization enkf loc the localization method is the same as discussed in section 3 the same 100 ensemble of models mentioned in section 3 2 is used for all the three methods comparison of flow rates the comparison of flow rates from inda mps and inda loc updated models suggest that the performance of both methods are comparable with respect to the ability to forecast tracer fluid rates of wells the same holds true for comparison of flow rates from inda mps and enkf loc updated models figures illustrating these comparisons are showin in appendix e comparison of ensemble mean noticeable and important differences become evident when the ensemble mean of the updated conductivity models from the different methods are compared fig 15 shows the comparison of ensemble average of models obtained by inda mps compared to those obtained by inda loc the average map fig 15 b of the initial ensemble do not reproduce the spatial variability exhibited by reference conductivity field fig 15 a accurately the average of the final inda loc updated models fig 15 c exhibit spatial variability more consistent with the reference model but the continuity of high conductivity features is disrupted because of the two point covariance used for updating the conductivity values the inda mps updated models fig 15 d exhibit the continuity of conductivity exhibited by the reference most accurately similar comparisons of ensemble mean of inda mps updates with enkf loc are also provided in appendix e for further reference comparison of ensemble variance additional important differences are noticed when the ensemble variance of the updated conductivity models from the different methods are compared fig 16 the comparison of the initial variance map of lnk fig 16 a with the inda mps updated variance map fig 16 d reveals that the uncertainty associated with conductivity predictions is reduced after data assimilation when compared to the reference model fig 15 a the low uncertainty areas show similar spatial extent and connectivity to the channel distribution in the reference model this can be attributed directly to the mps based updates coupled with the local conditioning brought about by the conditional probability p a c it should also be noted that despite the reduction of uncertainty in the middle of the channel structures higher uncertainty still persists at the channel edges as is evident in fig 16 d another important observation is the reduction in uncertainty in areas away from the well locations primarily in the bottom right corner of the map the corresponding updated values of conductivity in this region primarily comprises of low conductivity values fig 15 d which is in accordance with the reference values fig 15 a in that region the reason behind this outcome of clay distribution in the bottom right section of the map lies in the inclusion of local probability values in form of p a c as the region under consideration is far from any data observation source well the inda loc results in p a c values that are closer to p a values as a result of localization embedded in the inda loc algorithm this is because in areas far away from any well locations in the initial ensemble of models p a b is approximately equal to p a or global proportions of event a consequently the two point inda loc result in p a b c approximately equal to p a b in other words minimal update from the tau model definitions eq 2 we can say x and b in these areas are approximately equal to a as the aforementioned p a b c from inda loc and p a b are used together in the process of updating p a c and tau values as discussed in section 2 2 the p a c in these regions equals p a approximately because plugging x b a in the tau model all approximately equal results in c approximately equal to a next the p a b p a c values at each location and the average of tau values are used to evaluate a p a b c value using secondary data conditioning the tau model within the snesim algorithm since both p a b and p a c values are close to p a in the region in question it results in p a b c being close to p a the proportions for clay p a clay and sand p a sand used in this example are 0 64 and 0 36 respectively which means that p a c values at any of these locations under consideration are more likely to be higher for the clay category than the sand category thus the snesim algorithm with secondary data conditioning assigns clay category with high confidence to all these locations under consideration this can be seen in the final updated ensemble average and variance maps of lnk figs 15 d 16 d respectively in case of inda loc the regions located away from any secondary data sources undergo small or no updates as a result of this reduction in uncertainty is noticed in the areas closer to the well locations and along the channel features fig 16 c that are consistent with the reference values of conductivity fig 15 a although enkf loc employs the same strategy of small or no updates at distances away from secondary data sources the mismatch between the simulated and observed data are linearly mapped to new update values because of the gaussian and linear assumptions involved in enkf this linear mapping leads to low updated variance very close to the well locations and a smoothly varying reduced variance in regions away from the well that are impacted by the update process fig 16 b the various comparisons made in this section bring out the benefits of inda mps over two point methods like inda loc and enkf loc the updates based on multiple point statistics yield well defined channel structures in the updated models better than the two point update methods despite starting from initial ensemble of models that do not capture the reference distributions quite well the final ensemble of models closely represents the reference models these models have well defined channel structures with crisp boundaries between high and low conductivity regions while inda loc updated models provide a good average description of channels the boundaries of these structures are not well defined as in case of inda mps the lack of clear definition of boundaries is because the updates are performed using two point statistics indicator covariances in the case of enkf loc the channel descriptions are less accurate than those obtained using inda loc and inda mps although the channel bodies of the inda mps updated models have reduced uncertainty the edges of the well defined channel features exhibit persistent high uncertainty this is because sparse conditioning data results in high initial uncertainty in the spatial distribution of channels when flow response data from these well locations are assimilated the uncertainty along the structure of the channel is reduced but remains high along the channel edges because flow response data is only an indirect source of information and carries limited information about the location of channel edges 5 discussion and conclusions this paper presents the inda mps method that integrates dynamic data in order to model a conductivity field that is strongly non gaussian with a strong bi modal characteristic the initial ensemble of models is generated using only 26 hard data point out of 13 000 locations 0 2 of the locations in the area under consideration this sparsity of conditional data leads to an initial ensemble of models the average of which do not capture the complexities of the reference model but after the inda mps updates the average of the ensemble of models capture the complex connectivities of the channel distribution to an acceptable degree this demonstration of inda mps to reproduce reference channel features shows the potential of this algorithm to be applied to realistic cases the proposed inda mps method falls under the category of data assimilation algorithms called probability conditioning methods pcm this method feeds the probability map for the occurrence of a certain category of parameter variable from the ensemble of inda kumar and srinivasan 2019 updated models to the snesim strebelle 2002 algorithm as secondary conditioning data for mps simulation of an ensemble of updated models an optimization scheme is used to extract the usable probability values and a measure of redundancy τ values of information from various data sources are calculated during the model updates inda mps also performs the update of training image statistics at each update step in order to calibrate the ti statistics to the observed dynamic data in the demonstration of application of inda mps in this paper the conductivity field undergoes an mps update while the dynamic state variables like head are updated using the two point indicator data assimilation technique with localization although this approach performs very well in updating the ensemble of models this also provides some exciting avenues of research for developing methods to extract multiple point statistics directly from dynamic data and update them using an mps scheme the models obtained after multiple update steps retain the multiple point features of the training image while closely representing the reference model channel distributions additionally the calculation and correct usage of τ values make it possible to get the correct magnitude of update depending upon the information content in the observed data this means that if the observed data contains less additional information the models undergo smaller updates the correct use of information in the data during the mps updates prevent ensemble collapse which is a very common problem in majority of data assimilation schemes and also results in the flow rate responses from the final ensemble of models honoring the reference flow rate values in the example demonstrated in this paper a reference model with a bi modal conductivity distribution representing two facies was used but in general models with any number of facies can easily be handled by the inda mps algorithm credit authorship contribution statement devesh kumar conceptualization methodology software sanjay srinivasan conceptualization methodology declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the funding received as part of the john and willie leone endowment chair is gratefully acknowledged appendix a example data sets and binary versions of codes used for this research work are freely available for usage and testing as github releases ensemble kalman filter with localization enkf loc indicator based data assimilation with localization inda loc indicator based data assimilation with muliple point statistics inda mps appendix b the snesim algorithm for model updates as mentioned in strebelle 2002 snesim is an algorithm to generate geostatistical models honoring multiple point statistics of geologic properties like conductivity in this algorithm the variable being simulated is annotated as a in the context of a categorical variable simulation a can take any k number of categories a 1 2 3 k k with prior probabilities p a k or p a for simplicity these prior probabilities are often simple proportions from expert opinion or geological sources the information related to the spatial distribution of the variable is provided in form of a training image and some hard conditioning data that are recorded values typically at well locations combination of these information are collectively annotated as b training images capture the plausible ergodic variations of categorical properties across the spatial domain under consideration the statistics pertaining to the training image are calculated by evaluating the number of occurences of all possible spatial template t of a certain size in the ti the template consists of a central node and certain number size of template of neighboring nodes surrounding that central node for computational efficiency the training image is scanned once at the beginning of the algorithm for all possible combinations of outcomes in the neighboring nodes of a template pattern t and the number of repeats of all categories a k where k 1 2 k at the central node of these templates are stored in a search tree these repeats are used as multiple point statistics during the simulaton stage all the locations to be simulated in the model are visited in a random order considering both previously simulated and observed values of geologic properties as conditioning data a location to be simulated serves as a central node while the previously simulated values and original conditioning data within a template t form a pattern around the central node the probability of all possible categories at the central node p a k b where k 1 2 3 k are calculated from the mps extracted from the search tree and then the category with the highest probability is assigned to the central node the process is repeated until all locations are simulated appendix c indicator based data assimilation inda is a data assimilation method proposed by kumar and srinivasan 2019 to handle non gaussian distribution of state variables and non linear nature of transfer functions in this method secondary data from multiple observations like liquid rate information at well locations are used to update the parameter variables like permeability values in a reservoir after both the primary and the secondary data are transformed in an indicator space where the transformed values are either one or zero multiple indicator transformations are performed for multiple threshold values chosen along the range of the cumulative density function cdf of the primary variables for the secondary variable a single threshold corresponding to the fiftieth percentile value in the ensemble of the absolute mismatch between the observed and simulated secondary variable responses is applied to make the indicator transformation the indicator variables can be used to update the cdf values of the primary variable for all the thresholds at any spatial location in the reservoir to get an updated cdf at that location using the proposed inda method the initial and the updated cdfs are then used to map the cdf updates to actual primary variable updates the inda update equation does not rely on any linear least square assumptions that might involve minimization of variance and thus is free from the assumptions of the primary and the secondary variables being multi gaussian the updates are therefore optimal for non gaussian variables additionally the inidicator transformation leads to the treatment of the bi variate relationships between the primary and the secondary variables to be non gaussian as well in fact the indicator cross covariance describing the relationship between the primary and secondary indicator variables used within the algorithm can be directly related to the non parameteric non gaussian bivariate distribution describing the two variables the indicator variables are immune to any non linear transformations and so the update process remains optimal regardless of the relationship linear or non linear between the data mismatch and the primary and or secondary variables a general treatment of this bi variate non gaussian relationship implies the treatment of the non linear nature of the transfer functions like reservoir simulators appendix d tau model is used to evaluate the probability of outcome of a given event a given some conditional information from multiple sources two in this paper like b and c in this paper the event of interest is the outcome of a categorical variable a being equal to a certain category k clay sand or simply put a k additionally the information source b is the hard conditioning data and multiple point statistics from training image and c is flow response data recorded at well location at different instances of times in the tau model the combined probability p a b c for a category of choice is given by the equation d 1 x a b a τ 1 c a τ 2 w h e r e x p a b c 1 p a b c b p a b 1 p a b c p a c 1 p a c a p a 1 p a τ 1 a n d τ 2 a r e p a r a m e t e r s u s e d i n t h i s e q u a t i o n in the first line of eq d 1 the term x represents the inverse odds ratio of occurrence of an event a k at a location u x y z conditioned to the information b and c the terms b and c represent the inverse odds ratio of occurrence of the event a k conditional to the data information b and c respectively these inverse odds ratios measure the information contribution of the data source in explaining the occurence of a variable of interest they approach infinity when the individual conditional probabilities tend to 1 and to 0 when the conditional probabilities tend to 0 for instance if p a b tends to 1 b tends to infinity meaning the outcome of a can be fully explained by infomation source b same holds true for p a c and c in context of information source c the terms τ 1 and τ 2 account for the redundancy of information krishnan 2008 arriving from the data sources b and c respectively redundancy when quantified implies the amount of information coming from one information source that is also contributed by another information source in this work the tau model implementation in the snesim source code stanford center for earth resources forecasting 2015 is used and is given by the following equation d 2 τ 1 u p a u b p a 1 p a if p a u b p a p a p a u b p a otherwise τ 2 u p a u c p a 1 p a if p a u c p a p a p a u c p a otherwise in eq d 2 τ 1 for instance is proportional to p a u b p a the probability of outcome of a in the absence of any additional information is given by p a and by p a b in the presence of an information source b a large value for p a u b p a signifies a large amount of information from the source b towards outcome of a leading to a large τ 1 value and vice versa following the same logic a large τ 2 value would mean a large amount of information from source c towards outcome of a and vice versa the relative difference in magnitude of τ 1 and τ 2 is used as a measure of redundancy a large difference means less redundancy and a small difference means more redundancy as in the tau model first line of eq d 2 the inverse odds ratios are raised to the exponent of τ values a large τ adds more weight to its corresponding information source for instance if τ 2 is very large compared to τ 1 it will add more weight to information c by raising it to a higher exponent in the tau model and vice versa appendix e the comparisons of flow responses of inda mps with inda loc at various well locations are shown in fig e 1 while the comparisons for inda mps with enkf loc are shown in fig e 2 in these figures the reference responses are shown by the solid line with dotted markers figure e 3 shows the comparison of the average of all updated conductivity models obtained using inda mps against those obtained using enkf loc the average of the final enkf loc updated models fig e 3 c does not represent the reference distribution as accurately as either inda mps or inda loc which is clearly evident by the poor connectivity of high conductivity between wells inj2 and prod11 and a general lack of channel like features in the ensemble average the connectivity in this region is better represented by inda mps and inda loc figs 15 c 15 d respectively in fact the ensemble average of the enkf loc models does not exhibit the crisp contrast between high and low conductivity values that characterizes the non gaussian reference the lack of continuity of extreme conductivity values is a result of the inherent gaussian assumption in enkf the inda mps updated models fig e 3 d not only captures the reference distribution of conductivity very well but also represents the connectivity of the conductivity field accurately figure e 4 shows the ensemble average of pressure distribution before and after the first inda mps update step it should be noted that as mentioned in section 3 3 the pressure update within the inda mps framework are performed simply by the two point inda loc method as evident from the fig e 4 the initial ensemble pressure distribution is very smoothly distributed as expected and hence the updates lead to a smooth distribution as well the top and bottom five rows of cells appear to have a very high pressure because they are separated from the rest of the reservoir with a zero permeability row which causes the pressure to remain at the high initial value this is done because conductivity models generated using most geostatistical models might have some inconsistent conductivity distributions at the edges this is often referred to as the edge effects and it is always advisable to not use these values for flow simulation purposes supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103611 appendix f supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
476,ground water flow and the transport of contaminants in groundwater systems are strongly controlled by geologic heterogeneities because the data available to model these subsurface heterogeneities are generally sparse the observed groundwater flow piezometric head at well locations or the concentration profile of solutes provide valuable additional information about these complex subsurface systems regardless there is likely to be considerable uncertainty associated with the predictions of the subsurface heterogeneities this has motivated the development of several ensemble based schemes for assimilating flow response data into predictions of subsurface heterogeneities most ensemble based data assimilation methods including ensemble kalman filter enkf or indicator based data assimilation inda used for assimilation of dynamic data into geologic models utilize statistics in the form of covariances calculated using the ensemble of models to perform model updates these covariance based updates do not preserve the complex spatial characteristics of geologic structures that are better represented using multiple point statistics additionally in enkf the mismatch between the observed and the simulated values are assumed to be linearly related to the parameter updates and the distribution of the state variable is assumed to be multi gaussian the spatial distribution of the primary variables like facies can be non gaussian because of the spatial continuity exhibited by channel facies sands and non channel facies clay that have distinctly different properties also the relationship between the mismatch and the parameter updates can be strongly non linear in indicator based data assimilation inda method the indicator transforms of parameter and mismatch variables are used which affords the treatment of these variables as bi variate non gaussian furthermore in the indicator transformed space restrictive linear assumption can also be lifted as the indicator transform is invariant under non linear transformations however the updates are still governed by the bivariate interactions between the state variables a multiple point extension of inda is proposed in this paper which utilizes existing multiple point simulation algorithms like single normal equation simulation snesim in combination with inda to preserve the spatial characteristics of the geologic model while at the same time honor the observed flow or transport response the proposed method is applied to a synthetic groundwater system with complex channel distributions the channel features of the final updated ensemble of models is shown to converge towards the reference model and the ensemble flow responses are also shown to match the reference flow response keywords data assimilation multipoint geostatistics inda enkf non gaussian parameters flow data integration probability map indicator transform methods 1 introduction modeling of ground water flow entails an understanding of the subsurface distribution of different facies having different conductivities that controls the flow of fluids therefore a proper understanding of ground water flow in an aquifer requires detailed characterization of the subsurface conductivites within the area under consideration the inability to directly measure underground conductivity compounded by the fact that it is spatially distributed along complex channel like structures make its characterization very difficult with the help of advanced simulation algorithms like snesim strebelle 2002 direct sampling mariethoz et al 2010 and simpat arpat and caers 2007 filtersim zhang et al 2006 and using rock facies data at limited number of locations within the modeling domain it is often possible to come up with an ensemble of models for channel distributions representing the uncertainty associated with the prediction of facies distribution in a given aquifer although the initial uncertainty in conductivity distribution is often a good starting point for the characterization of an aquifer this uncertainty is often high because the available hard data is often sparse and thus some secondary data is assimilated to reduce this uncertainty data assimilation algorithms are suitable for this task as these algorithms use some observed variables like fluid flow rates and heads at well locations to update the subsurface facies or the conductivity distribution as the main variables of interest for subsurface characterization are parameters like facies and conductivities these are called primary variables while flow responses like flow rates and head are termed secondary variables the secondary variables are generally related indirectly to the primary variables and this relationship is often non linear data assimilation algorithms like ensemble kalman filter enkf evensen 1994 and burgers et al 1998 utilize the ensemble of model realizations along with the observed secondary variables to update the uncertainty in the primary variable distributions it is known that kalman filter is a linear least squares estimator llse that is the estimator is by definition a linear function of the observations this can also be said about enkf as it is applying the same update using sample statistics such a linear update is optimal only if the multivariate distribution describing the variables involved in the update is multi gaussian these simplifications render the applicability of enkf to be limited especially in cases where the ground water flow and transport of chemical species in the groundwater are strongly controlled by complex spatial patterns of geologic facies that reflect non gaussianity efforts to handle the non gaussian distribution of variables involved in the update equation led to the development of the normal score ensemble kalman filter ns enkf as discussed in zhou et al 2011 and schoniger et al 2012 although the method does propose a remedy by first applying normal score transform to the variable distribution and then performing an enkf update the transform procedure is univariate and does not ensure multi gaussianity of the multivariate distribution describing the relationship between state variables another issue that requires attention in data assimilation is that of the non linear linear nature of the transfer function while enkf requires a full ensemble of flow simulations to propagate the covariance of state variables in time other methods like extended kalman filter leng and yeh 2003 and yeh and huang 2005 use a linearized transfer function relating model parameters and the flow response variables this linearization is achieved by applying a taylor expansion and then using this simplified transfer function to propagate the state covariances in time for complex non linear transfer functions this method can be both time consuming and complicated to apply in another method called the indicator based data assimilation inda kumar and srinivasan 2019 indicator transforms of the state variables are used to update the primary variable uncertainty directly see appendix c it has been shown that inda addresses the issue of univariate non gaussian distribution of variables and the bivariate non gaussian relationship between the secondary data mismatch and primary variables since this bivariate non gaussian relationship is a result of non linear mapping of the state variables to the data mismatch by the non linear transfer function inda is designed to handle this non linearity although all these techniques show advancement over other ensemble based data assimilation methods they do not address the update of spatial distributions that are characterized by complex non linear features that are best described using multiple point statistics the current ensemble based methods utilize covariances two point statistics in the update equation that is only adequate for representing the statistical characteristics of multi gaussian distributions since the initial ensemble of models are often generated using multiple point statistics based simulation algorithms the application of two point update methods often degrade the geological realism of the final updated models the need for multiple point statistics mps based data assimilation techniques in ground water modeling becomes even more evident when a deeper look is taken into the mechanisms of flow and transport in complex porous media for instance the well fluid rates and or the well head values are not only dependent on the conductivity values at individual locations but also depend on the conductivity of the flow path for fluids i e the conductivity at several locations taken jointly this leads to the following observations the mismatch between the observed and simulated values should be associated to a pattern of conductivity rather than the value at an individual location the update procedure should yield the updated higher order statistics describing the pattern of variability of conductivity these observations motivate the discussion of multipoint based conductivity updating beyond the two point covariance with a goal to preserve the initial multiple point features throughout the update process an interesting data assimilation method introduced by zhou et al 2012 utilizes the ensemble of models as a repertoir of multiple point statistics that is used to perform mps update of reservoir variables a detailed evolution of data integration methods to improve parameter estimation and model forecasting in hydrogeology is provided in zhou et al 2014 several methods like gradual deformation method gdm hu 2000 and caers 2003 aiming at mps updates have also been proposed in the past in gdm an optimization aimed at minimizing the mismatch between observed flow responses secondary data and the ones simulated from a model derived as a weighted linear combination of the realizations in the ensemble of models the weights resulting in the minimum mismatch are then used to define the most representative model honoring the observed data since the entire model is treated as an update variable and not broken down into pixel based updates the multiple point distributions of the geologic properties are conserved after the updates are performed however the method works consistently only if the variables being updated are multivariate gaussian probability perturbation methods ppm hoffman and caers 2005 tarun and sanjay 2003 yadav et al 2005 are another set of algorithms that are aimed at performing mps updates in ppm the probability of the model parameters conditional to the given flow response data are updated by optimization of a single parameter called the deformation parameter since the matching is done directly in the probability space there is no multi gaussian assumption underlying this method an interesting approach to mps updates is the probability conditioning method pcm jafarpour and khodabakhshi 2011 which uses probability distribution maps of occurrence of certain parameter categories conditioned to some flow response data and then combines this information with a mps simulation algorithm like snesim strebelle 2002 in form of secondary data in addition to the methods discussed in this section a repository of methods aimed at combining conditional probabilities from different sources to get a final conditional probability are discussed in great detail by allard et al 2012 one advantage of the pcm method is that it does not explicitly require any assumptions related to the distribution of parameters or the relationship between the observed variables and the variables being updated however these assumptions might be implicitly imposed depending upon the technique that is used to deduce the secondary probabilities used in the method for instance in jafarpour and khodabakhshi 2011 the enkf method is used for the evaluation of secondary probabilities meaning the assumptions related to enkf also apply to the these probabilities additionally this implementation of pcm assumes no redundancy in information from the training image ti used in the mps simulation and the secondary probability calculated from enkf this is because in the τ model journel 2002 used for combining the probabilities from training image and that from production data the τ value is assumed to be equal to 1 0 in a another work by ma and jafarpour 2019 a modification of the pcm is proposed where a sensitivity based evaluation of spatially varrying τ values is proposed for proper probability conditioning of probability based on the distance between the points in the reservoir and the secondry data source locations further this method also investigates the reasons which lead to snesim method resulting in almost determininstic output once few initial locations are simulated in the reservoir some of the limitations assumed within the aforementioned methods are overcome in this paper that introduces a new mps update algorithm that combines the snesim algorithm stanford center for earth resources forecasting 2015 with the inda algorithm as inda algorithm kumar and srinivasan 2019 is free of any gaussian assumptions for the variables being updated and permits non linear mapping of the data mismatch to the update variable the secondary probabilities are more reliable than the ones deduced from enkf further the proposed method calculates the redundancy in the information obtained from secondary information and the training image used within snesim it is shown that the redundancy or τ parameter changes with each update step and follows a trend that is speculated in jafarpour and khodabakhshi 2011 this method also considers uncertainty in the ti and updates the ti statistics at each update step in the next few sections the proposed method called the indicator based data assimilation with multiple point statistics inda mps is introduced the method for evaluation of secondary probabilities is explained in detail then the results obtained from the application of the proposed method to a synthetic aquifer model are analyzed and finally these results are also compared with the results obtained from the application of enkf and inda to the same model further some relevant discussions are also added in the appendix sections 2 proposed multiple point extension to inda in this section a data assimilation method called indicator based data assimilation to update multiple point statistics inda mps is proposed by extending the existing inda kumar and srinivasan 2019 see appendix c algorithm this extension uses the update procedure to yield updates of multiple point statistics corresponding to pattern templates describing subsurface conductivity variations represented within a training image ti these updated multiple point statistics are used within an mps simulation algorithm like snesim strebelle 2002 as the exhaustive update of higher order statistics for an entire aquifer is computationally impractical for most real cases the statistics updates are restricted only to a few selective templates within the ti in the two point scheme of inda the cumulative probabilities are updated one location at a time by using some secondary data but in the multiple point counterpart the updates of higher order statistics are made only for the aforementioned selective templates then the updated template statistics can serve as an input to a multiple point simulation scheme like single normal equation simulation snesim strebelle 2002 that relies on multi point statistics drawn from training images ti maharaja 2008 this update step addresses the uncertainty in the ti by conditioning it with some observed secondary data in the subsequent sections a method for isolating the most relevant or selective templates in the training image describing the channel connectivity in the ensemble of models is presented updates to the statistics for those selective templates are performed using inda with the flow response data at well locations as the secondary data it should be noted that although the ensemble of conductivity models generated using the updated ti statistics will have different spatial connectivity than the one generated using the initial ti statistics it will not capture the local update of variables to achieve the local update of variables the information from the conventional application two point inda updates to each location in the model is provided as secondary data to snesim in addition to the update ti statistics it is shown that models thus generated are conditioned to the production data and honor the spatial continuity of geologic properties captured by the ti at the same time this results in updated models that represent complex channel features governed by multiple point statistics in summary the inda mps update process comprises of two steps update of the global multiple point statistics of the primary variables by updating the ti statistics for a selective set of templates using the observed secondary data local update of the primary variables using the observed secondary data an important remark the update of the multiple point statistics mps for the selected templates is performed by the two point inda update method using the observed flow responses as secondary data this is done by treating the mps for a given template in the ti as an update parameter and using the available repeats of this mps from the ensemble of models to create an ensemble of these statistics indicator transforms of the ensemble of mps data and ensemble of mismatch between the observed and simulated flow response data are used within the two point inda this process updates the global mps from the ti in order to render models that accurately reflect local updates from flow response data in the vicinity of wells the global mp statistics will have to be combined with local information such as soft probabilities at each location in the reservoir inferred by assimilating flow response data the process is discussed in detail in the following sections 2 1 global mps update by ti statistics update as discussed in section 2 the first step in the inda mps update method is the update of ti statistics for selective templates used in the snesim algorithm a brief recap of the snesim algorithm is presented in appendix b so that the process of updating the multiple point statistics using dynamic data can be better understood within the data assimilation framework proposed in this paper the ti statistics for some selective templates are updated in the search tree using the inda procedure these selective templates are chosen based on their number of repetitions within the training image the templates appearing more frequently in a training image are considered to be a key spatial characteristic of the ensemble of models generated using the training image for the example case of data assimilation presented in some later sections the templates appearing more than 200 times in a training image are considered as selective templates the multiple point statistics corresponding to these templates are updated using the inda procedure this update is performed by using the inda update equation discussed in eq 12 of kumar and srinivasan 2019 which treats the mutiple point statistics of the selective templates from the ti as the primary variable ψf that is updated by the indicator transformed secondary data yp where p 1 2 n from n observation locations 1 e i ψ f ψ k y 1 1 y 2 1 y 3 1 y n 1 e i ψ f ψ k i 1 i n λ i 1 e y i the thresholds ψk required for the selective templates ψf in eq 1 are considered to be percentile values of 1 through 100 in the intervals of 5 calculated by pooling the multiple point statistics of all the selective templates for all the models in the ensemble in this paper the ti statistics treated as the primary variable ψf are the number of repetitions of a categorical variable clay or sand at the center of a selective template while the absolute mismatch between the simulated and observed flow responses tracer fluid rate at p 1 2 n well locations at a given time step are considered the secondary variables yp it should be noted that any update scheme other than inda can also be used to update the ti statistics if desired the benefit of inda over other schemes is because of its ability to handle non gaussian distribution of state variables naturally the ensemble of models is re simulated using the updated multiple point statistics and the hard conditionning data clay and sand information at well locations in the reservoir using this updated ensemble the updated probability p a b at any spatial location can be estimated by pooling the categorical values of the variable at that location from the entire ensemble as mentioned in section 2 the inda based procedure yields updated global multiple point statistics thus the updated p a b do not ensure that the simulations reflect the local flow characteristics in the vicinity of the wells providing flow response data for the update to render the updates locally accurate the integration of secondary flow response data c is required 2 2 secondary data integration into snesim for local updates as mentioned in section 2 the integration of secondary data c is required to achieve local uncertainty updates of model parameters the inference of local information at a location u from the flow response data c can be designated as p a u c while the informtion from the updated training image 2 1 is designated as p a u b the updated local probabilities combining both geologic information b as well as information c from production data is annotated as p a u b c this section describes a way of combining information from aforementioned sources by using the tau model bordley 1982 journel 2002 krishnan 2008 this method requires at each update location u the probability values p a u b p a u c and a pair of global tau values τ 1 τ 2 describing the redundacy in the information sources b and c impacting the outcome of a u as shown later in this section that τ 1 and τ 2 are arithmetic averages of τ 1 u and τ 2 u respectively τ 1 u and τ 2 u at any location u can be written as a functions of p a u b and p a u c respectively as described in section 2 1 the p a u b are known at the begining of the local update step thus τ 1 u values can be easily evaluated for any location u for the evaluation of p a u c a non linear function in p a u b p a u c p a u b c and τ 1 u is solved iteratively since the goal of the local step is to solve for p a u b c this is not known at this stage thus an approximate value of p a u b c is used to solve for p a u c the probability of the outcome of a u given the geologic information b and conditioned by secondary data c using the two point inda with localization is used as the approximate value of p a u b c the localization embedded in inda loc ensures that resultant p a u c is impacted more by secondary data sources that have smaller spatial lags from u both primary conductivity and secondary flow response data are indicator transformed for the inda step as described in kumar and srinivasan 2019 once the p a u c values are solved they are used to solve for τ 2 u values at each location u next the arithmetic average τ 1 and τ 2 of τ 1 u and τ 2 u respectively are calculated to get an average representation of the redundancy in information for the two data sources b and c at the given timestep for the entire reservoir finally the p a u b p a u c τ 1 and τ 2 values are used as input to the snesim algorithm to generate an updated ensemble of models that represent the updated multiple point statistics the equations involved for the achieving the aforementioned calculations are discussed in detail in the follwing paragraphs in snesim stanford center for earth resources forecasting 2015 the probability p a u b c at any spatial location u x y z is calculated by the integration of p a u b and p a u c using the tau τ bordley 1982 journel 2002 krishnan 2008 model according to this model 2 x a b a τ 1 c a τ 2 w h e r e x p a b c 1 p a b c b p a b 1 p a b c p a c 1 p a c a p a 1 p a τ 1 a n d τ 2 a r e p a r a m e t e r s u s e d i n t h i s e q u a t i o n a brief description of terms used in the tau model are provided in appendix d it is clear from eq 2 that secondary data integration at any location u x y z using this approach requires p a p a u b p a u c τ 1 u and τ 2 u p a is usually known in the form of prior proportion of category k and the values of p a u b are calculated using the multiple point statistics section 2 1 from the training image within the snesim algorithm appendix b so the only remaining unknown quantities are p a u c τ 1 u and τ 2 u one approach to compute the τ values accounting for redundancy of different data sources is implemented within the snesim algorithm stanford center for earth resources forecasting 2015 and is given by the following equation 3 τ 1 u p a u b p a 1 p a if p a u b p a p a p a u b p a otherwise τ 2 u p a u c p a 1 p a if p a u c p a p a p a u c p a otherwise in the following for simplicity p a b p a c τ 1 and τ 2 are assumed to designate p a u b p a u c τ 1 u and τ 2 u respectively unless specified otherwise throughout the rest of the paper now applying the natural logarithm of the first line in eq 2 and substitute for τ 2 as a function of p a c from eq 3 the resultant expression is 4 ln x a τ 1 ln b a p a c p a 1 p a ln c a if p a c p a τ 1 ln b a p a p a c p a ln c a otherwise further if we replace c in eq 4 as a function of p a c from the third line in eq 2 then the following can be written as 5 ln x a τ 1 ln b a p a c p a 1 p a ln p a c 1 p a c a if p a c p a τ 1 ln b a p a p a c p a ln p a c 1 p a c a otherwise taking all the terms to the l h s a non linear objective function g in p a c can be written as shown in eq 6 6 g p a c ln x a τ 1 ln b a p a c p a 1 p a ln p a c 1 p a c a if p a c p a ln x a τ 1 ln b a p a p a c p a ln p a c 1 p a c a otherwise in a root finding sense the objective would be to update guesses of τ 1 and p a c in order to converge to a value close to 0 in this case convergence corresponds to an objective function value of 10 3 for g eq 6 with an initial guess value for p a c o the converged value of p a c can be evaluated using a scheme such as the newton raphson iterative method assuming τ 1 o τ 2 o 1 the p a c o values are evaluated using eq 2 the assumption of τ 1 o τ 2 o 1 ensures that the initial guess for p a c o disregards data redundancy between information sources b and c after this stage all further calculations are performed with τ 1 u values calculated using eq 3 the additional variable required for the calculation of p a c o is x which is a function of p a b c it should be noted that the goal of inda mps itself is to integrate information from training image and hard conditioning data b with secondary data c to calculate p a b c using mps based updates so these probability values are not yet available the two point inda kumar and srinivasan 2019 updated ensemble of models can be used to get an approximate measure of p a b c this is because the two point inda updates evaluate probabilities p a b c by combining secondary data c into an ensemble of models generated using training image and hard conditioning data b thus the p a b c calculated using inda are a good estimate of the actual p a b c values and are used for the calculation p a c o in theory the final converged values of p a c for each category at every location should add up to one to ensure that they represent legitimate set of probability values but practically they often add up to values very close but not exactly one to ensure this legitimacy the probability values for each category are normalized at every location and are then used to evaluate final τ 2 values for secondary data integration in snesim the p a u c values are required at all spatial locations u x y z and can be readily provided in form of the converged and normalized p a u c values the snesim algorithm also requires scalar values of τ 1 and τ 2 values that are representative of all locations within the model the application of eq 3 results in a different value of τ 1 and τ 2 at each location u x y z in the model average values of τ 1 u and τ 2 u respectively are calculated over the entire spatial domain u x y z and specified as the unique τ values for the entire simulation the ensemble of models thus generated have updated global multiple point statistics for the selected templates and the secondary data integration results in reduced local uncertainty in summary the two point inda loc provides a way to calculate approximate values of local conditional probabilities for the spatial parameter k at a given location p a k c conditioned to the secondary data c the redundancy of that conditional probability to the conditional probability due to other information sources b is described by the parameters τ 1 and τ 2 in order to determine these redundancy parameters an iterative scheme is implemented a flow chart of this data assimilation scheme implemented in inda mps is shown in fig 1 3 demonstration of indicator based data assimilation with multiple point statistics inda mps updates using a synthetic channel aquifer the indicator based data assimilation with multiple point statistics inda mps method has been explained in detail in the previous sections in this section an example of this data assimilation method is demonstrated with an aritficial but realistic working example the inda mps requires p a c values for the generation of mps updates section 2 2 and the p a c calculations in turn require inda with localization updates localization ensures that observation data points located far from the update parameter location do not contribute significantly towards the magnitude of update this may happen because of spurious covariance relationships occurring at large length scales due to the use of a limited ensemble size the localization method employed in this research is detailed in greybush et al 2011 where the contributions of the observed data toward the update of a parameter are weighed according to their spatial lag from the location to be updated in greybush et al 2011 the elements of the diagonal observation error matrix in the enkf analysis equation are weighed with the weights increasing exponentially as the spatial lag between the update parameter and the observation data source increase for the application of localization in the context of inda the diagonal elements of the secondary data indicator auto covariance matrix lhs of eq 13 in kumar and srinivasan 2019 were weighed using the same exponential weight function as in greybush et al 2011 3 1 description of reference model the model used in this example is a two dimensional model with a total of 13 000 100 130 1 grid blocks the grid dimensions in the x and y direction are same for all grid blocks with a value of 25m while the thickness of the grid blocks vary in space as shown in fig 2 a additional model description is provided in table 1 the reference model has sand channels high conductivity allowing easy flow of fluid and clay areas low conductivity fluid flow is not easy with conductivity values of sand and clay facies in m d 1 sampled from the distributions given by n 1 31 0 026 2 and n 0 026 0 013 2 respectively fig 2 b shows the reference conductivity distribution of the model used in this exercise the tracer rate production injection data at the well locations shown in fig 2 b are used as observed data for data assimilation all maps representing the distribution of sand and clay facies and the corresponding statistical attributes like mean and variance are reported in the units of natural logarithm of conductivity lnk 3 2 generation of initial ensemble of models the ensemble of initial models is generated using the single normal equation simulation snesim strebelle 2002 available in gslib format stanford center for earth resources forecasting 2015 the snesim algorithm produces realizations that are modeled by multiple point statistics which enables representation of channel like features that are signature characteristics of a fluvial depositional environment the algorithm relies on a training image maharaja 2008 to calculate the higher order statistics the training image generated for the purposes of this exercise is shown in fig 3 a in order to replicate a realistic scenario the conditioning data used for the generation of the initial ensemble of 100 conductivity models were restricted only to the well locations for sand facies and additional six locations with clay facies assumed to be dry well locations fig 3 b the conductivity distribution of some models ensemble members 10 20 50 and the reference conductivity distribution are shown in fig 4 the cumulative probability distribution cdf of conductivity values of the entire reference model shows a non gaussian characteristic fig 5 the same characteristic is captured well by the initial ensemble of conductivity models fig 5 for flow simulation purposes the top and bottom five grid rows are set to zero transmissibility to avoid edge effects which might have impacted the snesim algorithm 3 3 assimilation of secondary data to update ensemble of conductivity models the initial ensemble of conductivity models represents the initial uncertainty associated with the distribution of conductivity in the aquifer the tracer fluid rate production or injection data from the reference model is used as secondary data for updating the ensemble of models the cmg black oil with seawater simulation option with default parameters for the sea water properties is used in for generating flow response data in this paper the sea water saline water serves as a tracer in this work the initial reservoir pressure is 3 600psi with a water saturation of 1 0 a zero tracer concentration and all four boundaries as no flow all injection wells inject tracers from initial time to the end of simulation time into the reservoir the primary data cut off is defined as shown in fig 5 it is evident that more cut off values are specified in the parts of the cdf where there is a steeper change in probability than in parts with gradual change for calculation of the secondary thresholds the procedure discussed in kumar and srinivasan 2019 is used an uncorrelated gaussian error with 0m 3 day mean and a standard deviation of 48m 3 day is used for all observations the tracer fluid rate data is assimilated every 1 month for 14 months resulting in a total of 14 model update steps the conductivity values are updated using the inda mps while in the absence of training images the state variables like tracer fluid fraction and head values in the grid block locations are updated using inda with localization as discussed in section 2 2 each of these assimilation steps requires calculations for spatial distribution of p a c values and average τ values the plot of τ 1 and τ 2 as they vary with time is shown in fig 6 where τ 1 describes the redundancy of the conditioning data and the multiple point statistics collectively b with the other information available to model the parameter variable a the information from conditioning data is constant throughout the assimilation process but the pattern statistics information retrieved from the training image is updated at each assimilation step by updating the global training image statistics fig 1 however because the pattern statistics corresponding to only a selective number of templates are updated the mp statistics only change by a small amount this is indicated by the τ 1 value remaining almost constant with time the τ 2 describes the redundancy of the tracer fluid rate response data c with other data sources as they pertain to the parameter variable a as the information contained in c is changing throughout the history matching process the τ 2 values also change the values increase with time at a decreasing rate this means that although the information in c tracer fluid rate responses related to the facies distribution in the model increases with time the additional information contained in c with each additional assimilation becomes smaller this diminishing value of information in the secondary information source c is also reflected clearly by the update exhibited by a single model ensemble member 50 with each update step as shown in fig 7 starting from the initial model fig 7 a the first few update steps figs 7 b to 7 e result in large changes in conductivity distribution in the model while the updates to the conductivity distribution after the 7 th fig 7 e and the 14 th update fig 7 f is minimal as expected from the profile of τ values shown in fig 6 for the assimilation of data for the first month the maps of p a sand b p a sand c and the combined probability of p a sand b c is shown in fig 8 which clearly shows how the combined probability information weighs the information from the two sources b and c with their respective τ values for that time step the updated models are then run to the next time step where observed data is available for assimilation 3 4 analysis of results the ensemble of models once updated are then flow simulated for a period of 47 months from the initial time as the models are calibrated only for the first 14 months section 3 3 the ability of the models to match future rates for the next 33 months is tested comparisons of tracer fluid rate forecasts with respect to reference rate values are made both for the initial ensemble of models and the final inda mps updated models fig 9 these comparisons bring out the effectiveness of the proposed data assimilation technique it is evident that post assimilation the updated models predict the tracer fluid rate responses that are very close to the reference rate the impact of the update process on the spatial distribution of conductivity is clearly seen in fig 10 where the final updated models figs 10 a 10 b and 10 c show channel continuity and location that are very similar to the reference model fig 10 d the channels in the initial models figs 4 a 4 b 4 c on the other hand exhibit characteristics quite different from the reference model fig 4 d in addition to matching tracer fluid rates the inda mps update scheme preserves the initial non gaussian distribution of the conductivity fig 11 this is a direct implication of the indicator coding of the data and the subsequent probability update that eliminate the multi gaussian assumption implicit within traditional enkf this approach also results in the range of conductivity values falling within desired or realistic limits as discussed in section 2 1 the inda mps method updates the global training image statistics and the local conductivity values the update of training image statistics is a unique feature of the inda mps method which ensures that the initial uncertainty in the training image is addressed during the update process selective templates are chosen from all the possible templates in the training image by using the criteria mentioned in section 2 1 some of these templates are shown in fig 12 the frequency of occurrence of sand or clay facies at the center of these templates is then updated using inda fig 13 these new frequencies are then used within the snesim algorithm along with additional local conditioning information to generate a new ensemble of models section 2 2 fig 14 shows the comparison of the initial conductivity realizations with the realizations generated using the updated training image statistics after 14 update steps this gives an idea about the contribution of global multiple point statistics update alone towards the description of final models as can be seen from the figure the updating of the multiple point statistics results in better depiction of the curvilinear characteristics of the channel especially in the regions highlighted in fig 14 however the updated regions do not neccessarily reflect the position of the channels around wells where the production information is available this motivates the assimilation of the conditional probability p a c derived using the dynamic data discussed in section 2 2 4 comparison of results from inda mps inda with localization inda loc and enkf with localization enkf loc this section compares the models and the responses from models generated using different data assimilation methods like inda mps inda using two point covariance with localization inda loc and enkf with localization enkf loc as is the case in section 3 the two point update encoded within inda mps in order to infer the conditional probability p a c is inda with localization the updates are implemented using a filtering approach meaning after updating the parameters and the state variables flow simulations are conducted using the ensemble of models until the next update time step the other methods which are being compared to inda mps are inda with localization inda loc and enkf with localization enkf loc the localization method is the same as discussed in section 3 the same 100 ensemble of models mentioned in section 3 2 is used for all the three methods comparison of flow rates the comparison of flow rates from inda mps and inda loc updated models suggest that the performance of both methods are comparable with respect to the ability to forecast tracer fluid rates of wells the same holds true for comparison of flow rates from inda mps and enkf loc updated models figures illustrating these comparisons are showin in appendix e comparison of ensemble mean noticeable and important differences become evident when the ensemble mean of the updated conductivity models from the different methods are compared fig 15 shows the comparison of ensemble average of models obtained by inda mps compared to those obtained by inda loc the average map fig 15 b of the initial ensemble do not reproduce the spatial variability exhibited by reference conductivity field fig 15 a accurately the average of the final inda loc updated models fig 15 c exhibit spatial variability more consistent with the reference model but the continuity of high conductivity features is disrupted because of the two point covariance used for updating the conductivity values the inda mps updated models fig 15 d exhibit the continuity of conductivity exhibited by the reference most accurately similar comparisons of ensemble mean of inda mps updates with enkf loc are also provided in appendix e for further reference comparison of ensemble variance additional important differences are noticed when the ensemble variance of the updated conductivity models from the different methods are compared fig 16 the comparison of the initial variance map of lnk fig 16 a with the inda mps updated variance map fig 16 d reveals that the uncertainty associated with conductivity predictions is reduced after data assimilation when compared to the reference model fig 15 a the low uncertainty areas show similar spatial extent and connectivity to the channel distribution in the reference model this can be attributed directly to the mps based updates coupled with the local conditioning brought about by the conditional probability p a c it should also be noted that despite the reduction of uncertainty in the middle of the channel structures higher uncertainty still persists at the channel edges as is evident in fig 16 d another important observation is the reduction in uncertainty in areas away from the well locations primarily in the bottom right corner of the map the corresponding updated values of conductivity in this region primarily comprises of low conductivity values fig 15 d which is in accordance with the reference values fig 15 a in that region the reason behind this outcome of clay distribution in the bottom right section of the map lies in the inclusion of local probability values in form of p a c as the region under consideration is far from any data observation source well the inda loc results in p a c values that are closer to p a values as a result of localization embedded in the inda loc algorithm this is because in areas far away from any well locations in the initial ensemble of models p a b is approximately equal to p a or global proportions of event a consequently the two point inda loc result in p a b c approximately equal to p a b in other words minimal update from the tau model definitions eq 2 we can say x and b in these areas are approximately equal to a as the aforementioned p a b c from inda loc and p a b are used together in the process of updating p a c and tau values as discussed in section 2 2 the p a c in these regions equals p a approximately because plugging x b a in the tau model all approximately equal results in c approximately equal to a next the p a b p a c values at each location and the average of tau values are used to evaluate a p a b c value using secondary data conditioning the tau model within the snesim algorithm since both p a b and p a c values are close to p a in the region in question it results in p a b c being close to p a the proportions for clay p a clay and sand p a sand used in this example are 0 64 and 0 36 respectively which means that p a c values at any of these locations under consideration are more likely to be higher for the clay category than the sand category thus the snesim algorithm with secondary data conditioning assigns clay category with high confidence to all these locations under consideration this can be seen in the final updated ensemble average and variance maps of lnk figs 15 d 16 d respectively in case of inda loc the regions located away from any secondary data sources undergo small or no updates as a result of this reduction in uncertainty is noticed in the areas closer to the well locations and along the channel features fig 16 c that are consistent with the reference values of conductivity fig 15 a although enkf loc employs the same strategy of small or no updates at distances away from secondary data sources the mismatch between the simulated and observed data are linearly mapped to new update values because of the gaussian and linear assumptions involved in enkf this linear mapping leads to low updated variance very close to the well locations and a smoothly varying reduced variance in regions away from the well that are impacted by the update process fig 16 b the various comparisons made in this section bring out the benefits of inda mps over two point methods like inda loc and enkf loc the updates based on multiple point statistics yield well defined channel structures in the updated models better than the two point update methods despite starting from initial ensemble of models that do not capture the reference distributions quite well the final ensemble of models closely represents the reference models these models have well defined channel structures with crisp boundaries between high and low conductivity regions while inda loc updated models provide a good average description of channels the boundaries of these structures are not well defined as in case of inda mps the lack of clear definition of boundaries is because the updates are performed using two point statistics indicator covariances in the case of enkf loc the channel descriptions are less accurate than those obtained using inda loc and inda mps although the channel bodies of the inda mps updated models have reduced uncertainty the edges of the well defined channel features exhibit persistent high uncertainty this is because sparse conditioning data results in high initial uncertainty in the spatial distribution of channels when flow response data from these well locations are assimilated the uncertainty along the structure of the channel is reduced but remains high along the channel edges because flow response data is only an indirect source of information and carries limited information about the location of channel edges 5 discussion and conclusions this paper presents the inda mps method that integrates dynamic data in order to model a conductivity field that is strongly non gaussian with a strong bi modal characteristic the initial ensemble of models is generated using only 26 hard data point out of 13 000 locations 0 2 of the locations in the area under consideration this sparsity of conditional data leads to an initial ensemble of models the average of which do not capture the complexities of the reference model but after the inda mps updates the average of the ensemble of models capture the complex connectivities of the channel distribution to an acceptable degree this demonstration of inda mps to reproduce reference channel features shows the potential of this algorithm to be applied to realistic cases the proposed inda mps method falls under the category of data assimilation algorithms called probability conditioning methods pcm this method feeds the probability map for the occurrence of a certain category of parameter variable from the ensemble of inda kumar and srinivasan 2019 updated models to the snesim strebelle 2002 algorithm as secondary conditioning data for mps simulation of an ensemble of updated models an optimization scheme is used to extract the usable probability values and a measure of redundancy τ values of information from various data sources are calculated during the model updates inda mps also performs the update of training image statistics at each update step in order to calibrate the ti statistics to the observed dynamic data in the demonstration of application of inda mps in this paper the conductivity field undergoes an mps update while the dynamic state variables like head are updated using the two point indicator data assimilation technique with localization although this approach performs very well in updating the ensemble of models this also provides some exciting avenues of research for developing methods to extract multiple point statistics directly from dynamic data and update them using an mps scheme the models obtained after multiple update steps retain the multiple point features of the training image while closely representing the reference model channel distributions additionally the calculation and correct usage of τ values make it possible to get the correct magnitude of update depending upon the information content in the observed data this means that if the observed data contains less additional information the models undergo smaller updates the correct use of information in the data during the mps updates prevent ensemble collapse which is a very common problem in majority of data assimilation schemes and also results in the flow rate responses from the final ensemble of models honoring the reference flow rate values in the example demonstrated in this paper a reference model with a bi modal conductivity distribution representing two facies was used but in general models with any number of facies can easily be handled by the inda mps algorithm credit authorship contribution statement devesh kumar conceptualization methodology software sanjay srinivasan conceptualization methodology declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the funding received as part of the john and willie leone endowment chair is gratefully acknowledged appendix a example data sets and binary versions of codes used for this research work are freely available for usage and testing as github releases ensemble kalman filter with localization enkf loc indicator based data assimilation with localization inda loc indicator based data assimilation with muliple point statistics inda mps appendix b the snesim algorithm for model updates as mentioned in strebelle 2002 snesim is an algorithm to generate geostatistical models honoring multiple point statistics of geologic properties like conductivity in this algorithm the variable being simulated is annotated as a in the context of a categorical variable simulation a can take any k number of categories a 1 2 3 k k with prior probabilities p a k or p a for simplicity these prior probabilities are often simple proportions from expert opinion or geological sources the information related to the spatial distribution of the variable is provided in form of a training image and some hard conditioning data that are recorded values typically at well locations combination of these information are collectively annotated as b training images capture the plausible ergodic variations of categorical properties across the spatial domain under consideration the statistics pertaining to the training image are calculated by evaluating the number of occurences of all possible spatial template t of a certain size in the ti the template consists of a central node and certain number size of template of neighboring nodes surrounding that central node for computational efficiency the training image is scanned once at the beginning of the algorithm for all possible combinations of outcomes in the neighboring nodes of a template pattern t and the number of repeats of all categories a k where k 1 2 k at the central node of these templates are stored in a search tree these repeats are used as multiple point statistics during the simulaton stage all the locations to be simulated in the model are visited in a random order considering both previously simulated and observed values of geologic properties as conditioning data a location to be simulated serves as a central node while the previously simulated values and original conditioning data within a template t form a pattern around the central node the probability of all possible categories at the central node p a k b where k 1 2 3 k are calculated from the mps extracted from the search tree and then the category with the highest probability is assigned to the central node the process is repeated until all locations are simulated appendix c indicator based data assimilation inda is a data assimilation method proposed by kumar and srinivasan 2019 to handle non gaussian distribution of state variables and non linear nature of transfer functions in this method secondary data from multiple observations like liquid rate information at well locations are used to update the parameter variables like permeability values in a reservoir after both the primary and the secondary data are transformed in an indicator space where the transformed values are either one or zero multiple indicator transformations are performed for multiple threshold values chosen along the range of the cumulative density function cdf of the primary variables for the secondary variable a single threshold corresponding to the fiftieth percentile value in the ensemble of the absolute mismatch between the observed and simulated secondary variable responses is applied to make the indicator transformation the indicator variables can be used to update the cdf values of the primary variable for all the thresholds at any spatial location in the reservoir to get an updated cdf at that location using the proposed inda method the initial and the updated cdfs are then used to map the cdf updates to actual primary variable updates the inda update equation does not rely on any linear least square assumptions that might involve minimization of variance and thus is free from the assumptions of the primary and the secondary variables being multi gaussian the updates are therefore optimal for non gaussian variables additionally the inidicator transformation leads to the treatment of the bi variate relationships between the primary and the secondary variables to be non gaussian as well in fact the indicator cross covariance describing the relationship between the primary and secondary indicator variables used within the algorithm can be directly related to the non parameteric non gaussian bivariate distribution describing the two variables the indicator variables are immune to any non linear transformations and so the update process remains optimal regardless of the relationship linear or non linear between the data mismatch and the primary and or secondary variables a general treatment of this bi variate non gaussian relationship implies the treatment of the non linear nature of the transfer functions like reservoir simulators appendix d tau model is used to evaluate the probability of outcome of a given event a given some conditional information from multiple sources two in this paper like b and c in this paper the event of interest is the outcome of a categorical variable a being equal to a certain category k clay sand or simply put a k additionally the information source b is the hard conditioning data and multiple point statistics from training image and c is flow response data recorded at well location at different instances of times in the tau model the combined probability p a b c for a category of choice is given by the equation d 1 x a b a τ 1 c a τ 2 w h e r e x p a b c 1 p a b c b p a b 1 p a b c p a c 1 p a c a p a 1 p a τ 1 a n d τ 2 a r e p a r a m e t e r s u s e d i n t h i s e q u a t i o n in the first line of eq d 1 the term x represents the inverse odds ratio of occurrence of an event a k at a location u x y z conditioned to the information b and c the terms b and c represent the inverse odds ratio of occurrence of the event a k conditional to the data information b and c respectively these inverse odds ratios measure the information contribution of the data source in explaining the occurence of a variable of interest they approach infinity when the individual conditional probabilities tend to 1 and to 0 when the conditional probabilities tend to 0 for instance if p a b tends to 1 b tends to infinity meaning the outcome of a can be fully explained by infomation source b same holds true for p a c and c in context of information source c the terms τ 1 and τ 2 account for the redundancy of information krishnan 2008 arriving from the data sources b and c respectively redundancy when quantified implies the amount of information coming from one information source that is also contributed by another information source in this work the tau model implementation in the snesim source code stanford center for earth resources forecasting 2015 is used and is given by the following equation d 2 τ 1 u p a u b p a 1 p a if p a u b p a p a p a u b p a otherwise τ 2 u p a u c p a 1 p a if p a u c p a p a p a u c p a otherwise in eq d 2 τ 1 for instance is proportional to p a u b p a the probability of outcome of a in the absence of any additional information is given by p a and by p a b in the presence of an information source b a large value for p a u b p a signifies a large amount of information from the source b towards outcome of a leading to a large τ 1 value and vice versa following the same logic a large τ 2 value would mean a large amount of information from source c towards outcome of a and vice versa the relative difference in magnitude of τ 1 and τ 2 is used as a measure of redundancy a large difference means less redundancy and a small difference means more redundancy as in the tau model first line of eq d 2 the inverse odds ratios are raised to the exponent of τ values a large τ adds more weight to its corresponding information source for instance if τ 2 is very large compared to τ 1 it will add more weight to information c by raising it to a higher exponent in the tau model and vice versa appendix e the comparisons of flow responses of inda mps with inda loc at various well locations are shown in fig e 1 while the comparisons for inda mps with enkf loc are shown in fig e 2 in these figures the reference responses are shown by the solid line with dotted markers figure e 3 shows the comparison of the average of all updated conductivity models obtained using inda mps against those obtained using enkf loc the average of the final enkf loc updated models fig e 3 c does not represent the reference distribution as accurately as either inda mps or inda loc which is clearly evident by the poor connectivity of high conductivity between wells inj2 and prod11 and a general lack of channel like features in the ensemble average the connectivity in this region is better represented by inda mps and inda loc figs 15 c 15 d respectively in fact the ensemble average of the enkf loc models does not exhibit the crisp contrast between high and low conductivity values that characterizes the non gaussian reference the lack of continuity of extreme conductivity values is a result of the inherent gaussian assumption in enkf the inda mps updated models fig e 3 d not only captures the reference distribution of conductivity very well but also represents the connectivity of the conductivity field accurately figure e 4 shows the ensemble average of pressure distribution before and after the first inda mps update step it should be noted that as mentioned in section 3 3 the pressure update within the inda mps framework are performed simply by the two point inda loc method as evident from the fig e 4 the initial ensemble pressure distribution is very smoothly distributed as expected and hence the updates lead to a smooth distribution as well the top and bottom five rows of cells appear to have a very high pressure because they are separated from the rest of the reservoir with a zero permeability row which causes the pressure to remain at the high initial value this is done because conductivity models generated using most geostatistical models might have some inconsistent conductivity distributions at the edges this is often referred to as the edge effects and it is always advisable to not use these values for flow simulation purposes supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2020 103611 appendix f supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
477,in this paper we focus on a general model to describe compressible and immiscible three phase flow in porous media the underlying idea is to replace darcy s law by more general momentum balance equations in particular we want to account for viscous coupling effects by introducing fluid fluid interaction terms in qiao et al 2018 adv water resour 112 170 188 a water oil model based on the theory of mixtures was explored it was demonstrated how the inclusion of viscous coupling effects could allow the model to better capture flow regimes which involve a combination of co current and counter current flow in this work we extend the model in different aspects i account for three phases water oil gas instead of two ii deal with both the compressible and incompressible case iii include viscous terms that represent frictional forces within the fluid brinkman type a main objective of this work is to explore this three phase model which appears to be more realistic than standard formulation in the context of petroleum related applications we first provide development of stable numerical schemes in a one dimensional setting which can be used to explore the generalized water oil gas model both for the compressible and incompressible case then several numerical examples with waterflooding in a gas reservoir and water alternating gas wag experiments in an oil reservoir are investigated differences and similarities between the compressible and incompressible model are highlighted and the fluid fluid interaction effect is illustrated by comparison of results from the generalized model and a conventional model formulation keywords multiphase flow in porous media three phase flow viscous coupling mixture theory compressible model water alternating gas wag waterflooding 1 introduction generally the processes of multiphase flow in porous media occur in many subsurface systems and have found many applications of practical interest such as hydrology petroleum engineering geothermal energy development and carbon storage bakhshian et al 2019 bakhshian and hosseini 2019 wu 2016 the immiscible three phase flow is always encountered in waterflooding for oil reservoirs with gas cap in immiscible co2 storage in depleted oil and gas reservoirs and steam floods and water alternating gas wag processes bentsen and trivedi 2012 juanes 2008 darcy s law was originally developed for single phase flow darcy 1856 conventional modeling of multiphase flow is normally based on darcy s extended law rose 2000 by incorporation of relative permeabilities muskat et al 1937 however recent experimental observations indicate that the flow mode co current or counter current can have a strong impact on the flowing phase mobilities that is to say the relative permeabilities are not only function of saturation but are also related to the effect of how the fluids flow relatively to each other bentsen and manai 1992 bourbiaux and kalaydjian 1990 viscous coupling viscous coupling i e fluid fluid interaction was firstly mentioned by yuster 1951 by using theoretical analysis to derive that relative permeability is a function of both saturation and viscosity ratio in addition capillary number was also proposed to be a factor affecting relative permeabilities ehrlich 1993 avraam and payatakes 1995 in general momentum transfer due to differences in interstitial velocities induces acceleration of the slower and deceleration of the faster moving fluid when the fluids are moving co currently deceleration of both fluid velocities will occur if they are moving counter currently ayodele 2006 bentsen and manai 1993 dullien and dong 1996 li et al 2004 in order to extend the single phase darcy s law to multiphase flow de la cruz and spanos 1983 derived theoretically darcy s empirical extended law by applying the method of volume averaging to stokes equation in kalaydjian 1987 1990 kalaydjian developed flow equations using the concepts of irreversible thermodynamics katchalsky and curran 1975 from a macroscopic understanding of two phase flow in porous media in addition some researchers tried to gain insight into how two immiscible phases flow through a porous medium by using simple analogous models such as tubular flow yuster 1951 bacri et al 1990 in langaas and papatzacos 2001 langaas and papatzacos used the lattice boltzmann lb approach to investigate effects of viscous coupling and concluded that counter current relative permeabilities caused partly by viscous coupling are always less than the corresponding co current curves under different levels of capillary forces using the same method li et al 2005 showed that their model was able to capture main experimental effects caused by viscous coupling they also mentioned that the interfacial area between the fluids is a key variable for relative permeability functions for two immiscible fluids flow in porous media a generalized model was developed in qiao et al 2018 for two phase flow with viscous coupling effect numerical investigations showed a better agreement with the experimental tests bourbiaux and kalaydjian 1990 compared to the conventional modeling the authors in bentsen and trivedi 2012 constructed modified transport equations for both co current and counter current three phase flow through a vertical incompressible model based on partition concepts their equations are used to estimate the amount of model error because of a failure to account for the effect of interfacial coupling which has two types viscous coupling and capillary coupling moreover sherafati and jessen 2017 investigated the effect of mobility changes due to flow reversals from co current to counter current flow on the displacement of wag injection processes complex multiphase flow in porous media and use of the theory of mixtures motivated by petroleum related applications various attempts to solve the three phase porous media flow model have been reported during the past decade falls and schulte 1992 guzmán and fayers 1997a 1997b juanes and patzek 2004 an interesting investigation was carried out in lie and juanes 2005 where a front tracking algorithm was proposed for constructing very accurate solutions to one dimensional problems for example wag test therein this was explored in the context of streamline simulation which decouples the three dimensional problem into a set of one dimensional problems along streamlines this work is limited to three phase immiscible incompressible flow and also gravity and capillarity were ignored different numerical methods have been implemented to simulate three phase flow in porous media a finite volume method was used in lee et al 2008 for solving compressible immiscible flow with gravity in heterogeneous formations by using the black oil formulation a hybrid upwinding scheme for phase flux was proposed in lee and efendiev 2016 for a finite difference approximation to solve the three phase transport equations in the presence of viscous and buoyancy forces a finite element method was applied to simulate fluid injection and imbibition processes in a deformable porous media gajo et al 2017 moreover dong and rivière 2016 applied a semi implicit method with discontinuous galerkin dg discretization to solve the incompressible three phase flow in two dimensions additional physical effects are also discussed and explored for three phase porous media flow such as hysteresis effects of relative permeabilities ranaee et al 2019 and elliptic regions juanes and patzek 2004 juanes 2008 lee and efendiev 2016 in juanes 2008 juanes presented a nonequilibrium model of incompressible three phase flow in porous media the nonequilibrium effects by introducing a pair of effective water and gas saturations into the formulations have the ability to smear saturation fronts from numerical simulations the theory of mixtures offers a general framework for developing models for complex multiphase flow systems rajagopal 2007 more lately biomedical applications have been a driver for the development of various models relying on this approach for example the study how cancer cells are able to break loose from a primary tumor involves a solid matrix the so called extracellular matrix different type of cells cancer cells stromal cells immune cells and interstitial fluid evje 2017 evje and waldeland 2019 a recent example of this is described in waldeland and evje 2018b urdal et al 2019 where respectively a cell fluid two phase model and a cell fibroblast fluid three phase model are developed to shed light on the experimentally observed tumor cell behavior reported in shieh et al 2011 the model that is derived relies on replacing darcy s law by more general momentum balance equations which incorporate both the cell matrix resistance force and the cell fibroblast interaction the latter is understood as a viscous coupling effect caused by a mechanical coupling that can occur between tumor cells and fibroblasts and has been reported in experimental studies labernadie 2017 another example how generalized momentum equations can be used to capture non standard multiphase behavior in the context of aggressive tumor cells is explored in waldeland and evje 2018a in polacheck et al 2011 two competing migration mechanisms were observed one in the upstream direction and another in the downstream direction the use of generalized momentum equations allowed us to account for both this fluid stress generated upstream migration and a chemotactive migration in the direction of increasing concentration of chemical concentrations waldeland and evje 2018a the aim of this work the objective of this paper is to investigate a mixture theory approach to simulate three immiscible fluids flowing in a 1d reservoir we shall consider both the case with compressible and incompressible fluids the model which is introduced is quite general since it can automatically capture flow that involves a combination of co current and counter current flow the current work represents extension of previous work in two ways extend the incompressible two phase model that was explored in qiao et al 2018 andersen et al 2019 to include three phases extend the compressible two phase model studied in qiao et al 2019a to include three phases in addition the models we study in the current work are more general than those studied in qiao et al 2018 andersen et al 2019 since we consider stokes like momentum equations which involve viscous terms that account for internal friction due to viscosity in particular appropriate numerical schemes are introduced to investigate compressible and incompressible three phase flow scenarios that are motivated by injection production flow scenarios main observations from our numerical experiments with two and three phase flow scenarios where the flow dynamics are generated by injection of water or gas in the center of the domain and production of fluids at the left and right boundary are i the simulation cases involve competition between pressure driven co current flow and counter current gravity driven flow ii both the incompressible and compressible discrete version of the model appear to have good stability properties the numerical experiments indicate that the numerical schemes can be useful as a tool to deepen the insight into the relation between the incompressible and compressible version of the model the model and its discrete approximate counterparts appear to be a good starting point for extending to more complex flow systems as mentioned above that involve competition between different distinct non standard transport mechanisms the rest of this paper is organized as follows in section 2 we briefly describe the mixture flux approach in a three phase setting in section 3 we summarize the generalized three phase porous media model both a compressible and an incompressible version of it section 4 is devoted to numerical simulations to demonstrate three phase dynamics and verify basic features of the numerical schemes the details of the compressible and incompressible scheme are given in appendix a appendix d 2 mixture theory framework 2 1 conventional model based on darcy s law we firstly describe the traditional formulation of incompressible multiphase flow model without source terms transport equations for incompressible and immiscible phases oil o water w and gas g in porous media are normally given by 2 1 t ϕ s i u i q i 2 2 u i ϕ s i u i i w o g where ϕ is porosity si is phase saturation qi is the source term and u i and u i are the darcy velocity and interstitial velocity of each phase i o w g respectively for simplicity the irreducible immobile phase saturation sir is not considered in the equations by assuming it is equal to 0 hence the normalized phase saturation s i s i r 1 s w r s o r s g r equals the phase saturation value si the traditional macroscopic formulation of darcy s law that relates the volumetric flux of a phase to the pressure gradient of that phase is given by 2 3 u i k k r i μ i p i ρ i g i w o g where k is the absolute permeability of porous media pi is phase pressure g is the acceleration of gravity and kri ρi and μi are phase relative permeability density and viscosity respectively 2 2 a generalized multiphase flow model based on mixture theory for our investigations the mass balance equations with source terms in the case of compressible water oil gas transport can be given by 2 4 ϕ n w t ϕ n w u w n w q p ρ w q i w n w s w ρ w ϕ n o t ϕ n o u o n o q p n o s o ρ o ϕ n g t ϕ n g u g n g q p ρ g q i g n g s g ρ g where ui i w o g represents the interstitial velocity of phase i in the porous media in addition qp is the production rate and qiw qig represent the injection rate of water and gas respectively the starting point for developing our model that can account for more detailed physical mechanisms for water oil gas porous media flow than conventional modeling is the theory of mixtures this is a theory based on balance laws and conservation principles which is well known in continuum mechanics bowen 1976 rajagopal and tao 1995 byrne and preziosi 2003 ambrosi and preziosi 2002 preziosi and farina 2002 and has been widely applied to the biological tumor growth systems which can be characterized as a mixture of interacting continua neglecting inertial effects acceleration effects as is usual when dealing with creeping flow in porous materials the mechanical stress balance is given by ambrosi and preziosi 2002 2 5 0 s i σ i m i g i i w o g where σi refers to the cauchy stress tensor mi represents the interaction forces exerted on the constituents by other constituents of the mixture and g i s i ρ i g is the external body force due to gravity the standard expression for the stress terms σi is given by 2 6 σ i p i δ τ i i w o g where δ is the unitary tensor and 2 7 τ i 2 μ i e i e i 1 2 u i u i t i w o g the viscous part τi reflects that the water oil and gas behave as a viscous fluid according to general principles of the theory of mixtures the interaction forces mi between the constituents appearing in 2 5 may be described as in preziosi and farina 2002 ambrosi and preziosi 2002 byrne and preziosi 2003 2 8 m o p o s o f w o f o g m o m m w p w s w f w o f w g m w m m g p g s g f w g f o g m g m where fij i j o w g denotes the force drag that the i phase exerts on the j phase the j phase exerts an equal and opposite force f i j similarly mom mwm and mgm represent interaction forces drag forces between fluid and pore walls solid matrix respectively for oil water and gas the term pi si is related to interfacial force exerted by other phases on phase i arising from mathematical derivation of averaged equations drew and segel 1971 to close the system we must specify the drag force term fwo fwg and fog and the stresses σi i o w g and interaction force terms mim between fluid i w o g and matrix drag force represents the interaction between one phase and another phase and is modelled as rajagopal 2007 preziosi and farina 2002 ambrosi and preziosi 2002 2 9 f w o k w o u w u o f w g k w g u w u g f o g k o g u o u g where k i j i j o w g remains to be determined typically k i j s i s j to reflect that this force term will vanish when one of the phases vanishes similarly the interaction force between fluid and pore wall matrix which is stagnant is naturally expressed then as rajagopal and tao 1995 rajagopal 2007 preziosi and farina 2002 ambrosi and preziosi 2002 2 10 m i m k i u i i o w g the coefficients k i j and k i dimension pa s m2 that characterize the magnitude of interaction terms can be chosen such that the model recovers the classical porous media model based on darcy s law at the same time the approach used here will open for development of reservoir models where more detailed physics can be taken into account 3 a summary of the general three fluid model for porous media flow 3 1 the compressible case we are interested in studying a 1 d model for three compressible immiscible fluids moving in a porous media after combining 2 4 2 10 the model takes the following form 3 11 ϕ n w t ϕ n w u w x n w q p ρ w q i w n w s w ρ w ϕ n o t ϕ n o u o x n o q p n o s o ρ o ϕ n g t ϕ n g u g x n g q p ρ g q i g n g s g ρ g s w p w x k w u w k w o u w u o k w g u w u g n w g ε w n w u w x x s o p o x k o u o k w o u o u w k o g u o u g n o g ε o n o u o x x s g p g x k g u g k w g u g u w k o g u g u o n g g ε g n g u g x x δ p o w s w p o p w δ p g o s g p g p o with capillary pressure δpow defined as the pressure difference between the oil and water and capillary pressure δpgo defined as the pressure difference between the gas and oil we may choose to use the following expressions for capillary force 3 12 δ p o w p o p w δ p o w s w p c 1 ln δ 1 s w a 1 and δ 1 a 1 0 δ p g o p g p o δ p g o s g p c 2 s g a 2 and a 2 0 with non negative constants p c i representing interfacial tension this allows us to mimic capillary pressure functions that previsously have been proposed for three phase reservoir flow chen and ewing 1997 odd and david 2010 in addition we have the fundamental relation that the three phases fill the pore space 3 13 s o s w s g 1 the above model must be combined with appropriate closure relations for ρ i ρ i p i we represent the three phases by linear pressure density relations of the form 3 14 ρ w ρ w 0 p w c w ρ o ρ o 0 p o c o ρ g p g c g where cw co and cg represent the inverse of the compressibility of water oil and gas respectively we refer to appendix b for a semi discrete approximation of 3 11 as well as a fully discrete scheme remark 3 1 we may also study a higher dimensional case e g 2d where the model consists of three mass balance equations for three phases water oil and gas and six momentum equations each phase has two directions such as x and y the scheme has been tested in 2d for two phases and shows similar properties as in 1d 3 2 the incompressible case 3 2 1 viscous flow we may let cw co cg go to infinity in 3 14 then we obtain the incompressible version of the model 3 11 we refer to appendix c for a semi discrete as well as a fully discrete scheme for this incompressible case 3 2 2 inviscid flow moreover in order to relate this incompressible version to the classical darcy based formulation we ignore the viscosity terms in the momentum equations by setting ε i 0 i w o g in 3 11 4 5 6 solving momentum equations with respect to interstitial phase velocities ui the darcy velocities of fluid phase are expressed as follows based on 2 2 3 15 u w ϕ s w u w λ w w p w x ρ w g λ w o p o x ρ o g λ w g p g x ρ g g u o ϕ s o u o λ w o p w x ρ w g λ o o p o x ρ o g λ o g p g x ρ g g u g ϕ s g u g λ w g p w x ρ w g λ o g p o x ρ o g λ g g p g x ρ g g and the following relations are defined 3 16 λ w w ϕ s w 2 r r o r g k o g 2 λ w o λ o w ϕ s w s o r k w o r g k o g k w g λ o o ϕ s o 2 r r w r g k w g 2 λ w g λ g w ϕ s w s g r k w g r o k o g k w o λ g g ϕ s g 2 r r w r o k w o 2 λ o g λ g o ϕ s o s g r k o g r w k w g k w o where 3 17 r w k w k w g k w o r o k o k w o k o g r g k g k w g k o g r k w k o k g k w k o k g k w g k w o k o g k w o k w g k o g k g k w o k w k o k w k o g k o k g k o k w g k w k g using capillary pressure relations 3 12 it follows that 3 15 take the following equivalent form 3 18 u w λ w p w x λ w o λ w g δ p o w x λ w g δ p g o x λ w w ρ w λ w o ρ o λ w g ρ g g u o λ o p w x λ o o λ o g δ p o w x λ o g δ p g o x λ w o ρ w λ o o ρ o λ o g ρ g g u g λ g p w x λ g g λ o g δ p o w x λ g g δ p g o x λ w g ρ w λ o g ρ o λ g g ρ g g here we define the following notation for generalized phase mobilities λ i 3 19 λ w λ w w λ w o λ w g λ o λ o o λ w o λ o g λ g λ g g λ w g λ o g by summing uw uo and ug in 3 18 and using the notation introduced in 3 19 the total darcy velocity can be expressed as follows 3 20 u t λ t p w x λ o λ g δ p o w x λ g δ p g o x λ w ρ w λ o ρ o λ g ρ g g where we have used 3 21 λ t λ w λ o λ g therefore the water pressure gradient can be derived from 3 20 3 22 p w x 1 λ t u t f o f g δ p o w x f g δ p g o x f w ρ w f o ρ o f g ρ g g with generalized fractional flow function 3 23 f i λ i λ t i w o g inserting 3 22 into 3 18 we get 3 24 u w f w u t w o w g δ p o w x w g δ p g o x w w ρ w w o ρ o w g ρ g g u o f o u t o o o g δ p o w x o g δ p g o x o w ρ w o o ρ o o g ρ g g u g f g u t g o g g δ p o w x g g δ p g o x g w ρ w g o ρ o g g ρ g g where 3 25 w i λ w f i λ w i o i λ o f i λ o i g i λ g f i λ g i i w o g it should be noted that w i o i g i 0 i w o g in light of 3 16 3 21 and 3 23 4 numerical examples we mainly focus on a reservoir model where there are one injection well at the center and two production wells distributed at two sides the injection rate is equal to the total production rate and the rates of two production wells are also same see fig 1 in addition reservoir inclination θ is also accounted for in the model interaction terms the model 3 11 4 5 6 should be armed with appropriate functional correlations for fluid rock resistance force k w k o k g and fluid fluid drag force k w o k w g k o g here we use the interaction terms suggested in the recent works standnes et al 2017 qiao et al 2018 andersen et al 2019 4 26 k w i w s w α μ w k ϕ k o i o s o β μ o k ϕ k g i g s g γ μ g k ϕ k w o i w o s w s o μ w μ o k ϕ k w g i w g s w s g μ w μ g k ϕ k o g i o g s o s g μ o μ g k ϕ all the interaction terms k i and k i j have dimension pa s m2 the parameters α β and γ are dimensionless exponents whereas iw io and ig are dimensionless friction coefficients characterizing the strength of fluid solid interaction finally iwo iwg and iog are coefficients characterizing the strength of the fluid fluid drag force with dimension pa s 1 input data the input parameters used in the simulations are listed in table 1 we use 101 grid cells for a 100 meter reservoir layer we refer to appendix d for a convergence test the magnitude of the interaction coefficients iwo iwg and iog are chosen as in qiao et al 2018 where we applied a generalized two phase model to match the experimentally measured relative permeability curves and obtained values for the input parameters such as iwo whose magnitude is around several thousands in order to avoid too many complicating effects at the same time in the subsequent discussion we have set the viscosity terms to zero i e ε w ε o ε g 0 we use the similar capillary pressure relations as qiao et al 2019b for water and oil and lewis and pao 2002 for oil and gas see fig 2 the expression of an effective water fractional flow function fw sw so in the conventional water oil gas model assuming no capillary pressure i e δ p o w δ p g o 0 is 4 27 f w s w s o def u w u t λ w λ t u t w w ρ w w o ρ o w g ρ g g sin θ u t where we have used 3 24 and 3 25 where u t 0 x q i q p d x similarly fo and fg can also be expressed in the same manner in order to illustrate the phase flow fraction fw see fig 3 we represent ut by a reference total velocity u t q p 2 q p 2 we refer to table 1 for other input data that are used initial conditions for the waterflooding case we assume the reservoir initially is mostly filled with gas 90 and some oil 10 4 28 s g x t 0 0 9 s o x t 0 0 1 for the wag injection case the reservoir is assumed initially filled with oil 90 and some extra water 10 4 29 s o x t 0 0 9 s w x t 0 0 1 for the compressible case a reference pressure pwl at the left boundary of the layer is given at initial state 4 30 p w l x 0 t 0 10 6 pa boundary conditions we assume a closed boundary for both compressible and incompressible models which means that 4 31 u i x 0 t 0 u i x l t 0 i w o g for the incompressible case we give a reference pressure pwl at the left boundary of the layer 4 32 p w l x 0 t 10 6 pa source terms for wag experiments gas and water are injected at different time periods during the whole oil recovery process we assume that qi x and qp x take the form 4 33 q i w i g x q i w i g σ 1 if x x i σ 2 0 otherwise q p x q p σ 1 if x x p i σ 2 0 otherwise where i 1 2 and q i w i g 0 125 m 3 day and q p 0 0625 m 3 day the width of the small region associated with the injector and producer is σ in the numerical scheme σ δ x 4 1 waterflooding in a gas reservoir we first test the proposed compressible three phase model applied to a gas reservoir development in this example water is injected at 50 m into a gas reservoir layer of length 100 m with a little proportion of oil 10 two cases respectively for the horizontal fig 4 and vertical reservoir fig 5 are shown below the results of the horizontal compressible three phase model with water injection for a total period of 400 days are shown in fig 4 where pressures first column velocities middle column and saturations right column are symmetric with the injection well located at the center of reservoir layer the gas is mostly recovered during the first 130 days see i whereas oil recovery takes place over more than 300 days see f due to its lower mobility than gas it is also observed that at early stage gas pressure along the reservoir layer has less gradient than both the water s and the oil s see first column in fig 4 the injected water displaces both oil and gas in the reservoir near the injection well region where a high pressure gradient is necessary for both water and oil to flow see panel a and b because of their low mobilities after water has arrived the production wells at around 100 days see c water and oil pressures drop owing to the fact that water then can find an easy flow path to the production wells in fig 5 we show the results phase pressures velocities and saturations of a compressible vertical three phase model with a 400 day waterflooding displacement water is injected to displace oil and gas at both sides of the reservoir layer it quickly fills the bottom part then starts accumulating see panel c correspondingly gas is displaced faster in the lower part than in the upper part because the reservoir layer is vertical gravity segregation is seen in the lower part where gas is squeezed upwardly see h and i in contrast to what is shown in fig 4 g gas pressure distribution shows a similar behavior as water and oil higher at bottom and lower at top see first column in fig 5 we refer to the figure text for more details 4 1 1 comparison of the compressible and incompressible models we continue the discussion of the case shown in fig 5 in particular we want to compare the behavior of the compressible and incompressible model constant density values ρ w 1000 kg m 3 ρ o 800 kg m 3 and ρ g 18 kg m3 are used in the incompressible model fig 6 shows a comparison between the compressible and incompressible model after 30 and 120 days a shows that at early stage the injected water in the compressible model prefers to displace gas in the lower part high positive value since water leads to higher pressure at the bottom such that the gas is compressed there with compressed gas produced at the bottom and gas expanding in the upper part gas will only slowly migrate towards the upper part resulting in comparably lower velocity negative in the compressible model the velocity difference shown in d fits well with the saturation difference after 120 days at the early time 30 days the saturation differences are not distinct see c however after a long time 120 days the differences are more significant especially in the water displacing part see f this is due to the increasing phase pressure difference between compressible and incompressible model see b and e the removal of compressed gas from the gas reservoir as almost incompressible water is injected clearly generates additional space for the water to fill which gives rise to a lower pressure 4 2 the compressible three phase model with a wag experiment in wag processes the injected water will migrate towards the bottom of the formation while the injected gas will flow upwardly therefore counter current flow occurs in the vertical direction of the reservoir due to the gravity segregation of water oil and gas significant differences in terms of saturation distribution and producing gor gas oil ratio have been reported between a conventional model and models that better can account for the mix of different flow regimes co current and counter current for example in sherafati and jessen 2017 an explicit representation of flow transitions between co current and counter current flow was used to improve the design of wag injection processes in this part we conduct a water alternating gas wag injection in a 1d reservoir 250 md layer which initially contains 90 oil and 10 water the water and gas injection well is located at 50 m and two production wells are set at 10 m and 90 m gas is injected for the first 10 days followed by the water injection the next 10 days fluids can be produced in both production wells the whole wag experiment continues with an injection circulation of water and gas each for 10 days fig 7 shows the result for a wag injection process produced by the compressible three phase vertical model where gravity segregation has a significant effect from the simulation we see that pressure increases with time first column in fig 7 moreover pressure values at the lower part of the layer are larger than at the upper part due to the density difference water displaces oil faster in the bottom part see b and c in addition gas flows quickly towards the upper part of the reservoir layer see the saturation plots in the upper part oil is recovered faster than in the lower part because of the larger density difference between gas and oil than the one between water and oil see the second column in fig 7 we also observe that gas reaches the bottom production well but does not move further this can be explained by the fact that gravity segregation effect overcomes the capillarity however a lot of gas is accumulated in the upper edge region 0 m 10 m due to the buoyancy force see i 4 3 comparison of compressible and incompressible three phase models with wag experiments in this part we compute solutions from incompressible three phase models with same wag injection process and compare the relevant results with those from the compressible three phase model constant density values ρ w 1000 kg m 3 ρ o 800 kg m 3 and ρ g 18 kg m3 are used in the incompressible model fig 8 shows a comparison between the compressible and incompressible model of the vertical three phase reservoir with a wag process similar to what was observed in fig 6 differences are seen for phase velocity pressure and saturation with increasing time this difference will be enhanced especially for the pressure this is mainly due to the gas compressibility see b and e and the figure text for more explanation because of the density difference water prefers to flow towards the bottom of the layer whereas gas moves faster towards the upper part of layer see c and f 4 3 1 effect of fluid fluid interactions here we want to illustrate the impact from fluid fluid interaction terms on the compressible model with a wag process two situations are compared below one with i w o i w g i o g 0 p a s 1 and one with i w o i w g i o g 5000 p a s 1 fig 9 compares the results for the horizontal model for a wag process with and without fluid fluid interaction effect at 60 and 120 days in b and e we observe that due to the fluid fluid interaction pressure is elevated compared with the case with no fluid fluid interaction the water velocity a and saturation profiles c show that water to a less extent displaces oil and instead flows through the original water channels when fluid fluid interaction is included the difference in the water saturation profiles between c and f is enhanced with time due to the additional resistance force from the fluid fluid interaction term fig 10 compares the results for the vertical model for a wag process with and without fluid fluid interaction effect at 60 and 120 days due to the density difference a large proportion of gas flows to the upper part of layer see c and f and more of the water flows towards the bottom part of layer as a result differences are seen for the water velocity a d and saturation c f for the case with and without fluid fluid interaction similar to fig 9 the build up of the water front is less efficient for the case with fluid fluid interaction since a larger portion of water tends to move through the original water channels a 5 concluding remarks we have presented a three phase compressible and incompressible viscous model based on the mixture theory approach the formulation represents an extension of the conventional darcy type formulations by including fluid fluid viscous coupling effects the three phase flow model consists of a set of mass balance equations which are coupled to a set of momentum balance equations that involve both fluid matrix fluid fluid interactions and internal viscosity effects numerical schemes have been developed for both the compressible and incompressible model moreover various waterflooding displacement scenarios in a gas reservoir and wag injection in an oil reservoir have been investigated to illustrate the effects of fluid compressibility and fluid fluid viscous coupling main findings are i the numerical schemes proposed in this paper appear to be robust and stable for simulation of various three phase flow scenarios both for the incompressible and compressible case ii comparison of the results for the compressible and incompressible model show that the differences between these two models can be significant especially in the vertical case where the effect of gravity segregation is rather strong see f in fig 6 iii the viscous coupling fluid fluid interaction can have a significant effect on the results a strong fluid fluid viscous coupling results in a large resistance force for the flow of displacing fluid water such that a water prefers to move through the original water channels rather than displacing oil see a and c in figs 9 and 10 b water front is slow and builds up see f in figs 9 and 10 credit authorship contribution statement yangyang qiao methodology writing review editing steinar evje methodology writing original draft declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a from the three mass balance equations we get after multiplying the oil mass balance with ρwρg the water mass balance with ρoρg and the c mass balance with ρwρo 5 34 s w t ρ w s w ρ w t ρ o ρ g ρ o ρ g s w ρ w u w x s w ρ w ρ o ρ g q p ϕ ρ w ρ o ρ g q i w ϕ s o t ρ o s o ρ o t ρ w ρ g ρ w ρ g s o ρ o u o x s o ρ w ρ o ρ g q p ϕ s g t ρ g s g ρ g t ρ w ρ o ρ w ρ o s g ρ g u g x s g ρ w ρ o ρ g q p ϕ ρ w ρ o ρ g q i g ϕ and summing the three resulting equations 5 35 f 1 f 2 f 3 f 4 where f 1 s w t ρ w ρ o ρ g s o t ρ w ρ o ρ g s g t ρ w ρ o ρ g ρ w ρ o ρ g s w t s o t s g t 0 f 2 s w ρ w t ρ o ρ g s o ρ o t ρ w ρ g s g ρ g t ρ w ρ o f 3 ρ o ρ g s w ρ w u w x ρ w ρ g s o ρ o u o x ρ w ρ o s g ρ g u g x f 4 s w ρ w ρ o ρ g q p ϕ ρ w ρ o ρ g q i w ϕ s o ρ w ρ o ρ g q p ϕ s g ρ w ρ o ρ g q p ϕ ρ w ρ o ρ g q i g ϕ ρ w ρ o ρ g q i w q i g q p ϕ here we want to focus on dealing with expression f 2 5 36 f 2 s w ρ o ρ g p w t c w s o ρ w ρ g p o t c o s g ρ w ρ o p g t c g 5 37 p o t p w δ p o w t p w t δ p o w s w t p g t p w δ p o w δ p g o t p w t δ p o w s w t δ p g o s g t a n d s g t s w t s o t s w t n o t ρ o n o c o ρ o 2 p o t s w t n o t ρ o n o c o ρ o 2 p w t δ p o w s w t therefore we have 5 38 f 2 κ ρ w s w ρ o ρ g c w p w t κ ρ w δ p o w s w t s g ρ w ρ o c g δ p g o s w t s g ρ w c g n o t where 5 39 κ s o ρ g c o s g ρ o c g s g s o c o c g clearly s w t n w ρ w t 1 ρ w n w t m ρ w 2 ρ w t 1 ρ w n w t m c w ρ w 2 p w t consequently 5 40 f 2 κ ρ w s w ρ o ρ g c w s w c w κ δ p o w s g ρ o c g δ p g o p w t κ δ p o w s g ρ o c g δ p g o s w ρ w u w x s g ρ w c g s o ρ o u o x s g ρ w c g s o ρ o q p ρ w s w q p ρ w q i w κ δ p o w s g ρ o c g δ p g o since that f 1 0 5 35 will have the following form 5 41 κ ρ w s w ρ o ρ g c w s w c w κ δ p o w s g ρ o c g δ p g o p w t ρ o ρ g s g ρ o c g δ p g o κ δ p o w s w ρ w u w x ρ w ρ g s g ρ w c g s o ρ o u o x ρ w ρ o s g ρ g u g x ρ w ρ o ρ g q i w q i g q p ϕ s g ρ w c g s o ρ o q p ϕ ρ w s w q p ϕ ρ w q i w ϕ κ δ p o w s g ρ o c g δ p g o the upper equation can be reformulated to be 5 42 p w t η 1 n w u w x η 2 n o u o x η 3 n g u g x η 4 q p ϕ η 5 q i w ϕ η 6 q i g ϕ where 5 43 η κ ρ w s w ρ o ρ g c w s w c w κ δ p o w s g ρ o c g δ p g o η 1 1 η ρ o ρ g s g ρ o c g δ p g o κ δ p o w η 2 1 η ρ w ρ g s g ρ w c g η 3 1 η ρ w ρ o η 4 1 η s g ρ w c g s o ρ o ρ w s w κ δ p o w s g ρ o c g δ p g o ρ w ρ o ρ g η 5 1 η ρ w ρ o ρ g ρ w κ δ p o w s g ρ o c g δ p g o η 6 1 η ρ w ρ o ρ g remark 5 1 δ p o w is always non positive and δ p g o non negative appendix b numerical discretization of compressible version we develop a numerical scheme for this general three fluid flow model in a 1d setting the proposed numerical methods are described separately for the compressible appendix b and dummytxdummy incompressible appendix c model b1 a semi discrete scheme for the compressible model we consider a slight reformulation of the model where we shall make use of the pressure evolution eq 5 42 this will be convenient to account for the highly nonlinear coupling between the mass and momentum equations through the pressure terms it also makes the discretization of the compressible and incompressible model consistent the original model takes the form with nw no ng uw uo ug as the main variables 5 44 ϕ n w t ϕ n w u w x n w q p ρ w q i w n w s w ρ w ϕ n o t ϕ n o u o x n o q p n o s o ρ o ϕ n g t ϕ n g u g x n g q p ρ g q i g n g s g ρ g s w p w x k w u w k w o u w u o k w g u w u g n w g ε w n w u w x x s o p o x k o u o k w o u o u w k o g u o u g n o g ε o n o u o x x s g p g x k g u g k w g u g u w k o g u g u o n g g ε g n g u g x x δ p o w s w p o p w δ p g o s g p g p o note that we may rewrite the model in the following equivalent form with nw no pw uw uo ug as the main variables 5 45 ϕ n w t ϕ n w u w x n w q p ρ w q i w ϕ n o t ϕ n o u o x n o q p p w t η 1 n w u w x η 2 n o u o x η 3 n g u g x η 4 q p ϕ η 5 q i w ϕ η 6 q i g ϕ s w p w x k w u w k w o u w u o k w g u w u g n w g ε w n w u w x x s o p w δ p o w x k o u o k w o u o u w k o g u o u g n o g ε o n o u o x x s g p w δ p o w δ p g o x k g u g k w g u g u w k o g u g u o n g g ε g n g u g x x δ p o w s w p o p w δ p g o s g p g p o here ng is determined by 5 46 n g s g ρ g p g 1 s w s o ρ g p g 1 n w ρ w p w n o ρ o p o ρ g p g n g n w n o p w where p o p o s w p w p o n w p w and p g p g s w s o p w p g n w n o p w we may solve 5 45 on our domain ω with boundary conditions 5 47 u w ω u o ω u g ω 0 and initial condition 5 48 n w x t 0 n w 0 x n o x t 0 n o 0 x n g x t 0 n g 0 x p w x 0 t 0 p w l system of odes we consider the domain ω 0 1 and introduce a grid of nx cells with nodes xj placed at the center of the cells x 1 1 2 δ x x 2 1 1 2 δ x x j j 1 2 δ x x n x n x 1 2 δ x and cell interfaces x j 1 2 at the cell interfaces x 1 2 0 x 3 2 δ x x j 1 2 j δ x x n x 1 2 n x δ x 1 where δ x 1 n x we introduce the approximate masses n w j t j 1 n x n o j t j 1 n x and n g j t j 1 n x associated with the nodes x j j 1 n x whereas the approximate velocities u w j 1 2 j 0 n x u o j 1 2 j 0 n x and u g j 1 2 j 0 n x are associated with the cell interfaces x j 1 2 j 0 n x step 1 mass transport we solve for nwj t by considering the following ode for the water phase 5 49 n w j 1 δ x n w u w j 1 2 n w u w j 1 2 n w j q p j ϕ ρ w j q i w j ϕ n w s w ρ w where 5 50 n w u w j 1 2 n w j u w j 1 2 if u w j 1 2 0 n w j 1 u w j 1 2 if u w j 1 2 0 this can also be expressed as n w u w j 1 2 n w j n w j 1 2 u w j 1 2 1 2 n w j 1 n w j u w j 1 2 for the oil phase 5 51 n o j 1 δ x n o u o j 1 2 n o u o j 1 2 n o j q p j ϕ n o s o ρ o where 5 52 remark 5 2 it should be pointed out that q p j q p σ and q i w j q i w σ where j refers to a grid cell which contains a producer injector due to the fact that production qp or injection qiw in 5 44 is interpreted as a value at a point location the width of the small region associated with the injector and producer is σ δ x consistent with 4 33 this also applies for q ig j in 5 53 that is to say q i g j q i g σ step 2 computation of velocities and pressure next we solve for p w j t and u w j 1 2 t u o j 1 2 t and u g j 1 2 t by considering the following ode system 5 53 p w j η 1 j 1 δ x n w u w j 1 2 n w u w j 1 2 η 2 j 1 δ x n o u o j 1 2 n o u o j 1 2 η 3 j 1 δ x n g u g j 1 2 n g u g j 1 2 η 4 j q p j ϕ η 5 j q i w j ϕ η 6 j q i g j ϕ which is combined with the momentum balance equations 5 54 s w j 1 2 1 δ x p w j 1 p w j k w j 1 2 u w j 1 2 k w o j 1 2 u w j 1 2 u o j 1 2 k w g j 1 2 u w j 1 2 u g j 1 2 g n w j 1 2 ε w 1 δ x 2 n w j 1 u w j 3 2 u w j 1 2 n w j u w j 1 2 u w j 1 2 s o j 1 2 1 δ x p w j 1 p w j s o j 1 2 1 δ x δ p o w j 1 δ p o w j k o j 1 2 u o j 1 2 k w o j 1 2 u o j 1 2 u w j 1 2 k o g j 1 2 u o j 1 2 u g j 1 2 g n o j 1 2 ε o 1 δ x 2 n o j 1 u o j 3 2 u o j 1 2 n o j u o j 1 2 u o j 1 2 s g j 1 2 1 δ x p w j 1 p w j s g j 1 2 1 δ x δ p o w j 1 δ p o w j δ p g o j 1 δ p g o j k g j 1 2 u g j 1 2 k w g j 1 2 u g j 1 2 u w j 1 2 k o g j 1 2 u g j 1 2 u o j 1 2 g c g j 1 2 ε g 1 δ x 2 c g j 1 u g j 3 2 u g j 1 2 n g j u g j 1 2 u g j 1 2 here we note that the average s w j 1 2 in 5 54 is based on upwind relatively u w j 1 2 5 55 s w j 1 2 s w j if u w j 1 2 0 s w j s w j 1 2 if u w j 1 2 0 s w j 1 if u w j 1 2 0 similarly for s o j 1 2 s g j 1 2 and for the interaction terms k w j 1 2 k o j 1 2 and k g j 1 2 for k w o j 1 2 k o g j 1 2 and k w g j 1 2 we use the following method 5 56 k w o j 1 2 k w o j if u w j 1 2 0 u o j 1 2 0 k w o j k w o j 1 2 if u w j 1 2 u o j 1 2 0 k w o j 1 if u w j 1 2 0 u o j 1 2 0 k w g j 1 2 and k o g j 1 2 are also approximated using the similar way on the other hand n w u w j 1 2 n o u o j 1 2 and n g u g j 1 2 appearing in 5 53 employ upwind as described in 5 50 now we are in a position where we can describe a fully discrete model b2 a fully discrete scheme we assume that we have given n w j k n o j k p w j k u w j k u o j k u g j k we then compute the approximate solution at time t k 1 expressed by n w j k 1 n o j k 1 p w j k 1 u w j k 1 u o j k 1 u g j k 1 as follows step 1 mass transport 5 57 n w j k 1 n w j k δ t 1 δ x n w u w j 1 2 k n w u w j 1 2 k n w j k q p j k ϕ ρ w j k q i w j k ϕ where 5 58 n w u w j 1 2 k n w j k u w j 1 2 k if u w j 1 2 k 0 n w j 1 k u w j 1 2 k if u w j 1 2 k 0 5 59 n o j k 1 n o j k δ t 1 δ x n o u o j 1 2 k n o u o j 1 2 k n o j k q p j k ϕ where 5 60 n o u o j 1 2 k n o j k u o j 1 2 k if u w j 1 2 k 0 n o j 1 k u o j 1 2 k if u w j 1 2 k 0 having computed n w j k 1 and n o j k 1 we can compute an updated water saturation s w j k 1 2 and s o j k 1 2 given by 5 61 s w j k 1 2 n w j k 1 ρ w p w j k s o j k 1 2 n o j k 1 ρ o p o j k 1 2 n o j k 1 ρ o p w j k δ p o w s w j k 1 2 similarly we compute updated mass n g j k 1 2 and p g j k 1 2 needed to evaluate coefficients in the next step step 2 computation of velocities and pressure next we solve simultaneously for p w j k 1 and u w j 1 2 k 1 u o j 1 2 k 1 and u g j 1 2 k 1 by considering the following algebraic system 5 62 p w j k 1 p w j k δ t η 1 j k 1 2 1 δ x n w k 1 u w k 1 j 1 2 n w k 1 u w k 1 j 1 2 η 2 j k 1 2 1 δ x n o k 1 u o k 1 j 1 2 n o k 1 u o k 1 j 1 2 η 3 j k 1 2 1 δ x n g k 1 2 u g k 1 j 1 2 n g k 1 2 u g k 1 j 1 2 η 4 j k 1 2 q p j k ϕ η 5 j k 1 2 q i w j k ϕ η 6 j k 1 2 q i g j k ϕ which is combined with the momentum balance equations 5 63 s w j 1 2 k 1 2 1 δ x p w j 1 k 1 p w j k 1 k w j 1 2 k 1 2 u w j 1 2 k 1 k w o j 1 2 k 1 2 u w j 1 2 k 1 2 u o j 1 2 k 1 2 k w g j 1 2 k 1 2 u w j 1 2 k 1 2 u g j 1 2 k 1 2 n w j 1 2 k 1 g ε w 1 δ x 2 n w j 1 k 1 u w j 3 2 k 1 u w j 1 2 k 1 n w j k 1 u w j 1 2 k 1 u w j 1 2 k 1 s o j 1 2 k 1 2 1 δ x p w j 1 k 1 p w j k 1 s o j 1 2 k 1 2 1 δ x δ p o w j 1 k 1 2 δ p o w j k 1 2 k o j 1 2 k 1 2 u o j 1 2 k 1 k w o j 1 2 k 1 2 u o j 1 2 k 1 u w j 1 2 k 1 k o g j 1 2 k 1 2 u o j 1 2 k 1 u g j 1 2 k 1 n o j 1 2 k 1 g ε o 1 δ x 2 n o j 1 k 1 2 u o j 3 2 k 1 u o j 1 2 k 1 n o j k 1 2 u o j 1 2 k 1 u o j 1 2 k 1 s g j 1 2 k 1 2 1 δ x p w j 1 k 1 p w j k 1 s g j 1 2 k 1 2 1 δ x δ p o w j 1 k 1 2 δ p o w j k 1 2 δ p g o j 1 k 1 2 δ p g o j k 1 2 k g j 1 2 k 1 2 u g j 1 2 k 1 k w g j 1 2 k 1 2 u g j 1 2 k 1 u w j 1 2 k 1 k o g j 1 2 k 1 2 u g j 1 2 k 1 u o j 1 2 k 1 n g j 1 2 k 1 2 g ε g 1 δ x 2 n g j 1 k 1 2 u g j 3 2 k 1 u g j 1 2 k 1 n g j k 1 2 u g j 1 2 k 1 u g j 1 2 k 1 equipped with p w j k 1 u w j 1 2 k 1 u o j 1 2 k 1 u g j 1 2 k 1 we can now update the saturation 5 64 s w j k 1 n w j k 1 ρ w p w j k 1 s o j k 1 n o j k 1 ρ o p o j k 1 n o j k 1 ρ o p w j k 1 δ p o w j k 1 s w j k 1 from which we also compute the updated gas mass n g j k 1 via 5 46 if necessary we may repeat step 2 to improve the accuracy before we proceed to next time level remark 5 3 the upwind discretization of n w k 1 u w k 1 j 1 2 n o k 1 2 u o k 1 j 1 2 and n g k 1 2 u g k 1 j 1 2 appearing in 5 62 are based on old velocities u w j 1 2 k u o j 1 2 k and u g j 1 2 k remark 5 4 for the higher dimensional case mentioned in remark 3 1 we can use a similar way to solve two mass equations as step 1 for the 1d case then compute pressure and velocities by using a 2d pressure evolution equation similar to the one derived for the 1d case in appendix a together with six momentum equations for three phases in x and y direction appendix c numerical discretization of incompressible version we first describe a semi discrete approximation of the incompressible version of model 3 11 c1 a semidiscrete scheme for the incompressible model when fluids are incompressible the model 5 45 takes the form 5 65 s w t s w u w x s w q p ϕ q i w ϕ s o t s o u o x s o q p ϕ s w u w s o u o s g u g x q p ϕ q i w ϕ q i g ϕ s w p w x k w u w k w o u w u o k w g u w u g n w g ε w ρ w s w u w x x s o p o x k o u o k w o u o u w k o g u o u g n o g ε o ρ o s o u o x x s g p g x k g u g k w g u g u w k o g u g u o n g g ε g ρ g s g u g x x δ p o w s w p o p w δ p g o s g p g p o step 1 mass transport 5 66 s w j 1 δ x s w u w j 1 2 s w u w j 1 2 s w j q p j ϕ q i w j ϕ where 5 67 s w u w j 1 2 s w j u w j 1 2 if u w j 1 2 0 s w j 1 u w j 1 2 if u w j 1 2 0 5 68 s o j 1 δ x s o u o j 1 2 s o u o j 1 2 s o j q p j ϕ where 5 69 s o u o j 1 2 s o j u o j 1 2 if u o j 1 2 0 s o j 1 u o j 1 2 if u o j 1 2 0 step 2 computation of velocities and pressure next we solve for p w j t and u w j 1 2 t u o j 1 2 t and u g j 1 2 t by considering the following ode system 5 70 1 δ x s w u w j 1 2 s w u w j 1 2 1 δ x s o u o j 1 2 s o u o j 1 2 1 δ x s g u g j 1 2 s g u g j 1 2 q i w j ϕ q i g j ϕ q p j ϕ which is combined with the momentum balance equations 5 71 s w j 1 2 1 δ x p w j 1 p w j k w j 1 2 u w j 1 2 k w o j 1 2 u w j 1 2 u o j 1 2 k w g j 1 2 u w j 1 2 u g j 1 2 g s w j 1 2 ρ w ε w ρ w δ x 2 s w j 1 u w j 3 2 u w j 1 2 s w j u w j 1 2 u w j 1 2 s o j 1 2 1 δ x p w j 1 p w j s o j 1 2 1 δ x δ p o w j 1 δ p o w j k o j 1 2 u o j 1 2 k w o j 1 2 u o j 1 2 u w j 1 2 k o g j 1 2 u o j 1 2 u g j 1 2 g s o j 1 2 ρ o ε o ρ o δ x 2 s o j 1 u o j 3 2 u o j 1 2 s o j u o j 1 2 u o j 1 2 s g j 1 2 1 δ x p w j 1 p w j s g j 1 2 1 δ x δ p o w j 1 δ p o w j δ p g o j 1 δ p g o j k g j 1 2 u g j 1 2 k w g j 1 2 u g j 1 2 u w j 1 2 k o g j 1 2 u g j 1 2 u o j 1 2 g s g j 1 2 ρ g ε g ρ g δ x 2 s g j 1 u g j 3 2 u g j 1 2 s g j u g j 1 2 u g j 1 2 here we note that the average s w j 1 2 in 5 71 is based on upwind relatively u w j 1 2 5 72 s w j 1 2 s w j if u w j 1 2 0 s w j s w j 1 2 if u w j 1 2 0 s w j 1 if u w j 1 2 0 similarly for s o j 1 2 s g j 1 2 and for the interaction terms k w j 1 2 k o j 1 2 and k g j 1 2 in addition k w o j 1 2 is based on upwind relatively u w j 1 2 and u o j 1 2 5 73 k w o j 1 2 k w o j if u w j 1 2 0 u o j 1 2 0 k w o j k w o j 1 2 if u w j 1 2 u o j 1 2 0 k w o j 1 if u w j 1 2 0 u o j 1 2 0 k w g j 1 2 and k o g j 1 2 are also approximated using the similar way on the other hand s w u w j 1 2 s o u o j 1 2 and s g u g j 1 2 appearing in 5 70 employ upwind as described in 5 72 c2 a fully discrete scheme for the incompressible model step 1 mass transport 5 74 s w j k 1 s w j k δ t 1 δ x s w u w j 1 2 k s w u w j 1 2 k s w j k q p j k ϕ q i w j k ϕ where 5 75 s w u w j 1 2 k s w j k u w j 1 2 k if u w j 1 2 k 0 s w j 1 k u w j 1 2 k if u w j 1 2 k 0 5 76 s o j k 1 s o j k δ t 1 δ x s o u o j 1 2 k s o u o j 1 2 k s o j k q p j k ϕ where 5 77 s o u o j 1 2 k s o j k u o j 1 2 k if u o j 1 2 k 0 s o j 1 k u o j 1 2 k if u o j 1 2 k 0 having computed s w j k 1 and s o j k 1 we can compute pressure and velocities simultaneously at time level k 1 step 2 computation of velocities and pressure we solve for p w j k 1 and u w j 1 2 k 1 u o j 1 2 k 1 and u g j 1 2 k 1 by considering the following algebraic system 5 78 1 δ x s w k 1 u w k 1 j 1 2 s w k 1 u w k 1 j 1 2 1 δ x s o k 1 u o k 1 j 1 2 s o k 1 u o k 1 j 1 2 1 δ x s g k 1 u g k 1 j 1 2 s g k 1 u g k 1 j 1 2 q i j k ϕ q p j k ϕ which is combined with the momentum balance equations 5 79 s w j 1 2 k 1 2 1 δ x p w j 1 k 1 p w j k 1 k w j 1 2 k 1 2 u w j 1 2 k 1 k w o j 1 2 k 1 2 u w j 1 2 k 1 2 u o j 1 2 k 1 2 k w g j 1 2 k 1 2 u w j 1 2 k 1 2 u g j 1 2 k 1 2 s w j 1 2 k 1 ρ w g ε w ρ w δ x 2 s w j 1 k 1 u w j 3 2 k 1 u w j 1 2 k 1 s w j k 1 u w j 1 2 k 1 u w j 1 2 k 1 s o j 1 2 k 1 2 1 δ x p w j 1 k 1 p w j k 1 s o j 1 2 k 1 2 1 δ x δ p o w j 1 k 1 2 δ p o w j k 1 2 k o j 1 2 k 1 2 u o j 1 2 k 1 k w o j 1 2 k 1 2 u o j 1 2 k 1 u w j 1 2 k 1 k o g j 1 2 k 1 2 u o j 1 2 k 1 u g j 1 2 k 1 s o j 1 2 k 1 ρ o g ε o ρ o δ x 2 s o j 1 k 1 2 u o j 3 2 k 1 u o j 1 2 k 1 s o j k 1 2 u o j 1 2 k 1 u o j 1 2 k 1 s g j 1 2 k 1 2 1 δ x p w j 1 k 1 p w j k 1 s g j 1 2 k 1 2 1 δ x δ p o w j 1 k 1 2 δ p o w j k 1 2 δ p g o j 1 k 1 2 δ p g o j k 1 2 k g j 1 2 k 1 2 u g j 1 2 k 1 k w g j 1 2 k 1 2 u g j 1 2 k 1 u w j 1 2 k 1 k o g j 1 2 k 1 2 u g j 1 2 k 1 u o j 1 2 k 1 s g j 1 2 k 1 2 ρ g g ε g ρ g δ x 2 s g j 1 k 1 2 u g j 3 2 k 1 u g j 1 2 k 1 s g j k 1 2 u g j 1 2 k 1 u g j 1 2 k 1 remark 5 5 the upwind discretization of s w k 1 u w k 1 j 1 2 s o k 1 u o k 1 j 1 2 and s g k 1 u g k 1 j 1 2 appearing in 5 78 are based on old velocities u w j 1 2 k u o j 1 2 k and u g j 1 2 k appendix d the convergence of numerical scheme here we illustrate one example to show the convergence of the presented numerical scheme in appendix c for the incompressible three phase flow a sensitivity test is conducted by using different number of grid cells to compare the water saturation profiles for the same case as shown in fig 6 panel f we refer to fig 11 for an illustration of the convergence test 
477,in this paper we focus on a general model to describe compressible and immiscible three phase flow in porous media the underlying idea is to replace darcy s law by more general momentum balance equations in particular we want to account for viscous coupling effects by introducing fluid fluid interaction terms in qiao et al 2018 adv water resour 112 170 188 a water oil model based on the theory of mixtures was explored it was demonstrated how the inclusion of viscous coupling effects could allow the model to better capture flow regimes which involve a combination of co current and counter current flow in this work we extend the model in different aspects i account for three phases water oil gas instead of two ii deal with both the compressible and incompressible case iii include viscous terms that represent frictional forces within the fluid brinkman type a main objective of this work is to explore this three phase model which appears to be more realistic than standard formulation in the context of petroleum related applications we first provide development of stable numerical schemes in a one dimensional setting which can be used to explore the generalized water oil gas model both for the compressible and incompressible case then several numerical examples with waterflooding in a gas reservoir and water alternating gas wag experiments in an oil reservoir are investigated differences and similarities between the compressible and incompressible model are highlighted and the fluid fluid interaction effect is illustrated by comparison of results from the generalized model and a conventional model formulation keywords multiphase flow in porous media three phase flow viscous coupling mixture theory compressible model water alternating gas wag waterflooding 1 introduction generally the processes of multiphase flow in porous media occur in many subsurface systems and have found many applications of practical interest such as hydrology petroleum engineering geothermal energy development and carbon storage bakhshian et al 2019 bakhshian and hosseini 2019 wu 2016 the immiscible three phase flow is always encountered in waterflooding for oil reservoirs with gas cap in immiscible co2 storage in depleted oil and gas reservoirs and steam floods and water alternating gas wag processes bentsen and trivedi 2012 juanes 2008 darcy s law was originally developed for single phase flow darcy 1856 conventional modeling of multiphase flow is normally based on darcy s extended law rose 2000 by incorporation of relative permeabilities muskat et al 1937 however recent experimental observations indicate that the flow mode co current or counter current can have a strong impact on the flowing phase mobilities that is to say the relative permeabilities are not only function of saturation but are also related to the effect of how the fluids flow relatively to each other bentsen and manai 1992 bourbiaux and kalaydjian 1990 viscous coupling viscous coupling i e fluid fluid interaction was firstly mentioned by yuster 1951 by using theoretical analysis to derive that relative permeability is a function of both saturation and viscosity ratio in addition capillary number was also proposed to be a factor affecting relative permeabilities ehrlich 1993 avraam and payatakes 1995 in general momentum transfer due to differences in interstitial velocities induces acceleration of the slower and deceleration of the faster moving fluid when the fluids are moving co currently deceleration of both fluid velocities will occur if they are moving counter currently ayodele 2006 bentsen and manai 1993 dullien and dong 1996 li et al 2004 in order to extend the single phase darcy s law to multiphase flow de la cruz and spanos 1983 derived theoretically darcy s empirical extended law by applying the method of volume averaging to stokes equation in kalaydjian 1987 1990 kalaydjian developed flow equations using the concepts of irreversible thermodynamics katchalsky and curran 1975 from a macroscopic understanding of two phase flow in porous media in addition some researchers tried to gain insight into how two immiscible phases flow through a porous medium by using simple analogous models such as tubular flow yuster 1951 bacri et al 1990 in langaas and papatzacos 2001 langaas and papatzacos used the lattice boltzmann lb approach to investigate effects of viscous coupling and concluded that counter current relative permeabilities caused partly by viscous coupling are always less than the corresponding co current curves under different levels of capillary forces using the same method li et al 2005 showed that their model was able to capture main experimental effects caused by viscous coupling they also mentioned that the interfacial area between the fluids is a key variable for relative permeability functions for two immiscible fluids flow in porous media a generalized model was developed in qiao et al 2018 for two phase flow with viscous coupling effect numerical investigations showed a better agreement with the experimental tests bourbiaux and kalaydjian 1990 compared to the conventional modeling the authors in bentsen and trivedi 2012 constructed modified transport equations for both co current and counter current three phase flow through a vertical incompressible model based on partition concepts their equations are used to estimate the amount of model error because of a failure to account for the effect of interfacial coupling which has two types viscous coupling and capillary coupling moreover sherafati and jessen 2017 investigated the effect of mobility changes due to flow reversals from co current to counter current flow on the displacement of wag injection processes complex multiphase flow in porous media and use of the theory of mixtures motivated by petroleum related applications various attempts to solve the three phase porous media flow model have been reported during the past decade falls and schulte 1992 guzmán and fayers 1997a 1997b juanes and patzek 2004 an interesting investigation was carried out in lie and juanes 2005 where a front tracking algorithm was proposed for constructing very accurate solutions to one dimensional problems for example wag test therein this was explored in the context of streamline simulation which decouples the three dimensional problem into a set of one dimensional problems along streamlines this work is limited to three phase immiscible incompressible flow and also gravity and capillarity were ignored different numerical methods have been implemented to simulate three phase flow in porous media a finite volume method was used in lee et al 2008 for solving compressible immiscible flow with gravity in heterogeneous formations by using the black oil formulation a hybrid upwinding scheme for phase flux was proposed in lee and efendiev 2016 for a finite difference approximation to solve the three phase transport equations in the presence of viscous and buoyancy forces a finite element method was applied to simulate fluid injection and imbibition processes in a deformable porous media gajo et al 2017 moreover dong and rivière 2016 applied a semi implicit method with discontinuous galerkin dg discretization to solve the incompressible three phase flow in two dimensions additional physical effects are also discussed and explored for three phase porous media flow such as hysteresis effects of relative permeabilities ranaee et al 2019 and elliptic regions juanes and patzek 2004 juanes 2008 lee and efendiev 2016 in juanes 2008 juanes presented a nonequilibrium model of incompressible three phase flow in porous media the nonequilibrium effects by introducing a pair of effective water and gas saturations into the formulations have the ability to smear saturation fronts from numerical simulations the theory of mixtures offers a general framework for developing models for complex multiphase flow systems rajagopal 2007 more lately biomedical applications have been a driver for the development of various models relying on this approach for example the study how cancer cells are able to break loose from a primary tumor involves a solid matrix the so called extracellular matrix different type of cells cancer cells stromal cells immune cells and interstitial fluid evje 2017 evje and waldeland 2019 a recent example of this is described in waldeland and evje 2018b urdal et al 2019 where respectively a cell fluid two phase model and a cell fibroblast fluid three phase model are developed to shed light on the experimentally observed tumor cell behavior reported in shieh et al 2011 the model that is derived relies on replacing darcy s law by more general momentum balance equations which incorporate both the cell matrix resistance force and the cell fibroblast interaction the latter is understood as a viscous coupling effect caused by a mechanical coupling that can occur between tumor cells and fibroblasts and has been reported in experimental studies labernadie 2017 another example how generalized momentum equations can be used to capture non standard multiphase behavior in the context of aggressive tumor cells is explored in waldeland and evje 2018a in polacheck et al 2011 two competing migration mechanisms were observed one in the upstream direction and another in the downstream direction the use of generalized momentum equations allowed us to account for both this fluid stress generated upstream migration and a chemotactive migration in the direction of increasing concentration of chemical concentrations waldeland and evje 2018a the aim of this work the objective of this paper is to investigate a mixture theory approach to simulate three immiscible fluids flowing in a 1d reservoir we shall consider both the case with compressible and incompressible fluids the model which is introduced is quite general since it can automatically capture flow that involves a combination of co current and counter current flow the current work represents extension of previous work in two ways extend the incompressible two phase model that was explored in qiao et al 2018 andersen et al 2019 to include three phases extend the compressible two phase model studied in qiao et al 2019a to include three phases in addition the models we study in the current work are more general than those studied in qiao et al 2018 andersen et al 2019 since we consider stokes like momentum equations which involve viscous terms that account for internal friction due to viscosity in particular appropriate numerical schemes are introduced to investigate compressible and incompressible three phase flow scenarios that are motivated by injection production flow scenarios main observations from our numerical experiments with two and three phase flow scenarios where the flow dynamics are generated by injection of water or gas in the center of the domain and production of fluids at the left and right boundary are i the simulation cases involve competition between pressure driven co current flow and counter current gravity driven flow ii both the incompressible and compressible discrete version of the model appear to have good stability properties the numerical experiments indicate that the numerical schemes can be useful as a tool to deepen the insight into the relation between the incompressible and compressible version of the model the model and its discrete approximate counterparts appear to be a good starting point for extending to more complex flow systems as mentioned above that involve competition between different distinct non standard transport mechanisms the rest of this paper is organized as follows in section 2 we briefly describe the mixture flux approach in a three phase setting in section 3 we summarize the generalized three phase porous media model both a compressible and an incompressible version of it section 4 is devoted to numerical simulations to demonstrate three phase dynamics and verify basic features of the numerical schemes the details of the compressible and incompressible scheme are given in appendix a appendix d 2 mixture theory framework 2 1 conventional model based on darcy s law we firstly describe the traditional formulation of incompressible multiphase flow model without source terms transport equations for incompressible and immiscible phases oil o water w and gas g in porous media are normally given by 2 1 t ϕ s i u i q i 2 2 u i ϕ s i u i i w o g where ϕ is porosity si is phase saturation qi is the source term and u i and u i are the darcy velocity and interstitial velocity of each phase i o w g respectively for simplicity the irreducible immobile phase saturation sir is not considered in the equations by assuming it is equal to 0 hence the normalized phase saturation s i s i r 1 s w r s o r s g r equals the phase saturation value si the traditional macroscopic formulation of darcy s law that relates the volumetric flux of a phase to the pressure gradient of that phase is given by 2 3 u i k k r i μ i p i ρ i g i w o g where k is the absolute permeability of porous media pi is phase pressure g is the acceleration of gravity and kri ρi and μi are phase relative permeability density and viscosity respectively 2 2 a generalized multiphase flow model based on mixture theory for our investigations the mass balance equations with source terms in the case of compressible water oil gas transport can be given by 2 4 ϕ n w t ϕ n w u w n w q p ρ w q i w n w s w ρ w ϕ n o t ϕ n o u o n o q p n o s o ρ o ϕ n g t ϕ n g u g n g q p ρ g q i g n g s g ρ g where ui i w o g represents the interstitial velocity of phase i in the porous media in addition qp is the production rate and qiw qig represent the injection rate of water and gas respectively the starting point for developing our model that can account for more detailed physical mechanisms for water oil gas porous media flow than conventional modeling is the theory of mixtures this is a theory based on balance laws and conservation principles which is well known in continuum mechanics bowen 1976 rajagopal and tao 1995 byrne and preziosi 2003 ambrosi and preziosi 2002 preziosi and farina 2002 and has been widely applied to the biological tumor growth systems which can be characterized as a mixture of interacting continua neglecting inertial effects acceleration effects as is usual when dealing with creeping flow in porous materials the mechanical stress balance is given by ambrosi and preziosi 2002 2 5 0 s i σ i m i g i i w o g where σi refers to the cauchy stress tensor mi represents the interaction forces exerted on the constituents by other constituents of the mixture and g i s i ρ i g is the external body force due to gravity the standard expression for the stress terms σi is given by 2 6 σ i p i δ τ i i w o g where δ is the unitary tensor and 2 7 τ i 2 μ i e i e i 1 2 u i u i t i w o g the viscous part τi reflects that the water oil and gas behave as a viscous fluid according to general principles of the theory of mixtures the interaction forces mi between the constituents appearing in 2 5 may be described as in preziosi and farina 2002 ambrosi and preziosi 2002 byrne and preziosi 2003 2 8 m o p o s o f w o f o g m o m m w p w s w f w o f w g m w m m g p g s g f w g f o g m g m where fij i j o w g denotes the force drag that the i phase exerts on the j phase the j phase exerts an equal and opposite force f i j similarly mom mwm and mgm represent interaction forces drag forces between fluid and pore walls solid matrix respectively for oil water and gas the term pi si is related to interfacial force exerted by other phases on phase i arising from mathematical derivation of averaged equations drew and segel 1971 to close the system we must specify the drag force term fwo fwg and fog and the stresses σi i o w g and interaction force terms mim between fluid i w o g and matrix drag force represents the interaction between one phase and another phase and is modelled as rajagopal 2007 preziosi and farina 2002 ambrosi and preziosi 2002 2 9 f w o k w o u w u o f w g k w g u w u g f o g k o g u o u g where k i j i j o w g remains to be determined typically k i j s i s j to reflect that this force term will vanish when one of the phases vanishes similarly the interaction force between fluid and pore wall matrix which is stagnant is naturally expressed then as rajagopal and tao 1995 rajagopal 2007 preziosi and farina 2002 ambrosi and preziosi 2002 2 10 m i m k i u i i o w g the coefficients k i j and k i dimension pa s m2 that characterize the magnitude of interaction terms can be chosen such that the model recovers the classical porous media model based on darcy s law at the same time the approach used here will open for development of reservoir models where more detailed physics can be taken into account 3 a summary of the general three fluid model for porous media flow 3 1 the compressible case we are interested in studying a 1 d model for three compressible immiscible fluids moving in a porous media after combining 2 4 2 10 the model takes the following form 3 11 ϕ n w t ϕ n w u w x n w q p ρ w q i w n w s w ρ w ϕ n o t ϕ n o u o x n o q p n o s o ρ o ϕ n g t ϕ n g u g x n g q p ρ g q i g n g s g ρ g s w p w x k w u w k w o u w u o k w g u w u g n w g ε w n w u w x x s o p o x k o u o k w o u o u w k o g u o u g n o g ε o n o u o x x s g p g x k g u g k w g u g u w k o g u g u o n g g ε g n g u g x x δ p o w s w p o p w δ p g o s g p g p o with capillary pressure δpow defined as the pressure difference between the oil and water and capillary pressure δpgo defined as the pressure difference between the gas and oil we may choose to use the following expressions for capillary force 3 12 δ p o w p o p w δ p o w s w p c 1 ln δ 1 s w a 1 and δ 1 a 1 0 δ p g o p g p o δ p g o s g p c 2 s g a 2 and a 2 0 with non negative constants p c i representing interfacial tension this allows us to mimic capillary pressure functions that previsously have been proposed for three phase reservoir flow chen and ewing 1997 odd and david 2010 in addition we have the fundamental relation that the three phases fill the pore space 3 13 s o s w s g 1 the above model must be combined with appropriate closure relations for ρ i ρ i p i we represent the three phases by linear pressure density relations of the form 3 14 ρ w ρ w 0 p w c w ρ o ρ o 0 p o c o ρ g p g c g where cw co and cg represent the inverse of the compressibility of water oil and gas respectively we refer to appendix b for a semi discrete approximation of 3 11 as well as a fully discrete scheme remark 3 1 we may also study a higher dimensional case e g 2d where the model consists of three mass balance equations for three phases water oil and gas and six momentum equations each phase has two directions such as x and y the scheme has been tested in 2d for two phases and shows similar properties as in 1d 3 2 the incompressible case 3 2 1 viscous flow we may let cw co cg go to infinity in 3 14 then we obtain the incompressible version of the model 3 11 we refer to appendix c for a semi discrete as well as a fully discrete scheme for this incompressible case 3 2 2 inviscid flow moreover in order to relate this incompressible version to the classical darcy based formulation we ignore the viscosity terms in the momentum equations by setting ε i 0 i w o g in 3 11 4 5 6 solving momentum equations with respect to interstitial phase velocities ui the darcy velocities of fluid phase are expressed as follows based on 2 2 3 15 u w ϕ s w u w λ w w p w x ρ w g λ w o p o x ρ o g λ w g p g x ρ g g u o ϕ s o u o λ w o p w x ρ w g λ o o p o x ρ o g λ o g p g x ρ g g u g ϕ s g u g λ w g p w x ρ w g λ o g p o x ρ o g λ g g p g x ρ g g and the following relations are defined 3 16 λ w w ϕ s w 2 r r o r g k o g 2 λ w o λ o w ϕ s w s o r k w o r g k o g k w g λ o o ϕ s o 2 r r w r g k w g 2 λ w g λ g w ϕ s w s g r k w g r o k o g k w o λ g g ϕ s g 2 r r w r o k w o 2 λ o g λ g o ϕ s o s g r k o g r w k w g k w o where 3 17 r w k w k w g k w o r o k o k w o k o g r g k g k w g k o g r k w k o k g k w k o k g k w g k w o k o g k w o k w g k o g k g k w o k w k o k w k o g k o k g k o k w g k w k g using capillary pressure relations 3 12 it follows that 3 15 take the following equivalent form 3 18 u w λ w p w x λ w o λ w g δ p o w x λ w g δ p g o x λ w w ρ w λ w o ρ o λ w g ρ g g u o λ o p w x λ o o λ o g δ p o w x λ o g δ p g o x λ w o ρ w λ o o ρ o λ o g ρ g g u g λ g p w x λ g g λ o g δ p o w x λ g g δ p g o x λ w g ρ w λ o g ρ o λ g g ρ g g here we define the following notation for generalized phase mobilities λ i 3 19 λ w λ w w λ w o λ w g λ o λ o o λ w o λ o g λ g λ g g λ w g λ o g by summing uw uo and ug in 3 18 and using the notation introduced in 3 19 the total darcy velocity can be expressed as follows 3 20 u t λ t p w x λ o λ g δ p o w x λ g δ p g o x λ w ρ w λ o ρ o λ g ρ g g where we have used 3 21 λ t λ w λ o λ g therefore the water pressure gradient can be derived from 3 20 3 22 p w x 1 λ t u t f o f g δ p o w x f g δ p g o x f w ρ w f o ρ o f g ρ g g with generalized fractional flow function 3 23 f i λ i λ t i w o g inserting 3 22 into 3 18 we get 3 24 u w f w u t w o w g δ p o w x w g δ p g o x w w ρ w w o ρ o w g ρ g g u o f o u t o o o g δ p o w x o g δ p g o x o w ρ w o o ρ o o g ρ g g u g f g u t g o g g δ p o w x g g δ p g o x g w ρ w g o ρ o g g ρ g g where 3 25 w i λ w f i λ w i o i λ o f i λ o i g i λ g f i λ g i i w o g it should be noted that w i o i g i 0 i w o g in light of 3 16 3 21 and 3 23 4 numerical examples we mainly focus on a reservoir model where there are one injection well at the center and two production wells distributed at two sides the injection rate is equal to the total production rate and the rates of two production wells are also same see fig 1 in addition reservoir inclination θ is also accounted for in the model interaction terms the model 3 11 4 5 6 should be armed with appropriate functional correlations for fluid rock resistance force k w k o k g and fluid fluid drag force k w o k w g k o g here we use the interaction terms suggested in the recent works standnes et al 2017 qiao et al 2018 andersen et al 2019 4 26 k w i w s w α μ w k ϕ k o i o s o β μ o k ϕ k g i g s g γ μ g k ϕ k w o i w o s w s o μ w μ o k ϕ k w g i w g s w s g μ w μ g k ϕ k o g i o g s o s g μ o μ g k ϕ all the interaction terms k i and k i j have dimension pa s m2 the parameters α β and γ are dimensionless exponents whereas iw io and ig are dimensionless friction coefficients characterizing the strength of fluid solid interaction finally iwo iwg and iog are coefficients characterizing the strength of the fluid fluid drag force with dimension pa s 1 input data the input parameters used in the simulations are listed in table 1 we use 101 grid cells for a 100 meter reservoir layer we refer to appendix d for a convergence test the magnitude of the interaction coefficients iwo iwg and iog are chosen as in qiao et al 2018 where we applied a generalized two phase model to match the experimentally measured relative permeability curves and obtained values for the input parameters such as iwo whose magnitude is around several thousands in order to avoid too many complicating effects at the same time in the subsequent discussion we have set the viscosity terms to zero i e ε w ε o ε g 0 we use the similar capillary pressure relations as qiao et al 2019b for water and oil and lewis and pao 2002 for oil and gas see fig 2 the expression of an effective water fractional flow function fw sw so in the conventional water oil gas model assuming no capillary pressure i e δ p o w δ p g o 0 is 4 27 f w s w s o def u w u t λ w λ t u t w w ρ w w o ρ o w g ρ g g sin θ u t where we have used 3 24 and 3 25 where u t 0 x q i q p d x similarly fo and fg can also be expressed in the same manner in order to illustrate the phase flow fraction fw see fig 3 we represent ut by a reference total velocity u t q p 2 q p 2 we refer to table 1 for other input data that are used initial conditions for the waterflooding case we assume the reservoir initially is mostly filled with gas 90 and some oil 10 4 28 s g x t 0 0 9 s o x t 0 0 1 for the wag injection case the reservoir is assumed initially filled with oil 90 and some extra water 10 4 29 s o x t 0 0 9 s w x t 0 0 1 for the compressible case a reference pressure pwl at the left boundary of the layer is given at initial state 4 30 p w l x 0 t 0 10 6 pa boundary conditions we assume a closed boundary for both compressible and incompressible models which means that 4 31 u i x 0 t 0 u i x l t 0 i w o g for the incompressible case we give a reference pressure pwl at the left boundary of the layer 4 32 p w l x 0 t 10 6 pa source terms for wag experiments gas and water are injected at different time periods during the whole oil recovery process we assume that qi x and qp x take the form 4 33 q i w i g x q i w i g σ 1 if x x i σ 2 0 otherwise q p x q p σ 1 if x x p i σ 2 0 otherwise where i 1 2 and q i w i g 0 125 m 3 day and q p 0 0625 m 3 day the width of the small region associated with the injector and producer is σ in the numerical scheme σ δ x 4 1 waterflooding in a gas reservoir we first test the proposed compressible three phase model applied to a gas reservoir development in this example water is injected at 50 m into a gas reservoir layer of length 100 m with a little proportion of oil 10 two cases respectively for the horizontal fig 4 and vertical reservoir fig 5 are shown below the results of the horizontal compressible three phase model with water injection for a total period of 400 days are shown in fig 4 where pressures first column velocities middle column and saturations right column are symmetric with the injection well located at the center of reservoir layer the gas is mostly recovered during the first 130 days see i whereas oil recovery takes place over more than 300 days see f due to its lower mobility than gas it is also observed that at early stage gas pressure along the reservoir layer has less gradient than both the water s and the oil s see first column in fig 4 the injected water displaces both oil and gas in the reservoir near the injection well region where a high pressure gradient is necessary for both water and oil to flow see panel a and b because of their low mobilities after water has arrived the production wells at around 100 days see c water and oil pressures drop owing to the fact that water then can find an easy flow path to the production wells in fig 5 we show the results phase pressures velocities and saturations of a compressible vertical three phase model with a 400 day waterflooding displacement water is injected to displace oil and gas at both sides of the reservoir layer it quickly fills the bottom part then starts accumulating see panel c correspondingly gas is displaced faster in the lower part than in the upper part because the reservoir layer is vertical gravity segregation is seen in the lower part where gas is squeezed upwardly see h and i in contrast to what is shown in fig 4 g gas pressure distribution shows a similar behavior as water and oil higher at bottom and lower at top see first column in fig 5 we refer to the figure text for more details 4 1 1 comparison of the compressible and incompressible models we continue the discussion of the case shown in fig 5 in particular we want to compare the behavior of the compressible and incompressible model constant density values ρ w 1000 kg m 3 ρ o 800 kg m 3 and ρ g 18 kg m3 are used in the incompressible model fig 6 shows a comparison between the compressible and incompressible model after 30 and 120 days a shows that at early stage the injected water in the compressible model prefers to displace gas in the lower part high positive value since water leads to higher pressure at the bottom such that the gas is compressed there with compressed gas produced at the bottom and gas expanding in the upper part gas will only slowly migrate towards the upper part resulting in comparably lower velocity negative in the compressible model the velocity difference shown in d fits well with the saturation difference after 120 days at the early time 30 days the saturation differences are not distinct see c however after a long time 120 days the differences are more significant especially in the water displacing part see f this is due to the increasing phase pressure difference between compressible and incompressible model see b and e the removal of compressed gas from the gas reservoir as almost incompressible water is injected clearly generates additional space for the water to fill which gives rise to a lower pressure 4 2 the compressible three phase model with a wag experiment in wag processes the injected water will migrate towards the bottom of the formation while the injected gas will flow upwardly therefore counter current flow occurs in the vertical direction of the reservoir due to the gravity segregation of water oil and gas significant differences in terms of saturation distribution and producing gor gas oil ratio have been reported between a conventional model and models that better can account for the mix of different flow regimes co current and counter current for example in sherafati and jessen 2017 an explicit representation of flow transitions between co current and counter current flow was used to improve the design of wag injection processes in this part we conduct a water alternating gas wag injection in a 1d reservoir 250 md layer which initially contains 90 oil and 10 water the water and gas injection well is located at 50 m and two production wells are set at 10 m and 90 m gas is injected for the first 10 days followed by the water injection the next 10 days fluids can be produced in both production wells the whole wag experiment continues with an injection circulation of water and gas each for 10 days fig 7 shows the result for a wag injection process produced by the compressible three phase vertical model where gravity segregation has a significant effect from the simulation we see that pressure increases with time first column in fig 7 moreover pressure values at the lower part of the layer are larger than at the upper part due to the density difference water displaces oil faster in the bottom part see b and c in addition gas flows quickly towards the upper part of the reservoir layer see the saturation plots in the upper part oil is recovered faster than in the lower part because of the larger density difference between gas and oil than the one between water and oil see the second column in fig 7 we also observe that gas reaches the bottom production well but does not move further this can be explained by the fact that gravity segregation effect overcomes the capillarity however a lot of gas is accumulated in the upper edge region 0 m 10 m due to the buoyancy force see i 4 3 comparison of compressible and incompressible three phase models with wag experiments in this part we compute solutions from incompressible three phase models with same wag injection process and compare the relevant results with those from the compressible three phase model constant density values ρ w 1000 kg m 3 ρ o 800 kg m 3 and ρ g 18 kg m3 are used in the incompressible model fig 8 shows a comparison between the compressible and incompressible model of the vertical three phase reservoir with a wag process similar to what was observed in fig 6 differences are seen for phase velocity pressure and saturation with increasing time this difference will be enhanced especially for the pressure this is mainly due to the gas compressibility see b and e and the figure text for more explanation because of the density difference water prefers to flow towards the bottom of the layer whereas gas moves faster towards the upper part of layer see c and f 4 3 1 effect of fluid fluid interactions here we want to illustrate the impact from fluid fluid interaction terms on the compressible model with a wag process two situations are compared below one with i w o i w g i o g 0 p a s 1 and one with i w o i w g i o g 5000 p a s 1 fig 9 compares the results for the horizontal model for a wag process with and without fluid fluid interaction effect at 60 and 120 days in b and e we observe that due to the fluid fluid interaction pressure is elevated compared with the case with no fluid fluid interaction the water velocity a and saturation profiles c show that water to a less extent displaces oil and instead flows through the original water channels when fluid fluid interaction is included the difference in the water saturation profiles between c and f is enhanced with time due to the additional resistance force from the fluid fluid interaction term fig 10 compares the results for the vertical model for a wag process with and without fluid fluid interaction effect at 60 and 120 days due to the density difference a large proportion of gas flows to the upper part of layer see c and f and more of the water flows towards the bottom part of layer as a result differences are seen for the water velocity a d and saturation c f for the case with and without fluid fluid interaction similar to fig 9 the build up of the water front is less efficient for the case with fluid fluid interaction since a larger portion of water tends to move through the original water channels a 5 concluding remarks we have presented a three phase compressible and incompressible viscous model based on the mixture theory approach the formulation represents an extension of the conventional darcy type formulations by including fluid fluid viscous coupling effects the three phase flow model consists of a set of mass balance equations which are coupled to a set of momentum balance equations that involve both fluid matrix fluid fluid interactions and internal viscosity effects numerical schemes have been developed for both the compressible and incompressible model moreover various waterflooding displacement scenarios in a gas reservoir and wag injection in an oil reservoir have been investigated to illustrate the effects of fluid compressibility and fluid fluid viscous coupling main findings are i the numerical schemes proposed in this paper appear to be robust and stable for simulation of various three phase flow scenarios both for the incompressible and compressible case ii comparison of the results for the compressible and incompressible model show that the differences between these two models can be significant especially in the vertical case where the effect of gravity segregation is rather strong see f in fig 6 iii the viscous coupling fluid fluid interaction can have a significant effect on the results a strong fluid fluid viscous coupling results in a large resistance force for the flow of displacing fluid water such that a water prefers to move through the original water channels rather than displacing oil see a and c in figs 9 and 10 b water front is slow and builds up see f in figs 9 and 10 credit authorship contribution statement yangyang qiao methodology writing review editing steinar evje methodology writing original draft declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix a from the three mass balance equations we get after multiplying the oil mass balance with ρwρg the water mass balance with ρoρg and the c mass balance with ρwρo 5 34 s w t ρ w s w ρ w t ρ o ρ g ρ o ρ g s w ρ w u w x s w ρ w ρ o ρ g q p ϕ ρ w ρ o ρ g q i w ϕ s o t ρ o s o ρ o t ρ w ρ g ρ w ρ g s o ρ o u o x s o ρ w ρ o ρ g q p ϕ s g t ρ g s g ρ g t ρ w ρ o ρ w ρ o s g ρ g u g x s g ρ w ρ o ρ g q p ϕ ρ w ρ o ρ g q i g ϕ and summing the three resulting equations 5 35 f 1 f 2 f 3 f 4 where f 1 s w t ρ w ρ o ρ g s o t ρ w ρ o ρ g s g t ρ w ρ o ρ g ρ w ρ o ρ g s w t s o t s g t 0 f 2 s w ρ w t ρ o ρ g s o ρ o t ρ w ρ g s g ρ g t ρ w ρ o f 3 ρ o ρ g s w ρ w u w x ρ w ρ g s o ρ o u o x ρ w ρ o s g ρ g u g x f 4 s w ρ w ρ o ρ g q p ϕ ρ w ρ o ρ g q i w ϕ s o ρ w ρ o ρ g q p ϕ s g ρ w ρ o ρ g q p ϕ ρ w ρ o ρ g q i g ϕ ρ w ρ o ρ g q i w q i g q p ϕ here we want to focus on dealing with expression f 2 5 36 f 2 s w ρ o ρ g p w t c w s o ρ w ρ g p o t c o s g ρ w ρ o p g t c g 5 37 p o t p w δ p o w t p w t δ p o w s w t p g t p w δ p o w δ p g o t p w t δ p o w s w t δ p g o s g t a n d s g t s w t s o t s w t n o t ρ o n o c o ρ o 2 p o t s w t n o t ρ o n o c o ρ o 2 p w t δ p o w s w t therefore we have 5 38 f 2 κ ρ w s w ρ o ρ g c w p w t κ ρ w δ p o w s w t s g ρ w ρ o c g δ p g o s w t s g ρ w c g n o t where 5 39 κ s o ρ g c o s g ρ o c g s g s o c o c g clearly s w t n w ρ w t 1 ρ w n w t m ρ w 2 ρ w t 1 ρ w n w t m c w ρ w 2 p w t consequently 5 40 f 2 κ ρ w s w ρ o ρ g c w s w c w κ δ p o w s g ρ o c g δ p g o p w t κ δ p o w s g ρ o c g δ p g o s w ρ w u w x s g ρ w c g s o ρ o u o x s g ρ w c g s o ρ o q p ρ w s w q p ρ w q i w κ δ p o w s g ρ o c g δ p g o since that f 1 0 5 35 will have the following form 5 41 κ ρ w s w ρ o ρ g c w s w c w κ δ p o w s g ρ o c g δ p g o p w t ρ o ρ g s g ρ o c g δ p g o κ δ p o w s w ρ w u w x ρ w ρ g s g ρ w c g s o ρ o u o x ρ w ρ o s g ρ g u g x ρ w ρ o ρ g q i w q i g q p ϕ s g ρ w c g s o ρ o q p ϕ ρ w s w q p ϕ ρ w q i w ϕ κ δ p o w s g ρ o c g δ p g o the upper equation can be reformulated to be 5 42 p w t η 1 n w u w x η 2 n o u o x η 3 n g u g x η 4 q p ϕ η 5 q i w ϕ η 6 q i g ϕ where 5 43 η κ ρ w s w ρ o ρ g c w s w c w κ δ p o w s g ρ o c g δ p g o η 1 1 η ρ o ρ g s g ρ o c g δ p g o κ δ p o w η 2 1 η ρ w ρ g s g ρ w c g η 3 1 η ρ w ρ o η 4 1 η s g ρ w c g s o ρ o ρ w s w κ δ p o w s g ρ o c g δ p g o ρ w ρ o ρ g η 5 1 η ρ w ρ o ρ g ρ w κ δ p o w s g ρ o c g δ p g o η 6 1 η ρ w ρ o ρ g remark 5 1 δ p o w is always non positive and δ p g o non negative appendix b numerical discretization of compressible version we develop a numerical scheme for this general three fluid flow model in a 1d setting the proposed numerical methods are described separately for the compressible appendix b and dummytxdummy incompressible appendix c model b1 a semi discrete scheme for the compressible model we consider a slight reformulation of the model where we shall make use of the pressure evolution eq 5 42 this will be convenient to account for the highly nonlinear coupling between the mass and momentum equations through the pressure terms it also makes the discretization of the compressible and incompressible model consistent the original model takes the form with nw no ng uw uo ug as the main variables 5 44 ϕ n w t ϕ n w u w x n w q p ρ w q i w n w s w ρ w ϕ n o t ϕ n o u o x n o q p n o s o ρ o ϕ n g t ϕ n g u g x n g q p ρ g q i g n g s g ρ g s w p w x k w u w k w o u w u o k w g u w u g n w g ε w n w u w x x s o p o x k o u o k w o u o u w k o g u o u g n o g ε o n o u o x x s g p g x k g u g k w g u g u w k o g u g u o n g g ε g n g u g x x δ p o w s w p o p w δ p g o s g p g p o note that we may rewrite the model in the following equivalent form with nw no pw uw uo ug as the main variables 5 45 ϕ n w t ϕ n w u w x n w q p ρ w q i w ϕ n o t ϕ n o u o x n o q p p w t η 1 n w u w x η 2 n o u o x η 3 n g u g x η 4 q p ϕ η 5 q i w ϕ η 6 q i g ϕ s w p w x k w u w k w o u w u o k w g u w u g n w g ε w n w u w x x s o p w δ p o w x k o u o k w o u o u w k o g u o u g n o g ε o n o u o x x s g p w δ p o w δ p g o x k g u g k w g u g u w k o g u g u o n g g ε g n g u g x x δ p o w s w p o p w δ p g o s g p g p o here ng is determined by 5 46 n g s g ρ g p g 1 s w s o ρ g p g 1 n w ρ w p w n o ρ o p o ρ g p g n g n w n o p w where p o p o s w p w p o n w p w and p g p g s w s o p w p g n w n o p w we may solve 5 45 on our domain ω with boundary conditions 5 47 u w ω u o ω u g ω 0 and initial condition 5 48 n w x t 0 n w 0 x n o x t 0 n o 0 x n g x t 0 n g 0 x p w x 0 t 0 p w l system of odes we consider the domain ω 0 1 and introduce a grid of nx cells with nodes xj placed at the center of the cells x 1 1 2 δ x x 2 1 1 2 δ x x j j 1 2 δ x x n x n x 1 2 δ x and cell interfaces x j 1 2 at the cell interfaces x 1 2 0 x 3 2 δ x x j 1 2 j δ x x n x 1 2 n x δ x 1 where δ x 1 n x we introduce the approximate masses n w j t j 1 n x n o j t j 1 n x and n g j t j 1 n x associated with the nodes x j j 1 n x whereas the approximate velocities u w j 1 2 j 0 n x u o j 1 2 j 0 n x and u g j 1 2 j 0 n x are associated with the cell interfaces x j 1 2 j 0 n x step 1 mass transport we solve for nwj t by considering the following ode for the water phase 5 49 n w j 1 δ x n w u w j 1 2 n w u w j 1 2 n w j q p j ϕ ρ w j q i w j ϕ n w s w ρ w where 5 50 n w u w j 1 2 n w j u w j 1 2 if u w j 1 2 0 n w j 1 u w j 1 2 if u w j 1 2 0 this can also be expressed as n w u w j 1 2 n w j n w j 1 2 u w j 1 2 1 2 n w j 1 n w j u w j 1 2 for the oil phase 5 51 n o j 1 δ x n o u o j 1 2 n o u o j 1 2 n o j q p j ϕ n o s o ρ o where 5 52 remark 5 2 it should be pointed out that q p j q p σ and q i w j q i w σ where j refers to a grid cell which contains a producer injector due to the fact that production qp or injection qiw in 5 44 is interpreted as a value at a point location the width of the small region associated with the injector and producer is σ δ x consistent with 4 33 this also applies for q ig j in 5 53 that is to say q i g j q i g σ step 2 computation of velocities and pressure next we solve for p w j t and u w j 1 2 t u o j 1 2 t and u g j 1 2 t by considering the following ode system 5 53 p w j η 1 j 1 δ x n w u w j 1 2 n w u w j 1 2 η 2 j 1 δ x n o u o j 1 2 n o u o j 1 2 η 3 j 1 δ x n g u g j 1 2 n g u g j 1 2 η 4 j q p j ϕ η 5 j q i w j ϕ η 6 j q i g j ϕ which is combined with the momentum balance equations 5 54 s w j 1 2 1 δ x p w j 1 p w j k w j 1 2 u w j 1 2 k w o j 1 2 u w j 1 2 u o j 1 2 k w g j 1 2 u w j 1 2 u g j 1 2 g n w j 1 2 ε w 1 δ x 2 n w j 1 u w j 3 2 u w j 1 2 n w j u w j 1 2 u w j 1 2 s o j 1 2 1 δ x p w j 1 p w j s o j 1 2 1 δ x δ p o w j 1 δ p o w j k o j 1 2 u o j 1 2 k w o j 1 2 u o j 1 2 u w j 1 2 k o g j 1 2 u o j 1 2 u g j 1 2 g n o j 1 2 ε o 1 δ x 2 n o j 1 u o j 3 2 u o j 1 2 n o j u o j 1 2 u o j 1 2 s g j 1 2 1 δ x p w j 1 p w j s g j 1 2 1 δ x δ p o w j 1 δ p o w j δ p g o j 1 δ p g o j k g j 1 2 u g j 1 2 k w g j 1 2 u g j 1 2 u w j 1 2 k o g j 1 2 u g j 1 2 u o j 1 2 g c g j 1 2 ε g 1 δ x 2 c g j 1 u g j 3 2 u g j 1 2 n g j u g j 1 2 u g j 1 2 here we note that the average s w j 1 2 in 5 54 is based on upwind relatively u w j 1 2 5 55 s w j 1 2 s w j if u w j 1 2 0 s w j s w j 1 2 if u w j 1 2 0 s w j 1 if u w j 1 2 0 similarly for s o j 1 2 s g j 1 2 and for the interaction terms k w j 1 2 k o j 1 2 and k g j 1 2 for k w o j 1 2 k o g j 1 2 and k w g j 1 2 we use the following method 5 56 k w o j 1 2 k w o j if u w j 1 2 0 u o j 1 2 0 k w o j k w o j 1 2 if u w j 1 2 u o j 1 2 0 k w o j 1 if u w j 1 2 0 u o j 1 2 0 k w g j 1 2 and k o g j 1 2 are also approximated using the similar way on the other hand n w u w j 1 2 n o u o j 1 2 and n g u g j 1 2 appearing in 5 53 employ upwind as described in 5 50 now we are in a position where we can describe a fully discrete model b2 a fully discrete scheme we assume that we have given n w j k n o j k p w j k u w j k u o j k u g j k we then compute the approximate solution at time t k 1 expressed by n w j k 1 n o j k 1 p w j k 1 u w j k 1 u o j k 1 u g j k 1 as follows step 1 mass transport 5 57 n w j k 1 n w j k δ t 1 δ x n w u w j 1 2 k n w u w j 1 2 k n w j k q p j k ϕ ρ w j k q i w j k ϕ where 5 58 n w u w j 1 2 k n w j k u w j 1 2 k if u w j 1 2 k 0 n w j 1 k u w j 1 2 k if u w j 1 2 k 0 5 59 n o j k 1 n o j k δ t 1 δ x n o u o j 1 2 k n o u o j 1 2 k n o j k q p j k ϕ where 5 60 n o u o j 1 2 k n o j k u o j 1 2 k if u w j 1 2 k 0 n o j 1 k u o j 1 2 k if u w j 1 2 k 0 having computed n w j k 1 and n o j k 1 we can compute an updated water saturation s w j k 1 2 and s o j k 1 2 given by 5 61 s w j k 1 2 n w j k 1 ρ w p w j k s o j k 1 2 n o j k 1 ρ o p o j k 1 2 n o j k 1 ρ o p w j k δ p o w s w j k 1 2 similarly we compute updated mass n g j k 1 2 and p g j k 1 2 needed to evaluate coefficients in the next step step 2 computation of velocities and pressure next we solve simultaneously for p w j k 1 and u w j 1 2 k 1 u o j 1 2 k 1 and u g j 1 2 k 1 by considering the following algebraic system 5 62 p w j k 1 p w j k δ t η 1 j k 1 2 1 δ x n w k 1 u w k 1 j 1 2 n w k 1 u w k 1 j 1 2 η 2 j k 1 2 1 δ x n o k 1 u o k 1 j 1 2 n o k 1 u o k 1 j 1 2 η 3 j k 1 2 1 δ x n g k 1 2 u g k 1 j 1 2 n g k 1 2 u g k 1 j 1 2 η 4 j k 1 2 q p j k ϕ η 5 j k 1 2 q i w j k ϕ η 6 j k 1 2 q i g j k ϕ which is combined with the momentum balance equations 5 63 s w j 1 2 k 1 2 1 δ x p w j 1 k 1 p w j k 1 k w j 1 2 k 1 2 u w j 1 2 k 1 k w o j 1 2 k 1 2 u w j 1 2 k 1 2 u o j 1 2 k 1 2 k w g j 1 2 k 1 2 u w j 1 2 k 1 2 u g j 1 2 k 1 2 n w j 1 2 k 1 g ε w 1 δ x 2 n w j 1 k 1 u w j 3 2 k 1 u w j 1 2 k 1 n w j k 1 u w j 1 2 k 1 u w j 1 2 k 1 s o j 1 2 k 1 2 1 δ x p w j 1 k 1 p w j k 1 s o j 1 2 k 1 2 1 δ x δ p o w j 1 k 1 2 δ p o w j k 1 2 k o j 1 2 k 1 2 u o j 1 2 k 1 k w o j 1 2 k 1 2 u o j 1 2 k 1 u w j 1 2 k 1 k o g j 1 2 k 1 2 u o j 1 2 k 1 u g j 1 2 k 1 n o j 1 2 k 1 g ε o 1 δ x 2 n o j 1 k 1 2 u o j 3 2 k 1 u o j 1 2 k 1 n o j k 1 2 u o j 1 2 k 1 u o j 1 2 k 1 s g j 1 2 k 1 2 1 δ x p w j 1 k 1 p w j k 1 s g j 1 2 k 1 2 1 δ x δ p o w j 1 k 1 2 δ p o w j k 1 2 δ p g o j 1 k 1 2 δ p g o j k 1 2 k g j 1 2 k 1 2 u g j 1 2 k 1 k w g j 1 2 k 1 2 u g j 1 2 k 1 u w j 1 2 k 1 k o g j 1 2 k 1 2 u g j 1 2 k 1 u o j 1 2 k 1 n g j 1 2 k 1 2 g ε g 1 δ x 2 n g j 1 k 1 2 u g j 3 2 k 1 u g j 1 2 k 1 n g j k 1 2 u g j 1 2 k 1 u g j 1 2 k 1 equipped with p w j k 1 u w j 1 2 k 1 u o j 1 2 k 1 u g j 1 2 k 1 we can now update the saturation 5 64 s w j k 1 n w j k 1 ρ w p w j k 1 s o j k 1 n o j k 1 ρ o p o j k 1 n o j k 1 ρ o p w j k 1 δ p o w j k 1 s w j k 1 from which we also compute the updated gas mass n g j k 1 via 5 46 if necessary we may repeat step 2 to improve the accuracy before we proceed to next time level remark 5 3 the upwind discretization of n w k 1 u w k 1 j 1 2 n o k 1 2 u o k 1 j 1 2 and n g k 1 2 u g k 1 j 1 2 appearing in 5 62 are based on old velocities u w j 1 2 k u o j 1 2 k and u g j 1 2 k remark 5 4 for the higher dimensional case mentioned in remark 3 1 we can use a similar way to solve two mass equations as step 1 for the 1d case then compute pressure and velocities by using a 2d pressure evolution equation similar to the one derived for the 1d case in appendix a together with six momentum equations for three phases in x and y direction appendix c numerical discretization of incompressible version we first describe a semi discrete approximation of the incompressible version of model 3 11 c1 a semidiscrete scheme for the incompressible model when fluids are incompressible the model 5 45 takes the form 5 65 s w t s w u w x s w q p ϕ q i w ϕ s o t s o u o x s o q p ϕ s w u w s o u o s g u g x q p ϕ q i w ϕ q i g ϕ s w p w x k w u w k w o u w u o k w g u w u g n w g ε w ρ w s w u w x x s o p o x k o u o k w o u o u w k o g u o u g n o g ε o ρ o s o u o x x s g p g x k g u g k w g u g u w k o g u g u o n g g ε g ρ g s g u g x x δ p o w s w p o p w δ p g o s g p g p o step 1 mass transport 5 66 s w j 1 δ x s w u w j 1 2 s w u w j 1 2 s w j q p j ϕ q i w j ϕ where 5 67 s w u w j 1 2 s w j u w j 1 2 if u w j 1 2 0 s w j 1 u w j 1 2 if u w j 1 2 0 5 68 s o j 1 δ x s o u o j 1 2 s o u o j 1 2 s o j q p j ϕ where 5 69 s o u o j 1 2 s o j u o j 1 2 if u o j 1 2 0 s o j 1 u o j 1 2 if u o j 1 2 0 step 2 computation of velocities and pressure next we solve for p w j t and u w j 1 2 t u o j 1 2 t and u g j 1 2 t by considering the following ode system 5 70 1 δ x s w u w j 1 2 s w u w j 1 2 1 δ x s o u o j 1 2 s o u o j 1 2 1 δ x s g u g j 1 2 s g u g j 1 2 q i w j ϕ q i g j ϕ q p j ϕ which is combined with the momentum balance equations 5 71 s w j 1 2 1 δ x p w j 1 p w j k w j 1 2 u w j 1 2 k w o j 1 2 u w j 1 2 u o j 1 2 k w g j 1 2 u w j 1 2 u g j 1 2 g s w j 1 2 ρ w ε w ρ w δ x 2 s w j 1 u w j 3 2 u w j 1 2 s w j u w j 1 2 u w j 1 2 s o j 1 2 1 δ x p w j 1 p w j s o j 1 2 1 δ x δ p o w j 1 δ p o w j k o j 1 2 u o j 1 2 k w o j 1 2 u o j 1 2 u w j 1 2 k o g j 1 2 u o j 1 2 u g j 1 2 g s o j 1 2 ρ o ε o ρ o δ x 2 s o j 1 u o j 3 2 u o j 1 2 s o j u o j 1 2 u o j 1 2 s g j 1 2 1 δ x p w j 1 p w j s g j 1 2 1 δ x δ p o w j 1 δ p o w j δ p g o j 1 δ p g o j k g j 1 2 u g j 1 2 k w g j 1 2 u g j 1 2 u w j 1 2 k o g j 1 2 u g j 1 2 u o j 1 2 g s g j 1 2 ρ g ε g ρ g δ x 2 s g j 1 u g j 3 2 u g j 1 2 s g j u g j 1 2 u g j 1 2 here we note that the average s w j 1 2 in 5 71 is based on upwind relatively u w j 1 2 5 72 s w j 1 2 s w j if u w j 1 2 0 s w j s w j 1 2 if u w j 1 2 0 s w j 1 if u w j 1 2 0 similarly for s o j 1 2 s g j 1 2 and for the interaction terms k w j 1 2 k o j 1 2 and k g j 1 2 in addition k w o j 1 2 is based on upwind relatively u w j 1 2 and u o j 1 2 5 73 k w o j 1 2 k w o j if u w j 1 2 0 u o j 1 2 0 k w o j k w o j 1 2 if u w j 1 2 u o j 1 2 0 k w o j 1 if u w j 1 2 0 u o j 1 2 0 k w g j 1 2 and k o g j 1 2 are also approximated using the similar way on the other hand s w u w j 1 2 s o u o j 1 2 and s g u g j 1 2 appearing in 5 70 employ upwind as described in 5 72 c2 a fully discrete scheme for the incompressible model step 1 mass transport 5 74 s w j k 1 s w j k δ t 1 δ x s w u w j 1 2 k s w u w j 1 2 k s w j k q p j k ϕ q i w j k ϕ where 5 75 s w u w j 1 2 k s w j k u w j 1 2 k if u w j 1 2 k 0 s w j 1 k u w j 1 2 k if u w j 1 2 k 0 5 76 s o j k 1 s o j k δ t 1 δ x s o u o j 1 2 k s o u o j 1 2 k s o j k q p j k ϕ where 5 77 s o u o j 1 2 k s o j k u o j 1 2 k if u o j 1 2 k 0 s o j 1 k u o j 1 2 k if u o j 1 2 k 0 having computed s w j k 1 and s o j k 1 we can compute pressure and velocities simultaneously at time level k 1 step 2 computation of velocities and pressure we solve for p w j k 1 and u w j 1 2 k 1 u o j 1 2 k 1 and u g j 1 2 k 1 by considering the following algebraic system 5 78 1 δ x s w k 1 u w k 1 j 1 2 s w k 1 u w k 1 j 1 2 1 δ x s o k 1 u o k 1 j 1 2 s o k 1 u o k 1 j 1 2 1 δ x s g k 1 u g k 1 j 1 2 s g k 1 u g k 1 j 1 2 q i j k ϕ q p j k ϕ which is combined with the momentum balance equations 5 79 s w j 1 2 k 1 2 1 δ x p w j 1 k 1 p w j k 1 k w j 1 2 k 1 2 u w j 1 2 k 1 k w o j 1 2 k 1 2 u w j 1 2 k 1 2 u o j 1 2 k 1 2 k w g j 1 2 k 1 2 u w j 1 2 k 1 2 u g j 1 2 k 1 2 s w j 1 2 k 1 ρ w g ε w ρ w δ x 2 s w j 1 k 1 u w j 3 2 k 1 u w j 1 2 k 1 s w j k 1 u w j 1 2 k 1 u w j 1 2 k 1 s o j 1 2 k 1 2 1 δ x p w j 1 k 1 p w j k 1 s o j 1 2 k 1 2 1 δ x δ p o w j 1 k 1 2 δ p o w j k 1 2 k o j 1 2 k 1 2 u o j 1 2 k 1 k w o j 1 2 k 1 2 u o j 1 2 k 1 u w j 1 2 k 1 k o g j 1 2 k 1 2 u o j 1 2 k 1 u g j 1 2 k 1 s o j 1 2 k 1 ρ o g ε o ρ o δ x 2 s o j 1 k 1 2 u o j 3 2 k 1 u o j 1 2 k 1 s o j k 1 2 u o j 1 2 k 1 u o j 1 2 k 1 s g j 1 2 k 1 2 1 δ x p w j 1 k 1 p w j k 1 s g j 1 2 k 1 2 1 δ x δ p o w j 1 k 1 2 δ p o w j k 1 2 δ p g o j 1 k 1 2 δ p g o j k 1 2 k g j 1 2 k 1 2 u g j 1 2 k 1 k w g j 1 2 k 1 2 u g j 1 2 k 1 u w j 1 2 k 1 k o g j 1 2 k 1 2 u g j 1 2 k 1 u o j 1 2 k 1 s g j 1 2 k 1 2 ρ g g ε g ρ g δ x 2 s g j 1 k 1 2 u g j 3 2 k 1 u g j 1 2 k 1 s g j k 1 2 u g j 1 2 k 1 u g j 1 2 k 1 remark 5 5 the upwind discretization of s w k 1 u w k 1 j 1 2 s o k 1 u o k 1 j 1 2 and s g k 1 u g k 1 j 1 2 appearing in 5 78 are based on old velocities u w j 1 2 k u o j 1 2 k and u g j 1 2 k appendix d the convergence of numerical scheme here we illustrate one example to show the convergence of the presented numerical scheme in appendix c for the incompressible three phase flow a sensitivity test is conducted by using different number of grid cells to compare the water saturation profiles for the same case as shown in fig 6 panel f we refer to fig 11 for an illustration of the convergence test 
478,the feasibility of probabilistic bayesian inversion strongly depends on the dimensionality and complexity of the statistical prior model most geostatistical inversion approaches assume multi gaussian fields and some assume non gaussian categorical fields e g via multiple point geostatistics we combine these two into one hierarchical joint problem which accounts for two and possibly more categories as well as heterogeneities inside each category recent works developed the conditional probability field method based on the ensemble kalman filter enkf for this scenario however enkf type approaches take implicit linearity and trans gaussian assumptions which are not feasible in weak information regimes therefore we develop a tailored gibbs sampler a kind of markov chain monte carlo mcmc method it can do this inversion without assumptions our algorithm extends an existing gibbs sampler with parallel tempering for categorical fields to account for multi gaussian internal heterogeneity we show our key idea and derive our algorithm from the detailed balance required for mcmc algorithms we test our algorithm on a synthetic channelized flow scenario for different levels of data available a highly informative setting transient flow data where the synthetic truth can be recovered and a weakly informative setting steady state data only where the synthetic truth cannot be recovered instead we obtain a multi modal posterior for the proper testing of convergence we use the scale reduction factor by gelman and rubin overall the test illustrates that our algorithm performs well in both settings keywords bayesian inversion mcmc parallel tempering multiple point statistics sequential geostatistical resampling training image 1 introduction heterogeneity of hydraulic parameters is a key control on subsurface flow transport and energy transfer processes characterizing these heterogeneities is difficult because we cannot measure subsurface parameters directly and at sufficient resolution bayesian geostatistical inversion is one way to obtain spatially variable estimates of parameters bayesian inversion combines prior knowledge of the system with the likelihood of observation data the resulting estimate is obtained as a posterior probability distribution in this process a model of the prior knowledge for spatial heterogeneity is needed conventionally multi gaussian fields e g matheron 1975 or categorical fields e g hansen et al 2012 laloy et al 2016 strebelle 2002 are used following iglesias et al 2014 xu and gómez hernández 2015 and mo et al 2020 we combine these two approaches and create a hierarchical model consisting of categorical fields with internal gaussian fields formulating an analytical solution of the posterior based on available data is not possible for such an inverse hierarchical model instead the problem is solved numerically for this purpose two different approaches can be taken approximate or exact methods examples for approximate methods were proposed by xu and gómez hernández 2015 and mo et al 2020 who used an ensemble kalman filter and an ensemble smoother approach respectively both converge to an approximate solution of the bayesian inverse problem because they take implicit linearity and trans gaussian assumptions as discussed for geostatistical inversion by nowak 2009 especially the gaussian assumption is not reasonable for multi facies systems this can be addressed using normal score transformations zhou et al 2011 schöniger et al 2012 however the normal score transformation only removes the multi modality of the parameters the non linear forward models still introduce an error consequently ensemble kalman filters converge with increasing sample size to an implicitly linearized approximation of the true bayesian solution on the contrary iglesias et al 2014 presented a markov chain monte carlo mcmc method that converges to the exact solution of the bayesian inverse problem however their work assumes that the categorical field can be parameterized by a low here 5 number of geometrical parameters this is a strong assumption on the structure of the categorical field and is often not reasonable in applied geostatistical and geological setups hence our work will present an approach that assumes the same spatial discretization here 50 50 for the categorical and the multi gauss fields and avoids a low dimensional parametrization this enables solutions to be more appropriate and more flexible representations of geological reality the difference in dimensionality leads iglesias et al 2014 to use a metropolis hastings proposal for the geometrical parameters instead we use a parallel tempering sequential gibbs approach tailored to the high dimensional situation many mcmc methods have been presented in the literature to solve bayesian inverse problems in this work we use a gibbs based approach this means to resample one or several parameter s conditional with respect to the posterior distribution on the remaining unmodified parameters for geostatistical inversion hansen et al 2012 proposed a sequential gibbs sampler that resamples different spatial blocks of the geostatistical parameter field with this algorithm they were able to sample from categorical fields efficiently we extend their approach of resampling blocks of the parameter field to hierarchical models where the parameter field consists of a categorical field with internal multi gaussian heterogeneity per category to speed up convergence we adopt the parallel tempering approach geyer and thompson 1995 parallel tempering is a method in which several mcmc chains run on similar problems with increasing hardness the chains communicate their results with each other to improve the exploitation of possible solutions laloy et al 2016 showed that this enables faster and more efficient computation on geostatistical problems we test our proposed algorithm in two fundamentally different information regimes for a fully saturated two dimensional groundwater flow first we feature a high information regime many measurements where accurate inversion is challenging xu and gómez hernández 2015 showed that ensemble kalman filters can get good approximations in this regime because the implicit multi gaussian assumption of enkfs holds well over narrow unimodal posterior distributions second we feature a weak information regime few measurements that results in a multi modal posterior we show that our proposed method can reliably find and quantify all modes the key contribution of our paper is to develop a method that can handle both weakly and highly informative regimes for hierarchical geostatistical models with multiple point geostatistics and internal heterogeneity in section 2 we derive our mcmc algorithm section 3 presents the test cases and implementation of our model in section 4 we show and discuss our results finally in section 5 we conclude the most important findings with a short summary 2 methods in this section we give an overview over related mcmc approaches and present how our algorithm extends them first we present our problem in the framework of bayesian inference section 2 1 then we present our key idea in section 2 2 and present the details of our gibbs based mcmc algorithm in section 2 3 finally we extend it in section 2 4 by parallel tempering for increased efficiency 2 1 bayesian inference we assume a stochastic representation of a forward problem 1 f θ d e where f θ is an error free deterministic forward model that describes the relation between the measurement data d and the unknown parameters θ the noise term e condenses all possible error terms our goal is to infer the parameters θ based on the data d and the prior knowledge of θ the parameters θ are viewed as random variables with some prior distribution p θ and a posterior distribution p θ d the posterior is given as 2 p θ d p θ p d θ p d p θ p d θ p θ l θ d we define the likelihood l θ d p d θ and the prior distribution p θ as p θ p θ for a clearer notation in the next subsection the probability density of the data p d p d θ p θ d θ also called bayesian model evidence can be obtained by numerical integration over the parameter space however this integration is difficult and is not required for the parameter inference when the parameter dimensionality is fixed laloy et al 2016 therefore we use the unnormalized density 3 π θ p θ l θ d p θ d assuming some fixed data d where the unnormalized posterior probability π equals prior p times likelihood l we will sample from π θ in the following the likelihood l θ d can often assume values close to machine precision in order to avoid numerical underflow error it is convenient to use the log likelihood l θ d log l θ d instead assuming an uncorrelated normally distributed error term e with standard deviation σe the log likelihood is 4 l θ d n log 1 2 π σ e 2 1 2 σ e 2 i 1 n d i f i θ 2 where fi θ are the simulated equivalents to the measured data di and n is the number of measurements however any other distribution of errors is possible as well to simplify our notation we define the likelihood l θ l θ d and l θ l θ d independent of the data d because we assume constant d during the run time of the algorithm 2 2 markov chain monte carlo markov chain monte carlo mcmc is a popular accurate yet sometimes inefficient algorithm to solve bayesian inverse problems most modern mcmc methods are based or inspired by the metropolis hastings algorithm metropolis et al 1953 hastings 1970 we name all properties an mcmc method needs to fulfill to have proven convergence to the exact distribution based on these we derive the formulas needed for our proposed mcmc algorithm for a general introduction to mcmc methods we point to chib and greenberg 1995 mcmc methods converge to π presented in eq 3 at the limit of infinite runtime if and only if irreducibility aperiodicity and the detailed balance are fulfilled smith and roberts 1993 the first two are almost always fulfilled hence we focus on the detailed balance from now on it is defined as 5 π θ i h θ i θ j π θ j h θ j θ i with the transition kernel h which is usually defined as 6 h θ i θ j q θ i θ j α θ i θ j here q θi θj is the so called proposal distribution and α θi θj is called the acceptance probability combining eqs 3 5 and 6 see appendix for derivation leads to 7 α θ i θ j m i n p θ j l θ j q θ j θ i p θ i l θ i q θ i θ j 1 for any prior p any likelihood l and any proposal distribution q eq 7 provides an α such that the detailed balance is fulfilled hence we can construct an mcmc with almost any proposal distribution q the only restriction is that irreducibility and aperiodicity are not always fulfilled for arbitrarily chosen proposal distributions this yields the question of how to choose q for fast convergence for a given problem class the convergence rate of the mcmc algorithm depends on how fast it can explore the parameter space the faster it moves through the parameter space the faster it converges gelman et al 1996 hence it is desirable to make large changes to the parameter set and accept them with a high probability gelman et al 1996 in practice however these two things contradict each other making small changes in θ results in similar p θj l θj and p θi l θi if the prior and the likelihood function are smooth so that α is around 1 making large changes in θ results in distinct p θj l θj and p θi l θi which results in a small α thus a trade off between the size of the change and the acceptance rate needs to be found gelman et al 1996 other approaches e g hamiltonian mcmc betancourt 2017 construct clever proposal distributions to make far jumps with high acceptance rates however they are not applicable to our problem class because they use they use derivatives of the prior distribution which are not attainable for hierarchical models 2 2 1 metropolis hasting the standard metropolis hasting algorithm metropolis et al 1953 hastings 1970 assumes a symmetric proposal distribution 8 q θ i θ j q θ j θ i inserting this into eq 7 yields that 9 α θ i θ j m i n p θ j l θ j p θ i l θ i 1 m i n π θ j π θ i 1 the metropolis hasting algorithm can sample from π θ without taking any assumptions about its form a standard realization of this approach is the random walk proposal function 10 g θ i θ i ϵ ϵ n 0 σ this function g θi fulfills eq 8 because the normal distribution n 0 σ with mean μ 0 and standard deviation σ is symmetric however the acceptance rate eq 8 depends on the prior and the likelihood which leads to a fast decrease of α for increasing σ especially in high dimensional problems roberts and rosenthal 2002 we want to improve this by using all available information about π θ to increase α and speed up convergence 2 2 2 sampling from the prior the basic idea of many bayesian inversion methods is to use the knowledge that π θ p θ l θ using this information the performance of mcmc methods can be increased in many problem classes especially in high dimensional geoscience problems the prior p θ is complex hence the acceptance rate α often depends almost exclusively on the prior if a standard metropolis hastings algorithm is used furthermore there are cases where the prior p θ can not be evaluated for any given θ because no closed form is known examples are multiple point geostatistics tools that use training images strebelle 2002 or any other prior p θ which is implicitly defined by some random field generator in our work we use training images in these cases it is more reasonable to have an acceptance rate α independent of the prior p θ but explictily enforcing the prior within the proposal density through changing the proposal distribution q θi θj to 11 q θ i θ j p θ j p θ i q θ j θ i this can be achived mosegaard and tarantola 1995 inserting eq 11 into eq 7 results in e g tarantola 2005 12 α θ i θ j m i n l θ j l θ i 1 m i n e l θ j l θ i 1 this idea was termed extended metropolis sampling by hansen et al 2012 this strategy can be nicknamed sampling from the prior distribution for easy understanding new proposed values are only rejected based on the likelihood ratio and not based on the prior sampling from the prior distribution makes the algorithm converge faster because we can make larger changes with similar acceptance rates this leads to the conclusive question how can we find a proposal distribution which satisfies eq 11 for a given problem class different algorithms in the literature satisfy eq 11 in the following we name two approaches first the preconditioned crank nicolson algorithm pcn mcmc fulfills this property for multi gaussian priors beskos et al 2008 cotter et al 2013 second the gibbs approach fulfills this property by conditional resampling parts of the parameter space geman and geman 1984 in hansen et al 2012 this approach was used for resampling boxes in the parameter space of a categorical field we are extending this approach and make it applicable to hierarchical models in the next section 2 3 sequential box resampling in this subsection we first describe how we express categorical geostatistical fields with internal heterogeneity as a hierarchical model then we present our novel mcmc algorithm which fulfills eq 11 we explain our procedure with the help of fig 1 throughout this subsection let us start by looking at the first column and defining our model we assume a hierarchical model that consists of several multi gaussian fields therefore we assume three things first that we have an indicator field θ c which decides which category facies is present at which location second that each category facies θ f i is internally multi gaussian third that nf categories facies exist in our domain two categories in fig 1 we assume a fixed discretization of the considered domain in our example in fig 1 we have 50 50 elements which results in a total of 2500 elements we call this the number of elements ne with that the categorical or indicator field θ c 1 2 n f n e is a vector of size ne which takes integer values the internal heterogeneity of each category θ f i r n e are vectors of the same size and take real values for shorter notation we define θ as a combination of θ c and θ f i 13 θ θ c θ f 1 θ f n f here the parameters θ c and θ f i are vectors where each element θ c k and θ f i k is the parameter at a specific spatial position k thus θ is a matrix of parameters where θ k is the vector containing θ c k and all θ f i k given the categorical field θ c and the internal heterogeneity θ f i we define the quantity of interest s the log conductivity in our application as 14 s θ i 1 n f δ i θ c θ f i with kronecker delta δ i j in this formula θ c is an indicator which category is present at which spatial location δ i θ c which is δ i θ c k is a vector of zeros and ones indicating whether category number i is present at location k taking the element wise product δ i θ c θ f i sets the quantity of interest s equals to θ f i if and only if i θ c at that position this step is visualized in the first column of fig 1 next we want to sample new parameters θ i in the following we use lower indices e g θ i to declare different samples and upper indices e g θ c to declare a part of a sample we define the random proposal function g θ with proposal distribution q θi θj hereby q θi θj is the probability density that g θ i θ j under the probilistic proposal function g we take a block gibbs approach as presented in hansen et al 2012 and do this in two steps first we decide which parameters to keep which one to delete and second we conditionally resample the deleteted parameters to do so we define a box of parameters the set γ 1 2 n e to resample based on two arguments a center point and a diameter of the box the center point is chosen randomly and independently of θ this independence is needed for convergence see appendix in our implementation each position has the same probability of being the center point next we need to fix the size of the box the size of the box defines how much the resulting field s θ changes in one step similar to the σ in the metropolis hastings proposal in eq 10 a large box diameter leads to large changes in each proposal and a low acceptance rate whereas a small diameter leads to small changes and a high acceptance rate one could resample a random set of points in the parameter space instead of a box at a specific position e g mariethoz et al 2010 however we found that this is less efficient compared to the box approach let us first explain some notation we define the set θ c k γ which inherits all parameters of the categorical field θ c which are not resampled respectively we define θ f i k γ as the set of parameters of the internal heterogeneity fields θ f i which are not resampled for shorter notation we define θ k γ as the union of θ c k γ and θ f i k γ see eq 15 furthermore we define the resampled parameters of the categorical and the internal heterogeneity fields as θ c k γ and θ f i k γ respectively with that we define the selection function v θ 15 v θ θ k γ i 1 n e θ c k γ i 1 n e θ f 1 k γ i 1 n e θ f n e k γ to choose the persistent parameters θ k γ we want to keep for the resampling note that the same set γ is used for the categorical field θ c as well as the multi gaussian fields θ f i this step is visualized in the first two columns of fig 1 now we need to conditionally resample the chosen box θ k γ therefore we need to sample from the conditional probabilities pq θ c k γ θ c k γ for the categorical field and p q θ f i k γ θ f i k γ for the internal heterogeneity field here pq is the conditional probability of the prior let us start with the resampling of the categorical field θ c any conditional sampling method uc θ c k γ which can sample from pq θ c k γ θ c k γ can do this job for the categorical field many multiple point geostatistic mps methods exist in the literature for this part and we use the snesim strebelle 2002 algorithm next each multi gaussian field θ f i is repopulated for the internal heterogeneity therefore we need a conditional sampler u f i θ f i k γ which is able to sample from p q θ f i k γ θ f i k γ different sequential gaussian simulation sgsim tools exist which are capable of doing so we use the sgsim algorithm of the gslib library described in deutsch and journel 1992 we chose the snesim algorithm and the gslib library because they are widely used and freely available online the last two columns in fig 1 show how the conditional sampling method is resampling the deleted parameter box we can rewrite the proposal function g θ as one function and get 16 θ j g θ i u c v θ i u f 1 v θ i u f n f v θ i in which v θi decides the position of the resampling box and we use the conditional resampling functions uc θ c k γ and u f i θ f i k γ discussed above in the appendix we show a proof that if uc and u f i are chosen correctly we fulfill the detailed balance as in eq 11 2 4 parallel tempering mcmc the problem specified in section 2 3 is high dimensional and multi modal this leads to two challenges for mcmc techniques long burn in times the period in which the mcmc chain converges towards the final range of values and the risk of getting stuck in one mode i e one local optima laloy et al 2016 showed that parallel tempering solves both these problems first it increases efficiency in high dimensional geostatistical inversion they state that parallel tempering increases convergence towards appropriate data misfit and the sampling diversity second it reduces the risk of only finding one mode local optima in the posterior especially for complex multimodal problems not using parallel tempering results in being trapped in local optima this phenomenon is less likely with parallel tempering laloy et al 2016 the idea of parallel tempering e g earl and deem 2005 is to run several chains on different temperatures t t 1 t n with 1 t 1 t 2 t n each temperature defines the posterior density at temperature t 17 p θ t d p θ l θ d 1 t increasing the temperature t flattens the posterior towards the prior in the limit of t the tempered distirbution p θ t d becomes equal to the prior distirbution p θ in the other limit t 1 yields p θ t d equal to the real posterior distribution p θ d thus only the chain with a temperature of t 1 can be used for posterior sampling the remaining chains are constructed to help the first productive chain in exploring the posterior distribution in the meantime the first chain exploits the good regions found by the other chains hot chains can be built to make farther jumps due to the smoother tempered likelihood function than the colder chains while accepting a similar percentage of proposals the farther jumps in our context mean larger resampling boxes of hotter chains a chain at t always accepts all proposals from the prior when using a sampling from the prior strategy as in eq 11 to make use of all chains the chains need to communicate with each other therefore between every few in chain mcmc steps a between chain swap is proposed which gets accepted with probability 18 α s θ i θ j m i n l θ j l θ i 1 t i 1 t j 1 where ti and tj are the temperatures of the chain of θi and θj if accepted the parameters of these two chains get swapped 3 test cases and implementation 3 1 testing procedure as application test we infer the hydraulic conductivity of a confined aquifer based on hydraulic head data using a groundwater flow model for fully saturated conditions we chose this problem because it is a typical problem in geoscience which is challenging due to dimensionality as a result of the spatial discretization we focus on channelized flow consisting of two different heterogeneous porous media here sand and shale we are interested in two different test cases first a steady state test case with weakly informative data 25 measurements once in time second a transient highly informative test case 25 measurements at ten different time steps 250 measurements in total in a highly informative case the main challenge lies in finding a suitable parameter set in contrast in a weakly informative case with many possible outcomes due to the limited available data exploring the possibly multi modal posterior is challenging for the latter case we use clustering algorithms to show the different parameter modes and to quantify how likely they are this visualization is an enrichment to only showing mean and variance because latter statistics cannot visualize the multi modality of distributions next we show that the algorithm convergences during runtime we do so by independently restarting our algorithm five times and then computing the potential scale reduction factor r introduced by gelman and rubin 1992 r measures how similar the results of different runs are by showing that the results of different runs are similar we can conclude that convergence is likely this way we can asses convergence without having a reference solution which cannot be produced in our problems due to the high complexity of the model gelman et al 1995 proposed that r 1 2 signifies acceptable convergence we try to reach that value for all parameters i e for each pixel of the random field a complete introduction to r can be found in gelman et al 1995 3 2 setup and test cases in this section we give a short overview of the test cases and describe them shortly we use the benchmark proposed in xu and gómez hernández 2015 it is a synthetic confined aquifer which is 50m 50m 5m large it is discretized into 50 50 1 cells it is composed of 65 low conductivity shale and 35 high conductivity sand the spatial sand and shale distributions are characterized by the training image by strebelle 2002 shown in fig 2 the hydraulic log conductivity inside each facies follows a multi gaussian distribution with exponential variograms the parameters for these variograms are shown in table 1 for simplicity the specific storage s 0 is homogenous with s 0 0 1 m 1 inside the domain flow can be described using the saturated groundwater flow equation 19 k x y h x y t η x y s 0 h x y t t where k x y is the isotropic hydraulic conductivity and η encapsulates all source and sink terms this equation can be solved for the hydraulic head h x y t fig 3 shows the synthetic reference conductivity field k x y and the used boundary conditions it consists of a spatially distributed conductivity field with a so called general head boundary condition harbaugh et al 2000 xu and gómez hernández 2015 on the left side further we assume no flow boundary conditions on the top and bottom and fixed outflow on the right side at the positions marked in fig 3 as an initial condition for the transient case we assume a constant head of 8 m as mentioned earlier we define a highly informative transient flow and a weakly informative steady state flow test case in the transient flow case the head values change from the initial conditions towards a steady state solution we assume 25 measurements at the marked positions in fig 3 over time in practice we save the computed heads at these points after 10 21 34 49 65 83 104 127 153 days and add normal distributed noise with standard deviation 0 05 m to simulate real world measurements in the second scenario we assume that only the steady state measurements after days are available 3 3 implementation this section specifies all parameters of the used algorithms we first introduce our forward solver then our conditional samplers and then focus on parallel tempering finally we report the machine used for the numerical experiments the groundwater equation is solved using modflow mcdonald and harbaugh 1988 harbaugh et al 2000 we decided to use this solver because it is widely used in the literature the snesim resampling is done using the training image shown in fig 2 the sgsim algorithm is run with the parameters presented in table 1 the parameters of the mcmc are heavily influencing the result the algorithm only converges fast if the right parameters are chosen choosing these parameters is complicated and is broadly discussed in the literature gelman et al 1996 roberts et al 1997 roberts and rosenthal 2002 a target acceptance rate in the range of 10 50 is generally recommended we tried to get an acceptance rate of approximately 23 4 gelman et al 1996 for all chains we recommend reading gelman et al 1996 for a good introduction on how to chose these parameters for one chain and laloy et al 2016 for parallel tempering the box size can be chosen adaptively hansen et al 2012 laloy et al 2016 during burn in we refrain from doing that because that would lead to different box sizes in each test run this makes it ambiguous whether differences between independent mcmc runs occur due to different box sizes or because of slow convergence of the algorithm instead we did a manual tuning in smaller test runs and used identical settings in each independent mcmc run in the transient case we use 20 parallel chains with the parameters shown in table 2 in the steady state case we use 12 chains with the parameters shown in table 3 we use different box sizes for each chain because preliminary test runs showed that this leads to better results than using the same box size for all chains assuming some box size w number of pixels for the edge length of a square the number of resampled parameters is always smaller or equal than w 2 it can be smaller than w 2 if the center of the box is close to the border of the domain in both scenarios we randomly propose swaps between neighboring chains ti and t i 1 after 10 in chain mcmc steps we run our experiments on a high performance cluster where each node has two intel r xeon r cpu e5 2680 v2 2 80ghz processors with 10 cores each on each node we run independent repetitions of our algorithm to compute r with our implementation each mcmc step takes around 2 seconds in the transient and 1 2 seconds in the steady state case this time mainly consists of the forward simulation groundwater solver and the conditional resampling for 1 million samples the algorithm runs for 23 days in the transient and 14 days in the steady state case our matlab implementation of the algorithm and all data of the mcmc runs are available at https doi org 10 18419 darus 741 4 results and discussion 4 1 preparatory investigations we define the l 2 error of one sample 20 l 2 1 n i 1 n d i f i θ e i 2 l θ d 2 σ e 2 n where fi θ are the simulated equivalents to the measured data di ei are the residuals and n is the number of measurements this l 2 error can be seen as an averaged error of predictions and is a slight variant of the log likelihood l θ d defined in eq 4 the l 2 error converges roughly to σe total error standard derivation here 0 05 independent of the number of measurements n used if n 1 it converges exactly to σe for n fig 4 shows the l 2 error of all experiments over the number of iterations the l 2 error in the first iteration is high in all cases and converges towards 0 05 the burn in contains samples with extremely low likelihoods high l 2 errors which would distort all further investigations thus we delete all burn in samples in all further investigations detecting the length of the burn in was done manually and chosen to be 10 000 samples in the weakly informative case and 100 000 samples in the highly informative case the long burn in time in the highly informative test case shows that finding samples with a good fit to the data is harder compared to the weakly informative test case the remaining 990 000 samples out of 1 000 000 in the weakly informative case and 1 000 000 samples out of 1 100 000 in the highly informative case are used for all statistics below a second observation in fig 4 besides the burn in effect is that the l 2 error fluctuates less in the highly informative test case the key explanation is that the number of data over which the squared residuals e i 2 are averaged in eq 20 is larger 250 versus 25 this provides a more stable l 2 simply by more stable sample statistics the signal to noise ratio snr is defined as 21 snr average l 2 error of prior samples measurement error the transient test case has a snr of approximately 650 and the steady state test case has a snr of approximately 2000 the snr in the transient case is lower because the head at t 0 is fixed leading to a smaller prior variance of the heads this snr suggests that the sampling problem is reasonably hard in both cases 4 2 test case 1 highly informative data first we have a look at the highly informative test case fig 5 shows independent samples of the posterior sampled by our mcmc algorithm all samples are alike and similar to the synthetic solution next we want to look at the whole ensemble fig 6 shows the mean and standard deviation of the whole ensemble with burn in removed we can see two things first the mcmc can find the spatial position of the sand channel second it is uncertain about the exact position of it the latter is expressed by the high standard deviation at the borders of the channel in conclusion the algorithm can produce results that are similar to the synthetic truth next we want to make sure that we achieve this behavior every time we restart the algorithm thus we want to compare different independent test runs and show that all converge to the same posterior we do that in a two step approach first we have a look at several independent mean and standard deviation fields to get a better understanding of where weaknesses could lie second we use the scale reduction factor r to quantify the convergence of our test runs fig 7 shows two mean and standard deviation fields of two independent test runs at first sight the results look mainly similar although we also see some differences e g in the top right corner this observation emphasizes the question of how crucial these differences are the scale reduction factor r can answer this question and is shown in fig 8 the left side of fig 8 shows the spatial distribution of r we see that it did not converge to values lower than 1 2 everywhere we see that the top right corner is an area of concern and we should try to improve predictions in this area the right side of fig 8 shows the mean and maximum scale reduction factor r over the length of the markov chains this indicates how much longer we need to run the mcmc algorithm until the maximum r gets smaller than the 1 2 treshold furthermore it shows that the mean r value has reached the 1 2 mark after 2 105 iterations 4 3 test case 2 weakly informative data let us have a look at the weakly informative test case fig 9 shows individual samples of the posterior sampled by our mcmc algorithm one can see that these samples neither look alike nor similar to the synthetic reference solution nevertheless all these samples are valid solutions the reason they look different is that the likelihood is less restrictive weakly informative this leads to a broader posterior many different possible solutions to investigate this aspect further we have a look at the mean and variance of the whole ensemble fig 10 shows the mean and standard deviation of one test run the posterior has a low uncertainty concerning the position of the sand channel at the right and left boundary of the domain similar to the highly informative test case however in the middle of the domain the inversion results suggest that the connection of the sand channel from left to right is unknown hence based on our limited measurements we do not know the spatial course of the sand channel between the left and right boundary only looking at mean and standard deviation is not informative for two reasons first the mean does not look similar to individual samples due to the spatial smoothing that occurs in the ensemble average second the standard deviation is remarkably high which could indicate a multi modal posterior in such a non linear and non gaussian problem thus we need to do further analysis and investigate the mean and standard deviation of each potential mode from a machine learning point of view each mode can be represented by one cluster hence we can find modes by clustering the posterior we used k means clustering with 4 clusters and a euclidean distance an introduction to k means clustering can be found in hastie et al 2005 we chose 4 clusters because it produced the best results for our test case because the cluster algorithms are susceptible to the input data we want to emphasize that the cluster look remarkably different when produced for various test runs furthermore other norms instead of the euclidean distance and different clustering algorithms change the results as well however the discussed conclusions are not affected fig 11 shows the mean and standard deviation of the posterior of a representative clustering example on the top row of fig 11 we see the mean fields of the clusters and the respective probability of the clusters the probability is defined as the percentage of samples that lie inside the respective cluster we see that 48 of samples are similar to the synthetic reference solution cluster 1 however based on the available information we see that three other clusters are also possible to exclude these other clusters in the inversion one would need more informative measurement data we show this example to emphasize clustering as a possible tool to investigate multi modal distributions by splitting them into more homogeneous sub distributions this clustering resembles a non parametric version of gaussian mixture models for the posterior next we consider the standard deviation within the clusters we can see that the standard deviation of the clusters is significantly smaller than the total standard deviation in fig 10 this observation indicates that clustering reduced the uncertainty around the cluster wise mean fields shown in fig 11 drastically when compared to the non clustered field in fig 10 hence the mean fields of the clusters are more reliable and should be used for further investigations next we show the convergence of our algorithm fig 12 shows the mean of two different test runs we can see that the mean fields look similar to quantify this similarity we have a look at the scale reduction factor r in fig 13 we see that it is lower than 1 2 everywhere in the domain this indicates that the mcmc algorithm converged sufficiently well furthermore we can see that this criterion is reached after approximately 2 105 mcmc steps this looks nice but is the scale reduction factor r the right statistic to use the scale reduction factor r checks the convergence of the mean value and not of the distribution hence r might be the wrong norm in multi modal applications such as the current test case other measures like the kullback leibler divergence kullback and leibler 1951 might be more suitable however we used r because it is widely used in the literature 4 4 summary the proposed algorithm converges to the posterior in the weakly and highly informative test case settings the convergence is measured using the scale reduction factor r the highly informative test case reaches the benchmark value of r 1 2 in 92 76 of the parameter cells and the weakly informative test case reaches it everywhere the highly informative test case shows a posterior that is uni modal and similar to the reference solution the weakly informative test case shows a posterior that is multi modal and can be split using a clustering algorithm we visualize all possible scenarios and their respective probabilities 5 conclusion this work enables realistic inversion of channelized flow in the subsurface thereby it solves the categorical decision which facies is present and the heterogeneity within each facies in a hierarchical framework to achieve this goal we propose a novel mcmc algorithm that combines parallel tempering and sequential resampling this algorithm is an extension of laloy et al 2016 who only treated categorical fields compared to existing solution methods such as the enkf approach in xu and gómez hernández 2015 it converges to the true solution by design not just to an implicit quasi linearized solution we test our algorithm on a highly and weakly informative test case and it converges in both cases the mcmc converges although the posterior is extremely narrow in the highly informative test case and broad and multi modal in the weakly informative test case this shows the general applicability of our method credit authorship contribution statement sebastian reuschen conceptualization methodology software validation formal analysis investigation data curation writing original draft writing review editing visualization teng xu software resources writing review editing supervision wolfgang nowak conceptualization resources writing review editing supervision project administration funding acquisition declaration of competing interest the authors declare that they do not have any financial or nonfinancial conflict of interests acknowledgements funded by the deutsche forschungsgemeinschaft dfg german research foundation project number 327154368 sfb 1313 and project number 359880532 compu flow appendix a in the following we show the derivation of the detailed balance the detailed balance is defined as a 1 π θ i h θ i θ j π θ j h θ j θ i with the transition kernel h which is usually defined as a 2 h θ i θ j q θ i θ j α θ i θ j combining these equations the detailed balance can be written as a 3 π θ i q θ i θ j α θ i θ j π θ j q θ j θ i α θ j θ i inserting eq 3 it follows that a 4 p θ i l θ i q θ i θ j α θ i θ j p θ j l θ j q θ j θ i α θ j θ i after resorting this equation we find that a 5 α θ i θ j p θ j l θ j q θ j θ i p θ i l θ i q θ i θ j α θ j θ i combining this equation with the property of 0 α 1 one obtains that a 6 α θ i θ j m i n p θ j l θ j q θ j θ i p θ i l θ i q θ i θ j 1 appendix b in the following we show a proof that our algorithm fulfills the proposal distribution as specified in eq 11 and hence fulfills the detailed balance assuming that the functions uc and u f i are able to sample from the distribution pq θj c θ k γ and p q θ j f i θ k γ respectively first we partion the parameter space into b 1 θ i θ i k γ θ i k γ with the persistent parameters θ i k γ and the resampled parameters θ i k γ eq 16 defines the proposal function to be b 2 θ j g θ i u c v θ i u f 1 v θ i u f n f v θ i evidently if θ i k γ θ j k γ it follows that q θ i θ j q θ j θ i 0 which fulfills eq 11 thus we know that θ i k γ θ j k γ θ k γ we can resample the rest θi r of the parameter space based on fixed part θ k γ in the given setting we can express q θi θj as b 3 q θ i θ j p q θ j θ k γ p w θ k γ θ i θ k γ where pq θj θ k γ is the probability density of θj based on θ k γ given by some conditional re sampling method and pw θ k γ θi is the probability of the persistent data based on the previous sample assuming that we choose the parameter box indepently of the θi θj we know that b 4 p θ k γ θ i p s where p s is the probability that θ k γ is chosen one could imagine it as the probability to place the box at a certain position such that it excludes exactly θ k γ assuming non zero probabilities p s and p θ k γ it follows that b 5 q θ i θ j q θ j θ i p θ j θ k γ p s p θ i θ k γ p s p θ j θ k γ p θ i θ k γ p θ j θ k γ p θ k γ p θ i θ k γ p θ k γ p θ j p θ i which is equivalent to eq 11 p s 0 or p θ k γ 0 leads to q θ i θ j q θ j θ i 0 which fulfills eq 11 as well 
478,the feasibility of probabilistic bayesian inversion strongly depends on the dimensionality and complexity of the statistical prior model most geostatistical inversion approaches assume multi gaussian fields and some assume non gaussian categorical fields e g via multiple point geostatistics we combine these two into one hierarchical joint problem which accounts for two and possibly more categories as well as heterogeneities inside each category recent works developed the conditional probability field method based on the ensemble kalman filter enkf for this scenario however enkf type approaches take implicit linearity and trans gaussian assumptions which are not feasible in weak information regimes therefore we develop a tailored gibbs sampler a kind of markov chain monte carlo mcmc method it can do this inversion without assumptions our algorithm extends an existing gibbs sampler with parallel tempering for categorical fields to account for multi gaussian internal heterogeneity we show our key idea and derive our algorithm from the detailed balance required for mcmc algorithms we test our algorithm on a synthetic channelized flow scenario for different levels of data available a highly informative setting transient flow data where the synthetic truth can be recovered and a weakly informative setting steady state data only where the synthetic truth cannot be recovered instead we obtain a multi modal posterior for the proper testing of convergence we use the scale reduction factor by gelman and rubin overall the test illustrates that our algorithm performs well in both settings keywords bayesian inversion mcmc parallel tempering multiple point statistics sequential geostatistical resampling training image 1 introduction heterogeneity of hydraulic parameters is a key control on subsurface flow transport and energy transfer processes characterizing these heterogeneities is difficult because we cannot measure subsurface parameters directly and at sufficient resolution bayesian geostatistical inversion is one way to obtain spatially variable estimates of parameters bayesian inversion combines prior knowledge of the system with the likelihood of observation data the resulting estimate is obtained as a posterior probability distribution in this process a model of the prior knowledge for spatial heterogeneity is needed conventionally multi gaussian fields e g matheron 1975 or categorical fields e g hansen et al 2012 laloy et al 2016 strebelle 2002 are used following iglesias et al 2014 xu and gómez hernández 2015 and mo et al 2020 we combine these two approaches and create a hierarchical model consisting of categorical fields with internal gaussian fields formulating an analytical solution of the posterior based on available data is not possible for such an inverse hierarchical model instead the problem is solved numerically for this purpose two different approaches can be taken approximate or exact methods examples for approximate methods were proposed by xu and gómez hernández 2015 and mo et al 2020 who used an ensemble kalman filter and an ensemble smoother approach respectively both converge to an approximate solution of the bayesian inverse problem because they take implicit linearity and trans gaussian assumptions as discussed for geostatistical inversion by nowak 2009 especially the gaussian assumption is not reasonable for multi facies systems this can be addressed using normal score transformations zhou et al 2011 schöniger et al 2012 however the normal score transformation only removes the multi modality of the parameters the non linear forward models still introduce an error consequently ensemble kalman filters converge with increasing sample size to an implicitly linearized approximation of the true bayesian solution on the contrary iglesias et al 2014 presented a markov chain monte carlo mcmc method that converges to the exact solution of the bayesian inverse problem however their work assumes that the categorical field can be parameterized by a low here 5 number of geometrical parameters this is a strong assumption on the structure of the categorical field and is often not reasonable in applied geostatistical and geological setups hence our work will present an approach that assumes the same spatial discretization here 50 50 for the categorical and the multi gauss fields and avoids a low dimensional parametrization this enables solutions to be more appropriate and more flexible representations of geological reality the difference in dimensionality leads iglesias et al 2014 to use a metropolis hastings proposal for the geometrical parameters instead we use a parallel tempering sequential gibbs approach tailored to the high dimensional situation many mcmc methods have been presented in the literature to solve bayesian inverse problems in this work we use a gibbs based approach this means to resample one or several parameter s conditional with respect to the posterior distribution on the remaining unmodified parameters for geostatistical inversion hansen et al 2012 proposed a sequential gibbs sampler that resamples different spatial blocks of the geostatistical parameter field with this algorithm they were able to sample from categorical fields efficiently we extend their approach of resampling blocks of the parameter field to hierarchical models where the parameter field consists of a categorical field with internal multi gaussian heterogeneity per category to speed up convergence we adopt the parallel tempering approach geyer and thompson 1995 parallel tempering is a method in which several mcmc chains run on similar problems with increasing hardness the chains communicate their results with each other to improve the exploitation of possible solutions laloy et al 2016 showed that this enables faster and more efficient computation on geostatistical problems we test our proposed algorithm in two fundamentally different information regimes for a fully saturated two dimensional groundwater flow first we feature a high information regime many measurements where accurate inversion is challenging xu and gómez hernández 2015 showed that ensemble kalman filters can get good approximations in this regime because the implicit multi gaussian assumption of enkfs holds well over narrow unimodal posterior distributions second we feature a weak information regime few measurements that results in a multi modal posterior we show that our proposed method can reliably find and quantify all modes the key contribution of our paper is to develop a method that can handle both weakly and highly informative regimes for hierarchical geostatistical models with multiple point geostatistics and internal heterogeneity in section 2 we derive our mcmc algorithm section 3 presents the test cases and implementation of our model in section 4 we show and discuss our results finally in section 5 we conclude the most important findings with a short summary 2 methods in this section we give an overview over related mcmc approaches and present how our algorithm extends them first we present our problem in the framework of bayesian inference section 2 1 then we present our key idea in section 2 2 and present the details of our gibbs based mcmc algorithm in section 2 3 finally we extend it in section 2 4 by parallel tempering for increased efficiency 2 1 bayesian inference we assume a stochastic representation of a forward problem 1 f θ d e where f θ is an error free deterministic forward model that describes the relation between the measurement data d and the unknown parameters θ the noise term e condenses all possible error terms our goal is to infer the parameters θ based on the data d and the prior knowledge of θ the parameters θ are viewed as random variables with some prior distribution p θ and a posterior distribution p θ d the posterior is given as 2 p θ d p θ p d θ p d p θ p d θ p θ l θ d we define the likelihood l θ d p d θ and the prior distribution p θ as p θ p θ for a clearer notation in the next subsection the probability density of the data p d p d θ p θ d θ also called bayesian model evidence can be obtained by numerical integration over the parameter space however this integration is difficult and is not required for the parameter inference when the parameter dimensionality is fixed laloy et al 2016 therefore we use the unnormalized density 3 π θ p θ l θ d p θ d assuming some fixed data d where the unnormalized posterior probability π equals prior p times likelihood l we will sample from π θ in the following the likelihood l θ d can often assume values close to machine precision in order to avoid numerical underflow error it is convenient to use the log likelihood l θ d log l θ d instead assuming an uncorrelated normally distributed error term e with standard deviation σe the log likelihood is 4 l θ d n log 1 2 π σ e 2 1 2 σ e 2 i 1 n d i f i θ 2 where fi θ are the simulated equivalents to the measured data di and n is the number of measurements however any other distribution of errors is possible as well to simplify our notation we define the likelihood l θ l θ d and l θ l θ d independent of the data d because we assume constant d during the run time of the algorithm 2 2 markov chain monte carlo markov chain monte carlo mcmc is a popular accurate yet sometimes inefficient algorithm to solve bayesian inverse problems most modern mcmc methods are based or inspired by the metropolis hastings algorithm metropolis et al 1953 hastings 1970 we name all properties an mcmc method needs to fulfill to have proven convergence to the exact distribution based on these we derive the formulas needed for our proposed mcmc algorithm for a general introduction to mcmc methods we point to chib and greenberg 1995 mcmc methods converge to π presented in eq 3 at the limit of infinite runtime if and only if irreducibility aperiodicity and the detailed balance are fulfilled smith and roberts 1993 the first two are almost always fulfilled hence we focus on the detailed balance from now on it is defined as 5 π θ i h θ i θ j π θ j h θ j θ i with the transition kernel h which is usually defined as 6 h θ i θ j q θ i θ j α θ i θ j here q θi θj is the so called proposal distribution and α θi θj is called the acceptance probability combining eqs 3 5 and 6 see appendix for derivation leads to 7 α θ i θ j m i n p θ j l θ j q θ j θ i p θ i l θ i q θ i θ j 1 for any prior p any likelihood l and any proposal distribution q eq 7 provides an α such that the detailed balance is fulfilled hence we can construct an mcmc with almost any proposal distribution q the only restriction is that irreducibility and aperiodicity are not always fulfilled for arbitrarily chosen proposal distributions this yields the question of how to choose q for fast convergence for a given problem class the convergence rate of the mcmc algorithm depends on how fast it can explore the parameter space the faster it moves through the parameter space the faster it converges gelman et al 1996 hence it is desirable to make large changes to the parameter set and accept them with a high probability gelman et al 1996 in practice however these two things contradict each other making small changes in θ results in similar p θj l θj and p θi l θi if the prior and the likelihood function are smooth so that α is around 1 making large changes in θ results in distinct p θj l θj and p θi l θi which results in a small α thus a trade off between the size of the change and the acceptance rate needs to be found gelman et al 1996 other approaches e g hamiltonian mcmc betancourt 2017 construct clever proposal distributions to make far jumps with high acceptance rates however they are not applicable to our problem class because they use they use derivatives of the prior distribution which are not attainable for hierarchical models 2 2 1 metropolis hasting the standard metropolis hasting algorithm metropolis et al 1953 hastings 1970 assumes a symmetric proposal distribution 8 q θ i θ j q θ j θ i inserting this into eq 7 yields that 9 α θ i θ j m i n p θ j l θ j p θ i l θ i 1 m i n π θ j π θ i 1 the metropolis hasting algorithm can sample from π θ without taking any assumptions about its form a standard realization of this approach is the random walk proposal function 10 g θ i θ i ϵ ϵ n 0 σ this function g θi fulfills eq 8 because the normal distribution n 0 σ with mean μ 0 and standard deviation σ is symmetric however the acceptance rate eq 8 depends on the prior and the likelihood which leads to a fast decrease of α for increasing σ especially in high dimensional problems roberts and rosenthal 2002 we want to improve this by using all available information about π θ to increase α and speed up convergence 2 2 2 sampling from the prior the basic idea of many bayesian inversion methods is to use the knowledge that π θ p θ l θ using this information the performance of mcmc methods can be increased in many problem classes especially in high dimensional geoscience problems the prior p θ is complex hence the acceptance rate α often depends almost exclusively on the prior if a standard metropolis hastings algorithm is used furthermore there are cases where the prior p θ can not be evaluated for any given θ because no closed form is known examples are multiple point geostatistics tools that use training images strebelle 2002 or any other prior p θ which is implicitly defined by some random field generator in our work we use training images in these cases it is more reasonable to have an acceptance rate α independent of the prior p θ but explictily enforcing the prior within the proposal density through changing the proposal distribution q θi θj to 11 q θ i θ j p θ j p θ i q θ j θ i this can be achived mosegaard and tarantola 1995 inserting eq 11 into eq 7 results in e g tarantola 2005 12 α θ i θ j m i n l θ j l θ i 1 m i n e l θ j l θ i 1 this idea was termed extended metropolis sampling by hansen et al 2012 this strategy can be nicknamed sampling from the prior distribution for easy understanding new proposed values are only rejected based on the likelihood ratio and not based on the prior sampling from the prior distribution makes the algorithm converge faster because we can make larger changes with similar acceptance rates this leads to the conclusive question how can we find a proposal distribution which satisfies eq 11 for a given problem class different algorithms in the literature satisfy eq 11 in the following we name two approaches first the preconditioned crank nicolson algorithm pcn mcmc fulfills this property for multi gaussian priors beskos et al 2008 cotter et al 2013 second the gibbs approach fulfills this property by conditional resampling parts of the parameter space geman and geman 1984 in hansen et al 2012 this approach was used for resampling boxes in the parameter space of a categorical field we are extending this approach and make it applicable to hierarchical models in the next section 2 3 sequential box resampling in this subsection we first describe how we express categorical geostatistical fields with internal heterogeneity as a hierarchical model then we present our novel mcmc algorithm which fulfills eq 11 we explain our procedure with the help of fig 1 throughout this subsection let us start by looking at the first column and defining our model we assume a hierarchical model that consists of several multi gaussian fields therefore we assume three things first that we have an indicator field θ c which decides which category facies is present at which location second that each category facies θ f i is internally multi gaussian third that nf categories facies exist in our domain two categories in fig 1 we assume a fixed discretization of the considered domain in our example in fig 1 we have 50 50 elements which results in a total of 2500 elements we call this the number of elements ne with that the categorical or indicator field θ c 1 2 n f n e is a vector of size ne which takes integer values the internal heterogeneity of each category θ f i r n e are vectors of the same size and take real values for shorter notation we define θ as a combination of θ c and θ f i 13 θ θ c θ f 1 θ f n f here the parameters θ c and θ f i are vectors where each element θ c k and θ f i k is the parameter at a specific spatial position k thus θ is a matrix of parameters where θ k is the vector containing θ c k and all θ f i k given the categorical field θ c and the internal heterogeneity θ f i we define the quantity of interest s the log conductivity in our application as 14 s θ i 1 n f δ i θ c θ f i with kronecker delta δ i j in this formula θ c is an indicator which category is present at which spatial location δ i θ c which is δ i θ c k is a vector of zeros and ones indicating whether category number i is present at location k taking the element wise product δ i θ c θ f i sets the quantity of interest s equals to θ f i if and only if i θ c at that position this step is visualized in the first column of fig 1 next we want to sample new parameters θ i in the following we use lower indices e g θ i to declare different samples and upper indices e g θ c to declare a part of a sample we define the random proposal function g θ with proposal distribution q θi θj hereby q θi θj is the probability density that g θ i θ j under the probilistic proposal function g we take a block gibbs approach as presented in hansen et al 2012 and do this in two steps first we decide which parameters to keep which one to delete and second we conditionally resample the deleteted parameters to do so we define a box of parameters the set γ 1 2 n e to resample based on two arguments a center point and a diameter of the box the center point is chosen randomly and independently of θ this independence is needed for convergence see appendix in our implementation each position has the same probability of being the center point next we need to fix the size of the box the size of the box defines how much the resulting field s θ changes in one step similar to the σ in the metropolis hastings proposal in eq 10 a large box diameter leads to large changes in each proposal and a low acceptance rate whereas a small diameter leads to small changes and a high acceptance rate one could resample a random set of points in the parameter space instead of a box at a specific position e g mariethoz et al 2010 however we found that this is less efficient compared to the box approach let us first explain some notation we define the set θ c k γ which inherits all parameters of the categorical field θ c which are not resampled respectively we define θ f i k γ as the set of parameters of the internal heterogeneity fields θ f i which are not resampled for shorter notation we define θ k γ as the union of θ c k γ and θ f i k γ see eq 15 furthermore we define the resampled parameters of the categorical and the internal heterogeneity fields as θ c k γ and θ f i k γ respectively with that we define the selection function v θ 15 v θ θ k γ i 1 n e θ c k γ i 1 n e θ f 1 k γ i 1 n e θ f n e k γ to choose the persistent parameters θ k γ we want to keep for the resampling note that the same set γ is used for the categorical field θ c as well as the multi gaussian fields θ f i this step is visualized in the first two columns of fig 1 now we need to conditionally resample the chosen box θ k γ therefore we need to sample from the conditional probabilities pq θ c k γ θ c k γ for the categorical field and p q θ f i k γ θ f i k γ for the internal heterogeneity field here pq is the conditional probability of the prior let us start with the resampling of the categorical field θ c any conditional sampling method uc θ c k γ which can sample from pq θ c k γ θ c k γ can do this job for the categorical field many multiple point geostatistic mps methods exist in the literature for this part and we use the snesim strebelle 2002 algorithm next each multi gaussian field θ f i is repopulated for the internal heterogeneity therefore we need a conditional sampler u f i θ f i k γ which is able to sample from p q θ f i k γ θ f i k γ different sequential gaussian simulation sgsim tools exist which are capable of doing so we use the sgsim algorithm of the gslib library described in deutsch and journel 1992 we chose the snesim algorithm and the gslib library because they are widely used and freely available online the last two columns in fig 1 show how the conditional sampling method is resampling the deleted parameter box we can rewrite the proposal function g θ as one function and get 16 θ j g θ i u c v θ i u f 1 v θ i u f n f v θ i in which v θi decides the position of the resampling box and we use the conditional resampling functions uc θ c k γ and u f i θ f i k γ discussed above in the appendix we show a proof that if uc and u f i are chosen correctly we fulfill the detailed balance as in eq 11 2 4 parallel tempering mcmc the problem specified in section 2 3 is high dimensional and multi modal this leads to two challenges for mcmc techniques long burn in times the period in which the mcmc chain converges towards the final range of values and the risk of getting stuck in one mode i e one local optima laloy et al 2016 showed that parallel tempering solves both these problems first it increases efficiency in high dimensional geostatistical inversion they state that parallel tempering increases convergence towards appropriate data misfit and the sampling diversity second it reduces the risk of only finding one mode local optima in the posterior especially for complex multimodal problems not using parallel tempering results in being trapped in local optima this phenomenon is less likely with parallel tempering laloy et al 2016 the idea of parallel tempering e g earl and deem 2005 is to run several chains on different temperatures t t 1 t n with 1 t 1 t 2 t n each temperature defines the posterior density at temperature t 17 p θ t d p θ l θ d 1 t increasing the temperature t flattens the posterior towards the prior in the limit of t the tempered distirbution p θ t d becomes equal to the prior distirbution p θ in the other limit t 1 yields p θ t d equal to the real posterior distribution p θ d thus only the chain with a temperature of t 1 can be used for posterior sampling the remaining chains are constructed to help the first productive chain in exploring the posterior distribution in the meantime the first chain exploits the good regions found by the other chains hot chains can be built to make farther jumps due to the smoother tempered likelihood function than the colder chains while accepting a similar percentage of proposals the farther jumps in our context mean larger resampling boxes of hotter chains a chain at t always accepts all proposals from the prior when using a sampling from the prior strategy as in eq 11 to make use of all chains the chains need to communicate with each other therefore between every few in chain mcmc steps a between chain swap is proposed which gets accepted with probability 18 α s θ i θ j m i n l θ j l θ i 1 t i 1 t j 1 where ti and tj are the temperatures of the chain of θi and θj if accepted the parameters of these two chains get swapped 3 test cases and implementation 3 1 testing procedure as application test we infer the hydraulic conductivity of a confined aquifer based on hydraulic head data using a groundwater flow model for fully saturated conditions we chose this problem because it is a typical problem in geoscience which is challenging due to dimensionality as a result of the spatial discretization we focus on channelized flow consisting of two different heterogeneous porous media here sand and shale we are interested in two different test cases first a steady state test case with weakly informative data 25 measurements once in time second a transient highly informative test case 25 measurements at ten different time steps 250 measurements in total in a highly informative case the main challenge lies in finding a suitable parameter set in contrast in a weakly informative case with many possible outcomes due to the limited available data exploring the possibly multi modal posterior is challenging for the latter case we use clustering algorithms to show the different parameter modes and to quantify how likely they are this visualization is an enrichment to only showing mean and variance because latter statistics cannot visualize the multi modality of distributions next we show that the algorithm convergences during runtime we do so by independently restarting our algorithm five times and then computing the potential scale reduction factor r introduced by gelman and rubin 1992 r measures how similar the results of different runs are by showing that the results of different runs are similar we can conclude that convergence is likely this way we can asses convergence without having a reference solution which cannot be produced in our problems due to the high complexity of the model gelman et al 1995 proposed that r 1 2 signifies acceptable convergence we try to reach that value for all parameters i e for each pixel of the random field a complete introduction to r can be found in gelman et al 1995 3 2 setup and test cases in this section we give a short overview of the test cases and describe them shortly we use the benchmark proposed in xu and gómez hernández 2015 it is a synthetic confined aquifer which is 50m 50m 5m large it is discretized into 50 50 1 cells it is composed of 65 low conductivity shale and 35 high conductivity sand the spatial sand and shale distributions are characterized by the training image by strebelle 2002 shown in fig 2 the hydraulic log conductivity inside each facies follows a multi gaussian distribution with exponential variograms the parameters for these variograms are shown in table 1 for simplicity the specific storage s 0 is homogenous with s 0 0 1 m 1 inside the domain flow can be described using the saturated groundwater flow equation 19 k x y h x y t η x y s 0 h x y t t where k x y is the isotropic hydraulic conductivity and η encapsulates all source and sink terms this equation can be solved for the hydraulic head h x y t fig 3 shows the synthetic reference conductivity field k x y and the used boundary conditions it consists of a spatially distributed conductivity field with a so called general head boundary condition harbaugh et al 2000 xu and gómez hernández 2015 on the left side further we assume no flow boundary conditions on the top and bottom and fixed outflow on the right side at the positions marked in fig 3 as an initial condition for the transient case we assume a constant head of 8 m as mentioned earlier we define a highly informative transient flow and a weakly informative steady state flow test case in the transient flow case the head values change from the initial conditions towards a steady state solution we assume 25 measurements at the marked positions in fig 3 over time in practice we save the computed heads at these points after 10 21 34 49 65 83 104 127 153 days and add normal distributed noise with standard deviation 0 05 m to simulate real world measurements in the second scenario we assume that only the steady state measurements after days are available 3 3 implementation this section specifies all parameters of the used algorithms we first introduce our forward solver then our conditional samplers and then focus on parallel tempering finally we report the machine used for the numerical experiments the groundwater equation is solved using modflow mcdonald and harbaugh 1988 harbaugh et al 2000 we decided to use this solver because it is widely used in the literature the snesim resampling is done using the training image shown in fig 2 the sgsim algorithm is run with the parameters presented in table 1 the parameters of the mcmc are heavily influencing the result the algorithm only converges fast if the right parameters are chosen choosing these parameters is complicated and is broadly discussed in the literature gelman et al 1996 roberts et al 1997 roberts and rosenthal 2002 a target acceptance rate in the range of 10 50 is generally recommended we tried to get an acceptance rate of approximately 23 4 gelman et al 1996 for all chains we recommend reading gelman et al 1996 for a good introduction on how to chose these parameters for one chain and laloy et al 2016 for parallel tempering the box size can be chosen adaptively hansen et al 2012 laloy et al 2016 during burn in we refrain from doing that because that would lead to different box sizes in each test run this makes it ambiguous whether differences between independent mcmc runs occur due to different box sizes or because of slow convergence of the algorithm instead we did a manual tuning in smaller test runs and used identical settings in each independent mcmc run in the transient case we use 20 parallel chains with the parameters shown in table 2 in the steady state case we use 12 chains with the parameters shown in table 3 we use different box sizes for each chain because preliminary test runs showed that this leads to better results than using the same box size for all chains assuming some box size w number of pixels for the edge length of a square the number of resampled parameters is always smaller or equal than w 2 it can be smaller than w 2 if the center of the box is close to the border of the domain in both scenarios we randomly propose swaps between neighboring chains ti and t i 1 after 10 in chain mcmc steps we run our experiments on a high performance cluster where each node has two intel r xeon r cpu e5 2680 v2 2 80ghz processors with 10 cores each on each node we run independent repetitions of our algorithm to compute r with our implementation each mcmc step takes around 2 seconds in the transient and 1 2 seconds in the steady state case this time mainly consists of the forward simulation groundwater solver and the conditional resampling for 1 million samples the algorithm runs for 23 days in the transient and 14 days in the steady state case our matlab implementation of the algorithm and all data of the mcmc runs are available at https doi org 10 18419 darus 741 4 results and discussion 4 1 preparatory investigations we define the l 2 error of one sample 20 l 2 1 n i 1 n d i f i θ e i 2 l θ d 2 σ e 2 n where fi θ are the simulated equivalents to the measured data di ei are the residuals and n is the number of measurements this l 2 error can be seen as an averaged error of predictions and is a slight variant of the log likelihood l θ d defined in eq 4 the l 2 error converges roughly to σe total error standard derivation here 0 05 independent of the number of measurements n used if n 1 it converges exactly to σe for n fig 4 shows the l 2 error of all experiments over the number of iterations the l 2 error in the first iteration is high in all cases and converges towards 0 05 the burn in contains samples with extremely low likelihoods high l 2 errors which would distort all further investigations thus we delete all burn in samples in all further investigations detecting the length of the burn in was done manually and chosen to be 10 000 samples in the weakly informative case and 100 000 samples in the highly informative case the long burn in time in the highly informative test case shows that finding samples with a good fit to the data is harder compared to the weakly informative test case the remaining 990 000 samples out of 1 000 000 in the weakly informative case and 1 000 000 samples out of 1 100 000 in the highly informative case are used for all statistics below a second observation in fig 4 besides the burn in effect is that the l 2 error fluctuates less in the highly informative test case the key explanation is that the number of data over which the squared residuals e i 2 are averaged in eq 20 is larger 250 versus 25 this provides a more stable l 2 simply by more stable sample statistics the signal to noise ratio snr is defined as 21 snr average l 2 error of prior samples measurement error the transient test case has a snr of approximately 650 and the steady state test case has a snr of approximately 2000 the snr in the transient case is lower because the head at t 0 is fixed leading to a smaller prior variance of the heads this snr suggests that the sampling problem is reasonably hard in both cases 4 2 test case 1 highly informative data first we have a look at the highly informative test case fig 5 shows independent samples of the posterior sampled by our mcmc algorithm all samples are alike and similar to the synthetic solution next we want to look at the whole ensemble fig 6 shows the mean and standard deviation of the whole ensemble with burn in removed we can see two things first the mcmc can find the spatial position of the sand channel second it is uncertain about the exact position of it the latter is expressed by the high standard deviation at the borders of the channel in conclusion the algorithm can produce results that are similar to the synthetic truth next we want to make sure that we achieve this behavior every time we restart the algorithm thus we want to compare different independent test runs and show that all converge to the same posterior we do that in a two step approach first we have a look at several independent mean and standard deviation fields to get a better understanding of where weaknesses could lie second we use the scale reduction factor r to quantify the convergence of our test runs fig 7 shows two mean and standard deviation fields of two independent test runs at first sight the results look mainly similar although we also see some differences e g in the top right corner this observation emphasizes the question of how crucial these differences are the scale reduction factor r can answer this question and is shown in fig 8 the left side of fig 8 shows the spatial distribution of r we see that it did not converge to values lower than 1 2 everywhere we see that the top right corner is an area of concern and we should try to improve predictions in this area the right side of fig 8 shows the mean and maximum scale reduction factor r over the length of the markov chains this indicates how much longer we need to run the mcmc algorithm until the maximum r gets smaller than the 1 2 treshold furthermore it shows that the mean r value has reached the 1 2 mark after 2 105 iterations 4 3 test case 2 weakly informative data let us have a look at the weakly informative test case fig 9 shows individual samples of the posterior sampled by our mcmc algorithm one can see that these samples neither look alike nor similar to the synthetic reference solution nevertheless all these samples are valid solutions the reason they look different is that the likelihood is less restrictive weakly informative this leads to a broader posterior many different possible solutions to investigate this aspect further we have a look at the mean and variance of the whole ensemble fig 10 shows the mean and standard deviation of one test run the posterior has a low uncertainty concerning the position of the sand channel at the right and left boundary of the domain similar to the highly informative test case however in the middle of the domain the inversion results suggest that the connection of the sand channel from left to right is unknown hence based on our limited measurements we do not know the spatial course of the sand channel between the left and right boundary only looking at mean and standard deviation is not informative for two reasons first the mean does not look similar to individual samples due to the spatial smoothing that occurs in the ensemble average second the standard deviation is remarkably high which could indicate a multi modal posterior in such a non linear and non gaussian problem thus we need to do further analysis and investigate the mean and standard deviation of each potential mode from a machine learning point of view each mode can be represented by one cluster hence we can find modes by clustering the posterior we used k means clustering with 4 clusters and a euclidean distance an introduction to k means clustering can be found in hastie et al 2005 we chose 4 clusters because it produced the best results for our test case because the cluster algorithms are susceptible to the input data we want to emphasize that the cluster look remarkably different when produced for various test runs furthermore other norms instead of the euclidean distance and different clustering algorithms change the results as well however the discussed conclusions are not affected fig 11 shows the mean and standard deviation of the posterior of a representative clustering example on the top row of fig 11 we see the mean fields of the clusters and the respective probability of the clusters the probability is defined as the percentage of samples that lie inside the respective cluster we see that 48 of samples are similar to the synthetic reference solution cluster 1 however based on the available information we see that three other clusters are also possible to exclude these other clusters in the inversion one would need more informative measurement data we show this example to emphasize clustering as a possible tool to investigate multi modal distributions by splitting them into more homogeneous sub distributions this clustering resembles a non parametric version of gaussian mixture models for the posterior next we consider the standard deviation within the clusters we can see that the standard deviation of the clusters is significantly smaller than the total standard deviation in fig 10 this observation indicates that clustering reduced the uncertainty around the cluster wise mean fields shown in fig 11 drastically when compared to the non clustered field in fig 10 hence the mean fields of the clusters are more reliable and should be used for further investigations next we show the convergence of our algorithm fig 12 shows the mean of two different test runs we can see that the mean fields look similar to quantify this similarity we have a look at the scale reduction factor r in fig 13 we see that it is lower than 1 2 everywhere in the domain this indicates that the mcmc algorithm converged sufficiently well furthermore we can see that this criterion is reached after approximately 2 105 mcmc steps this looks nice but is the scale reduction factor r the right statistic to use the scale reduction factor r checks the convergence of the mean value and not of the distribution hence r might be the wrong norm in multi modal applications such as the current test case other measures like the kullback leibler divergence kullback and leibler 1951 might be more suitable however we used r because it is widely used in the literature 4 4 summary the proposed algorithm converges to the posterior in the weakly and highly informative test case settings the convergence is measured using the scale reduction factor r the highly informative test case reaches the benchmark value of r 1 2 in 92 76 of the parameter cells and the weakly informative test case reaches it everywhere the highly informative test case shows a posterior that is uni modal and similar to the reference solution the weakly informative test case shows a posterior that is multi modal and can be split using a clustering algorithm we visualize all possible scenarios and their respective probabilities 5 conclusion this work enables realistic inversion of channelized flow in the subsurface thereby it solves the categorical decision which facies is present and the heterogeneity within each facies in a hierarchical framework to achieve this goal we propose a novel mcmc algorithm that combines parallel tempering and sequential resampling this algorithm is an extension of laloy et al 2016 who only treated categorical fields compared to existing solution methods such as the enkf approach in xu and gómez hernández 2015 it converges to the true solution by design not just to an implicit quasi linearized solution we test our algorithm on a highly and weakly informative test case and it converges in both cases the mcmc converges although the posterior is extremely narrow in the highly informative test case and broad and multi modal in the weakly informative test case this shows the general applicability of our method credit authorship contribution statement sebastian reuschen conceptualization methodology software validation formal analysis investigation data curation writing original draft writing review editing visualization teng xu software resources writing review editing supervision wolfgang nowak conceptualization resources writing review editing supervision project administration funding acquisition declaration of competing interest the authors declare that they do not have any financial or nonfinancial conflict of interests acknowledgements funded by the deutsche forschungsgemeinschaft dfg german research foundation project number 327154368 sfb 1313 and project number 359880532 compu flow appendix a in the following we show the derivation of the detailed balance the detailed balance is defined as a 1 π θ i h θ i θ j π θ j h θ j θ i with the transition kernel h which is usually defined as a 2 h θ i θ j q θ i θ j α θ i θ j combining these equations the detailed balance can be written as a 3 π θ i q θ i θ j α θ i θ j π θ j q θ j θ i α θ j θ i inserting eq 3 it follows that a 4 p θ i l θ i q θ i θ j α θ i θ j p θ j l θ j q θ j θ i α θ j θ i after resorting this equation we find that a 5 α θ i θ j p θ j l θ j q θ j θ i p θ i l θ i q θ i θ j α θ j θ i combining this equation with the property of 0 α 1 one obtains that a 6 α θ i θ j m i n p θ j l θ j q θ j θ i p θ i l θ i q θ i θ j 1 appendix b in the following we show a proof that our algorithm fulfills the proposal distribution as specified in eq 11 and hence fulfills the detailed balance assuming that the functions uc and u f i are able to sample from the distribution pq θj c θ k γ and p q θ j f i θ k γ respectively first we partion the parameter space into b 1 θ i θ i k γ θ i k γ with the persistent parameters θ i k γ and the resampled parameters θ i k γ eq 16 defines the proposal function to be b 2 θ j g θ i u c v θ i u f 1 v θ i u f n f v θ i evidently if θ i k γ θ j k γ it follows that q θ i θ j q θ j θ i 0 which fulfills eq 11 thus we know that θ i k γ θ j k γ θ k γ we can resample the rest θi r of the parameter space based on fixed part θ k γ in the given setting we can express q θi θj as b 3 q θ i θ j p q θ j θ k γ p w θ k γ θ i θ k γ where pq θj θ k γ is the probability density of θj based on θ k γ given by some conditional re sampling method and pw θ k γ θi is the probability of the persistent data based on the previous sample assuming that we choose the parameter box indepently of the θi θj we know that b 4 p θ k γ θ i p s where p s is the probability that θ k γ is chosen one could imagine it as the probability to place the box at a certain position such that it excludes exactly θ k γ assuming non zero probabilities p s and p θ k γ it follows that b 5 q θ i θ j q θ j θ i p θ j θ k γ p s p θ i θ k γ p s p θ j θ k γ p θ i θ k γ p θ j θ k γ p θ k γ p θ i θ k γ p θ k γ p θ j p θ i which is equivalent to eq 11 p s 0 or p θ k γ 0 leads to q θ i θ j q θ j θ i 0 which fulfills eq 11 as well 
479,understanding fluid flow and transport within clay rock is essential for predicting caprock integrity in underground gas storage or as host rock in deep radioactive waste storage the connectivity and topology of the nanopore space which drive the transfer mechanisms of these materials are still poorly known and direct 3d imaging is particularly challenging in this work we investigate and compare different stochastic reconstruction approaches based on two point and multiple point statistics mps methods and using information from 2d training images for 3d volume rendering at submicron scale a particular emphasis is given to the maximal critical distance of sampling between two consecutive 2d images which is necessary to obtain a coherent 3d reconstruction of the nanopore structure we assess how these realizations honour various crucial transport properties of material namely permeability effective diffusion and longitudinal dispersion morphological features such as pore volume specific surface euler characteristic and tortuosity are used to analyze the results the methods are employed on a synthetic clay of nanometric porosity for which fib sem images are available results indicate that the 3da mps and weighted 3da mps approaches are the most suited for preserving pore space features and transport properties the choice depending on the level of conditioning data available keywords nanoporous rock digital rock physics mps method transport property lattice boltzmann simulation 1 introduction characterization of transport and retention properties of argillaceous rocks in the context of underground storage is a crucial topic for assessing the gas storage capacity or the reliability of long term evolution scenarios for radioactive wastes e g marschall et al 2005 it requires to determine with great precision their macroscopic properties e g intrinsic permeability retention curve relative permeability effective diffusion but the experimental acquisition of these data can be particularly challenging and time consuming and in some instances even unreliable due to intrinsic couplings with other physical processes or damages of the rock samples during essays there is therefore a very serious hope to significantly improve the characterization of these properties digitally by taking full advantage of advances in imaging at different scales andrä et al 2013a 2013b this is the aim of the so called digital rock physics drp approach based on the use of high resolution digital images of rocks so that the material properties are evaluated numerically at the pore scale blunt et al 2013 guibert et al 2015 tahmasebi et al 2017c shiota et al 2019 in addition to classical indirect methods such as mercury intrusion porosimetry mip or nuclear magnetic resonance relaxometry which give only access to pore size distribution the recent evolution of visualization techniques micro x ray computed tomography or μ ct in particular of 3d porous networks has led to an ever more detailed understanding of the pore structure blunt et al 2013 wildenschild and sheppard 2013 rozenbaum and du roscoat 2014 however the very low permeability materials have nanometric and micrometric pores whose μ ct description is still fragmented given the current technological limitations maximum accessible resolution within the micrometric range moreover a great care must be exercised to capture the pore geometry at the appropriate scale since the connectivity and topology of the pores play a particularly important role in driving the transfer mechanisms for these hygroscopic materials hu et al 2012 indeed although they may have a significant overall porosity e g of the order of 18 20 for argillite more than 90 consists of pores smaller than 100 nm micro and mesopores whose architecture remains poorly known their connectivity or not will contribute for instance to a more or less marked hysteresis during the drainage imbibition cycles whereas the shape of these pores can modify the transfer conditions locally scanning electron microscopy sem is still the most appropriate approach to obtain information on mesoporosity recent advances in bib or fib broad or focused ion beam methods have resulted in series of nanometer thick slices that can be imaged with sem to characterize the 3d pore space at a very high resolution level holzer et al 2010 desbois et al 2014 hemes et al 2015 song et al 2015 however besides the fact that it is a destructive method a better resolution implies a smaller studied volume and thus an increased difficulty to visualize a representative elementary volume rev and hence the interactions between nano and micro pores mehmani and prodanović 2014 the fib sem method is in fact constrained by the size of the 3d sample to be analyzed of the order of a few hundreds of μm 3 which remains generally lower than the rev blunt et al 2013 information can also be lost during fib preparation and cavity milling or from imaging artefacts caused by the interactions between beam and different density minerals lemmens et al 2011 finally transmission electron microscopy tem provides access to pore connectivity at smaller scales gaboreau et al 2016 but remains limited to 2d slices 3d reconstruction of the sample at this scale is still particularly delicate and requires electron tomography the complexity and sample size limitations when acquiring 3d high resolution images in comparison with the ease of imaging larger 2d surfaces at high resolution calls for another solution this alternative consists of using geostatistical methods to reconstruct the pore network from 2d sem or tem sections under the same or different orientations initially developed for reservoir rock applications they have been very recently employed on nanoporous materials yang et al 2015 wu et al 2018 various approaches have been considered for this purpose including markov chain of monte carlo mcmc simulations wu et al 2006 chen et al 2015 tahmasebi et al 2017a two point keehm 2004 or multiple point statistics okabe and blunt 2007 hajizadeh et al 2011 renard and straubhaar 2012a tahmasebi and sahimi 2013 wu et al 2018 process based methods oren and bakke 2002 2003 simulated annealing ju et al 2014 or some combination of these methods yang et al 2015 ju et al 2017 the digitalized 3d porous media after reconstruction are then employed in lattice boltzmann model lbm smooth particle hydrodynamics sph or pore network model pnm simulations for predicting the transport properties of the material mainly the intrinsic permeability but also water retention curve relative permeability and even klinkenberg effect or knudsen diffusion for specific applications to nanoporous rocks e g dymitrowska et al 2014 chen et al 2015 song et al 2017 davy and adler 2017 pazdniakou et al 2018 pazdniakou and dymitrowska 2018 wei et al 2019 if these reconstruction methods are obviously appealing a new bias appears the reconstruction of the 3d medium is no longer unique and depends on the interpolation methods used to link the discrete data a proper reconstruction giving a fair confidence in the accuracy of effective properties prediction notwithstanding the uncertainties generated by flow model calculations needs to be capable of addressing the following questions how to evaluate which reconstruction method s is are the most suited to capture the effective transport property of interest of the studied sample and how can we evaluate a priori the goodness of fit of the reconstructed geometries to which extent the data sparsity is acceptable in other words how many sampling images are required as input for constraining properly the 3d reconstructed domain and finally what is the impact on transport properties and the uncertainty generated by the reconstruction process in this work we attempt to give a comprehensive overview on these questions we explore the capability of various stochastic methods to provide a coherent 3d reconstruction of the nanoporous structure of one rock sample from 2d images several realisations are performed and the impact of the distance between two consecutive conditioning data is also discussed a general framework for analysing the reconstruction methods is proposed based on minkowski functionals since it is well known that permeability tends to average and filter a part of the lower scale microstructure information a set of morphological descriptors is used to assess the discrepancy between the original and the reconstructed microstructures finally three transport properties of interest longitudinal permeability effective diffusion and longitudinal dispersion are computed and results are discussed 2 methods for this comparative analysis we use a stack of fib sem serial section images of a synthetic clay made from compacted illite powder which has an overall porosity of 32 gaboreau et al 2016 the original sample measures 5480 5475 900 nm and was acquired at 5 nm resolution producing a grid of 1096 1095 180 voxels a preprocessing treatment has been first applied on raw fib sem images to remove artifacts then pore space was segmented using a custom made watershed method as described in gaboreau et al 2016 for computational efficiency the reconstruction methods were developed and tested on a smaller part of this domain a regular grid of 180 180 180 voxels fig 1 which is hereafter referred to as the reference 3d image note that we do not achieve with this sub sample the representative elementary volume rev of the rock material the characteristic length of the porosity based rev is of about 1µm according to gaboreau et al 2016 and it is probably much larger for the other transport property related rev however it does not affect the study results since the computed properties are compared with the ones of the reference 3d image and not with experimental measurements 2d regularly sampled parallel slices xy planes are then extracted and used as conditioning data for the 3d reconstruction in the following we analyse four reconstruction methods and investigate the optimum spacing required between the 2d acquisitions to maintain a correct estimate of hydrodynamic properties note that a stochastic approach over a large number of realizations is not applied in the present paper since we do not aim at confronting the predicted transport properties against experimental data 2 1 reconstruction methods various methods of 3d reconstructions of the porous medium have been developed in the recent years they can be classified between two types object based and pixel based methods a short survey of these methods is detailed below but we refer the reader to the recent review papers of tahmasebi 2018 or zhu et al 2019 for a more comprehensive overview on the advantages and disadvantages of each approach object based methods directly simulate shapes described by input geometrical or morphogenetical parameters among the object based methods pore and throats models also referred to as pore network models pnm are the most common the pores and throats are respectively assimilated to spheres and cylinders in early models these objects were placed following a reconstruction of the sedimentation process bakke and oren 1997 in other models they were randomly placed to reproduce the global porosity chen et al 2015 the most recent approaches use iterative processes based on genetic algorithms and target porosity and permeability values xu et al 2014 pixel based methods consist in affecting a value to each voxel of a grid there are three main approaches i the two point statistics types ii the markov chain monte carlo types mcmc and iii the multiple point statistics mps types the two point statistics simulations stochastically set values to voxels in order to reproduce an input property distribution the histogram and a spatial covariance provided by a variogram the mcmc simulations e g wu et al 2006 tahmasebi et al 2017a consist in a progressive filling of the medium according to a pre determined path and depending on the values of the previously simulated pixels the porosity of the medium determined from the analysis of 2d sections is an input parameter that influences the values affected at each voxel with several common points with mcmc methods mps simulations allow a stochastic reconstitution of a medium based on the analysis of training images that are conceptual models of the simulated media on which high order statistics can be extracted okabe and blunt 2007 renard and straubhaar 2012a chen et al 2018 they mainly differ from mcmc by the simulation path which is in mps completely random if object based methods have shown to be able to reproduce media with consistent hydraulic properties they need a high parametrization which is not easily compatible with 2d conditioning data and involves intensive computing oren and bakke 2002 on the contrary pixel based methods need less parametrization since all the information is computed from the input image data flexible and computationally efficient mcmc methods could be difficult to apply in case of spatially distributed conditioning data but two point and multiple point statistics simulation are designed to handle conditioning data whatever their location in the domain for these reasons we decided to focus on these two kinds of geostatistical methods to explore their respective ability to consistently reconstruct 3d porous medium from 2d regularly sampled parallel fib sem slices xy planes at nanometric scale for each reconstruction method three sampling distances between conditioning parallel slices were tested 5 voxels 11 voxels and 22 voxels respectively 25 55 and 110 nm sequential indicator simulation sis sequential indicator simulation is the classical two points statistics simulation method developed for categorical variables e g journel and isaaks 1984 deutsch et al 1997 its simplicity of numerical implementation made it one of the most common geostatistical methods available in several software a random path is defined between all voxels for each selected node an indicator kriging is performed using the neighbouring subsurface data previously simulated values and the global average porosity the node value is randomly drawn from the resulting local conditional distribution function this new simulated node is added to the known nodes this action is repeated until all grid nodes have been simulated in this paper we use the sis method implemented in the skua gocad software 1 1 http www pdgm com products skua gocad to carry out the simulation under the same conditions as those set by our application case we did not compute variograms on the 3d porous medium but only on the spaced conditioning slices thus on horizontal planes similar results maximal variations of 1 voxel are obtained for all distances between conditioning slices i e 5 11 and 22 voxels the experimental variograms show a slight anisotropy along the horizontal plane fig 2 the used model variogram is an exponential one with no nugget effect a sill of 0 226 and with minimal and maximal ranges of respectively 9 2 and 12 9 voxels as no vertical information is considered to be accessible in our case study the vertical range is arbitrary set to 10 voxels an average value of the horizontal ranges general principles of multiple point simulation mps the principles of multiple point statistics have been established by guardiano and srivastava 1993 to overcome the limitations of two points statistics when dealing with complex geometries of connected geological structures mps extract the spatial structures from a training image ti which is a conceptual representation of the modelled medium thus if the ti is anisotropic the realisations would reproduce this anisotropy strebelle 2002 in practice mps proceed sequentially at each successive location a data event centered around the simulated point is extracted from the simulation domain it regroups the known neighbouring voxels and their relative position around the simulated point the value assigned to the node is extracted from a cumulative distribution function cdf which has been built on the central nodes of ti patterns equal or similar to the data event several mps algorithms exist they differ by the way the cdf is built for example the first implementation of mps snesim strebelle 2002 proposes to scan the ti and store all the data templates in a tree while in impala straubhaar et al 2011 they are stored in a list the direct sampling algorithm ds mariethoz and renard 2010 mariethoz et al 2010 does not use a cdf but proposes to go through the training model on a random path and as soon as the difference between the event in the ti and the data event is inferior to a user defined threshold the corresponding central value is set to the simulated node quite soon after the initial algorithm of strebelle et al 2002 focused on a pixel by pixel simulation an alternative version of mps has also emerged called pattern based mps e g arpat and caers 2004 zhang et al 2005 in these methods the simulation processes pattern by pattern like a patchwork reconstruction several algorithms exist with the same hierarchy than for pixel based mps and some recent applications have been made to porous media reconstructions e g tahmasebi and sahimi 2013 tahmasebi et al 2017b a complete review can be found in tahmasebi 2018 in this work we did not intend to compare all existing methods but provide general guidelines to compare them we thus decided to focus on one category of mps the pixel based mps in our application case a main limitation for using classical algorithms of mps is that we want to reconstruct the porous media in 3d but only from parallel 2d sections thus we do not have a 3d ti at our disposal adaptations of mps have been proposed by several authors okabe and blunt 2007 hajizadeh et al 2011 renard and straubhaar 2012a ding et al 2018 in all works 2d ti are required for each orientation i e xy xz and yz planes in our case we consider having only xy plane data thus like okabe and blunt 2007 we are going to use xy images also for xz or yz ti this can be considered as a similar approach to taking an average horizontal range for vertical direction in sis and should limit the application to media with reduced anisotropy for strong anisotropic media different tis should be used one for each orthogonal planes we have implemented three derived versions of mps i a new implementation of the three directional aggregation mps method 3da mps proposed by okabe and blunt 2007 ii a slice sequential mps method s2dcd inspired by renard and straubhaar 2012a iii and a modified version of the 3da mps the weighted 3da mps 3da mps a method inspired by the three directional aggregation mps the principle of the 3da mps is to approximate the 3d conditional probability distribution function by a combination of the probabilities obtained with 2d mps performed along each direction fig 3 in okabe and blunt 2007 the 2d mps are performed with the impala algorithm straubhaar et al 2011 to gain in computational efficiency and handle the particular placement of the conditioning data in our application we implemented 3da mps with the ds algorithm mariethoz and renard 2010 which produces the same ensemble of stochastic realisations when used with the same parameters mariethoz et al 2010 for each randomly picked simulated node j three data events one for each orthogonal direction are extracted these data events dn j are defined by the number n of conditioning data or closest previously simulated voxels in a search radius l the user defined size of the template each ti i e for each direction is then explored starting from a random location and as soon as a similar data event dn k is met around the point k i e d dn j dn k t with t the user defined threshold value the value z k at the central point is selected the distance d dn j dn k between the data event and the ti event is the proportion of non matching voxels mariethoz and renard 2010 1 d d n j d n k 1 n i 1 n m i where m i 1 if z j i z k i 0 if z j i z k i the three values one coming from each direction are then aggregated to set the final simulated value different weighting schemes can be used and as already stated by comunian et al 2012b this choice has a big impact on the resulting simulations for the probability aggregation okabe and blunt 2007 use linear pooling formula while renard and straubhaar 2012a underlined the absence of external bayesianity property and propose to use bordley s formula instead in this work as the three ti images come from parallel xy sections there is no reason to prefer one ti over another and using linear pooling formula is adapted renard and straubhaar 2012a however at the beginning of the simulation when the data event is almost empty of conditioning data e g in the middle of two distanced slices for each direction the probability of setting a pore at the simulated node is almost equal to the porosity of the medium i e around 30 thus the probability of having a pore value in at least two of the three directional mps goes down to 11 and only 3 for having it in the three directional mps this point could strongly lead to a minimization of the global porosity particularly when spacing the conditioning slices thus to reduce this effect a simulated value is set to pore as soon as one of the three directional mps set the central value to pore we use a square 2d template of 9 9 voxels centred on the simulated value like proposed by okabe and blunt 2007 an acceptance threshold of 0 1 and a maximal number of iterations of 300 for the ds parametrization the ti used for the presented realizations fig 3 were randomly selected in the available 2d slices excluding the conditioning data slices s2dcd a method inspired by the sequential bi dimensional mps s2dcd the sequential bi dimensional mps simulation and conditioning data s2dcd proposed by renard and straubhaar 2012a differs from the method of okabe and blunt 2007 as instead of randomly selecting nodes in the 3d volume the media is built slices by slices the order of simulation of 2d surfaces is not completely random and as said by the authors should be customized according to the location of the conditioning data and to the shape of the simulation domain renard and straubhaar 2012a they advise to look for orders maximizing the number of conditioning data and alternating the different directions the mps are performed on the 2d surfaces using impala algorithm but the simulation uses a merged list whose construction is adapted to the particular case of 2d data events combination this merged list has also the advantage of indicating the 3d compatibility of the 2d ti in this paper we do not intend to propose a generic approach for all geological features at all scales but we focus only on the reconstruction of 3d porous medium at nanometric scale thus we only have two facies to manage grains and voids and our ti coming from the reference image should be compatible if the rock does not have a strong anisotropy thus we just implemented a simplified version of the s2dcd the s2dcd using direct sampling algorithm mariethoz and renard 2010 instead of impala and choosing randomly the slices with alternating directions fig 4 we keep the same ds parameters as in the former 3da mps approach but uses a unique but larger ti for all directions fig 4 weighted three directional aggregation mps weighted 3da mps a third class of the 3da mps was implemented motivated by the idea to limit the porosity minimization effect of the combination of three independent simulated values along the 2d planes it consists in modifying the way the distance d dn j dn k between the data event dn j and the configurations found in the ti dn k are compared in the previous methods the distance is simply equal to the proportion of non matching voxels as initially proposed by mariethoz and renard 2010 in weighted 3da mps implementation the distance d dn j dn k is a function of the porosity ϕ 2 d d n j d n k i 1 n m i c i where m i 1 ϕ if z j i z k i p o r e ϕ if z j i z k i r o c k 0 if z j i z k i and c i m i if z j i z k i ϕ if z j i z k i with this implementation not only the number of absolute voxel mismatchs counts but also the porosity of the observed ti events for an equal number of non equal voxel values if ϕ 50 the ti event with the highest porosity will be preferred if ϕ 50 it would be the lowest one in the application we used ϕ 30 which corresponds to the average porosity computed on the used ti images fig 3 2 2 post processing when performing reconstruction processes regardless how successful the pore space is digitalized some artifacts are generated which may affect computation of transport properties ortega ramírez et al 2019 pot et al 2020 they result both from image acquisition cavity milling etc image segmentation loss of pores during identification or from the reconstruction method itself and need to be managed these artifacts are manifested in different ways including curtaining effect disconnected pores or intensity variation and an appropriate advanced image processing has to be applied to mitigate them salzer et al 2015 zhu et al 2019 for instance isolated pores or solid voxels are often generated when using mps approaches and even more with sis part of the difficulty is that each artifact requires a specific image filtering method namely noise reduction sharpening shading correction and fft or median filtering but such a digital processing may also significantly modify the original image and hence alter the related transport properties gunda et al 2011 hereafter we chose to apply the same morphological post processing steps to all the 3d volumes independently of the reconstruction method used the image processing could be probably optimized by selecting the most appropriate filtering methods for each realization but it would also skew our comparative analysis as a consequence we removed first the non connected porosity and then we applied an opening of 1 voxel to suppress the isolated solid voxels in order to assess the impact of this post processing step it was also applied to the reference image and will be later labelled as post processed a comparison of alternative solutions of filtering including without postprocessing is investigated in a second step 2 3 image analysis in order to compare reconstructed images to the reference image and therefore to assess the performance of the reconstruction methods different morphological descriptors are selected armstrong et al 2019 provided a review and prospects of the use of minkowski functionals for characterization of porous media especially regarding single phase and multi phase flow properties for instance intrinsic permeability is classically related to the porosity for instance using kozeny carman equation but may also be linked to the euler characteristic scholz et al 2012 in the present study three minkowski functionals are used in regards to the pore structure the first functional computes the pore volume and thus the porosity ϕ the second functional allows calculation of the specific surface ss and the fourth functional corresponds to the euler characteristic χ the latter may also be computed from the betti numbers isolated pores loops and isolated solids in the present context porosity specific surface and euler characteristic were computed using the imagej plug in morpholibj legland et al 2016 the tortuosity is calculated as the ratio of the smallest geodesic distance shortest distance between two points within a given phase to the euclidian distance between the inlet face and the outlet face in the z direction the geodesic distance is computed by a series of dilations and intersections following the methodology described in gommes et al 2009 and the euclidian distance corresponds to the size of the sample the pore size distribution psd is computed using the imagej plug in xlib münch and holzer 2008 following the continuous psd approach on the 3d images 2 4 numerical upscaling the computed intrinsic permeability later refered as intrinsic permeability is computed using numerical upscaling the navier stokes equation for flow in the pore structure is solved using a lattice boltzmann approach the lattice boltzmann methods represent the fluid as fictive particles that undergo collision and propagation processes and lead to the calculation of particle density ρ and velocity field ux uy uz in the present study a d3q19 trt lbm aproach was used with bounce back fluid solid boundaries ginzburg et al 2008 pazdniakou and dymitrowska 2018 more details on the model may be found in pazdniakou and dymitrowska 2018 the original image was mirrored is the z direction and padded with walls on the lateral sides in order for periodic boundary conditions to be used flow in the porous system was modeled by a force f along the z direction pan et al 2006 the intrinsic permeability m 2 is computed from the density and velocity fields following eq 3 3 k z z δ 2 ν ρ u z v f where δ is the image resolution 5 nm ν is the viscosity in lattice units 1 6 v is the number of voxels in the mirrored image f 10 7 is the imposed force in lattice units and ρ and uz are the density and velocity in z direction fields respectively the computed effective diffusion and computed dispersion later refered as effective diffusion and dispersion respectively are computed using numerical upscaling the advection diffusion transport equation is solved using a d3q7 lbm following yang and chu 2013 the lbm methods thus allow the computation of the concentration field for the dispersion simulations the velocity field used is the one calculated from the flow equation normalized to reach an average z velocity of 0 005 in lattice units leading to an average local peclet of 3 3 similarly to yang and chu 2013 the transport is solved from an initial pulse considering periodic boundaries on all sides the initial pulse is set at z 90 voxels and the simulation is computed at least until the pulse spreads over 180 voxels corresponding to the original image size the method of moments garabedian et al 1991 is then applied as a post processing step in order to compute the effective diffusion and dispersion for a given imposed diffusion parameter 0 05 for the effective diffusion calculation and 0 002 for the dispersion calculation eq 4 as well as the mean displacement rate eq 5 4 d z z t z z 2 2 m 0 c d x d y d z 5 u z z t t z m 0 c d x d y d z where z is the mean displacement of the solute and m 0 c d x d y d z is the total solute injected in the system the effective diffusion and the dispersion computed are normalized against the imposed diffusion parameter 3 results and discussion the four reconstruction models sequential indicator simulation sis slice sequential mps s2dcd three directional aggregation mps 3da mps weighted three directional aggregation mps weighted 3da mps are applied to the reference image data considering training images with average porosity of 30 for the mps approaches see figs 3 and 4 the conditioning slices are normal to the z direction and spaced by 5 11 or 22 voxels fig 1 for each case five realizations were performed morphological parameters porosity specific surface euler characteristic tortuosity and pore size distributions are computed and compared to the reference image properties then effective transport properties permeability effective diffusion and dispersion for a given realization of each reconstruction approach and for each distance between conditioning data are compared finally the impact of the post processing is assessed 3 1 reference data the reference image fig 5 has a porosity of 34 4 with over 98 of the porosity interconnected and percolating and a specific surface of 113 5 μ m 1 its euler characteristic is of 2392 5 reflecting its connected pore structure the tortuosity of the reference image is quite low at a value of 1 05 the pore size distribution of the reference image fig 8 is rather uniformly graded between diameter 5 nm and 60 nm for the most part d 50 33 nm d 10 15 nm and d 90 58 nm the permeability of the sample along the z axis is of 4 10 18 m 2 the effective diffusion and longitudinal dispersion normalized against input diffusion are respectively of 0 43 and 12 4 in the reference image fig 5 larger solid structure with included porosity can be observed it should be noted that the post processing removes such included porosity the pores appear to be orientated preferably in the z direction reflecting the anisotropy that could be observed when performing a 3d variogram vertical range of 14 voxels it should be noted that the medium was compacted in the x axis direction 3 2 direct observation images of the first realization for each reconstruction approach and each distance between conditioning data are shown in fig 6 the 3da mps approach demonstrates a little of curtain like noise but allows good description of the larger grains in other words solid voxels planes or lines appear in a pore volume or inversely fig 6 zoomed areas the pores elongation in the z direction is well recovered for a distance of 5 voxels between conditioning data though this behaviour is diminished when the distance increases indeed at 22 voxels distance the pores have seemingly a more sphere like shape the s2dcd and weighted 3da mps approaches demonstrate heavier curtain like noise even after post processing the larger solid grains are well described albeit tend to disappear with less conditioning data especially for the weighted 3da mps approach similarly to the 3da mps method the pore elongations in the z direction can be observed with high conditioning and tend to decrease when the distance between conditioning data increases the weighted 3da mps approach appears to maintain the pore shapes better than the s2dcd reconstruction in low conditioning cases for all mps based method the loss of vertical continuity of pore shapes observed for 22 voxels spaced simulations is certainly linked to the chosen search radius size indeed a template size of 9 9 9 voxels allows simulations to start with almost always conditioning data in the data event for 5 and 11 voxel spaced cases in this last case only the points located exactly in the middle of two conditioning slices are not constrained at the beginning of the simulation but with a 22 voxel spaced slices almost 50 of the simulated points have empty data events at the beginning of the simulation favouring random mis connections of two consecutive slices using a higher search radius should certainly improve the results the sis realizations do not show curtain like noise but are fuzzier than mps reconstructions larger grains tend to be filled with connected porosity especially when the amount of conditioning data diminishes the pore elongations are barely observable from a distance of 11 voxels which is consistent with the input vertical range of 10 voxels using a higher vertical range should increase the pore elongation but supposes the anisotropy of the media is known at the beginning 3 3 morphological comparison variations of morphological parameters porosity specific surface euler characteristic and tortuosity compared to the reference sample are shown for each reconstructed approach in fig 7 the variations of the morphological parameters in between realizations remain reasonable even with limited conditioning data and are significantly lower than the discrepancy between the various reconstruction methods it supports the fact that the gap between methods can be investigated based on this comparative analysis the post processed reference image demonstrates an increase in porosity of 7 of the euler characteristic of 60 a decrease in specific surface of 18 and maintains the tortuosity the post processed reference psd translates the reference psd by roughly 5 nm 1 voxel fig 8 therefore leading to larger pore size the sis approach shows higher porosity than the reference fig 7 considering that the sis fits inherently the input porosity provided by the histogram from conditioning slices this higher value is due to the post processing the euler characteristic is also significantly higher than the one of the reference sample which indicates more loops within the pore volume and may be linked to the fuzziness observed on the images the specific surface and tortuosity are well described for the sis approach the error for porosity and tortuosity compared to the reference increases when distance between conditioning data increases whilst no specific trends can be determined for specific surface and euler characteristic the psd fig 8 of the sis reconstructed with a distance of 5 voxels superimposes the psd of the reference data when the distance between conditioning data increases however the sis tends to homogenize the pore sizes even further reducing the amount of larger pores the 3da mps shows very similar behaviour for distance 5 and 11 voxels fig 7 with good description of porosity and tortuosity underestimation by about 30 of the specific surface and by about 40 of the euler characteristic at a distance of 22 voxels between conditioning data the morphological parameters significantly differ with a loss in porosity of about half a large increase in tortuosity and a further increase of the euler characteristic error the specific surface is however improved the 3da mps approach leads to larger pores compared to both the reference and the post processed reference at the largest distance however the psd of the 3da mps reconstruction fig 8 is very similar to the one of the post processed sample except for larger pores it should be noted that the evolution of specific surface and psd for 3da mps is similar the 3da mps images with smaller conditionning distance lead to good representation of the larger pores as observed in the previous section yet the interface details with smaller characteristic length are not as well represented especially due to the post processing therefore the specific surface is significantly lower and the psd tends to favor medium size pores over smaller size pores the s2dcd reconstructions demonstrate clear correlations with the amount of conditioning data fig 7 indeed the porosity and euler characteristics decrease with increasing distance and conversely specific surface and tortuosity increase in regards to the error between realizations and the reference sample there is a steady increase of this error for porosity and tortuosity the s2dcd psd fig 8 is very close to the post processed reference at a distance of 5 voxels with a diminution of the pore size of roughly 1 voxel with increasing distance between conditioning data the weighted 3da mps approach provides consistent morphological characteristics with the reference sample even with little conditioning fig 7 at a conditioning distance of 5 voxels the weighted 3da mps reconstruction is however closer to the post processed sample than to the reference sample at a larger distance the porosity diminishes slightly and the tortuosity increases albeit much less than the ones from s2dcd and 3da mps realizations similarly to the s2dcd the weighted 3da mps psd fig 8 is close to the post processed psd with high amount of conditioning data however pore sizes seemingly increases by about 1 voxel for lesser conditioning it should also be noted that the variation of the psd between realizations is more important for the weighted 3da mps approach than for the other methods especially with little conditioning 3 4 effective properties given the very low variability over realizations of these various morphological parameters comparative analysis of effective properties is conducted hereafter for a single realization of each reconstruction method but for different spacings between conditioning data quantifying the uncertainty in transport properties would require to perform simulations over a large ensemble of stochastic realizations and will not be addressed here variations of effective properties namely the permeability effective diffusion and dispersion are provided in fig 9 first we observe that the post processed sample shows a higher permeability and effective diffusion than the reference sample likely linked with the increase in pore size diameter and subsequent increase in porosity the dispersion is however lowered by about 30 compared to the reference value as stated in section 2 4 the dispersion computations were performed for a local average peclet of 3 3 considering the median pore size as characteristic length but for some realizations s2dcd and 3da mps with 22 voxels distance the ratio between maximum velocity and average velocity in the z direction was too important leading to significantly locally higher peclet and thus to divergence of the transport simulation the acceptable ratio is roughly below 30 leading to the limit peclet of 100 stated in yang and chu 2013 it should be noted that aside from s2dcd and 3da mps realizations at the largest distance between conditioning data the permeability effective diffusion and dispersion for all methods remains within the same order of magnitude the sis approach shows a steady decrease of the permeability and effective diffusion with the increase of the distance between conditioning data despite an increase in porosity for the permeability this behaviour is explained by both the decrease in pore size and the increase in tortuosity the mps methods except for the weighted 3da mps 11 case demonstrate a decrease in permeability and effective diffusion when the amount of conditioning data is decreasing this is consistent with the increase of porosity and of tortuosity observed on the realizations the permeability of the weighted 3da mps approach for 5 and 11 voxels distance and sis 11 shows good agreement with the reference while the 3da 5 3da 11 and sis 5 have better agreement with the post processed reference data all these six cases demonstrate good agreement with the reference data for effective diffusion the s2dcd and the 22 voxels distance realizations underestimate however the permeability and the effective diffusion more significantly this is coherent with the lower porosity and the larger tortuosity of these reconstructions the evolution of permeability with conditioning distance cannot however be only correlated to the porosity and tortuosity indeed these morphological properties show little variation for the weighted 3da mps and the evolution of permeability does not follow the trend defined by the euler characteristic either therefore it is likely that other morphological properties would be required to properly explain the permeability variation with conditioning distance it should be noted that the underestimation of specific surface and discrepancy in the psd observed for 3da 5 and 3da 11 do not seem to significantly influence the computed effective properties in order to further illustrate the transport and therefore the dispersion the normalized velocity fields and the normalized concentration at a given time are shown in fig 10 11 the reference sample demonstrates preferential pathways fig 10 this leads to a very inhomogeneous distribution of concentration with finger like progress of the concentration front along the preferential pathways fig 11 such a behaviour is observed in 3da 5 3da 11 and to a lesser extent in the weighted 3da mps realizations consequently these cases have a dispersion close to the reference the flow pattern for 3da 22 s2dcd 11 and s2dcd 22 differs significantly from the other mps simulations fig 10 indeed the flow seems more structured with pores generated around the conditioning data mostly normal to the z direction and smaller links joining the conditioning slices such patterns explain the larger tortuosities despite this the s2dcd 5 and s2dcd 11 realizations still maintain preferential flow in z direction albeit less than the reference samples the velocity field of the sis reconstructions are more homogeneous leading to a more horizontally spread concentration front and therefore to a smaller dispersion 3 5 impact of post processing it is interesting to note that the effective properties of 3da 5 and sis 5 resemble more those of the post processed reference than those of the reference on top of that as stated previously curtain like noise is still significant in the s2dcd and weighted 3da mps realizations therefore it is important to assess the impact of post processing if any on the morphological and effective properties to this end these properties were computed on 3da 5 3da 11 and sis 5 where no post processing was applied and on weighted 3da mps 5 weighted 3da mps 11 and s2dcd 5 where a median filter was applied as post processing the median was a 3d filter with 2 voxels radius if the result of the median filter led to an uncertain value of the voxel i e the voxel has the same amount of neighbouring solid and pore voxel the voxel was considered as a pore voxel fig 12 illustrates the impact of the post processing or lack of on a given image on the no filter images the isolated voxels pore and solid generated with the weighted 3da mps approach can be easily spotted the initial post processing effectively removes such features while the median filter further smooths the pore and solid interfaces removing the post processing led to a loss of porosity which was increased by the post processing and an increase in specific surface due to non connected porosity the lower porosity might be due to the fact that the ti average porosity is lower than the reference porosity despite the loss in porosity the 3da mps realizations without post processing showed good agreement with the reference both in regards to morphological and transport properties this is especially true at 5 voxels distance between conditioning data where the realization properties are almost identical to the reference for the sis realization however the lack of post processing did not lead to improvement indeed the error for specific surface but also for effective diffusion remains and the dispersion could not be computed the permeability for the sis sample is significantly lower than the post processed one yet the discrepancy with the reference value remains similar median filtering provides much better removal of the curtain like noise compared to the initially chosen post processing in regards to morphological properties median filtering led to a decrease in porosity specific surface and to a lesser extent of the tortuosity fig 13 the pore size distribution is however mostly translated towards larger pores by 1 to 1 5 voxels with smaller size pores disappearing the effective diffusivities fig 14 are increasing despite the loss in porosity permeability increases for the weighted 3da mps 5 voxels decreases for the weighted 3da mps 11 voxels and is maintained for the s2dcd filtered realizations dispersion is increased for both weighted 3da mps simulations unlike in the previous post processing dispersion computation could be performed for the s2dcd 5 reconstruction and shows significant overestimation compared to the reference overall whilst the prediction of morphological properties porosity and specific surface were degraded compared to the previous post processing and even though the change of filter leads to modification of the effective properties there is no evident improvement or degradation of the transport properties with median filtering moreover the general behaviour of the reconstructed samples for each method remains similar to the one using the initial post processing with the weighted 3da mps realizations showing relatively good agreement with the reference and the s2dcd 5 realization being less accurate 4 conclusion in this study we investigated the capability of different pixel based methods to reconstruct a coherent 3d nanopore space of clay rock from parallel 2d images as we could obtain from fib sem or tem imaging the studied media was almost isotropic and the different methods we used would have to be adapted in the case of a strong anisotropic media notably by i choosing dedicated tis for each orthogonal directions and ii eventually adapting the template size which should be bigger in the main anisotropy direction the aim was not to make an exhaustive comparison of all existing reconstruction methods nor to find an universal best one but to propose based on a real sample study general guidelines i to choose the most appropriate method and ii to identify the accurate sampling distance when dealing with spaced parallel cross sections because a direct 3d comparison does not reflect the relevance of the methods we also analysed the amount of morphological features that was preserved this is achieved by comparing minkowski functionals namely pore volume specific surface and euler characteristic as well as tortuosity and psd given the low variability of these various morphological criteria over realizations we assessed how these 3d reconstructions honour the transport properties of interest from a given realization and we discussed how the sampling distance should be chosen according to the nature of the properties at stake from the results described in this work the following conclusions can be put forward the sis approach reveals a good agreement of averaged properties but also a lack of description of preferential patterns as indicated by the observed discrepancy for the euler characteristic and hence for the predicted dispersion coefficient in addition to exhibiting artefacts the relevance of s2dcd approach is highly dependent on the amount of conditioning data the image reconstruction is satisfying with high conditioning but quickly degrades according to renard and straubhaar 2012a the path of reconstruction is crucial it is very likely that an improvement of the path would lead to better results on the contrary the 3da mps approach exhibits very good results for all the transport properties when the density of conditioning data is large but with no post processing required when the distance between conditioning data is close to the characteristic length as obtained from the variogram the reconstruction remains acceptable but degrades strongly afterwards finally the weighted 3da mps approach shows better consistency with decreasing conditioning data but leads to noise in the reconstructed images and is not as accurate at high conditioning as 3da mps it calls for more advanced weighting and post processing steps permeability and effective diffusion were shown to be strongly related to morphological parameters such as porosity and tortuosity no evident correlation between dispersion and the measured morphological properties was however put forward this calls for other morphological characterisation to fully assess the acceptability of a given realisation in regards to this transport process indeed dispersion further discriminates the reconstruction approach in comparison to permeability and effective diffusion concerning the sampling distance it is clear that choosing a spacing inferior to the vertical range of correlation in our case 14 voxels clearly facilitates a satisfying 3d reconstruction in practice this also supposes to be capable of estimating this range and thus of performing a variogram analysis in the orthogonal direction to the further sampling this upper limit of sampling distance is particularly critical for sis but mps based methods could be less impacted since increasing the maximum search distance would partially mitigate this in a more comprehensive way this is a severe disadvantage for application to nanoporous rocks which exhibit a large pore size distribution and multi scale features with different correlation lengths here we face a classical drawback of mps methods which are found to be good in conditioning the point data but fail to reproduce long distance connectivity tahmasebi 2018 as a conclusion this study provides a general guideline for deciding which method will predict the best 3d reconstruction results depending on the amount of 2d images available and the transport property targeted for prediction these findings are however valuable for the specific conditions we used and as previously said the studied media was almost isotropic mps based methods imply various parameters among them the choice of the ti the search radius size and in this specific application the value aggregation method for 3da mps methods and the similarity computation between events all these parameters strongly impact the results and a sensitivity analysis implying all these aspects could be performed to refine the recommendations on the most suitable reconstruction method our comparative analysis could also be extended to more recent algorithms for example a hybrid approach combining mps method with object based or pattern based method as proposed by tahmasebi 2017 could certainly be a research route for future improvements as well as the ones combining the forces of mps and deep learning kamrava et al 2019 it should be also kept in mind that determining the most suited reconstruction method for predicting the effective property of interest does not prevent from quantifying the uncertainties an ensemble of stochastic realizations would be ultimately required to estimate the distribution of transport property and the averaged value for drp application to real porous media credit authorship contribution statement anne julie tinet methodology software validation visualization data curation supervision writing original draft quentin corlay software investigation writing original draft pauline collon methodology supervision visualization writing review editing fabrice golfier conceptualization supervision visualization writing review editing project administration funding acquisition kassem kalo investigation resources declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the financial support from needs mipor rpm project is gratefully acknowledged this work was also partially funded by the iceel carnot institute multiec hydro project we are thankful to stephane gaboreau who provided the digital images of the compacted illite used in this article we also thank emerson paradigm for providing the skua gocad software and api 
479,understanding fluid flow and transport within clay rock is essential for predicting caprock integrity in underground gas storage or as host rock in deep radioactive waste storage the connectivity and topology of the nanopore space which drive the transfer mechanisms of these materials are still poorly known and direct 3d imaging is particularly challenging in this work we investigate and compare different stochastic reconstruction approaches based on two point and multiple point statistics mps methods and using information from 2d training images for 3d volume rendering at submicron scale a particular emphasis is given to the maximal critical distance of sampling between two consecutive 2d images which is necessary to obtain a coherent 3d reconstruction of the nanopore structure we assess how these realizations honour various crucial transport properties of material namely permeability effective diffusion and longitudinal dispersion morphological features such as pore volume specific surface euler characteristic and tortuosity are used to analyze the results the methods are employed on a synthetic clay of nanometric porosity for which fib sem images are available results indicate that the 3da mps and weighted 3da mps approaches are the most suited for preserving pore space features and transport properties the choice depending on the level of conditioning data available keywords nanoporous rock digital rock physics mps method transport property lattice boltzmann simulation 1 introduction characterization of transport and retention properties of argillaceous rocks in the context of underground storage is a crucial topic for assessing the gas storage capacity or the reliability of long term evolution scenarios for radioactive wastes e g marschall et al 2005 it requires to determine with great precision their macroscopic properties e g intrinsic permeability retention curve relative permeability effective diffusion but the experimental acquisition of these data can be particularly challenging and time consuming and in some instances even unreliable due to intrinsic couplings with other physical processes or damages of the rock samples during essays there is therefore a very serious hope to significantly improve the characterization of these properties digitally by taking full advantage of advances in imaging at different scales andrä et al 2013a 2013b this is the aim of the so called digital rock physics drp approach based on the use of high resolution digital images of rocks so that the material properties are evaluated numerically at the pore scale blunt et al 2013 guibert et al 2015 tahmasebi et al 2017c shiota et al 2019 in addition to classical indirect methods such as mercury intrusion porosimetry mip or nuclear magnetic resonance relaxometry which give only access to pore size distribution the recent evolution of visualization techniques micro x ray computed tomography or μ ct in particular of 3d porous networks has led to an ever more detailed understanding of the pore structure blunt et al 2013 wildenschild and sheppard 2013 rozenbaum and du roscoat 2014 however the very low permeability materials have nanometric and micrometric pores whose μ ct description is still fragmented given the current technological limitations maximum accessible resolution within the micrometric range moreover a great care must be exercised to capture the pore geometry at the appropriate scale since the connectivity and topology of the pores play a particularly important role in driving the transfer mechanisms for these hygroscopic materials hu et al 2012 indeed although they may have a significant overall porosity e g of the order of 18 20 for argillite more than 90 consists of pores smaller than 100 nm micro and mesopores whose architecture remains poorly known their connectivity or not will contribute for instance to a more or less marked hysteresis during the drainage imbibition cycles whereas the shape of these pores can modify the transfer conditions locally scanning electron microscopy sem is still the most appropriate approach to obtain information on mesoporosity recent advances in bib or fib broad or focused ion beam methods have resulted in series of nanometer thick slices that can be imaged with sem to characterize the 3d pore space at a very high resolution level holzer et al 2010 desbois et al 2014 hemes et al 2015 song et al 2015 however besides the fact that it is a destructive method a better resolution implies a smaller studied volume and thus an increased difficulty to visualize a representative elementary volume rev and hence the interactions between nano and micro pores mehmani and prodanović 2014 the fib sem method is in fact constrained by the size of the 3d sample to be analyzed of the order of a few hundreds of μm 3 which remains generally lower than the rev blunt et al 2013 information can also be lost during fib preparation and cavity milling or from imaging artefacts caused by the interactions between beam and different density minerals lemmens et al 2011 finally transmission electron microscopy tem provides access to pore connectivity at smaller scales gaboreau et al 2016 but remains limited to 2d slices 3d reconstruction of the sample at this scale is still particularly delicate and requires electron tomography the complexity and sample size limitations when acquiring 3d high resolution images in comparison with the ease of imaging larger 2d surfaces at high resolution calls for another solution this alternative consists of using geostatistical methods to reconstruct the pore network from 2d sem or tem sections under the same or different orientations initially developed for reservoir rock applications they have been very recently employed on nanoporous materials yang et al 2015 wu et al 2018 various approaches have been considered for this purpose including markov chain of monte carlo mcmc simulations wu et al 2006 chen et al 2015 tahmasebi et al 2017a two point keehm 2004 or multiple point statistics okabe and blunt 2007 hajizadeh et al 2011 renard and straubhaar 2012a tahmasebi and sahimi 2013 wu et al 2018 process based methods oren and bakke 2002 2003 simulated annealing ju et al 2014 or some combination of these methods yang et al 2015 ju et al 2017 the digitalized 3d porous media after reconstruction are then employed in lattice boltzmann model lbm smooth particle hydrodynamics sph or pore network model pnm simulations for predicting the transport properties of the material mainly the intrinsic permeability but also water retention curve relative permeability and even klinkenberg effect or knudsen diffusion for specific applications to nanoporous rocks e g dymitrowska et al 2014 chen et al 2015 song et al 2017 davy and adler 2017 pazdniakou et al 2018 pazdniakou and dymitrowska 2018 wei et al 2019 if these reconstruction methods are obviously appealing a new bias appears the reconstruction of the 3d medium is no longer unique and depends on the interpolation methods used to link the discrete data a proper reconstruction giving a fair confidence in the accuracy of effective properties prediction notwithstanding the uncertainties generated by flow model calculations needs to be capable of addressing the following questions how to evaluate which reconstruction method s is are the most suited to capture the effective transport property of interest of the studied sample and how can we evaluate a priori the goodness of fit of the reconstructed geometries to which extent the data sparsity is acceptable in other words how many sampling images are required as input for constraining properly the 3d reconstructed domain and finally what is the impact on transport properties and the uncertainty generated by the reconstruction process in this work we attempt to give a comprehensive overview on these questions we explore the capability of various stochastic methods to provide a coherent 3d reconstruction of the nanoporous structure of one rock sample from 2d images several realisations are performed and the impact of the distance between two consecutive conditioning data is also discussed a general framework for analysing the reconstruction methods is proposed based on minkowski functionals since it is well known that permeability tends to average and filter a part of the lower scale microstructure information a set of morphological descriptors is used to assess the discrepancy between the original and the reconstructed microstructures finally three transport properties of interest longitudinal permeability effective diffusion and longitudinal dispersion are computed and results are discussed 2 methods for this comparative analysis we use a stack of fib sem serial section images of a synthetic clay made from compacted illite powder which has an overall porosity of 32 gaboreau et al 2016 the original sample measures 5480 5475 900 nm and was acquired at 5 nm resolution producing a grid of 1096 1095 180 voxels a preprocessing treatment has been first applied on raw fib sem images to remove artifacts then pore space was segmented using a custom made watershed method as described in gaboreau et al 2016 for computational efficiency the reconstruction methods were developed and tested on a smaller part of this domain a regular grid of 180 180 180 voxels fig 1 which is hereafter referred to as the reference 3d image note that we do not achieve with this sub sample the representative elementary volume rev of the rock material the characteristic length of the porosity based rev is of about 1µm according to gaboreau et al 2016 and it is probably much larger for the other transport property related rev however it does not affect the study results since the computed properties are compared with the ones of the reference 3d image and not with experimental measurements 2d regularly sampled parallel slices xy planes are then extracted and used as conditioning data for the 3d reconstruction in the following we analyse four reconstruction methods and investigate the optimum spacing required between the 2d acquisitions to maintain a correct estimate of hydrodynamic properties note that a stochastic approach over a large number of realizations is not applied in the present paper since we do not aim at confronting the predicted transport properties against experimental data 2 1 reconstruction methods various methods of 3d reconstructions of the porous medium have been developed in the recent years they can be classified between two types object based and pixel based methods a short survey of these methods is detailed below but we refer the reader to the recent review papers of tahmasebi 2018 or zhu et al 2019 for a more comprehensive overview on the advantages and disadvantages of each approach object based methods directly simulate shapes described by input geometrical or morphogenetical parameters among the object based methods pore and throats models also referred to as pore network models pnm are the most common the pores and throats are respectively assimilated to spheres and cylinders in early models these objects were placed following a reconstruction of the sedimentation process bakke and oren 1997 in other models they were randomly placed to reproduce the global porosity chen et al 2015 the most recent approaches use iterative processes based on genetic algorithms and target porosity and permeability values xu et al 2014 pixel based methods consist in affecting a value to each voxel of a grid there are three main approaches i the two point statistics types ii the markov chain monte carlo types mcmc and iii the multiple point statistics mps types the two point statistics simulations stochastically set values to voxels in order to reproduce an input property distribution the histogram and a spatial covariance provided by a variogram the mcmc simulations e g wu et al 2006 tahmasebi et al 2017a consist in a progressive filling of the medium according to a pre determined path and depending on the values of the previously simulated pixels the porosity of the medium determined from the analysis of 2d sections is an input parameter that influences the values affected at each voxel with several common points with mcmc methods mps simulations allow a stochastic reconstitution of a medium based on the analysis of training images that are conceptual models of the simulated media on which high order statistics can be extracted okabe and blunt 2007 renard and straubhaar 2012a chen et al 2018 they mainly differ from mcmc by the simulation path which is in mps completely random if object based methods have shown to be able to reproduce media with consistent hydraulic properties they need a high parametrization which is not easily compatible with 2d conditioning data and involves intensive computing oren and bakke 2002 on the contrary pixel based methods need less parametrization since all the information is computed from the input image data flexible and computationally efficient mcmc methods could be difficult to apply in case of spatially distributed conditioning data but two point and multiple point statistics simulation are designed to handle conditioning data whatever their location in the domain for these reasons we decided to focus on these two kinds of geostatistical methods to explore their respective ability to consistently reconstruct 3d porous medium from 2d regularly sampled parallel fib sem slices xy planes at nanometric scale for each reconstruction method three sampling distances between conditioning parallel slices were tested 5 voxels 11 voxels and 22 voxels respectively 25 55 and 110 nm sequential indicator simulation sis sequential indicator simulation is the classical two points statistics simulation method developed for categorical variables e g journel and isaaks 1984 deutsch et al 1997 its simplicity of numerical implementation made it one of the most common geostatistical methods available in several software a random path is defined between all voxels for each selected node an indicator kriging is performed using the neighbouring subsurface data previously simulated values and the global average porosity the node value is randomly drawn from the resulting local conditional distribution function this new simulated node is added to the known nodes this action is repeated until all grid nodes have been simulated in this paper we use the sis method implemented in the skua gocad software 1 1 http www pdgm com products skua gocad to carry out the simulation under the same conditions as those set by our application case we did not compute variograms on the 3d porous medium but only on the spaced conditioning slices thus on horizontal planes similar results maximal variations of 1 voxel are obtained for all distances between conditioning slices i e 5 11 and 22 voxels the experimental variograms show a slight anisotropy along the horizontal plane fig 2 the used model variogram is an exponential one with no nugget effect a sill of 0 226 and with minimal and maximal ranges of respectively 9 2 and 12 9 voxels as no vertical information is considered to be accessible in our case study the vertical range is arbitrary set to 10 voxels an average value of the horizontal ranges general principles of multiple point simulation mps the principles of multiple point statistics have been established by guardiano and srivastava 1993 to overcome the limitations of two points statistics when dealing with complex geometries of connected geological structures mps extract the spatial structures from a training image ti which is a conceptual representation of the modelled medium thus if the ti is anisotropic the realisations would reproduce this anisotropy strebelle 2002 in practice mps proceed sequentially at each successive location a data event centered around the simulated point is extracted from the simulation domain it regroups the known neighbouring voxels and their relative position around the simulated point the value assigned to the node is extracted from a cumulative distribution function cdf which has been built on the central nodes of ti patterns equal or similar to the data event several mps algorithms exist they differ by the way the cdf is built for example the first implementation of mps snesim strebelle 2002 proposes to scan the ti and store all the data templates in a tree while in impala straubhaar et al 2011 they are stored in a list the direct sampling algorithm ds mariethoz and renard 2010 mariethoz et al 2010 does not use a cdf but proposes to go through the training model on a random path and as soon as the difference between the event in the ti and the data event is inferior to a user defined threshold the corresponding central value is set to the simulated node quite soon after the initial algorithm of strebelle et al 2002 focused on a pixel by pixel simulation an alternative version of mps has also emerged called pattern based mps e g arpat and caers 2004 zhang et al 2005 in these methods the simulation processes pattern by pattern like a patchwork reconstruction several algorithms exist with the same hierarchy than for pixel based mps and some recent applications have been made to porous media reconstructions e g tahmasebi and sahimi 2013 tahmasebi et al 2017b a complete review can be found in tahmasebi 2018 in this work we did not intend to compare all existing methods but provide general guidelines to compare them we thus decided to focus on one category of mps the pixel based mps in our application case a main limitation for using classical algorithms of mps is that we want to reconstruct the porous media in 3d but only from parallel 2d sections thus we do not have a 3d ti at our disposal adaptations of mps have been proposed by several authors okabe and blunt 2007 hajizadeh et al 2011 renard and straubhaar 2012a ding et al 2018 in all works 2d ti are required for each orientation i e xy xz and yz planes in our case we consider having only xy plane data thus like okabe and blunt 2007 we are going to use xy images also for xz or yz ti this can be considered as a similar approach to taking an average horizontal range for vertical direction in sis and should limit the application to media with reduced anisotropy for strong anisotropic media different tis should be used one for each orthogonal planes we have implemented three derived versions of mps i a new implementation of the three directional aggregation mps method 3da mps proposed by okabe and blunt 2007 ii a slice sequential mps method s2dcd inspired by renard and straubhaar 2012a iii and a modified version of the 3da mps the weighted 3da mps 3da mps a method inspired by the three directional aggregation mps the principle of the 3da mps is to approximate the 3d conditional probability distribution function by a combination of the probabilities obtained with 2d mps performed along each direction fig 3 in okabe and blunt 2007 the 2d mps are performed with the impala algorithm straubhaar et al 2011 to gain in computational efficiency and handle the particular placement of the conditioning data in our application we implemented 3da mps with the ds algorithm mariethoz and renard 2010 which produces the same ensemble of stochastic realisations when used with the same parameters mariethoz et al 2010 for each randomly picked simulated node j three data events one for each orthogonal direction are extracted these data events dn j are defined by the number n of conditioning data or closest previously simulated voxels in a search radius l the user defined size of the template each ti i e for each direction is then explored starting from a random location and as soon as a similar data event dn k is met around the point k i e d dn j dn k t with t the user defined threshold value the value z k at the central point is selected the distance d dn j dn k between the data event and the ti event is the proportion of non matching voxels mariethoz and renard 2010 1 d d n j d n k 1 n i 1 n m i where m i 1 if z j i z k i 0 if z j i z k i the three values one coming from each direction are then aggregated to set the final simulated value different weighting schemes can be used and as already stated by comunian et al 2012b this choice has a big impact on the resulting simulations for the probability aggregation okabe and blunt 2007 use linear pooling formula while renard and straubhaar 2012a underlined the absence of external bayesianity property and propose to use bordley s formula instead in this work as the three ti images come from parallel xy sections there is no reason to prefer one ti over another and using linear pooling formula is adapted renard and straubhaar 2012a however at the beginning of the simulation when the data event is almost empty of conditioning data e g in the middle of two distanced slices for each direction the probability of setting a pore at the simulated node is almost equal to the porosity of the medium i e around 30 thus the probability of having a pore value in at least two of the three directional mps goes down to 11 and only 3 for having it in the three directional mps this point could strongly lead to a minimization of the global porosity particularly when spacing the conditioning slices thus to reduce this effect a simulated value is set to pore as soon as one of the three directional mps set the central value to pore we use a square 2d template of 9 9 voxels centred on the simulated value like proposed by okabe and blunt 2007 an acceptance threshold of 0 1 and a maximal number of iterations of 300 for the ds parametrization the ti used for the presented realizations fig 3 were randomly selected in the available 2d slices excluding the conditioning data slices s2dcd a method inspired by the sequential bi dimensional mps s2dcd the sequential bi dimensional mps simulation and conditioning data s2dcd proposed by renard and straubhaar 2012a differs from the method of okabe and blunt 2007 as instead of randomly selecting nodes in the 3d volume the media is built slices by slices the order of simulation of 2d surfaces is not completely random and as said by the authors should be customized according to the location of the conditioning data and to the shape of the simulation domain renard and straubhaar 2012a they advise to look for orders maximizing the number of conditioning data and alternating the different directions the mps are performed on the 2d surfaces using impala algorithm but the simulation uses a merged list whose construction is adapted to the particular case of 2d data events combination this merged list has also the advantage of indicating the 3d compatibility of the 2d ti in this paper we do not intend to propose a generic approach for all geological features at all scales but we focus only on the reconstruction of 3d porous medium at nanometric scale thus we only have two facies to manage grains and voids and our ti coming from the reference image should be compatible if the rock does not have a strong anisotropy thus we just implemented a simplified version of the s2dcd the s2dcd using direct sampling algorithm mariethoz and renard 2010 instead of impala and choosing randomly the slices with alternating directions fig 4 we keep the same ds parameters as in the former 3da mps approach but uses a unique but larger ti for all directions fig 4 weighted three directional aggregation mps weighted 3da mps a third class of the 3da mps was implemented motivated by the idea to limit the porosity minimization effect of the combination of three independent simulated values along the 2d planes it consists in modifying the way the distance d dn j dn k between the data event dn j and the configurations found in the ti dn k are compared in the previous methods the distance is simply equal to the proportion of non matching voxels as initially proposed by mariethoz and renard 2010 in weighted 3da mps implementation the distance d dn j dn k is a function of the porosity ϕ 2 d d n j d n k i 1 n m i c i where m i 1 ϕ if z j i z k i p o r e ϕ if z j i z k i r o c k 0 if z j i z k i and c i m i if z j i z k i ϕ if z j i z k i with this implementation not only the number of absolute voxel mismatchs counts but also the porosity of the observed ti events for an equal number of non equal voxel values if ϕ 50 the ti event with the highest porosity will be preferred if ϕ 50 it would be the lowest one in the application we used ϕ 30 which corresponds to the average porosity computed on the used ti images fig 3 2 2 post processing when performing reconstruction processes regardless how successful the pore space is digitalized some artifacts are generated which may affect computation of transport properties ortega ramírez et al 2019 pot et al 2020 they result both from image acquisition cavity milling etc image segmentation loss of pores during identification or from the reconstruction method itself and need to be managed these artifacts are manifested in different ways including curtaining effect disconnected pores or intensity variation and an appropriate advanced image processing has to be applied to mitigate them salzer et al 2015 zhu et al 2019 for instance isolated pores or solid voxels are often generated when using mps approaches and even more with sis part of the difficulty is that each artifact requires a specific image filtering method namely noise reduction sharpening shading correction and fft or median filtering but such a digital processing may also significantly modify the original image and hence alter the related transport properties gunda et al 2011 hereafter we chose to apply the same morphological post processing steps to all the 3d volumes independently of the reconstruction method used the image processing could be probably optimized by selecting the most appropriate filtering methods for each realization but it would also skew our comparative analysis as a consequence we removed first the non connected porosity and then we applied an opening of 1 voxel to suppress the isolated solid voxels in order to assess the impact of this post processing step it was also applied to the reference image and will be later labelled as post processed a comparison of alternative solutions of filtering including without postprocessing is investigated in a second step 2 3 image analysis in order to compare reconstructed images to the reference image and therefore to assess the performance of the reconstruction methods different morphological descriptors are selected armstrong et al 2019 provided a review and prospects of the use of minkowski functionals for characterization of porous media especially regarding single phase and multi phase flow properties for instance intrinsic permeability is classically related to the porosity for instance using kozeny carman equation but may also be linked to the euler characteristic scholz et al 2012 in the present study three minkowski functionals are used in regards to the pore structure the first functional computes the pore volume and thus the porosity ϕ the second functional allows calculation of the specific surface ss and the fourth functional corresponds to the euler characteristic χ the latter may also be computed from the betti numbers isolated pores loops and isolated solids in the present context porosity specific surface and euler characteristic were computed using the imagej plug in morpholibj legland et al 2016 the tortuosity is calculated as the ratio of the smallest geodesic distance shortest distance between two points within a given phase to the euclidian distance between the inlet face and the outlet face in the z direction the geodesic distance is computed by a series of dilations and intersections following the methodology described in gommes et al 2009 and the euclidian distance corresponds to the size of the sample the pore size distribution psd is computed using the imagej plug in xlib münch and holzer 2008 following the continuous psd approach on the 3d images 2 4 numerical upscaling the computed intrinsic permeability later refered as intrinsic permeability is computed using numerical upscaling the navier stokes equation for flow in the pore structure is solved using a lattice boltzmann approach the lattice boltzmann methods represent the fluid as fictive particles that undergo collision and propagation processes and lead to the calculation of particle density ρ and velocity field ux uy uz in the present study a d3q19 trt lbm aproach was used with bounce back fluid solid boundaries ginzburg et al 2008 pazdniakou and dymitrowska 2018 more details on the model may be found in pazdniakou and dymitrowska 2018 the original image was mirrored is the z direction and padded with walls on the lateral sides in order for periodic boundary conditions to be used flow in the porous system was modeled by a force f along the z direction pan et al 2006 the intrinsic permeability m 2 is computed from the density and velocity fields following eq 3 3 k z z δ 2 ν ρ u z v f where δ is the image resolution 5 nm ν is the viscosity in lattice units 1 6 v is the number of voxels in the mirrored image f 10 7 is the imposed force in lattice units and ρ and uz are the density and velocity in z direction fields respectively the computed effective diffusion and computed dispersion later refered as effective diffusion and dispersion respectively are computed using numerical upscaling the advection diffusion transport equation is solved using a d3q7 lbm following yang and chu 2013 the lbm methods thus allow the computation of the concentration field for the dispersion simulations the velocity field used is the one calculated from the flow equation normalized to reach an average z velocity of 0 005 in lattice units leading to an average local peclet of 3 3 similarly to yang and chu 2013 the transport is solved from an initial pulse considering periodic boundaries on all sides the initial pulse is set at z 90 voxels and the simulation is computed at least until the pulse spreads over 180 voxels corresponding to the original image size the method of moments garabedian et al 1991 is then applied as a post processing step in order to compute the effective diffusion and dispersion for a given imposed diffusion parameter 0 05 for the effective diffusion calculation and 0 002 for the dispersion calculation eq 4 as well as the mean displacement rate eq 5 4 d z z t z z 2 2 m 0 c d x d y d z 5 u z z t t z m 0 c d x d y d z where z is the mean displacement of the solute and m 0 c d x d y d z is the total solute injected in the system the effective diffusion and the dispersion computed are normalized against the imposed diffusion parameter 3 results and discussion the four reconstruction models sequential indicator simulation sis slice sequential mps s2dcd three directional aggregation mps 3da mps weighted three directional aggregation mps weighted 3da mps are applied to the reference image data considering training images with average porosity of 30 for the mps approaches see figs 3 and 4 the conditioning slices are normal to the z direction and spaced by 5 11 or 22 voxels fig 1 for each case five realizations were performed morphological parameters porosity specific surface euler characteristic tortuosity and pore size distributions are computed and compared to the reference image properties then effective transport properties permeability effective diffusion and dispersion for a given realization of each reconstruction approach and for each distance between conditioning data are compared finally the impact of the post processing is assessed 3 1 reference data the reference image fig 5 has a porosity of 34 4 with over 98 of the porosity interconnected and percolating and a specific surface of 113 5 μ m 1 its euler characteristic is of 2392 5 reflecting its connected pore structure the tortuosity of the reference image is quite low at a value of 1 05 the pore size distribution of the reference image fig 8 is rather uniformly graded between diameter 5 nm and 60 nm for the most part d 50 33 nm d 10 15 nm and d 90 58 nm the permeability of the sample along the z axis is of 4 10 18 m 2 the effective diffusion and longitudinal dispersion normalized against input diffusion are respectively of 0 43 and 12 4 in the reference image fig 5 larger solid structure with included porosity can be observed it should be noted that the post processing removes such included porosity the pores appear to be orientated preferably in the z direction reflecting the anisotropy that could be observed when performing a 3d variogram vertical range of 14 voxels it should be noted that the medium was compacted in the x axis direction 3 2 direct observation images of the first realization for each reconstruction approach and each distance between conditioning data are shown in fig 6 the 3da mps approach demonstrates a little of curtain like noise but allows good description of the larger grains in other words solid voxels planes or lines appear in a pore volume or inversely fig 6 zoomed areas the pores elongation in the z direction is well recovered for a distance of 5 voxels between conditioning data though this behaviour is diminished when the distance increases indeed at 22 voxels distance the pores have seemingly a more sphere like shape the s2dcd and weighted 3da mps approaches demonstrate heavier curtain like noise even after post processing the larger solid grains are well described albeit tend to disappear with less conditioning data especially for the weighted 3da mps approach similarly to the 3da mps method the pore elongations in the z direction can be observed with high conditioning and tend to decrease when the distance between conditioning data increases the weighted 3da mps approach appears to maintain the pore shapes better than the s2dcd reconstruction in low conditioning cases for all mps based method the loss of vertical continuity of pore shapes observed for 22 voxels spaced simulations is certainly linked to the chosen search radius size indeed a template size of 9 9 9 voxels allows simulations to start with almost always conditioning data in the data event for 5 and 11 voxel spaced cases in this last case only the points located exactly in the middle of two conditioning slices are not constrained at the beginning of the simulation but with a 22 voxel spaced slices almost 50 of the simulated points have empty data events at the beginning of the simulation favouring random mis connections of two consecutive slices using a higher search radius should certainly improve the results the sis realizations do not show curtain like noise but are fuzzier than mps reconstructions larger grains tend to be filled with connected porosity especially when the amount of conditioning data diminishes the pore elongations are barely observable from a distance of 11 voxels which is consistent with the input vertical range of 10 voxels using a higher vertical range should increase the pore elongation but supposes the anisotropy of the media is known at the beginning 3 3 morphological comparison variations of morphological parameters porosity specific surface euler characteristic and tortuosity compared to the reference sample are shown for each reconstructed approach in fig 7 the variations of the morphological parameters in between realizations remain reasonable even with limited conditioning data and are significantly lower than the discrepancy between the various reconstruction methods it supports the fact that the gap between methods can be investigated based on this comparative analysis the post processed reference image demonstrates an increase in porosity of 7 of the euler characteristic of 60 a decrease in specific surface of 18 and maintains the tortuosity the post processed reference psd translates the reference psd by roughly 5 nm 1 voxel fig 8 therefore leading to larger pore size the sis approach shows higher porosity than the reference fig 7 considering that the sis fits inherently the input porosity provided by the histogram from conditioning slices this higher value is due to the post processing the euler characteristic is also significantly higher than the one of the reference sample which indicates more loops within the pore volume and may be linked to the fuzziness observed on the images the specific surface and tortuosity are well described for the sis approach the error for porosity and tortuosity compared to the reference increases when distance between conditioning data increases whilst no specific trends can be determined for specific surface and euler characteristic the psd fig 8 of the sis reconstructed with a distance of 5 voxels superimposes the psd of the reference data when the distance between conditioning data increases however the sis tends to homogenize the pore sizes even further reducing the amount of larger pores the 3da mps shows very similar behaviour for distance 5 and 11 voxels fig 7 with good description of porosity and tortuosity underestimation by about 30 of the specific surface and by about 40 of the euler characteristic at a distance of 22 voxels between conditioning data the morphological parameters significantly differ with a loss in porosity of about half a large increase in tortuosity and a further increase of the euler characteristic error the specific surface is however improved the 3da mps approach leads to larger pores compared to both the reference and the post processed reference at the largest distance however the psd of the 3da mps reconstruction fig 8 is very similar to the one of the post processed sample except for larger pores it should be noted that the evolution of specific surface and psd for 3da mps is similar the 3da mps images with smaller conditionning distance lead to good representation of the larger pores as observed in the previous section yet the interface details with smaller characteristic length are not as well represented especially due to the post processing therefore the specific surface is significantly lower and the psd tends to favor medium size pores over smaller size pores the s2dcd reconstructions demonstrate clear correlations with the amount of conditioning data fig 7 indeed the porosity and euler characteristics decrease with increasing distance and conversely specific surface and tortuosity increase in regards to the error between realizations and the reference sample there is a steady increase of this error for porosity and tortuosity the s2dcd psd fig 8 is very close to the post processed reference at a distance of 5 voxels with a diminution of the pore size of roughly 1 voxel with increasing distance between conditioning data the weighted 3da mps approach provides consistent morphological characteristics with the reference sample even with little conditioning fig 7 at a conditioning distance of 5 voxels the weighted 3da mps reconstruction is however closer to the post processed sample than to the reference sample at a larger distance the porosity diminishes slightly and the tortuosity increases albeit much less than the ones from s2dcd and 3da mps realizations similarly to the s2dcd the weighted 3da mps psd fig 8 is close to the post processed psd with high amount of conditioning data however pore sizes seemingly increases by about 1 voxel for lesser conditioning it should also be noted that the variation of the psd between realizations is more important for the weighted 3da mps approach than for the other methods especially with little conditioning 3 4 effective properties given the very low variability over realizations of these various morphological parameters comparative analysis of effective properties is conducted hereafter for a single realization of each reconstruction method but for different spacings between conditioning data quantifying the uncertainty in transport properties would require to perform simulations over a large ensemble of stochastic realizations and will not be addressed here variations of effective properties namely the permeability effective diffusion and dispersion are provided in fig 9 first we observe that the post processed sample shows a higher permeability and effective diffusion than the reference sample likely linked with the increase in pore size diameter and subsequent increase in porosity the dispersion is however lowered by about 30 compared to the reference value as stated in section 2 4 the dispersion computations were performed for a local average peclet of 3 3 considering the median pore size as characteristic length but for some realizations s2dcd and 3da mps with 22 voxels distance the ratio between maximum velocity and average velocity in the z direction was too important leading to significantly locally higher peclet and thus to divergence of the transport simulation the acceptable ratio is roughly below 30 leading to the limit peclet of 100 stated in yang and chu 2013 it should be noted that aside from s2dcd and 3da mps realizations at the largest distance between conditioning data the permeability effective diffusion and dispersion for all methods remains within the same order of magnitude the sis approach shows a steady decrease of the permeability and effective diffusion with the increase of the distance between conditioning data despite an increase in porosity for the permeability this behaviour is explained by both the decrease in pore size and the increase in tortuosity the mps methods except for the weighted 3da mps 11 case demonstrate a decrease in permeability and effective diffusion when the amount of conditioning data is decreasing this is consistent with the increase of porosity and of tortuosity observed on the realizations the permeability of the weighted 3da mps approach for 5 and 11 voxels distance and sis 11 shows good agreement with the reference while the 3da 5 3da 11 and sis 5 have better agreement with the post processed reference data all these six cases demonstrate good agreement with the reference data for effective diffusion the s2dcd and the 22 voxels distance realizations underestimate however the permeability and the effective diffusion more significantly this is coherent with the lower porosity and the larger tortuosity of these reconstructions the evolution of permeability with conditioning distance cannot however be only correlated to the porosity and tortuosity indeed these morphological properties show little variation for the weighted 3da mps and the evolution of permeability does not follow the trend defined by the euler characteristic either therefore it is likely that other morphological properties would be required to properly explain the permeability variation with conditioning distance it should be noted that the underestimation of specific surface and discrepancy in the psd observed for 3da 5 and 3da 11 do not seem to significantly influence the computed effective properties in order to further illustrate the transport and therefore the dispersion the normalized velocity fields and the normalized concentration at a given time are shown in fig 10 11 the reference sample demonstrates preferential pathways fig 10 this leads to a very inhomogeneous distribution of concentration with finger like progress of the concentration front along the preferential pathways fig 11 such a behaviour is observed in 3da 5 3da 11 and to a lesser extent in the weighted 3da mps realizations consequently these cases have a dispersion close to the reference the flow pattern for 3da 22 s2dcd 11 and s2dcd 22 differs significantly from the other mps simulations fig 10 indeed the flow seems more structured with pores generated around the conditioning data mostly normal to the z direction and smaller links joining the conditioning slices such patterns explain the larger tortuosities despite this the s2dcd 5 and s2dcd 11 realizations still maintain preferential flow in z direction albeit less than the reference samples the velocity field of the sis reconstructions are more homogeneous leading to a more horizontally spread concentration front and therefore to a smaller dispersion 3 5 impact of post processing it is interesting to note that the effective properties of 3da 5 and sis 5 resemble more those of the post processed reference than those of the reference on top of that as stated previously curtain like noise is still significant in the s2dcd and weighted 3da mps realizations therefore it is important to assess the impact of post processing if any on the morphological and effective properties to this end these properties were computed on 3da 5 3da 11 and sis 5 where no post processing was applied and on weighted 3da mps 5 weighted 3da mps 11 and s2dcd 5 where a median filter was applied as post processing the median was a 3d filter with 2 voxels radius if the result of the median filter led to an uncertain value of the voxel i e the voxel has the same amount of neighbouring solid and pore voxel the voxel was considered as a pore voxel fig 12 illustrates the impact of the post processing or lack of on a given image on the no filter images the isolated voxels pore and solid generated with the weighted 3da mps approach can be easily spotted the initial post processing effectively removes such features while the median filter further smooths the pore and solid interfaces removing the post processing led to a loss of porosity which was increased by the post processing and an increase in specific surface due to non connected porosity the lower porosity might be due to the fact that the ti average porosity is lower than the reference porosity despite the loss in porosity the 3da mps realizations without post processing showed good agreement with the reference both in regards to morphological and transport properties this is especially true at 5 voxels distance between conditioning data where the realization properties are almost identical to the reference for the sis realization however the lack of post processing did not lead to improvement indeed the error for specific surface but also for effective diffusion remains and the dispersion could not be computed the permeability for the sis sample is significantly lower than the post processed one yet the discrepancy with the reference value remains similar median filtering provides much better removal of the curtain like noise compared to the initially chosen post processing in regards to morphological properties median filtering led to a decrease in porosity specific surface and to a lesser extent of the tortuosity fig 13 the pore size distribution is however mostly translated towards larger pores by 1 to 1 5 voxels with smaller size pores disappearing the effective diffusivities fig 14 are increasing despite the loss in porosity permeability increases for the weighted 3da mps 5 voxels decreases for the weighted 3da mps 11 voxels and is maintained for the s2dcd filtered realizations dispersion is increased for both weighted 3da mps simulations unlike in the previous post processing dispersion computation could be performed for the s2dcd 5 reconstruction and shows significant overestimation compared to the reference overall whilst the prediction of morphological properties porosity and specific surface were degraded compared to the previous post processing and even though the change of filter leads to modification of the effective properties there is no evident improvement or degradation of the transport properties with median filtering moreover the general behaviour of the reconstructed samples for each method remains similar to the one using the initial post processing with the weighted 3da mps realizations showing relatively good agreement with the reference and the s2dcd 5 realization being less accurate 4 conclusion in this study we investigated the capability of different pixel based methods to reconstruct a coherent 3d nanopore space of clay rock from parallel 2d images as we could obtain from fib sem or tem imaging the studied media was almost isotropic and the different methods we used would have to be adapted in the case of a strong anisotropic media notably by i choosing dedicated tis for each orthogonal directions and ii eventually adapting the template size which should be bigger in the main anisotropy direction the aim was not to make an exhaustive comparison of all existing reconstruction methods nor to find an universal best one but to propose based on a real sample study general guidelines i to choose the most appropriate method and ii to identify the accurate sampling distance when dealing with spaced parallel cross sections because a direct 3d comparison does not reflect the relevance of the methods we also analysed the amount of morphological features that was preserved this is achieved by comparing minkowski functionals namely pore volume specific surface and euler characteristic as well as tortuosity and psd given the low variability of these various morphological criteria over realizations we assessed how these 3d reconstructions honour the transport properties of interest from a given realization and we discussed how the sampling distance should be chosen according to the nature of the properties at stake from the results described in this work the following conclusions can be put forward the sis approach reveals a good agreement of averaged properties but also a lack of description of preferential patterns as indicated by the observed discrepancy for the euler characteristic and hence for the predicted dispersion coefficient in addition to exhibiting artefacts the relevance of s2dcd approach is highly dependent on the amount of conditioning data the image reconstruction is satisfying with high conditioning but quickly degrades according to renard and straubhaar 2012a the path of reconstruction is crucial it is very likely that an improvement of the path would lead to better results on the contrary the 3da mps approach exhibits very good results for all the transport properties when the density of conditioning data is large but with no post processing required when the distance between conditioning data is close to the characteristic length as obtained from the variogram the reconstruction remains acceptable but degrades strongly afterwards finally the weighted 3da mps approach shows better consistency with decreasing conditioning data but leads to noise in the reconstructed images and is not as accurate at high conditioning as 3da mps it calls for more advanced weighting and post processing steps permeability and effective diffusion were shown to be strongly related to morphological parameters such as porosity and tortuosity no evident correlation between dispersion and the measured morphological properties was however put forward this calls for other morphological characterisation to fully assess the acceptability of a given realisation in regards to this transport process indeed dispersion further discriminates the reconstruction approach in comparison to permeability and effective diffusion concerning the sampling distance it is clear that choosing a spacing inferior to the vertical range of correlation in our case 14 voxels clearly facilitates a satisfying 3d reconstruction in practice this also supposes to be capable of estimating this range and thus of performing a variogram analysis in the orthogonal direction to the further sampling this upper limit of sampling distance is particularly critical for sis but mps based methods could be less impacted since increasing the maximum search distance would partially mitigate this in a more comprehensive way this is a severe disadvantage for application to nanoporous rocks which exhibit a large pore size distribution and multi scale features with different correlation lengths here we face a classical drawback of mps methods which are found to be good in conditioning the point data but fail to reproduce long distance connectivity tahmasebi 2018 as a conclusion this study provides a general guideline for deciding which method will predict the best 3d reconstruction results depending on the amount of 2d images available and the transport property targeted for prediction these findings are however valuable for the specific conditions we used and as previously said the studied media was almost isotropic mps based methods imply various parameters among them the choice of the ti the search radius size and in this specific application the value aggregation method for 3da mps methods and the similarity computation between events all these parameters strongly impact the results and a sensitivity analysis implying all these aspects could be performed to refine the recommendations on the most suitable reconstruction method our comparative analysis could also be extended to more recent algorithms for example a hybrid approach combining mps method with object based or pattern based method as proposed by tahmasebi 2017 could certainly be a research route for future improvements as well as the ones combining the forces of mps and deep learning kamrava et al 2019 it should be also kept in mind that determining the most suited reconstruction method for predicting the effective property of interest does not prevent from quantifying the uncertainties an ensemble of stochastic realizations would be ultimately required to estimate the distribution of transport property and the averaged value for drp application to real porous media credit authorship contribution statement anne julie tinet methodology software validation visualization data curation supervision writing original draft quentin corlay software investigation writing original draft pauline collon methodology supervision visualization writing review editing fabrice golfier conceptualization supervision visualization writing review editing project administration funding acquisition kassem kalo investigation resources declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the financial support from needs mipor rpm project is gratefully acknowledged this work was also partially funded by the iceel carnot institute multiec hydro project we are thankful to stephane gaboreau who provided the digital images of the compacted illite used in this article we also thank emerson paradigm for providing the skua gocad software and api 
