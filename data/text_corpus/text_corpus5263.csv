index,text
26315,when studying the empirical phenomenon of wildfires we can distinguish between the occurrence at a specific location and time and the burnt area measured this study proposes using structured additive regression models based on zero one inflated beta distribution for studying wildfire occurrence and burnt area simultaneously beta distribution affords a convenient way of studying the percentage of burnt area in cases where such percentages are bounded away from zero and one inflation with zeros and ones enables observations without wildfires or with 100 burnt areas to be treated as special cases structured additive regression allows one to include a variety of covariates while simultaneously exploring spatial and temporal correlations our inferences are based on an efficient markov chain monte carlo simulation algorithm utilizing iteratively weighted least squares approximations as proposal densities application of the proposed methodology to a large wildfire database covering galicia spain provides essential information for improved wildfire management keywords beta regression burnt area deviance information criterion markov chain monte carlo simulations temporal and spatial effect zero one inflated beta distribution software availability the structured additive regression model based on the zero one inflated beta distribution which was used to study wildfire occurrence and burnt area simultaneously is implemented in bayesx which is a stand alone software package freely available for various operating systems at http www bayesx org all programming was performed by the authors unless stated otherwise and the code of the model is provided in a zip file included in the supplementary material 1 introduction recent years have witnessed a rise both in the frequency of occurrence and in the burnt area of wildfires across europe koutsias et al 2015 with an increasing concentration of wildfires in the south urbieta et al 2015 though differences in climatic conditions might afford one possible explanation for this situation this would not suffice to explain the large number of arson related fires in this area viedma et al 2015 alcasena et al 2015 other potential influences include 1 fuel load and continuity loepfe et al 2014 san roman sanz et al 2013 2 increased forest area catry et al 2009 3 the increasing wildland urban interface wui lampin maillet et al 2010 weise and wotton 2010 paton and tedim 2012 4 new patterns of agricultural land management verde and zêzere 2010 ruiz mirazo et al 2012 guiomar et al 2015 viedma et al 2015 5 agriculture abandonment lasanta et al 2015 regos et al 2015 6 socio economic changes curt et al 2013 ganteaume et al 2013 and 7 post fire reactions conedera et al 2011 as a consequence quantifying the risk of wildfires and managing such risk have become important issues world wide thompson and calkin 2011 in the past a number of different techniques have been used to explain and predict the probability of fire occurrence in which a binary response variable represents the presence or absence of wildfires at a given location and time point these are 1 linear logistic regression lr gonzález and pukkala 2007 martínez fernández et al 2013 rodrigues et al 2014 2 generalised linear models glms chuvieco et al 2010 ordóñez et al 2012 3 generalised additive models gams with fixed effects random effects and mixed effects brillinger et al 2006 reid et al 2015 4 spatial scan statistics orozco et al 2012 pereira et al 2015 and 5 structured additive regression star ríos pena et al 2015 2017 in the above studies different types of covariates have been used to predict the probability of wildfires namely temperature huld et al 2006 yiannakoulias and kielasinska 2015 relative humidity padilla and vega garcía 2011 plucinski et al 2014 freeborn et al 2015 and variables related to fuel management gonzález olabarria et al 2012 fréjaville and curt 2015 while the probability of wildfire occurrence is certainly an interesting quantity from the perspective of risk management it is also important to quantify the burnt area of a fire at a given location and given time point the importance of fire can for instance be reflected by the percentage of burnt area loepfe et al 2014 hernandez et al 2015 ruffault et al 2016 this paper proposes a new approach for simultaneously studying the occurrence and burnt area of wildfires within the framework of structured additive distributional regression illustrated by reference to data for galicia the reason for using a star model fahrmeir et al 2004 lies in the fact that non linear covariate effects can be combined with the possibility of correcting for spatial autocorrelation through the inclusion of spatial effects at the same time the zero one inflated beta distribution incorporates the beta distribution representing the percentage of burnt area at a specific grid point this distribution assumed for the fractional response is supplemented by zero one inflation the resulting response distribution is then characterised by a location and a dispersion parameter as well as two further parameters linked to the probabilities of zero one inflation klein et al 2015 each of these parameters will then be related to a regression predictor consisting of several effect types including non linear and spatial effects the main advantage of zero one inflated beta regression is that it enables the occurrence and burnt area of wildfires to be studied simultaneously this therefore makes more efficient use of the information in wildfire databases that contain more than just the presence absence of wildfires the model also has the potential to disentangle covariate effects on the probability of effects on the burnt area occurring and may lend better insight into the causal mechanisms underlying wildfire spread indeed to date there has been no attempt to explain burnt area of wildfires by reference to fractional response regression models 2 dataset development to create our own database for analysis purposes we used the wildfire database of the galician regional rural and maritime authority consellería de medio rural y mar www medioruralemar xunta es this database contains information on the site geographical coordinates and time hour and date at which a fire occurs we selected wildfires recorded during the first half of august 2006 the study period focused on an extraordinary situation in galicia balsa barreiro and hermosilla 2013 inasmuch as there were a total of 2060 wildfires during the first half of august 2006 alone the choice of this time period 15 days was due to the fact that outside this time range the number of wildfires was quite low which would make it almost impossible to identify the effects on the probability of occurrence or burnt area of wildfires in addition it should be noted that the identification of the effects in this period of time is particularly important because of the large number of wildfires that would otherwise be difficult to identify most of these fires were concentrated along the coastal strip and south east of galicia see fig 1 at the same time we obtained the percentage of area affected by wildfires dividing galicia into a 1 1 km cells and calculating the burnt surface area in each grid for this the ignition point of each wildfire has been located in one of the cells from the burnt area data cells have been selected totally or partially affected by the wildfire around 99 of the grid cells remained fire free during the study period the explanatory covariates selected to predict the occurrence and burnt area of wildfires included meteorological covariates dickson et al 2006 which were obtained from meteorological monitoring stations distributed throughout the study area www meteogalicia es except the wind speed covariate that can highly influence the spread of fire but less than 10 of the meteorological monitoring stations recorded this information despite there being a total of 134 monitoring stations in place only 71 were operational at the time and thus recorded data the weather covariates selected for study purposes were daily average temperature in c daily average relative humidity in and rainfall in l m 2 we further restricted our analyses to meteorological stations situated at a maximum distance of 30 km from any given wildfire due to differences in altitude between fires and the nearest meteorological station it was necessary to correct the temperature and rainfall covariates by reference to the difference in altimetry the variation in daily average temperature with height was 0 65 c 100 m to 11 000 m nunez and calhoum 1986 i e t h 15 c 0 0065 c m h where t h is the air temperature at height and h is the difference between the wildfire and the nearest meteorological station likewise the rainfall gradient amounted to an 8 variation per 100 m difference in height between any given ignition point and the meteorological station selected i e p c o r r e c t e d m o n t h l y p m o n t h l y 0 08 where p is rainfall moreover we studied the impact of calendar time days in order to capture dependence on climatic factors in the preceding days and the altitude in each grid the spatial component was made up of 30 158 cells derived by dividing galicia into a 1 1 km cells with each cell being associated with one of the 315 municipalities in which the study area is administratively divided the centroid of each cell is associated with a municipal area so that each grid belongs to a municipality the database contained a total of 452 370 records 30 158 cells per fifteen days of study table 1 shows all the variables used in the study along with a brief description of each 3 statistical methodology 3 1 zero one inflated beta regression zero one inflated beta regression is particularly well suited to the analysis of wildfire occurrence and burnt area since it accommodates both fractional response values i e responses presenting percentages and responses observed at the extremes i e clusters of observations exactly equal to zero or one the zero one inflated beta distribution is defined as the mixture of one continuous component following a beta distribution and two point masses at zero and one such that the mixed discrete continuous density is finally obtained as p y π 0 y 0 1 π 0 π 1 1 b a b y a 1 1 y b 1 0 y 1 π 1 y 1 where b a b denotes the beta function b a b 0 1 y a 1 1 y b 1 dy with parameters a 0 and b 0 while π 0 and π 1 0 1 the probability π 0 is the probability of no wildfire while π 1 is the probability that the pixel is completely burnt therefore π 0 and π 1 are the probabilities for the point masses at zero and one respectively such that π 0 π 1 1 ospina and ferrari 2010 propose an alternative parameterisation of the zero one inflated beta distribution which enables an easier interpretation in the regression context firstly the continuous part corresponding to a beta distribution is parameterised based on the expectation μ a a b ε 0 1 and the scale parameter σ 1 a b 1 ε 0 1 in a second step the probabilities for the point masses at zero and one are replaced by ν 0 and τ 0 determined from the equations π 0 ν 1 ν τ π 1 τ 1 ν τ i e ν and τ are proportional to the probabilities for no and complete damage at a given observation point respectively the advantage of this parameterisation is that it avoids the constraint π 0 π 1 1 following the idea of distributional regression klein et al 2015 and generalised additive models for location scale and shape rigby and stasinopoulos 2005 we relate each of the four parameters of the zero one inflated beta distribution to regression predictors such that μ i exp η i μ 1 exp η i μ σ i exp η i σ 1 exp η i σ ν i exp η i ν τ i exp η i τ each predictor is chosen to be of a structured additive type as explained in the following section 3 2 structured additive regression structured additive regression fahrmeir et al 2004 provides a generalisation of generalised linear and additive models where a more flexible structure is assumed for regression predictors in generic notation a structured additive predictor is given by 1 η β 0 f 1 x f j x where the parameter index has been dropped for the sake of simplicity which also suppresses the potential dependence of the number of effects j and the chosen covariates x on the parameter β 0 is the overall intercept and the functions f 1 f j represent regression effects of different types of the generic covariate vector x each of the functions in the additive predictor is now approximated by a linear combination of basis functions b 1 x b k x such that f x k 1 k β k b k x where β k denotes the corresponding basis coefficients examples include penalised splines markov random fields or random effects as will be discussed in more detail in the next section in matrix notation a structured additive predictor of the form 1 can then be expressed as η β 0 1 z 1 β 1 z j β j where η is obtained by stacking all individual observations 1 refers to a vector of ones z j arises from evaluating the basis functions at the observed covariates and β j is the vector of corresponding basis coefficients to ensure favourable properties of the function estimates such as smoothness in the case of non linear and spatial effects or shrinkage in the case of random effects we rely on a bayesian formulation where prior distributions are assigned to the vectors of regression coefficients generically these are given by multivariate normal distributions p β j τ j 2 exp 1 2 τ j 2 β j k j β j where τ j 2 is a prior variance determining the impact of the prior distribution on the function estimates while k j is the prior precision matrix that implements the smoothness sparseness assumptions from a frequentist perspective these prior distributions correspond to penalties of the form λ j β j k j β j where λ j 0 is the smoothing parameter that determines the impact of the penalty on the penalised likelihood estimate 3 3 model components by using three specific instances of effects from the class of structured additive predictors we were able to allow for non linear spatial and random effects for non linear effects we relied on bayesian penalised splines eilers and marx 1996 lang and brezger 2004 where b spline basis functions are used to approximate the effect of a continuous covariate by way of a default we considered cubic b splines based on 20 equidistant knots something that yields twice continuously differentiable i e visually smooth function estimates and typically induces enough flexibility to capture even severe non linearity as a prior distribution we used a second order random walk that from a frequentist perspective corresponds to penalising second order differences of parameters for adjacent basis functions for the spatial effect we used a markov random field prior that assigns a separate regression coefficient to each region from a discrete set of spatial locations this implies that the basis functions in this case correspond to simple indicator functions for the different regions to enforce spatial smoothness we assumed a gaussian markov random field prior for the basis coefficients that induce a penalty where differences between effects of spatially adjacent regions were penalised for random effects we assumed independent and identically distributed normal regression coefficients a more detailed discussion of potential model components and how they fit into the generic framework outlined above can be found in fahrmeir et al 2013 3 4 bayesian inference for bayesian inference we based our approach on the distributional regression framework of klein et al 2015 who developed a generic markov chain monte carlo simulation approach in which proposal densities for blocks of regression coefficients are obtained from a locally quadratic approximation of the log full conditionals the smoothing variances τ 2 are assigned inverse gamma hyperpriors such that gibbs sampling steps can be realised due to the conditionally gaussian conjugate prior for the regression coefficients this approach is implemented in the bayesx software package www bayesx org with the bayesx 3 0 2 version being used for the purposes of our analyses belitz et al 2015 4 results 4 1 model specification the increased flexibility of distributional regression models for fitting complex response structures such as that observed for the occurrence and burnt area of wildfires comes at the price of increased model choice challenges more specifically for each of the four parameters of the zero one inflated beta distribution it was necessary to decide which covariates should be included in the corresponding predictor and which form the corresponding covariate effect should take e g linear versus non linear effects for continuous covariates based on prior biological and environmental knowledge this study focused on three meteorological temperature precipitation relative humidity and one environmental covariate altitude while simultaneously accounting for temporal and spatial variability included via a non linear effect of time represented by successive days from 1 to 15 august 2006 as well as a spatial effect defined on the basis of the 315 municipalities comprising the study area the meteorological covariates in particular influenced the state of hydration of dead fuels with low relative humidity and high temperatures providing favourable conditions for the initiation and development of wildfires lack of precipitation similarly affects both living and dead fuels and by extension the occurrence and burnt area of wildfire development based on this pre selection of covariates a number of preliminary models were fitted to determine which covariates were associated with specific distributional parameters of the zero one inflated beta distribution cubic penalised splines based on twenty inner knots and a second order random walk prior were used for potentially non linear effects the spatial effect was split into a markov random field for the spatially structured effect and an independent identically distributed i i d random effect for the spatially unstructured effect for all variance hyperparameters inverse gamma priors were used with both parameters set at 0 001 based on the consideration of these preliminary models the following specification was favoured both by theoretical considerations and the comparison of models via the deviance information criterion dic η i μ β 0 μ f 1 μ h r i β 2 μ t e m p i f 3 μ d a y i β 4 μ p p i β 5 μ a l t i t u d i f s t r μ c d c o n c i f u n s t r μ c d c o n c i η i σ 2 β 0 σ 2 f 1 σ 2 h r i β 2 σ 2 t e m p i f 3 σ 2 d a y i β 4 σ 2 p p i β 5 σ 2 a l t i t u d i η i ν β 0 ν f 1 ν h r i f 2 ν t e m p i f 3 ν d a y i β 4 ν p p i β 5 ν a l t i t u d i f s t r ν c d c o n c i f u n s t r ν c d c o n c i η i τ β 0 τ f 1 τ h r i f 3 τ d a y i f s t r τ c d c o n c i f u n s t r τ c d c o n c i for this model 12 000 iterations of the markov chain monte carlo simulation algorithm were performed with 2000 iterations being discarded as burn in and every 10th iteration stored to reduce autocorrelation from the markov chain as a consequence all inferences were based on samples of size 1000 drawn from the posterior predictive distribution of our model 4 2 estimated covariate effects to interpret effects in a distributional regression model it is necessary to overcome the challenge posed by the common type of ceteris paribus analysis in which changes in one covariate can be related to changes in the expectation of the response while all other covariates are held constant this change in interpretation results from the fact that covariates generally have an impact on all distributional parameters however the advantage of the general distributional regression framework for the zero one inflated beta distribution is that it allows one to evaluate effects on many interesting quantities linked to the response distribution and not just effects on the mean the following measures were considered the expected burnt area caused by a potential wildfire the posterior mean expectation i e the expectation of the zero one inflated beta distribution which is based on all four model parameters and accounts for the probability of no damage represented by the zero probability partial damage represented by the beta part of the distribution and complete damage represented by the one probability as a consequence the expected burnt area represents an overall risk assessment for specific covariate combinations the expected burnt area given fire occurrence the posterior mean conditional expectation i e the expected amount of damage given that there has been a fire at a specific location this allows for a conditional assessment of wildfire burnt area given fire occurrence whereas the unconditional expectation represents a mixture of occurrence probability and burnt area the variability of burnt area given fire occurrence the posterior mean conditional variance i e the variance in the amount of damage given the occurrence of a fire this can be interpreted as a measure of risk which quantifies uncertainty in predicting expected burnt area given fire occurrence if two locations in an expected burnt area are close together but differ considerably in terms of variability the location with the higher variance may be considered more risky and this in turn can provide additional insights for managing fire risks the probability of occurrence the posterior mean probability of occurrence i e the probability of observing either partial or complete damage at a given location figs 2 5 depict these quantities for the given covariates more specifically the quantities are shown to vary over the range of the respective covariates while all other covariates are held constant at their averages fig 6 shows the predicted values in comparison with those observed for the burnt area and the probability of occurrence this leads to the following conclusions in the case of precipitation the largest expected burnt area was identified for values of around 4 5 l m 2 i e relatively low values here it surprised that the burnt area expected for precipitation levels is larger than for dryer levels and this is probably a result of the two opposing effects on the conditional expectation increasing with precipitation fig 3 and the occurrence probability decreasing with precipitation fig 5 taken together this result in the mode at an intermediate value for larger amounts of rainfall the expected burnt area was small similarly the probability of occurrence decreased to zero monotonically with increasing amounts of rainfall and quickly approached zero surprisingly both conditional expectation and conditional variance rose with increasing precipitation and quickly approached their most extreme value of 1 this is likely to be an artefact induced by the linear specification for precipitation in our model which however was favoured in our initial mode comparisons and therefore seems to provide an adequate fit for the majority of the data points in the case of temperature it is observed that it does not seem to have a great influence on the probability of occurrence and behavior of the wildfires the origin provoked of practically all of them causes the ambient temperature to become a necessary condition but not sufficient for the occurrence of the wildfires in this way once the average daily temperature reaches about 25 30 c an increase does not significantly increase the probability of occurrence of wildfires as can be seen in the width of the confidence intervals the enormous variability in the thermometric extremes of the analyzed period is remarkable in summary there was a weakly increasing effect on all four parameters of interest with increasing temperature while this increasing effect is very plausible the overall explanatory power of temperature would seem to be small in the case of all four parameters of interest the highest values were achieved either at a relative humidity of around 30 40 or at a relative humidity of around 80 for the expected burnt area there was a clear peak at 30 40 relative humidity this fits very well with the observation that most fires occurred close to the coast where low humidity values of under 50 are common a similar effect structure was found for the probability of occurrence for the conditional expectation of burnt area the peaks at 30 40 and at 80 relative humidity were of roughly the same size while the conditional variance was largest at 80 relative humidity the latter finding hints at an increased risk at higher humidity values if a fire is actually observed the temporal effect indicated considerable variability over the study period with a clear peak for all quantities at around the 8th day of the two week period for the conditional variance there was a second peak at 12 days note that the temporal effect captures all kinds of unobserved heterogeneity that is left after adjusting for the included covariates and is therefore a surrogated of unobserved effects the effect of altitude was not too strong overall but there seemed to be some positive association with the conditionally expected burnt area there is an increase in variability from 1000 m of altitude this situation contrasts with that found in other studies of this same research team ríos pena et al 2017 where as the altitude increased the risk of wildfires decreased the methodology presented here seems to be more effective in integrating this variable into the predictive model the spatial effect reflected an increased risk towards the coastal areas a finding that fits well with the clustering of fire events along this strip despite allowing for covariates in our model specification there is obviously still some remaining spatial variation that is captured by the spatial effects 5 discussion wildfires are a major environmental economic and social threat in mediterranean europe they have become the main problem for environmental authorities oliveira et al 2012 in galicia they have a remarkable impact on certain areas of the autonomous region comunidad autónoma and pose a challenge for the future of the region the development of new methodologies especially those which are evidence based allows for more efficient organisation and planning of firefighting and an ensuingly smaller burnt area and lower risk of loss of life this study adopted and tested a new approach to the task of simultaneously studying the occurrence and burnt area of wildfires within the framework of structured additive distributional regression illustrated by reference to data for galicia as shown in fig 6 our model seems to fit both the probability of occurrence and the burnt area reasonably well structured additive models with non linear interaction effects which consider spatial information in the form of a location variable can accelerate the development of fire behavior models and prove most useful for drawing up fire prevention and firefighting plans ríos pena et al 2017 their principal advantage lies in their flexibility in that they can include spatial and temporal covariates along with the potentially non linear effects of other continuous covariates i e non linear covariate effects can be combined with the possibility of correcting for spatial autocorrelation through the inclusion of spatial effects fahrmeir et al 2013 in addition the zero one inflated beta distribution incorporates the beta distribution with degenerate distributions for the purpose of modelling extreme values the measures evaluated through estimated covariate effects namely expected burnt area of the wildfire expected burnt area given the occurrence of a wildfire variability of burnt area given the occurrence of the wildfire and probability of occurrence of a wildfire at a particular location showed fire risk scenarios for the covariates studied as indicated by krasovskii et al 2016 fire regimes are determined by climate vegetation and direct human influence in our case we have focused on the climatological and physiographic variables among the numerous contributions of approaches to wildfire climate relationships note the recent works littell et al 2016 nunes et al 2016 turco et al 2017 keeley and syphard 2017 as expected spatial and temporal weather conditions influenced the quantity and characteristics of wildfires trigo et al 2013 2016 established that large summertime wildfire activity was strongly related to previous climatic conditions in the north western sector of the iberian peninsula russo et al 2017 in that the periods of previous drought are a necessary condition for the presence of fires our data are centered of the specific days of the arson fires and in which the conditions were adequate for fires spread in the same way as dacamara et al 2014 use generalised pareto models to improve the general pattern of the canadian fire weather index fwi including among others temperature and 24 h cumulated precipitation one of the most important factors shaping behavior of the arson fires if we analyze relative humidity as it did bedia et al 2017 through multivariate spatial models next to roc curves are proposed their results show how the combination of low relative humidity and high air temperature appears in the large fires in the mediterranean region which coincides with our results in the case of the august 2006 fires in galicia these conditions were ideal for fire spread on analysing and comparing the specific variables of the model it seems only logical that an increase in temperature would make it easier for wildfires to occur in the case of aragon spain molina terrén and cardil 2016 observed that when the air temperature was higher than 20 c at 850 hpa the frequency and burnt area of fires increased our results in terms of average and variance show that temperature exerts a positive influence on the percentage of burnt area oliveira et al 2014 explained the importance of temperature for wildfire occurrence in mediterranean europe and we found that it was also a key factor for wildfire ignition and behavior in greece paschalidou and kassomenos 2016 showed that forest fires and burnt forest area were produced in some very specific weather conditions similar to our work regarding altitude our results show that it does not present a significant importance for ignition and fire behavior it seems an interesting debate because other authors have analyzed the influence of altitude on fires in the iberian peninsula the model proposed by botequim et al 2013 to predict annual wildfire risk in pure and even aged eucalyptus stands in portugal showed altitude as having no significance coinciding with our results in a larger scale study of all wildfires in portugal from 1987 to 2004 however marques et al 2011 reported that the higher the altitude and distance to roads the higher the proportion of burnt area in a nearby area in contrast ordóñez et al 2012 found altitude to be a negative factor in their generalised linear and generalised least squares models the explanation given by these authors for this surprising result was that altitude plays a negative role in the probability of a fire being ignited by lightning authors like rodrigues et al 2016 conclude that the more mountainous areas to make up for the greater risk of fires with the lowest human activity it is remarkable the relation between the elevation and the density of roads and its influence on the behavior of the fire yang et al 2007 and narayaraj and wimberly 2012 have developed models to measure their influence in galicia chas amil et al 2013 indicate that the density of roads is correlated with fires but in turn conditioned by population density balsa barreiro and hermosilla 2013 concluded in an analysis of the same fires we studied that the proximity of roads and inhabited areas caused the fires to increase in size at the end the researchers do not reach a consensus on the relationship between altitude and fire possibly due to the interactions of other variables such as land use population density and roads with respect to the variables with non linear effects several authors have incorporated relative humidity rh in their indices and in the canadian forest fire weather index in particular camia et al 2008 dimitrakopoulos et al 2011 bedia et al 2013 padilla and vega garcía 2011 analyzed the variables that accounted for the fires in each of the 53 regions into which they divided spain and concluded that rh provided a good explanation for the fires in the interior of the country but not those in the galician region pereira et al 2013 found that the worse wildfire situation in portugal was due to a combination of wind and low rh our results are in line with this finding in that a rh of around 60 or lower was shown to be accompanied by more and larger fires our analysis of spatial effects showed that fires in august 2006 in galicia were not distributed randomly but were instead concentrated in certain municipalities in the south western coastal area without incidence in the north eastern region in the same sense chas amil et al 2015 noted that while the southern and eastern areas acted as hot spots with a clustering of parishes with high fire counts the north eastern areas acted as cold spots with a clustering of parishes with a low number of wildfires other papers of this team and in co authorship with other researchers have recently used different methodologies to study the distribution of fires in galicia fuentes santos et al 2013 analyzed the spatial distribution of ignition points in the fonsagrada ancares forest district with ripley s inhomogeneous k function their results indicate that there was spatial dependence between fires within an interaction radius of 4 km for the whole period 1991 2008 and within an interaction radius of 3 km in 1992 subsequently fuentes santos et al 2015 used smooth bootstrap kernel intensity estimation for inhomogeneous spatial point processes to analyze the entire region in 2006 and concluded that the spatial distribution of arson and natural wildfires was different boubeta et al 2015 applied poisson mixed models with areas included as random effects to 63 forest areas constituting the basic structure of the fight against wildfires in galicia during the summer of 2007 the results showed significant differences between forest areas across the period of analysis similar to our results lastly having already developed a sound approach to studying the probability of occurrence of wildfires in space and time by using star models for the same data ríos pena et al 2015 2017 we now present a refined and highly reliable method of study and understanding two simultaneous phenomena i e the occurrence of wildfires in a specific place at a given time and the burnt area of the wildfire as measured by the percentage of the area burnt in that place 6 conclusions since the last third of the 20th century there has been an increase in the number of forest fires affecting mediterranean europe with repercussions on social environmental and economic activities in particular accordingly forest service offices and researchers have been developing models capable of predicting fire occurrence zones most of the models developed to date have made separate predictions for the number and behavior of fires in view of the results yielded by this study star models based on the zero one inflated beta distribution can be said to afford a simple and satisfactory way of predicting both the probability of the occurrence of a wildfire and at the same time the burnt area associated with such a fire hence we suggest that this statistical tool is a novel methodological contribution to the study of wildfires we validated the novel modelling approach on data from galicia nw of spain in august 2006 when in a rather extreme scenario a total of 83 000 ha were affected by wildfires this in fact places the model in its operational context now and in the future regional policy makers affected by wildfires need models of support for extreme situations with simultaneous concentrations of fires in time and space they need to know the number of events and the surface that can be affected in each climatological situation conflicts of interest the authors declare that they have no conflicts of interest acknowledgements t kneib and c cadarso suárez would like to thank the spanish ministry of innovation competitiveness for support from the mtm2014 52975 c2 1 r project and m marey pérez and c cadarso suárez would like to thank the galician regional authority xunta de galicia for supporting research of the inbioest network 2014 pg141 and competitive research reference group proepla cn2012 015 the work of n klein and t kneib was supported by the german research foundation dfg via the research project kn 922 4 1 2 appendix a supplementary data the following is the supplementary data related to this article supplementary material supplementary material appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 03 008 
26315,when studying the empirical phenomenon of wildfires we can distinguish between the occurrence at a specific location and time and the burnt area measured this study proposes using structured additive regression models based on zero one inflated beta distribution for studying wildfire occurrence and burnt area simultaneously beta distribution affords a convenient way of studying the percentage of burnt area in cases where such percentages are bounded away from zero and one inflation with zeros and ones enables observations without wildfires or with 100 burnt areas to be treated as special cases structured additive regression allows one to include a variety of covariates while simultaneously exploring spatial and temporal correlations our inferences are based on an efficient markov chain monte carlo simulation algorithm utilizing iteratively weighted least squares approximations as proposal densities application of the proposed methodology to a large wildfire database covering galicia spain provides essential information for improved wildfire management keywords beta regression burnt area deviance information criterion markov chain monte carlo simulations temporal and spatial effect zero one inflated beta distribution software availability the structured additive regression model based on the zero one inflated beta distribution which was used to study wildfire occurrence and burnt area simultaneously is implemented in bayesx which is a stand alone software package freely available for various operating systems at http www bayesx org all programming was performed by the authors unless stated otherwise and the code of the model is provided in a zip file included in the supplementary material 1 introduction recent years have witnessed a rise both in the frequency of occurrence and in the burnt area of wildfires across europe koutsias et al 2015 with an increasing concentration of wildfires in the south urbieta et al 2015 though differences in climatic conditions might afford one possible explanation for this situation this would not suffice to explain the large number of arson related fires in this area viedma et al 2015 alcasena et al 2015 other potential influences include 1 fuel load and continuity loepfe et al 2014 san roman sanz et al 2013 2 increased forest area catry et al 2009 3 the increasing wildland urban interface wui lampin maillet et al 2010 weise and wotton 2010 paton and tedim 2012 4 new patterns of agricultural land management verde and zêzere 2010 ruiz mirazo et al 2012 guiomar et al 2015 viedma et al 2015 5 agriculture abandonment lasanta et al 2015 regos et al 2015 6 socio economic changes curt et al 2013 ganteaume et al 2013 and 7 post fire reactions conedera et al 2011 as a consequence quantifying the risk of wildfires and managing such risk have become important issues world wide thompson and calkin 2011 in the past a number of different techniques have been used to explain and predict the probability of fire occurrence in which a binary response variable represents the presence or absence of wildfires at a given location and time point these are 1 linear logistic regression lr gonzález and pukkala 2007 martínez fernández et al 2013 rodrigues et al 2014 2 generalised linear models glms chuvieco et al 2010 ordóñez et al 2012 3 generalised additive models gams with fixed effects random effects and mixed effects brillinger et al 2006 reid et al 2015 4 spatial scan statistics orozco et al 2012 pereira et al 2015 and 5 structured additive regression star ríos pena et al 2015 2017 in the above studies different types of covariates have been used to predict the probability of wildfires namely temperature huld et al 2006 yiannakoulias and kielasinska 2015 relative humidity padilla and vega garcía 2011 plucinski et al 2014 freeborn et al 2015 and variables related to fuel management gonzález olabarria et al 2012 fréjaville and curt 2015 while the probability of wildfire occurrence is certainly an interesting quantity from the perspective of risk management it is also important to quantify the burnt area of a fire at a given location and given time point the importance of fire can for instance be reflected by the percentage of burnt area loepfe et al 2014 hernandez et al 2015 ruffault et al 2016 this paper proposes a new approach for simultaneously studying the occurrence and burnt area of wildfires within the framework of structured additive distributional regression illustrated by reference to data for galicia the reason for using a star model fahrmeir et al 2004 lies in the fact that non linear covariate effects can be combined with the possibility of correcting for spatial autocorrelation through the inclusion of spatial effects at the same time the zero one inflated beta distribution incorporates the beta distribution representing the percentage of burnt area at a specific grid point this distribution assumed for the fractional response is supplemented by zero one inflation the resulting response distribution is then characterised by a location and a dispersion parameter as well as two further parameters linked to the probabilities of zero one inflation klein et al 2015 each of these parameters will then be related to a regression predictor consisting of several effect types including non linear and spatial effects the main advantage of zero one inflated beta regression is that it enables the occurrence and burnt area of wildfires to be studied simultaneously this therefore makes more efficient use of the information in wildfire databases that contain more than just the presence absence of wildfires the model also has the potential to disentangle covariate effects on the probability of effects on the burnt area occurring and may lend better insight into the causal mechanisms underlying wildfire spread indeed to date there has been no attempt to explain burnt area of wildfires by reference to fractional response regression models 2 dataset development to create our own database for analysis purposes we used the wildfire database of the galician regional rural and maritime authority consellería de medio rural y mar www medioruralemar xunta es this database contains information on the site geographical coordinates and time hour and date at which a fire occurs we selected wildfires recorded during the first half of august 2006 the study period focused on an extraordinary situation in galicia balsa barreiro and hermosilla 2013 inasmuch as there were a total of 2060 wildfires during the first half of august 2006 alone the choice of this time period 15 days was due to the fact that outside this time range the number of wildfires was quite low which would make it almost impossible to identify the effects on the probability of occurrence or burnt area of wildfires in addition it should be noted that the identification of the effects in this period of time is particularly important because of the large number of wildfires that would otherwise be difficult to identify most of these fires were concentrated along the coastal strip and south east of galicia see fig 1 at the same time we obtained the percentage of area affected by wildfires dividing galicia into a 1 1 km cells and calculating the burnt surface area in each grid for this the ignition point of each wildfire has been located in one of the cells from the burnt area data cells have been selected totally or partially affected by the wildfire around 99 of the grid cells remained fire free during the study period the explanatory covariates selected to predict the occurrence and burnt area of wildfires included meteorological covariates dickson et al 2006 which were obtained from meteorological monitoring stations distributed throughout the study area www meteogalicia es except the wind speed covariate that can highly influence the spread of fire but less than 10 of the meteorological monitoring stations recorded this information despite there being a total of 134 monitoring stations in place only 71 were operational at the time and thus recorded data the weather covariates selected for study purposes were daily average temperature in c daily average relative humidity in and rainfall in l m 2 we further restricted our analyses to meteorological stations situated at a maximum distance of 30 km from any given wildfire due to differences in altitude between fires and the nearest meteorological station it was necessary to correct the temperature and rainfall covariates by reference to the difference in altimetry the variation in daily average temperature with height was 0 65 c 100 m to 11 000 m nunez and calhoum 1986 i e t h 15 c 0 0065 c m h where t h is the air temperature at height and h is the difference between the wildfire and the nearest meteorological station likewise the rainfall gradient amounted to an 8 variation per 100 m difference in height between any given ignition point and the meteorological station selected i e p c o r r e c t e d m o n t h l y p m o n t h l y 0 08 where p is rainfall moreover we studied the impact of calendar time days in order to capture dependence on climatic factors in the preceding days and the altitude in each grid the spatial component was made up of 30 158 cells derived by dividing galicia into a 1 1 km cells with each cell being associated with one of the 315 municipalities in which the study area is administratively divided the centroid of each cell is associated with a municipal area so that each grid belongs to a municipality the database contained a total of 452 370 records 30 158 cells per fifteen days of study table 1 shows all the variables used in the study along with a brief description of each 3 statistical methodology 3 1 zero one inflated beta regression zero one inflated beta regression is particularly well suited to the analysis of wildfire occurrence and burnt area since it accommodates both fractional response values i e responses presenting percentages and responses observed at the extremes i e clusters of observations exactly equal to zero or one the zero one inflated beta distribution is defined as the mixture of one continuous component following a beta distribution and two point masses at zero and one such that the mixed discrete continuous density is finally obtained as p y π 0 y 0 1 π 0 π 1 1 b a b y a 1 1 y b 1 0 y 1 π 1 y 1 where b a b denotes the beta function b a b 0 1 y a 1 1 y b 1 dy with parameters a 0 and b 0 while π 0 and π 1 0 1 the probability π 0 is the probability of no wildfire while π 1 is the probability that the pixel is completely burnt therefore π 0 and π 1 are the probabilities for the point masses at zero and one respectively such that π 0 π 1 1 ospina and ferrari 2010 propose an alternative parameterisation of the zero one inflated beta distribution which enables an easier interpretation in the regression context firstly the continuous part corresponding to a beta distribution is parameterised based on the expectation μ a a b ε 0 1 and the scale parameter σ 1 a b 1 ε 0 1 in a second step the probabilities for the point masses at zero and one are replaced by ν 0 and τ 0 determined from the equations π 0 ν 1 ν τ π 1 τ 1 ν τ i e ν and τ are proportional to the probabilities for no and complete damage at a given observation point respectively the advantage of this parameterisation is that it avoids the constraint π 0 π 1 1 following the idea of distributional regression klein et al 2015 and generalised additive models for location scale and shape rigby and stasinopoulos 2005 we relate each of the four parameters of the zero one inflated beta distribution to regression predictors such that μ i exp η i μ 1 exp η i μ σ i exp η i σ 1 exp η i σ ν i exp η i ν τ i exp η i τ each predictor is chosen to be of a structured additive type as explained in the following section 3 2 structured additive regression structured additive regression fahrmeir et al 2004 provides a generalisation of generalised linear and additive models where a more flexible structure is assumed for regression predictors in generic notation a structured additive predictor is given by 1 η β 0 f 1 x f j x where the parameter index has been dropped for the sake of simplicity which also suppresses the potential dependence of the number of effects j and the chosen covariates x on the parameter β 0 is the overall intercept and the functions f 1 f j represent regression effects of different types of the generic covariate vector x each of the functions in the additive predictor is now approximated by a linear combination of basis functions b 1 x b k x such that f x k 1 k β k b k x where β k denotes the corresponding basis coefficients examples include penalised splines markov random fields or random effects as will be discussed in more detail in the next section in matrix notation a structured additive predictor of the form 1 can then be expressed as η β 0 1 z 1 β 1 z j β j where η is obtained by stacking all individual observations 1 refers to a vector of ones z j arises from evaluating the basis functions at the observed covariates and β j is the vector of corresponding basis coefficients to ensure favourable properties of the function estimates such as smoothness in the case of non linear and spatial effects or shrinkage in the case of random effects we rely on a bayesian formulation where prior distributions are assigned to the vectors of regression coefficients generically these are given by multivariate normal distributions p β j τ j 2 exp 1 2 τ j 2 β j k j β j where τ j 2 is a prior variance determining the impact of the prior distribution on the function estimates while k j is the prior precision matrix that implements the smoothness sparseness assumptions from a frequentist perspective these prior distributions correspond to penalties of the form λ j β j k j β j where λ j 0 is the smoothing parameter that determines the impact of the penalty on the penalised likelihood estimate 3 3 model components by using three specific instances of effects from the class of structured additive predictors we were able to allow for non linear spatial and random effects for non linear effects we relied on bayesian penalised splines eilers and marx 1996 lang and brezger 2004 where b spline basis functions are used to approximate the effect of a continuous covariate by way of a default we considered cubic b splines based on 20 equidistant knots something that yields twice continuously differentiable i e visually smooth function estimates and typically induces enough flexibility to capture even severe non linearity as a prior distribution we used a second order random walk that from a frequentist perspective corresponds to penalising second order differences of parameters for adjacent basis functions for the spatial effect we used a markov random field prior that assigns a separate regression coefficient to each region from a discrete set of spatial locations this implies that the basis functions in this case correspond to simple indicator functions for the different regions to enforce spatial smoothness we assumed a gaussian markov random field prior for the basis coefficients that induce a penalty where differences between effects of spatially adjacent regions were penalised for random effects we assumed independent and identically distributed normal regression coefficients a more detailed discussion of potential model components and how they fit into the generic framework outlined above can be found in fahrmeir et al 2013 3 4 bayesian inference for bayesian inference we based our approach on the distributional regression framework of klein et al 2015 who developed a generic markov chain monte carlo simulation approach in which proposal densities for blocks of regression coefficients are obtained from a locally quadratic approximation of the log full conditionals the smoothing variances τ 2 are assigned inverse gamma hyperpriors such that gibbs sampling steps can be realised due to the conditionally gaussian conjugate prior for the regression coefficients this approach is implemented in the bayesx software package www bayesx org with the bayesx 3 0 2 version being used for the purposes of our analyses belitz et al 2015 4 results 4 1 model specification the increased flexibility of distributional regression models for fitting complex response structures such as that observed for the occurrence and burnt area of wildfires comes at the price of increased model choice challenges more specifically for each of the four parameters of the zero one inflated beta distribution it was necessary to decide which covariates should be included in the corresponding predictor and which form the corresponding covariate effect should take e g linear versus non linear effects for continuous covariates based on prior biological and environmental knowledge this study focused on three meteorological temperature precipitation relative humidity and one environmental covariate altitude while simultaneously accounting for temporal and spatial variability included via a non linear effect of time represented by successive days from 1 to 15 august 2006 as well as a spatial effect defined on the basis of the 315 municipalities comprising the study area the meteorological covariates in particular influenced the state of hydration of dead fuels with low relative humidity and high temperatures providing favourable conditions for the initiation and development of wildfires lack of precipitation similarly affects both living and dead fuels and by extension the occurrence and burnt area of wildfire development based on this pre selection of covariates a number of preliminary models were fitted to determine which covariates were associated with specific distributional parameters of the zero one inflated beta distribution cubic penalised splines based on twenty inner knots and a second order random walk prior were used for potentially non linear effects the spatial effect was split into a markov random field for the spatially structured effect and an independent identically distributed i i d random effect for the spatially unstructured effect for all variance hyperparameters inverse gamma priors were used with both parameters set at 0 001 based on the consideration of these preliminary models the following specification was favoured both by theoretical considerations and the comparison of models via the deviance information criterion dic η i μ β 0 μ f 1 μ h r i β 2 μ t e m p i f 3 μ d a y i β 4 μ p p i β 5 μ a l t i t u d i f s t r μ c d c o n c i f u n s t r μ c d c o n c i η i σ 2 β 0 σ 2 f 1 σ 2 h r i β 2 σ 2 t e m p i f 3 σ 2 d a y i β 4 σ 2 p p i β 5 σ 2 a l t i t u d i η i ν β 0 ν f 1 ν h r i f 2 ν t e m p i f 3 ν d a y i β 4 ν p p i β 5 ν a l t i t u d i f s t r ν c d c o n c i f u n s t r ν c d c o n c i η i τ β 0 τ f 1 τ h r i f 3 τ d a y i f s t r τ c d c o n c i f u n s t r τ c d c o n c i for this model 12 000 iterations of the markov chain monte carlo simulation algorithm were performed with 2000 iterations being discarded as burn in and every 10th iteration stored to reduce autocorrelation from the markov chain as a consequence all inferences were based on samples of size 1000 drawn from the posterior predictive distribution of our model 4 2 estimated covariate effects to interpret effects in a distributional regression model it is necessary to overcome the challenge posed by the common type of ceteris paribus analysis in which changes in one covariate can be related to changes in the expectation of the response while all other covariates are held constant this change in interpretation results from the fact that covariates generally have an impact on all distributional parameters however the advantage of the general distributional regression framework for the zero one inflated beta distribution is that it allows one to evaluate effects on many interesting quantities linked to the response distribution and not just effects on the mean the following measures were considered the expected burnt area caused by a potential wildfire the posterior mean expectation i e the expectation of the zero one inflated beta distribution which is based on all four model parameters and accounts for the probability of no damage represented by the zero probability partial damage represented by the beta part of the distribution and complete damage represented by the one probability as a consequence the expected burnt area represents an overall risk assessment for specific covariate combinations the expected burnt area given fire occurrence the posterior mean conditional expectation i e the expected amount of damage given that there has been a fire at a specific location this allows for a conditional assessment of wildfire burnt area given fire occurrence whereas the unconditional expectation represents a mixture of occurrence probability and burnt area the variability of burnt area given fire occurrence the posterior mean conditional variance i e the variance in the amount of damage given the occurrence of a fire this can be interpreted as a measure of risk which quantifies uncertainty in predicting expected burnt area given fire occurrence if two locations in an expected burnt area are close together but differ considerably in terms of variability the location with the higher variance may be considered more risky and this in turn can provide additional insights for managing fire risks the probability of occurrence the posterior mean probability of occurrence i e the probability of observing either partial or complete damage at a given location figs 2 5 depict these quantities for the given covariates more specifically the quantities are shown to vary over the range of the respective covariates while all other covariates are held constant at their averages fig 6 shows the predicted values in comparison with those observed for the burnt area and the probability of occurrence this leads to the following conclusions in the case of precipitation the largest expected burnt area was identified for values of around 4 5 l m 2 i e relatively low values here it surprised that the burnt area expected for precipitation levels is larger than for dryer levels and this is probably a result of the two opposing effects on the conditional expectation increasing with precipitation fig 3 and the occurrence probability decreasing with precipitation fig 5 taken together this result in the mode at an intermediate value for larger amounts of rainfall the expected burnt area was small similarly the probability of occurrence decreased to zero monotonically with increasing amounts of rainfall and quickly approached zero surprisingly both conditional expectation and conditional variance rose with increasing precipitation and quickly approached their most extreme value of 1 this is likely to be an artefact induced by the linear specification for precipitation in our model which however was favoured in our initial mode comparisons and therefore seems to provide an adequate fit for the majority of the data points in the case of temperature it is observed that it does not seem to have a great influence on the probability of occurrence and behavior of the wildfires the origin provoked of practically all of them causes the ambient temperature to become a necessary condition but not sufficient for the occurrence of the wildfires in this way once the average daily temperature reaches about 25 30 c an increase does not significantly increase the probability of occurrence of wildfires as can be seen in the width of the confidence intervals the enormous variability in the thermometric extremes of the analyzed period is remarkable in summary there was a weakly increasing effect on all four parameters of interest with increasing temperature while this increasing effect is very plausible the overall explanatory power of temperature would seem to be small in the case of all four parameters of interest the highest values were achieved either at a relative humidity of around 30 40 or at a relative humidity of around 80 for the expected burnt area there was a clear peak at 30 40 relative humidity this fits very well with the observation that most fires occurred close to the coast where low humidity values of under 50 are common a similar effect structure was found for the probability of occurrence for the conditional expectation of burnt area the peaks at 30 40 and at 80 relative humidity were of roughly the same size while the conditional variance was largest at 80 relative humidity the latter finding hints at an increased risk at higher humidity values if a fire is actually observed the temporal effect indicated considerable variability over the study period with a clear peak for all quantities at around the 8th day of the two week period for the conditional variance there was a second peak at 12 days note that the temporal effect captures all kinds of unobserved heterogeneity that is left after adjusting for the included covariates and is therefore a surrogated of unobserved effects the effect of altitude was not too strong overall but there seemed to be some positive association with the conditionally expected burnt area there is an increase in variability from 1000 m of altitude this situation contrasts with that found in other studies of this same research team ríos pena et al 2017 where as the altitude increased the risk of wildfires decreased the methodology presented here seems to be more effective in integrating this variable into the predictive model the spatial effect reflected an increased risk towards the coastal areas a finding that fits well with the clustering of fire events along this strip despite allowing for covariates in our model specification there is obviously still some remaining spatial variation that is captured by the spatial effects 5 discussion wildfires are a major environmental economic and social threat in mediterranean europe they have become the main problem for environmental authorities oliveira et al 2012 in galicia they have a remarkable impact on certain areas of the autonomous region comunidad autónoma and pose a challenge for the future of the region the development of new methodologies especially those which are evidence based allows for more efficient organisation and planning of firefighting and an ensuingly smaller burnt area and lower risk of loss of life this study adopted and tested a new approach to the task of simultaneously studying the occurrence and burnt area of wildfires within the framework of structured additive distributional regression illustrated by reference to data for galicia as shown in fig 6 our model seems to fit both the probability of occurrence and the burnt area reasonably well structured additive models with non linear interaction effects which consider spatial information in the form of a location variable can accelerate the development of fire behavior models and prove most useful for drawing up fire prevention and firefighting plans ríos pena et al 2017 their principal advantage lies in their flexibility in that they can include spatial and temporal covariates along with the potentially non linear effects of other continuous covariates i e non linear covariate effects can be combined with the possibility of correcting for spatial autocorrelation through the inclusion of spatial effects fahrmeir et al 2013 in addition the zero one inflated beta distribution incorporates the beta distribution with degenerate distributions for the purpose of modelling extreme values the measures evaluated through estimated covariate effects namely expected burnt area of the wildfire expected burnt area given the occurrence of a wildfire variability of burnt area given the occurrence of the wildfire and probability of occurrence of a wildfire at a particular location showed fire risk scenarios for the covariates studied as indicated by krasovskii et al 2016 fire regimes are determined by climate vegetation and direct human influence in our case we have focused on the climatological and physiographic variables among the numerous contributions of approaches to wildfire climate relationships note the recent works littell et al 2016 nunes et al 2016 turco et al 2017 keeley and syphard 2017 as expected spatial and temporal weather conditions influenced the quantity and characteristics of wildfires trigo et al 2013 2016 established that large summertime wildfire activity was strongly related to previous climatic conditions in the north western sector of the iberian peninsula russo et al 2017 in that the periods of previous drought are a necessary condition for the presence of fires our data are centered of the specific days of the arson fires and in which the conditions were adequate for fires spread in the same way as dacamara et al 2014 use generalised pareto models to improve the general pattern of the canadian fire weather index fwi including among others temperature and 24 h cumulated precipitation one of the most important factors shaping behavior of the arson fires if we analyze relative humidity as it did bedia et al 2017 through multivariate spatial models next to roc curves are proposed their results show how the combination of low relative humidity and high air temperature appears in the large fires in the mediterranean region which coincides with our results in the case of the august 2006 fires in galicia these conditions were ideal for fire spread on analysing and comparing the specific variables of the model it seems only logical that an increase in temperature would make it easier for wildfires to occur in the case of aragon spain molina terrén and cardil 2016 observed that when the air temperature was higher than 20 c at 850 hpa the frequency and burnt area of fires increased our results in terms of average and variance show that temperature exerts a positive influence on the percentage of burnt area oliveira et al 2014 explained the importance of temperature for wildfire occurrence in mediterranean europe and we found that it was also a key factor for wildfire ignition and behavior in greece paschalidou and kassomenos 2016 showed that forest fires and burnt forest area were produced in some very specific weather conditions similar to our work regarding altitude our results show that it does not present a significant importance for ignition and fire behavior it seems an interesting debate because other authors have analyzed the influence of altitude on fires in the iberian peninsula the model proposed by botequim et al 2013 to predict annual wildfire risk in pure and even aged eucalyptus stands in portugal showed altitude as having no significance coinciding with our results in a larger scale study of all wildfires in portugal from 1987 to 2004 however marques et al 2011 reported that the higher the altitude and distance to roads the higher the proportion of burnt area in a nearby area in contrast ordóñez et al 2012 found altitude to be a negative factor in their generalised linear and generalised least squares models the explanation given by these authors for this surprising result was that altitude plays a negative role in the probability of a fire being ignited by lightning authors like rodrigues et al 2016 conclude that the more mountainous areas to make up for the greater risk of fires with the lowest human activity it is remarkable the relation between the elevation and the density of roads and its influence on the behavior of the fire yang et al 2007 and narayaraj and wimberly 2012 have developed models to measure their influence in galicia chas amil et al 2013 indicate that the density of roads is correlated with fires but in turn conditioned by population density balsa barreiro and hermosilla 2013 concluded in an analysis of the same fires we studied that the proximity of roads and inhabited areas caused the fires to increase in size at the end the researchers do not reach a consensus on the relationship between altitude and fire possibly due to the interactions of other variables such as land use population density and roads with respect to the variables with non linear effects several authors have incorporated relative humidity rh in their indices and in the canadian forest fire weather index in particular camia et al 2008 dimitrakopoulos et al 2011 bedia et al 2013 padilla and vega garcía 2011 analyzed the variables that accounted for the fires in each of the 53 regions into which they divided spain and concluded that rh provided a good explanation for the fires in the interior of the country but not those in the galician region pereira et al 2013 found that the worse wildfire situation in portugal was due to a combination of wind and low rh our results are in line with this finding in that a rh of around 60 or lower was shown to be accompanied by more and larger fires our analysis of spatial effects showed that fires in august 2006 in galicia were not distributed randomly but were instead concentrated in certain municipalities in the south western coastal area without incidence in the north eastern region in the same sense chas amil et al 2015 noted that while the southern and eastern areas acted as hot spots with a clustering of parishes with high fire counts the north eastern areas acted as cold spots with a clustering of parishes with a low number of wildfires other papers of this team and in co authorship with other researchers have recently used different methodologies to study the distribution of fires in galicia fuentes santos et al 2013 analyzed the spatial distribution of ignition points in the fonsagrada ancares forest district with ripley s inhomogeneous k function their results indicate that there was spatial dependence between fires within an interaction radius of 4 km for the whole period 1991 2008 and within an interaction radius of 3 km in 1992 subsequently fuentes santos et al 2015 used smooth bootstrap kernel intensity estimation for inhomogeneous spatial point processes to analyze the entire region in 2006 and concluded that the spatial distribution of arson and natural wildfires was different boubeta et al 2015 applied poisson mixed models with areas included as random effects to 63 forest areas constituting the basic structure of the fight against wildfires in galicia during the summer of 2007 the results showed significant differences between forest areas across the period of analysis similar to our results lastly having already developed a sound approach to studying the probability of occurrence of wildfires in space and time by using star models for the same data ríos pena et al 2015 2017 we now present a refined and highly reliable method of study and understanding two simultaneous phenomena i e the occurrence of wildfires in a specific place at a given time and the burnt area of the wildfire as measured by the percentage of the area burnt in that place 6 conclusions since the last third of the 20th century there has been an increase in the number of forest fires affecting mediterranean europe with repercussions on social environmental and economic activities in particular accordingly forest service offices and researchers have been developing models capable of predicting fire occurrence zones most of the models developed to date have made separate predictions for the number and behavior of fires in view of the results yielded by this study star models based on the zero one inflated beta distribution can be said to afford a simple and satisfactory way of predicting both the probability of the occurrence of a wildfire and at the same time the burnt area associated with such a fire hence we suggest that this statistical tool is a novel methodological contribution to the study of wildfires we validated the novel modelling approach on data from galicia nw of spain in august 2006 when in a rather extreme scenario a total of 83 000 ha were affected by wildfires this in fact places the model in its operational context now and in the future regional policy makers affected by wildfires need models of support for extreme situations with simultaneous concentrations of fires in time and space they need to know the number of events and the surface that can be affected in each climatological situation conflicts of interest the authors declare that they have no conflicts of interest acknowledgements t kneib and c cadarso suárez would like to thank the spanish ministry of innovation competitiveness for support from the mtm2014 52975 c2 1 r project and m marey pérez and c cadarso suárez would like to thank the galician regional authority xunta de galicia for supporting research of the inbioest network 2014 pg141 and competitive research reference group proepla cn2012 015 the work of n klein and t kneib was supported by the german research foundation dfg via the research project kn 922 4 1 2 appendix a supplementary data the following is the supplementary data related to this article supplementary material supplementary material appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 03 008 
26316,industrial symbiosis aims to stimulate or enhance cooperation between industrial firms to utilize industrial waste streams from other industries and to share related knowledge in order to achieve sustainable production recommenders can support industries through the identification of item opportunities in waste marketplaces enhancing activities that may lead to the development of an active waste exchange network to build effective recommendation we study the role of knowledge in the design of a recommender that suggests waste materials to be used in process industries this paper compares the performance of a knowledge based input output recommender with a recommender based on association rules the two recommenders are evaluated with real world data collected through deploying surveys in a workshop setting our research shows that many data challenges arise when creating recommendations from explicit knowledge and suggests that techniques based on the concept of implicit knowledge may be preferable in the design of an industrial symbiosis recommender keywords industrial symbiosis recommender systems decision support systems input output matching association rule mining 1 introduction the reduction of waste emissions and primary resource use in resource intensive industries is suggested as one of the critical pathways to accelerate sustainable development european environmental agency 2016 the european union highlights industrial symbiosis is as a methodology that stimulates industries to become more sustainable this policy follows from the vast amount of positive environmental effects generally associated with the methodology and its ability to scale at a european level laybourn and lombardi 2012 is entails the identification and utilization of an organization s traditional secondary production process output that is generally considered as waste such waste can be used to substitute part of a primary resource possibly after some pre processing in the production process of another organization usually located in a different industrial sector chertow 2000 both researchers and practitioners recognize the impact of is as a useful business opportunity and tool for eco innovation it involves engaging diverse organizations in a network to foster eco innovation and long term culture change creating and sharing knowledge through the network yielding mutually profitable transactions for novel sourcing of required inputs value added destinations for non product outputs and improved business and technical processes lombardi and laybourn 2012 page 28 new is opportunities are mostly identified using facilitated industry workshops paquin and howard grenville 2012 van beers et al 2007 mirata 2004 is identification systems grant et al 2010 and waste exchange marketplaces dhanorkar et al 2015 the key role of these methods is to facilitate information exchange of waste and resource interests van capelleveen et al 2018 numerous pathways explain the emergence of is proposed in different stylized models chertow 2007 paquin and howard grenville 2012 boons et al 2017 eco industrial parks generally involve a continuous effort from coordinating bodies e g municipalities or regional governments to locate industries that can potentially cooperate together in the park regions in order to share wastes and by products gibbs and deutz 2007 other is based industrial ecosystems arose by self organization resulting from collaborations without top down planning and mainly driven by economic or strategic business motivations that lead to increasing resource and waste transactions over time chertow and ehrenfeld 2012 a third type of is emergence is a facilitated approach that utilizes intermediaries that provide a role of strengthening trust between firms using expertise and the ability to connect industries paquin and howard grenville 2012 these pathways not only characterize the different types of emergence and explain how the process of is unfolds but also help to deduce the critical catalyzers to initiate new symbiotic actions boons et al 2017 in relation to the facilitated approach various scholars have studied waste exchange systems as a tool to enhance is identification clayton et al 2002 sterr and ott 2004 mirata 2004 chen et al 2006 van beers et al 2007 veiga and magrini 2009 grant et al 2010 dietrich et al 2014 dhanorkar et al 2015 cecelja et al 2015 hein et al 2015 cutaia et al 2015 van capelleveen et al 2018 such systems can enhance symbiotic transactions in the network while substantially reducing the time investment required to investigate the symbiotic potential the capability of supporting the learning and decision making processes of environmental problems recurs as an interesting opportunity to assess poch et al 2004 decision support tools grant et al 2010 and in particular recommender support van capelleveen et al 2018 are suggested as promising techniques to stimulate and facilitate the identification and assessment of new exchanges recommenders are able to support users in identifying item opportunities and to pro actively engage system use resulting in both increased sales and a more active community freyne et al 2009 pathak et al 2010 gomez uribe and hunt 2015 however building systems that can provide decision support or recommend is opportunities remains a challenge grant et al 2010 firstly many of such tools lack the key characteristic of sociability grant et al 2010 focusing more on determining technical opportunities rather than building human relationships secondly while a critical mass of industries is required to engage in network activity it is difficult to attract them to join the network finally systems struggle with analyzing data because of the high level of implicit knowledge which is a burden to the development of techniques that help to identify is exchanges grant et al 2010 this particular data challenge is the key focus of this research for example process data from manufacturing industries that disclose inputs outputs and wastes can be both used for the identification of potential synergies as well as to provide input for recommender systems however the extent and level of detail with which such data is shared may be hindered by a lack of trust among organizations because process data might reveal competitive information that organizations want to keep partly confidential paquin and howard grenville 2012 moreover organizations have to justify time and resource investment to explore potential ideas for which the expected benefits are not clearly predicted or even known therefore the provision of detailed process data in a wider context may increase identification of is opportunities but this is challenging at the initial stage this study contributes to the discussion initiated by grant et al 2010 on the role of implicit and explicit knowledge in waste marketplaces by transferring the implications of data characteristics to recommendation technology implicit knowledge sometimes referred to as tacit knowledge is know how that is subconsciously understood and applied it consists of complex information that individuals are unable to express and codify moreover implicit knowledge resides primarily in its field of application on the other side of the spectrum explicit knowledge is formally articulated referring to situations where one is able to capture know how into codification schemes that can be communicated and used to reason with kogut and zander 1992 zack 1999 the theory on implicit versus explicit knowledge is used in organizations supply chains and markets to seek for explanations of problems related to communication reasoning and broader knowledge management schoenherr et al 2014 kimble 2013 a similar discussion on implicit versus explicit knowledge is also prevalent in the field of recommender systems bobadilla et al 2013 for example data can be derived from implicit e g monitoring user s behavior and explicit content e g user ratings also filtering techniques can be built using either an implicit knowledge based e g association rule mining or an explicit knowledge based e g case based reasoning recommendation hence an important question is what effects do these different recommendation techniques have on analyzing environmental domain data in order to create recommendations therefore the aim of this study is to evaluate and understand the influential role of knowledge in the design of an effective waste material recommender for is marketplaces in this paper we examine the problem of is identification by creating a model for utilizing environmental data in recommender systems we therefore employ the design science method peffers et al 2007 gregor and hevner 2013 for constructing our recommender artifact that utilizes a novel input output io algorithm this io algorithm is evaluated through a comparison with another recommendation algorithm we have developed based on association rule mining arm section 2 describes the methodology used while section 3 presents the design of a generic approach to create explicit and implicit knowledge based recommendations in the environmental data landscape this part provides an instantiation of our proposed model that is applied to is by designing two algorithms an explicit knowledge based recommender and an implicit knowledge based recommender section 4 presents the result of a comparison of the two algorithms section 5 discusses the interpretation of recommender performance and reviews the internal and external validity of the achieved results finally section 6 concludes the paper with implications and future research directions this structure follows the guidelines suggested by gregor and hevner 2013 2 method the main objectives of this study are to design a model that utilizes environmental data to make is recommendations and to develop an instantiation of this model to identify is opportunities this design science research is guided by peffers et al 2007 in relation to the discussion of grant et al 2010 our study focuses on the role of implicit versus explicit knowledge in the design of recommenders thus the instantiation of the model represents both an implicit and an explicit knowledge based recommender design we evaluate the design empirically by investigating the extent to which explicit and implicit knowledge influences the effectiveness of the recommendations our research approach consists of four major steps 1 data collection 2 the design of a model for utilizing environmental data to create recommendations 3 an application of this model to the problem of identification of is resulting in 3a the design of techniques for pre processing and 3b the design of an implicit knowledge based as well as an explicit knowledge based recommender and finally 4 a comparative design evaluation using recommender evaluation metrics 2 1 step 1 data collection two data sources are utilized as a knowledge base for making recommendations firstly data is collected from industrial symbiotic workshops this is data containing a variety of waste items and resource interests from industry originate from is workshops held in two different european industrial clusters the data were collected as part of the eu funded sharebox project european commission 2017 this data is explored in order to identify the item properties that are valuable to generate useful recommendations secondly knowing that confidentiality is a key challenge in is development we study the usefulness of an external database which provides the process input output data from life cycle inventories this database contains data regarding the wastes produced and primary resources used in the production activities of process industries in addition it serves as a knowledge base to investigate potential synergies between industrial firms based on the substitution of traditional primary resources with wastes a detailed overview of both data structures is presented in section 3 2 2 2 step 2 model design after studying the data we create a model that uses a step wise approach to explain how to build a recommender system that can operate under challenging data characteristics e g the noise in survey data see fig 1 such noisy characteristics are often present in environmental data sets due to ill defined and unstructured data collection practices 2 3 step 3 instantiation of the model next we create an instantiation of this model by applying it to the problem of is identification based on the data gathered according to the model see fig 1 firstly the data requires pre processing before it becomes useful as input to recommender algorithms in the case of is data this involves the clustering of ephemeral items in the data set into latent product concepts based on a technique described in chen and canny 2011 which is explained in detail in section 3 3 then we design both an implicit and an explicit knowledge based recommender explicit knowledge refers to the use of an external knowledge base while implicit knowledge is used for a recommender that learns from data on the behavior of users in a marketplace the explicit knowledge based resource recommender is constructed on the concept that a waste to input match i e a waste that can substitute a primary resource can be predicted by utilizing information about the manufacturing processes associated with a particular industry often the manual use of life cycle inventory lci databases containing explicit knowledge for the detection and assessment of new is business opportunities already takes place grant et al 2010 van capelleveen et al 2018 industry profiles are created that contain the major inputs and outputs associated with the industry s manufacturing processes our explicit knowledge based recommender exploits the potential of inventory databases that provide process data revealing the inputs outputs and wastes associated with the manufacturing of products in a particular industry on the other hand the implicit knowledge based recommender utilizes the well known techniques of association rule mining agrawal et al 1993 in order to evaluate the impact of implicit knowledge on the recommendation association rule mining is a promising technique often applied in e marketplaces to recommend items park et al 2012 hence it is expected to be a good candidate for an implicit knowledge based recommendation technique 2 4 step 4 design evaluation the final step of the methodology involves running the algorithms on the is data set and evaluating the performance of the recommenders usually recommender evaluation is first performed in an off line setting using sample data in this case the is workshop data such data is either derived from a relevant external data source or extracted from the on line system for which the recommender is designed ekstrand et al 2011 we evaluate recommendations by measuring the prediction effectiveness in terms of a binary classification more precise we classify the predictions as recommended items of non recommended items and use this binary classification to measure the performance of an algorithm then we quantify four metrics for evaluating the performance i e precision recall accuracy and the f measure in a recommender context the precision and recall measures are described in terms of a set of retrieved items that are then compared to the set of items relevant to a user in equations 1 3 t p denotes the number of true positives f p the number of false positives t n the number of true negatives and f n the number of false negatives a true positive is a recommendation that matches the user s possesive stated preference a false positive is a recommendation that does not match the user s possesive stated interest a true negative regards the situation when no recommendation is provided and the user has no stated interest a false negative occurs when no recommendation is provided to a user who has a stated interest precision is the fraction of recommended items that are relevant to a specific user see equation 1 recall is the fraction of relevant recommended items that are retrieved see equation 2 accuracy is the fraction of measurements of a true value where the true value measurements consist of both the recommended items relevant to a user and the non relevant items that are also not recommended see equation 3 the f measure is a different measure of a test s accuracy that evaluates the precision and recall in a weighted harmonic mean see equation 4 1 p r e c i s i o n t p t p f p 2 r e c a l l t p t p f n 3 a c c u r a c y t p t n t p t n f p f n 4 f m e a s u r e 2 p r e c i s i o n r e c a l l p r e c i s i o n r e c a l l recommender performance is measured at two levels i e 1 an item level and 2 a cluster level this distinction is based on our assumption that many items in a resource marketplace are considered by users as similar products based on their usefulness e g iron scrap from one organization is as good as those of other organizations see also section 3 3 where we discuss how we cluster items into latent products thus we also can evaluate a recommendation as being valid if either the exact item is retrieved item level or if one of the latent product items one of the items belonging to an item cluster is retrieved cluster level 3 model design recommendation in an environmental data landscape 3 1 the model fig 1 presents a generic approach to create recommendations out of environmental data that is the result of ill defined or unstructured data collection the resulting data characteristics may not only constrain the effectiveness of a recommender but also affect necessary data transformations and taxonomy translations prior to making a recommendation the model addresses such data issues by building recommenders based on the implicit versus explicit knowledge based reasoning grant et al 2010 kimble 2013 schoenherr et al 2014 it contains an explanation of how implicit and explicit knowledge based recommendation algorithms are affected by the characterization of this data landscape explicit knowledge based recommenders require the utilization of external standardized databases containing detailed data as explained in the model fig 1 environmental data is often noisy not transparent non standardized and incomplete to create a recommendation often a number of actions are required including preprocessing of data mapping data and the selection of an algorithm fig 1 shows that both types of algorithms implicit or explicit knowledge based require the pre processing of data but explicit knowledge based recommenders also often require the data to be mapped because of their dependence on external knowledge bases once the environmental data is prepared and mapped the recommender algorithms are able to generate recommendations 3 2 the model applied to the case of industrial symbiosis recommender systems can contribute to the reduction of time and resource investments of industries by guiding organizations to the first set of potential waste items that are likely to involve industrial symbiosis business cases the following section provides illustrative examples to understand how the two recommender algorithms utilize data to construct recommendations fig 2 provides information about the composition of the original workshop data the workshops were held in two different european regions and a record was made of the waste items of participating organizations as wells as their expressed interest this data has similar characteristics to those presented in waste ontologies for waste markets cecelja et al 2015 raafat et al 2013 the data set has a sample size of 421 for region a and a sample size of 150 for region b in this data we identify five different classifications of items namely 1 materials 2 tools 3 services 4 energy and 5 others the materials are of particular interest as the input output based recommender is designed to predict the material preference of an organization the process industry is considered a good target audience for such a recommender therefore in recommending items organizations that do not fit the profile of a manufacturer of materials were excluded from the recommendation hence in the evaluation of both recommenders a selection of only the offered items by sellers of waste categorized as material is utilized this resulted in a total of 139 waste items for region a and 54 waste items for region b this sample size is relatively small compared to the evaluation of recommenders in many other types of e marketplace studies the reason for this is because is waste markets do not have such large participation thus careful interpretation of the statistical validity of the recommender performance is needed in particular one needs to keep in mind that there is a possibility of overfitting the model of the arm algorithm in this paper we will also derive qualitative insights on the applicability and challenges of recommender design in is marketplaces table 1 1 systems for statistical classifications of economic activities were beneficial to identify industry types e g the international standard industrial classification of all economic activities isic or the statistical classification of economic activities in the european community nace nace is derived from isic in the sense that it is more detailed than isic isic and nace have exactly the same items at the highest levels where nace is more detailed at lower levels eurostat 2018 2 european waste catalogue european commission 2000 shows a sample of the used subset of data acquired from is workshops organized to facilitate the creation of new is initiatives among industries we notice that most waste items addressed in the is workshop survey data often have short waste descriptions commonly defined with less than 10 words the data illustrates the noise in item descriptions e g from waste item a it is difficult to understand the exact relationship between the iron steel the slag and the concrete tiles however we can consider to a certain extent that the offered item s have a potential for iron reuse the industry description contributes as shown further on to identify the related traditional inputs to that firm during on line evaluation the arm algorithm can learn rules from system transactions the workshop however provides off line data containing the interests of organizations in items the survey data is considered a representative data set for is market transactions in particular because such items share the characteristic that users compose their own item descriptions when offering these in either an online marketplace or during a workshop therefore these stated interests are interpreted as transactions to detect rules tables 2 and 3 illustrate the structure of the data that is obtained from ecoinvent one of the world s largest lci databases table 2 shows examples of manufacturing processes of goods or services that are available e g the production of iron casts table 3 shows examples of the inputs and outputs connected to those production processes e g iron furthermore information is included regarding the amounts usually associated with the production of one functional unit using the analysis of the is and lci data we can build an instance of our earlier proposed model cf fig 1 that addresses the challenge of identifying new is business fig 3 illustrates this instantiation in fig 3 the is workshop data represents the environmental data box from fig 1 and the lci database represents an external standard database the particular data characteristics represented in fig 3 help in the design of the data pre processing techniques the mapping and the recommender algorithms in the following three sub sections sections 3 3 3 4 and 3 5 we explain how we handle the pre processing of the data i e the need to apply natural language processing nlp in order to cluster groups of similar items steps 1 and 2 in fig 3 after which we show the mapping of items on the input output database and the design of a recommender algorithm based on this explicit input output knowledge steps 4 and 5 in fig 3 as well as the design of a recommender algorithm based on implicit knowledge using association rule mining step 3 in fig 3 3 3 pre processing is data the pre processing of data is addressed by steps 1 and 2 in the model in fig 3 this consists of applying nlp and of the clustering of items a major challenge to many recommenders is that data are often sparse e g due to low item transaction history and hence item space reduction is needed similar to data in e commerce marketplaces that predominantly consist of ephemeral items these ephemeral items are items submitted by users and composed of users individual product descriptions thus item descriptions hardly correspond to any catalog taxonomy and often lack detailed product descriptions in addition the frequency with which the supply of items is renewed in a marketplace is high wroblewska et al 2016 therefore item space reduction strategies are often applied to reduce the number of item types increasing the number of transactions of each item type e g considering all items listing iron fillings as equal iron waste products item space reduction methods thus create a richer history for each item type by grouping items and treating them as one item space thereby allowing algorithms such as arm to deduce more and stronger associations moreover it can improve the io matching of items with short item descriptions by using a richer product concept description hence grouping similar items helps to increase quality efficiency and effectiveness of recommender algorithms chen and canny 2011 nlp is used to assess the similarity between waste item descriptions an obvious choice to group waste market items would be to rely on existing taxonomies that already play an important role in waste treatment for example the european waste catalogue ewc european commission 2000 or the central product classification cpc united nations statistics division 2015 are examples used in reports on waste disposal and in duty of care documents in waste transfers natural resources wales et al 2015 that define the similarity between items however such classifications are often absent in item descriptions moreover these systems classify goods and services in the industry from which they originate causing an overlap in product concepts within the taxonomy thus such a taxonomy fails to relate two similar waste items if they are produced in different industries the isdata project 2015 sander et al 2008 for example recycled glass can be produced either with uncontaminated glass residues resulting from a glass bottle production facility or it may be extracted from construction and demolition waste international synergies ltd 2016 a solution to this problem is to map items into latent product concepts chen and canny 2011 by inferring the latent product concept the dynamics and diverse item inventories can be used to group items considered as similar or identical products by a recommender this way item clustering reduces complexity in an intermediate step towards enabling recommender algorithms to learn from data in such contexts hence there is a trade off between having item specific information to be used by a recommender enable individual based reasoning and building item history on similar types of items history is required to deduce user preference the elbow technique salvador and chan 2004 is used to detect the cut off point for clustering thereby identifying the trade off the clustering algorithm see a 1 based on chen and canny 2011 explains how to cluster similar waste items the algorithm extracts the latent product concept from item descriptions in the is data set it uses stem frequency vectorization as a means to identify the latent product concept of an offered waste item stemming is the process of removing the inflectional forms and sometimes the derivations of the word by identifying the morphological root of a word manning et al 2008 the frequency vector of an item within a set of item descriptions is determined by the frequency of every unique stem in that item along with the unique stems in the set an example of such an item vector is provided in fig 4 in order to derive the set of stems from an item description we use the nltk package a platform for working with human language data in python natural language toolkit 2017 firstly all the characters are converted to lowercase and all numbers and special characters are removed then items are tokenized into a bag of words using the nltk corpus all english stop words are removed from these bags and some non significant terminology commonly used in is is filtered as well e g waste material and process finally the empirically justified porter algorithm porter 1980 is applied to stem the bag of words then the resulting item vectors are utilized in a multi dimensional hierarchical agglomerative clustering algorithm based on an algorithm presented in manning et al 2008 to cluster items using the cosine similarity of item vectors see a 2 in contrast to most clustering methods in our multidimensional clustering the traditional coordinates x y are replaced by the stem frequency vector in hierarchical clustering the determination of the elbow is a common method to determine the number of clusters the method utilizes statistical variance in order to explain the best number of clusters the variance is calculated using the sum of squared errors sse as a function of the decrease in the number of clusters the method exploits the point of maximum curvature in the sum of squared errors sse of all clusters as the cut off point for clustering the maximum curvature is the point on the curve with maximal bending also explained as the point where the curve appears to curve the most salvador and chan 2004 this point often represents a knee or elbow i e the point that shows a marginal drop or gain in variance this point determines the finally selected number of clusters although evaluation methods that can mathematically locate the elbow in a curve salvador and chan 2004 exist visual interpretation is preferred over statistical methods when no objective measures have been defined for cluster optimality in the domain of application jung et al 2003 3 4 design of an explicit knowledge based is recommendation this section addresses steps 4 and 5 in fig 3 i e the mapping of items to the input output data in the lci database and the io algorithm design the io based recommender see algorithm 1 is a knowledge based recommender suggesting items based on the inferences about the needs and preferences of a user burke 2002 this type of recommender makes use of explicit knowledge a knowledge base to recommend items to users the algorithm is based on the assumption that the items offered in is marketplaces correspond to the specific item interest of an organization and subsequently to the primary inputs in their manufacturing processes as primary inputs of a production process we consider only those inputs that are not provided by one of the production processes of the associated production chain such as raw materials and natural resources e g silver ore limestone clay water and energy resources e g gasoline natural gas coal the primary outputs result from a manufacturing process which is the primary focus of production secondary output generally refers to waste that has no perceived economic value and that traditionally is to be discarded as this method uses the identification of potential relations between the primary inputs of production processes and secondary output waste offered in a marketplace we indicate this type of raw material recommender as an input output recommender io recommender the information on resource use for the production of goods or services originates from an lci database in our case the ecoinvent database version 3 3 ecoinvent 2017 this type of database provides well documented process data for thousands of production processes including information on the raw material consumption in particular this information about primary inputs is used to identify the most consumed raw materials of an organization and matches those to the potential waste resources available in the marketplace recommendations made by the io recommender algorithm are created through the use of pre compiled manufacturing profiles a profile is constructed through the identification of the type of products produced by an industry typically constructed using the corresponding company websites the products are used to find the associated manufacturing processes available in lci databases this results in predefined profiles created for each type of organization e g a manufacturer of castings the profiles list among others the raw material consumption that is associated with the selected manufacturing processes the profiles of the industries required by the algorithm are constructed prior to recommendation once the profiles are created the algorithm extracts all resources listed from the lci database that is linked to a production process in the organizational profile then it filters the resources which make up the largest fraction of resource consumption within the production of a product these resources are selected as the candidates for recommendation the resource candidates are then compared with the marketplace item descriptions to see whether they contain similarities the matching algorithm calculates the cosine similarity between the vectorized stem frequency of a waste product description a cluster and that of a resource description the stem frequency vectorization algorithm applied in the clustering of items see section 3 3 is identical to the one used for io matching the algorithm operates as follows first for each resource connected to the industrial profile of the organization the algorithm iterates over all clusters in the marketplace within this iteration a bag of stems is created for both the selected cluster the latent product concept and the selected resource from the industry profile the first bag of stems created from the cluster selects the most frequently occurring stems in all items descriptions belonging to that cluster the second bag of stems is created from the resource and uses the name of the material based on the taxonomy used in the lci these bags of stems are used to compose the item vectors for both the cluster and the resource next the similarity between the two item vectors is assessed using a cosine similarity function the stem frequency item vector combinations that exceed the minimum cosine threshold and are not yet a recommendation are added to the set of recommendations algorithm 1 input output recommender algorithm image 1 3 5 design of an implicit knowledge based is recommendation association rule mining is one of the techniques based on implicit knowledge that found successful application in recommender systems park et al 2012 the arm technique attempts to discover regularities in transaction data based on the concept of strong rules agrawal et al 1993 in general association rules perform best on large scale data with a broad history of transactions the popular apriori algorithm is used to generate candidates for identifying these rules as it is both simple and exact agrawal and srikant 1994 the pace at which users change preference affects the decay of association rules to find the best length of the time period from which rules can be detected requires an estimate of how stable the user preference is over time the nature of is is characterized by a low frequency of transactions but preference is considered to remain fairly stable therefore in the context of is it is reasonable to select a longer period from which association rules may be learned this period ranges from several months up to years to recommend on the basis of association rules one needs to define which rules are accepted to generate recommendations it is common to set a threshold for the minimal support and the minimal confidence in order for association rules to become a rule for recommendation support is a measure of item pair frequency it is defined as the percentage of transactions in a dataset that contains a particular item combination confidence is a measure of rule strength defined as the number of times an association rule is valid in case of a relatively small dataset k fold cross validation is applied in order to increase the validity of the evaluation k fold cross validation is a technique that randomly partitions data into equally sized samples in order to test an algorithm in various test and training combinations algorithm 2 k fold association rule mining recommender image 2 4 design evaluation 4 1 data preparation fig 5 presents the results of how clusters were formed using the maximum curvature concept explained in section 3 3 the resulting number of clusters k is 84 for region a and 40 for region b respectively the clustering is applied to both data sets separately the remaining parameters of the algorithms are consistently used and initialized with similar values for both data sets for the input output algorithm a minimal cosine similarity of 0 1 is set reflecting that one of the stems derived from the item description matches the resource taxonomy of the lci this value needs to be kept low as nearly all item descriptions contain more stems often not more than 10 than the solely resource taxonomic term used in the lci for example a resource taxonomy names a material iron while the item description contains more words than the resource term only e g iron steel and slag concrete tiles can be taken as one of the main components see table 1 of course having more stems to consider increases the likelihood to find a match between the is data and the external database however it may also increase the number of incorrect matchings the k fold cross validation technique used in the arm uses the value k 10 furthermore the minimal support parameter is initialized with a value of 0 1 and the minimal confidence parameter with a value of 0 7 as thresholds for a recommendation rule the value for minimal confidence is selected based on a sensitivity analysis which examines the highest potentially achievable result martnez ballesteros et al 2016 fig 6 shows this sensitivity analysis from which we obtained the optimal initialization value of minimal confidence the intersection of lines representing precision and recall is generally recognized as a good balance see the crossing of lines in fig 6f although it could be desirable in a system implementation to exploit precision in favor of recall and a larger number of recommendations we select this point to present the lower bound for our recommender system 4 2 results of recommender performance table 4 shows the results of a comparison of the two recommenders the results are presented with three variables in the first column we distinguish between the two recommender methods that were tested the second column shows the level of measurement item level or cluster level as explained in section 2 the third column defines the data set used to test the algorithm note that the average performance for the combined regions a b is constructed by first aggregating the recommendation results of the individual regional experiments in order to calculate the combined performance metrics table 4 also presents the percentage of items that were recommended as well as the values of all performance metrics precision recall accuracy and the f1 or f measure the io and arm method are first compared at cluster level the results demonstrate that the arm method outperforms the io method on average with a substantial difference in precision ranging from 4 to 6 times and 2 or 3 times on recall in general also the evaluation of the accuracy and f measure shows a noticeable difference in performance a similar substantial difference is found at the item level the results of the arm method compared to the io method is 3 4 times higher for precision and 2 to 3 times for recall that both methods score much lower at the item level is most likely explained by the number of similar items organizations purchase for example if three similar items grouped into a latent product are available and one organization only purchases two of these then the precision at item level is lower than at the cluster level the scores on accuracy require more detailed explanation the results show that the accuracy values in the experiments are less different ranging from 0 7 to 0 9 however accuracy measures may portray a misleading perspective of recommender performance although it is true that the accuracy results indicate that many items were correctly classified the majority of correct classifications in this experiment consists of items that were correctly rejected for recommendation the effectiveness of a recommender is more clearly understood by having high precision rates along with a reasonable recall rather than by the overall accuracy result therefore also the f measure is calculated f measure provides a combined perspective on precision and recall and a better demonstration of the difference in performance of the recommenders finally the two regions show a small difference in the performance of recommendation algorithms regional characteristics may have affected the performance of the algorithms the io method shows only small differences between the regions however the arm method shows a larger disparity between regions i e region b performs considerably better at precision than region a while no such precision increase is achieved with the io method this can be explained by the fact that in a relatively small data set only a small number of rules can be deduced it can be understood that in region b the recommender model overfits based on the few learned rules with a larger dataset it is likely that more rules would be available leading to a considerable drop in confidence levels consequently a lower precision recall balance would be established based on the larger set of rules the wilcoxon signed rank test is applied to show that the results obtained in our experiment are significantly different see table 5 this test compares the two algorithms based on the paired results between industries without making distributional assumptions on the differences shani and gunawardana 2011 hence the test that detects whether the performance difference between the arm and the io recommender is statistically significant when evaluating at the cluster level using data from both regions a and b such a hypothesis is non directional and therefore the two sided test is applied all tests are applied at a 95 confidence interval thus using an α of 0 05 the resulting wilcoxon critical t values for each sample size n are obtained from the wilcoxon critical t values list wilcoxon and wilcox 1964 the selected wilcoxon test adjusts for the ranking in case of ties furthermore it is initialized with the setting that accounts for the likely binomial distribution in a small data set by applying continuity correction both the precision and recall results are significant at p 0 05 because n 20 we confirm the difference by showing that t 52 for precision and t 46 for recall tests at item level also result in a confirmation that there is a significant difference for both precision and recall between the arm method and the io method see table 5 5 discussion 5 1 the role of explicit and implicit knowledge for the total set of available items the experiments show that a substantial number of recommendations can be generated using both the arm and the io based methods however the results indicate a preference to utilize implicit knowledge over explicit knowledge in recommender design for is identification in both regional data sets measured at item and cluster levels the arm algorithm is clearly the better performer for most of the recommender metrics except for accuracy that shows a weaker and less consistent difference these findings confirm the ideas of grant et al 2010 that implicit knowledge is a challenging characteristic of industrial symbiotic markets that partly affects the success of decision support designs nevertheless we see a role for recommenders in is markets in particular for implicit knowledge based algorithms with high precision rates recommenders could attract organizations to form a critical mass that can sustain sufficient supply demand in the marketplace such recommenders could be applied through e g newsletters that are used as a recurring tool by organizations to be notified of new items after they have participated in workshop sessions in such a way slowly we may grow from separate business transactions at workshops towards an active private e marketplace balancing precision and recall is a frequent concern of many recommender system designers e g see geyer schulz and hahsler 2003 in general we target customers in marketplaces with a preference for precision over recall providing good recommendations irrespective of the number of recommendations which is what precision measures in case users don t mind seeing a few extra irrelevant recommendations in order to receive more recommendations they would be interested in we could optimize on recall however it is often the case that over time optimizing on precision may limit the users exposure to novel item types whereas one of the goals of an is recommender is also to initiate new types of symbiotic ideas a trade off mechanism between all such performance measures e g accuracy novelty dispersity and stability with respect to the delivery format predictions vs top n can provide a more balanced evaluation of recommender goals bobadilla et al 2013 therefore the performance of a recommender is always highly suggested to be frequently evaluated within the context of the application according to the changing preferences of its users the process of industrial symbiotic business development is generally categorized in four phases identification assessment implementation and monitoring recommenders proposed in this paper address the first phase i e identification therefore the initial baseline for using recommenders for more complex domains is that the success rate of recommendation is at least considerably better than random recommendation or search information filtering approaches our results are not comparable to those obtained in b2c markets where the precision of e g movie recommendation is more likely to be high most consumers like many movies compared to the precision in b2b markets characterized by narrow interests our expectation is that arm could perform better in specialized markets that reflect a narrow business interest current is markets offer all sorts of items varying from waste heat to iron scrap metals specialized markets such as metal markets have fewer sorts of items this potentially helps recommenders because of the expected stronger rules that can be deduced however once organizations start to use the recommenders for identifying business opportunities they might improve the precision and recall based on their business experience in the successive phase of the assessment to begin with organizations may already save costs when recommenders are able to substantially reduce the number of irrelevant matches 5 2 data challenges to explicit knowledge based is recommenders the io recommender was able to deduce item preference given the fact that preferred items were retrieved this demonstrates the value of studying explicit knowledge based recommendation however it is questionable whether the precision of the io algorithm can be improved to become an effective predictor in an industrial symbiotic marketplace for this data quality and standardization issues first need to be improved in order to determine whether this form of explicit based recommendation can also perform at the rate arm is currently able to under the conditions of the current is data landscape we do not expect to reach such results anytime soon data quality has always been considered as one of the major challenges that need to be addressed by organizations working on predictive analytics projects in supply chain management hazen et al 2014 to understand the problem of data quality in recommendations a number of data quality dimensions need to be assessed e g accuracy timeliness consistency and completeness furthermore ontological problems that have an impact on the effectiveness of prediction or matchmaking technologies have to be taken into account cecelja et al 2015 current decision support tools operate with data from industrial symbiotic markets gathered under real world business conditions as a result the level of detail and information richness is generally limited in addition approximate estimates are used in quantitative values and often a variety of data formats is used leading to unstructured data poor data quality not only hinders the determination of an is match but also hinders better identification of potential opportunities during the design of the io recommender a number of data quality problems were encountered table 6 illustrates the reasons for these knowledge mismatches that result in bad or missing item recommendations we classify the data challenges with a severity level based on the expected frequency of each type of challenge in the is data a number of studies illustrate these data challenges which we found to obstruct knowledge based recommendation see column literature support in table 6 for example a waste offer that lists certain types of bio materials that can be used to produce bio energy may not be directly linked to a demand of bio energy in order to let a system suggest a logical knowledge based recommendation the system should be aware of the link between bio materials as a resource to produce bio energy or should be able to infer the relation between the offered waste and the demand for bio energy e g based on linked or historical data one of the major data concerns is encountered in the different hierarchical levels in which wastes are addressed sander et al 2008 consider the relationship between iron and metal iron refers to raw material largely consisting of the chemical element ferrum is a type of metal and likely to be found as scrap metal however without such an explicit relation text mining algorithms would struggle to relate the two based on this analysis there are three main strategies to make recommendation work in an is context 1 increase the data quality so that knowledge based recommenders have better data to reason with and therefore are likely to make better predictions 2 adapt the knowledge based recommenders to work with limited and less accurate data evaluate if the recommenders can perform at an acceptable rate of recommendation 3 rely on implicit knowledge based recommenders that are not dependent on the quality of data in external knowledge bases 5 3 internal and external validity considering the internal validity of the study several remarks are in place the evaluation indicates that in a practical is context an algorithm based on implicit knowledge perform better in predicting item preference than those based on explicit knowledge this finding is in line with the study of grant et al 2010 and indicates that the theory explaining the role of implicit versus explicit knowledge is transferable to the design of is recommenders however the study selects and designs two algorithms based on existing concepts of recommender systems related to implicit and explicit knowledge e g ontology based versus association rule based therefore the evaluation of the design a part of design science methodology relies on qualitative interpretation a more in depth analysis might include other filtering techniques that represent the implicit and explicit knowledge based recommendation to strengthen the validity of our research however based on previous literature grant et al 2010 we believe that different performance outcomes are highly unlikely because of the typical data problems faced in is marketplaces another aspect of internal validity is that the lci database used to create company profiles has its limitations not all industrial manufacturers processes could be identified thereby resulting in fewer recommendations with respect to arm training and testing the algorithms on a larger data set might improve the results hence the arm is not considered to be heavily influenced by potentially negative effects e g by overfitting the classifier on this is data set finally we focused on the exchange of materials i e waste items and process industry organizations were selected to study the role of implicit and explicit knowledge in the design of an is recommender is is a wider concept that goes beyond the exchange of raw materials the study did not enclose is item types such as waste energy tools machinery or over capacity in a service based industry regarding the external validity a number of generalizations can be derived from this research firstly the proposed model presented in fig 1 is applied to other cases in which recommenders are designed using environmental data that are extracted from ill defined or unstructured data collections the approach can be applied to all types of environmental problems characterized by such typical survey data that one might use together with external data to generate recommendations secondly the instantiation of the model for the case of is fig 3 could be generalized to other cases of recommender design for similar symbiosis settings we believe that the data structures of is and lci are representative for other situations in which is identification is sought as this study is a first attempt of building a recommender in an is context we do not believe these designs are exclusive on the contrary although these designs may be transferable to other is settings other types of implicit and explicit knowledge based recommender filtering techniques e g rule based may replace them and show a different performance the generalizability of the role of knowledge in the recommendation is affected by the quality of available data sources for example ecoinvent as a knowledge base of input output data is currently the best possible source for an io based recommender although we expect the quality of such sources to increase they require substantial collaborative efforts that can be part of long term development goals moreover the proposed model can be applied under circumstances having such data quality issues our methodology explicitly attempts to work with these data sets without considering the possibility of iterative refinement of data quality in collaboration with users the reason for this approach is that if extensive efforts are associated with improving the data quality industries are unlikely to make the necessary investments this has a more direct impact on the sustainability of similar industries 6 conclusion a recurring type of is research is the development of software applications to support the process of facilitating symbiotic developments this paper addresses one of the major problems for the adoption of is identification tools in a practical context following the theory of grant et al 2010 on the dominance of implicit knowledge in is markets we test the role of implicit versus explicit knowledge in designing is recommenders we find that the implicit knowledge based recommender significantly outperforms the explicit knowledge based is recommender in other words the performance of the io recommender is considerably lower than the arm recommender algorithm that relies on implicit knowledge the study further shows that the design of an explicit knowledge based recommender is affected by many data challenges including the linkage of waste streams to process inputs structural and semantic representation of data attribute availability code standardization data reliability and data integrity problems the design evaluation shows that implicit knowledge based algorithm can outperform explicit knowledge based algorithms in identifying is opportunities we argue that such a performance difference can be explained by a number of data challenges that first have to be resolved before explicit knowledge based recommendation can be of practical value to is decision support on the other hand we observe a noticeable role for implicit knowledge based techniques in recommendation the current performance of implicit knowledge based recommenders might enable its acceptance among industries that intend to investigate potential is opportunities this provides an indication to practitioners that usually implicit knowledge based algorithms are more promising for the design of is recommenders on the other hand a new research challenge emerges on how to improve the environmental data landscape so that explicit knowledge based recommendation can become a more viable option finally the results clearly demonstrate the benefits and limitations of both the io and the arm method more generally they demonstrate the challenges and preconditions for a successful recommender design in is markets moreover we confirm that recommender evaluation is essential to the design of an effective prediction algorithm the following contributions are provided by this study i to the best of the authors knowledge this paper is a first attempt in the is literature to propose a recommender for is waste markets and to test the proposed recommenders with real world data ii our proposed model explains the design of recommenders in an environmental data context subject to data quality issues iii in the instantiation of the model we construct two algorithm designs based on explicit and implicit knowledge the proposed explicit knowledge based recommender algorithm uses manufacturing profiles created with input output data of life cycle inventory databases such a recommender demonstrates the applicability of knowledge based recommendations iv we provide comparative performance results for both recommenders so that practical implications are provided based on statistically tested performance indicators this first attempt to implement recommenders in is provides insights on how to design recommenders in symbiotic networks in future work we intend to address other types of recommenders our findings along with the support of grant et al 2010 indicate that given the current issues with is data one might preferably focus efforts and resources in designing recommenders based on implicit knowledge rather than explicit knowledge is recommender design can also be studied in a broader perspective of the identified is categories including waste energy services and manufacturing tools in addition different recommender techniques can be discussed such as is sector based recommendation along with further testing various recommendation mechanisms based on implicit and explicit knowledge acknowledgements this research is funded by european union s horizon 2020 program under grant agreement no 680843 appendix a appendix appendix a 1 stem frequency vectorization algorithm 3 stem frequency vectorization of waste item descriptions image 3 appendix a 2 a simple multi dimensional hierarchical agglomerative clustering algorithm algorithm 4 a simple multi dimensional hierarchical agglomerative clustering algorithm image 4 
26316,industrial symbiosis aims to stimulate or enhance cooperation between industrial firms to utilize industrial waste streams from other industries and to share related knowledge in order to achieve sustainable production recommenders can support industries through the identification of item opportunities in waste marketplaces enhancing activities that may lead to the development of an active waste exchange network to build effective recommendation we study the role of knowledge in the design of a recommender that suggests waste materials to be used in process industries this paper compares the performance of a knowledge based input output recommender with a recommender based on association rules the two recommenders are evaluated with real world data collected through deploying surveys in a workshop setting our research shows that many data challenges arise when creating recommendations from explicit knowledge and suggests that techniques based on the concept of implicit knowledge may be preferable in the design of an industrial symbiosis recommender keywords industrial symbiosis recommender systems decision support systems input output matching association rule mining 1 introduction the reduction of waste emissions and primary resource use in resource intensive industries is suggested as one of the critical pathways to accelerate sustainable development european environmental agency 2016 the european union highlights industrial symbiosis is as a methodology that stimulates industries to become more sustainable this policy follows from the vast amount of positive environmental effects generally associated with the methodology and its ability to scale at a european level laybourn and lombardi 2012 is entails the identification and utilization of an organization s traditional secondary production process output that is generally considered as waste such waste can be used to substitute part of a primary resource possibly after some pre processing in the production process of another organization usually located in a different industrial sector chertow 2000 both researchers and practitioners recognize the impact of is as a useful business opportunity and tool for eco innovation it involves engaging diverse organizations in a network to foster eco innovation and long term culture change creating and sharing knowledge through the network yielding mutually profitable transactions for novel sourcing of required inputs value added destinations for non product outputs and improved business and technical processes lombardi and laybourn 2012 page 28 new is opportunities are mostly identified using facilitated industry workshops paquin and howard grenville 2012 van beers et al 2007 mirata 2004 is identification systems grant et al 2010 and waste exchange marketplaces dhanorkar et al 2015 the key role of these methods is to facilitate information exchange of waste and resource interests van capelleveen et al 2018 numerous pathways explain the emergence of is proposed in different stylized models chertow 2007 paquin and howard grenville 2012 boons et al 2017 eco industrial parks generally involve a continuous effort from coordinating bodies e g municipalities or regional governments to locate industries that can potentially cooperate together in the park regions in order to share wastes and by products gibbs and deutz 2007 other is based industrial ecosystems arose by self organization resulting from collaborations without top down planning and mainly driven by economic or strategic business motivations that lead to increasing resource and waste transactions over time chertow and ehrenfeld 2012 a third type of is emergence is a facilitated approach that utilizes intermediaries that provide a role of strengthening trust between firms using expertise and the ability to connect industries paquin and howard grenville 2012 these pathways not only characterize the different types of emergence and explain how the process of is unfolds but also help to deduce the critical catalyzers to initiate new symbiotic actions boons et al 2017 in relation to the facilitated approach various scholars have studied waste exchange systems as a tool to enhance is identification clayton et al 2002 sterr and ott 2004 mirata 2004 chen et al 2006 van beers et al 2007 veiga and magrini 2009 grant et al 2010 dietrich et al 2014 dhanorkar et al 2015 cecelja et al 2015 hein et al 2015 cutaia et al 2015 van capelleveen et al 2018 such systems can enhance symbiotic transactions in the network while substantially reducing the time investment required to investigate the symbiotic potential the capability of supporting the learning and decision making processes of environmental problems recurs as an interesting opportunity to assess poch et al 2004 decision support tools grant et al 2010 and in particular recommender support van capelleveen et al 2018 are suggested as promising techniques to stimulate and facilitate the identification and assessment of new exchanges recommenders are able to support users in identifying item opportunities and to pro actively engage system use resulting in both increased sales and a more active community freyne et al 2009 pathak et al 2010 gomez uribe and hunt 2015 however building systems that can provide decision support or recommend is opportunities remains a challenge grant et al 2010 firstly many of such tools lack the key characteristic of sociability grant et al 2010 focusing more on determining technical opportunities rather than building human relationships secondly while a critical mass of industries is required to engage in network activity it is difficult to attract them to join the network finally systems struggle with analyzing data because of the high level of implicit knowledge which is a burden to the development of techniques that help to identify is exchanges grant et al 2010 this particular data challenge is the key focus of this research for example process data from manufacturing industries that disclose inputs outputs and wastes can be both used for the identification of potential synergies as well as to provide input for recommender systems however the extent and level of detail with which such data is shared may be hindered by a lack of trust among organizations because process data might reveal competitive information that organizations want to keep partly confidential paquin and howard grenville 2012 moreover organizations have to justify time and resource investment to explore potential ideas for which the expected benefits are not clearly predicted or even known therefore the provision of detailed process data in a wider context may increase identification of is opportunities but this is challenging at the initial stage this study contributes to the discussion initiated by grant et al 2010 on the role of implicit and explicit knowledge in waste marketplaces by transferring the implications of data characteristics to recommendation technology implicit knowledge sometimes referred to as tacit knowledge is know how that is subconsciously understood and applied it consists of complex information that individuals are unable to express and codify moreover implicit knowledge resides primarily in its field of application on the other side of the spectrum explicit knowledge is formally articulated referring to situations where one is able to capture know how into codification schemes that can be communicated and used to reason with kogut and zander 1992 zack 1999 the theory on implicit versus explicit knowledge is used in organizations supply chains and markets to seek for explanations of problems related to communication reasoning and broader knowledge management schoenherr et al 2014 kimble 2013 a similar discussion on implicit versus explicit knowledge is also prevalent in the field of recommender systems bobadilla et al 2013 for example data can be derived from implicit e g monitoring user s behavior and explicit content e g user ratings also filtering techniques can be built using either an implicit knowledge based e g association rule mining or an explicit knowledge based e g case based reasoning recommendation hence an important question is what effects do these different recommendation techniques have on analyzing environmental domain data in order to create recommendations therefore the aim of this study is to evaluate and understand the influential role of knowledge in the design of an effective waste material recommender for is marketplaces in this paper we examine the problem of is identification by creating a model for utilizing environmental data in recommender systems we therefore employ the design science method peffers et al 2007 gregor and hevner 2013 for constructing our recommender artifact that utilizes a novel input output io algorithm this io algorithm is evaluated through a comparison with another recommendation algorithm we have developed based on association rule mining arm section 2 describes the methodology used while section 3 presents the design of a generic approach to create explicit and implicit knowledge based recommendations in the environmental data landscape this part provides an instantiation of our proposed model that is applied to is by designing two algorithms an explicit knowledge based recommender and an implicit knowledge based recommender section 4 presents the result of a comparison of the two algorithms section 5 discusses the interpretation of recommender performance and reviews the internal and external validity of the achieved results finally section 6 concludes the paper with implications and future research directions this structure follows the guidelines suggested by gregor and hevner 2013 2 method the main objectives of this study are to design a model that utilizes environmental data to make is recommendations and to develop an instantiation of this model to identify is opportunities this design science research is guided by peffers et al 2007 in relation to the discussion of grant et al 2010 our study focuses on the role of implicit versus explicit knowledge in the design of recommenders thus the instantiation of the model represents both an implicit and an explicit knowledge based recommender design we evaluate the design empirically by investigating the extent to which explicit and implicit knowledge influences the effectiveness of the recommendations our research approach consists of four major steps 1 data collection 2 the design of a model for utilizing environmental data to create recommendations 3 an application of this model to the problem of identification of is resulting in 3a the design of techniques for pre processing and 3b the design of an implicit knowledge based as well as an explicit knowledge based recommender and finally 4 a comparative design evaluation using recommender evaluation metrics 2 1 step 1 data collection two data sources are utilized as a knowledge base for making recommendations firstly data is collected from industrial symbiotic workshops this is data containing a variety of waste items and resource interests from industry originate from is workshops held in two different european industrial clusters the data were collected as part of the eu funded sharebox project european commission 2017 this data is explored in order to identify the item properties that are valuable to generate useful recommendations secondly knowing that confidentiality is a key challenge in is development we study the usefulness of an external database which provides the process input output data from life cycle inventories this database contains data regarding the wastes produced and primary resources used in the production activities of process industries in addition it serves as a knowledge base to investigate potential synergies between industrial firms based on the substitution of traditional primary resources with wastes a detailed overview of both data structures is presented in section 3 2 2 2 step 2 model design after studying the data we create a model that uses a step wise approach to explain how to build a recommender system that can operate under challenging data characteristics e g the noise in survey data see fig 1 such noisy characteristics are often present in environmental data sets due to ill defined and unstructured data collection practices 2 3 step 3 instantiation of the model next we create an instantiation of this model by applying it to the problem of is identification based on the data gathered according to the model see fig 1 firstly the data requires pre processing before it becomes useful as input to recommender algorithms in the case of is data this involves the clustering of ephemeral items in the data set into latent product concepts based on a technique described in chen and canny 2011 which is explained in detail in section 3 3 then we design both an implicit and an explicit knowledge based recommender explicit knowledge refers to the use of an external knowledge base while implicit knowledge is used for a recommender that learns from data on the behavior of users in a marketplace the explicit knowledge based resource recommender is constructed on the concept that a waste to input match i e a waste that can substitute a primary resource can be predicted by utilizing information about the manufacturing processes associated with a particular industry often the manual use of life cycle inventory lci databases containing explicit knowledge for the detection and assessment of new is business opportunities already takes place grant et al 2010 van capelleveen et al 2018 industry profiles are created that contain the major inputs and outputs associated with the industry s manufacturing processes our explicit knowledge based recommender exploits the potential of inventory databases that provide process data revealing the inputs outputs and wastes associated with the manufacturing of products in a particular industry on the other hand the implicit knowledge based recommender utilizes the well known techniques of association rule mining agrawal et al 1993 in order to evaluate the impact of implicit knowledge on the recommendation association rule mining is a promising technique often applied in e marketplaces to recommend items park et al 2012 hence it is expected to be a good candidate for an implicit knowledge based recommendation technique 2 4 step 4 design evaluation the final step of the methodology involves running the algorithms on the is data set and evaluating the performance of the recommenders usually recommender evaluation is first performed in an off line setting using sample data in this case the is workshop data such data is either derived from a relevant external data source or extracted from the on line system for which the recommender is designed ekstrand et al 2011 we evaluate recommendations by measuring the prediction effectiveness in terms of a binary classification more precise we classify the predictions as recommended items of non recommended items and use this binary classification to measure the performance of an algorithm then we quantify four metrics for evaluating the performance i e precision recall accuracy and the f measure in a recommender context the precision and recall measures are described in terms of a set of retrieved items that are then compared to the set of items relevant to a user in equations 1 3 t p denotes the number of true positives f p the number of false positives t n the number of true negatives and f n the number of false negatives a true positive is a recommendation that matches the user s possesive stated preference a false positive is a recommendation that does not match the user s possesive stated interest a true negative regards the situation when no recommendation is provided and the user has no stated interest a false negative occurs when no recommendation is provided to a user who has a stated interest precision is the fraction of recommended items that are relevant to a specific user see equation 1 recall is the fraction of relevant recommended items that are retrieved see equation 2 accuracy is the fraction of measurements of a true value where the true value measurements consist of both the recommended items relevant to a user and the non relevant items that are also not recommended see equation 3 the f measure is a different measure of a test s accuracy that evaluates the precision and recall in a weighted harmonic mean see equation 4 1 p r e c i s i o n t p t p f p 2 r e c a l l t p t p f n 3 a c c u r a c y t p t n t p t n f p f n 4 f m e a s u r e 2 p r e c i s i o n r e c a l l p r e c i s i o n r e c a l l recommender performance is measured at two levels i e 1 an item level and 2 a cluster level this distinction is based on our assumption that many items in a resource marketplace are considered by users as similar products based on their usefulness e g iron scrap from one organization is as good as those of other organizations see also section 3 3 where we discuss how we cluster items into latent products thus we also can evaluate a recommendation as being valid if either the exact item is retrieved item level or if one of the latent product items one of the items belonging to an item cluster is retrieved cluster level 3 model design recommendation in an environmental data landscape 3 1 the model fig 1 presents a generic approach to create recommendations out of environmental data that is the result of ill defined or unstructured data collection the resulting data characteristics may not only constrain the effectiveness of a recommender but also affect necessary data transformations and taxonomy translations prior to making a recommendation the model addresses such data issues by building recommenders based on the implicit versus explicit knowledge based reasoning grant et al 2010 kimble 2013 schoenherr et al 2014 it contains an explanation of how implicit and explicit knowledge based recommendation algorithms are affected by the characterization of this data landscape explicit knowledge based recommenders require the utilization of external standardized databases containing detailed data as explained in the model fig 1 environmental data is often noisy not transparent non standardized and incomplete to create a recommendation often a number of actions are required including preprocessing of data mapping data and the selection of an algorithm fig 1 shows that both types of algorithms implicit or explicit knowledge based require the pre processing of data but explicit knowledge based recommenders also often require the data to be mapped because of their dependence on external knowledge bases once the environmental data is prepared and mapped the recommender algorithms are able to generate recommendations 3 2 the model applied to the case of industrial symbiosis recommender systems can contribute to the reduction of time and resource investments of industries by guiding organizations to the first set of potential waste items that are likely to involve industrial symbiosis business cases the following section provides illustrative examples to understand how the two recommender algorithms utilize data to construct recommendations fig 2 provides information about the composition of the original workshop data the workshops were held in two different european regions and a record was made of the waste items of participating organizations as wells as their expressed interest this data has similar characteristics to those presented in waste ontologies for waste markets cecelja et al 2015 raafat et al 2013 the data set has a sample size of 421 for region a and a sample size of 150 for region b in this data we identify five different classifications of items namely 1 materials 2 tools 3 services 4 energy and 5 others the materials are of particular interest as the input output based recommender is designed to predict the material preference of an organization the process industry is considered a good target audience for such a recommender therefore in recommending items organizations that do not fit the profile of a manufacturer of materials were excluded from the recommendation hence in the evaluation of both recommenders a selection of only the offered items by sellers of waste categorized as material is utilized this resulted in a total of 139 waste items for region a and 54 waste items for region b this sample size is relatively small compared to the evaluation of recommenders in many other types of e marketplace studies the reason for this is because is waste markets do not have such large participation thus careful interpretation of the statistical validity of the recommender performance is needed in particular one needs to keep in mind that there is a possibility of overfitting the model of the arm algorithm in this paper we will also derive qualitative insights on the applicability and challenges of recommender design in is marketplaces table 1 1 systems for statistical classifications of economic activities were beneficial to identify industry types e g the international standard industrial classification of all economic activities isic or the statistical classification of economic activities in the european community nace nace is derived from isic in the sense that it is more detailed than isic isic and nace have exactly the same items at the highest levels where nace is more detailed at lower levels eurostat 2018 2 european waste catalogue european commission 2000 shows a sample of the used subset of data acquired from is workshops organized to facilitate the creation of new is initiatives among industries we notice that most waste items addressed in the is workshop survey data often have short waste descriptions commonly defined with less than 10 words the data illustrates the noise in item descriptions e g from waste item a it is difficult to understand the exact relationship between the iron steel the slag and the concrete tiles however we can consider to a certain extent that the offered item s have a potential for iron reuse the industry description contributes as shown further on to identify the related traditional inputs to that firm during on line evaluation the arm algorithm can learn rules from system transactions the workshop however provides off line data containing the interests of organizations in items the survey data is considered a representative data set for is market transactions in particular because such items share the characteristic that users compose their own item descriptions when offering these in either an online marketplace or during a workshop therefore these stated interests are interpreted as transactions to detect rules tables 2 and 3 illustrate the structure of the data that is obtained from ecoinvent one of the world s largest lci databases table 2 shows examples of manufacturing processes of goods or services that are available e g the production of iron casts table 3 shows examples of the inputs and outputs connected to those production processes e g iron furthermore information is included regarding the amounts usually associated with the production of one functional unit using the analysis of the is and lci data we can build an instance of our earlier proposed model cf fig 1 that addresses the challenge of identifying new is business fig 3 illustrates this instantiation in fig 3 the is workshop data represents the environmental data box from fig 1 and the lci database represents an external standard database the particular data characteristics represented in fig 3 help in the design of the data pre processing techniques the mapping and the recommender algorithms in the following three sub sections sections 3 3 3 4 and 3 5 we explain how we handle the pre processing of the data i e the need to apply natural language processing nlp in order to cluster groups of similar items steps 1 and 2 in fig 3 after which we show the mapping of items on the input output database and the design of a recommender algorithm based on this explicit input output knowledge steps 4 and 5 in fig 3 as well as the design of a recommender algorithm based on implicit knowledge using association rule mining step 3 in fig 3 3 3 pre processing is data the pre processing of data is addressed by steps 1 and 2 in the model in fig 3 this consists of applying nlp and of the clustering of items a major challenge to many recommenders is that data are often sparse e g due to low item transaction history and hence item space reduction is needed similar to data in e commerce marketplaces that predominantly consist of ephemeral items these ephemeral items are items submitted by users and composed of users individual product descriptions thus item descriptions hardly correspond to any catalog taxonomy and often lack detailed product descriptions in addition the frequency with which the supply of items is renewed in a marketplace is high wroblewska et al 2016 therefore item space reduction strategies are often applied to reduce the number of item types increasing the number of transactions of each item type e g considering all items listing iron fillings as equal iron waste products item space reduction methods thus create a richer history for each item type by grouping items and treating them as one item space thereby allowing algorithms such as arm to deduce more and stronger associations moreover it can improve the io matching of items with short item descriptions by using a richer product concept description hence grouping similar items helps to increase quality efficiency and effectiveness of recommender algorithms chen and canny 2011 nlp is used to assess the similarity between waste item descriptions an obvious choice to group waste market items would be to rely on existing taxonomies that already play an important role in waste treatment for example the european waste catalogue ewc european commission 2000 or the central product classification cpc united nations statistics division 2015 are examples used in reports on waste disposal and in duty of care documents in waste transfers natural resources wales et al 2015 that define the similarity between items however such classifications are often absent in item descriptions moreover these systems classify goods and services in the industry from which they originate causing an overlap in product concepts within the taxonomy thus such a taxonomy fails to relate two similar waste items if they are produced in different industries the isdata project 2015 sander et al 2008 for example recycled glass can be produced either with uncontaminated glass residues resulting from a glass bottle production facility or it may be extracted from construction and demolition waste international synergies ltd 2016 a solution to this problem is to map items into latent product concepts chen and canny 2011 by inferring the latent product concept the dynamics and diverse item inventories can be used to group items considered as similar or identical products by a recommender this way item clustering reduces complexity in an intermediate step towards enabling recommender algorithms to learn from data in such contexts hence there is a trade off between having item specific information to be used by a recommender enable individual based reasoning and building item history on similar types of items history is required to deduce user preference the elbow technique salvador and chan 2004 is used to detect the cut off point for clustering thereby identifying the trade off the clustering algorithm see a 1 based on chen and canny 2011 explains how to cluster similar waste items the algorithm extracts the latent product concept from item descriptions in the is data set it uses stem frequency vectorization as a means to identify the latent product concept of an offered waste item stemming is the process of removing the inflectional forms and sometimes the derivations of the word by identifying the morphological root of a word manning et al 2008 the frequency vector of an item within a set of item descriptions is determined by the frequency of every unique stem in that item along with the unique stems in the set an example of such an item vector is provided in fig 4 in order to derive the set of stems from an item description we use the nltk package a platform for working with human language data in python natural language toolkit 2017 firstly all the characters are converted to lowercase and all numbers and special characters are removed then items are tokenized into a bag of words using the nltk corpus all english stop words are removed from these bags and some non significant terminology commonly used in is is filtered as well e g waste material and process finally the empirically justified porter algorithm porter 1980 is applied to stem the bag of words then the resulting item vectors are utilized in a multi dimensional hierarchical agglomerative clustering algorithm based on an algorithm presented in manning et al 2008 to cluster items using the cosine similarity of item vectors see a 2 in contrast to most clustering methods in our multidimensional clustering the traditional coordinates x y are replaced by the stem frequency vector in hierarchical clustering the determination of the elbow is a common method to determine the number of clusters the method utilizes statistical variance in order to explain the best number of clusters the variance is calculated using the sum of squared errors sse as a function of the decrease in the number of clusters the method exploits the point of maximum curvature in the sum of squared errors sse of all clusters as the cut off point for clustering the maximum curvature is the point on the curve with maximal bending also explained as the point where the curve appears to curve the most salvador and chan 2004 this point often represents a knee or elbow i e the point that shows a marginal drop or gain in variance this point determines the finally selected number of clusters although evaluation methods that can mathematically locate the elbow in a curve salvador and chan 2004 exist visual interpretation is preferred over statistical methods when no objective measures have been defined for cluster optimality in the domain of application jung et al 2003 3 4 design of an explicit knowledge based is recommendation this section addresses steps 4 and 5 in fig 3 i e the mapping of items to the input output data in the lci database and the io algorithm design the io based recommender see algorithm 1 is a knowledge based recommender suggesting items based on the inferences about the needs and preferences of a user burke 2002 this type of recommender makes use of explicit knowledge a knowledge base to recommend items to users the algorithm is based on the assumption that the items offered in is marketplaces correspond to the specific item interest of an organization and subsequently to the primary inputs in their manufacturing processes as primary inputs of a production process we consider only those inputs that are not provided by one of the production processes of the associated production chain such as raw materials and natural resources e g silver ore limestone clay water and energy resources e g gasoline natural gas coal the primary outputs result from a manufacturing process which is the primary focus of production secondary output generally refers to waste that has no perceived economic value and that traditionally is to be discarded as this method uses the identification of potential relations between the primary inputs of production processes and secondary output waste offered in a marketplace we indicate this type of raw material recommender as an input output recommender io recommender the information on resource use for the production of goods or services originates from an lci database in our case the ecoinvent database version 3 3 ecoinvent 2017 this type of database provides well documented process data for thousands of production processes including information on the raw material consumption in particular this information about primary inputs is used to identify the most consumed raw materials of an organization and matches those to the potential waste resources available in the marketplace recommendations made by the io recommender algorithm are created through the use of pre compiled manufacturing profiles a profile is constructed through the identification of the type of products produced by an industry typically constructed using the corresponding company websites the products are used to find the associated manufacturing processes available in lci databases this results in predefined profiles created for each type of organization e g a manufacturer of castings the profiles list among others the raw material consumption that is associated with the selected manufacturing processes the profiles of the industries required by the algorithm are constructed prior to recommendation once the profiles are created the algorithm extracts all resources listed from the lci database that is linked to a production process in the organizational profile then it filters the resources which make up the largest fraction of resource consumption within the production of a product these resources are selected as the candidates for recommendation the resource candidates are then compared with the marketplace item descriptions to see whether they contain similarities the matching algorithm calculates the cosine similarity between the vectorized stem frequency of a waste product description a cluster and that of a resource description the stem frequency vectorization algorithm applied in the clustering of items see section 3 3 is identical to the one used for io matching the algorithm operates as follows first for each resource connected to the industrial profile of the organization the algorithm iterates over all clusters in the marketplace within this iteration a bag of stems is created for both the selected cluster the latent product concept and the selected resource from the industry profile the first bag of stems created from the cluster selects the most frequently occurring stems in all items descriptions belonging to that cluster the second bag of stems is created from the resource and uses the name of the material based on the taxonomy used in the lci these bags of stems are used to compose the item vectors for both the cluster and the resource next the similarity between the two item vectors is assessed using a cosine similarity function the stem frequency item vector combinations that exceed the minimum cosine threshold and are not yet a recommendation are added to the set of recommendations algorithm 1 input output recommender algorithm image 1 3 5 design of an implicit knowledge based is recommendation association rule mining is one of the techniques based on implicit knowledge that found successful application in recommender systems park et al 2012 the arm technique attempts to discover regularities in transaction data based on the concept of strong rules agrawal et al 1993 in general association rules perform best on large scale data with a broad history of transactions the popular apriori algorithm is used to generate candidates for identifying these rules as it is both simple and exact agrawal and srikant 1994 the pace at which users change preference affects the decay of association rules to find the best length of the time period from which rules can be detected requires an estimate of how stable the user preference is over time the nature of is is characterized by a low frequency of transactions but preference is considered to remain fairly stable therefore in the context of is it is reasonable to select a longer period from which association rules may be learned this period ranges from several months up to years to recommend on the basis of association rules one needs to define which rules are accepted to generate recommendations it is common to set a threshold for the minimal support and the minimal confidence in order for association rules to become a rule for recommendation support is a measure of item pair frequency it is defined as the percentage of transactions in a dataset that contains a particular item combination confidence is a measure of rule strength defined as the number of times an association rule is valid in case of a relatively small dataset k fold cross validation is applied in order to increase the validity of the evaluation k fold cross validation is a technique that randomly partitions data into equally sized samples in order to test an algorithm in various test and training combinations algorithm 2 k fold association rule mining recommender image 2 4 design evaluation 4 1 data preparation fig 5 presents the results of how clusters were formed using the maximum curvature concept explained in section 3 3 the resulting number of clusters k is 84 for region a and 40 for region b respectively the clustering is applied to both data sets separately the remaining parameters of the algorithms are consistently used and initialized with similar values for both data sets for the input output algorithm a minimal cosine similarity of 0 1 is set reflecting that one of the stems derived from the item description matches the resource taxonomy of the lci this value needs to be kept low as nearly all item descriptions contain more stems often not more than 10 than the solely resource taxonomic term used in the lci for example a resource taxonomy names a material iron while the item description contains more words than the resource term only e g iron steel and slag concrete tiles can be taken as one of the main components see table 1 of course having more stems to consider increases the likelihood to find a match between the is data and the external database however it may also increase the number of incorrect matchings the k fold cross validation technique used in the arm uses the value k 10 furthermore the minimal support parameter is initialized with a value of 0 1 and the minimal confidence parameter with a value of 0 7 as thresholds for a recommendation rule the value for minimal confidence is selected based on a sensitivity analysis which examines the highest potentially achievable result martnez ballesteros et al 2016 fig 6 shows this sensitivity analysis from which we obtained the optimal initialization value of minimal confidence the intersection of lines representing precision and recall is generally recognized as a good balance see the crossing of lines in fig 6f although it could be desirable in a system implementation to exploit precision in favor of recall and a larger number of recommendations we select this point to present the lower bound for our recommender system 4 2 results of recommender performance table 4 shows the results of a comparison of the two recommenders the results are presented with three variables in the first column we distinguish between the two recommender methods that were tested the second column shows the level of measurement item level or cluster level as explained in section 2 the third column defines the data set used to test the algorithm note that the average performance for the combined regions a b is constructed by first aggregating the recommendation results of the individual regional experiments in order to calculate the combined performance metrics table 4 also presents the percentage of items that were recommended as well as the values of all performance metrics precision recall accuracy and the f1 or f measure the io and arm method are first compared at cluster level the results demonstrate that the arm method outperforms the io method on average with a substantial difference in precision ranging from 4 to 6 times and 2 or 3 times on recall in general also the evaluation of the accuracy and f measure shows a noticeable difference in performance a similar substantial difference is found at the item level the results of the arm method compared to the io method is 3 4 times higher for precision and 2 to 3 times for recall that both methods score much lower at the item level is most likely explained by the number of similar items organizations purchase for example if three similar items grouped into a latent product are available and one organization only purchases two of these then the precision at item level is lower than at the cluster level the scores on accuracy require more detailed explanation the results show that the accuracy values in the experiments are less different ranging from 0 7 to 0 9 however accuracy measures may portray a misleading perspective of recommender performance although it is true that the accuracy results indicate that many items were correctly classified the majority of correct classifications in this experiment consists of items that were correctly rejected for recommendation the effectiveness of a recommender is more clearly understood by having high precision rates along with a reasonable recall rather than by the overall accuracy result therefore also the f measure is calculated f measure provides a combined perspective on precision and recall and a better demonstration of the difference in performance of the recommenders finally the two regions show a small difference in the performance of recommendation algorithms regional characteristics may have affected the performance of the algorithms the io method shows only small differences between the regions however the arm method shows a larger disparity between regions i e region b performs considerably better at precision than region a while no such precision increase is achieved with the io method this can be explained by the fact that in a relatively small data set only a small number of rules can be deduced it can be understood that in region b the recommender model overfits based on the few learned rules with a larger dataset it is likely that more rules would be available leading to a considerable drop in confidence levels consequently a lower precision recall balance would be established based on the larger set of rules the wilcoxon signed rank test is applied to show that the results obtained in our experiment are significantly different see table 5 this test compares the two algorithms based on the paired results between industries without making distributional assumptions on the differences shani and gunawardana 2011 hence the test that detects whether the performance difference between the arm and the io recommender is statistically significant when evaluating at the cluster level using data from both regions a and b such a hypothesis is non directional and therefore the two sided test is applied all tests are applied at a 95 confidence interval thus using an α of 0 05 the resulting wilcoxon critical t values for each sample size n are obtained from the wilcoxon critical t values list wilcoxon and wilcox 1964 the selected wilcoxon test adjusts for the ranking in case of ties furthermore it is initialized with the setting that accounts for the likely binomial distribution in a small data set by applying continuity correction both the precision and recall results are significant at p 0 05 because n 20 we confirm the difference by showing that t 52 for precision and t 46 for recall tests at item level also result in a confirmation that there is a significant difference for both precision and recall between the arm method and the io method see table 5 5 discussion 5 1 the role of explicit and implicit knowledge for the total set of available items the experiments show that a substantial number of recommendations can be generated using both the arm and the io based methods however the results indicate a preference to utilize implicit knowledge over explicit knowledge in recommender design for is identification in both regional data sets measured at item and cluster levels the arm algorithm is clearly the better performer for most of the recommender metrics except for accuracy that shows a weaker and less consistent difference these findings confirm the ideas of grant et al 2010 that implicit knowledge is a challenging characteristic of industrial symbiotic markets that partly affects the success of decision support designs nevertheless we see a role for recommenders in is markets in particular for implicit knowledge based algorithms with high precision rates recommenders could attract organizations to form a critical mass that can sustain sufficient supply demand in the marketplace such recommenders could be applied through e g newsletters that are used as a recurring tool by organizations to be notified of new items after they have participated in workshop sessions in such a way slowly we may grow from separate business transactions at workshops towards an active private e marketplace balancing precision and recall is a frequent concern of many recommender system designers e g see geyer schulz and hahsler 2003 in general we target customers in marketplaces with a preference for precision over recall providing good recommendations irrespective of the number of recommendations which is what precision measures in case users don t mind seeing a few extra irrelevant recommendations in order to receive more recommendations they would be interested in we could optimize on recall however it is often the case that over time optimizing on precision may limit the users exposure to novel item types whereas one of the goals of an is recommender is also to initiate new types of symbiotic ideas a trade off mechanism between all such performance measures e g accuracy novelty dispersity and stability with respect to the delivery format predictions vs top n can provide a more balanced evaluation of recommender goals bobadilla et al 2013 therefore the performance of a recommender is always highly suggested to be frequently evaluated within the context of the application according to the changing preferences of its users the process of industrial symbiotic business development is generally categorized in four phases identification assessment implementation and monitoring recommenders proposed in this paper address the first phase i e identification therefore the initial baseline for using recommenders for more complex domains is that the success rate of recommendation is at least considerably better than random recommendation or search information filtering approaches our results are not comparable to those obtained in b2c markets where the precision of e g movie recommendation is more likely to be high most consumers like many movies compared to the precision in b2b markets characterized by narrow interests our expectation is that arm could perform better in specialized markets that reflect a narrow business interest current is markets offer all sorts of items varying from waste heat to iron scrap metals specialized markets such as metal markets have fewer sorts of items this potentially helps recommenders because of the expected stronger rules that can be deduced however once organizations start to use the recommenders for identifying business opportunities they might improve the precision and recall based on their business experience in the successive phase of the assessment to begin with organizations may already save costs when recommenders are able to substantially reduce the number of irrelevant matches 5 2 data challenges to explicit knowledge based is recommenders the io recommender was able to deduce item preference given the fact that preferred items were retrieved this demonstrates the value of studying explicit knowledge based recommendation however it is questionable whether the precision of the io algorithm can be improved to become an effective predictor in an industrial symbiotic marketplace for this data quality and standardization issues first need to be improved in order to determine whether this form of explicit based recommendation can also perform at the rate arm is currently able to under the conditions of the current is data landscape we do not expect to reach such results anytime soon data quality has always been considered as one of the major challenges that need to be addressed by organizations working on predictive analytics projects in supply chain management hazen et al 2014 to understand the problem of data quality in recommendations a number of data quality dimensions need to be assessed e g accuracy timeliness consistency and completeness furthermore ontological problems that have an impact on the effectiveness of prediction or matchmaking technologies have to be taken into account cecelja et al 2015 current decision support tools operate with data from industrial symbiotic markets gathered under real world business conditions as a result the level of detail and information richness is generally limited in addition approximate estimates are used in quantitative values and often a variety of data formats is used leading to unstructured data poor data quality not only hinders the determination of an is match but also hinders better identification of potential opportunities during the design of the io recommender a number of data quality problems were encountered table 6 illustrates the reasons for these knowledge mismatches that result in bad or missing item recommendations we classify the data challenges with a severity level based on the expected frequency of each type of challenge in the is data a number of studies illustrate these data challenges which we found to obstruct knowledge based recommendation see column literature support in table 6 for example a waste offer that lists certain types of bio materials that can be used to produce bio energy may not be directly linked to a demand of bio energy in order to let a system suggest a logical knowledge based recommendation the system should be aware of the link between bio materials as a resource to produce bio energy or should be able to infer the relation between the offered waste and the demand for bio energy e g based on linked or historical data one of the major data concerns is encountered in the different hierarchical levels in which wastes are addressed sander et al 2008 consider the relationship between iron and metal iron refers to raw material largely consisting of the chemical element ferrum is a type of metal and likely to be found as scrap metal however without such an explicit relation text mining algorithms would struggle to relate the two based on this analysis there are three main strategies to make recommendation work in an is context 1 increase the data quality so that knowledge based recommenders have better data to reason with and therefore are likely to make better predictions 2 adapt the knowledge based recommenders to work with limited and less accurate data evaluate if the recommenders can perform at an acceptable rate of recommendation 3 rely on implicit knowledge based recommenders that are not dependent on the quality of data in external knowledge bases 5 3 internal and external validity considering the internal validity of the study several remarks are in place the evaluation indicates that in a practical is context an algorithm based on implicit knowledge perform better in predicting item preference than those based on explicit knowledge this finding is in line with the study of grant et al 2010 and indicates that the theory explaining the role of implicit versus explicit knowledge is transferable to the design of is recommenders however the study selects and designs two algorithms based on existing concepts of recommender systems related to implicit and explicit knowledge e g ontology based versus association rule based therefore the evaluation of the design a part of design science methodology relies on qualitative interpretation a more in depth analysis might include other filtering techniques that represent the implicit and explicit knowledge based recommendation to strengthen the validity of our research however based on previous literature grant et al 2010 we believe that different performance outcomes are highly unlikely because of the typical data problems faced in is marketplaces another aspect of internal validity is that the lci database used to create company profiles has its limitations not all industrial manufacturers processes could be identified thereby resulting in fewer recommendations with respect to arm training and testing the algorithms on a larger data set might improve the results hence the arm is not considered to be heavily influenced by potentially negative effects e g by overfitting the classifier on this is data set finally we focused on the exchange of materials i e waste items and process industry organizations were selected to study the role of implicit and explicit knowledge in the design of an is recommender is is a wider concept that goes beyond the exchange of raw materials the study did not enclose is item types such as waste energy tools machinery or over capacity in a service based industry regarding the external validity a number of generalizations can be derived from this research firstly the proposed model presented in fig 1 is applied to other cases in which recommenders are designed using environmental data that are extracted from ill defined or unstructured data collections the approach can be applied to all types of environmental problems characterized by such typical survey data that one might use together with external data to generate recommendations secondly the instantiation of the model for the case of is fig 3 could be generalized to other cases of recommender design for similar symbiosis settings we believe that the data structures of is and lci are representative for other situations in which is identification is sought as this study is a first attempt of building a recommender in an is context we do not believe these designs are exclusive on the contrary although these designs may be transferable to other is settings other types of implicit and explicit knowledge based recommender filtering techniques e g rule based may replace them and show a different performance the generalizability of the role of knowledge in the recommendation is affected by the quality of available data sources for example ecoinvent as a knowledge base of input output data is currently the best possible source for an io based recommender although we expect the quality of such sources to increase they require substantial collaborative efforts that can be part of long term development goals moreover the proposed model can be applied under circumstances having such data quality issues our methodology explicitly attempts to work with these data sets without considering the possibility of iterative refinement of data quality in collaboration with users the reason for this approach is that if extensive efforts are associated with improving the data quality industries are unlikely to make the necessary investments this has a more direct impact on the sustainability of similar industries 6 conclusion a recurring type of is research is the development of software applications to support the process of facilitating symbiotic developments this paper addresses one of the major problems for the adoption of is identification tools in a practical context following the theory of grant et al 2010 on the dominance of implicit knowledge in is markets we test the role of implicit versus explicit knowledge in designing is recommenders we find that the implicit knowledge based recommender significantly outperforms the explicit knowledge based is recommender in other words the performance of the io recommender is considerably lower than the arm recommender algorithm that relies on implicit knowledge the study further shows that the design of an explicit knowledge based recommender is affected by many data challenges including the linkage of waste streams to process inputs structural and semantic representation of data attribute availability code standardization data reliability and data integrity problems the design evaluation shows that implicit knowledge based algorithm can outperform explicit knowledge based algorithms in identifying is opportunities we argue that such a performance difference can be explained by a number of data challenges that first have to be resolved before explicit knowledge based recommendation can be of practical value to is decision support on the other hand we observe a noticeable role for implicit knowledge based techniques in recommendation the current performance of implicit knowledge based recommenders might enable its acceptance among industries that intend to investigate potential is opportunities this provides an indication to practitioners that usually implicit knowledge based algorithms are more promising for the design of is recommenders on the other hand a new research challenge emerges on how to improve the environmental data landscape so that explicit knowledge based recommendation can become a more viable option finally the results clearly demonstrate the benefits and limitations of both the io and the arm method more generally they demonstrate the challenges and preconditions for a successful recommender design in is markets moreover we confirm that recommender evaluation is essential to the design of an effective prediction algorithm the following contributions are provided by this study i to the best of the authors knowledge this paper is a first attempt in the is literature to propose a recommender for is waste markets and to test the proposed recommenders with real world data ii our proposed model explains the design of recommenders in an environmental data context subject to data quality issues iii in the instantiation of the model we construct two algorithm designs based on explicit and implicit knowledge the proposed explicit knowledge based recommender algorithm uses manufacturing profiles created with input output data of life cycle inventory databases such a recommender demonstrates the applicability of knowledge based recommendations iv we provide comparative performance results for both recommenders so that practical implications are provided based on statistically tested performance indicators this first attempt to implement recommenders in is provides insights on how to design recommenders in symbiotic networks in future work we intend to address other types of recommenders our findings along with the support of grant et al 2010 indicate that given the current issues with is data one might preferably focus efforts and resources in designing recommenders based on implicit knowledge rather than explicit knowledge is recommender design can also be studied in a broader perspective of the identified is categories including waste energy services and manufacturing tools in addition different recommender techniques can be discussed such as is sector based recommendation along with further testing various recommendation mechanisms based on implicit and explicit knowledge acknowledgements this research is funded by european union s horizon 2020 program under grant agreement no 680843 appendix a appendix appendix a 1 stem frequency vectorization algorithm 3 stem frequency vectorization of waste item descriptions image 3 appendix a 2 a simple multi dimensional hierarchical agglomerative clustering algorithm algorithm 4 a simple multi dimensional hierarchical agglomerative clustering algorithm image 4 
26317,various tools and methods are used in participatory modelling at different stages of the process and for different purposes the diversity of tools and methods can create challenges for stakeholders and modelers when selecting the ones most appropriate for their projects we offer a systematic overview assessment and categorization of methods to assist modelers and stakeholders with their choices and decisions most available literature provides little justification or information on the reasons for the use of particular methods or tools in a given study in most of the cases it seems that the prior experience and skills of the modelers had a dominant effect on the selection of the methods used while we have not found any real evidence of this approach being wrong we do think that putting more thought into the method selection process and choosing the most appropriate method for the project can produce better results based on expert opinion and a survey of modelers engaged in participatory processes we offer practical guidelines to improve decisions about method selection at different stages of the participatory modeling process keywords stakeholders collaborative learning qualitative analysis quantitative modeling participatory planning mental models 1 introduction numerous tools and methods facilitate stakeholder engagement in participatory modeling pm which stave 2010 defined broadly as an approach for including a broad group of stakeholders in the process of formal decision analysis in the pm process participants co formulate a problem and use modeling to describe the problem to identify develop and test solutions and to inform the decision making and actions of the group therefore we define pm specifically as a purposeful learning process for action that engages the implicit and explicit knowledge of stakeholders to create formalized and shared representations of reality since pm is heavily focused on collaborative learning the tools and methods used during pm projects are expected to promote system understanding and awareness for all stakeholders by stakeholders we mean all who have a stake in the project this includes modelers and researchers themselves who are often considered external to the project but still have interests in it come with their own biases and cannot be assumed totally objective and neutral voinov et al 2014 the level of engagement differs across stakeholders and varies from one stage of the project to another arnstein 1969 hurlbert and gupta 2015 reed et al 2009 argyris and schön 2002 showed that there are two levels of learning referred to as single loop and double loop learning in single loop learning individuals and groups act within a single reference frame where specific hypotheses values norms beliefs and objectives are assumed to describe the world learning in these systems consists of observing the results of actions and potentially modifying future actions based on what is observed in double loop learning actors question and learn about the reference frame itself and may change their fundamental hypotheses values norms and beliefs based on what they learn about the system as well as what they learn about the outcomes of specific actions zellner and campbell 2015 the transition between single and double loop learning can result from the interaction between individual and organizational learning argyris and schön 2002 found complex retrospection and feedback mechanisms between individual and organizational learning the individual mental models that are used to construct shared mental models of an organization coalesce thereby modifying the perception of the organization and transforming organizational values and paradigms in turn this modifies the environment of the individuals and affects their own mental models daré et al 2014 as a result the act of model co creation is in itself an act of knowledge construction at both the single and double loop learning levels in some cases pm processes deliberately avoid formal model co creation to first allow the identification and challenging of stakeholders causal beliefs and expectations and consequently a reconstruction of knowledge habermas 1990 smajgl and ward 2013 the goal of this paper is to provide an overview of some of the methods and tools for pm identify some of their strengths and weaknesses and provide some guidance for practitioners as they select methods for their pm projects for the purposes of this paper we define a tool as a modeling technique used to carry out a particular function to achieve a specific goal tools are defined documented do not change significantly through use and are clearly external to their users and often not created by them in contrast a method is a way of doing something in particular a way of using tools according to mingers 2000 a method is a structured set of processes and activities that includes tools techniques and models that can be used in dealing with a problem or problem situation a particular method can be supported by one or several tools for example in this context agent based modeling abm is a method netlogo mason or repast are some of the possible open source tools used to perform abm multiple tools often exist to support a single method and some tools also serve several methods for example netlogo anylogic or numerus are tools that can be used within both abm and system dynamics sd methods while the choice of methods used can heavily impact both processes and decisions there is little scholarly discussion about how tools and methods are chosen during pm certainly decisions about methods are more influential for the whole process than the choice of a particular tool and therefore should come before choice of tools for instance there are not many implications in deciding to use stella rather than vensim or simile all are well established tools that support the sd method but the decision to implement a more quantitative method rather than a qualitative or conceptual one can potentially significantly change the outcome of a pm process for example a companion abm based on role playing games see barreteau et al 2001 can increase stakeholder involvement in the pm process and may generate much different results than computational abm using only computer simulations and modelers assumptions previously voinov et al 2016 reviewed several participatory tools and methods that have been used to enhance stakeholder participation for different components of the pm process they concluded that while many different methods are used for various stages of the process in practice there is rarely much justification given for the use of a particular method it is difficult to find examples of participatory projects that used different combinations of modeling methods when dealing with the same problem in most cases once the method or combination of methods is chosen it becomes the only one reported we recently reviewed 180 papers related to participatory modeling as part of a sesync project on synergizing public participation and participatory modeling methods for action oriented outcomes https www sesync org project enhancing socio environmental research education participatory modeling we found no papers that reported using one method and then a switch to another method this may be due to a general reluctance to report failures rather than only success stories but it complicates the comparison of different methods another reason most studies report only one method might be that switching from one method to another is costly in terms of time and resources a similar though much more limited effort in healthcare research which focused on comparison of three dynamic simulation methods sd abm and discrete event simulation marshall et al 2015 also reports very few failures of particular methods that led to switching to other methods a careful and conscious selection of methods is important for the modelling process and its outcomes ideally the selection would be accompanied by effective evaluations to monitor the impact of individual methods used during in pm hassenforder et al 2015 smajgl and ward 2015 however in many case studies the choice of methods and tools seems largely driven by the experiences of participating researchers prell et al 2007 this is a manifestation of the hammer and nail syndrome once someone learns to use a hammer everything starts to look like a nail a researcher with expertise in system dynamics is very likely to apply system dynamics for the next modeling project even if other methods could be equally or even more appropriate to address the full set of driving questions retraining is time consuming and resources are always scarce engaging colleagues with experience in alternative approaches could help expand the scope of methods considered but this is not always feasible there are practical and social reasons why this experience driven approach to method and tool selection is not optimal especially in the field of pm first the value of pm in developing models that effectively and efficiently meet participants requirements will be improved by using methods that best fit the project purpose and context the modeling skill set available should be considered only to identify gaps in the skills required to address the problem in question pm seeks to be transparent to the users and it is critical to make sure that pm practitioners are not treating all problems as nails just because they are good at using a hammer stakeholders defined broadly as above are expected to engage in all steps of the pm process which includes method selection as well as the modeling steps while the participation of various groups of stakeholders will certainly be different at each stage all stakeholders should understand why the chosen methods and tools are appropriate this requires some flexibility in the pm process whereby stakeholders move collectively from the problem to an appropriate method and onto tools and associated skills found within the project team a sharper focus on method and tool selection is needed this requires understanding stakeholders preferences and constraints including their experience with particular methods the availability of training for specific methodologies the ability to use and maintain a particular tool for the long term given the costs to do so and or the ability to combine a new tool with existing tools or methods smajgl 2015 second social factors may also affect method and tool selection the choice of methods is more than a technical decision it can also involve ethical or other social judgments it may make it easier or more difficult for specific groups to participate effectively and to adequately represent specific technical aspects of the problem in implementing a pm process decisions must be made about who is involved and what is included midgley 1995 tradeoffs between narrow technical accuracy and more inclusive participation in the modeling processes themselves may add more legitimacy to the process nabavi et al 2017 or help to level the playing field in the case of asymmetries in the power i e influence or control or knowledge of different stakeholders e g barnaud and van paassen 2013 campo et al 2010 when the choice of modeling methods and tools becomes largely a personal decision of certain more knowledgeable stakeholders it represents an ethical posture based on their own preferences and experiences and may not reflect the larger pm group methods and tools ought to be chosen in service to ethical or social needs in contrast method driven pm practice can result in methods that are epistemically violent to vulnerable participants they forcibly replace one structure of beliefs with another individuals must be invited to join the process but it is rarely possible to invite every individual who might be interested in the questions being addressed time and resource constraints as well as the need to have effective and useful interactions among the participants means that some individuals are necessarily excluded further because modeling often requires some element of rules or strategy guiding the approach prior to the decision making process certain participants may have greater power the choice of methods and tools can significantly empower some participants at the expense of others often these others may already be traditionally disenfranchised if the method chosen is one with which the project leaders have a lot of experience it might give them substantial advantage in understanding and controlling the process relative to other participants for whom the method is novel the confidence and knowledge they have make them more likely to guide the participatory process while subordinating the novices but using a method that the practitioner is not familiar with just to maintain equality of power would be also unrealistic and unproductive because inequality in power can manifest itself in many ways kraus 2014 it is important for a truly participatory process to have all individuals informed not only about the decisions being made but also about the decision making process itself ultimately the research team can be even assembled after stakeholders co designed the project and select the most effective methods based on the policy indicators and the scale they perceived as most relevant smajgl 2010 smajgl et al 2009 on the positive side methods can also empower and integrate many perspectives any of the methods and tools described in this paper may promote both individual and social learning through the use of the model as a boundary object a representation with a shared meaning that can facilitate exchanges of ideas and worldviews between participants schmitt olabisi et al 2014 johnson et al 2012 zellner 2008 a boundary object implies a distance from reality and situations that are sometimes tense and painful this distancing can allow for discussions on subjects that are conflictual or taboo by fitting into a social issue the model co designed with the stakeholders becomes an object of mediation d aquino et al 2002 2003 promoting conflict resolution and collective decision making when selecting methods for participatory modeling modelers and facilitators should consider how the methods or tools will provide evidence of learning for example a before and after systems diagram may reveal shifts in mental models that occur as the result of a pm exercise discourse analysis may demonstrate changes in the ways groups conceptualize problems and problem solve as the result of interaction with the model radinsky et al 2017 consideration of learning is therefore an integral part of method selection and process design in pm the selection of methods is both a critical and a difficult task that ideally requires 1 knowledge of available methods and tools and 2 careful examination of selection criteria and trade offs this paper addresses both of these issues section 2 describes a broad array of available methods and tools available to scientists modelers and stakeholders and section 3 systematically examines pm practice and the issue of method and tool selection 2 overview of pm methods there are numerous methods used in pm projects in fig 1 we propose a typology of methods and some possible combinations thereof it is sometimes difficult to distill the particular methods and tools used within the context of broader methodologies proposed for pm these methodologies tend to cover the whole process and assume a particular type or set of tools embedded within for example the soft systems methodology ssm checkland and holwell 1998 and the companion modeling commod approach bousquet et al 2002 barreteau et al 2003 etienne 2014 are two well known broader methodologies ssm uses a sequence of stages it 1 considers a problem 2 expresses the problem using rich pictures a freestyle mapping of the different elements that make up the problem e g using pictures and text to represent processes actors issues 3 develops conceptual models to represent possible actions to improve the situation 4 compares models to the real world 5 debates and identifies desirable and culturally feasible changes and 6 takes action to improve the situation the ssm approach may well cover the whole pm process but mentions only one particular method rich pictures the commod approach combines such methods as role playing games and abm to promote single and double loop learning for both individuals and groups for the first steps of the process concerning fact finding the approach involves stakeholders in the co design of a conceptual model of the system at stake using role playing games this sharing of representations is done by means of a series of collective workshops during which actors resources dynamics and interactions ardi are identified and clarified etienne et al 2011 these conceptual models are then implemented as abms and brought back to all stakeholders for further discussion and improvement in the following sections we identify and describe specific methods within each level of this typology these methods can be used separately or be combined within some of the more general methodologies such as ssm or commod described above here we view them as reusable components that can be reassembled in a variety of ways for future pm projects the methods and tools discussed below are commonly used in pm but do not constitute an exhaustive list 2 1 fact finding the fact finding stage s of pm focuses on finding generating and communicating data information and knowledge relevant to the problem being considered this stage may continue throughout or be revisited multiple times during the pm process in addition to standard research techniques that include literature searches and reviews typical approaches to fact finding specific for pm are described below 2 1 1 surveys and interviews surveys consist of a suite of questions they can be undertaken in person by phone on paper or electronically when surveys are conducted face to face they are usually called interviews these can be time consuming but offer the possibility to clarify the questions to gather additional valuable information not covered by the questionnaire and to follow up conversations to explore results virtual surveys collecting responses without presence of interviewer could potentially reach a larger number of people than in person surveys but suffer from a self selection bias and it can be difficult to know or understand who responds and how reflective of the interests of the full group those respondents are it is also useful to distinguish between structured surveys or interviews and semi structured interviews structured surveys or interviews use pre defined questions in a set order often with a closed response format i e respondents choose from a list of possible responses while this format limits the information that can be gathered results are easily quantified and are relatively straightforward to analyze semi structured interviews can include a mix of closed ended questions open ended questions i e respondents answer without choosing from a preset list of possible responses and discussion discussion may be directed based on a particular response to a closed ended question or may simply follow themes that arise from responses to the open ended questions the qualitative results of such surveys may be challenging to analyze and summarize for others but they may allow for deeper understanding of responses phone and paper based surveys are relatively common in a variety of contexts telephone interviews pose recruiting scheduling and response rate challenges paper based survey data are difficult to manage and process especially when sample size is large studies of the potential for using personal digital assistants pdas for gathering data electronically indicates that electronic surveys improve input data accuracy facilitate data management and allow for automated data processing lane et al 2006 onono et al 2011 ficek 2014 recently the increased availability of tablets and mobile phones especially of low end smartphones has helped overcome some of the barriers to electronic data collection tomlinson et al 2009 kolagani and ramu 2017 smartphones also permit collection of location and multimedia data photographs video and audio segments in addition to text and allow better visualization accuracy and analysis of the data several free and open source solutions such as open data kit odk https opendatakit org kobotoolbox http www kobotoolbox org and village gis http mspaceapps in villagegis help users customize these solutions to their needs and help collect analyze and manage their data still reliance on this kind of technology tool may disenfranchise the poor and less technologically facile groups of respondents moreover this approach is less efficient with open ended questions where users are expected to enter significant amounts of text 2 1 2 crowdsourcing crowdsourcing is another data acquisition method that is becoming increasingly popular it involves gathering data from a large number of people crowd including those unknown to the individual or organization gathering the data some main advantages are the relatively low cost to the data collector quick speed scalability and the diversity of participation and types of data that can be obtained however data obtained by crowdsourcing may be hard to reproduce and its quality may be difficult to validate the data may differ significantly from the judgment of the experts especially when more expertise is required sen et al 2015 another disadvantage from the point of view of pm is that crowdsourcing is usually used as a one way data collection method voinov et al 2016 it rarely gets used for higher levels on the participation ladder arnstein 1969 that expect greater stakeholder engagement individuals either affiliated with a particular project or not can volunteer to collect and provide data for example volunteered geographic information vgi goodchild 2007 is provided by individuals associated with a specific geographic region a prominent example of vgi is openstreetmap osm which has the goal of creating a free editable map of the world osm was designed to overcome restrictions that exist on map availability in many places it has produced spatial data of high quality comparable to that of proprietary spatial data for most parts of the world haklay 2010 alternatively data can be collected from social media derived from information provided by people even without knowing how it will be used for example van zanten et al 2016 estimated continental landscape values based on the social media data and usgs has the twitter earthquake detection data mining program which is used to help determine the intensity of earthquake energy felt by twitter users earle et al 2012 1 1 https earthquake usgs gov earthquakes ted https blog twitter com official en us a 2015 usgs twitter data earthquake detection html 2 2 process orchestration by definition pm is a process therefore its success depends on how well the process is organized managed monitored and reported process orchestration methods may overlap or combine with other methods note that in fig 1 process orchestration spans across all stages of pm facilitation for example is essential at multiple stages of a pm process the focus of the facilitation may change e g from understanding stakeholders ideas and data to visualizing results and making decisions but facilitation is still required three commonly used process orchestration approaches are described below but there are many others 2 2 1 facilitation facilitation is key to pm processes regardless of other methods and tools used facilitation and the analyses that support it come with their own set of techniques and tools for example capability and knowledge mapping can help determine who has specific skills and capacities that are needed and what knowledge gaps might be present they can help map out the distribution and intensity of expertise and knowledge jetter et al 2006 chapter 6 other techniques such as diagramming or the use of manipulatives e g dice can be used to help individuals express their ideas cards stickers or digital tools can also help facilitate and capture ideas facilitation and its tools must be carefully employed and focused on moving the pm process towards attaining its goals if mishandled the facilitation process can become a source of frustration and alienation multiple facilitators may be needed to offer different kinds of support for example a technician to facilitate with modeling tools and a community leader to facilitate interaction among participants hovmand 2014 the facilitation process must generally be open accessible and safe for honest discourse during a facilitated participatory process a good facilitator will strive to allow all participants to express themselves by trying to give everyone time to speak and express their points of view to encourage mutual learning and understanding and to help foster a collaborative environment it can be helpful for a facilitator to understand the background of the involved participants to guide their initial and continued interactions and ultimately their perceptions of the tools the model and the value of the process kaner 2007 an important aspect of facilitation in the pm process is the focus on modeling and the use of some of the structured modeling tools described below at some stages of the pm process the facilitator may need to understand the affordances and constraints of specific modeling methods tools and associated approaches the facilitator role extends to encouraging all participants to see others as legitimate and valuable contributors to the development and growth of the model and associated analyses and processes depending on the pm problem it may be important to consider and address cultural differences in how participants interact and differences in their willingness to enunciate or modify beliefs or be receptive to contradicting beliefs or values this links directly to methods such as cultural consensus described below good facilitation should also recognize the role of biases beliefs heuristics and values bbhv in the pm process according to glynn et al 2017 biases represent tendencies to believe in or pay attention to certain observations ideas or people consciously or unconsciously but with no good or testable reasons this may result in decisions or actions which may be hard to explain or expect heuristics are innately derived rules of thumb mental shortcuts or simplifications that help us navigate through the complexity of the world and its relationships kralik et al 2012 levine and perlovsky 2008 relatedly values are conceptions of the desirability undesirability or relative prioritization or importance of actions or things beliefs create 1 an acceptance or a conviction that something or some statement is true or real or 2 a trust faith or confidence in a set of values and attitudes in a tradition in a thing or concept in a tribe or in a person including oneself good facilitation should help participants in recognizing mitigating or otherwise modifying or shaping their bbhv to improve pm processes this involves some type of reframing personal or community questioning and learning about oneself or itself and training to both ease bbhv recognition and to create more effective and appropriate communication scientific ethics and integrity suggest that transparency and participant awareness are needed for bbhv elicitation and communication hill 2012 kelman 1982 cahill et al 2007 different cultural norms may affect how awareness is created or how participants are willing to enunciate or modify beliefs or are receptive to contradicting their beliefs or values this links directly to such methods as cultural consensus described below facilitation can be improved and may be more useful if records of the pm activities include documentation of the facilitation processes used and the results of those processes such a record increases transparency and allows reconstruction and analysis of what happened what was used and how and what impacts and outcomes resulted it creates a temporal record useful in understanding the future evolution of the system it also aids in learning from successes and failures and provides insights that may help in applying or transferring the facilitation processes to other pm efforts radinsky et al 2017 describe methods derived from the learning sciences for transcribing coding and analyzing video recorded discussions in participatory modeling settings these methods help us understand how groups of participants interact with each other and with the modeling tools how they learn about the complex problem they are facing to what extent new knowledge and learning is translated into a plan for action and the role facilitators played in supporting the process 2 2 2 role playing games rpg a role playing game is a useful method to exchange knowledge among stakeholders in a desired context rpgs involve creation and use of a virtual world with simplified real world conditions to collect information explore and understand context and situation and develop and explore collectively possible solutions a rpg comprises four main elements environmental settings player components rules of operation and inputs to the game the rules and structures of the rpgs promote player understanding by facilitating communication among stakeholders in an open environment eden and ackermann 2004 in the game different members play the role of different stakeholders and develop proposals collectively rpgs can create more effective teams help identify and address various stakeholders common or conflicting interests effectively build a supportive coalition and increase the effectiveness of implementation rpgs may also reveal implicit social rules and interactions between actors that might not have been evident during interviews and other interactions 2 2 3 brainstorming b brainstorming is a process that encourages all participants to offer ideas on a particular topic that are captured prior to any critical assessments of those ideas only after a robust list of ideas from the full set of participants is generated are decisions made about whether and how to exclude include or incorporate those ideas brainstorming can be used at many stages throughout the pm process and is often used when a facilitator feels that the group has narrowed their discussions prematurely or as a tool to encourage broader thinking and participation and to ensure that all voices are being heard 2 3 qualitative modeling in qualitative modeling in pm project participants build conceptual visual representations of the components of the problem being considered the focus of qualitative modeling is on identifying articulating and representing the relationships among the many components of a problem on the spatial temporal relationships and on how changes in one area affect other factors that may be important to solutions and to stakeholder concerns 2 3 1 rich pictures rp rich pictures is a diagramming tool that was developed as a part of the soft systems methodology checkland 1999 rp makes use of cliparts texts and symbols to represent how a group of people think about a particular issue bell and morse 2013 describe rp as a powerful intellectual and participatory device because it allows people to draw what they think but may not be able to write or speak about there are no strict rules or formal conventions for drawing rp it has to make sense for those who are involved in the process and be seen as a useful device communicating their ideas about the problem although this freestyle nature allows for creativity it makes it difficult to share a rich picture outside the group without very clear explanation of the meaning embodied in the picture lewis 1992 some attempts have been made to provide general guidelines on practices for drawing coherent and useful rp open university 2000 bell et al 2015 2 3 2 cognitive concept mapping ccm concept maps are graphical representations of organized knowledge that visually illustrate the relationships between elements within a knowledge domain a concept map results in a network where concepts nodes are connected through directed links edges these links are labeled to indicate semantic or otherwise meaningful relationships e g are in includes these labels allow one to logically define the structure novak and cañas 2008 the argument for representing knowledge with concept maps emerges from constructivist psychology which postulates that individuals actively construct knowledge by creating mental systems which serve to catalogue interpret and assign meaning to environmental stimuli and experiences raskin 2002 knowledge constructed in this manner forms the foundation of an individual s organized understanding of the workings of the world around them and thus influences decisions about appropriate interaction with it several other mapping approaches are related to concept maps a cognitive map usually represents an individual s knowledge or beliefs about a particular issue or system of interest whereas a concept map represents the perspectives of several individuals who worked together to identify key concepts link them and decide on the most appropriate labels describing the nature of each link eden and ackermann 1998 additional constraints or steps can be imposed to create different types of maps for instance a mind map follows a similar process to a concept map but the core idea s would be positioned in the center of the map with all other ideas branching off radially differences between these maps and implications for research have been discussed by davies 2011 2 3 3 causal loop diagram cld causual loop diagrams are commonly used in system dynamics modelling to represent the key variables and relationships that are assumed to explain dynamic behavior the cld method uses a relatively small number of conventions making it simple to use even for a non technical audience lane 2000 arrows represent causal relationships where relationships are indicated by direction i e positive or negative the emphasis in drawing a cld is on eliciting and representing feedback loops and delays that explain the problem behavior lane 2008 presents a critical review of the use of cld in system dynamics and notes that the role of cld changed from a back end tool to communicate about the output behavior from the simulation model i e expository mode to a front end model conceptualization tool cld can be used as a standalone method for model conceptualization without being necessarily extended to the stage of a system dynamics simulation model the cld method has been credited for its simplicity and ability to give an aggregate or strategic view of the problem structure which helps to keep focus on feedback loops rather than on details the method has been criticized see morecroft 1982 richardson 1997 for more details for example for not adhering to fundamental principles of accumulation which could lead to ambiguous and flawed inferences about problem dynamics in the context of pm sedlacko et al 2014 examined the use of cld as a tool for promoting knowledge co production and facilitating group learning they found that to be effective cld require that groups have an agreed ontology about what variables mean and how the system works otherwise there is a risk of producing shallow diagrams that hide both unexpected depths about given problems and interesting insights in the differences between various stakeholders mental models and views 2 3 4 cultural consensus cc cultural consensus is a collection of analytical techniques and models that can be used to estimate cultural beliefs and the degree to which individuals know or report those beliefs weller 2007 formally cc theory estimates the culturally correct answers to a series of questions group beliefs based on responses to similar questions and simultaneously estimates each respondent s knowledge or degree of sharing of beliefs romney et al 1986 a structured questionnaire is used to collect nominal or ordinal data on set of relevant questions those questions are typically designed after interviews participant observation and direct input from stakeholders statements that capture key themes and knowledge are elicited from stakeholders paolisso 2015 descriptive statistics can be applied to stakeholder responses to identify any within and between group patterns in the answers individual responses are processed through factor analysis to produce estimates of degree of sharing between individual and group cultural knowledge the method assumes that there is only a single factor solution which represents the cultural consensus stakeholders can be brought in again at this point to help interpret the pattern of responses in the informal model the competence scores tell how well the responses of each individual correspond with those of the group stakeholder engagement is critical to interpret these results since cc does not provide definitive answers to what are the nature and boundaries of the shared underlying knowledge only that there is a shared knowledge system underlying the pattern of responses paolisso 2015 cc complements bbhv recognition above in that it formalizes a methodological approach that captures the implicit and tacit knowledge that help drive behaviors beliefs and values 2 3 5 decision tree analyses dta a variety of approaches can be used in qualitative modeling that emphasize identifying and illustrating the relationships between decisions actions that can be taken to influence the situation of interest and the outcomes of interest to stakeholders in the context of the pm study their objectives for example decision trees kirkwood 2002 are used to illustrate the sequence of decisions and system changes that occur over time and how they affect the outcomes that stakeholders care about dta are also used in quantitative modeling but can be used primarily as a qualitative structuring tool a more general name for these methods would be decision focused structuring both adaptive management holling 1978 williams and brown 2012 and dynamic adaptive policy pathways dapp haasnoot et al 2013 are decision focused structuring and modeling methods that clearly differentiate actions system uncertainties and evolution and stakeholder objectives early in the model structuring phases these methods are designed to stimulate thinking about how decisions may change or other decisions may need to be taken as the system evolves they focus on the concepts of dynamic change and adaptation of actions as with decision trees these methods can be useful in qualitative modeling to provide both structure and explicit consideration of timing they can also be carried further into semi quantitative or fully quantitative modeling a recent case study using dapp lawrence and haasnoot 2017 highlighted the benefit of this approach in stimulating discussion among decision makers planners and stakeholders on future actions by making uncertainty explicit making the modelling process much more transparent and connecting decisions to outcomes of interest 2 4 semi quantitative modeling conceptual quantification the distinction between qualitative and quantitative methods is not always clear cut quantitative methods use formulas and equations and make calculations based on data however in many cases the data are qualitative or semi quantitative the data may consist explicitly of qualitative information they may be numeric estimates of values that are agreed upon or negotiated among participants or they may be based on experimental data but have significant uncertainty about them in our typology a method is classified as quantitative if technically there are ways to quantify most of the information used this can be done through experiments monitoring surveying etc if it is impossible or very difficult to obtain or use numeric information then the method is considered qualitative some methods are semi quantitative for example we categorize fuzzy cognitive mapping fcm as semi quantitative since it employs some numerical analyses of the values assumed in the model but the values themselves are most likely to be only qualitative or conceptual on the other hand bayesian belief networks bbn are considered as quantitative because experiments can potentially be designed to measure some of the probabilities used in the method 2 4 1 fuzzy cognitive mapping fcm fuzzy cognitive mapping allows groups to share and negotiate knowledge about a problem and build semi quantitative conceptual models fcm facilitates the explicit representation of group assumptions or beliefs about a system being modeled through parameterized cognitive mapping özesmi and özesmi 2004 gray et al 2014 example https participatorymodeling org node 36 as in ccm fcm starts with defining the most relevant variables that comprise a system and the dynamic relationships between these variables and then extends the ccm method by assigning the degree of influence either positive or negative that one variable can have on another fcm has three specific strengths compared to qualitative concept mapping techniques which have led to their increased use in futures studies scenario planning and complex systems modeling jetter and kok 2014 papageorgiou and salmeron 2013 first since the models created are semi quantitative they can be evaluated to understand system trends based on what if scenarios in a pm context this allows stakeholders to contrast and compare the effect of different scenarios or evaluate the effectiveness of different management interventions in a given a socio environmental problem see gray et al 2015 second an fcm can be constructed in many ways providing a way to combine the experiences or expertise of several individuals with various qualitative data sources see for examples singer et al 2017 for instance individuals can share their experiences and understandings and these can be aggregate to create a group level map gray et al 2014 if the right data are available the model can be derived entirely from the data using learning algorithms papageorgiou and salmeron 2013 third fcm can be subjected to a range of network metrics allowing researchers to contrast the ways in which individuals or groups think about a potential problem lavin et al 2018 measure the degree of structural variation across stakeholders and hence provide insight into uncertainty and complex socio environmental problems that groups seek to understand there are numerous extensions to the fcm methodology and software tools have been developed specifically to support participatory fcm see www mentalmodeler org gray et al 2013 fcm can be employed with a stakeholder group to build and evaluate through scenario analysis a model in a short time 1 2 h or through individual interviews that take less than 30 min however such quick analysis comes at a cost since fcm does not represent specific quantities and is largely limited to defining linear relationships between concepts additionally time and thus delays are not represented in fcm as the system changes in steps that bear no connection to real world time therefore although a useful tool to quickly and efficiently evaluate the structure and function of a dynamic problem the model output is limited to conceptual and qualitative units with no real time reference for how dimensions of a system may change over any real time horizon that stakeholders may desire for their decision making 2 4 2 scenario building sb scenario building or planning or exploration amer et al 2013 is a practical approach to dealing with uncertainties about the future scenario planning relies on a broad analysis of trends and policies to cover a range of plausible futures it is distinct from forecasting or predicting a specific future each scenario should be internally consistent meaning that given current conditions and trends it is plausible that the different aspects of the scenario could play out in the described way each scenario is designed to be substantially different from other scenarios and to highlight a unique and interesting possible future in participatory modeling scenarios can build from quantitative models e g systems dynamics in this approach stakeholders provide knowledge about the structure of these models and indicate which input variables are critical and uncertain a quantitative model is run for multiple input combinations within the plausible range and the results provide the final value state for each system element the resulting internally consistent scenario may then be described in qualitative terms in the form of a scenario narrative other pm approaches create scenarios in a fully qualitative fashion stories scenarios are used to identify robust policies that are successful in most or all future scenarios as well as to proactively develop backup plans 2 4 3 social network analysis sna social network analysis is a method for studying a set of social relations among actors and how these relations and their patterning can impact or be impacted by actors views behavior perceptions and by learning prell 2012 actors can be individual persons or social entities such as organizations or even countries e g prell and feng 2016 social relations can represent friendship communication or trust or can refer to other types of flows such as membership trade or various kinds of resources a relation in sna usually involves at most two entities which allows sna to use analytical tools from network graph theory shirinivas et al 2010 data on social relation networks can be binary or valued although most analyses ultimately require that the analyst decides on a cut off value used to dichotomize the data before modeling the modeling of networks ranges in complexity simple descriptive measures and or visual digraphs of a network can be helpful in identifying which stakeholders are more popular or powerful which are more peripheral and how stakeholders might cluster together such simple descriptive measures can be helpful in designing participatory workshops and or helping stakeholders understand the social context in which they are embedded prell et al 2008 2009 more complex stochastic models have been designed for handling network independencies such as exponential random graph models or ergms robins et al 2007 and stochastic actor oriented models saoms snijders et al 2010 these stochastic models can help analysts better understand with greater precision the decisions perceptions or behaviors of stakeholders especially in the context of natural resource management or governance bodin et al 2016 matous and todo 2015 prell et al 2017 2 4 4 analytic hierarchy process ahp during the pm process it is often useful to consider the effects of scenarios or alternatives on a diverse set of criteria or objectives identified by the participants while economic or cost benefit analysis is sometimes used to summarize the impacts it can be helpful to combine those effects into a summary metric through a model that explicitly accounts for conflicts and tradeoffs among those criteria including criteria not easily monetized several approaches for evaluating options against multiple criteria assessing tradeoffs among those criteria and recombining results into a summary metric have been used in pm a popular method is analytic hierarchy process ahp saaty 1980 in ahp tradeoffs among criteria are derived as criteria weights from stakeholder input on the pairwise relative importance of all criteria alternatives are evaluated against those criteria using similar stakeholder input and the results are combined in a weighted linear summation hajkowicz and higgins 2008 howard 1991 example https www participatorymodeling org node 45 criteria weights are often assessed from individual stakeholders when there is significant variability among weights obtained from different stakeholders it may be challenging to identify an appropriate group summary metric means ryu et al 2011 tian et al 2013 medians kolagani et al 2015 and even the geometric mean saengsupavanich 2013 have been proposed some modelers preserve the range of values by propagating the variability across stakeholders through the model using the monte carlo simulation approach rosenbloom 1996 hauser and tadikamalla 1996 lafleur 2011 kolagani et al 2016 ahp is a special case of a more general approach known as multiple criteria decision analysis mcda greco et al 2016 other popular approaches for mcda in environmental decision making are multi attribute utility theory and outranking approaches huang et al 2011 any of these approaches can be used at various levels of quantification they can be used as qualitative tools to support problem structuring and they can be partially or entirely quantified to create and support concrete valuation and comparison of values tradeoffs necessary in any decision 2 5 quantitative modeling 2 5 1 geographic information systems gis geographical information systems gis are computer based mapping frameworks that can be used to help stakeholders in visualizing and modelling their problems spatially for example gis can be used to analyze and display how various scenarios play out on the landscape being considered and how those changes provide benefits or costs to various stakeholders gis can also be used to provide inputs to other models for example stakeholders can map the land use and soil characteristics of their land parcels in a gis and use these maps to measure quantitatively the extent of land parcels under various land use and soil categories in participatory mapping local stakeholders can sketch out spatial features on the ground paper or a touch screen on top of remote sensing imagery chambers 2006 such use of gis by ordinary stakeholders has been termed public participation gis pp gis sieber 2006 however implementation of quantitative gis models typically requires quite a high level of technical skill and over reliance on the technical aspects of gis may alienate less skilled stakeholders chambers 2008 there are several efforts to simplify gis tools to facilitate use by less technically trained stakeholders taking advantage for example of the increasing popularity of mobile and web technologies kolagani and ramu 2017 example https participatorymodeling org node 38 https participatorymodeling org node 121 2 5 2 empirical modeling em empirical modeling refers to the process of identifying and quantifying relationships among factors of interest using observed and experimental data em is sometimes called best fit modeling and is contrasted with mechanistic process based modeling voinov 2008 in best fit or empirical models mathematical relationships are derived from data they may or may not represent actual physical relationships between those factors they are often used early in modeling projects to explore interpret and understand available quantitative data these models are sometimes referred to as black box models because they operate as closed devices that process information with no explanation of processes or parameters involved serrat capdevila et al 2011 refsgaard et al 2005 these models are entirely driven by the specific data available and they are risky to use outside the ranges covered by that data extrapolation because they do not necessarily explain real world relationships between factors they can be difficult to use or communicate in a pm process basco carrera et al 2017a though their accuracy can be very high 2 5 3 cost benefit and other economic analyses cba economic analysis may be conducted as part of the pm process especially in the latest stages of the planning cycle to help assess the benefits and costs of alternative decisions or investments an economic analysis may help guide the design and ultimate choice of policy alternatives and associated system scenarios and forecasts economic analyses may be used to place a total monetary value on specific outcomes of interest this approach spread into the environmental arena with the concept of ecosystem services national research council 2005 total value is generally composed of use values and non use values use values can further be parsed into direct use values e g fishing ecological function values e g water availability and option values e g potential protection from floods non use values can take the form of an existence value e g satisfaction of knowing that a species exists or a bequest value e g preserving a resource for the next generation economic analysis can also be used to help determine the worth or benefits of acquiring additional data or information cf young 1992 cost benefit or benefit cost analysis cba e g hanley and barbier 2009 nasa 2013 is a commonly used methodology for assessing the anticipated costs and benefits of an investment or policy change compared to those that would accrue without an investment or policy change the credibility of the no change scenario is essential in assessing the credibility of the cba in each case the analysis generally requires developing a time series of costs and benefits that would accrue under each scenario and then using a discounting hypothesis to summarize that time stream in a given reference year the assessment of both costs and benefits is likely to consider only a subset of the potential costs and benefits and is also likely to miss indirect costs and benefits that may result from a particular application or policy choice there are many examples of cba including one undertaken by the u s geological survey to assess the value of creating a national map halsing et al 2004 different levels of implementation of a national map were compared to the counterfactual of not creating a national map 2 5 4 system dynamics sd system dynamics is a simulation based method used to articulate and understand the causal interactions that explain how the system behavior changes over time key to the sd method is the representation of a system in terms of stocks where material energy or items are stored and accumulated and flows which are rates of exchange between stocks an sd model provides useful insights into the feedbacks delays and nonlinear interactions helping decision makers to see the long term system wide and sometimes counterintuitive outcomes of their decisions example https participatorymodeling org node 39 https participatorymodeling org node 82 sd is the foundation method for several participatory modelling methodologies such as group model building vennix 1999 mediated modelling van den belt 2004 participatory sd antunes et al 2015 sd learning laboratories nguyen et al 2011 bosch et al 2013 system dynamics models were initially developed to investigate the temporal dimension in non spatial systems some efforts have focused on extending the capability of systems dynamics to spatial modelling i e spatial system dynamics modelling to investigate the effects of spatial characteristics on the problem behavior over time ahmad and simonovic 2004 bendor and kaza 2012 costanza and voinov 2003 these efforts include 1 breaking down the system into zones where each zone is represented as system dynamics model ford 1999 and 2 coupling system dynamics models with gis to exchange information between spatially distributed models over the simulation time neuwirth et al 2015 2 5 5 bayesian networks bn bayesian networks are a statistical modelling method where the model takes the form of a unidirectional network a directed acyclic graph dag nodes represent variables in the problem while links represent the causal relationships among these variables variables usually take discrete states with certain probabilities the graphical representation makes bn intuitive and useful for communicating model assumptions uncertainty and the complex interactions among variables especially with non technical stakeholder groups carmona et al 2013 castelletti and soncini sessa 2007 chen and pollino 2012 in addition to the qualitative and graphical component i e the dag bayesian networks also use conditional probability tables cpts to quantify the strengths and probabilistic relationship between the causal variables parent nodes and children variables pearl 2009 bns can use and integrate qualitative data e g the prior knowledge gained from experts or literature and quantitative data e g survey data bns also have other advantages such as a capability to handle missing observations potentially high accuracy for small amounts of data and the possible support of scenario based analyses 2 5 6 cellular automata ca cellular automata is a simple yet powerful modeling method developed by ulam and von neumann in the 1940s a ca model is composed of cells with finite and discrete states located in a regular lattice space e g a square grid the state of each cell is updated at each discrete time step based on rules taking into account the state of the cell and its neighbors up to a certain distance this modeling method is especially suitable for spatial modeling where the landscape is represented as a grid of cells each cell described by a certain state that can change to one of other states depending on its current state and interactions with other cells this method is often used to model land use change veldkamp and fresco 1996 verburg et al 2006 batty et al 1999 parallel computations can be implemented by partitioning the lattice space into smaller spaces which allows one to model large landscapes and or at a detailed spatial resolution sun et al 2009 ca can also be used in conjunction with other types of models to bring a spatially explicit component in spatial versions of sd models local sd models are replicated over the grid of cells costanza and voinov 2003 when sd is involved the models usually turn out to be quite complicated and may require substantial computer power to run 2 5 7 agent based modeling abm similar to sd agent based modeling is a simulation method used to articulate system behavior and state changes over time instead of considering aggregates global variables representing whole entities populations amounts of water energy material etc abm aims at the system level and macro patterns that emerge from individual behavior of elements and interactions between them it is a bottom up process bonabeau 2002 the main elements of abm are called agents represented by attributes state location etc behavioral rules and interactions with other agents and with the environment some agents are able to take decisions based on certain rules or goals e g maximize profit and even learn and adjust their behavior adapt based on past experience and performance of other agents where cas are focused on landscapes and transitions abms focus on individual actions and behavior agents vary in their preferences and abilities to act on their environment as well as their ability to learn and adopt new practices spreading them via their social network abms are particularly well suited for representing complex spatial interactions under heterogeneous conditions and for modeling decentralized autonomous decision making parker et al 2003 zellner 2008 filatova et al 2013 they have been widely used to study socio ecological systems bousquet and le page 2004 schulze et al 2017 an 2012 examples https participatorymodeling org node 36 https participatorymodeling org node 37 https participatorymodeling org node 74 https participatorymodeling org node 75 abar et al 2017 provide an impressive list of abm tools available to support this method giabbanelli et al 2017 discuss a possible connection between abm and fcm methods 2 5 8 integrated modeling im integrated modeling is a way of building models by combining or coupling existing models used as components to represent complex systems laniak et al 2013 belete et al 2017 output from one model becomes input for another model since component models can come from different disciplines im is often seen as transdisciplinary exercise complex and powerful simulation models can be created by finding existing well tested modules and plugging them together to represent the systems of interest with properly documented models and with appropriate user friendly interfaces this could potentially be done on the fly with stakeholder participation example https participatorymodeling org node 90 im tends to produce quite complex models which may be hard to communicate to stakeholders fast integrated systems modelling and metamodeling try to integrate and simplify interactions and relevant feedbacks among complex systems haasnoot et al 2014 basco carrera and mendoza 2017 metamodels are models of models intended to mimic the behaviour of complex models see e g davis and bigelow 2003 walker and van daalen 2013 creating meta models and other fast integrated models normally requires pre running the complex models saving their output under various combinations of parameters and then using the output instead of running the actual models such models are also known as low resolution models repro models or fast and simple models basco carrera and mendoza 2017 describe a fast integrated systems modeling approach as part of collaborative prototyping example https participatorymodeling org node 119 integrated models may use something as widely available as excel as a front end or more sophisticated tools such as python or pc raster depending on the needs of the process resolution in time space and system processes to be included 3 selecting appropriate methods as summarized above there are a large number of methods and tools that can and have been used in pm processes yet it is difficult to identify the best strategy for deciding on what methods and tools and or combinations thereof are most appropriate for a particular pm project what makes these decisions especially difficult is that as previously mentioned there are hardly any reported cases where more than one method has been tried for the same problem within the same project combining several methods is quite common but replacing one method for another is not in operational research the mixing of methods has been viewed as a positive trend howick and ackermann 2011 have produced an extensive review of papers on mixing methods but were not able to produce any general recommendations on what methods to mix and how the selection and mixing of methods and tools is a decision making process on its own see ormerod 1997 that clearly one expects would be driven by the specifics of the problems being addressed however current practice reported in published literature tells a different story rarely is there much justification provided for the methods used either individually or in combination this section offers support for researchers as they consider and select methods and tools the first subsection reviews three pm case studies with a focus on the methods selected for each the next subsection describes some problem characteristics that should be taken into account when selecting pm methods followed by the results of a survey of modelers engaged in pm that explored their perceptions of the pm methods described in section 2 we end the section with some recommendations on the process and criteria for selecting methods and tools 3 1 methods used in case studies how are methods chosen in real pm case studies this section describes three studies identifying the methods and tools used in each and explaining the rationale for their selection n two of the three examples presented there was not much discussion about methods used they were determined by the modelers in the indian case study however the stakeholders moved from one method to another one choosing what worked best for them at each stage this was the one project that was not funded by any major external donors it was implemented largely through volunteer efforts of the participants 3 1 1 modeling the causes consequences and solutions of the flint water crisis residents of flint michigan usa experienced a serious compromise in their water quality beginning with an emergency manager s decision to switch the city s water source to the flint river in 2014 by 2016 thousands of flint residents had been exposed to unsafe levels of lead in their drinking water and the governor of michigan had declared a state of emergency center for disease control and prevention 2016 a modeling team from michigan state university was asked by the community foundation of greater flint to conduct a modeling exercise to capture the voices and views of flint residents around the causes and consequences of and potential solutions to the flint water crisis hereafter referred to as the water crisis gray et al 2017 singer et al 2017 the goal of this exercise was to represent flint resident views in a manner that could be communicated to city leadership and to the state appointed team in charge of developing a response to the water crisis the timeline for this exercise was very short community partners wanted a modeling product within a few months in order for the results to be timely and relevant to the water crisis response this short timeline effectively ruled out a simulation modeling approach which would have taken significantly longer given the goals of the exercise it was important to select a tool which could easily capture and synthesize flint residents views and knowledge about the systemic nature of the water crisis and which could represent those views in an easily understandable format the fuzzy cognitive mapping fcm method was selected implemented with the mental modeler gray et al 2013 online tool fig 2 during the spring of 2016 a series of four mental modeling workshops was conducted throughout the city of flint attended by a total of 36 residents a screen projecting the mental modeler software was displayed in front of the workshop participants and a facilitator led the group through a discussion of the causes consequences and solutions of the water crisis posed as broad questions concepts and relationships between concepts were suggested by workshop participants and captured by the facilitator for all to see when necessary the facilitator posed clarification questions for discussion around the meaning of specific concepts or the nature of the relationships between concepts after the final workshop the modeling team aggregated the four models developed by residents and shared the aggregate model as well as the similarities and differences between the community models with flint residents in a final workshop the results of the modeling process were shared widely through public meetings and an online report when reflecting on the modeling experience flint residents and flint based members of the research team expressed satisfaction with the modeling exercise in meeting its goals as a tool to communicate community views to the officials responsible for responding to the water crisis residents appreciated the opportunity to see their views and experiences reflected in a modeling product workshop participants also liked mental modeler s ability to run scenarios examining how changing one variable would affect other variables in the model several participants expressed interest in using mental modeler to address other community problems and a desire to be trained in the software thus the selection of a method with a software tool that s relatively accessible to non modelers was an appropriate choice a few flint residents did express a desire to see the modeling results integrated with spatial information about lead exposure hanna attisha et al 2016 a lack of spatial capability is a weakness of the fcm method 3 1 2 indian groundwater crisis in a village of about 240 households in southern india a major problem in the recent past has been drought and over extraction of groundwater a typical tragedy of the commons problem over several years local leaders have used various informal tools to better understand the causes of the problem and come up with sustainable solutions they organized and facilitated village meetings i e focus groups field visits i e transect walks and individual discussions i e semi structured individual interviews as spatial information was increasingly needed to better visualize and understand the problem villagers and leaders began using handheld gps units to plot the locations of their farms and wells on paper they then made the process less time consuming and less error prone by moving to public participation gis pp gis tools kolagani and ramu 2017 example https www participatorymodeling org node 45 with the help of their school going children who were in turn assisted by their computer literate teachers fig 3 they started showing the maps to other stakeholders in an effort to come up with potential solutions fig 4 emphasis was placed on the use of simple calculations and visualizations to plan and implement rainwater harvesting rwh systems kolagani et al 2015 and other solutions that were found most appropriate to the situation with some extensions to capture temporal dynamics they were able to undertake participatory water accounting gray et al 2018 example https www participatorymodeling org node 38 this helped them understand that increasing water recharge through rwh systems was only a short term solution and that rwh alone would not solve the problems unless water discharge was also controlled however discharge regulation is a socially and politically sensitive issue that requires buy in and voluntary participation from all stakeholders the leaders then felt that quantitative what if scenario analysis would greatly help convince the stakeholders of the need for discharge regulations it is difficult to do what if scenario analysis with pp gis however and project leaders looked for more formal yet simple quantitative models that could predict the future at least in the short term while remaining themselves in full control of the process they looked for a pm method that was easy to use yet had the power to answer their questions they considered sd abm bn and fcm approaches with the help of researchers from a nearby academic institute they recently started using sd as they became comfortable with quantitative models they also started looking at abm using rules they developed from their collective experience initial results seem to point to the need for a hybrid approach an easy to use platform that facilitates such learning and use of different modelling methodologies might really help such innovations in participatory modelling efforts especially by making stakeholders take ownership of and extend the models themselves 3 1 3 territorial transformations in the amazon in the amazonian floodplains of brazil i e várzea forests climate change is disrupting the frequencies and magnitudes of floods leading to great uncertainties for local populations a project focused on the flooding of curuaí big lake a territory with 30 000 inhabitants spread over 133 communities in the para state investigated how these populations were adapting their production systems to changes in flooding a multidisciplinary research team collaborated with feagle 2 2 http governancaflorestal iieb org br manejos view 10 a civil organization in charge of monitoring agrarian reforms by granting deforestation permits and hunting and fishing licenses today regional conflicts and pressures from mining and timber extraction companies and the complex landuse situation threaten feagle s existence and the future of the small scale farmers in the area given the vulnerability expressed by the social actors the research team first studied their concerns and strategies through field visits and semi structured individual interviews in several communities and then collectively discussed most probable scenarios to better understand ongoing dynamics regarding landuse activities on medium term futures a rpg was collectively designed with students of a rural school around 30 future farmers the rpg was organized as a board game that roughly displayed four communities on a transect from the lake to the forest in this game all players managed their respective farms according to their own strategies within the constraints of the game considering issues such as labor money land cover livestock and so forth by observing how players act in the game all participants were able to better understand how people behave in real situations the fun aspect of the game was also fundamental in freeing up conversations the rpg debriefings enabled rich exchanges with farmers and fishermen especially about the various constraints in the region they spontaneously addressed the impact of their activities on natural resources even if they were not always capable of explaining causal relationships the main drawback of this game was its slowness half a day of play was needed to simulate 4 to 5 years consequently an abm was implmented to better formalize the relationship between human activities and environment and to increase the time horizons of the sessions le page et al 2014 any of the 16 agents representing 4 smallholders from 4 communities could be controlled by participants while the not under control agents performed computerized decision making algorithms both types of agents simultaneously made seasonal decisions on agriculture fishing and animal husbandry while the computer also simulated biophysical processes by integrating their activities the abm was built as a continuation of the rpg based on the structure of the game validated by the actors it also seeks to specify the impacts of the activities based on research data moving from rpg to abm enabled sophisticated calculations and scenarios on a broader timeframe this allowed improved visualization and understanding of pasture degradation and dwindling fish stocks fig 5 through collective analysis resulting from the sessions socio economic and demographic changes were identified as additional factors along with climate change that contribute to water shortages and to the difficulties of addressing related issues for example without sewage treatment systems population growth could impact water quality and lead to the proliferation of cyanobacteria threatening human and animal health as well as fish stocks that are already under pressure from commercial fishing and non compliance with community fishing rules bommel et al 2016 3 2 problem characteristics from the examples above and from our experience running pm projects we identified some characteristics to take into account when deciding on particular methods to use for a given problem 3 2 1 nature of the problem the nature of the problem is critical for example a problem may focus on how to manage a common resource acute risks or slowly emerging risks it may involve tradeoffs between conservation and economic development or it may relate to environmental protection issues researchers considering the nature of the problem at hand may need to think about some of the following issues the range of domain specific expertise needed for the pm study for example expertise needed in such fields as public health natural resource management business organization and others the spatial and temporal scales of the system studied and whether and how specific methods and tools used will be suitable given those spatial and temporal dimensions the characteristics of the boundaries of the system relative to its processes stocks and flows isolated closed or open the structure of the problem or issue and its degree of wickedness the characterization of the level and types of uncertainties involved 3 2 2 nature of community engagement the participatory process largely relies on who is playing a role in decision making therefore it is important to understand not only who is participating but also how they are participating this depends on one s capacity to participate and both realized and perceived divisions of power not all tools are appropriate for all user groups by considering the nature of the community stakeholders and practitioners can choose the methods and tools that will be most effective researchers and participants may want to consider factors such as the number diversity background and skills of the stakeholders and also their social political positions and roles education age sex etc level and intensity of stakeholder participation low participation groups those on the lower end of arnstein s participation ladder i e ignorance awareness or information may benefit from an entirely different set of methods or tools than those on the higher end lynam et al 2007 of the participation ladder e g consultation discussion co design co decision making timing and stages of participation some stakeholders may desire to participate in all stages of the pm process others may want to focus on specific areas or may have skills and information that makes their input for specific pm stages particularly useful interaction context it is important to understand the level of cooperation or conflict among stakeholders it affects the facilitation approaches and techniques that might be used and possibly the selection of modeling methods power asymmetries where significant power asymmetries exist the choice of process orchestration methods and the composition of the modeling and facilitation team may strongly influence which groups have stronger or weaker voices in the pm process the team may need to take special care to assure that specific values or beliefs are recognized power asymmetries may also complicate the question of how much transparency the methods and tools should promote power asymmetries have both methodological and ethical implications the question arises as to whether some powerful actors whose actions contribute significantly to the evolution of a socio ecological system should be included in the pm process or not in some situations social violence is part of everyday life and extreme pressure is often exerted on the weak by inviting powerful actors to pm workshops there is a risk of inciting more of this social violence this issue creates a dilemma claiming a neutral posture by inviting all relevant people may further strengthen the most powerful actors while adopting a non neutral posture to empower the weakest actors may compromise the legitimacy of the process barnaud and van paassen 2013 the proper balance should be taken into account in choosing facilitation approaches although it may be difficult to translate these issues into selection criteria for other methods in some cases the level of power may correlate with level of education which directly correlates with what tools are more likely to be understood and used effectively by given constituencies stakeholder preferences and constraints can be driven by existing skills and experiences in particular methods or by the availability of training for newly suggested methods smajgl 2015 this depends on the level of co implementation e g model co construction and the expectation that methods that result from the pm process will continue to be used in these cases stakeholders may often consider the complementarity of the new proposed methods with the models and instruments they had been previously using stakeholders rarely advocate for the replacement of existing capacity stakeholders are also often concerned about the maintenance of methods e g requirements for model re parameterisations if they are to be used beyond the near term pm study 3 2 3 desired results the goals of the pm process also have a strong influence on the methods and tools that should be used for example a focus primarily on building trust and understanding among stakeholders may rely more on qualitative tools and on process orchestration in contrast a focus on helping a small group of decision makers solve a particular problem may require use of more sophisticated quantitative modeling approaches questions that can be asked include what is the intent and prospective use of the modeling process and model outputs will the pm process help to make a decision build trust understand spatial distribution temporal dynamics and causality relationships or is there another intent is descriptive analysis sufficient or are prediction and scenario analyses also expected is the goal only to promulgate or achieve greater system understanding or is there a desire to create forecasts or produce quantitative estimates is the description of trends sufficient or is the elucidation of actual process dynamics needed what are specific social objectives for the pm process e g decision making collaborative learning shared or social learning mediation model improvement what are the political or governance types of actions that may result from the process e g unilateral action coordination collaboration joint action 3 2 4 resources available the final critical factor to be considered in selecting methods and tools relates to the type and amount of available resources resources required include time and money people skills information and data these resources are often limited so method selection should be informed by considering types quantity and quality of available data and the time and expertise required for any additional fact finding available analytical tools platforms and visualization communication tools that can be used by the full project team human resources and expertise the types and levels of expertise among project participants and how their expertise aligns with their desired level of participation timing the length of time needed for various approaches or methodologies varies greatly depending also on the level and expertise of participants considering when results from the project are expected and when they will have the most impact may suggest which methods have the greatest chance of success to meet the needs of the project and its stakeholders financial resources available to support process orchestration meetings and workshops model implementation and consensus building around possible outcomes or recommendations the importance of these factors was confirmed through the survey described in the next section 3 3 survey of pm practitioners to understand how pm researchers typically select methods we administered an online survey of pm practitioners the survey included four main parts that elicited 1 experience in the use of different methods participants indicated their level of experience with each of the 23 methods shown in fig 1 using a 3 point scale not experienced somewhat experienced or very experienced 2 most preferred modeling methods participants were provided with a list of the 18 methods from the modeling portions of fig 1 and asked to indicate their first second and third most preferred methods 3 ways of selecting different modeling methods participants were asked to rate their level of agreement on a 1 to 5 scale with each of the following statements regarding their choice of methods in their past pm projects i used the method s with which i am most familiar i explored all options and then chose the methods that were most appropriate even if i was not experienced with them i started with the method that i thought was most appropriate if i found that it did not work then i switched to another one trial and error approach i have only chosen to work on projects that can be addressed with the methods i already know i typically selected methods based on the nature of the problem at hand i typically selected methods based on the nature of the community involved in the project 4 the importance of different factors that may influence the selection of methods participants were given a list of factors derived from the discussion above and were asked to rate the importance of each factor to them when they select a method using a 1 to 5 likert type scale from a strongly agree to a strongly disagree 3 3 1 survey administration survey responses were solicited through a listserv to more than 1000 individuals included on a mailing list from the innovations in collaborative modelling group as well as through convenience sampling to various colleagues and collaborators of the authors to ensure the respondents were professionals in the field of pm the first question included the following statement we define participatory modeling or collaborative modeling as a purposeful learning process for action that engages the implicit and explicit knowledge of stakeholders to create formalized and shared representation s of reality have you participated as a modeler or researcher in a project that involved participatory modeling if respondents indicated no to this question they were excluded from taking the survey in total 93 respondents identified themselves as having experience in pm and 84 completed the entire survey for which results are shown 3 3 2 survey results and discussion on the subject of experience with different tools survey responses indicated some unevenness across the different methods fig 6 while it could be expected that some of the fact finding methods such as surveys and interviews are quite well known and similarly for some of the facilitation methods such as brainstorming it was surprising to find that system dynamics was as well known as interviews and better known than causal loop diagrams cld which in a way are embedded in system dynamics overall our results indicated that respondents were knowledgeable of a diversity of methods the preference for methods used fig 7 aligned with responses regarding experience with the methods except for some cases for example many respondents 60 indicated that they were experienced in cost benefit analysis but just a few 5 preferred that method system dynamics was the most preferred method 26 followed by cld 22 and then scenario building 20 and these were also methods with which many respondents had experience for the most preferred method we also asked respondents to specify the strengths and weaknesses of the method in the context of pm these comments have been taken into account in describing the methods in section 2 above interesting results emerged from questions about how participants chose methods for their recent pm projects fig 8 on the one hand respondents clearly admit that they choose methods that they are most familiar with 92 totally agree or strongly agree with this statement at the same time 60 claim that they choose the most appropriate methods this suggests that perhaps researchers choose the projects that where their methods expertise is the most appropriate choice yet only 35 of our participants said that was a factor indeed the vast majority say that they choose methods based on the problem characteristics 87 and on the nature of the community involved 73 these responses are difficult to reconcile when asked to rate factors in terms of how important each is when selecting a method all the identified factors were considered important time money and level of stakeholder involvement required as well as the availability of data had the highest importance fig 9 skills and education of stakeholders were of lower importance in the survey though still important overall the survey suggests that practitioners consider many things when selecting methods but that they do not necessarily have a clear hierarchy of criteria or approach for choosing those methods one interpretation of the results in figs 6 and 7 is that our respondents are guilty of a hammer and nail interpretation where they simply believe the tools they know best are the most appropriate and may be imposing their favorite tools on the stakeholders involved another interpretation is that our particular respondents were quite knowledgeable about the various methods of pm out of the 23 methods listed all respondents indicated they were experienced in at least 5 one claimed experience in all 23 and on average respondents were very or somewhat experienced with 14 methods this level of experience might allow them to choose both methods they are familiar with and those that are most appropriate to the problem they may also tend to choose projects where their expertise is most relevant 3 4 some recommendations we differentiate between three types of method selection the expert approach in which modelers choose or recommend the methods and tools to be used their recommendation is likely to be strongly influenced by the methods and tools with which they are most familiar and most comfortable with among those applicable to a problem while there may be nothing wrong with this approach there is always a risk that more appropriate methods exist that the expert is less comfortable with that would garner more effective participation in the process the experimental approach in which the stakeholders decide to experiment with new research methods or explore the applicability of existing methods in the project this may be also driven by the modeler usually an academic with a research agenda wishing to learn how new methods work in new applications using the project as a testbed this trial and error approach has the potential to create new insights but may be costly both in terms of time and resources needed and has to be well explained so as not to undermine stakeholder trust in the pm process the participatory approach in which all stakeholders including modelers take part in the process of selecting methods typically this requires extensive engagement between all stakeholders at the very beginning of a project so that all stakeholders can make an educated choice some stakeholders may also advocate for the use of a method that may not necessarily be the best for the task at hand we propose a critical approach that incorporates elements of all three types the experts can start with a systematic comparative analysis to identify and reflect on the merits and weaknesses of various methods and their applicability to the multiple dimensions of the problem they seek to address the other stakeholders are engaged early in the pm process to learn about the existing alternatives evaluate the options and decide as a group on what methods to use experimentation with methods should be encouraged whenever sufficient time and funding are available there is still the problem of power and knowledge asymmetries that put experts in a favorable dominant position their opinion may be hard to contest for less educated and less prepared stakeholders we have started a web portal https www participatorymodeling org as one attempt to facilitate access to knowledge about methods and their past use and also to promote further collaboration and communication about pm much of the information about methods presented in this paper is available on the web portal the site is a content management system that uses the drupal open source platform and allows the users to enter and upload all sorts of information we encourage anybody practicing pm or interested in pm to share their experience case studies information and skills to facilitate access to methods and knowledge in particular domains we are also collecting a listing of experts and provide a collection of other resources such as models videos papers etc there are several ways to guide the selection process a decision tree may be the most straightforward and easy to follow selection technique one previous attempt to apply such an approach to model selection was conducted by kelly et al 2013 however for pm we failed to identify an appropriate set sequence of decision points that could enable building a decision tree the process always appeared more nuanced and too many considerations had to be simultaneously taken into account mingers 2000 developed a questionnaire to help researchers think about selecting and designing multi method processes that involve the use of participatory and numerical analytical models questions are grouped into the following categories 1 questions related to the relations between the researcher and the candidate methods e g experience and skills 2 questions asking about the relations between the researcher and the problem characteristics e g interactions with stakeholders and 3 questions connecting the problem situation and the intellectual resources such as the suitability of methods to the organizational or situational culture these factors are similar to the list described in section 3 2 above that list and the mingers questionnaire provide useful guidance to help researchers think about what factors they need to take into consideration however they do not provide practical insights into how these selection methods perform we need a meta tool that supports both the selection criteria and the nuanced judgments possible for each criterion our method selection tables rank each of the pm methods shown in fig 1 first against a set of desired model characteristics table 1 and second against a set of resources required for implementation table 2 this is somewhat similar to the approach that the rand corporation suggests when choosing models for infection disease prevention manheim et al 2016 the goals of a pm study should offer the starting point for any discussion about the pm methods needed these goals may be positioned on two extreme ends of a continuum at one end some studies are designed to highlight knowledge diversity to make different voices in the community heard and to understand sources of conflict in these studies the models themselves are mainly boundary objects for communication of different worldviews they do not have to be scientifically accepted representations of the real world systems and they may not be consolidated into a single model participants are chosen based on a desire and need for comprehensive representation diversity of perspectives and to maximize engagement and understanding at this end of the continuum ease of communication and interpretation might be the most important factors to consider in selecting methods in contrast at the other end of the continuum some studies aim to pool expert knowledge about a system and to create a model that allows predictions to be made and that supports detailed exploration of the implications of different decisions or actions the desired output of this kind of study may be a quantitative model in of itself that is validated against empirical data and expert knowledge accordingly these studies are very concerned with the modeling process e g expert selection strategies for model validation and emphasize consolidation and aggregation fig 10 is a generalized version of fig 1 and positions different groups of modeling methods on this continuum selecting methods for a participatory modeling exercise should be a flexible process it may even involve the invention of new methods and tools if existing modeling approaches are not appropriate throughout the selection process stakeholders including modelers are typically engaged in asking the following questions sometimes repeating these questions at multiple stages of the modeling process 1 does this problem require detailed spatial or geographic information to solve at what scale or resolution are there spatial interactions to consider 2 is there a need to project current system conditions into the future in order to make or improve decision making how far into the future do we need to look and how precisely what level of uncertainty are we comfortable with are there temporal interactions to consider 3 what is the goal of the participatory modeling process is it community engagement community organization solution building planning implementation 4 how much time and how many people and resources can be devoted to the modeling effort what skills do the people have 5 how much does the community know about the problem being modeled do scientific data need to be collected and integrated into the model to help answer the community s questions what data are available kind quantity quality scale resolution what interactions with the modeling tools will different stakeholders want to have will they want to be consumers of outputs users and scenario testers developers what capacity building is needed for the different stakeholders to facilitate the pm process interactions they aspire to 6 what capacity does the modeling team have to build or use appropriate modeling tools do other modeling or scientific experts need to be brought in and is there sufficient time and financial resources for this what is the ability to continue using the particular method as output of the pm project after project completion how alternative methods can be linked to existing and already used data and methods answering these questions and using tables 1 and 2 the stakeholders should be able to evaluate the various methods available and choose one or a combination of methods that will be most appropriate for a given problem or situation subjectivity will always remain in how stakeholders treat these questions which is why it is hard to expect that the choice of methods will be always optimal in fact there is probably no optimal choice however by designing the process as open ended and adaptive the project team could ensure ongoing evaluation of process outcomes and methods used and change direction when deemed necessary 3 5 further considerations for method selection clearly the evaluation of appropriateness of pm methods entails multiple criteria which can be summarized as follow effectiveness how well can a specific method succeed given the focal problem of interest and how well it meets the goals of the pm team and the needs of the pm processes efficiency whether the methods can achieve the pm goals in the needed time and with the appropriate use of the available human financial and technical resources social value added how well the methods support the broader goals of the pm process such as promoting gender racial and income equality learning and education dialogue among diverse groups and social capital of stakeholders in line with the social network development mentioned below evaluation of the usefulness of the method used usually occurs only at the end of the project when time and money are most likely running out and when participants are fully invested in what they have done this only makes reporting of failures in addition to successes even more essential so that everyone can learn from mistakes or problematic choices it also explains the rarity of such reports if instead we evaluate the appropriateness of the methods early and throughout the pm process it is more likely that the project can adapt and change course if needed some methods may offer additional benefits that go beyond questions of effectiveness efficiency and social value for example to the extent that participants can really engage i e lose themselves in a gaming modeling or simulation process some conceptual modeling and qualitative or numerical simulation tools can help the decontextualisation process and potentially reduce conflict modeling and simulation tools and processes that are flexible enough to effectively create abstraction while fully engaging participants may be of great interest the preferred approach may be a combination of methods and tools so considering the compatibility of different methods to work together in the pm process is desirable for example there is theoretical and empirical support for combining mapping and sd modelling companion modeling involves the combined usually sequential use of both rpg s and abm where rpg is used to first create an engaging abstraction that can then foster more complex participant understanding and engagement in the use of abm social network analysis can be used in conjunction with cognitive and or behavioral measures as a means to assess how stakeholders views beliefs and practices co evolve with the relationships formed via the ongoing pm process doing so would involve measuring stakeholders ties with one another at various points in the pm process and collecting at the same points in time relevant cognitive and behavioral data such a combined tool kit could thus readily be used as means to evaluate the extent to which 1 the pm process has resulted in creating strengthening or improving stakeholder relations and 2 the process has created channels through which stakeholders can mutually influence and learn from one another the subsections below discuss some issues related to combining or using multiple methods and tools 3 5 1 interfacing qualitative and quantitative methods following the voinov and bousquet 2010 diagram we can point at another version of the generic framework in which the two big leaps in the process are stressed a first leap happens in the move from the conceptual qualitative phase of model development to the quantitative phase of model formalization and computer runs a second leap happens in the move back from quantitative analysis to qualitative interpretations for example in the simplified visualizations and communications that are essential for the delivery of model results and for their translation into policy and actions fig 11 this also somewhat resembles the modeling ladder in fig 10 we go from data and concepts and gradually attempt to make sense of them through various forms of reasoning and analyses bridging the gaps between qualitative and quantitative modeling remains a challenge and often disrupts the pm process quantitative models especially when they become quite complex are often built behind the scenes by experts and later on are reintroduced to the rest of the group a smoother transition between qualitative and quantitative phases is much needed but is yet to be achieved 3 5 2 interactive modelling for true engagement of stakeholders in the modeling process it is essential that models be transparent and easily modified and tuned according to the needs of the stakeholders they should be able to interact and make direct changes in the models as they use them and see the results of their changes almost instantly such direct interaction facilitates stakeholder understanding of complex physical processes this does not mean that for all cases the whole pm framework has to be applied and implemented usually only some parts may be exposed to stakeholders while there are many benefits of staging a full complete participatory modeling process in reality there often exists restrictions of time resources and needs that make it necessary to limit the pm approach to a partial implementation even then there is a lot that can be derived from the connection between models and stakeholders in the process following this perspective several modeling tools are popular in the pm community for their ease of use with interactive groups for example cormas an abm tool dedicated to natural resources management has been used for collective design of models and interactive simulation bommel et al 2016 here users can interact with a simulation by manipulating avatars the agents that represent them in the simulated word changing their environment and their behavior by modifying simple activity diagrams thus it is possible to collectively explore medium and long term scenarios to better understand how a desired situation may be reached similar functionality explains the success of stella an sd tool actively used by the mediated modeling approach van den belt 2004 more recently the use of direct dynamic visualizations of a system often in the form of interactive submersive graphics i e virtual reality mixed reality and 3d environments 3 3 some examples include interactive sandbox https www deltares nl en news interactive sandbox new interactive design tool for the direct visualisation of coastal interventions 3di water model http www 3di nu en international or flexible mesh https www deltares nl en software 3d interactive modelling using delft3d flexible mesh is an emerging technological trend that facilitates understanding of model outputs and stimulates stakeholder engagement in modifying and refining the models basco carrera et al 2017b example https www participatorymodeling org node 117 here also there are certain restrictions on the realism of the output generated virtual reality may overburden stakeholders with information and hide the important message of the modeling process voinov et al 2017 3 5 3 user interfaces better understanding of the interconnections among the social behavioral and material elements involved in the participatory modelling activities and how these interconnections influence the participatory modelling process and outcomes is necessary to fully understand the effectiveness of different combinations of methods theoretical insights from relevant scientific fields including cognitive science and user psychology could identify the characteristics of modelling techniques that fit into a particular pm activity for example golnam et al 2012 used cognitive fit theory to explain the cognitive capabilities required for building sd models and to determine the suitability of various problem structuring and conceptualization techniques to fit to these capabilities herrera et al 2016 developed an experimental framework to compare the effectiveness of 1 a single method group modelling process and 2 a multi method process combining strategic options development and analysis with computer simulations at promoting changes in participants mental models and achieving better negotiation outcomes shelley et al 2010 2011 studied how different tangible and mobile interfaces can help stakeholders state their preferences and values collectively design scenarios and make sense of simulation outputs and deliberate towards compromises there is much improvement still needed in the modeling interfaces that could make model formulation running testing and communication simpler for non expert stakeholders 4 conclusions there is much improvement yet to be made in how modeling methods are selected for pm projects there are many methods already available and choices are not simple in too many cases the selection process seems to be largely driven by the past experience of participants rather than by the particular needs of the project while logic tells us that this is probably not the best strategy we do not have much if any evidence that this is a bad thing to a large extent this is because there are almost no method comparisons documented for pm projects i e where one method was substituted by another and where results were meaningfully compared comparing across projects is difficult because each project is unique while the problems may be similar the stakeholders involved are always special and group dynamics are hard to reproduce there is also much subjectivity in how stakeholders perceive the outcomes of a pm process and how those processes are evaluated what may be a success for one stakeholder group may turn out to be a failure for another there are too many biases beliefs and values involved in any kind of comparison and evaluation done by stakeholder groups to assume that the evaluation is correct and universal post audits and other tools to assess longer term outcomes of pm driven actions decisions and provide feedbacks i e pm as a part of adaptive management and governance could certainly help but are still very rare the challenge here is that there are many confounding factors influencing the pm outcomes tied to the individual and collective characteristics and relationships within the project teams and these change over time as participants interact with each other and with the modeling approaches and tools at best we can attempt quasi experimental setups but controlled experiments are not possible despite this limitation evaluation of each pm study should become standard practice so that a body of knowledge can be built to inform new applications while all stages in the pm process assume possible iteration method and tool selection is crucial because there may be insufficient time or resources available to go back and do the whole pm process once again with another method the modeling method chosen depends upon but can also determine the types of data to be collected and processed while the hammer and nail syndrome always has a negative connotation past experience certainly matters and indeed it may not be bad to use a method that someone is most comfortable with besides as shown here there is often considerable overlap between some methods this only makes it harder to come up with a unique optimal choice with no strict rules or guidelines for method selection more consultation and access to past and present experience could help an inventory and systematic analysis of methods and tools can also provide a stronger basis for model selection and can narrow the array of choices we hope that the web portal introduced above https participatorymodeling org will engage pm practitioners enlist them in adding their knowledge and experiences to the web portal and will generally serve the community of practice interested in the development of better techniques for participatory collaborative modeling the web portal is a community led endeavor and we expect the users to identify what features and functions it should provide we will also need community suggestions regarding how to best manage and moderate the portal for example we would like to ensure that additional openness in project reporting and potentially ensuing critiques from the pm community do not discourage stakeholder groups from presenting their results and methods for public scrutiny and evaluation a good pm project should be open minded about the particular type of modeling methods to be employed itis often problematic to justify this to funders who usually want to know in advance everything about what will be done accomplished and how the pm process will unfold it takes a considerable amount of trust from funding agencies and clients to devote resources to processes that are vaguely defined and or to rely on the project team to provide or add expertise as a problem requires prell et al 2007 if it is harder to get funding for a multi method project such projects are likely to remain rare offering little help in convincing the funders about the benefits of adaptive modeling approaches it also may sometimes be difficult for funders and publication authors and editors to recognize that documenting pm failures is as important as documenting pm successes and indeed may be more important in advancing pm knowledge and best practices this is one more reason for the pm community to organize itself to recognize the advantages and disadvantages of the rich array of pm tools and methods that are already available to improve and innovate as needed and to educate itself as well as its funders and stakeholders on the factors that should be considered in the selection of pm methods and tools we hope that this paper and our creation of the https participatorymodeling org web site will help in this regard acknowledgements this work was supported by the national socio environmental synthesis center sesync under funding received from the national science foundation dbi 1052875 any use of trade product or firm names in this publication is for descriptive purposes only and does not imply endorsement by the u s government the authors thank toni lyn morelli bruce taggart and two other anonymous reviewers for helpful comments appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 08 028 
26317,various tools and methods are used in participatory modelling at different stages of the process and for different purposes the diversity of tools and methods can create challenges for stakeholders and modelers when selecting the ones most appropriate for their projects we offer a systematic overview assessment and categorization of methods to assist modelers and stakeholders with their choices and decisions most available literature provides little justification or information on the reasons for the use of particular methods or tools in a given study in most of the cases it seems that the prior experience and skills of the modelers had a dominant effect on the selection of the methods used while we have not found any real evidence of this approach being wrong we do think that putting more thought into the method selection process and choosing the most appropriate method for the project can produce better results based on expert opinion and a survey of modelers engaged in participatory processes we offer practical guidelines to improve decisions about method selection at different stages of the participatory modeling process keywords stakeholders collaborative learning qualitative analysis quantitative modeling participatory planning mental models 1 introduction numerous tools and methods facilitate stakeholder engagement in participatory modeling pm which stave 2010 defined broadly as an approach for including a broad group of stakeholders in the process of formal decision analysis in the pm process participants co formulate a problem and use modeling to describe the problem to identify develop and test solutions and to inform the decision making and actions of the group therefore we define pm specifically as a purposeful learning process for action that engages the implicit and explicit knowledge of stakeholders to create formalized and shared representations of reality since pm is heavily focused on collaborative learning the tools and methods used during pm projects are expected to promote system understanding and awareness for all stakeholders by stakeholders we mean all who have a stake in the project this includes modelers and researchers themselves who are often considered external to the project but still have interests in it come with their own biases and cannot be assumed totally objective and neutral voinov et al 2014 the level of engagement differs across stakeholders and varies from one stage of the project to another arnstein 1969 hurlbert and gupta 2015 reed et al 2009 argyris and schön 2002 showed that there are two levels of learning referred to as single loop and double loop learning in single loop learning individuals and groups act within a single reference frame where specific hypotheses values norms beliefs and objectives are assumed to describe the world learning in these systems consists of observing the results of actions and potentially modifying future actions based on what is observed in double loop learning actors question and learn about the reference frame itself and may change their fundamental hypotheses values norms and beliefs based on what they learn about the system as well as what they learn about the outcomes of specific actions zellner and campbell 2015 the transition between single and double loop learning can result from the interaction between individual and organizational learning argyris and schön 2002 found complex retrospection and feedback mechanisms between individual and organizational learning the individual mental models that are used to construct shared mental models of an organization coalesce thereby modifying the perception of the organization and transforming organizational values and paradigms in turn this modifies the environment of the individuals and affects their own mental models daré et al 2014 as a result the act of model co creation is in itself an act of knowledge construction at both the single and double loop learning levels in some cases pm processes deliberately avoid formal model co creation to first allow the identification and challenging of stakeholders causal beliefs and expectations and consequently a reconstruction of knowledge habermas 1990 smajgl and ward 2013 the goal of this paper is to provide an overview of some of the methods and tools for pm identify some of their strengths and weaknesses and provide some guidance for practitioners as they select methods for their pm projects for the purposes of this paper we define a tool as a modeling technique used to carry out a particular function to achieve a specific goal tools are defined documented do not change significantly through use and are clearly external to their users and often not created by them in contrast a method is a way of doing something in particular a way of using tools according to mingers 2000 a method is a structured set of processes and activities that includes tools techniques and models that can be used in dealing with a problem or problem situation a particular method can be supported by one or several tools for example in this context agent based modeling abm is a method netlogo mason or repast are some of the possible open source tools used to perform abm multiple tools often exist to support a single method and some tools also serve several methods for example netlogo anylogic or numerus are tools that can be used within both abm and system dynamics sd methods while the choice of methods used can heavily impact both processes and decisions there is little scholarly discussion about how tools and methods are chosen during pm certainly decisions about methods are more influential for the whole process than the choice of a particular tool and therefore should come before choice of tools for instance there are not many implications in deciding to use stella rather than vensim or simile all are well established tools that support the sd method but the decision to implement a more quantitative method rather than a qualitative or conceptual one can potentially significantly change the outcome of a pm process for example a companion abm based on role playing games see barreteau et al 2001 can increase stakeholder involvement in the pm process and may generate much different results than computational abm using only computer simulations and modelers assumptions previously voinov et al 2016 reviewed several participatory tools and methods that have been used to enhance stakeholder participation for different components of the pm process they concluded that while many different methods are used for various stages of the process in practice there is rarely much justification given for the use of a particular method it is difficult to find examples of participatory projects that used different combinations of modeling methods when dealing with the same problem in most cases once the method or combination of methods is chosen it becomes the only one reported we recently reviewed 180 papers related to participatory modeling as part of a sesync project on synergizing public participation and participatory modeling methods for action oriented outcomes https www sesync org project enhancing socio environmental research education participatory modeling we found no papers that reported using one method and then a switch to another method this may be due to a general reluctance to report failures rather than only success stories but it complicates the comparison of different methods another reason most studies report only one method might be that switching from one method to another is costly in terms of time and resources a similar though much more limited effort in healthcare research which focused on comparison of three dynamic simulation methods sd abm and discrete event simulation marshall et al 2015 also reports very few failures of particular methods that led to switching to other methods a careful and conscious selection of methods is important for the modelling process and its outcomes ideally the selection would be accompanied by effective evaluations to monitor the impact of individual methods used during in pm hassenforder et al 2015 smajgl and ward 2015 however in many case studies the choice of methods and tools seems largely driven by the experiences of participating researchers prell et al 2007 this is a manifestation of the hammer and nail syndrome once someone learns to use a hammer everything starts to look like a nail a researcher with expertise in system dynamics is very likely to apply system dynamics for the next modeling project even if other methods could be equally or even more appropriate to address the full set of driving questions retraining is time consuming and resources are always scarce engaging colleagues with experience in alternative approaches could help expand the scope of methods considered but this is not always feasible there are practical and social reasons why this experience driven approach to method and tool selection is not optimal especially in the field of pm first the value of pm in developing models that effectively and efficiently meet participants requirements will be improved by using methods that best fit the project purpose and context the modeling skill set available should be considered only to identify gaps in the skills required to address the problem in question pm seeks to be transparent to the users and it is critical to make sure that pm practitioners are not treating all problems as nails just because they are good at using a hammer stakeholders defined broadly as above are expected to engage in all steps of the pm process which includes method selection as well as the modeling steps while the participation of various groups of stakeholders will certainly be different at each stage all stakeholders should understand why the chosen methods and tools are appropriate this requires some flexibility in the pm process whereby stakeholders move collectively from the problem to an appropriate method and onto tools and associated skills found within the project team a sharper focus on method and tool selection is needed this requires understanding stakeholders preferences and constraints including their experience with particular methods the availability of training for specific methodologies the ability to use and maintain a particular tool for the long term given the costs to do so and or the ability to combine a new tool with existing tools or methods smajgl 2015 second social factors may also affect method and tool selection the choice of methods is more than a technical decision it can also involve ethical or other social judgments it may make it easier or more difficult for specific groups to participate effectively and to adequately represent specific technical aspects of the problem in implementing a pm process decisions must be made about who is involved and what is included midgley 1995 tradeoffs between narrow technical accuracy and more inclusive participation in the modeling processes themselves may add more legitimacy to the process nabavi et al 2017 or help to level the playing field in the case of asymmetries in the power i e influence or control or knowledge of different stakeholders e g barnaud and van paassen 2013 campo et al 2010 when the choice of modeling methods and tools becomes largely a personal decision of certain more knowledgeable stakeholders it represents an ethical posture based on their own preferences and experiences and may not reflect the larger pm group methods and tools ought to be chosen in service to ethical or social needs in contrast method driven pm practice can result in methods that are epistemically violent to vulnerable participants they forcibly replace one structure of beliefs with another individuals must be invited to join the process but it is rarely possible to invite every individual who might be interested in the questions being addressed time and resource constraints as well as the need to have effective and useful interactions among the participants means that some individuals are necessarily excluded further because modeling often requires some element of rules or strategy guiding the approach prior to the decision making process certain participants may have greater power the choice of methods and tools can significantly empower some participants at the expense of others often these others may already be traditionally disenfranchised if the method chosen is one with which the project leaders have a lot of experience it might give them substantial advantage in understanding and controlling the process relative to other participants for whom the method is novel the confidence and knowledge they have make them more likely to guide the participatory process while subordinating the novices but using a method that the practitioner is not familiar with just to maintain equality of power would be also unrealistic and unproductive because inequality in power can manifest itself in many ways kraus 2014 it is important for a truly participatory process to have all individuals informed not only about the decisions being made but also about the decision making process itself ultimately the research team can be even assembled after stakeholders co designed the project and select the most effective methods based on the policy indicators and the scale they perceived as most relevant smajgl 2010 smajgl et al 2009 on the positive side methods can also empower and integrate many perspectives any of the methods and tools described in this paper may promote both individual and social learning through the use of the model as a boundary object a representation with a shared meaning that can facilitate exchanges of ideas and worldviews between participants schmitt olabisi et al 2014 johnson et al 2012 zellner 2008 a boundary object implies a distance from reality and situations that are sometimes tense and painful this distancing can allow for discussions on subjects that are conflictual or taboo by fitting into a social issue the model co designed with the stakeholders becomes an object of mediation d aquino et al 2002 2003 promoting conflict resolution and collective decision making when selecting methods for participatory modeling modelers and facilitators should consider how the methods or tools will provide evidence of learning for example a before and after systems diagram may reveal shifts in mental models that occur as the result of a pm exercise discourse analysis may demonstrate changes in the ways groups conceptualize problems and problem solve as the result of interaction with the model radinsky et al 2017 consideration of learning is therefore an integral part of method selection and process design in pm the selection of methods is both a critical and a difficult task that ideally requires 1 knowledge of available methods and tools and 2 careful examination of selection criteria and trade offs this paper addresses both of these issues section 2 describes a broad array of available methods and tools available to scientists modelers and stakeholders and section 3 systematically examines pm practice and the issue of method and tool selection 2 overview of pm methods there are numerous methods used in pm projects in fig 1 we propose a typology of methods and some possible combinations thereof it is sometimes difficult to distill the particular methods and tools used within the context of broader methodologies proposed for pm these methodologies tend to cover the whole process and assume a particular type or set of tools embedded within for example the soft systems methodology ssm checkland and holwell 1998 and the companion modeling commod approach bousquet et al 2002 barreteau et al 2003 etienne 2014 are two well known broader methodologies ssm uses a sequence of stages it 1 considers a problem 2 expresses the problem using rich pictures a freestyle mapping of the different elements that make up the problem e g using pictures and text to represent processes actors issues 3 develops conceptual models to represent possible actions to improve the situation 4 compares models to the real world 5 debates and identifies desirable and culturally feasible changes and 6 takes action to improve the situation the ssm approach may well cover the whole pm process but mentions only one particular method rich pictures the commod approach combines such methods as role playing games and abm to promote single and double loop learning for both individuals and groups for the first steps of the process concerning fact finding the approach involves stakeholders in the co design of a conceptual model of the system at stake using role playing games this sharing of representations is done by means of a series of collective workshops during which actors resources dynamics and interactions ardi are identified and clarified etienne et al 2011 these conceptual models are then implemented as abms and brought back to all stakeholders for further discussion and improvement in the following sections we identify and describe specific methods within each level of this typology these methods can be used separately or be combined within some of the more general methodologies such as ssm or commod described above here we view them as reusable components that can be reassembled in a variety of ways for future pm projects the methods and tools discussed below are commonly used in pm but do not constitute an exhaustive list 2 1 fact finding the fact finding stage s of pm focuses on finding generating and communicating data information and knowledge relevant to the problem being considered this stage may continue throughout or be revisited multiple times during the pm process in addition to standard research techniques that include literature searches and reviews typical approaches to fact finding specific for pm are described below 2 1 1 surveys and interviews surveys consist of a suite of questions they can be undertaken in person by phone on paper or electronically when surveys are conducted face to face they are usually called interviews these can be time consuming but offer the possibility to clarify the questions to gather additional valuable information not covered by the questionnaire and to follow up conversations to explore results virtual surveys collecting responses without presence of interviewer could potentially reach a larger number of people than in person surveys but suffer from a self selection bias and it can be difficult to know or understand who responds and how reflective of the interests of the full group those respondents are it is also useful to distinguish between structured surveys or interviews and semi structured interviews structured surveys or interviews use pre defined questions in a set order often with a closed response format i e respondents choose from a list of possible responses while this format limits the information that can be gathered results are easily quantified and are relatively straightforward to analyze semi structured interviews can include a mix of closed ended questions open ended questions i e respondents answer without choosing from a preset list of possible responses and discussion discussion may be directed based on a particular response to a closed ended question or may simply follow themes that arise from responses to the open ended questions the qualitative results of such surveys may be challenging to analyze and summarize for others but they may allow for deeper understanding of responses phone and paper based surveys are relatively common in a variety of contexts telephone interviews pose recruiting scheduling and response rate challenges paper based survey data are difficult to manage and process especially when sample size is large studies of the potential for using personal digital assistants pdas for gathering data electronically indicates that electronic surveys improve input data accuracy facilitate data management and allow for automated data processing lane et al 2006 onono et al 2011 ficek 2014 recently the increased availability of tablets and mobile phones especially of low end smartphones has helped overcome some of the barriers to electronic data collection tomlinson et al 2009 kolagani and ramu 2017 smartphones also permit collection of location and multimedia data photographs video and audio segments in addition to text and allow better visualization accuracy and analysis of the data several free and open source solutions such as open data kit odk https opendatakit org kobotoolbox http www kobotoolbox org and village gis http mspaceapps in villagegis help users customize these solutions to their needs and help collect analyze and manage their data still reliance on this kind of technology tool may disenfranchise the poor and less technologically facile groups of respondents moreover this approach is less efficient with open ended questions where users are expected to enter significant amounts of text 2 1 2 crowdsourcing crowdsourcing is another data acquisition method that is becoming increasingly popular it involves gathering data from a large number of people crowd including those unknown to the individual or organization gathering the data some main advantages are the relatively low cost to the data collector quick speed scalability and the diversity of participation and types of data that can be obtained however data obtained by crowdsourcing may be hard to reproduce and its quality may be difficult to validate the data may differ significantly from the judgment of the experts especially when more expertise is required sen et al 2015 another disadvantage from the point of view of pm is that crowdsourcing is usually used as a one way data collection method voinov et al 2016 it rarely gets used for higher levels on the participation ladder arnstein 1969 that expect greater stakeholder engagement individuals either affiliated with a particular project or not can volunteer to collect and provide data for example volunteered geographic information vgi goodchild 2007 is provided by individuals associated with a specific geographic region a prominent example of vgi is openstreetmap osm which has the goal of creating a free editable map of the world osm was designed to overcome restrictions that exist on map availability in many places it has produced spatial data of high quality comparable to that of proprietary spatial data for most parts of the world haklay 2010 alternatively data can be collected from social media derived from information provided by people even without knowing how it will be used for example van zanten et al 2016 estimated continental landscape values based on the social media data and usgs has the twitter earthquake detection data mining program which is used to help determine the intensity of earthquake energy felt by twitter users earle et al 2012 1 1 https earthquake usgs gov earthquakes ted https blog twitter com official en us a 2015 usgs twitter data earthquake detection html 2 2 process orchestration by definition pm is a process therefore its success depends on how well the process is organized managed monitored and reported process orchestration methods may overlap or combine with other methods note that in fig 1 process orchestration spans across all stages of pm facilitation for example is essential at multiple stages of a pm process the focus of the facilitation may change e g from understanding stakeholders ideas and data to visualizing results and making decisions but facilitation is still required three commonly used process orchestration approaches are described below but there are many others 2 2 1 facilitation facilitation is key to pm processes regardless of other methods and tools used facilitation and the analyses that support it come with their own set of techniques and tools for example capability and knowledge mapping can help determine who has specific skills and capacities that are needed and what knowledge gaps might be present they can help map out the distribution and intensity of expertise and knowledge jetter et al 2006 chapter 6 other techniques such as diagramming or the use of manipulatives e g dice can be used to help individuals express their ideas cards stickers or digital tools can also help facilitate and capture ideas facilitation and its tools must be carefully employed and focused on moving the pm process towards attaining its goals if mishandled the facilitation process can become a source of frustration and alienation multiple facilitators may be needed to offer different kinds of support for example a technician to facilitate with modeling tools and a community leader to facilitate interaction among participants hovmand 2014 the facilitation process must generally be open accessible and safe for honest discourse during a facilitated participatory process a good facilitator will strive to allow all participants to express themselves by trying to give everyone time to speak and express their points of view to encourage mutual learning and understanding and to help foster a collaborative environment it can be helpful for a facilitator to understand the background of the involved participants to guide their initial and continued interactions and ultimately their perceptions of the tools the model and the value of the process kaner 2007 an important aspect of facilitation in the pm process is the focus on modeling and the use of some of the structured modeling tools described below at some stages of the pm process the facilitator may need to understand the affordances and constraints of specific modeling methods tools and associated approaches the facilitator role extends to encouraging all participants to see others as legitimate and valuable contributors to the development and growth of the model and associated analyses and processes depending on the pm problem it may be important to consider and address cultural differences in how participants interact and differences in their willingness to enunciate or modify beliefs or be receptive to contradicting beliefs or values this links directly to methods such as cultural consensus described below good facilitation should also recognize the role of biases beliefs heuristics and values bbhv in the pm process according to glynn et al 2017 biases represent tendencies to believe in or pay attention to certain observations ideas or people consciously or unconsciously but with no good or testable reasons this may result in decisions or actions which may be hard to explain or expect heuristics are innately derived rules of thumb mental shortcuts or simplifications that help us navigate through the complexity of the world and its relationships kralik et al 2012 levine and perlovsky 2008 relatedly values are conceptions of the desirability undesirability or relative prioritization or importance of actions or things beliefs create 1 an acceptance or a conviction that something or some statement is true or real or 2 a trust faith or confidence in a set of values and attitudes in a tradition in a thing or concept in a tribe or in a person including oneself good facilitation should help participants in recognizing mitigating or otherwise modifying or shaping their bbhv to improve pm processes this involves some type of reframing personal or community questioning and learning about oneself or itself and training to both ease bbhv recognition and to create more effective and appropriate communication scientific ethics and integrity suggest that transparency and participant awareness are needed for bbhv elicitation and communication hill 2012 kelman 1982 cahill et al 2007 different cultural norms may affect how awareness is created or how participants are willing to enunciate or modify beliefs or are receptive to contradicting their beliefs or values this links directly to such methods as cultural consensus described below facilitation can be improved and may be more useful if records of the pm activities include documentation of the facilitation processes used and the results of those processes such a record increases transparency and allows reconstruction and analysis of what happened what was used and how and what impacts and outcomes resulted it creates a temporal record useful in understanding the future evolution of the system it also aids in learning from successes and failures and provides insights that may help in applying or transferring the facilitation processes to other pm efforts radinsky et al 2017 describe methods derived from the learning sciences for transcribing coding and analyzing video recorded discussions in participatory modeling settings these methods help us understand how groups of participants interact with each other and with the modeling tools how they learn about the complex problem they are facing to what extent new knowledge and learning is translated into a plan for action and the role facilitators played in supporting the process 2 2 2 role playing games rpg a role playing game is a useful method to exchange knowledge among stakeholders in a desired context rpgs involve creation and use of a virtual world with simplified real world conditions to collect information explore and understand context and situation and develop and explore collectively possible solutions a rpg comprises four main elements environmental settings player components rules of operation and inputs to the game the rules and structures of the rpgs promote player understanding by facilitating communication among stakeholders in an open environment eden and ackermann 2004 in the game different members play the role of different stakeholders and develop proposals collectively rpgs can create more effective teams help identify and address various stakeholders common or conflicting interests effectively build a supportive coalition and increase the effectiveness of implementation rpgs may also reveal implicit social rules and interactions between actors that might not have been evident during interviews and other interactions 2 2 3 brainstorming b brainstorming is a process that encourages all participants to offer ideas on a particular topic that are captured prior to any critical assessments of those ideas only after a robust list of ideas from the full set of participants is generated are decisions made about whether and how to exclude include or incorporate those ideas brainstorming can be used at many stages throughout the pm process and is often used when a facilitator feels that the group has narrowed their discussions prematurely or as a tool to encourage broader thinking and participation and to ensure that all voices are being heard 2 3 qualitative modeling in qualitative modeling in pm project participants build conceptual visual representations of the components of the problem being considered the focus of qualitative modeling is on identifying articulating and representing the relationships among the many components of a problem on the spatial temporal relationships and on how changes in one area affect other factors that may be important to solutions and to stakeholder concerns 2 3 1 rich pictures rp rich pictures is a diagramming tool that was developed as a part of the soft systems methodology checkland 1999 rp makes use of cliparts texts and symbols to represent how a group of people think about a particular issue bell and morse 2013 describe rp as a powerful intellectual and participatory device because it allows people to draw what they think but may not be able to write or speak about there are no strict rules or formal conventions for drawing rp it has to make sense for those who are involved in the process and be seen as a useful device communicating their ideas about the problem although this freestyle nature allows for creativity it makes it difficult to share a rich picture outside the group without very clear explanation of the meaning embodied in the picture lewis 1992 some attempts have been made to provide general guidelines on practices for drawing coherent and useful rp open university 2000 bell et al 2015 2 3 2 cognitive concept mapping ccm concept maps are graphical representations of organized knowledge that visually illustrate the relationships between elements within a knowledge domain a concept map results in a network where concepts nodes are connected through directed links edges these links are labeled to indicate semantic or otherwise meaningful relationships e g are in includes these labels allow one to logically define the structure novak and cañas 2008 the argument for representing knowledge with concept maps emerges from constructivist psychology which postulates that individuals actively construct knowledge by creating mental systems which serve to catalogue interpret and assign meaning to environmental stimuli and experiences raskin 2002 knowledge constructed in this manner forms the foundation of an individual s organized understanding of the workings of the world around them and thus influences decisions about appropriate interaction with it several other mapping approaches are related to concept maps a cognitive map usually represents an individual s knowledge or beliefs about a particular issue or system of interest whereas a concept map represents the perspectives of several individuals who worked together to identify key concepts link them and decide on the most appropriate labels describing the nature of each link eden and ackermann 1998 additional constraints or steps can be imposed to create different types of maps for instance a mind map follows a similar process to a concept map but the core idea s would be positioned in the center of the map with all other ideas branching off radially differences between these maps and implications for research have been discussed by davies 2011 2 3 3 causal loop diagram cld causual loop diagrams are commonly used in system dynamics modelling to represent the key variables and relationships that are assumed to explain dynamic behavior the cld method uses a relatively small number of conventions making it simple to use even for a non technical audience lane 2000 arrows represent causal relationships where relationships are indicated by direction i e positive or negative the emphasis in drawing a cld is on eliciting and representing feedback loops and delays that explain the problem behavior lane 2008 presents a critical review of the use of cld in system dynamics and notes that the role of cld changed from a back end tool to communicate about the output behavior from the simulation model i e expository mode to a front end model conceptualization tool cld can be used as a standalone method for model conceptualization without being necessarily extended to the stage of a system dynamics simulation model the cld method has been credited for its simplicity and ability to give an aggregate or strategic view of the problem structure which helps to keep focus on feedback loops rather than on details the method has been criticized see morecroft 1982 richardson 1997 for more details for example for not adhering to fundamental principles of accumulation which could lead to ambiguous and flawed inferences about problem dynamics in the context of pm sedlacko et al 2014 examined the use of cld as a tool for promoting knowledge co production and facilitating group learning they found that to be effective cld require that groups have an agreed ontology about what variables mean and how the system works otherwise there is a risk of producing shallow diagrams that hide both unexpected depths about given problems and interesting insights in the differences between various stakeholders mental models and views 2 3 4 cultural consensus cc cultural consensus is a collection of analytical techniques and models that can be used to estimate cultural beliefs and the degree to which individuals know or report those beliefs weller 2007 formally cc theory estimates the culturally correct answers to a series of questions group beliefs based on responses to similar questions and simultaneously estimates each respondent s knowledge or degree of sharing of beliefs romney et al 1986 a structured questionnaire is used to collect nominal or ordinal data on set of relevant questions those questions are typically designed after interviews participant observation and direct input from stakeholders statements that capture key themes and knowledge are elicited from stakeholders paolisso 2015 descriptive statistics can be applied to stakeholder responses to identify any within and between group patterns in the answers individual responses are processed through factor analysis to produce estimates of degree of sharing between individual and group cultural knowledge the method assumes that there is only a single factor solution which represents the cultural consensus stakeholders can be brought in again at this point to help interpret the pattern of responses in the informal model the competence scores tell how well the responses of each individual correspond with those of the group stakeholder engagement is critical to interpret these results since cc does not provide definitive answers to what are the nature and boundaries of the shared underlying knowledge only that there is a shared knowledge system underlying the pattern of responses paolisso 2015 cc complements bbhv recognition above in that it formalizes a methodological approach that captures the implicit and tacit knowledge that help drive behaviors beliefs and values 2 3 5 decision tree analyses dta a variety of approaches can be used in qualitative modeling that emphasize identifying and illustrating the relationships between decisions actions that can be taken to influence the situation of interest and the outcomes of interest to stakeholders in the context of the pm study their objectives for example decision trees kirkwood 2002 are used to illustrate the sequence of decisions and system changes that occur over time and how they affect the outcomes that stakeholders care about dta are also used in quantitative modeling but can be used primarily as a qualitative structuring tool a more general name for these methods would be decision focused structuring both adaptive management holling 1978 williams and brown 2012 and dynamic adaptive policy pathways dapp haasnoot et al 2013 are decision focused structuring and modeling methods that clearly differentiate actions system uncertainties and evolution and stakeholder objectives early in the model structuring phases these methods are designed to stimulate thinking about how decisions may change or other decisions may need to be taken as the system evolves they focus on the concepts of dynamic change and adaptation of actions as with decision trees these methods can be useful in qualitative modeling to provide both structure and explicit consideration of timing they can also be carried further into semi quantitative or fully quantitative modeling a recent case study using dapp lawrence and haasnoot 2017 highlighted the benefit of this approach in stimulating discussion among decision makers planners and stakeholders on future actions by making uncertainty explicit making the modelling process much more transparent and connecting decisions to outcomes of interest 2 4 semi quantitative modeling conceptual quantification the distinction between qualitative and quantitative methods is not always clear cut quantitative methods use formulas and equations and make calculations based on data however in many cases the data are qualitative or semi quantitative the data may consist explicitly of qualitative information they may be numeric estimates of values that are agreed upon or negotiated among participants or they may be based on experimental data but have significant uncertainty about them in our typology a method is classified as quantitative if technically there are ways to quantify most of the information used this can be done through experiments monitoring surveying etc if it is impossible or very difficult to obtain or use numeric information then the method is considered qualitative some methods are semi quantitative for example we categorize fuzzy cognitive mapping fcm as semi quantitative since it employs some numerical analyses of the values assumed in the model but the values themselves are most likely to be only qualitative or conceptual on the other hand bayesian belief networks bbn are considered as quantitative because experiments can potentially be designed to measure some of the probabilities used in the method 2 4 1 fuzzy cognitive mapping fcm fuzzy cognitive mapping allows groups to share and negotiate knowledge about a problem and build semi quantitative conceptual models fcm facilitates the explicit representation of group assumptions or beliefs about a system being modeled through parameterized cognitive mapping özesmi and özesmi 2004 gray et al 2014 example https participatorymodeling org node 36 as in ccm fcm starts with defining the most relevant variables that comprise a system and the dynamic relationships between these variables and then extends the ccm method by assigning the degree of influence either positive or negative that one variable can have on another fcm has three specific strengths compared to qualitative concept mapping techniques which have led to their increased use in futures studies scenario planning and complex systems modeling jetter and kok 2014 papageorgiou and salmeron 2013 first since the models created are semi quantitative they can be evaluated to understand system trends based on what if scenarios in a pm context this allows stakeholders to contrast and compare the effect of different scenarios or evaluate the effectiveness of different management interventions in a given a socio environmental problem see gray et al 2015 second an fcm can be constructed in many ways providing a way to combine the experiences or expertise of several individuals with various qualitative data sources see for examples singer et al 2017 for instance individuals can share their experiences and understandings and these can be aggregate to create a group level map gray et al 2014 if the right data are available the model can be derived entirely from the data using learning algorithms papageorgiou and salmeron 2013 third fcm can be subjected to a range of network metrics allowing researchers to contrast the ways in which individuals or groups think about a potential problem lavin et al 2018 measure the degree of structural variation across stakeholders and hence provide insight into uncertainty and complex socio environmental problems that groups seek to understand there are numerous extensions to the fcm methodology and software tools have been developed specifically to support participatory fcm see www mentalmodeler org gray et al 2013 fcm can be employed with a stakeholder group to build and evaluate through scenario analysis a model in a short time 1 2 h or through individual interviews that take less than 30 min however such quick analysis comes at a cost since fcm does not represent specific quantities and is largely limited to defining linear relationships between concepts additionally time and thus delays are not represented in fcm as the system changes in steps that bear no connection to real world time therefore although a useful tool to quickly and efficiently evaluate the structure and function of a dynamic problem the model output is limited to conceptual and qualitative units with no real time reference for how dimensions of a system may change over any real time horizon that stakeholders may desire for their decision making 2 4 2 scenario building sb scenario building or planning or exploration amer et al 2013 is a practical approach to dealing with uncertainties about the future scenario planning relies on a broad analysis of trends and policies to cover a range of plausible futures it is distinct from forecasting or predicting a specific future each scenario should be internally consistent meaning that given current conditions and trends it is plausible that the different aspects of the scenario could play out in the described way each scenario is designed to be substantially different from other scenarios and to highlight a unique and interesting possible future in participatory modeling scenarios can build from quantitative models e g systems dynamics in this approach stakeholders provide knowledge about the structure of these models and indicate which input variables are critical and uncertain a quantitative model is run for multiple input combinations within the plausible range and the results provide the final value state for each system element the resulting internally consistent scenario may then be described in qualitative terms in the form of a scenario narrative other pm approaches create scenarios in a fully qualitative fashion stories scenarios are used to identify robust policies that are successful in most or all future scenarios as well as to proactively develop backup plans 2 4 3 social network analysis sna social network analysis is a method for studying a set of social relations among actors and how these relations and their patterning can impact or be impacted by actors views behavior perceptions and by learning prell 2012 actors can be individual persons or social entities such as organizations or even countries e g prell and feng 2016 social relations can represent friendship communication or trust or can refer to other types of flows such as membership trade or various kinds of resources a relation in sna usually involves at most two entities which allows sna to use analytical tools from network graph theory shirinivas et al 2010 data on social relation networks can be binary or valued although most analyses ultimately require that the analyst decides on a cut off value used to dichotomize the data before modeling the modeling of networks ranges in complexity simple descriptive measures and or visual digraphs of a network can be helpful in identifying which stakeholders are more popular or powerful which are more peripheral and how stakeholders might cluster together such simple descriptive measures can be helpful in designing participatory workshops and or helping stakeholders understand the social context in which they are embedded prell et al 2008 2009 more complex stochastic models have been designed for handling network independencies such as exponential random graph models or ergms robins et al 2007 and stochastic actor oriented models saoms snijders et al 2010 these stochastic models can help analysts better understand with greater precision the decisions perceptions or behaviors of stakeholders especially in the context of natural resource management or governance bodin et al 2016 matous and todo 2015 prell et al 2017 2 4 4 analytic hierarchy process ahp during the pm process it is often useful to consider the effects of scenarios or alternatives on a diverse set of criteria or objectives identified by the participants while economic or cost benefit analysis is sometimes used to summarize the impacts it can be helpful to combine those effects into a summary metric through a model that explicitly accounts for conflicts and tradeoffs among those criteria including criteria not easily monetized several approaches for evaluating options against multiple criteria assessing tradeoffs among those criteria and recombining results into a summary metric have been used in pm a popular method is analytic hierarchy process ahp saaty 1980 in ahp tradeoffs among criteria are derived as criteria weights from stakeholder input on the pairwise relative importance of all criteria alternatives are evaluated against those criteria using similar stakeholder input and the results are combined in a weighted linear summation hajkowicz and higgins 2008 howard 1991 example https www participatorymodeling org node 45 criteria weights are often assessed from individual stakeholders when there is significant variability among weights obtained from different stakeholders it may be challenging to identify an appropriate group summary metric means ryu et al 2011 tian et al 2013 medians kolagani et al 2015 and even the geometric mean saengsupavanich 2013 have been proposed some modelers preserve the range of values by propagating the variability across stakeholders through the model using the monte carlo simulation approach rosenbloom 1996 hauser and tadikamalla 1996 lafleur 2011 kolagani et al 2016 ahp is a special case of a more general approach known as multiple criteria decision analysis mcda greco et al 2016 other popular approaches for mcda in environmental decision making are multi attribute utility theory and outranking approaches huang et al 2011 any of these approaches can be used at various levels of quantification they can be used as qualitative tools to support problem structuring and they can be partially or entirely quantified to create and support concrete valuation and comparison of values tradeoffs necessary in any decision 2 5 quantitative modeling 2 5 1 geographic information systems gis geographical information systems gis are computer based mapping frameworks that can be used to help stakeholders in visualizing and modelling their problems spatially for example gis can be used to analyze and display how various scenarios play out on the landscape being considered and how those changes provide benefits or costs to various stakeholders gis can also be used to provide inputs to other models for example stakeholders can map the land use and soil characteristics of their land parcels in a gis and use these maps to measure quantitatively the extent of land parcels under various land use and soil categories in participatory mapping local stakeholders can sketch out spatial features on the ground paper or a touch screen on top of remote sensing imagery chambers 2006 such use of gis by ordinary stakeholders has been termed public participation gis pp gis sieber 2006 however implementation of quantitative gis models typically requires quite a high level of technical skill and over reliance on the technical aspects of gis may alienate less skilled stakeholders chambers 2008 there are several efforts to simplify gis tools to facilitate use by less technically trained stakeholders taking advantage for example of the increasing popularity of mobile and web technologies kolagani and ramu 2017 example https participatorymodeling org node 38 https participatorymodeling org node 121 2 5 2 empirical modeling em empirical modeling refers to the process of identifying and quantifying relationships among factors of interest using observed and experimental data em is sometimes called best fit modeling and is contrasted with mechanistic process based modeling voinov 2008 in best fit or empirical models mathematical relationships are derived from data they may or may not represent actual physical relationships between those factors they are often used early in modeling projects to explore interpret and understand available quantitative data these models are sometimes referred to as black box models because they operate as closed devices that process information with no explanation of processes or parameters involved serrat capdevila et al 2011 refsgaard et al 2005 these models are entirely driven by the specific data available and they are risky to use outside the ranges covered by that data extrapolation because they do not necessarily explain real world relationships between factors they can be difficult to use or communicate in a pm process basco carrera et al 2017a though their accuracy can be very high 2 5 3 cost benefit and other economic analyses cba economic analysis may be conducted as part of the pm process especially in the latest stages of the planning cycle to help assess the benefits and costs of alternative decisions or investments an economic analysis may help guide the design and ultimate choice of policy alternatives and associated system scenarios and forecasts economic analyses may be used to place a total monetary value on specific outcomes of interest this approach spread into the environmental arena with the concept of ecosystem services national research council 2005 total value is generally composed of use values and non use values use values can further be parsed into direct use values e g fishing ecological function values e g water availability and option values e g potential protection from floods non use values can take the form of an existence value e g satisfaction of knowing that a species exists or a bequest value e g preserving a resource for the next generation economic analysis can also be used to help determine the worth or benefits of acquiring additional data or information cf young 1992 cost benefit or benefit cost analysis cba e g hanley and barbier 2009 nasa 2013 is a commonly used methodology for assessing the anticipated costs and benefits of an investment or policy change compared to those that would accrue without an investment or policy change the credibility of the no change scenario is essential in assessing the credibility of the cba in each case the analysis generally requires developing a time series of costs and benefits that would accrue under each scenario and then using a discounting hypothesis to summarize that time stream in a given reference year the assessment of both costs and benefits is likely to consider only a subset of the potential costs and benefits and is also likely to miss indirect costs and benefits that may result from a particular application or policy choice there are many examples of cba including one undertaken by the u s geological survey to assess the value of creating a national map halsing et al 2004 different levels of implementation of a national map were compared to the counterfactual of not creating a national map 2 5 4 system dynamics sd system dynamics is a simulation based method used to articulate and understand the causal interactions that explain how the system behavior changes over time key to the sd method is the representation of a system in terms of stocks where material energy or items are stored and accumulated and flows which are rates of exchange between stocks an sd model provides useful insights into the feedbacks delays and nonlinear interactions helping decision makers to see the long term system wide and sometimes counterintuitive outcomes of their decisions example https participatorymodeling org node 39 https participatorymodeling org node 82 sd is the foundation method for several participatory modelling methodologies such as group model building vennix 1999 mediated modelling van den belt 2004 participatory sd antunes et al 2015 sd learning laboratories nguyen et al 2011 bosch et al 2013 system dynamics models were initially developed to investigate the temporal dimension in non spatial systems some efforts have focused on extending the capability of systems dynamics to spatial modelling i e spatial system dynamics modelling to investigate the effects of spatial characteristics on the problem behavior over time ahmad and simonovic 2004 bendor and kaza 2012 costanza and voinov 2003 these efforts include 1 breaking down the system into zones where each zone is represented as system dynamics model ford 1999 and 2 coupling system dynamics models with gis to exchange information between spatially distributed models over the simulation time neuwirth et al 2015 2 5 5 bayesian networks bn bayesian networks are a statistical modelling method where the model takes the form of a unidirectional network a directed acyclic graph dag nodes represent variables in the problem while links represent the causal relationships among these variables variables usually take discrete states with certain probabilities the graphical representation makes bn intuitive and useful for communicating model assumptions uncertainty and the complex interactions among variables especially with non technical stakeholder groups carmona et al 2013 castelletti and soncini sessa 2007 chen and pollino 2012 in addition to the qualitative and graphical component i e the dag bayesian networks also use conditional probability tables cpts to quantify the strengths and probabilistic relationship between the causal variables parent nodes and children variables pearl 2009 bns can use and integrate qualitative data e g the prior knowledge gained from experts or literature and quantitative data e g survey data bns also have other advantages such as a capability to handle missing observations potentially high accuracy for small amounts of data and the possible support of scenario based analyses 2 5 6 cellular automata ca cellular automata is a simple yet powerful modeling method developed by ulam and von neumann in the 1940s a ca model is composed of cells with finite and discrete states located in a regular lattice space e g a square grid the state of each cell is updated at each discrete time step based on rules taking into account the state of the cell and its neighbors up to a certain distance this modeling method is especially suitable for spatial modeling where the landscape is represented as a grid of cells each cell described by a certain state that can change to one of other states depending on its current state and interactions with other cells this method is often used to model land use change veldkamp and fresco 1996 verburg et al 2006 batty et al 1999 parallel computations can be implemented by partitioning the lattice space into smaller spaces which allows one to model large landscapes and or at a detailed spatial resolution sun et al 2009 ca can also be used in conjunction with other types of models to bring a spatially explicit component in spatial versions of sd models local sd models are replicated over the grid of cells costanza and voinov 2003 when sd is involved the models usually turn out to be quite complicated and may require substantial computer power to run 2 5 7 agent based modeling abm similar to sd agent based modeling is a simulation method used to articulate system behavior and state changes over time instead of considering aggregates global variables representing whole entities populations amounts of water energy material etc abm aims at the system level and macro patterns that emerge from individual behavior of elements and interactions between them it is a bottom up process bonabeau 2002 the main elements of abm are called agents represented by attributes state location etc behavioral rules and interactions with other agents and with the environment some agents are able to take decisions based on certain rules or goals e g maximize profit and even learn and adjust their behavior adapt based on past experience and performance of other agents where cas are focused on landscapes and transitions abms focus on individual actions and behavior agents vary in their preferences and abilities to act on their environment as well as their ability to learn and adopt new practices spreading them via their social network abms are particularly well suited for representing complex spatial interactions under heterogeneous conditions and for modeling decentralized autonomous decision making parker et al 2003 zellner 2008 filatova et al 2013 they have been widely used to study socio ecological systems bousquet and le page 2004 schulze et al 2017 an 2012 examples https participatorymodeling org node 36 https participatorymodeling org node 37 https participatorymodeling org node 74 https participatorymodeling org node 75 abar et al 2017 provide an impressive list of abm tools available to support this method giabbanelli et al 2017 discuss a possible connection between abm and fcm methods 2 5 8 integrated modeling im integrated modeling is a way of building models by combining or coupling existing models used as components to represent complex systems laniak et al 2013 belete et al 2017 output from one model becomes input for another model since component models can come from different disciplines im is often seen as transdisciplinary exercise complex and powerful simulation models can be created by finding existing well tested modules and plugging them together to represent the systems of interest with properly documented models and with appropriate user friendly interfaces this could potentially be done on the fly with stakeholder participation example https participatorymodeling org node 90 im tends to produce quite complex models which may be hard to communicate to stakeholders fast integrated systems modelling and metamodeling try to integrate and simplify interactions and relevant feedbacks among complex systems haasnoot et al 2014 basco carrera and mendoza 2017 metamodels are models of models intended to mimic the behaviour of complex models see e g davis and bigelow 2003 walker and van daalen 2013 creating meta models and other fast integrated models normally requires pre running the complex models saving their output under various combinations of parameters and then using the output instead of running the actual models such models are also known as low resolution models repro models or fast and simple models basco carrera and mendoza 2017 describe a fast integrated systems modeling approach as part of collaborative prototyping example https participatorymodeling org node 119 integrated models may use something as widely available as excel as a front end or more sophisticated tools such as python or pc raster depending on the needs of the process resolution in time space and system processes to be included 3 selecting appropriate methods as summarized above there are a large number of methods and tools that can and have been used in pm processes yet it is difficult to identify the best strategy for deciding on what methods and tools and or combinations thereof are most appropriate for a particular pm project what makes these decisions especially difficult is that as previously mentioned there are hardly any reported cases where more than one method has been tried for the same problem within the same project combining several methods is quite common but replacing one method for another is not in operational research the mixing of methods has been viewed as a positive trend howick and ackermann 2011 have produced an extensive review of papers on mixing methods but were not able to produce any general recommendations on what methods to mix and how the selection and mixing of methods and tools is a decision making process on its own see ormerod 1997 that clearly one expects would be driven by the specifics of the problems being addressed however current practice reported in published literature tells a different story rarely is there much justification provided for the methods used either individually or in combination this section offers support for researchers as they consider and select methods and tools the first subsection reviews three pm case studies with a focus on the methods selected for each the next subsection describes some problem characteristics that should be taken into account when selecting pm methods followed by the results of a survey of modelers engaged in pm that explored their perceptions of the pm methods described in section 2 we end the section with some recommendations on the process and criteria for selecting methods and tools 3 1 methods used in case studies how are methods chosen in real pm case studies this section describes three studies identifying the methods and tools used in each and explaining the rationale for their selection n two of the three examples presented there was not much discussion about methods used they were determined by the modelers in the indian case study however the stakeholders moved from one method to another one choosing what worked best for them at each stage this was the one project that was not funded by any major external donors it was implemented largely through volunteer efforts of the participants 3 1 1 modeling the causes consequences and solutions of the flint water crisis residents of flint michigan usa experienced a serious compromise in their water quality beginning with an emergency manager s decision to switch the city s water source to the flint river in 2014 by 2016 thousands of flint residents had been exposed to unsafe levels of lead in their drinking water and the governor of michigan had declared a state of emergency center for disease control and prevention 2016 a modeling team from michigan state university was asked by the community foundation of greater flint to conduct a modeling exercise to capture the voices and views of flint residents around the causes and consequences of and potential solutions to the flint water crisis hereafter referred to as the water crisis gray et al 2017 singer et al 2017 the goal of this exercise was to represent flint resident views in a manner that could be communicated to city leadership and to the state appointed team in charge of developing a response to the water crisis the timeline for this exercise was very short community partners wanted a modeling product within a few months in order for the results to be timely and relevant to the water crisis response this short timeline effectively ruled out a simulation modeling approach which would have taken significantly longer given the goals of the exercise it was important to select a tool which could easily capture and synthesize flint residents views and knowledge about the systemic nature of the water crisis and which could represent those views in an easily understandable format the fuzzy cognitive mapping fcm method was selected implemented with the mental modeler gray et al 2013 online tool fig 2 during the spring of 2016 a series of four mental modeling workshops was conducted throughout the city of flint attended by a total of 36 residents a screen projecting the mental modeler software was displayed in front of the workshop participants and a facilitator led the group through a discussion of the causes consequences and solutions of the water crisis posed as broad questions concepts and relationships between concepts were suggested by workshop participants and captured by the facilitator for all to see when necessary the facilitator posed clarification questions for discussion around the meaning of specific concepts or the nature of the relationships between concepts after the final workshop the modeling team aggregated the four models developed by residents and shared the aggregate model as well as the similarities and differences between the community models with flint residents in a final workshop the results of the modeling process were shared widely through public meetings and an online report when reflecting on the modeling experience flint residents and flint based members of the research team expressed satisfaction with the modeling exercise in meeting its goals as a tool to communicate community views to the officials responsible for responding to the water crisis residents appreciated the opportunity to see their views and experiences reflected in a modeling product workshop participants also liked mental modeler s ability to run scenarios examining how changing one variable would affect other variables in the model several participants expressed interest in using mental modeler to address other community problems and a desire to be trained in the software thus the selection of a method with a software tool that s relatively accessible to non modelers was an appropriate choice a few flint residents did express a desire to see the modeling results integrated with spatial information about lead exposure hanna attisha et al 2016 a lack of spatial capability is a weakness of the fcm method 3 1 2 indian groundwater crisis in a village of about 240 households in southern india a major problem in the recent past has been drought and over extraction of groundwater a typical tragedy of the commons problem over several years local leaders have used various informal tools to better understand the causes of the problem and come up with sustainable solutions they organized and facilitated village meetings i e focus groups field visits i e transect walks and individual discussions i e semi structured individual interviews as spatial information was increasingly needed to better visualize and understand the problem villagers and leaders began using handheld gps units to plot the locations of their farms and wells on paper they then made the process less time consuming and less error prone by moving to public participation gis pp gis tools kolagani and ramu 2017 example https www participatorymodeling org node 45 with the help of their school going children who were in turn assisted by their computer literate teachers fig 3 they started showing the maps to other stakeholders in an effort to come up with potential solutions fig 4 emphasis was placed on the use of simple calculations and visualizations to plan and implement rainwater harvesting rwh systems kolagani et al 2015 and other solutions that were found most appropriate to the situation with some extensions to capture temporal dynamics they were able to undertake participatory water accounting gray et al 2018 example https www participatorymodeling org node 38 this helped them understand that increasing water recharge through rwh systems was only a short term solution and that rwh alone would not solve the problems unless water discharge was also controlled however discharge regulation is a socially and politically sensitive issue that requires buy in and voluntary participation from all stakeholders the leaders then felt that quantitative what if scenario analysis would greatly help convince the stakeholders of the need for discharge regulations it is difficult to do what if scenario analysis with pp gis however and project leaders looked for more formal yet simple quantitative models that could predict the future at least in the short term while remaining themselves in full control of the process they looked for a pm method that was easy to use yet had the power to answer their questions they considered sd abm bn and fcm approaches with the help of researchers from a nearby academic institute they recently started using sd as they became comfortable with quantitative models they also started looking at abm using rules they developed from their collective experience initial results seem to point to the need for a hybrid approach an easy to use platform that facilitates such learning and use of different modelling methodologies might really help such innovations in participatory modelling efforts especially by making stakeholders take ownership of and extend the models themselves 3 1 3 territorial transformations in the amazon in the amazonian floodplains of brazil i e várzea forests climate change is disrupting the frequencies and magnitudes of floods leading to great uncertainties for local populations a project focused on the flooding of curuaí big lake a territory with 30 000 inhabitants spread over 133 communities in the para state investigated how these populations were adapting their production systems to changes in flooding a multidisciplinary research team collaborated with feagle 2 2 http governancaflorestal iieb org br manejos view 10 a civil organization in charge of monitoring agrarian reforms by granting deforestation permits and hunting and fishing licenses today regional conflicts and pressures from mining and timber extraction companies and the complex landuse situation threaten feagle s existence and the future of the small scale farmers in the area given the vulnerability expressed by the social actors the research team first studied their concerns and strategies through field visits and semi structured individual interviews in several communities and then collectively discussed most probable scenarios to better understand ongoing dynamics regarding landuse activities on medium term futures a rpg was collectively designed with students of a rural school around 30 future farmers the rpg was organized as a board game that roughly displayed four communities on a transect from the lake to the forest in this game all players managed their respective farms according to their own strategies within the constraints of the game considering issues such as labor money land cover livestock and so forth by observing how players act in the game all participants were able to better understand how people behave in real situations the fun aspect of the game was also fundamental in freeing up conversations the rpg debriefings enabled rich exchanges with farmers and fishermen especially about the various constraints in the region they spontaneously addressed the impact of their activities on natural resources even if they were not always capable of explaining causal relationships the main drawback of this game was its slowness half a day of play was needed to simulate 4 to 5 years consequently an abm was implmented to better formalize the relationship between human activities and environment and to increase the time horizons of the sessions le page et al 2014 any of the 16 agents representing 4 smallholders from 4 communities could be controlled by participants while the not under control agents performed computerized decision making algorithms both types of agents simultaneously made seasonal decisions on agriculture fishing and animal husbandry while the computer also simulated biophysical processes by integrating their activities the abm was built as a continuation of the rpg based on the structure of the game validated by the actors it also seeks to specify the impacts of the activities based on research data moving from rpg to abm enabled sophisticated calculations and scenarios on a broader timeframe this allowed improved visualization and understanding of pasture degradation and dwindling fish stocks fig 5 through collective analysis resulting from the sessions socio economic and demographic changes were identified as additional factors along with climate change that contribute to water shortages and to the difficulties of addressing related issues for example without sewage treatment systems population growth could impact water quality and lead to the proliferation of cyanobacteria threatening human and animal health as well as fish stocks that are already under pressure from commercial fishing and non compliance with community fishing rules bommel et al 2016 3 2 problem characteristics from the examples above and from our experience running pm projects we identified some characteristics to take into account when deciding on particular methods to use for a given problem 3 2 1 nature of the problem the nature of the problem is critical for example a problem may focus on how to manage a common resource acute risks or slowly emerging risks it may involve tradeoffs between conservation and economic development or it may relate to environmental protection issues researchers considering the nature of the problem at hand may need to think about some of the following issues the range of domain specific expertise needed for the pm study for example expertise needed in such fields as public health natural resource management business organization and others the spatial and temporal scales of the system studied and whether and how specific methods and tools used will be suitable given those spatial and temporal dimensions the characteristics of the boundaries of the system relative to its processes stocks and flows isolated closed or open the structure of the problem or issue and its degree of wickedness the characterization of the level and types of uncertainties involved 3 2 2 nature of community engagement the participatory process largely relies on who is playing a role in decision making therefore it is important to understand not only who is participating but also how they are participating this depends on one s capacity to participate and both realized and perceived divisions of power not all tools are appropriate for all user groups by considering the nature of the community stakeholders and practitioners can choose the methods and tools that will be most effective researchers and participants may want to consider factors such as the number diversity background and skills of the stakeholders and also their social political positions and roles education age sex etc level and intensity of stakeholder participation low participation groups those on the lower end of arnstein s participation ladder i e ignorance awareness or information may benefit from an entirely different set of methods or tools than those on the higher end lynam et al 2007 of the participation ladder e g consultation discussion co design co decision making timing and stages of participation some stakeholders may desire to participate in all stages of the pm process others may want to focus on specific areas or may have skills and information that makes their input for specific pm stages particularly useful interaction context it is important to understand the level of cooperation or conflict among stakeholders it affects the facilitation approaches and techniques that might be used and possibly the selection of modeling methods power asymmetries where significant power asymmetries exist the choice of process orchestration methods and the composition of the modeling and facilitation team may strongly influence which groups have stronger or weaker voices in the pm process the team may need to take special care to assure that specific values or beliefs are recognized power asymmetries may also complicate the question of how much transparency the methods and tools should promote power asymmetries have both methodological and ethical implications the question arises as to whether some powerful actors whose actions contribute significantly to the evolution of a socio ecological system should be included in the pm process or not in some situations social violence is part of everyday life and extreme pressure is often exerted on the weak by inviting powerful actors to pm workshops there is a risk of inciting more of this social violence this issue creates a dilemma claiming a neutral posture by inviting all relevant people may further strengthen the most powerful actors while adopting a non neutral posture to empower the weakest actors may compromise the legitimacy of the process barnaud and van paassen 2013 the proper balance should be taken into account in choosing facilitation approaches although it may be difficult to translate these issues into selection criteria for other methods in some cases the level of power may correlate with level of education which directly correlates with what tools are more likely to be understood and used effectively by given constituencies stakeholder preferences and constraints can be driven by existing skills and experiences in particular methods or by the availability of training for newly suggested methods smajgl 2015 this depends on the level of co implementation e g model co construction and the expectation that methods that result from the pm process will continue to be used in these cases stakeholders may often consider the complementarity of the new proposed methods with the models and instruments they had been previously using stakeholders rarely advocate for the replacement of existing capacity stakeholders are also often concerned about the maintenance of methods e g requirements for model re parameterisations if they are to be used beyond the near term pm study 3 2 3 desired results the goals of the pm process also have a strong influence on the methods and tools that should be used for example a focus primarily on building trust and understanding among stakeholders may rely more on qualitative tools and on process orchestration in contrast a focus on helping a small group of decision makers solve a particular problem may require use of more sophisticated quantitative modeling approaches questions that can be asked include what is the intent and prospective use of the modeling process and model outputs will the pm process help to make a decision build trust understand spatial distribution temporal dynamics and causality relationships or is there another intent is descriptive analysis sufficient or are prediction and scenario analyses also expected is the goal only to promulgate or achieve greater system understanding or is there a desire to create forecasts or produce quantitative estimates is the description of trends sufficient or is the elucidation of actual process dynamics needed what are specific social objectives for the pm process e g decision making collaborative learning shared or social learning mediation model improvement what are the political or governance types of actions that may result from the process e g unilateral action coordination collaboration joint action 3 2 4 resources available the final critical factor to be considered in selecting methods and tools relates to the type and amount of available resources resources required include time and money people skills information and data these resources are often limited so method selection should be informed by considering types quantity and quality of available data and the time and expertise required for any additional fact finding available analytical tools platforms and visualization communication tools that can be used by the full project team human resources and expertise the types and levels of expertise among project participants and how their expertise aligns with their desired level of participation timing the length of time needed for various approaches or methodologies varies greatly depending also on the level and expertise of participants considering when results from the project are expected and when they will have the most impact may suggest which methods have the greatest chance of success to meet the needs of the project and its stakeholders financial resources available to support process orchestration meetings and workshops model implementation and consensus building around possible outcomes or recommendations the importance of these factors was confirmed through the survey described in the next section 3 3 survey of pm practitioners to understand how pm researchers typically select methods we administered an online survey of pm practitioners the survey included four main parts that elicited 1 experience in the use of different methods participants indicated their level of experience with each of the 23 methods shown in fig 1 using a 3 point scale not experienced somewhat experienced or very experienced 2 most preferred modeling methods participants were provided with a list of the 18 methods from the modeling portions of fig 1 and asked to indicate their first second and third most preferred methods 3 ways of selecting different modeling methods participants were asked to rate their level of agreement on a 1 to 5 scale with each of the following statements regarding their choice of methods in their past pm projects i used the method s with which i am most familiar i explored all options and then chose the methods that were most appropriate even if i was not experienced with them i started with the method that i thought was most appropriate if i found that it did not work then i switched to another one trial and error approach i have only chosen to work on projects that can be addressed with the methods i already know i typically selected methods based on the nature of the problem at hand i typically selected methods based on the nature of the community involved in the project 4 the importance of different factors that may influence the selection of methods participants were given a list of factors derived from the discussion above and were asked to rate the importance of each factor to them when they select a method using a 1 to 5 likert type scale from a strongly agree to a strongly disagree 3 3 1 survey administration survey responses were solicited through a listserv to more than 1000 individuals included on a mailing list from the innovations in collaborative modelling group as well as through convenience sampling to various colleagues and collaborators of the authors to ensure the respondents were professionals in the field of pm the first question included the following statement we define participatory modeling or collaborative modeling as a purposeful learning process for action that engages the implicit and explicit knowledge of stakeholders to create formalized and shared representation s of reality have you participated as a modeler or researcher in a project that involved participatory modeling if respondents indicated no to this question they were excluded from taking the survey in total 93 respondents identified themselves as having experience in pm and 84 completed the entire survey for which results are shown 3 3 2 survey results and discussion on the subject of experience with different tools survey responses indicated some unevenness across the different methods fig 6 while it could be expected that some of the fact finding methods such as surveys and interviews are quite well known and similarly for some of the facilitation methods such as brainstorming it was surprising to find that system dynamics was as well known as interviews and better known than causal loop diagrams cld which in a way are embedded in system dynamics overall our results indicated that respondents were knowledgeable of a diversity of methods the preference for methods used fig 7 aligned with responses regarding experience with the methods except for some cases for example many respondents 60 indicated that they were experienced in cost benefit analysis but just a few 5 preferred that method system dynamics was the most preferred method 26 followed by cld 22 and then scenario building 20 and these were also methods with which many respondents had experience for the most preferred method we also asked respondents to specify the strengths and weaknesses of the method in the context of pm these comments have been taken into account in describing the methods in section 2 above interesting results emerged from questions about how participants chose methods for their recent pm projects fig 8 on the one hand respondents clearly admit that they choose methods that they are most familiar with 92 totally agree or strongly agree with this statement at the same time 60 claim that they choose the most appropriate methods this suggests that perhaps researchers choose the projects that where their methods expertise is the most appropriate choice yet only 35 of our participants said that was a factor indeed the vast majority say that they choose methods based on the problem characteristics 87 and on the nature of the community involved 73 these responses are difficult to reconcile when asked to rate factors in terms of how important each is when selecting a method all the identified factors were considered important time money and level of stakeholder involvement required as well as the availability of data had the highest importance fig 9 skills and education of stakeholders were of lower importance in the survey though still important overall the survey suggests that practitioners consider many things when selecting methods but that they do not necessarily have a clear hierarchy of criteria or approach for choosing those methods one interpretation of the results in figs 6 and 7 is that our respondents are guilty of a hammer and nail interpretation where they simply believe the tools they know best are the most appropriate and may be imposing their favorite tools on the stakeholders involved another interpretation is that our particular respondents were quite knowledgeable about the various methods of pm out of the 23 methods listed all respondents indicated they were experienced in at least 5 one claimed experience in all 23 and on average respondents were very or somewhat experienced with 14 methods this level of experience might allow them to choose both methods they are familiar with and those that are most appropriate to the problem they may also tend to choose projects where their expertise is most relevant 3 4 some recommendations we differentiate between three types of method selection the expert approach in which modelers choose or recommend the methods and tools to be used their recommendation is likely to be strongly influenced by the methods and tools with which they are most familiar and most comfortable with among those applicable to a problem while there may be nothing wrong with this approach there is always a risk that more appropriate methods exist that the expert is less comfortable with that would garner more effective participation in the process the experimental approach in which the stakeholders decide to experiment with new research methods or explore the applicability of existing methods in the project this may be also driven by the modeler usually an academic with a research agenda wishing to learn how new methods work in new applications using the project as a testbed this trial and error approach has the potential to create new insights but may be costly both in terms of time and resources needed and has to be well explained so as not to undermine stakeholder trust in the pm process the participatory approach in which all stakeholders including modelers take part in the process of selecting methods typically this requires extensive engagement between all stakeholders at the very beginning of a project so that all stakeholders can make an educated choice some stakeholders may also advocate for the use of a method that may not necessarily be the best for the task at hand we propose a critical approach that incorporates elements of all three types the experts can start with a systematic comparative analysis to identify and reflect on the merits and weaknesses of various methods and their applicability to the multiple dimensions of the problem they seek to address the other stakeholders are engaged early in the pm process to learn about the existing alternatives evaluate the options and decide as a group on what methods to use experimentation with methods should be encouraged whenever sufficient time and funding are available there is still the problem of power and knowledge asymmetries that put experts in a favorable dominant position their opinion may be hard to contest for less educated and less prepared stakeholders we have started a web portal https www participatorymodeling org as one attempt to facilitate access to knowledge about methods and their past use and also to promote further collaboration and communication about pm much of the information about methods presented in this paper is available on the web portal the site is a content management system that uses the drupal open source platform and allows the users to enter and upload all sorts of information we encourage anybody practicing pm or interested in pm to share their experience case studies information and skills to facilitate access to methods and knowledge in particular domains we are also collecting a listing of experts and provide a collection of other resources such as models videos papers etc there are several ways to guide the selection process a decision tree may be the most straightforward and easy to follow selection technique one previous attempt to apply such an approach to model selection was conducted by kelly et al 2013 however for pm we failed to identify an appropriate set sequence of decision points that could enable building a decision tree the process always appeared more nuanced and too many considerations had to be simultaneously taken into account mingers 2000 developed a questionnaire to help researchers think about selecting and designing multi method processes that involve the use of participatory and numerical analytical models questions are grouped into the following categories 1 questions related to the relations between the researcher and the candidate methods e g experience and skills 2 questions asking about the relations between the researcher and the problem characteristics e g interactions with stakeholders and 3 questions connecting the problem situation and the intellectual resources such as the suitability of methods to the organizational or situational culture these factors are similar to the list described in section 3 2 above that list and the mingers questionnaire provide useful guidance to help researchers think about what factors they need to take into consideration however they do not provide practical insights into how these selection methods perform we need a meta tool that supports both the selection criteria and the nuanced judgments possible for each criterion our method selection tables rank each of the pm methods shown in fig 1 first against a set of desired model characteristics table 1 and second against a set of resources required for implementation table 2 this is somewhat similar to the approach that the rand corporation suggests when choosing models for infection disease prevention manheim et al 2016 the goals of a pm study should offer the starting point for any discussion about the pm methods needed these goals may be positioned on two extreme ends of a continuum at one end some studies are designed to highlight knowledge diversity to make different voices in the community heard and to understand sources of conflict in these studies the models themselves are mainly boundary objects for communication of different worldviews they do not have to be scientifically accepted representations of the real world systems and they may not be consolidated into a single model participants are chosen based on a desire and need for comprehensive representation diversity of perspectives and to maximize engagement and understanding at this end of the continuum ease of communication and interpretation might be the most important factors to consider in selecting methods in contrast at the other end of the continuum some studies aim to pool expert knowledge about a system and to create a model that allows predictions to be made and that supports detailed exploration of the implications of different decisions or actions the desired output of this kind of study may be a quantitative model in of itself that is validated against empirical data and expert knowledge accordingly these studies are very concerned with the modeling process e g expert selection strategies for model validation and emphasize consolidation and aggregation fig 10 is a generalized version of fig 1 and positions different groups of modeling methods on this continuum selecting methods for a participatory modeling exercise should be a flexible process it may even involve the invention of new methods and tools if existing modeling approaches are not appropriate throughout the selection process stakeholders including modelers are typically engaged in asking the following questions sometimes repeating these questions at multiple stages of the modeling process 1 does this problem require detailed spatial or geographic information to solve at what scale or resolution are there spatial interactions to consider 2 is there a need to project current system conditions into the future in order to make or improve decision making how far into the future do we need to look and how precisely what level of uncertainty are we comfortable with are there temporal interactions to consider 3 what is the goal of the participatory modeling process is it community engagement community organization solution building planning implementation 4 how much time and how many people and resources can be devoted to the modeling effort what skills do the people have 5 how much does the community know about the problem being modeled do scientific data need to be collected and integrated into the model to help answer the community s questions what data are available kind quantity quality scale resolution what interactions with the modeling tools will different stakeholders want to have will they want to be consumers of outputs users and scenario testers developers what capacity building is needed for the different stakeholders to facilitate the pm process interactions they aspire to 6 what capacity does the modeling team have to build or use appropriate modeling tools do other modeling or scientific experts need to be brought in and is there sufficient time and financial resources for this what is the ability to continue using the particular method as output of the pm project after project completion how alternative methods can be linked to existing and already used data and methods answering these questions and using tables 1 and 2 the stakeholders should be able to evaluate the various methods available and choose one or a combination of methods that will be most appropriate for a given problem or situation subjectivity will always remain in how stakeholders treat these questions which is why it is hard to expect that the choice of methods will be always optimal in fact there is probably no optimal choice however by designing the process as open ended and adaptive the project team could ensure ongoing evaluation of process outcomes and methods used and change direction when deemed necessary 3 5 further considerations for method selection clearly the evaluation of appropriateness of pm methods entails multiple criteria which can be summarized as follow effectiveness how well can a specific method succeed given the focal problem of interest and how well it meets the goals of the pm team and the needs of the pm processes efficiency whether the methods can achieve the pm goals in the needed time and with the appropriate use of the available human financial and technical resources social value added how well the methods support the broader goals of the pm process such as promoting gender racial and income equality learning and education dialogue among diverse groups and social capital of stakeholders in line with the social network development mentioned below evaluation of the usefulness of the method used usually occurs only at the end of the project when time and money are most likely running out and when participants are fully invested in what they have done this only makes reporting of failures in addition to successes even more essential so that everyone can learn from mistakes or problematic choices it also explains the rarity of such reports if instead we evaluate the appropriateness of the methods early and throughout the pm process it is more likely that the project can adapt and change course if needed some methods may offer additional benefits that go beyond questions of effectiveness efficiency and social value for example to the extent that participants can really engage i e lose themselves in a gaming modeling or simulation process some conceptual modeling and qualitative or numerical simulation tools can help the decontextualisation process and potentially reduce conflict modeling and simulation tools and processes that are flexible enough to effectively create abstraction while fully engaging participants may be of great interest the preferred approach may be a combination of methods and tools so considering the compatibility of different methods to work together in the pm process is desirable for example there is theoretical and empirical support for combining mapping and sd modelling companion modeling involves the combined usually sequential use of both rpg s and abm where rpg is used to first create an engaging abstraction that can then foster more complex participant understanding and engagement in the use of abm social network analysis can be used in conjunction with cognitive and or behavioral measures as a means to assess how stakeholders views beliefs and practices co evolve with the relationships formed via the ongoing pm process doing so would involve measuring stakeholders ties with one another at various points in the pm process and collecting at the same points in time relevant cognitive and behavioral data such a combined tool kit could thus readily be used as means to evaluate the extent to which 1 the pm process has resulted in creating strengthening or improving stakeholder relations and 2 the process has created channels through which stakeholders can mutually influence and learn from one another the subsections below discuss some issues related to combining or using multiple methods and tools 3 5 1 interfacing qualitative and quantitative methods following the voinov and bousquet 2010 diagram we can point at another version of the generic framework in which the two big leaps in the process are stressed a first leap happens in the move from the conceptual qualitative phase of model development to the quantitative phase of model formalization and computer runs a second leap happens in the move back from quantitative analysis to qualitative interpretations for example in the simplified visualizations and communications that are essential for the delivery of model results and for their translation into policy and actions fig 11 this also somewhat resembles the modeling ladder in fig 10 we go from data and concepts and gradually attempt to make sense of them through various forms of reasoning and analyses bridging the gaps between qualitative and quantitative modeling remains a challenge and often disrupts the pm process quantitative models especially when they become quite complex are often built behind the scenes by experts and later on are reintroduced to the rest of the group a smoother transition between qualitative and quantitative phases is much needed but is yet to be achieved 3 5 2 interactive modelling for true engagement of stakeholders in the modeling process it is essential that models be transparent and easily modified and tuned according to the needs of the stakeholders they should be able to interact and make direct changes in the models as they use them and see the results of their changes almost instantly such direct interaction facilitates stakeholder understanding of complex physical processes this does not mean that for all cases the whole pm framework has to be applied and implemented usually only some parts may be exposed to stakeholders while there are many benefits of staging a full complete participatory modeling process in reality there often exists restrictions of time resources and needs that make it necessary to limit the pm approach to a partial implementation even then there is a lot that can be derived from the connection between models and stakeholders in the process following this perspective several modeling tools are popular in the pm community for their ease of use with interactive groups for example cormas an abm tool dedicated to natural resources management has been used for collective design of models and interactive simulation bommel et al 2016 here users can interact with a simulation by manipulating avatars the agents that represent them in the simulated word changing their environment and their behavior by modifying simple activity diagrams thus it is possible to collectively explore medium and long term scenarios to better understand how a desired situation may be reached similar functionality explains the success of stella an sd tool actively used by the mediated modeling approach van den belt 2004 more recently the use of direct dynamic visualizations of a system often in the form of interactive submersive graphics i e virtual reality mixed reality and 3d environments 3 3 some examples include interactive sandbox https www deltares nl en news interactive sandbox new interactive design tool for the direct visualisation of coastal interventions 3di water model http www 3di nu en international or flexible mesh https www deltares nl en software 3d interactive modelling using delft3d flexible mesh is an emerging technological trend that facilitates understanding of model outputs and stimulates stakeholder engagement in modifying and refining the models basco carrera et al 2017b example https www participatorymodeling org node 117 here also there are certain restrictions on the realism of the output generated virtual reality may overburden stakeholders with information and hide the important message of the modeling process voinov et al 2017 3 5 3 user interfaces better understanding of the interconnections among the social behavioral and material elements involved in the participatory modelling activities and how these interconnections influence the participatory modelling process and outcomes is necessary to fully understand the effectiveness of different combinations of methods theoretical insights from relevant scientific fields including cognitive science and user psychology could identify the characteristics of modelling techniques that fit into a particular pm activity for example golnam et al 2012 used cognitive fit theory to explain the cognitive capabilities required for building sd models and to determine the suitability of various problem structuring and conceptualization techniques to fit to these capabilities herrera et al 2016 developed an experimental framework to compare the effectiveness of 1 a single method group modelling process and 2 a multi method process combining strategic options development and analysis with computer simulations at promoting changes in participants mental models and achieving better negotiation outcomes shelley et al 2010 2011 studied how different tangible and mobile interfaces can help stakeholders state their preferences and values collectively design scenarios and make sense of simulation outputs and deliberate towards compromises there is much improvement still needed in the modeling interfaces that could make model formulation running testing and communication simpler for non expert stakeholders 4 conclusions there is much improvement yet to be made in how modeling methods are selected for pm projects there are many methods already available and choices are not simple in too many cases the selection process seems to be largely driven by the past experience of participants rather than by the particular needs of the project while logic tells us that this is probably not the best strategy we do not have much if any evidence that this is a bad thing to a large extent this is because there are almost no method comparisons documented for pm projects i e where one method was substituted by another and where results were meaningfully compared comparing across projects is difficult because each project is unique while the problems may be similar the stakeholders involved are always special and group dynamics are hard to reproduce there is also much subjectivity in how stakeholders perceive the outcomes of a pm process and how those processes are evaluated what may be a success for one stakeholder group may turn out to be a failure for another there are too many biases beliefs and values involved in any kind of comparison and evaluation done by stakeholder groups to assume that the evaluation is correct and universal post audits and other tools to assess longer term outcomes of pm driven actions decisions and provide feedbacks i e pm as a part of adaptive management and governance could certainly help but are still very rare the challenge here is that there are many confounding factors influencing the pm outcomes tied to the individual and collective characteristics and relationships within the project teams and these change over time as participants interact with each other and with the modeling approaches and tools at best we can attempt quasi experimental setups but controlled experiments are not possible despite this limitation evaluation of each pm study should become standard practice so that a body of knowledge can be built to inform new applications while all stages in the pm process assume possible iteration method and tool selection is crucial because there may be insufficient time or resources available to go back and do the whole pm process once again with another method the modeling method chosen depends upon but can also determine the types of data to be collected and processed while the hammer and nail syndrome always has a negative connotation past experience certainly matters and indeed it may not be bad to use a method that someone is most comfortable with besides as shown here there is often considerable overlap between some methods this only makes it harder to come up with a unique optimal choice with no strict rules or guidelines for method selection more consultation and access to past and present experience could help an inventory and systematic analysis of methods and tools can also provide a stronger basis for model selection and can narrow the array of choices we hope that the web portal introduced above https participatorymodeling org will engage pm practitioners enlist them in adding their knowledge and experiences to the web portal and will generally serve the community of practice interested in the development of better techniques for participatory collaborative modeling the web portal is a community led endeavor and we expect the users to identify what features and functions it should provide we will also need community suggestions regarding how to best manage and moderate the portal for example we would like to ensure that additional openness in project reporting and potentially ensuing critiques from the pm community do not discourage stakeholder groups from presenting their results and methods for public scrutiny and evaluation a good pm project should be open minded about the particular type of modeling methods to be employed itis often problematic to justify this to funders who usually want to know in advance everything about what will be done accomplished and how the pm process will unfold it takes a considerable amount of trust from funding agencies and clients to devote resources to processes that are vaguely defined and or to rely on the project team to provide or add expertise as a problem requires prell et al 2007 if it is harder to get funding for a multi method project such projects are likely to remain rare offering little help in convincing the funders about the benefits of adaptive modeling approaches it also may sometimes be difficult for funders and publication authors and editors to recognize that documenting pm failures is as important as documenting pm successes and indeed may be more important in advancing pm knowledge and best practices this is one more reason for the pm community to organize itself to recognize the advantages and disadvantages of the rich array of pm tools and methods that are already available to improve and innovate as needed and to educate itself as well as its funders and stakeholders on the factors that should be considered in the selection of pm methods and tools we hope that this paper and our creation of the https participatorymodeling org web site will help in this regard acknowledgements this work was supported by the national socio environmental synthesis center sesync under funding received from the national science foundation dbi 1052875 any use of trade product or firm names in this publication is for descriptive purposes only and does not imply endorsement by the u s government the authors thank toni lyn morelli bruce taggart and two other anonymous reviewers for helpful comments appendix a supplementary data the following is the supplementary data related to this article data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 08 028 
26318,ecological disturbances i e pests fires floods biological invasions etc are a critical challenge for natural resource managers land managers play a key role in altering the rate and extent of disturbance propagation ecological disturbances propagate across the landscape while management strategies propagate across social networks of managers here we use an agent based model to examine the joint diffusion of ecological disturbances and management strategies across a social ecological network accounting for the fundamental role of social ecological feedbacks we examine the management of a generic ecological disturbance as a function of different learning strategies and the social ecological network our approach provides a general scaffold that can be modified to examine a variety of processes in which both social and ecological flows propagate across a social ecological network our findings highlight the importance of full and accurate information to assess successful strategy limited clustering and alignment between the social and the ecological system keywords ecological disturbances environmental management abm learning social ecological networks 1 introduction ecological disturbances defined broadly to include invasive species agricultural pests fires floods urbanization and land use change affect biodiversity and are a fundamental challenge in our interconnected and fast changing world chapin 2009 pimentel 2011 the decisions of land managers play an important role in constraining or promoting the spread of these disturbances baird et al 2016 management decisions spread across an informational network via learning while ecological disturbances spread across the landscape while much research has examined these two processes in isolation relatively little has explored the simultaneous propagation of a disturbance and the management of that disturbance across a linked social ecological network rebaudo and dangles 2011 2015 yet examining these processes in isolation misses the important role of reciprocal feedbacks in these complex systems in this paper we propose to integrate the analysis of social ecological networks using the tools developed to analyze multiplex networks and agent based models designed to capture the fundamental characteristics of the decision making process as well as the ecological disturbance and the feedbacks between the two in general managers adopt practices that they view as better than available alternatives rogers 2003 managers can compare strategies using experiential or individual learning in which they conduct trials of different strategies and observe the results to inform their future decisions ghadim and pannelli 1999 individual learning however is constrained when outcomes are difficult to observe or delayed over long periods as is the case for example in perennial cropping systems or with catastrophic events that occur very infrequently or if the results from individual learning are unproductive giraldeau and beauchamp 1999 given the challenges of relying exclusively on individual learning managers also employ social learning they seek out and use information from peers baird et al 2016 isaac et al 2007 especially when unsatisfied with current strategies schlag 1998 when constrained by authority or when outcome uncertainty is high morgan et al 2012 managers rely on information transmitted through social networks to shape their decision making baggio and hillis 2016 baumgart getz et al 2012 bodin and crona 2009 bodin and tengö 2012 crona and bodin 2006 cumming 2016 kininmonth et al 2015 prokopy 2011 managers acquire knowledge via peer to peer interactions local and regional organizations or other managers with greater authority while formal or legal authority constrain decision making in important ways informal social networks are also critical particularly across fragmented jurisdictions or when formal power dynamics are less prominent cumming 2016 kininmonth et al 2015 managers learn socially from network neighbors using various strategies collins 2005 laland 2004 two important strategies include 1 a success bias where individuals preferentially copy the strategies of other successful individuals boyd and richerson 1988 schlag 1998 and 2 a conformist bias in which individuals are disproportionately likely to adopt the most common strategy within a population independent of the actual success of that strategy henrich and boyd 1998 some ecological disturbances are easily observable and their effects are immediate e g fires and floods however sometimes disturbances can be hard to detect and their effects can be cumulative yet catastrophic chades et al 2008 chadès et al 2011 mackenzie et al 2002 the management of ecological disturbances is further complicated by increased ecological and social fragmentation epanchin niell et al 2010 rebaudo and dangles 2011 sayles and baggio 2017a schoon et al 2014 due to the fact that ecological processes often unfold at a different spatial scale than social processes efficient management requires adoption of strategies at the appropriate ecological scale ager et al 2017 crowder et al 2006 cumming et al 2006 folke et al 2007 galaz et al 2008 sayles and baggio 2017b given the complexity of managing ecological disturbances empirical research can be productively complemented with computational models that explicitly couple social and ecological networks thereby examining the simultaneous diffusion of ecological disturbances and the strategies used to manage them this endeavor can benefit from the combined use of network theoretical tools agent based modeling and specific case studies that can illustrate and guide the dynamics presented in models a number of recent studies have examined transmission across networks connected via multiple types of relationship i e multiplex networks and the robustness of such multiplex networks to social and ecological perturbations baggio et al 2016 de domenico et al 2016 lima et al 2015 agent based models abms have been increasingly applied in social and ecological systems deangelis and mooij 2005 fischer et al 2013 rebaudo and dangles 2015 uncovering emergent properties of systems represented from the bottom up by agent behavior here we propose an integration of concepts relating to multiplex networks and agent based models in order to more realistically portray decision making processes on fragmented social ecological landscapes our analysis is designed to capture the important characteristics of both social and ecological processes of diffusion across a landscape allowing us to assess the influence of network structural properties on a generic disturbance while our model is highly abstract the general approach can be modified to more realistically apply to any number of processes in which multiple flows diffuse across a multiplex network we examine the management of ecological disturbances as a function of learning and the disturbance itself on a social ecological network our study aims to provide some theoretical insight into the relationship between learning on social networks and ecological disturbances propagating on a landscape we build an abm in which managers can adopt strategies to counter ecological disturbances at a specific cost managers make decisions based on information they acquire via individual or social learning through feedbacks from the ecological patch they are managing we proceed by first explaining the model in detail we then highlight our results providing insights into the relationship between network connectivity learning type and disturbance management the discussion and conclusion of the paper focus on these relationships within the wider context of ecological disturbances while we consider the primary contribution of this work to be developing a methodological approach for the study of coupled social ecological systems using an approach integrating agent based modeling and network analysis in the discussion we also highlight the importance of extending these findings that use an idealized landscape to real world management practices and data 2 methods in order to assess the relationship between social ecological network structure the spread of ecological disturbances and the adoption of management strategies we developed an agent based model comprised of n connected social and ecological agents i e a social ecological network social agents are able to adopt treatment strategies that stop the spread of the disturbance and cure affected ecological patches each social agent has the authority to manage exclusively one ecological patch ecological disturbances can propagate through the ecological landscape the model however can be easily extended to assess situations in which social agents are able to manage multiple ecological patches as well as cases in which multiple social agents are managing the same ecological patch 2 1 the ecological system we specify the ecological landscape as an unweighted and undirected network in which n e 100 patches i e ecological nodes are connected via geoproximity the ecological nodes are scattered randomly on a 2 dimensional grid edges are added connecting each pair of most closely connected edges first where closest refers to the euclidean distance between the patches on the grid until the pre defined number of edges e ee is reached as described baggio et al 2011 all patches are assigned a base utility y and at the beginning of each simulation 10 of patches are considered affected by a generic ecological disturbance the disturbance can propagate via edges that link the ecological patches patches can be either treated or untreated treated patches are protected from the disturbance while untreated patches always become disturbed if neighboring patches are disturbed in other words an ecological patch becomes disturbed if it is connected to a disturbed ecological patch and it is not treated once disturbed the utility of that patch decreases at each time step by a specific amount see supplementary material table s2 input values 2 2 the social system social agents are connected via an unweighted undirected network and represent n s 100 social nodes more specifically social agents are connected via 6 different network generating processes a matching the ecological connectivity scale match b randomly c representing a small world with rewiring probability i e p rew 0 2 or d small world with rewiring probability p rew 0 3 e representing a scale free network with low preferential attachment p pref 0 6 or f a scale free network representing high preferential attachment p pref 1 see also figure s7 each social agent is able to exclusively manage one patch agents are able to adopt a treatment at a specific cost agents make their adoption based on their payoff the type of learning they employ the feedback from the ecological patch they are managing and the information they acquire from their social network further each agent can misinterpret signals or commit judgment errors and adopt either strategy with probability mu payoff is given by utility affected by the ecological disturbance and whether a manager has adopted treatment or not π i t y i a i c l s where cls cost of adopting treatment a indicates whether treatment has been adopted or not and y t y t 1 if the ecological node is not disturbed and y t y t 1 e f f if the ecological node is disturbed where eff effect of disturbance on utility see supplementary material for the actual values used in the simulations agents employ either individual or social learning and can switch between the two types of learning when agents employ social learning they are either conformists they adopt the strategy adopted by the majority of their social neighbors or success biased imitators they adopt the strategy of the individual neighbor that is doing best agents use either conformist or success biased social learning they are not able to switch between the two agents who are socially isolated i e have no social connections always employ individual learning and adopt treatment using the following algorithm 1 an agent s i will average her payoff over the last mem time steps where mem memory of social agents 2 s i will check how many times a specific strategy treatment adoption nt 1 or no treatment adoption nt 0 has led to a payoff better or equal to the average payoff calculated in the previous step 3 the probability of choosing no treatment adoption pr0 i is given by the following equation pr0 i n t 0 n t 0 n t 1 hence the probability of treatment adoption is pr1 i 1 pr0 i if agents are connected to other agents via the social network they have the ability to choose whether they will learn individually or socially the choice between individual or social learning depends on the clarity of strategy success and the inherent preference of agents for individual or social learning as explained in the following algorithm 1 s i checks the number of mem times a specific strategy treatment adoption or no treatment adoption has led to a better or equal payoff compared to the average payoff calculated over the last mem times 2 each s i calculates if there is a clear winning strategy cw abs nt 1 nt 0 where nt 1 number of times treatment was successful and nt 0 number of times not using the treatment was successful 3 if cw threshold given by social agent preference for individual vs social learning i e parameter confid s i will employ individual learning otherwise it will use social learning if an agent uses social learning they can either employ success or conformist biased learning success biased learners will assess the payoffs and management strategies of k neighboring agents treatment adoption is then a function of the difference between the maximum payoff of the k neighbors and ones own payoff δπ δ π max π k π i where max π k maximum payoff of neighbors if all neighbors adopt the same strategy as ones own than δπ 0 the probability of switching to the strategy leading to the maximum payoff between neighbors equals to 1 1 e δ π conformist learners assess the strategies of k neighbors and choose not to treat their patch with probability pr0 which is a function of the number of neighbors that are not treating p r 0 i n 0 t h n 0 t h n 1 t h where th exponent of the function that determines the gradient of the probability function th 1 corresponds to a linear increase in the probability of not adopting and th 8 simulates a step function see salau et al 2012 for details on the role of the parameter th n 0 and n 1 represent the number of social neighbors that have not adopted n 0 or adopted n 1 treatment finally adoption of strategy pr1 i is given by 1 pr0 i feedbacks between the social and ecological systems occur in the form of general utility that a social agent receives from the ecological patch they are managing if the patch is disturbed their utility is reduced social agents tend to want to keep the level of utility or to increase it the ability of a system to successfully manage a disturbance is assessed here by analyzing the average percentage of individuals employing a specific type of learning during the course of the simulation run the percentage of individuals adopting treatment in the 100 time steps preceding the end of the simulation run and in the percentage of ecological patches that are disturbed when eradication does not occur for an in depth description of the model and the parameter values used in simulations please see the overview design and detail protocol odd grimm et al 2010 2006 presented in the supplementary material the model code and the odd are also available at https www comses net codebases 5502 releases 1 1 0 2 3 social ecological network analysis multiplex networks the relationship between structural properties of the underlying social ecological system adoption and eradication is assessed via multiplex network metrics to analyze the social ecological network as a multiplex network we need to consider the ns and ne agents as one single group of agents nse that are connected via two different types of edges social and ecological the ensemble of the social connections between the nse agents thus forms the social layer while the ensemble of ecological connections between the nse agents forms the ecological layer of the multiplex network we can analyze multiplex networks by calculating the adjacency tensor of the social ecological network sen the adjacency tensor can be thought of as a multi dimensional array for example a two dimensional array can be represented by a matrix where one needs to specify two indices i and j to uniquely identify an edge i e the matrix is a specific case of a rank 2 tensor to uniquely identify an edge in a multiplex network one needs to specify four different indices two to identify involved nodes and two to identify the involved layers the edge between node i in layer s and node j in layer e is uniquely identified in a rank 4 tensor whose components are indicated as m i j s e baggio et al 2016 de domenico et al 2013 kivelä et al 2014 mucha et al 2010 here we analyze four specific multiplex network metrics that reveal the structure of the underlying social ecological system average degree average local clustering coefficient global clustering coefficient and inter layer correlation 2 3 1 multiplex average degree the multiplex degree mpx degree identifies the average potential for social learning and propagation of ecological disturbances the multiplex degree is a result of the combination of social and ecological connectivity the average multiplex degree of node i is calculated as the average degree of node i in each layer here s social layer and e ecological layer k i k i s k i e and diviced by divided by nse de domenico et al 2013 2 3 2 multiplex clustering coefficient the multiplex clustering coefficient identifies the potential for the network to receive localized information from the ecological layer and the potential for learning based on neighboring nodes in the social layer here we assume that the ability of information and disturbance signals have the same possibility of remaining within the same layer or crossing layers following cozzo et al 2015 we calculate the global clustering coefficient mpx global cc and the average local clustering coefficient mpx local cc based on random walks where the probability of changing layers is equal to 0 5 thus signals have the same probability of crossing or remaining in the same layer it is important to notice that while the average local clustering coefficient by construct places more emphasis on low multiplex degree nodes the global clustering coefficient puts more emphasis on nodes with high multiplex degree 2 3 3 inter layer assortativity the inter layer assortativity can be calculated as the inter layer correlation of the node degrees of the two layers assortativity identifies the relationship between the potential for propagation of the ecological disturbances and the potential for adoption of strategies via social learning assortativity is a key property of multiplex networks nicosia and latora 2015 here we employ spearman pairwise correlation to assess the correlation between the degree of a node in the social layer and its degree in the ecological layer formally we calculate the inter layer correlation as follows see also baggio et al 2016 ρ s e p q 1 6 i 1 n r i s p r i e q n n 2 1 where p q node degree and r i s is the rank of node i in layer s see de domenico et al 2013 for in depth information on multiplex metrics 2 4 analyzing model output the model assesses ecological disturbances as a function of learning and the structural properties of the underlying social ecological network to analyze the complex interactions between learning disturbance and structural properties of the network we evaluate the effect of various model parameters by statistically analyzing our model output given that disturbance prevalence lies in the 0 1 interval where both 0 and 1 have a positive probability to be an actual outcome we follow papke and wooldrige papke and wooldridge 1996 and analyze the results via the following equation e y x f β x estimated via quasi maximum likelihood methods as explained in gourieroux et al 2016 here e y x is the expected prevalence of ecological disturbance expected disturbance β x is a vector of model variables as parameterized see input table in the supplementary material and f represents a logistic function f g e g 1 e g the model variables analyzed are proportions of success biased conformist and individual learners multiplex global clustering coefficient multiplex local clustering coefficient and assortativity we interact learner types 2 at a time to avoid perfect multicollinearity as the sum of the proportion of success biased conformist and individual learners 1 and multiplex network metrics in order to assess the relative importance of each model variable we assess average marginal effects of learning type frequency and network metrics in order to assess how their main effect on expected disturbance changes due to their interactions 3 results our main objective was to understand the relationship between adoption of treatment strategies learning social ecological networks and expected disturbance in the overall system there is a clear link between adoption and eradication of ecological disturbances on average adoption of treatment strategies is almost three times higher 2 8 when the disturbance is eradicated than when it is not table 1 treatment of disturbances and their eradication is a clear cut relationship however expected disturbance also depends directly and indirectly on learning types and the underlying structural properties of the social ecological network the results portrayed below graphically display the relationship between learning and expected disturbance as well as social ecological network metrics and expected disturbance in table s1 we report the parameter estimates for three models examining the network metrics and interactions between success biased and conformist learners sc success biased and individual learners si and conformist and individual learners ci figure s1 to s3 report further analysis of the pairwise relationship between interacting variables and their effect on expected disturbances i e how the coefficient changes following we report how expected disturbance prevalence changes depending on learning and structural properties of the social ecological network we report results for the sc model si when individual learners are involved as there is no qualitative difference in the multiplex metrics interactions between the three models see figure s4 and s5 3 1 learning as the proportion of success biased learners approaches 1 expected disturbance decreases i e when 90 of the agents employ success biased learning on average expected disturbance 0 25 on the other hand if conformist learners or individual learners represent more than 60 of the population on average expected disturbance 0 70 0 64 o r 0 70 0 48 depending on the increased number of conformist or individual learners respectively fig 1 a and b fig 1c reiterates the importance of success biased learners as expected disturbance is minimized when both the proportions of conformist and individual learners are 0 1 3 2 social ecological networks the relationship between the structural properties of the underlying social ecological network and the ability of the system to reduce the ecological disturbance is dependent on the general connectivity of the overall social ecological network how it is clustered and whether the social and the ecological layer display inter layer assortativity on average increases in average multiplex degree reduce expected disturbance fig 2 a b and 2d on the other hand increased clustering in particular local clustering increases expected disturbance fig 2a b 2c 2e 2f the difference between multiplex global and local clustering is better observed together with multiplex degree fig 2a and b a minimum level of global clustering is necessary for lowering expected disturbance in fact if global clustering is 0 15 expected disturbance is expected to be around 0 5 further reduction occurs when global clustering increases local clustering always increases expected disturbance fig 2b clearly shows that as local clustering increases expected disturbance also increases at high levels of local clustering degree only reduces expected disturbance if it is very high fig 1 and fig 2 highlight the importance of success biased learners fig 1a and b and increased levels of average multiplex degree fig 2a b and 2c and assortativity 2d 2e and 2f for reducing expected disturbance on the other hand we have observed how individual and conformist learners fig 1 as well as local clustering constrain the reduction of expected disturbance fig 2f b and 2c at the same time while global clustering has a negligible effect at higher degree levels fig 2a it clearly influences the effect of assortativity fig 2e 3 3 learning and social ecological networks given the results so far it is important to disentangle the effect of learning and social ecological network metrics on expected disturbance an increased frequency of success biased learners fig 3 a d is associated with a reduction in expected disturbance further a higher proportion of success biased learners decreases the need for higher average multiplex degree or assortativity fig 3a and d however success biased learners are negatively affected by global and local clustering in fact clustering reduces the effect that success biased learners have on reducing expected disturbance fig 3b and c conformist learners however do not have a strong effect on disease prevalence across changes in proportions of conformist learners the main drivers of expected disturbance are the social ecological network metrics fig 3e through h we find similar patterns for individual learners fig 3i through l with some important exceptions individual learners benefit from increases in local clustering and are negatively affected by increases in assortativity individual learners thus seem to reduce expected disturbance at higher levels of clustering and lower levels of scale matching across the social and ecological sub systems 4 discussion when managers adopt the treatment they eliminate the disturbance in the area they are managing thereby decreasing the probability of transmission and reducing the overall prevalence of the ecological disturbance while straightforward this result highlights the importance of understanding the structural and institutional conditions that promote action on the part of managers our model allows us to examine the network structures and learning strategies that promote adoption success biased learners are critical for the reduction of ecological disturbances however this depends on agents being able to access more information than any other type of learner while all agents have information about their own behavior and payoffs conformist learners also have information about the behaviors of other individuals in their network and success biased learners have information about both the behaviors and payoffs of other individuals in their network interestingly the additional information about others behaviors actually harms conformist learners in that they are less able to control ecological disturbances conversely the additional information improves the decisions of success biased learners this finding is consistent with theoretical expectations that predict a mix of conformity and success biased learning produces adaptive outcomes boyd and richerson 1988 conformity alone however can lead to systematic maladaptation with groups of individuals converging on the wrong behavioral outcome these findings are also consistent with empirical evidence that individuals use success biased learning to guide their own behavioral decisions in the real world for example experiments have shown that people employ success biased and conformist learning strategies as predicted but that a success bias is often preferred to conformity and leads to better outcomes mcelreath et al 2008 mesoudi 2011 morgan et al 2012 the structure of the underlying social ecological network influences disturbance prevalence in important ways specifically increased assortativity associated with social ecological scale match multiplex degree and global clustering all result in reduced disturbance prevalence on the other hand local clustering increases the prevalence of ecological disturbances the important role of aligning structures across the social and ecological sub systems is well known bodin et al 2016 sayles and baggio 2017a treml et al 2015 our results reiterate that when the connectivity of the social system adequately reflects that of the ecological system ecological disturbances can be reduced and eradicated however scale matching alone is not sufficient for effective management the overall connectivity of the system is also important as shown by the relationship between multiplex global clustering average degree and disturbance levels on the other hand local clustering hinders the system s ability to reduce disturbances the role of local clustering is related to the existence of close knit communities and in the case of a multiplex network is related to a redundancy in information sharing and probability of disturbance propagation hence why local clustering increases disturbance prevalence bodin 2017 according to our findings close knit communities redundancy in information sharing and scale mismatches provide conditions in which individual learning can reduce ecological disturbances we can speculate that managers are better off trying new strategies when knowledge is redundant due to the closeness of the community this interaction between social ecological connectivity and learning strategy is an important area of potential further exploration for example hillis et al 2017 posit that in the preventive management of trunk diseases among perennial crop farmers an inability to engage in success biased learning limits the adoption of preventive disease controls the reason for the inability lies in the long latent period several years between infection and symptom expression in the trunk disease complex this latent period effectively disassociates plant health from management behavior at least temporarily in the sense that farmers who are not preventively managing their vines may still have productive yields the mismatch in temporal scales at which ecological mechanisms and management actions unfold prevents farmers from effectively engaging in a success bias by observing both the payoffs and strategies of their neighbors consequently this real world system exhibits high rates of ecological disturbance hillis et al 2016 analogous to our model findings when farmers are unable to correctly imitate successful strategies our findings generate a number of insights that inform outreach and policy intended to promote action on the part of land managers the model underscores the importance of the structural properties of the social ecological system because degrees of ecological connectivity vary substantially across disturbance types and ecosystems understanding the social structures and learning strategies best suited to promote effective management in that ecological setting is critical in other words understanding the relationship between social ecological connectivity and learning is key to promoting effective management strategies that can control ecological disturbances these results underscore the importance of promoting social connections among individuals with the important caveat that connections in and of themselves are not necessarily enough to promote appropriate action valente 2012 connections that allow for success biased imitation or those that allow individuals to observe both the behavior and payoffs of their decisions are most likely to effectively promote adoption policies that promote the sharing of information about both behaviors and their consequences are more likely to be effective than those that share merely information about the most common strategies this type of information sharing often the result of collaborative approaches can be a challenge in environments that are inherently competitive such as in the case of firms competing in a particular industry as a substitute for or complement to facilitating success biased learning decision makers might reduce the clustering of the overall system by reducing ecological connectivity and at the same time improve alignments in spatial scale such that highly connected ecological areas are managed by highly connected managers 5 conclusion there has been considerable progress in the past few years in understanding dynamic processes on multiplex networks and the robustness of multiplex networks to specific disturbances baggio et al 2016 de domenico et al 2014 granell et al 2013 lima et al 2015 most studies addressing structural properties of social ecological systems focus either on understanding how social networks or their origin influence the management and policies affecting the ecological system berardo and scholz 2010 bodin et al 2006 lubell et al 2014 sayles and baggio 2017b schoon et al 2017 vignola et al 2013 or employ a network perspective to examine and identify spatial scale mismatches existing in social ecological systems ernstson et al 2010 guerrero et al 2013 sayles and baggio 2017a treml et al 2015 research on how social ecological network structural properties influence the ability of social ecological systems to adapt and transform is still in its infancy bodin 2017 here we developed an agent based model to address the relationship between learning social ecological structural properties and the adoption of treatment strategies that counter ecological disturbances while our model is relatively abstract and general it provides some basic insights into the interactions between the relationship between social and ecological processes and how structural properties may come into play the framework we use to integrate social and ecological process propagating on a social ecological network can be modified and extended to examine specific questions or to assess particular empirical patterns observed in specific case studies while any number of extensions are possible we focus on three here that we believe are particularly promising in part because they move our model towards empirical realism first our model does not consider spatial heterogeneity either in disturbance transmission or management efficacy yet this type of environmental variation across the landscape is often a natural and important part of real world systems second our model assumes negative perturbations being transmitted across the ecological network and management strategies are thus aimed at reducing the overall ecological connectivity however in future work this assumption might be relaxed in order to account for beneficial ecological factors flowing across the network as species migration pollination ecosystem flows like water quality and quantity or nutrients in this latter context management strategies would aim at increasing the overall ecological connectivity to favor the diffusion of ecological processes see also schoon et al 2014 third while we don t vary the costs and benefits of adoption these are undoubtedly important in particular with regards to changing the nature of the decision such that it embodies a social dilemma because we expect motivations and processes of learning to differ in important ways in the strategic environment of a social dilemma we expect that we might observe important differences in the relationship between learning and reduction in disturbance prevalence in those environments these suggestions underscore the fact that our modeling framework can be parameterized in ways that more closely represent specific real world study systems in order to examine the relationship between structures and processes in those particular systems we present this model as an important preliminary step in representing the relationships between learning and decision making in linked social ecological systems while many further complexities await formalization we provide a framework for other modelers interested in representing explicitly the dynamic nature of both the social and ecological systems funding this work was conducted as a part of the evolution of sustainability working group at the national institute for mathematical and biological synthesis sponsored by the national science foundation through nsf award dbi 1300426 with additional support from the university of tennessee knoxville jb also acknowledges support from the national science foundation award aci 1639529 and award sma 1620457 and the u s department of agriculture national institute of food and agriculture award 2017 67019 26290 ethics statement no human or animal data were collected for this study data code and materials the model protocol is available within the electronic supplementary material the model code is available at https www openabm org model 5502 version 1 view conflicts of interest the authors declare no conflict of interest authors contributions jb designed and implemented the model jb designed the analysis and analyzed the results jb and vh conceived the study interpreted the results and wrote the paper acknowledgments the authors thank the evolution of sustainability working group at the national institute for mathematical and biological synthesis for comments and suggestions during the preparation of this manuscript appendix a supplementary data the following are the supplementary data related to this article supplmaterial final supplmaterial final data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 08 002 
26318,ecological disturbances i e pests fires floods biological invasions etc are a critical challenge for natural resource managers land managers play a key role in altering the rate and extent of disturbance propagation ecological disturbances propagate across the landscape while management strategies propagate across social networks of managers here we use an agent based model to examine the joint diffusion of ecological disturbances and management strategies across a social ecological network accounting for the fundamental role of social ecological feedbacks we examine the management of a generic ecological disturbance as a function of different learning strategies and the social ecological network our approach provides a general scaffold that can be modified to examine a variety of processes in which both social and ecological flows propagate across a social ecological network our findings highlight the importance of full and accurate information to assess successful strategy limited clustering and alignment between the social and the ecological system keywords ecological disturbances environmental management abm learning social ecological networks 1 introduction ecological disturbances defined broadly to include invasive species agricultural pests fires floods urbanization and land use change affect biodiversity and are a fundamental challenge in our interconnected and fast changing world chapin 2009 pimentel 2011 the decisions of land managers play an important role in constraining or promoting the spread of these disturbances baird et al 2016 management decisions spread across an informational network via learning while ecological disturbances spread across the landscape while much research has examined these two processes in isolation relatively little has explored the simultaneous propagation of a disturbance and the management of that disturbance across a linked social ecological network rebaudo and dangles 2011 2015 yet examining these processes in isolation misses the important role of reciprocal feedbacks in these complex systems in this paper we propose to integrate the analysis of social ecological networks using the tools developed to analyze multiplex networks and agent based models designed to capture the fundamental characteristics of the decision making process as well as the ecological disturbance and the feedbacks between the two in general managers adopt practices that they view as better than available alternatives rogers 2003 managers can compare strategies using experiential or individual learning in which they conduct trials of different strategies and observe the results to inform their future decisions ghadim and pannelli 1999 individual learning however is constrained when outcomes are difficult to observe or delayed over long periods as is the case for example in perennial cropping systems or with catastrophic events that occur very infrequently or if the results from individual learning are unproductive giraldeau and beauchamp 1999 given the challenges of relying exclusively on individual learning managers also employ social learning they seek out and use information from peers baird et al 2016 isaac et al 2007 especially when unsatisfied with current strategies schlag 1998 when constrained by authority or when outcome uncertainty is high morgan et al 2012 managers rely on information transmitted through social networks to shape their decision making baggio and hillis 2016 baumgart getz et al 2012 bodin and crona 2009 bodin and tengö 2012 crona and bodin 2006 cumming 2016 kininmonth et al 2015 prokopy 2011 managers acquire knowledge via peer to peer interactions local and regional organizations or other managers with greater authority while formal or legal authority constrain decision making in important ways informal social networks are also critical particularly across fragmented jurisdictions or when formal power dynamics are less prominent cumming 2016 kininmonth et al 2015 managers learn socially from network neighbors using various strategies collins 2005 laland 2004 two important strategies include 1 a success bias where individuals preferentially copy the strategies of other successful individuals boyd and richerson 1988 schlag 1998 and 2 a conformist bias in which individuals are disproportionately likely to adopt the most common strategy within a population independent of the actual success of that strategy henrich and boyd 1998 some ecological disturbances are easily observable and their effects are immediate e g fires and floods however sometimes disturbances can be hard to detect and their effects can be cumulative yet catastrophic chades et al 2008 chadès et al 2011 mackenzie et al 2002 the management of ecological disturbances is further complicated by increased ecological and social fragmentation epanchin niell et al 2010 rebaudo and dangles 2011 sayles and baggio 2017a schoon et al 2014 due to the fact that ecological processes often unfold at a different spatial scale than social processes efficient management requires adoption of strategies at the appropriate ecological scale ager et al 2017 crowder et al 2006 cumming et al 2006 folke et al 2007 galaz et al 2008 sayles and baggio 2017b given the complexity of managing ecological disturbances empirical research can be productively complemented with computational models that explicitly couple social and ecological networks thereby examining the simultaneous diffusion of ecological disturbances and the strategies used to manage them this endeavor can benefit from the combined use of network theoretical tools agent based modeling and specific case studies that can illustrate and guide the dynamics presented in models a number of recent studies have examined transmission across networks connected via multiple types of relationship i e multiplex networks and the robustness of such multiplex networks to social and ecological perturbations baggio et al 2016 de domenico et al 2016 lima et al 2015 agent based models abms have been increasingly applied in social and ecological systems deangelis and mooij 2005 fischer et al 2013 rebaudo and dangles 2015 uncovering emergent properties of systems represented from the bottom up by agent behavior here we propose an integration of concepts relating to multiplex networks and agent based models in order to more realistically portray decision making processes on fragmented social ecological landscapes our analysis is designed to capture the important characteristics of both social and ecological processes of diffusion across a landscape allowing us to assess the influence of network structural properties on a generic disturbance while our model is highly abstract the general approach can be modified to more realistically apply to any number of processes in which multiple flows diffuse across a multiplex network we examine the management of ecological disturbances as a function of learning and the disturbance itself on a social ecological network our study aims to provide some theoretical insight into the relationship between learning on social networks and ecological disturbances propagating on a landscape we build an abm in which managers can adopt strategies to counter ecological disturbances at a specific cost managers make decisions based on information they acquire via individual or social learning through feedbacks from the ecological patch they are managing we proceed by first explaining the model in detail we then highlight our results providing insights into the relationship between network connectivity learning type and disturbance management the discussion and conclusion of the paper focus on these relationships within the wider context of ecological disturbances while we consider the primary contribution of this work to be developing a methodological approach for the study of coupled social ecological systems using an approach integrating agent based modeling and network analysis in the discussion we also highlight the importance of extending these findings that use an idealized landscape to real world management practices and data 2 methods in order to assess the relationship between social ecological network structure the spread of ecological disturbances and the adoption of management strategies we developed an agent based model comprised of n connected social and ecological agents i e a social ecological network social agents are able to adopt treatment strategies that stop the spread of the disturbance and cure affected ecological patches each social agent has the authority to manage exclusively one ecological patch ecological disturbances can propagate through the ecological landscape the model however can be easily extended to assess situations in which social agents are able to manage multiple ecological patches as well as cases in which multiple social agents are managing the same ecological patch 2 1 the ecological system we specify the ecological landscape as an unweighted and undirected network in which n e 100 patches i e ecological nodes are connected via geoproximity the ecological nodes are scattered randomly on a 2 dimensional grid edges are added connecting each pair of most closely connected edges first where closest refers to the euclidean distance between the patches on the grid until the pre defined number of edges e ee is reached as described baggio et al 2011 all patches are assigned a base utility y and at the beginning of each simulation 10 of patches are considered affected by a generic ecological disturbance the disturbance can propagate via edges that link the ecological patches patches can be either treated or untreated treated patches are protected from the disturbance while untreated patches always become disturbed if neighboring patches are disturbed in other words an ecological patch becomes disturbed if it is connected to a disturbed ecological patch and it is not treated once disturbed the utility of that patch decreases at each time step by a specific amount see supplementary material table s2 input values 2 2 the social system social agents are connected via an unweighted undirected network and represent n s 100 social nodes more specifically social agents are connected via 6 different network generating processes a matching the ecological connectivity scale match b randomly c representing a small world with rewiring probability i e p rew 0 2 or d small world with rewiring probability p rew 0 3 e representing a scale free network with low preferential attachment p pref 0 6 or f a scale free network representing high preferential attachment p pref 1 see also figure s7 each social agent is able to exclusively manage one patch agents are able to adopt a treatment at a specific cost agents make their adoption based on their payoff the type of learning they employ the feedback from the ecological patch they are managing and the information they acquire from their social network further each agent can misinterpret signals or commit judgment errors and adopt either strategy with probability mu payoff is given by utility affected by the ecological disturbance and whether a manager has adopted treatment or not π i t y i a i c l s where cls cost of adopting treatment a indicates whether treatment has been adopted or not and y t y t 1 if the ecological node is not disturbed and y t y t 1 e f f if the ecological node is disturbed where eff effect of disturbance on utility see supplementary material for the actual values used in the simulations agents employ either individual or social learning and can switch between the two types of learning when agents employ social learning they are either conformists they adopt the strategy adopted by the majority of their social neighbors or success biased imitators they adopt the strategy of the individual neighbor that is doing best agents use either conformist or success biased social learning they are not able to switch between the two agents who are socially isolated i e have no social connections always employ individual learning and adopt treatment using the following algorithm 1 an agent s i will average her payoff over the last mem time steps where mem memory of social agents 2 s i will check how many times a specific strategy treatment adoption nt 1 or no treatment adoption nt 0 has led to a payoff better or equal to the average payoff calculated in the previous step 3 the probability of choosing no treatment adoption pr0 i is given by the following equation pr0 i n t 0 n t 0 n t 1 hence the probability of treatment adoption is pr1 i 1 pr0 i if agents are connected to other agents via the social network they have the ability to choose whether they will learn individually or socially the choice between individual or social learning depends on the clarity of strategy success and the inherent preference of agents for individual or social learning as explained in the following algorithm 1 s i checks the number of mem times a specific strategy treatment adoption or no treatment adoption has led to a better or equal payoff compared to the average payoff calculated over the last mem times 2 each s i calculates if there is a clear winning strategy cw abs nt 1 nt 0 where nt 1 number of times treatment was successful and nt 0 number of times not using the treatment was successful 3 if cw threshold given by social agent preference for individual vs social learning i e parameter confid s i will employ individual learning otherwise it will use social learning if an agent uses social learning they can either employ success or conformist biased learning success biased learners will assess the payoffs and management strategies of k neighboring agents treatment adoption is then a function of the difference between the maximum payoff of the k neighbors and ones own payoff δπ δ π max π k π i where max π k maximum payoff of neighbors if all neighbors adopt the same strategy as ones own than δπ 0 the probability of switching to the strategy leading to the maximum payoff between neighbors equals to 1 1 e δ π conformist learners assess the strategies of k neighbors and choose not to treat their patch with probability pr0 which is a function of the number of neighbors that are not treating p r 0 i n 0 t h n 0 t h n 1 t h where th exponent of the function that determines the gradient of the probability function th 1 corresponds to a linear increase in the probability of not adopting and th 8 simulates a step function see salau et al 2012 for details on the role of the parameter th n 0 and n 1 represent the number of social neighbors that have not adopted n 0 or adopted n 1 treatment finally adoption of strategy pr1 i is given by 1 pr0 i feedbacks between the social and ecological systems occur in the form of general utility that a social agent receives from the ecological patch they are managing if the patch is disturbed their utility is reduced social agents tend to want to keep the level of utility or to increase it the ability of a system to successfully manage a disturbance is assessed here by analyzing the average percentage of individuals employing a specific type of learning during the course of the simulation run the percentage of individuals adopting treatment in the 100 time steps preceding the end of the simulation run and in the percentage of ecological patches that are disturbed when eradication does not occur for an in depth description of the model and the parameter values used in simulations please see the overview design and detail protocol odd grimm et al 2010 2006 presented in the supplementary material the model code and the odd are also available at https www comses net codebases 5502 releases 1 1 0 2 3 social ecological network analysis multiplex networks the relationship between structural properties of the underlying social ecological system adoption and eradication is assessed via multiplex network metrics to analyze the social ecological network as a multiplex network we need to consider the ns and ne agents as one single group of agents nse that are connected via two different types of edges social and ecological the ensemble of the social connections between the nse agents thus forms the social layer while the ensemble of ecological connections between the nse agents forms the ecological layer of the multiplex network we can analyze multiplex networks by calculating the adjacency tensor of the social ecological network sen the adjacency tensor can be thought of as a multi dimensional array for example a two dimensional array can be represented by a matrix where one needs to specify two indices i and j to uniquely identify an edge i e the matrix is a specific case of a rank 2 tensor to uniquely identify an edge in a multiplex network one needs to specify four different indices two to identify involved nodes and two to identify the involved layers the edge between node i in layer s and node j in layer e is uniquely identified in a rank 4 tensor whose components are indicated as m i j s e baggio et al 2016 de domenico et al 2013 kivelä et al 2014 mucha et al 2010 here we analyze four specific multiplex network metrics that reveal the structure of the underlying social ecological system average degree average local clustering coefficient global clustering coefficient and inter layer correlation 2 3 1 multiplex average degree the multiplex degree mpx degree identifies the average potential for social learning and propagation of ecological disturbances the multiplex degree is a result of the combination of social and ecological connectivity the average multiplex degree of node i is calculated as the average degree of node i in each layer here s social layer and e ecological layer k i k i s k i e and diviced by divided by nse de domenico et al 2013 2 3 2 multiplex clustering coefficient the multiplex clustering coefficient identifies the potential for the network to receive localized information from the ecological layer and the potential for learning based on neighboring nodes in the social layer here we assume that the ability of information and disturbance signals have the same possibility of remaining within the same layer or crossing layers following cozzo et al 2015 we calculate the global clustering coefficient mpx global cc and the average local clustering coefficient mpx local cc based on random walks where the probability of changing layers is equal to 0 5 thus signals have the same probability of crossing or remaining in the same layer it is important to notice that while the average local clustering coefficient by construct places more emphasis on low multiplex degree nodes the global clustering coefficient puts more emphasis on nodes with high multiplex degree 2 3 3 inter layer assortativity the inter layer assortativity can be calculated as the inter layer correlation of the node degrees of the two layers assortativity identifies the relationship between the potential for propagation of the ecological disturbances and the potential for adoption of strategies via social learning assortativity is a key property of multiplex networks nicosia and latora 2015 here we employ spearman pairwise correlation to assess the correlation between the degree of a node in the social layer and its degree in the ecological layer formally we calculate the inter layer correlation as follows see also baggio et al 2016 ρ s e p q 1 6 i 1 n r i s p r i e q n n 2 1 where p q node degree and r i s is the rank of node i in layer s see de domenico et al 2013 for in depth information on multiplex metrics 2 4 analyzing model output the model assesses ecological disturbances as a function of learning and the structural properties of the underlying social ecological network to analyze the complex interactions between learning disturbance and structural properties of the network we evaluate the effect of various model parameters by statistically analyzing our model output given that disturbance prevalence lies in the 0 1 interval where both 0 and 1 have a positive probability to be an actual outcome we follow papke and wooldrige papke and wooldridge 1996 and analyze the results via the following equation e y x f β x estimated via quasi maximum likelihood methods as explained in gourieroux et al 2016 here e y x is the expected prevalence of ecological disturbance expected disturbance β x is a vector of model variables as parameterized see input table in the supplementary material and f represents a logistic function f g e g 1 e g the model variables analyzed are proportions of success biased conformist and individual learners multiplex global clustering coefficient multiplex local clustering coefficient and assortativity we interact learner types 2 at a time to avoid perfect multicollinearity as the sum of the proportion of success biased conformist and individual learners 1 and multiplex network metrics in order to assess the relative importance of each model variable we assess average marginal effects of learning type frequency and network metrics in order to assess how their main effect on expected disturbance changes due to their interactions 3 results our main objective was to understand the relationship between adoption of treatment strategies learning social ecological networks and expected disturbance in the overall system there is a clear link between adoption and eradication of ecological disturbances on average adoption of treatment strategies is almost three times higher 2 8 when the disturbance is eradicated than when it is not table 1 treatment of disturbances and their eradication is a clear cut relationship however expected disturbance also depends directly and indirectly on learning types and the underlying structural properties of the social ecological network the results portrayed below graphically display the relationship between learning and expected disturbance as well as social ecological network metrics and expected disturbance in table s1 we report the parameter estimates for three models examining the network metrics and interactions between success biased and conformist learners sc success biased and individual learners si and conformist and individual learners ci figure s1 to s3 report further analysis of the pairwise relationship between interacting variables and their effect on expected disturbances i e how the coefficient changes following we report how expected disturbance prevalence changes depending on learning and structural properties of the social ecological network we report results for the sc model si when individual learners are involved as there is no qualitative difference in the multiplex metrics interactions between the three models see figure s4 and s5 3 1 learning as the proportion of success biased learners approaches 1 expected disturbance decreases i e when 90 of the agents employ success biased learning on average expected disturbance 0 25 on the other hand if conformist learners or individual learners represent more than 60 of the population on average expected disturbance 0 70 0 64 o r 0 70 0 48 depending on the increased number of conformist or individual learners respectively fig 1 a and b fig 1c reiterates the importance of success biased learners as expected disturbance is minimized when both the proportions of conformist and individual learners are 0 1 3 2 social ecological networks the relationship between the structural properties of the underlying social ecological network and the ability of the system to reduce the ecological disturbance is dependent on the general connectivity of the overall social ecological network how it is clustered and whether the social and the ecological layer display inter layer assortativity on average increases in average multiplex degree reduce expected disturbance fig 2 a b and 2d on the other hand increased clustering in particular local clustering increases expected disturbance fig 2a b 2c 2e 2f the difference between multiplex global and local clustering is better observed together with multiplex degree fig 2a and b a minimum level of global clustering is necessary for lowering expected disturbance in fact if global clustering is 0 15 expected disturbance is expected to be around 0 5 further reduction occurs when global clustering increases local clustering always increases expected disturbance fig 2b clearly shows that as local clustering increases expected disturbance also increases at high levels of local clustering degree only reduces expected disturbance if it is very high fig 1 and fig 2 highlight the importance of success biased learners fig 1a and b and increased levels of average multiplex degree fig 2a b and 2c and assortativity 2d 2e and 2f for reducing expected disturbance on the other hand we have observed how individual and conformist learners fig 1 as well as local clustering constrain the reduction of expected disturbance fig 2f b and 2c at the same time while global clustering has a negligible effect at higher degree levels fig 2a it clearly influences the effect of assortativity fig 2e 3 3 learning and social ecological networks given the results so far it is important to disentangle the effect of learning and social ecological network metrics on expected disturbance an increased frequency of success biased learners fig 3 a d is associated with a reduction in expected disturbance further a higher proportion of success biased learners decreases the need for higher average multiplex degree or assortativity fig 3a and d however success biased learners are negatively affected by global and local clustering in fact clustering reduces the effect that success biased learners have on reducing expected disturbance fig 3b and c conformist learners however do not have a strong effect on disease prevalence across changes in proportions of conformist learners the main drivers of expected disturbance are the social ecological network metrics fig 3e through h we find similar patterns for individual learners fig 3i through l with some important exceptions individual learners benefit from increases in local clustering and are negatively affected by increases in assortativity individual learners thus seem to reduce expected disturbance at higher levels of clustering and lower levels of scale matching across the social and ecological sub systems 4 discussion when managers adopt the treatment they eliminate the disturbance in the area they are managing thereby decreasing the probability of transmission and reducing the overall prevalence of the ecological disturbance while straightforward this result highlights the importance of understanding the structural and institutional conditions that promote action on the part of managers our model allows us to examine the network structures and learning strategies that promote adoption success biased learners are critical for the reduction of ecological disturbances however this depends on agents being able to access more information than any other type of learner while all agents have information about their own behavior and payoffs conformist learners also have information about the behaviors of other individuals in their network and success biased learners have information about both the behaviors and payoffs of other individuals in their network interestingly the additional information about others behaviors actually harms conformist learners in that they are less able to control ecological disturbances conversely the additional information improves the decisions of success biased learners this finding is consistent with theoretical expectations that predict a mix of conformity and success biased learning produces adaptive outcomes boyd and richerson 1988 conformity alone however can lead to systematic maladaptation with groups of individuals converging on the wrong behavioral outcome these findings are also consistent with empirical evidence that individuals use success biased learning to guide their own behavioral decisions in the real world for example experiments have shown that people employ success biased and conformist learning strategies as predicted but that a success bias is often preferred to conformity and leads to better outcomes mcelreath et al 2008 mesoudi 2011 morgan et al 2012 the structure of the underlying social ecological network influences disturbance prevalence in important ways specifically increased assortativity associated with social ecological scale match multiplex degree and global clustering all result in reduced disturbance prevalence on the other hand local clustering increases the prevalence of ecological disturbances the important role of aligning structures across the social and ecological sub systems is well known bodin et al 2016 sayles and baggio 2017a treml et al 2015 our results reiterate that when the connectivity of the social system adequately reflects that of the ecological system ecological disturbances can be reduced and eradicated however scale matching alone is not sufficient for effective management the overall connectivity of the system is also important as shown by the relationship between multiplex global clustering average degree and disturbance levels on the other hand local clustering hinders the system s ability to reduce disturbances the role of local clustering is related to the existence of close knit communities and in the case of a multiplex network is related to a redundancy in information sharing and probability of disturbance propagation hence why local clustering increases disturbance prevalence bodin 2017 according to our findings close knit communities redundancy in information sharing and scale mismatches provide conditions in which individual learning can reduce ecological disturbances we can speculate that managers are better off trying new strategies when knowledge is redundant due to the closeness of the community this interaction between social ecological connectivity and learning strategy is an important area of potential further exploration for example hillis et al 2017 posit that in the preventive management of trunk diseases among perennial crop farmers an inability to engage in success biased learning limits the adoption of preventive disease controls the reason for the inability lies in the long latent period several years between infection and symptom expression in the trunk disease complex this latent period effectively disassociates plant health from management behavior at least temporarily in the sense that farmers who are not preventively managing their vines may still have productive yields the mismatch in temporal scales at which ecological mechanisms and management actions unfold prevents farmers from effectively engaging in a success bias by observing both the payoffs and strategies of their neighbors consequently this real world system exhibits high rates of ecological disturbance hillis et al 2016 analogous to our model findings when farmers are unable to correctly imitate successful strategies our findings generate a number of insights that inform outreach and policy intended to promote action on the part of land managers the model underscores the importance of the structural properties of the social ecological system because degrees of ecological connectivity vary substantially across disturbance types and ecosystems understanding the social structures and learning strategies best suited to promote effective management in that ecological setting is critical in other words understanding the relationship between social ecological connectivity and learning is key to promoting effective management strategies that can control ecological disturbances these results underscore the importance of promoting social connections among individuals with the important caveat that connections in and of themselves are not necessarily enough to promote appropriate action valente 2012 connections that allow for success biased imitation or those that allow individuals to observe both the behavior and payoffs of their decisions are most likely to effectively promote adoption policies that promote the sharing of information about both behaviors and their consequences are more likely to be effective than those that share merely information about the most common strategies this type of information sharing often the result of collaborative approaches can be a challenge in environments that are inherently competitive such as in the case of firms competing in a particular industry as a substitute for or complement to facilitating success biased learning decision makers might reduce the clustering of the overall system by reducing ecological connectivity and at the same time improve alignments in spatial scale such that highly connected ecological areas are managed by highly connected managers 5 conclusion there has been considerable progress in the past few years in understanding dynamic processes on multiplex networks and the robustness of multiplex networks to specific disturbances baggio et al 2016 de domenico et al 2014 granell et al 2013 lima et al 2015 most studies addressing structural properties of social ecological systems focus either on understanding how social networks or their origin influence the management and policies affecting the ecological system berardo and scholz 2010 bodin et al 2006 lubell et al 2014 sayles and baggio 2017b schoon et al 2017 vignola et al 2013 or employ a network perspective to examine and identify spatial scale mismatches existing in social ecological systems ernstson et al 2010 guerrero et al 2013 sayles and baggio 2017a treml et al 2015 research on how social ecological network structural properties influence the ability of social ecological systems to adapt and transform is still in its infancy bodin 2017 here we developed an agent based model to address the relationship between learning social ecological structural properties and the adoption of treatment strategies that counter ecological disturbances while our model is relatively abstract and general it provides some basic insights into the interactions between the relationship between social and ecological processes and how structural properties may come into play the framework we use to integrate social and ecological process propagating on a social ecological network can be modified and extended to examine specific questions or to assess particular empirical patterns observed in specific case studies while any number of extensions are possible we focus on three here that we believe are particularly promising in part because they move our model towards empirical realism first our model does not consider spatial heterogeneity either in disturbance transmission or management efficacy yet this type of environmental variation across the landscape is often a natural and important part of real world systems second our model assumes negative perturbations being transmitted across the ecological network and management strategies are thus aimed at reducing the overall ecological connectivity however in future work this assumption might be relaxed in order to account for beneficial ecological factors flowing across the network as species migration pollination ecosystem flows like water quality and quantity or nutrients in this latter context management strategies would aim at increasing the overall ecological connectivity to favor the diffusion of ecological processes see also schoon et al 2014 third while we don t vary the costs and benefits of adoption these are undoubtedly important in particular with regards to changing the nature of the decision such that it embodies a social dilemma because we expect motivations and processes of learning to differ in important ways in the strategic environment of a social dilemma we expect that we might observe important differences in the relationship between learning and reduction in disturbance prevalence in those environments these suggestions underscore the fact that our modeling framework can be parameterized in ways that more closely represent specific real world study systems in order to examine the relationship between structures and processes in those particular systems we present this model as an important preliminary step in representing the relationships between learning and decision making in linked social ecological systems while many further complexities await formalization we provide a framework for other modelers interested in representing explicitly the dynamic nature of both the social and ecological systems funding this work was conducted as a part of the evolution of sustainability working group at the national institute for mathematical and biological synthesis sponsored by the national science foundation through nsf award dbi 1300426 with additional support from the university of tennessee knoxville jb also acknowledges support from the national science foundation award aci 1639529 and award sma 1620457 and the u s department of agriculture national institute of food and agriculture award 2017 67019 26290 ethics statement no human or animal data were collected for this study data code and materials the model protocol is available within the electronic supplementary material the model code is available at https www openabm org model 5502 version 1 view conflicts of interest the authors declare no conflict of interest authors contributions jb designed and implemented the model jb designed the analysis and analyzed the results jb and vh conceived the study interpreted the results and wrote the paper acknowledgments the authors thank the evolution of sustainability working group at the national institute for mathematical and biological synthesis for comments and suggestions during the preparation of this manuscript appendix a supplementary data the following are the supplementary data related to this article supplmaterial final supplmaterial final data profile data profile appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 08 002 
26319,integrated hydrologic modeling ihm encompasses a vast number of processes and specifications variable in time and space and development of models can be arduous model input construction techniques have not been formalized or made easily reproducible creating the input files for integrated hydrologic models requires complex gis processing of raster and vector datasets from various sources developing stream network topology that is consistent with the model grid scale digital elevation model dem is important for robust simulation of surface water and groundwater exchanges distribution of meteorological data over the model domain is difficult in complex terrain at the model grid scale but is necessary for realistic simulations as model development requires extensive gis and computer programming expertise the use of ihms has mostly been limited to research groups with available financial human and technical resources here we present a series of open source python scripts that are combined with esri arcgis to provide a formalized technique for the parameterization and development of inputs for the readily available ihm called gsflow this python toolkit automates many of the necessary and laborious processes of parameterization including stream network development land coverages and meteorological distribution over the model domain the final products of the toolkit are prms ready parameter files along with several input parameters for a modflow model including input for the streamflow routing package a demonstration of the toolkit is provided to illustrate its capabilities program and computing requirements any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government mandatory software arcgis 10 1 or newer will include python 2 7 along with the arcpy package http pro arcgis com en pro app arcpy get started what is arcpy htm cascade routing tool crt executable version 1 3 1 http water usgs gov ogw crt gsflow 1 1 6 or newer http water usgs gov ogw gsflow downloads optional software usda soil data viewer http www nrcs usda gov wps portal nrcs detail soils survey geo cid nrcs142p2 053614 software availability version 1 release on github https github com gsflow gsflow arcpy software download includes a user guide and example model 1 introduction water resource managers and researchers require greater understanding of the connectivity between groundwater and surface water to effectively manage these resources integrated hydrologic models ihms can provide important information about water resources and are often used as decision support tools for resource management laniak et al 2013 construction of ihm inputs are not well formalized or automated making reproducibility very difficult a wider acceptance and distribution of process based approaches requires improved model visualization tools and streamlined approaches for model setup execution and analysis fatichi et al 2016 various techniques have been developed to create drainage networks but none have the benefit of a fully automated and topologically consistent approach that is applicable for simulating exchanges between streams and groundwater in grid based ihms applied to developed river basins the progression of ihms has spurred research and development for improving data processing and graphical user interfaces band 1986 gruber and peckham 2009 metz et al 2011 wilson 2012 tian et al 2016 bhatt et al 2008 ng et al 2018 advancements in gis technology and remote sensing have provided the ability to highly parameterize systems of interest but not without complex data processing metz et al 2011 jasiewicz and metz 2011 despite these advancements in gis data processing previous works have not focused on additional complexities related to simulating surface water and groundwater exchanges and input data development for fine resolution large scale hydrologic models is still a difficult problem that discourages and limits the use of ihms application of approaches strictly developed for surface networks to develop input for ihms can lead to erroneous input that compromises a model s usefulness clark et al 2015 just the near surface component of gsflow requires input data of approximately 50 global parameters and 35 model cell specific parameters that may be derived from large geospatial datasets markstrom et al 2008 creating input files for ihms requires complex gis processing of raster and vector datasets from various sources e g wilson 2012 viger and leavesley 2007 provide the basic steps for developing input parameters for a watershed model and their approach requires data management gis processing and relies upon proprietary software and scripting languages these tools are valuable but require a high level of process knowledge can require significant time investment and can be difficult to apply in large high resolution models bhatt et al 2008 presented software for developing input data for the ihm called pihm through integration with qgis providing input and output data processing tools qu and duffy 2007 presently automated software is not readily available that can be used to routinely and consistently generate input for gsflow tools and methods that are available require complicated and informal workflow processing on diverse geospatial datasets the tools being presented herein are intended to formalize the input data construction using a consistent approach automate and simplify data manipulation and data transfer support user intervention for customization and provide robust data input that is suitable for ihms this paper presents new software developed in the python programming language that can be used to automatically generate required data input for gsflow although these scripts are tailored to create data input formats required for gsflow the scripts could be adapted to develop input data for other ihms that rely on regular grids for spatial discretization for example these scripts could be used to automatically develop a consistent grid scale dem and stream network appropriate for simulating surface water and groundwater interactions and maps of climate land cover and land use distributed to each model grid cell a stream network must be congruent with the model grid used for groundwater simulation for robust and convergent calculation of hydraulic gradients between streams and groundwater deriving model input for large scale ihms that produce convergent solutions remains a challenging and time consuming process the software presented herein was developed to overcome these challenges and to provide an efficient reproducible and automated procedure for gsflow input data construction in the past ihm development has required extensive gis and computer programming expertise which has restricted the use of ihms in water resources management this toolkit called gsflow arcpy significantly simplifies gsflow model development thereby potentially expanding the benefits of this ihm to a broader user community gsflow arcpy automates many of the complicated and time consuming gis procedures necessary for parameterizing ihms including generating input representing the stream network land cover and meteorology distributions over complex terrains such as steep and varying topography or flat regions where it can be difficult to establish the locations of stream channels stream network development based on digital elevation models dems are needed to determine the paths of water sediment and contamination movement tarboton 1997 similar to topmodel beven and kirkby 1979 and hydrotel fortin et al 2001 gsflow arcpy uses a directional matrix and computes a topographic index to automatically determine drainage pathways unlike these previous studies that were solely focused on surface drainage the present work extends to the simulation of surface water and groundwater exchanges using equal area model grid cells for the groundwater model and correctly overlaying the streams onto the groundwater model cells streams that are incorrectly placed onto grid cells will cause numerical problems specifically the model grid scale dem and stream network are made consistent with respect to relative altitudes between cells containing streams cells adjacent to streams and the slope of cells following the streams this prevents inconsistencies between the model grid dem and the stream network that can lead to excessive surface water and groundwater exchanges excessive spring discharge rates and poor ihm convergence gsflow arcpy provides data input requirements for the groundwater model component of an ihm including the stream network and topology and the top altitude for model cells that are used to define altitudes for aquifer boundaries these groundwater parameters are developed at the same resolution as the model top grid additionally this toolkit provides the necessary data sets for creating input for the streamflow routing package which has been one of the greatest challenges for developing modflow and gsflow models additional processing outside of gsflow arcpy is required to generate the hydrogeologic framework used to characterize aquifer systems and the associated modflow input data there are many separate software tools available that can be used for the development of input data that characterize aquifer systems modflow has benefited from many options for automated model construction that has led to broad application around the world e g leapfrog hydro 2013 winston 2009 bakker et al 2016 these existing tools are readily available and can be used in concert with gsflow arcpy to provide capabilities to characterize aquifer systems using approaches more appropriately done outside the gis environment 2 background gsflow is an ihm developed by the usgs that integrates the watershed runoff model prms with the groundwater flow model modflow nwt markstrom et al 2008 2015 niswonger et al 2011 gsflow was developed for application to medium and large scale watersheds i e 10s 1000s of square kilometers and has primarily been used as a tool for water resources management gsflow has been applied to many basins around the world see for example gannet et al 2017 hassan et al 2014 and wu et al 2014 for examples from the us spain and china respectively and each model application has generally relied upon varying approaches for model input construction the difficulty in implementing required model input datasets for gsflow and other ihms has led to recent work to develop a consistent set of tools for model input construction primarily following the work of huntington and niswonger 2012 niswonger et al 2014 and rajagopal et al 2015 input data required for gsflow and other ihms are derived from diverse geospatial datasets available for the continental united states and other places across the globe these datasets are typically available through centralized data portals and can be downloaded freely to local computers for selected basins of interest gsflow arcpy was not designed to access geospatial datasets through web portals because these portals are not static and changes in data formats and storage locations make automated communication with web portals unreliable geospatial datasets must be downloaded and stored locally users may incorporate other datasets if available so long as the data are in an acceptable format i e raster files users can refer to the formats for the standard data sources to develop datasets that rely on non standard data sources gsflow arcpy develops the model input by reading geospatial datasets and making calculations using these data for each prms parameter and modflow dataset according to methods described by viger and leavesley 2007 markstrom et al 2008 markstrom et al 2015 harbaugh 2005 niswonger et al 2011 and henson et al 2013 most of the data required by gsflow arcpy can be downloaded for basins in the united states using the usda geospatial data gateway https gdg sc egov usda gov however some datasets must be obtained through regionally specific data sources such as local agencies that collect and maintain these datasets examples of regionally specific data are climate and streamflow data that often are collected and managed locally and these data must be obtained by regionally specific data collection efforts additionally other regionally specific data include driller s logs or geophysical datasets used to develop the hydrogeologic framework for the groundwater system the process of extracting the surface drainage network from a dem that is appropriate for ihms remains a difficult process in complex terrain despite well documented approaches bhatt et al 2008 peckham 1998 maidment 2002 daniels et al 2011 niu et al 2014 stream networks that are derived using previous approaches can potentially contain streams that float above and adjacent to stream canyons are overly incised below the model grid cell top altitude traverse uphill in the downstream direction or form loops with ambiguous flow directions previous approaches have relied on high resolution dems to generate stream networks as opposed to the method used by gsflow arcpy that applies a dem that has been up scaled to the model grid resolution furthermore no consideration is provided for constructed channels and canals that cannot be generated using topographic approaches for stream generation inconsistencies between a stream network and the model grid scale dem are problematic for ihms that explicitly route surface water and calculate exchanges between surface water and groundwater and spring discharge using hydraulic gradients dependent on the relative altitude between streams land surface and groundwater head prudic et al 2004 niswonger and prudic 2005 markstrom et al 2008 other errors in stream networks include dead ends common issue where ephemeral stream channels become obscured where they flow across alluvial plains misalignment relative to usgs national hydrography dataset nhd lines or aerial photos and incorrectly located confluences stream network issues also arise when streams are derived from fine scale dems in arid regions where stream channels are undefined or for dems that have not been properly conditioned to large grid scales in complex terrain drainage networks dem conditioning that can solve many of these problems include filling local swales that are artifacts of dem construction assuring that the grid cells have smooth and continuous downward slopes in the downstream direction confirming that grid cells that contain streams have lower altitudes relative to adjacent grid cells and creating and connecting intermittent stream reaches where channels are not well defined by topography in some cases dealing with these issues requires some level of subjectivity and the gsflow arcpy scripts provide the option for user intervention to support these special cases a model grid scale dem and surface water network are generated by gsflow arcpy using an iterative algorithm that conditions the model grid scale dem by filling local swales creating smoothly downward sloping streamlines and assuring that streams are not floating above or overly incised below model cell tops this approach uses the drainage networks provided by nhd as a guide however the final directional matrix and topographic index provided by gsflow arcpy is generated using model grid scale dem and flow accumulation and flow direction functions band 1986 maidment 2002 deriving the stream network from the model grid scale dem provides consistency of scale between streams and model grid cells that is appropriate for ihms however depending on the resolution of the model grid the location and altitude of streams may deviate from reality when using coarse grids 3 python scripts and the data development process developing a gsflow model using gsflow arcpy can be organized into a set of distinct steps input data are generated for both the prms and modflow components of gsflow the procedure is carried out by 13 separate scripts that handle various workflow procedures fig 1 these procedures include data retrieval and preparation model boundary delineation and horizontal discretization grid scale dem initialization grid scale dem fill and conditioning and surface water network delineation lateral overland and shallow subsurface flow cascade routing delineation land use vegetation and soil zone parameterization climate distribution parameterization and model input file construction each of these procedures represents a series of processes or commands that are carried out by running one or more python scripts that automate spatial data processing required for gsflow model input each script within gsflow arcpy reads input files from a local folder and performs the operations necessary to project transform calculate and format the data into an attribute table that is used to build the prms and modflow inputs along with the scripts a template configuration file field list file remap files template prms control file and two csv files that hold dimension and parameter settings are provided the configuration file guides the scripts to the correct folder locations and the csv files contain site specific settings the field list holds the names of each field column that will be created in the shapefile the remap files list unique ids to transfer remap vegetation or soil codes to prms values as the scripts are run folders and files appear in the working directory designated as the folder that contains the configuration file the parameter shapefile contains cell attributes and can be examined edited and displayed in arcmap after running each script to visualize and evaluate intermediate results the configuration file is a text file that acts as a guide for gsflow arcpy the purpose of the configuration file is to avoid having to change the python scripts for different models or model implementations each section of the configuration file applies to a certain script and provides file locations and model specific settings and flags the location of this file is considered the working directory that contains all folders and files created by the scripts several flags can be turned on or off in the configuration file depending on preferences of the user and project specifics the field list file field list ini comes with the package and contains a list of the parameter names that make up the attribute table of the shapefile generated by gsflow arcpy gsflow arcpy discretizes prms and or gsflow model areas using equal area square grid cells for gsflow arcpy models prms hydrologic response units hrus are coincident with modflow grid cells thus a single horizontal spatial discretization is used for both prms and all modflow layers spatial units are called hrus in prms and grid cells in modflow however gsflow arcpy refers to both prms and modflow spatial units as hrus when these spatial units are equal gsflow allows for different spatial discretization between prms and modflow and a user could create hrus that are larger or finer than the modflow grid cells with these scripts however this would require additional data handling outside the standard application of gsflow arcpy to first create gridded hrus and then separately create the modflow grid and stream network for this case additional parameters would be required by gsflow that are not created by gsflow arcpy to provide the connectivity between hrus markstrom et al 2008 previous testing of gsflow models indicate that maintaining congruent hrus and grid cells provides several advantages these advantages include ease of integration between the surface water and groundwater systems promoting model convergence and consistency by maintaining exchanges in water between storage reservoirs of similar volume and the ability to represent lateral convergent and divergent cascading flows according to variations in altitudes among gridded hrus gsflow arcpy uses a cascade routing tool crt henson et al 2013 to define lateral cascade pathways from hru to hru and hru to streams and lakes the crt provides reach and segment numbering and defines the cascades based on the model grid scale dem 4 example application gsflow arcpy is presented and demonstrated herein by applying the tools to develop the input datasets for a gsflow model of the russian river watershed located in northwestern california fig 2 the russian river traverses 177 km north to south through sonoma and mendocino counties and drains an area of approximately 3850 km2 the russian river watershed provides a useful example application as it has complex geography over 600 m of hilly relief alongside very low relief floodplains and a range of small streams that feed the main stem of the russian river permeable aquifers and stream sediments result in a high degree of connectivity between streams and groundwater and gsflow provides a useful model for evaluating water resources in the basin woolfenden and nishikawa 2014 5 geospatial and other ancillary data requirements for the case of the russian river watershed the dem and the russian river drainage area fig 2 were downloaded from the usda geospatial data gateway the model boundary follows the watershed drainage divide for the russian river watershed for cases where the model boundary does not align with true watershed boundaries custom polygon shapefiles can be derived from a dem using arcgis any polygon dataset that defines a land based study area can be used with gsflow arcpy spatially distributed vegetation type and percent cover was derived from the landfire dataset http www landfire gov vegetation php and spatially distributed soil data were derived by statsgo and ssurgo datasets also available through the usda geospatial gateway impervious area data were obtained from the national land cover database nlcd http www mrlc gov nlcd11 data php climate datasets used to spatially distribute daily precipitation and maximum and minimum temperatures are typically derived by combining daily climate measured at stations located in the model boundary with spatially distributed monthly climatologies of these variables monthly climatologies i e 30 year monthly averages were obtained from the parameter elevation regression on independent slopes model prism climate portal http www prism oregonstate edu normals daily climate measured at climate stations located in the russian river watershed were attained from national oceanic and atmospheric association noaa coop stations http www wrcc dri edu climatedata climsum and from local agencies a stream network for the russian river was attained from nhd https nhd usgs gov nhd high resolution html the nhd stream network was used as a guide for generating the model grid scale stream network used in the gsflow model 6 gsflow arcpy workflow the workflow for generating gsflow input follows the sequential application of each python script while displaying output in arcgis to evaluate the results scripts must be run in the order shown below with exceptions noted as results from one script are required to run subsequent scripts however in some cases such as for developing a stream network a subset of scripts can be run iteratively to correct and improve results to better represent the system the individual scripts are described below in order of their execution 6 1 fishnet generator py the fishnet generator script uses arcpy functions to discretize the model domain into square hrus and to spatially reference the model grid the term fishnet is used to describe the generated model grid in arcgis upon which the model parameters are developed this also is referred to as the parameter shapefile because it is a shapefile in arcmap that stores parameters for each hru for the russian river model hrus were set to 300 300 m fig 3 b this script reads cell size and buffer cells from the configuration file and uses arcpy tools to create the fishnet grid shapefile based off the study area boundary extent and spatial projection alternatively an existing model grid shapefile can be used for gsflow arcpy by setting the spatial location of the lower left corner of the grid and skipping to hru parameters py 6 2 hru parameters py the hru parameters script constructs the fields for the attribute table of the model parameter shapefile based on those provided in the field list ini configuration file the attribute table will display each of these field names as column headers these columns start out blank and fill with data as the scripts are run this script also will designate the hru type as inactive land lake or swale based on the study area boundary and shapefiles designating lakes and model points the lake shapefile if applicable is typically extracted from an nhd dataset or can be manually created 6 3 dem parameters py the dem parameters script populates the elevation fields by resampling the dem to the model grid resolution fig 3 dem parameters are used to develop the elevation dataset stream network and cascade parameters for gsflow minimum maximum and mean dem values are all written to the parameter shapefile providing the user with several options for model grid elevations this script also runs a flow accumulation function to weight stream elevations over the surrounding elevations to better represent streams as the dem values are scaled to the fishnet this helps with stream network development and cascade routing and numbering 6 4 dem 2 streams py the dem to streams script uses the hru elevations generated in dem parameters py to build the stream network applying the topographical index of the model grid scale dem for the directional matrix this is the first of the three steps in the iterative process of stream network development the initial flow accumulation from dem parameters py helps ensure that streams will be in the correct location this script runs the arc hydro flow accumulation and flow direction tools using threshold settings from the configuration file and to build the initial stream network maidment 2002 the flow accumulation threshold designates the minimum number of upslope drainage hrus required to generate a stream that will be explicitly represented in the model the flow length threshold sets a minimum length for a potential stream reach streams shorter than this threshold are excluded adjusting these values can give the user a range of stream network detail fig 4 water flowing in streams is routed as channelized flow whereas lateral surface flow on hrus that do not contain a stream is routed as sheet flow markstrom et al 2008 results from this step can be evaluated in arcmap to examine the stream network even with the flow accumulation function stream deviations can occur when the dem is scaled to the model grid scale so some adjustments may be necessary to better match streams to nhd flowlines and or aerial photos cell altitudes can be manually adjusted in arcgis by modifying the dem adj values in the hru parameter shapefile attribute table fig 5 these adjustments are purely up to the user and not necessary to continue the crt in the following script will provide several elevation adjustments to fill swales that will allow continuous flow of the stream network and cascades areas where manual adjustments are commonly applied are floodplains confluences and lake inlets outlets 6 5 crt fill parameters py the crt fill parameters script runs the cascade routing tool henson et al 2013 without streams this script identifies each hru that has no drainage pathway to any of the 8 adjacent hrus for 8 way flow direction referred to as a swale or sink crt will not fill swales that contain a stream so this step runs crt without streams to ensure smooth and continuous downward slopes along streams the amount an hru elevation must be filled to create at least one drainage pathway to another hru or model outflow point is recorded in the parameter shapefile the user can turn on a flag to automatically apply the fill amounts to dem adj values and continue which will eliminate most if not all swales within two iterations however the user may choose to manually change the dem adj values as shown above or use a combination of crt fill values and manual adjustments such as in areas where larger fills are required fig 6 provides an example of how gsflow arcpy is used to create clean flow paths in complex areas where nhd streams are inconsistent with hru elevations 6 6 stream parameters py after swales are filled the stream parameters script runs the crt with streams to generate the final cascade routing parameters for gsflow fig 7 cascade parameters define the direction and proportion of overland and shallow subsurface flow routing used to calculate lateral flows from hru to hru and hru to streams lakes and model outflow points the script also generates the streamflow routing package sfr2 niswonger and prudic 2005 input values for modflow these data are all added to the parameter shapefile attribute table this is the final step in the iterative stream network development process all stream network scripts read dem adj for hru elevation additional changes made to dem adj at this point would require that the stream scripts be re run starting from dem 2 streams py 6 7 veg parameters py the vegetation parameters script generates all prms parameters derived from the landfire vegetation dataset including coverage type winter and summer coverage density winter and summer precipitation canopy interception and root depth fig 8 the script applies remap files to transfer landfire vegetation values evt120 evt130 or evt140 to corresponding prms values for example a landfire value of 3015 ca coastal redwood forest would be remapped to a prms vegetation type of 4 conifer these remap files are provided with the scripts and can be modified to include additional vegetation types for a region the script produces vegetation maps with prms values that are used to populate the parameter shapefile attribute table this script must be run before the soil scripts such that vegetation dependent root depth is available to calculate root zone soil parameters 6 8 soil raster prep py the soil preparation script projects the input soil data rasters to the spatial projection of the fishnet and clips each raster to the extent of the model domain input rasters include percent of sand silt and clay available water capacity and saturated hydraulic conductivity soil depth can be optionally provided and soil zone calculations will use the greater of root depth vs soil depth these data are provided with the ssurgo statsgo data and must be imported into arcgis using the soil data viewer tool where they can be converted to rasters if gaps exist in the soil data the script will operate a data fill procedure using adjacent nearest neighbor data values it is recommended that statsgo data are used to cover any areas where ssurgo data are unavailable before using adjacent data values soil raster prep py must be completed before running soil parameters py 6 9 soil parameters py the soil parameter script uses the prepared soil rasters of percent sand silt and clay along with available water capacity and saturated hydraulic conductivity to derive gsflow soil zone parameters fig 9 soil depth is derived from the greater of root depth and soil depth if provided from statsgo or ssurgo and is incorporated into the attribute table by veg parameters py these initial parameterizations are often modified during calibration by scaling the parameters equally for all cells located in each gaged subbasin a flag can be set in the configuration file to read a user supplied surficial geology shapefile for deriving soil parameters that control deep percolation beneath the soil zone or shallow aquifer hydraulic conductivity the geology will be re mapped into user determined classifications based on a remap table for example consolidated rock unconsolidated rock and basin fill these general geologies will hold values that will be applied to the soil parameter derivations this allows for more realistic initial values of certain soil parameters that affect the transfer of water through the sub surface 6 10 prism parameters py gsflow arcpy uses prism or other spatial climatology datasets to distribute climate station data to all hrus in a manner that incorporates the effects of slope aspect and altitude previous applications of this script have relied upon the prism 30 year climatologies for the period 1981 2010 at an 800 m resolution the prism parameters script determines the prms parameters that are used to distribute daily maximum and minimum temperatures and a precipitation value for each hru during the gsflow model runtime this distribution is based on the relationship between observed station data and the gridded climate dataset provided the factors derived from this relationship is used to extrapolate climate station data over the model domain these gsflow climate parameters can be derived from different gridded resolutions and or time periods i e daymet depending on the model simulation period or other project requirements but the script is tailored to read prism data without any reformatting the script operates arcmap s zonal statistics tool to resample the prism gridded data to hrus along with projecting transforming clipping and snapping the data to hrus this script must be run before ppt ratio parameters py 6 11 ppt ratio parameters py the precipitation ratio script calculates precipitation factors assigned to each hru and each month these factors are calculated as the ratio between prism data and respective observed mean monthly precipitation values from a designated climate station or stations these spatially distributed monthly factors for each hru are then multiplied by the climate station s values to distribute daily precipitation to all hrus for each day in the simulation period fig 10 a flag is set in the configuration file to read the climate station identification number for each hru precipitation zones can be provided by the user to apply multiple climate stations to the precipitation ratio calculations the precipitation ratios are written to the shapefile attribute table as monthly ratios per hru and ultimately written to the prms parameter file as the rain snow adjustment factors markstrom et al 2008 6 12 impervious py the impervious script uses impervious cover data from the national land cover database to derive the percentage of impervious surface within each hru the script also generates the impervious percentage for all hrus 6 13 prms template fill py the template fill script writes the prms parameter files either in column or array format based on settings in the configurations file a single parameter file can be written if older versions of prms are being used required prms dimensions that are read by this script and written to the prms parameter files must be specified by the user in csv files that accompany the scripts these csv files hold basic settings such as specific model dimensions and required default values for parameters not defined by the ancillary data described above dimensions and default parameter values can be adjusted according to the needs of a model application example templates of the csv files are provided with gsflow arcpy a subset of the input data required for modflow including the input for the streamflow routing package and model top altitude required for the discretization package can be exported from the attributes of the parameter shapefile 7 supplementary files another python file called support functions py is included in gsflow arcpy this script contains functions that are used by other scripts within gsflow arcpy this file must be present in the directory with the rest of the python scripts but does not need to be run by the user currently gsflow arcpy does not create the data and control files that are required for a gsflow simulation templates for these files that can be used for manually creating these files are included with the gsflow arcpy toolkit although the modflow grid streamflow routing package data input and optionally layer thicknesses can be created by gsflow arcpy the suite of modflow input files that include these data are not generated by gsflow arcpy these modflow input files can be created using other software products that are currently available such as modelmuse winston 2009 or flopy bakker et al 2016 in the public domain or one of the several commercial software packages data sets for running these scripts on the russian river are available by request through the corresponding authors because the data files for the russian river watershed are very large example data input for another example is included as supplementary material to this paper supplementary material for this paper includes the application of the sagehen watershed in california usa markstrom et al 2008 the russian river example was presented here as it symbolizes a large complex model domain that suits the capabilities of gsflow to simulate diverse model conditions the sagehen examples provided with the toolkit are small model builds meant to be used by both the developers and users as quick single issue test models 8 discussion development of gsflow arcpy was motivated by the need to automate a very arduous and time consuming process required to convert diverse geospatial datasets into ihm input another important aspect of this work is the development of an approach that is transferable to other ihm modeling environments for creating models with a stream network topology that is consistent with the model grid scale dem here we address additional complexities in generating a stream network required for simulating surface water and groundwater exchanges whereas previous work on methods for generating stream networks in watersheds strictly considered surface drainage processes this process is challenging especially when using large hrus to discretize watersheds with steep topography furthermore a robust and consistent stream network development process is important for constructing stable and efficient ihms structural model errors due to poorly constructed geometries and parameters used in coupled governing equations for surface water and groundwater can severely hinder ihm applications specifically the algorithms in gsflow arcpy for constructing stream networks and elevations help ensure robust simulation of groundwater and surface water exchanges gsflow arcpy combines automation with the ability for user intervention and manual manipulation of data sets to ensure flexible model input data construction user intervention is facilitated by interfacing data construction with arcgis to provide visualization of intermediate products and specialization of data manipulation when required gsflow arcpy was developed recognizing the need to create large scale models that require resampling of dems to large hrus this process inevitably creates artifacts in the model grid scale altitude that must be fixed with some sacrifice in dem accuracy wilson 2012 however benefits from enhanced model efficiency attained through coarse discretization is often a necessity that must justify losses in resolution important future work is the evaluation of optimal ihm resolution that will no doubt benefit from automated and reproducible model input construction as provided by gsflow arcpy although beyond the scope of this present work post processing and visualization of model results is another important component of any study using an ihm for hydrologic investigation because gsflow simulates all major hydrologic processes in watersheds large amounts of output are produced making calculation of water budgets and visualization more difficult and important existing software for modflow e g modelmuse and flopy can be used to visualize output from gsflow however software that can be used to analyze and visualize gsflow results from a wholistic perspective is limited these authors hope that with the development of software for creating data input for gsflow more applications will lead to additional work to develop post processing software for gsflow 9 conclusions integrated modeling is increasingly used to support water resources management and for understanding natural and anthropogenic impacts on natural resources a new python toolkit called gsflow arcpy presented herein is intended to formalize much of the model input data construction using a consistent approach this paper introduces and demonstrates its capabilities to provide a streamlined reproducible process for creating input data required for the integrated hydrologic model gsflow a fully constructed prms parameter file is created by the package along with several modflow input parameters this approach automates data manipulation formatting transfer and gis processing and provides robust data input suitable for ihms although these scripts are tailored to fulfill input requirements for gsflow the datasets produced are general and these scripts could be adapted to develop input data for any ihm that relies on regular grids for spatial discretization the current version of gsflow arcpy does not convert the groundwater inputs into model ready input as it does for prms input files the process can be combined with other automated approaches such as flopy to fully complete the gsflow model input construction gsflow arcpy is demonstrated through application to the russian river watershed a 3850 km2 coastal watershed located in northwest california usa the russian river watershed provides a useful demonstration of the toolkit s capabilities because it contains complex topography and a highly dendritic stream network that typically make it challenging to model surface water and groundwater interactions gsflow arcpy significantly simplifies the model development process which may enhance accessibility of integrated hydrologic models to a broader user group that can benefit from decision support tools for managing natural resources acknowledgments research supported by grant from the water sustainability and climate program jointly funded by the national science foundation 1360506 and u s department of agriculture national institute of food and agriculture 1360507 support was also provided by the u s geological survey s water use and availability program we would like to thank paul barlow with the u s geological survey and three anonymous reviewers for their timely and insightful colleague reviews all data used in the analysis and to support our conclusions in this manuscript may be obtained from mag e mail mgardner usgs gov or rgn e mail rniswon usgs gov any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government appendix a supplementary data the following are the supplementary data related to this article data profile data profile supp material supp material appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 07 020 
26319,integrated hydrologic modeling ihm encompasses a vast number of processes and specifications variable in time and space and development of models can be arduous model input construction techniques have not been formalized or made easily reproducible creating the input files for integrated hydrologic models requires complex gis processing of raster and vector datasets from various sources developing stream network topology that is consistent with the model grid scale digital elevation model dem is important for robust simulation of surface water and groundwater exchanges distribution of meteorological data over the model domain is difficult in complex terrain at the model grid scale but is necessary for realistic simulations as model development requires extensive gis and computer programming expertise the use of ihms has mostly been limited to research groups with available financial human and technical resources here we present a series of open source python scripts that are combined with esri arcgis to provide a formalized technique for the parameterization and development of inputs for the readily available ihm called gsflow this python toolkit automates many of the necessary and laborious processes of parameterization including stream network development land coverages and meteorological distribution over the model domain the final products of the toolkit are prms ready parameter files along with several input parameters for a modflow model including input for the streamflow routing package a demonstration of the toolkit is provided to illustrate its capabilities program and computing requirements any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government mandatory software arcgis 10 1 or newer will include python 2 7 along with the arcpy package http pro arcgis com en pro app arcpy get started what is arcpy htm cascade routing tool crt executable version 1 3 1 http water usgs gov ogw crt gsflow 1 1 6 or newer http water usgs gov ogw gsflow downloads optional software usda soil data viewer http www nrcs usda gov wps portal nrcs detail soils survey geo cid nrcs142p2 053614 software availability version 1 release on github https github com gsflow gsflow arcpy software download includes a user guide and example model 1 introduction water resource managers and researchers require greater understanding of the connectivity between groundwater and surface water to effectively manage these resources integrated hydrologic models ihms can provide important information about water resources and are often used as decision support tools for resource management laniak et al 2013 construction of ihm inputs are not well formalized or automated making reproducibility very difficult a wider acceptance and distribution of process based approaches requires improved model visualization tools and streamlined approaches for model setup execution and analysis fatichi et al 2016 various techniques have been developed to create drainage networks but none have the benefit of a fully automated and topologically consistent approach that is applicable for simulating exchanges between streams and groundwater in grid based ihms applied to developed river basins the progression of ihms has spurred research and development for improving data processing and graphical user interfaces band 1986 gruber and peckham 2009 metz et al 2011 wilson 2012 tian et al 2016 bhatt et al 2008 ng et al 2018 advancements in gis technology and remote sensing have provided the ability to highly parameterize systems of interest but not without complex data processing metz et al 2011 jasiewicz and metz 2011 despite these advancements in gis data processing previous works have not focused on additional complexities related to simulating surface water and groundwater exchanges and input data development for fine resolution large scale hydrologic models is still a difficult problem that discourages and limits the use of ihms application of approaches strictly developed for surface networks to develop input for ihms can lead to erroneous input that compromises a model s usefulness clark et al 2015 just the near surface component of gsflow requires input data of approximately 50 global parameters and 35 model cell specific parameters that may be derived from large geospatial datasets markstrom et al 2008 creating input files for ihms requires complex gis processing of raster and vector datasets from various sources e g wilson 2012 viger and leavesley 2007 provide the basic steps for developing input parameters for a watershed model and their approach requires data management gis processing and relies upon proprietary software and scripting languages these tools are valuable but require a high level of process knowledge can require significant time investment and can be difficult to apply in large high resolution models bhatt et al 2008 presented software for developing input data for the ihm called pihm through integration with qgis providing input and output data processing tools qu and duffy 2007 presently automated software is not readily available that can be used to routinely and consistently generate input for gsflow tools and methods that are available require complicated and informal workflow processing on diverse geospatial datasets the tools being presented herein are intended to formalize the input data construction using a consistent approach automate and simplify data manipulation and data transfer support user intervention for customization and provide robust data input that is suitable for ihms this paper presents new software developed in the python programming language that can be used to automatically generate required data input for gsflow although these scripts are tailored to create data input formats required for gsflow the scripts could be adapted to develop input data for other ihms that rely on regular grids for spatial discretization for example these scripts could be used to automatically develop a consistent grid scale dem and stream network appropriate for simulating surface water and groundwater interactions and maps of climate land cover and land use distributed to each model grid cell a stream network must be congruent with the model grid used for groundwater simulation for robust and convergent calculation of hydraulic gradients between streams and groundwater deriving model input for large scale ihms that produce convergent solutions remains a challenging and time consuming process the software presented herein was developed to overcome these challenges and to provide an efficient reproducible and automated procedure for gsflow input data construction in the past ihm development has required extensive gis and computer programming expertise which has restricted the use of ihms in water resources management this toolkit called gsflow arcpy significantly simplifies gsflow model development thereby potentially expanding the benefits of this ihm to a broader user community gsflow arcpy automates many of the complicated and time consuming gis procedures necessary for parameterizing ihms including generating input representing the stream network land cover and meteorology distributions over complex terrains such as steep and varying topography or flat regions where it can be difficult to establish the locations of stream channels stream network development based on digital elevation models dems are needed to determine the paths of water sediment and contamination movement tarboton 1997 similar to topmodel beven and kirkby 1979 and hydrotel fortin et al 2001 gsflow arcpy uses a directional matrix and computes a topographic index to automatically determine drainage pathways unlike these previous studies that were solely focused on surface drainage the present work extends to the simulation of surface water and groundwater exchanges using equal area model grid cells for the groundwater model and correctly overlaying the streams onto the groundwater model cells streams that are incorrectly placed onto grid cells will cause numerical problems specifically the model grid scale dem and stream network are made consistent with respect to relative altitudes between cells containing streams cells adjacent to streams and the slope of cells following the streams this prevents inconsistencies between the model grid dem and the stream network that can lead to excessive surface water and groundwater exchanges excessive spring discharge rates and poor ihm convergence gsflow arcpy provides data input requirements for the groundwater model component of an ihm including the stream network and topology and the top altitude for model cells that are used to define altitudes for aquifer boundaries these groundwater parameters are developed at the same resolution as the model top grid additionally this toolkit provides the necessary data sets for creating input for the streamflow routing package which has been one of the greatest challenges for developing modflow and gsflow models additional processing outside of gsflow arcpy is required to generate the hydrogeologic framework used to characterize aquifer systems and the associated modflow input data there are many separate software tools available that can be used for the development of input data that characterize aquifer systems modflow has benefited from many options for automated model construction that has led to broad application around the world e g leapfrog hydro 2013 winston 2009 bakker et al 2016 these existing tools are readily available and can be used in concert with gsflow arcpy to provide capabilities to characterize aquifer systems using approaches more appropriately done outside the gis environment 2 background gsflow is an ihm developed by the usgs that integrates the watershed runoff model prms with the groundwater flow model modflow nwt markstrom et al 2008 2015 niswonger et al 2011 gsflow was developed for application to medium and large scale watersheds i e 10s 1000s of square kilometers and has primarily been used as a tool for water resources management gsflow has been applied to many basins around the world see for example gannet et al 2017 hassan et al 2014 and wu et al 2014 for examples from the us spain and china respectively and each model application has generally relied upon varying approaches for model input construction the difficulty in implementing required model input datasets for gsflow and other ihms has led to recent work to develop a consistent set of tools for model input construction primarily following the work of huntington and niswonger 2012 niswonger et al 2014 and rajagopal et al 2015 input data required for gsflow and other ihms are derived from diverse geospatial datasets available for the continental united states and other places across the globe these datasets are typically available through centralized data portals and can be downloaded freely to local computers for selected basins of interest gsflow arcpy was not designed to access geospatial datasets through web portals because these portals are not static and changes in data formats and storage locations make automated communication with web portals unreliable geospatial datasets must be downloaded and stored locally users may incorporate other datasets if available so long as the data are in an acceptable format i e raster files users can refer to the formats for the standard data sources to develop datasets that rely on non standard data sources gsflow arcpy develops the model input by reading geospatial datasets and making calculations using these data for each prms parameter and modflow dataset according to methods described by viger and leavesley 2007 markstrom et al 2008 markstrom et al 2015 harbaugh 2005 niswonger et al 2011 and henson et al 2013 most of the data required by gsflow arcpy can be downloaded for basins in the united states using the usda geospatial data gateway https gdg sc egov usda gov however some datasets must be obtained through regionally specific data sources such as local agencies that collect and maintain these datasets examples of regionally specific data are climate and streamflow data that often are collected and managed locally and these data must be obtained by regionally specific data collection efforts additionally other regionally specific data include driller s logs or geophysical datasets used to develop the hydrogeologic framework for the groundwater system the process of extracting the surface drainage network from a dem that is appropriate for ihms remains a difficult process in complex terrain despite well documented approaches bhatt et al 2008 peckham 1998 maidment 2002 daniels et al 2011 niu et al 2014 stream networks that are derived using previous approaches can potentially contain streams that float above and adjacent to stream canyons are overly incised below the model grid cell top altitude traverse uphill in the downstream direction or form loops with ambiguous flow directions previous approaches have relied on high resolution dems to generate stream networks as opposed to the method used by gsflow arcpy that applies a dem that has been up scaled to the model grid resolution furthermore no consideration is provided for constructed channels and canals that cannot be generated using topographic approaches for stream generation inconsistencies between a stream network and the model grid scale dem are problematic for ihms that explicitly route surface water and calculate exchanges between surface water and groundwater and spring discharge using hydraulic gradients dependent on the relative altitude between streams land surface and groundwater head prudic et al 2004 niswonger and prudic 2005 markstrom et al 2008 other errors in stream networks include dead ends common issue where ephemeral stream channels become obscured where they flow across alluvial plains misalignment relative to usgs national hydrography dataset nhd lines or aerial photos and incorrectly located confluences stream network issues also arise when streams are derived from fine scale dems in arid regions where stream channels are undefined or for dems that have not been properly conditioned to large grid scales in complex terrain drainage networks dem conditioning that can solve many of these problems include filling local swales that are artifacts of dem construction assuring that the grid cells have smooth and continuous downward slopes in the downstream direction confirming that grid cells that contain streams have lower altitudes relative to adjacent grid cells and creating and connecting intermittent stream reaches where channels are not well defined by topography in some cases dealing with these issues requires some level of subjectivity and the gsflow arcpy scripts provide the option for user intervention to support these special cases a model grid scale dem and surface water network are generated by gsflow arcpy using an iterative algorithm that conditions the model grid scale dem by filling local swales creating smoothly downward sloping streamlines and assuring that streams are not floating above or overly incised below model cell tops this approach uses the drainage networks provided by nhd as a guide however the final directional matrix and topographic index provided by gsflow arcpy is generated using model grid scale dem and flow accumulation and flow direction functions band 1986 maidment 2002 deriving the stream network from the model grid scale dem provides consistency of scale between streams and model grid cells that is appropriate for ihms however depending on the resolution of the model grid the location and altitude of streams may deviate from reality when using coarse grids 3 python scripts and the data development process developing a gsflow model using gsflow arcpy can be organized into a set of distinct steps input data are generated for both the prms and modflow components of gsflow the procedure is carried out by 13 separate scripts that handle various workflow procedures fig 1 these procedures include data retrieval and preparation model boundary delineation and horizontal discretization grid scale dem initialization grid scale dem fill and conditioning and surface water network delineation lateral overland and shallow subsurface flow cascade routing delineation land use vegetation and soil zone parameterization climate distribution parameterization and model input file construction each of these procedures represents a series of processes or commands that are carried out by running one or more python scripts that automate spatial data processing required for gsflow model input each script within gsflow arcpy reads input files from a local folder and performs the operations necessary to project transform calculate and format the data into an attribute table that is used to build the prms and modflow inputs along with the scripts a template configuration file field list file remap files template prms control file and two csv files that hold dimension and parameter settings are provided the configuration file guides the scripts to the correct folder locations and the csv files contain site specific settings the field list holds the names of each field column that will be created in the shapefile the remap files list unique ids to transfer remap vegetation or soil codes to prms values as the scripts are run folders and files appear in the working directory designated as the folder that contains the configuration file the parameter shapefile contains cell attributes and can be examined edited and displayed in arcmap after running each script to visualize and evaluate intermediate results the configuration file is a text file that acts as a guide for gsflow arcpy the purpose of the configuration file is to avoid having to change the python scripts for different models or model implementations each section of the configuration file applies to a certain script and provides file locations and model specific settings and flags the location of this file is considered the working directory that contains all folders and files created by the scripts several flags can be turned on or off in the configuration file depending on preferences of the user and project specifics the field list file field list ini comes with the package and contains a list of the parameter names that make up the attribute table of the shapefile generated by gsflow arcpy gsflow arcpy discretizes prms and or gsflow model areas using equal area square grid cells for gsflow arcpy models prms hydrologic response units hrus are coincident with modflow grid cells thus a single horizontal spatial discretization is used for both prms and all modflow layers spatial units are called hrus in prms and grid cells in modflow however gsflow arcpy refers to both prms and modflow spatial units as hrus when these spatial units are equal gsflow allows for different spatial discretization between prms and modflow and a user could create hrus that are larger or finer than the modflow grid cells with these scripts however this would require additional data handling outside the standard application of gsflow arcpy to first create gridded hrus and then separately create the modflow grid and stream network for this case additional parameters would be required by gsflow that are not created by gsflow arcpy to provide the connectivity between hrus markstrom et al 2008 previous testing of gsflow models indicate that maintaining congruent hrus and grid cells provides several advantages these advantages include ease of integration between the surface water and groundwater systems promoting model convergence and consistency by maintaining exchanges in water between storage reservoirs of similar volume and the ability to represent lateral convergent and divergent cascading flows according to variations in altitudes among gridded hrus gsflow arcpy uses a cascade routing tool crt henson et al 2013 to define lateral cascade pathways from hru to hru and hru to streams and lakes the crt provides reach and segment numbering and defines the cascades based on the model grid scale dem 4 example application gsflow arcpy is presented and demonstrated herein by applying the tools to develop the input datasets for a gsflow model of the russian river watershed located in northwestern california fig 2 the russian river traverses 177 km north to south through sonoma and mendocino counties and drains an area of approximately 3850 km2 the russian river watershed provides a useful example application as it has complex geography over 600 m of hilly relief alongside very low relief floodplains and a range of small streams that feed the main stem of the russian river permeable aquifers and stream sediments result in a high degree of connectivity between streams and groundwater and gsflow provides a useful model for evaluating water resources in the basin woolfenden and nishikawa 2014 5 geospatial and other ancillary data requirements for the case of the russian river watershed the dem and the russian river drainage area fig 2 were downloaded from the usda geospatial data gateway the model boundary follows the watershed drainage divide for the russian river watershed for cases where the model boundary does not align with true watershed boundaries custom polygon shapefiles can be derived from a dem using arcgis any polygon dataset that defines a land based study area can be used with gsflow arcpy spatially distributed vegetation type and percent cover was derived from the landfire dataset http www landfire gov vegetation php and spatially distributed soil data were derived by statsgo and ssurgo datasets also available through the usda geospatial gateway impervious area data were obtained from the national land cover database nlcd http www mrlc gov nlcd11 data php climate datasets used to spatially distribute daily precipitation and maximum and minimum temperatures are typically derived by combining daily climate measured at stations located in the model boundary with spatially distributed monthly climatologies of these variables monthly climatologies i e 30 year monthly averages were obtained from the parameter elevation regression on independent slopes model prism climate portal http www prism oregonstate edu normals daily climate measured at climate stations located in the russian river watershed were attained from national oceanic and atmospheric association noaa coop stations http www wrcc dri edu climatedata climsum and from local agencies a stream network for the russian river was attained from nhd https nhd usgs gov nhd high resolution html the nhd stream network was used as a guide for generating the model grid scale stream network used in the gsflow model 6 gsflow arcpy workflow the workflow for generating gsflow input follows the sequential application of each python script while displaying output in arcgis to evaluate the results scripts must be run in the order shown below with exceptions noted as results from one script are required to run subsequent scripts however in some cases such as for developing a stream network a subset of scripts can be run iteratively to correct and improve results to better represent the system the individual scripts are described below in order of their execution 6 1 fishnet generator py the fishnet generator script uses arcpy functions to discretize the model domain into square hrus and to spatially reference the model grid the term fishnet is used to describe the generated model grid in arcgis upon which the model parameters are developed this also is referred to as the parameter shapefile because it is a shapefile in arcmap that stores parameters for each hru for the russian river model hrus were set to 300 300 m fig 3 b this script reads cell size and buffer cells from the configuration file and uses arcpy tools to create the fishnet grid shapefile based off the study area boundary extent and spatial projection alternatively an existing model grid shapefile can be used for gsflow arcpy by setting the spatial location of the lower left corner of the grid and skipping to hru parameters py 6 2 hru parameters py the hru parameters script constructs the fields for the attribute table of the model parameter shapefile based on those provided in the field list ini configuration file the attribute table will display each of these field names as column headers these columns start out blank and fill with data as the scripts are run this script also will designate the hru type as inactive land lake or swale based on the study area boundary and shapefiles designating lakes and model points the lake shapefile if applicable is typically extracted from an nhd dataset or can be manually created 6 3 dem parameters py the dem parameters script populates the elevation fields by resampling the dem to the model grid resolution fig 3 dem parameters are used to develop the elevation dataset stream network and cascade parameters for gsflow minimum maximum and mean dem values are all written to the parameter shapefile providing the user with several options for model grid elevations this script also runs a flow accumulation function to weight stream elevations over the surrounding elevations to better represent streams as the dem values are scaled to the fishnet this helps with stream network development and cascade routing and numbering 6 4 dem 2 streams py the dem to streams script uses the hru elevations generated in dem parameters py to build the stream network applying the topographical index of the model grid scale dem for the directional matrix this is the first of the three steps in the iterative process of stream network development the initial flow accumulation from dem parameters py helps ensure that streams will be in the correct location this script runs the arc hydro flow accumulation and flow direction tools using threshold settings from the configuration file and to build the initial stream network maidment 2002 the flow accumulation threshold designates the minimum number of upslope drainage hrus required to generate a stream that will be explicitly represented in the model the flow length threshold sets a minimum length for a potential stream reach streams shorter than this threshold are excluded adjusting these values can give the user a range of stream network detail fig 4 water flowing in streams is routed as channelized flow whereas lateral surface flow on hrus that do not contain a stream is routed as sheet flow markstrom et al 2008 results from this step can be evaluated in arcmap to examine the stream network even with the flow accumulation function stream deviations can occur when the dem is scaled to the model grid scale so some adjustments may be necessary to better match streams to nhd flowlines and or aerial photos cell altitudes can be manually adjusted in arcgis by modifying the dem adj values in the hru parameter shapefile attribute table fig 5 these adjustments are purely up to the user and not necessary to continue the crt in the following script will provide several elevation adjustments to fill swales that will allow continuous flow of the stream network and cascades areas where manual adjustments are commonly applied are floodplains confluences and lake inlets outlets 6 5 crt fill parameters py the crt fill parameters script runs the cascade routing tool henson et al 2013 without streams this script identifies each hru that has no drainage pathway to any of the 8 adjacent hrus for 8 way flow direction referred to as a swale or sink crt will not fill swales that contain a stream so this step runs crt without streams to ensure smooth and continuous downward slopes along streams the amount an hru elevation must be filled to create at least one drainage pathway to another hru or model outflow point is recorded in the parameter shapefile the user can turn on a flag to automatically apply the fill amounts to dem adj values and continue which will eliminate most if not all swales within two iterations however the user may choose to manually change the dem adj values as shown above or use a combination of crt fill values and manual adjustments such as in areas where larger fills are required fig 6 provides an example of how gsflow arcpy is used to create clean flow paths in complex areas where nhd streams are inconsistent with hru elevations 6 6 stream parameters py after swales are filled the stream parameters script runs the crt with streams to generate the final cascade routing parameters for gsflow fig 7 cascade parameters define the direction and proportion of overland and shallow subsurface flow routing used to calculate lateral flows from hru to hru and hru to streams lakes and model outflow points the script also generates the streamflow routing package sfr2 niswonger and prudic 2005 input values for modflow these data are all added to the parameter shapefile attribute table this is the final step in the iterative stream network development process all stream network scripts read dem adj for hru elevation additional changes made to dem adj at this point would require that the stream scripts be re run starting from dem 2 streams py 6 7 veg parameters py the vegetation parameters script generates all prms parameters derived from the landfire vegetation dataset including coverage type winter and summer coverage density winter and summer precipitation canopy interception and root depth fig 8 the script applies remap files to transfer landfire vegetation values evt120 evt130 or evt140 to corresponding prms values for example a landfire value of 3015 ca coastal redwood forest would be remapped to a prms vegetation type of 4 conifer these remap files are provided with the scripts and can be modified to include additional vegetation types for a region the script produces vegetation maps with prms values that are used to populate the parameter shapefile attribute table this script must be run before the soil scripts such that vegetation dependent root depth is available to calculate root zone soil parameters 6 8 soil raster prep py the soil preparation script projects the input soil data rasters to the spatial projection of the fishnet and clips each raster to the extent of the model domain input rasters include percent of sand silt and clay available water capacity and saturated hydraulic conductivity soil depth can be optionally provided and soil zone calculations will use the greater of root depth vs soil depth these data are provided with the ssurgo statsgo data and must be imported into arcgis using the soil data viewer tool where they can be converted to rasters if gaps exist in the soil data the script will operate a data fill procedure using adjacent nearest neighbor data values it is recommended that statsgo data are used to cover any areas where ssurgo data are unavailable before using adjacent data values soil raster prep py must be completed before running soil parameters py 6 9 soil parameters py the soil parameter script uses the prepared soil rasters of percent sand silt and clay along with available water capacity and saturated hydraulic conductivity to derive gsflow soil zone parameters fig 9 soil depth is derived from the greater of root depth and soil depth if provided from statsgo or ssurgo and is incorporated into the attribute table by veg parameters py these initial parameterizations are often modified during calibration by scaling the parameters equally for all cells located in each gaged subbasin a flag can be set in the configuration file to read a user supplied surficial geology shapefile for deriving soil parameters that control deep percolation beneath the soil zone or shallow aquifer hydraulic conductivity the geology will be re mapped into user determined classifications based on a remap table for example consolidated rock unconsolidated rock and basin fill these general geologies will hold values that will be applied to the soil parameter derivations this allows for more realistic initial values of certain soil parameters that affect the transfer of water through the sub surface 6 10 prism parameters py gsflow arcpy uses prism or other spatial climatology datasets to distribute climate station data to all hrus in a manner that incorporates the effects of slope aspect and altitude previous applications of this script have relied upon the prism 30 year climatologies for the period 1981 2010 at an 800 m resolution the prism parameters script determines the prms parameters that are used to distribute daily maximum and minimum temperatures and a precipitation value for each hru during the gsflow model runtime this distribution is based on the relationship between observed station data and the gridded climate dataset provided the factors derived from this relationship is used to extrapolate climate station data over the model domain these gsflow climate parameters can be derived from different gridded resolutions and or time periods i e daymet depending on the model simulation period or other project requirements but the script is tailored to read prism data without any reformatting the script operates arcmap s zonal statistics tool to resample the prism gridded data to hrus along with projecting transforming clipping and snapping the data to hrus this script must be run before ppt ratio parameters py 6 11 ppt ratio parameters py the precipitation ratio script calculates precipitation factors assigned to each hru and each month these factors are calculated as the ratio between prism data and respective observed mean monthly precipitation values from a designated climate station or stations these spatially distributed monthly factors for each hru are then multiplied by the climate station s values to distribute daily precipitation to all hrus for each day in the simulation period fig 10 a flag is set in the configuration file to read the climate station identification number for each hru precipitation zones can be provided by the user to apply multiple climate stations to the precipitation ratio calculations the precipitation ratios are written to the shapefile attribute table as monthly ratios per hru and ultimately written to the prms parameter file as the rain snow adjustment factors markstrom et al 2008 6 12 impervious py the impervious script uses impervious cover data from the national land cover database to derive the percentage of impervious surface within each hru the script also generates the impervious percentage for all hrus 6 13 prms template fill py the template fill script writes the prms parameter files either in column or array format based on settings in the configurations file a single parameter file can be written if older versions of prms are being used required prms dimensions that are read by this script and written to the prms parameter files must be specified by the user in csv files that accompany the scripts these csv files hold basic settings such as specific model dimensions and required default values for parameters not defined by the ancillary data described above dimensions and default parameter values can be adjusted according to the needs of a model application example templates of the csv files are provided with gsflow arcpy a subset of the input data required for modflow including the input for the streamflow routing package and model top altitude required for the discretization package can be exported from the attributes of the parameter shapefile 7 supplementary files another python file called support functions py is included in gsflow arcpy this script contains functions that are used by other scripts within gsflow arcpy this file must be present in the directory with the rest of the python scripts but does not need to be run by the user currently gsflow arcpy does not create the data and control files that are required for a gsflow simulation templates for these files that can be used for manually creating these files are included with the gsflow arcpy toolkit although the modflow grid streamflow routing package data input and optionally layer thicknesses can be created by gsflow arcpy the suite of modflow input files that include these data are not generated by gsflow arcpy these modflow input files can be created using other software products that are currently available such as modelmuse winston 2009 or flopy bakker et al 2016 in the public domain or one of the several commercial software packages data sets for running these scripts on the russian river are available by request through the corresponding authors because the data files for the russian river watershed are very large example data input for another example is included as supplementary material to this paper supplementary material for this paper includes the application of the sagehen watershed in california usa markstrom et al 2008 the russian river example was presented here as it symbolizes a large complex model domain that suits the capabilities of gsflow to simulate diverse model conditions the sagehen examples provided with the toolkit are small model builds meant to be used by both the developers and users as quick single issue test models 8 discussion development of gsflow arcpy was motivated by the need to automate a very arduous and time consuming process required to convert diverse geospatial datasets into ihm input another important aspect of this work is the development of an approach that is transferable to other ihm modeling environments for creating models with a stream network topology that is consistent with the model grid scale dem here we address additional complexities in generating a stream network required for simulating surface water and groundwater exchanges whereas previous work on methods for generating stream networks in watersheds strictly considered surface drainage processes this process is challenging especially when using large hrus to discretize watersheds with steep topography furthermore a robust and consistent stream network development process is important for constructing stable and efficient ihms structural model errors due to poorly constructed geometries and parameters used in coupled governing equations for surface water and groundwater can severely hinder ihm applications specifically the algorithms in gsflow arcpy for constructing stream networks and elevations help ensure robust simulation of groundwater and surface water exchanges gsflow arcpy combines automation with the ability for user intervention and manual manipulation of data sets to ensure flexible model input data construction user intervention is facilitated by interfacing data construction with arcgis to provide visualization of intermediate products and specialization of data manipulation when required gsflow arcpy was developed recognizing the need to create large scale models that require resampling of dems to large hrus this process inevitably creates artifacts in the model grid scale altitude that must be fixed with some sacrifice in dem accuracy wilson 2012 however benefits from enhanced model efficiency attained through coarse discretization is often a necessity that must justify losses in resolution important future work is the evaluation of optimal ihm resolution that will no doubt benefit from automated and reproducible model input construction as provided by gsflow arcpy although beyond the scope of this present work post processing and visualization of model results is another important component of any study using an ihm for hydrologic investigation because gsflow simulates all major hydrologic processes in watersheds large amounts of output are produced making calculation of water budgets and visualization more difficult and important existing software for modflow e g modelmuse and flopy can be used to visualize output from gsflow however software that can be used to analyze and visualize gsflow results from a wholistic perspective is limited these authors hope that with the development of software for creating data input for gsflow more applications will lead to additional work to develop post processing software for gsflow 9 conclusions integrated modeling is increasingly used to support water resources management and for understanding natural and anthropogenic impacts on natural resources a new python toolkit called gsflow arcpy presented herein is intended to formalize much of the model input data construction using a consistent approach this paper introduces and demonstrates its capabilities to provide a streamlined reproducible process for creating input data required for the integrated hydrologic model gsflow a fully constructed prms parameter file is created by the package along with several modflow input parameters this approach automates data manipulation formatting transfer and gis processing and provides robust data input suitable for ihms although these scripts are tailored to fulfill input requirements for gsflow the datasets produced are general and these scripts could be adapted to develop input data for any ihm that relies on regular grids for spatial discretization the current version of gsflow arcpy does not convert the groundwater inputs into model ready input as it does for prms input files the process can be combined with other automated approaches such as flopy to fully complete the gsflow model input construction gsflow arcpy is demonstrated through application to the russian river watershed a 3850 km2 coastal watershed located in northwest california usa the russian river watershed provides a useful demonstration of the toolkit s capabilities because it contains complex topography and a highly dendritic stream network that typically make it challenging to model surface water and groundwater interactions gsflow arcpy significantly simplifies the model development process which may enhance accessibility of integrated hydrologic models to a broader user group that can benefit from decision support tools for managing natural resources acknowledgments research supported by grant from the water sustainability and climate program jointly funded by the national science foundation 1360506 and u s department of agriculture national institute of food and agriculture 1360507 support was also provided by the u s geological survey s water use and availability program we would like to thank paul barlow with the u s geological survey and three anonymous reviewers for their timely and insightful colleague reviews all data used in the analysis and to support our conclusions in this manuscript may be obtained from mag e mail mgardner usgs gov or rgn e mail rniswon usgs gov any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government appendix a supplementary data the following are the supplementary data related to this article data profile data profile supp material supp material appendix a supplementary data supplementary data related to this article can be found at https doi org 10 1016 j envsoft 2018 07 020 
