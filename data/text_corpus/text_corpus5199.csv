index,text
25995,real time flood forecasting computational frameworks that can dynamically integrate oceanic coastal and estuarine processes are becoming essential to provide accurate and timely information for emergency response and planning in largely populated estuaries during extreme events this study presents a newly developed real time total water flood guidance system that is fully automated based on the coupled surge wave adcirc swan model and provides water level forecasts in the chesapeake bay for a lead time of 84 h twice a day displayed on a web based public interface this system improved the current total water level predictions in the bay rmse 0 12 m when compared to the existing operational forecasting systems over the period of 6 months jan 19 jun 19 furthermore we demonstrated that a bias correction scheme and a multi member ensemble forecast improve the overall flood prediction results suggests that this framework can improve our current capacity to predict total water levels in large estuaries keywords surge wave guidance system flood forecasting in estuaries adcirc swan model ensemble guidance 1 introduction flooding due to extreme weather events poses an increasing threat to the east coast of the united states us along with devastating economic consequences and significant loss of lives for example super storm sandy in 2012 caused an estimated loss of 50 billion in infrastructure damages throughout the mid atlantic region noaa 2013 hurricane maria 2017 florence 2018 and dorian 2019 are more recent examples of the direct impacts to the us east coast in addition increasing global temperatures are likely to increase the number of severe events easterling et al 2000 van aalst 2006 and significantly impact weather driven global economic losses although oceanfront areas are at the greatest risk of storm surge flooding regions along rivers and estuaries are also extremely susceptible to flooding for instance the chesapeake bay and its tributaries are particularly vulnerable to flooding from hurricanes tropical storms and nor easters as exemplified by hurricanes isabel in 2003 sheng et al 2010 wang et al 2015 and irene in 2011 spring 2011 flooding conditions in the north atlantic ocean are mainly impacted by storm surges astronomical tides winds waves precipitation dresback et al 2013a and indirect effects from the gulf streams and ultimately the rise in the coastal sea levels ezer et al 2013 large scale processes such as the gulf stream have been identified to increase the number of flooding days in hampton roads virginia ezer et al 2013 global climate change rising sea levels and land subsidence are already affecting coastal flooding in major parts of the chesapeake bay boon et al 2010 almost 40 of the us population lives in counties near coastal and tidal areas relying on accurate prediction of coastal flood levels from hours to days of lead time to support emergency preparedness management and responses while several flood guidance systems are currently operational around the world focusing on global regional and local scales kourafalou et al 2015 the weather forecast office wfo from the national weather service nws is responsible for providing flood forecasting and alerts within the us the wfo relies on the storm surge guidance products produced by coastal models from the national center for environmental prediction ncep and the center for operational oceanographic products and services co ops the national oceanic and atmospheric agency noaa currently operates continental scale coastal flooding guidance systems specifically the extra tropical storm surge etss kim et al 1996 and the extratropical surge and tide operational forecast system estofs funakoshi et al 2012 supports the national weather service nws on a daily basis in addition to etss and estofs noaa has regional guidance systems like the chesapeake bay operational forecast system cbofs lanerolle et al 2010 which primarily focuses on providing forecast guidance in the chesapeake bay besides the government operated guidance systems research groups like the virginia institute of marine sciences vims and the university of north carolina unc maintain their own operational forecast systems e g tidewatch loftis et al 2019 and adcirc surge guidance system asgs fleming et al 2008a for flood guidance in the chesapeake bay the operational guidance systems developed by noaa e g etss estofs and cbofs are currently simulating storm surges without dynamically accounting for wave effects which can represent a contribution of 10 15 percent of water levels in the chesapeake bay due to wave setup garzon and ferreira 2016 while the asgs is the only fully coupled surge wave system kourafalou et al 2015 in operation today in the chesapeake bay it was designed to focus in the states of north carolina nc louisiana la and texas tx to provide decision support services to the public sector during active storms in the hurricane season additionally a station based static forecast correction used to improve the initial water level offset in those states leads to an exceptionally large offset of water levels at least 30 cm in the asgs forecasts in the chesapeake bay while the tidewatch was designed for the chesapeake bay it only provides the total water level predictions for a 36 h lead time which is shorter than the duration of the nws official flood forecasts in the chesapeake bay in addition water level forecasts from tidewatch are only available in the lower half of the chesapeake bay stations and are not publicly available to download for model comparison furthermore we hypothesize that water level guidance from a single guidance system may not always accurately represent the best predictions of total water levels in the chesapeake bay that an ensemble based guidance from different models inputs and configurations might provide a better overall total water level prediction in this paper we present a new flood operational guidance system for the chesapeake bay the iflood http iflood vse gmu edu which is a fully coupled surge wave guidance system based on the 2d depth integrated adcirc and the swan phase averaged spectral wave model and forced by the nam weather model this is the first flood guidance system to predict total water levels that incorporates wave set up dedicated to the chesapeake bay the iflood guidance system is currently initialized twice a day for a forecast span of 84 h the utility of the iflood system is demonstrated by its early adoption by the nws baltimore washington wfo lwx providing alternative flood related information at all the official forecasts stations in the chesapeake bay coastal water level forecasting in estuaries is particularly challenging and requires the integration of complex coastal and riverine processes cluckie et al 2000 accurate water level forecasting is highly dependent on correct atmospheric forcing and representation of complex coastline geometry in estuaries therefore the iflood was developed based on the best available topo bathymetric datasets and utilizes high resolution surface weather forcing to model water levels and waves in the bay in order to improve flood forecasting in large estuaries we investigate the use of real time bias correcting schemes salmun and molod 2015 and evaluate model ensembles in an effort to provide a unified flood guidance product to forecasters this manuscript presents the components of iflood guidance system its daily and extreme weather evaluation and the proposed improvements to operational water level forecasting in the chesapeake bay 2 methods 2 1 study area this study focuses on the chesapeake bay and its tributaries the largest estuary of the us scott phillips and blomquist 2015 xiong and berger 2010 the chesapeake bay is located in the mid atlantic region of the us along the east coast of united states and connected to the atlantic ocean the bay is approximately 320 km in length from the mouth of the bay to its head at the mouth of the susquehanna river the main stem has a surface area of 11 601 km2 and it runs entirely in the states of maryland and virginia the bay width varies throughout its axis with the mouth of the bay being 20 km and at the mouth of the potomac river being 45 km the depth of the bay is mostly shallow and almost half of the bay is less than 6 0 m deep only about 8 of the bay has a depth greater than 18 3 m xiong and berger 2010 the main stem and the tributaries of the chesapeake bay are defined as a tidal zone the bay is a complex estuary with more than 7000 km of shoreline and as many as 50 tributaries draining freshwater directly into it low lying areas along the shoreline of the chesapeake bay are more prone to periodic flooding and pose a threat to communities in the event of a tropical storm e g dorchester county maryland and accomack county virginia fig 1 shows the location of the chesapeake bay in reference to the mid atlantic region of the us noaa has several monitoring stations installed throughout the chesapeake bay and the atlantic ocean that record water level waves e g wave height wave period and wave direction and meteorological conditions e g winds speed wind direction and atmospheric pressure fifteen water level and eight wave monitoring stations as listed in table 1 are used to validate the water levels and waves forecasts from the iflood guidance system 2 2 state of the art 2 2 1 water level guidance systems the advanced hydrological prediction system ahps provides flood related forecast information from the nws https water weather gov ahps for the entire us the official water level forecasts issued by the nws hereafter referred as ahps are produced by expert judgment supported by existing surge based model guidance systems and flow forecasts from river forecast centers rfc etss estofs and cbofs are primarily responsible for providing water levels forecasts while the river flow forecast are provided by middle atlantic rfc in the chesapeake bay the etss model is based on two dimensional 2d depth integrated shallow water equations from the sea lake and overland surges from hurricanes slosh model jelesnianski and chen 1992 this model is forced by wind and pressure fields from the nws s global forecast system gfs at 0 25 resolution 28 km forecast cycles are initialized four times a day for an extended period of 102 h the estofs model is based on the most commonly used barotropic version of the advanced circulation adcirc 2d depth integrated 2ddi model luettich et al 1992a similar to etss estofs also makes use of gfs 0 25 28 km surface forcing to forecast storm surge for up to 180 h into the future and it is also initialized four times a day the cbofs is a regional guidance system specifically designed for the chesapeake bay and it is based on a three dimensional 3d baroclinic model the regional ocean modeling system roms shchepetkin and mcwilliams 2005 from rutgers university two meteorological outputs wind and pressure from the north american mesoscale s nam model are utilized as surface forcing to the cbofs model that is initialized four times a day and produce forecast guidance for 48 h into the future lanerolle et al 2010 the adcirc surge guidance system asgs fleming et al 2008a provides a framework supporting the use of the adcirc model for real time storm surge forecasting applications the asgs based forecast system is triggered by storm track advisories issued from the national hurricane center nhc and an expert team during the hurricane season and nam 12km meteorological data is used to simulate storm surge and waves during daily operations the coastal emergency risks assessment cera web based platform utilizes the asgs to support emergency operations for the state of north carolina the tidewatch system uses the semi implicit cross scale hydro science integrated system model schism a derivative product built from the original selfe zhang and baptista 2008 this system is forced by nam nest 5 km atmospheric forecast model to predict storm tide for a 36 h lead time around us east coast and gulf of mexico loftis et al 2019 2 2 2 waves guidance systems a number of noaa operational wave guidance systems provide forecasted information on the waves in the chesapeake bay which includes wavewatchiii global ww3 global wavewatchiii regional ww3 regional and near shore wave prediction system nwps in the atlantic ocean wave forecasts are not directly utilized to produce the official water level forecasts however official wave forecast are provide through nws marine forecast in the chesapeake bay ww3 global and regional models are based on the wavewatchiii model tolman 1989 which is a third generation wave model ww3 global and ww3 regional models are based on rectilinear nested grid ww3 global wave model provides boundary condition for a higher resolution ww3 regional wave model around the us east coast the input meteorological forcing in the ww3 global and ww3 regional models are provided by gfs wind model forecast cycles are initialized four times a day for an extended period of 180 h nwps van der westhuysen et al 2013 is based on the swan booij et al 1999 wave model which is also a third generation wave model ww3 and swan are both third generation wave models with similar source term packages however different numeric schemes ponce de león et al 2018 the nws forecaster s generated winds is used in the nwps model during the daily operations hudlow 1988 which is replaced with nhc forecasted winds during the hurricane season similar to ww3 global and ww3 regional forecast cycles for nwps are also initialized four times a day for an extended period of 180 h 2 3 iflood components 2 3 1 storm surge and wave prediction model hydrodynamic models coupled to phase average wave models are widely used to simulate storm surge and coastal inundation around the world casulli and walters 2000 hubbert et al 1999 n d luettich et al 1992a murty et al 1986 these models are regularly used for historic validation of storm flooding in coastal areas lakes and estuaries blain et al 2010 dresback et al 2013b garzon et al 2018 hanson et al 2013 shen et al 2006b in addition to real time coastal inundation guidance systems akbar et al 2013 dresback et al 2013b fleming et al 2008b funakoshi et al 2012 mattocks and forbes 2008 in this study the advanced circulation adcirc model is applied to simulate hydrodynamic conditions luettich et al 1992a which is a finite element hydrodynamic model based on an unstructured grid and used to simulate water levels currents storm surge and tides forced by meteorological fields e g wind and pressure and tidal constituents adcirc is an open source numerical model written in fortran and is very well documented both in the published scientific literature luettich et al 1992a and on the adcirc web site http adcirc org it has been used extensively for tidal and storm surge prediction bilskie et al 2015 blain et al 1994 garzon et al 2018 haddad et al 2015 li et al 2013 lin et al 2010 shen et al 2006a westerink et al 2008 the two dimensional depth integrated version of adcirc known as adcirc 2ddi is utilized in the iflood guidance system to simulate water levels storm surge tides forced by the forecasted meteorological information winds and pressure and astronomical tides the surge only adcirc only iflood guidance system was updated in november of 2018 to include a coupled surge wave forecast system adcirc swan dietrich et al 2011 by incorporating wave processes using the simulating waves nearshore swan model booij et al 1999 in the forecast operations as well swan is a third generation phase averaged wave numerical model used to calculate wave energy generation propagation and dissipation in shallow waters the adcirc 2ddi model computes water elevation using a vertically integrated continuity equation in the generalized wave continuity equation gwce form depth average current velocities are obtained from the solution of momentum equations whereas swan computes the wave energy from the wind radiation stresses computed by adcirc detailed information on the governing equations of adcirc and swan numerical models can be found at dietrich et al 2011 wave setup can contribute to an additional increase in water levels during storm surges the tightly coupled adcirc swan incorporates the wave setup contribution to total water level predictions dietrich et al 2011 based on local bathymetry and wave conditions wave setup can significantly contribute to storm surges sheng et al 2010 previous studies have demonstrated an increase of 0 1 0 2m in water levels inside the chesapeake bay from wave setup garzon and ferreira 2016 hanson et al 2013 therefore supporting the use of coupled surge wave modeling system in the bay the iflood hydrodynamic model grid domain which extends from 97 85 to 60 04 w and from 7 90 to 45 83 n encompasses the western atlantic the gulf of mexico and the caribbean sea the unstructured computational mesh was created by modifying the fema s region 3 numerical mesh garzon and ferreira 2016 it consists of 381 164 nodes and 750 479 elements in the iflood configuration and allows for higher horizontal resolution in the chesapeake bay each node in the mesh represents its longitude latitude depth and elements which are the group of nodes forming a triangle it ranges from hundreds of kilometers km offshore to tens of meters m in the bay and its tributaries minimum mesh size is 25 m the numerical mesh extends well beyond the wet areas in order to account for topography effects on the storm surge modeling the bathymetry was interpolated from the fema region 3 unstructured grid garzon and ferreira 2016 and merged with data from the national geophysical data center s coastal relief model crm and noaa digital nautical charts which have a horizontal resolution of 3 arc seconds 90 m topography from the united states geological services usgs digital elevation model dem archives was used to enhance the representation of land features dem from usgs has a resolution of 30 m over the chesapeake bay topography besides the chesapeake bay was interpolated from the fema region 3 unstructured grid all data was converted to the north american navd88 vertical datum forte et al 2011 2 3 2 model forcing 2 3 2 1 astronomical tides about 75 80 of the sea height variability during daily operations is represented by the astronomical tides as demonstrated by the observations made possible by the topex poseidon satellite altimetry le provost et al 1994 an accurate representation of the astronomical tidal variation in the flood guidance system is crucial for correct water level forecasting inside estuaries since errors in astronomical tidal simulations can introduce a significant noise in the total water level prediction various tidal databases are available around the world and can provide model boundaries and forcing for astronomical tidal simulations the le provost database le provost et al 1994 has been available for decades while tpxo developed by oregon state university egbert and erofeeva 2002 and the adcirc tidal database mukai et al 2002 are examples of most recent databases developed for the us east coast the accuracy of christian le provost le provost et al 1994 tidal simulations were found to be in the orders of 3 5 cm when compared with observations from nearly 103 coastal and island tidal gauges the open ocean boundary inside the iflood modeling domain was forced by the amplitude and phase of the eight major constituents k1 k2 o1 m2 n2 p1 q1 and s2 extracted from the le provost database the propagation of astronomical tides in large estuaries are also highly influenced by the bottom surface roughness resio and westerink 2008 which is represented by the manning s n coefficient in the adcirc model version used in this study previous studies in the chesapeake bay developed optimal spatial bottom surface roughness values that are utilized in the current formulation of iflood garzon and ferreira 2016 the iflood tidal analysis was performed for eight 8 major tidal constituents for 120 days in the year of 2018 jan 18 to apr 18 in order to analyze the model tidal prediction error the amplitudes and phases of major tidal constituents were studied for 12 stations inside the chesapeake bay fig 1 tidal simulations were forced with open ocean boundary forcing from the major tidal constituents k1 k2 m2 n2 o1 p1 q1 and s2 five principal tidal constituents including m2 n2 s2 o1 and k1 have been reported to be sufficient for accurately modeling tides in the mid atlantic region garzon and ferreira 2016 hanson et al 2013 a 5 day spin up was used to hot start the tidal simulation lasting for 4 months 120 days time series data of modeled astronomical tides was analyzed with a python package pytides based on the algorithm listed in the national ocean service nos publication schureman 1982 the modeled tides were decomposed into its constituents and compared against the noaa harmonic tidal constituents database at recording stations the amplitude is given in meters and the phase is relative to the greenwich meridian in units of degrees 2 3 2 2 north american mesoscale nam 12 km model the north american mesoscale forecast system nam is one of the main weather models operated by the ncep for producing weather forecasts for the us the nam is a non hydrostatic multiscale numerical weather prediction nwp model on the b grid and it utilizes a hybrid sigma pressure vertical grid to solve vertical momentum equations for weather modeling janjic 2005 the nam is based on the noaa earth modeling system nesm which is further based on the earth system modeling framework da silva et al 2002 several weather variables are available from the nam grids ranging from temperature and precipitation to lightning and turbulent kinetic energy the nam generates multiple grids or domains of weather forecasts over the north american continent at various horizontal resolutions high resolution forecasts are generated within the nam using additional numerical weather models these high resolution forecast windows are prepared over fixed regions and are triggered on an event based scales to follow significant weather events such as hurricanes the nam forecasts are made four times daily at 00 06 12 and 18 utc hours with a duration of 84 h into the future the nam 12km conus grid 218 best represents the atlantic ocean and the gulf of mexico with the highest resolution of 12 km the iflood utilizes the 10 m wind velocity u component of wind height above ground v component of wind height above ground and pressure fields pressure reduced to msl msl over the contiguous us to force the coupled adcirc swan model the nam 12km conus grid does not extend below 15 north therefore in order to accommodate the missing data points in the model grid the meteorological surface forcing is set to zero these forecasted products become available at the noaa ncep server http www ftp ncep noaa gov data nccf com nam prod as they are produced water levels in the chesapeake bay are significantly impacted by offshore winds and the astronomical tides shen et al 2006a thus the errors associated with forecasting winds and tides undermine the overall forecasting of water levels in the bay significant wave heights are highly correlated with local wind speeds and incoming swells cavaleri et al 2012 lin et al 2002 semedo et al 2015 the storm surge simulations driven by nam produced more accurate results in comparison to simulations forced by gfs during the hurricane isabel irene and sandy garzon et al 2018 in the chesapeake bay therefore the surface weather forcing in the iflood guidance system uses the high resolution nam model to forecast the storm surges 2 4 iflood guidance system operational workflow the following sections describe the methods used to produce the flood guidance this includes the retrieval and construction of input data the setup of the numerical models the structure of the workflow used to produce results and the web publishing of forecast information for the community 2 4 1 meteorological forcing processing the first step in the iflood forecast operation is to search for the newly available meteorological files from ncep on the noaa portal ncep uploads forecasted weather conditions raw files on their server daily every 6 h an automated python script looks for newly uploaded files and downloads it to the iflood server the iflood guidance system is designed to utilize nam gfs and the nhc best track data however the current iflood operational setup described here is utilizing the nam forcing to simulate water levels currents and waves the forecasted winds at a height of 10 m u10 v10 and pressure at mean sea level msl produced by the nam model are utilized to force the iflood guidance system two times a day 06 and 18utc during the forecast operation a python script downloads the recent forecasted nam files extending up to 84 h the original nam files contain voluminous gridded meteorological data in the gridded binary grib2 file and are projected in lambert conformal conic lcc projections nam files are converted to binary format netcdf network common data form maintained by unidata and are re projected to the geographical coordinate system gcs using wgrib2 utility developed by ncep 2 4 2 hindcast short term forecast and forecast setup this subsection explains the workflow of the hindcast short term forecast and forecast operation inside the iflood framework the hindcast setup of the iflood is initialized as a cold run meaning that the simulation starts with initial water levels set to zero across the entire model domain initial conditions and only tidal forcing is applied for several days e g 10 15 days until the calculated water levels match the noaa astronomical tidal predictions this process is only repeated again if the system needs to be re started the hot start files are written at the end of simulation to save the current state of the ocean the hindcast setup is usually required when the iflood guidance system is shut down due to power outages the hot start files will serve as the initial boundary condition file for a new forecast simulation in order to estimate the model initial conditions that best represent the current state of the ocean for each forecast cycle a short term forecast is produced within each forecast cycle this is necessary as the current version of adcirc swan used in this system does not save hot starting files along the simulation runtime the short term forecast is created by running the adcirc swan numerical model to simulate water levels for a period of 12 h into the future forced by the wind and pressure fields from the 6 hourly data assimilated da cycle of nam along with astronomical tidal forcing the water levels generated at the end of the 12 h simulation period are used as a hot start file for initializing the next forecast cycle the first cycle of short term forecast starts at 06 utc and saves the information for the 18 utc forecast and likewise for ongoing forecasts note that the short term forecast procedure will become obsolete once the adcirc swan model is updated to output and archive hot start files at user defined time steps the forecast simulation begins at the conclusion of the short term forecast therefore the forecast is on the same cycle as the preceding short term forecast starting from the same hot start file and forecasted information about wind atmospheric pressure and astronomical tides this simulation will run for three and a half days 3 5 which is the entire duration of the nam forecast released by ncep no hot start file is generated from the forecast the completion of the forecast simulations results in predicted conditions of water levels currents and waves for next 84 h fig 2 shows the workflow for hindcast short term forecast and forecast setup 2 4 3 high performance computing hpc a 56 core linux server zeus at the mason flood hazards research lab is dedicated for the iflood forecasting zeus is parallelized using hyper threading to split the computational tasks into 56 sub process while each dedicated processor takes part in the numerical calculations of the adcirc swan and reduces the time required to complete the simulation zeus takes about 1 75 h of computational time to complete a forecasted simulation of 84 h for adcirc swan in addition to daily operational forecast cycles instances of amazon web services aws are also initialized during hurricane season to run multiple scenarios of active hurricanes for probabilistic storm surge predictions aws forecast manager t3 low cost burstable amazon elastic compute cloud instance ec2 serves as the resources manager to execute and monitor the forecast instances of iflood for nhc based multiple track simulations 2 4 4 validation metrics the validation of the iflood modeled wind speeds water levels and wave heights is done in real time and offline by comparing the forecasted variables against observations and operational model guidance systems inside the study area the water level predictions from etss estofs and cbofs and wave forecasts from ww3 global ww3 regional and nwps were retrieved twice a day and compared against the iflood predictions for an overall comparison of the existing guidance systems in the chesapeake bay individual assessment of the forecast performances are made routinely by the respective modeling teams however limited records are available of those studies the validation scheme is operational for 14 gage stations nine meteorological stations and six wave buoys an analysis of the system performance for a period of 6 months is presented in the results section the average bias eq 1 measures the mean deviation in the predictions from the observed data the positive values of bias represents the over prediction of the model guidance while the negative values of bias means under prediction the root mean square error rmse eq 2 reported is between the modeled and observed time series for the entire 84 h lead time period for each forecast cycle for the extreme weather evaluation the rmse values are computed between the single forecasted peaks and the measured peaks the probability of detection pod eq 3 is used in the categorical forecast evaluation to determine the model s accuracy in predicting the nws flood levels assigned as action the nws has specified flood levels at the water level recording gages that corresponds to a certain level of threat with the action flood level being the smallest of them all action minor moderate and major this flood level is used as a threshold in determining the pod bias and rmse are in meters m while pod is presented in percentage 1 bias 1 n i 1 n p i o i 2 rmse i 1 n p i o i 2 n 3 pod d e t e c t e d f l o o d s t o t a l n u m b e r o f r e c o r d s 100 where oi are the observations and pi are the predicted values from the model whereas i is the time interval and n are the total number of datasets detected floods is the number of times a model issued forecast guidance was noted more than or equal to the action level height m at that station 2 4 5 web based flood guidance data portal flood warning systems are increasingly using cloud based technology like aws for pre processing the inputs execution of numerical programs and post processing of forecasts morsy et al 2018 a dedicated web portal for the iflood data visualization situation awareness and data download appendix c is available at the following url https iflood vse gmu edu the meteorological forcing from the most recent meteorological forecast cycle are preprocessed in the local server as described in section 2 4 1 the binary outputs netcdf from the most current iflood simulations are converted to various data formats geojson esri shapefile kml text files etc using a set of python and bash the local server takes an additional 35 min to complete these post processing tasks compared to the forecast simulation time 1 75 h mentioned in section 2 4 3 and all the processed data is copied to the amazon s3 for public sharing and storage in addition to forecasted time series data at the stations map based products of water levels wave components atmospheric pressure and wind time series are generated to support the forecast visualization a prototyped network of internet of things iot water level monitoring sensors is also deployed around the belmont bay area va that records the water levels every 6 min these iot based sensors provide additional validation information for the iflood forecast system besides the noaa observations additionally an alert service based on sms messages and email is provided using amazon s lambda function and python scripts for the dissemination of information to users subscribed to iflood the iflood web portal allows users to select alert levels for different water levels or wave stations in the chesapeake bay the overall iflood workflow is summarized in fig 3 2 4 6 case study hurricane dorian 2019 hurricane dorian aug was the first major hurricane of the 2019 hurricane season jun sep which caused significant storm damages along the eastern sea board of the u s the iflood forecasts for the six water levels and meteorological stations and six wave buoys inside and around the bay fig 4 were evaluated against observations of measured hourly water levels winds and significant wave heights the nam 12 km meteorological forcing was used to force the iflood system to forecast the storm surges and waves resulting from the tropical cyclone the iflood forecasts have a maximum lead time of 84 h therefore the noaa operational forecasts were trimmed to 84 h of forecast for this analyses water level winds and waves guidance predicted by iflood were compared with the other noaa operational flood guidance systems etss estofs and cbofs and wave forecast systems ww3 global ww3 regional and nwps from 3rd september to 7th september additionally nam 12 km and gfs 27 km operational wind forcing is compared against the noaa observations for evaluating the accuracy of operational winds 10 m height in simulating water levels and waves over chesapeake bay during the extreme weather 2 4 7 real time bias correction ensemble guidance various guidance systems utilize a post bias correction scheme for minimizing the forecast error resulting from missing physical processes nws also uses a similar correction factor called fudge factor in order to create official flood forecasts among noaa guidance system in the chesapeake bay only etss model results are post processed kim et al 1996 at all the forecast stations by applying a bias correction scheme to the water level predictions the etss model guidance for its 24 h lead time prediction is compared against the noaa observations at individual stations to calculate the forecast anomaly etss predictions minus observations at every forecast the forecasted anomaly is averaged over last 5 days and added to the latest etss water level guidance to minimize the prediction errors at the observing stations salmun and molod 2015 the water levels predicted by iflood were compared against the observations to compute its bias based on the previous 12 h of forecast the computed model bias for each forecast station was used as a correction value to the future forecast advisories a bias correction scheme similar to etss was tested on the calculation for the correction value at each individual station the correction value is based on the raw bias between the predictions and observed data equation 4 for the first 12 h of previous forecasts this bias was computed for the last 1 2 5 10 25 and 50 days average and applied as a correction to the latest water level forecasts equation 5 4 correction value j 1 d i 1 n o i p i d 5 bias corrected iflood raw iflood model outputs correction value where i is the hourly forecast n is the total number of hourly forecast data points compared against observations from the previous forecasts also j is the number of days considered in the previous forecast and d is the total number of days for which the bias was averaged for each station p is the water level predictions and o is the water level observations single deterministic forecast from one model could be misleading as they may fail to predict accurately therefore nws is interested in utilizing a suite of model generated water level predictions ensembles to forecast official flood forecasts the ensemble guidance for water levels prediction was produced by combining the individual guidance systems to improve the overall flood forecasts an equally weighted average scheme was applied to produce three alternative ensemble forecasts using equation 6 6 ensemble w 1 p 1 w 2 p 2 w 3 p 3 w n p n n where w is the weighting factor used for each model guidance prediction and p is the model prediction from selected guidance system also n is the total number of guidance systems 1 ensemble1 was produced by combining all four existing guidance systems etss cbofs estofs iflood representing a balance between nam and gfs forced coastal systems for this case the iflood estofs and cbofs were not bias corrected whereas the etss outputs were bias corrected by noaa before public distribution 2 ensemble2 was created by merging only the etss cbofs and iflood predictions representing the systems that had the least rmse of all the guidance systems when analyzed in section 4 2 1 similarly to ensemble1 the etss guidance is bias corrected while the iflood and cbofs guidance are the raw model outputs 3 ensemble3 was formed by grouping only the bias corrected etss and raw model outputs of cbofs 3 results and discussion this section presents an evaluation of the performance of the iflood guidance system during daily and extreme weather operations the predicted water levels winds and waves were compared against the noaa observations and other existing operational guidance systems in the chesapeake bay 3 1 astronomical tidal prediction evaluation results of the major tidal constituents of m2 and n2 are presented in fig 5 the amplitude and the time of arrival for crest and troughs was predicted satisfactorily and the tidal constituents were in good agreement with tidal constituents at noaa stations except for a few stations upstream the potomac river as shown in fig 5 a maximum tidal amplitude underestimation of 0 1 m was recorded for m2 constituent at the washington d c station while the stations in the main channel and at the mouth of the bay showed much less discrepancy bias less than 0 01 m between iflood and noaa tidal constituents the tidal phase was accurately predicted at the mouth of the bay however the difference between the modeled phases and the noaa stations varied by approximately 12 20 in the upper parts of the main channel as depicted by bism tbmd bltm and apam stations the detailed analysis on the individual tidal constituents can be found in appendix a fig 6 presents the rmse for the astronomical tidal amplitudes and phase of major constituents m2 n2 a rmse of tidal amplitude smaller than 0 20 m and phase below 22 5 are accepted for operational use according to the nos skill assessment criteria peng 2018 the results of the iflood tidal analysis showed an overall similarity to the findings from previous validation studies funakoshi et al 2012 garzon and ferreira 2016 lanerolle et al 2010 on average the modeled tide error rmse inside the chesapeake bay is found to be in the ranges of 0 01 0 29m garzon and ferreira 2016 0 01 0 38m lanerolle et al 2010 0 15 0 23m funakoshi et al 2012 and 0 01 0 08m hanson et al 2013 while the phase difference for the major tidal constituents was in the range of 10 40 garzon and ferreira 2016 the amplitudes and phases errors modeled by iflood were under the maximum range 0 08 m 20 for m2 and s2 at all the stations inside the bay fig 6 the under prediction of the tidal amplitudes and phases in the shallower parts of the potomac river wasd lwtv can be attributed to discrepancies in the bathymetric datasets and the representation of recent potential channel dredging in the numerical mesh the published literature also documented similar under prediction in the upper reaches of the tidal potomac river funakoshi et al 2012 garzon and ferreira 2016 lanerolle et al 2010 these authors also argued that the shallower depth curvature of the tributaries and bottom roughness could be the possible causes for this misrepresentation furthermore another sources of error could arise from missing baroclinic effects and seasonal variability of water levels in the bay lanerolle et al 2010 3 2 operational iflood predictions the iflood performance is evaluated based on water levels winds and waves from the coupled system adcirc swan produced twice a day for six 6 consecutive months in 2019 january june the extreme weather evaluation was performed during hurricane dorian 2019 evaluating the predicted water levels winds and wave heights from the coupled iflood adcirc swan the iflood forecasts were evaluated by comparing the model results against the noaa observations and other existing operational flood guidance systems etss estofs and cbofs and wave forecast systems wavewatch3 global ww3 regional and nwps daily operations refers to twice a day forecasts at 06 utc and 18 utc hours hereafter referred as daily these forecasted model outputs of water levels waves and winds from the iflood are also referred as forecast advisory in this section 3 2 1 daily forecasts winds the nam 12 km operational winds driving the iflood showed an overall negative bias under prediction of 2 8 m s compared to the observations when averaged over all the recording stations fig 7 presents a scatter plot of wind speed calculated by iflood after interpolation and drag modifications of the nam wind forecast from the coupled adcirc swan numerical model compared with noaa measurement at nine monitoring stations inside the chesapeake bay the scatter plot of predicted and observed wind speed showed that wind forecasts are in good agreement with observations however they tend to under predict larger bias of 10 m s for wind speeds greater than 15 m s in lower bay cbbv yktv kptv the mean rmse for nine meteorological station fig 1 was 3 72 m s and individual values are given on fig 7 the horizontal width of the upper bay varies between 18 and 20 km whereas the resolution of the nam weather model is 12 km this coarser resolution of the nam weather model in relation to the bay geometry resulted into few wind change locations grid points in major parts of the bay making it difficult to accurately represent the forecasted winds at noaa stations in the bay the gfs operational wind forcing used to drive the etss and estofs model were not evaluated against the nam wind forcing used in iflood which limited the direct comparison of gfs and nam operational winds during the daily operations however a hind cast study evaluated reanalysis winds at 10 m height comparing nam and gfs in four buoys around the us and found an average under prediction of 1 75 m s for nam winds speeds and 2 4 m s for gfs wind speeds at diamond shoals nc wang et al 2018 another study also concluded that a storm surge model forced by the nam reanalysis winds were able to capture the maximum storm surges more accurately when compared to the same model forced by the gfs in the chesapeake bay garzon et al 2018 while results are only shown here for the wind speed the wind direction present the similar patterns water levels the water level forecasts of iflood were simulated using the astronomical tides wave radiation stress from swan and nam 12 km wind forcing the highest water level measured at the noaa monitoring stations fig 1 during each forecast advisory were compared against the iflood predicted peaks for 15 stations inside and around the chesapeake bay fig 8 the scatter plots shows that the peaks of the predicted water levels matched well with the observations with 0 2 m variations at the lower bay and increased to a maximum of 0 4 m in wasd the rmse values for each station is averaged for the entire period and shown on fig 8 analysis of predicted water levels in the lower bay cbbv swpv kptv and yktv resulted in a small rmse in the range of 0 10 0 12 m results in the middle bay bism slim and lwtv showed rmse between 0 16 and 0 21 m whereas the maximum rmse of 0 34 m was observed for wasd fig 8 overall the model rmse for water level predictions was noted to be lower than 0 2 m for 84 h lead time for the majority of the stations except for wasd lwtv and bltm and were within the criteria set rmse lower than 0 2 m by nos peng 2018 the under prediction of the forecasted winds by the nam 12 km weather model and tidal amplitude errors discussed in previous section could explain the inaccurate modeling of water levels for stations with rmse greater than 0 2 m post processing bias correction due to consistent errors from winds and tidal amplitude and any seasonal variability on the water levels could lead to an improved forecast performance in the shallower parts of the bay this correction becomes necessary as demonstrated by tidal errors in the wasd station which had a consistent under prediction bias of 0 1 m in its m2 tidal amplitude and if removed can reduce the water level bias from 0 32 m to 0 22 m in order to evaluate the model performance based on different lead times the station based lead time guidance was computed for 12 24 36 48 60 72 and 84 h of forecasted information and is presented in appendix b the rmse associated to the forecast advisories at individual lead times during the period of 6 months jan 19 to jun 19 presented a small variance 0 05m in rmse from 12 h lead time to 84 h lead the stations at the lower bay kptv cbbv and swpv had lower rmse with the increasing lead times whereas wasd station inside the potomac river demonstrated an increase in rmse for extended lead times 84 h the initial conditions for every forecasts cycle are provided by the short term forecast which does not accurately represent the state of the ocean this leads to a slightly higher forecast error offset at the beginning of forecast lead time resulting in increased rmse the water levels predicted by iflood showed the lower bias for bism 0 08m and yktv 0 015m when compared to other guidance systems fig 9 in addition an increasing bias compared to etss and cbofs was noted for the stations in the shallow parts of the potomac river for example lwtv and wasd with average values of 0 06 and 0 16 m respectively the bias for the iflood water level predictions at lower bay stations was 0 04 m while a larger bias 0 15 m was calculated for the stations at the upper bay initial forecast evaluations of estofs reported lower rmse less than 0 2 m for the stations at the lower bay and higher rmse above 0 2 m for stations in the shallower parts wasd lwtv funakoshi et al 2012 cbofs presented the lowest bias for the lwtv 0 05 m while the higher bias was reported for kptv 0 2 m bias corrected etss exhibited the least forecast bias compared to the other guidance systems with the lowest bias of 0 025 m averaged over all the stations in the bay the estofs guidance system had the maximum forecast bias of 0 18 m averaged for all the stations inside the bay the evaluation of the other guidance systems also provides an insight into the effects of barotropic and baroclinic model configurations in predicting the water levels in the bay the baroclinic scheme utilized by the cbofs forced with the nam 12 km meteorological data during the period of analysis resulted in similar bias patterns than the barotropic models throughout the bay lanerolle et al 2010 it was interesting to note that the baroclinic cbofs water level prediction had a positive bias over prediction at all the stations contrary to barotropic etss estofs and iflood with an overall negative bias under prediction while temperature and density changes inside the chesapeake bay impact the overall bay circulation blumberg 1978 demirbilek et al 2008 majumdar et al 1987 the results suggests that the daily water levels predictions inside the bay produced by operational guidance systems considering the baroclinic effect cbofs are not significantly better than the barotropic models iflood and estofs unlike other noaa guidance systems the iflood water level prediction showed varying bias throughout the bay for example negative bias at shallower parts of the bay upper and middle and positive bias at lower bay with an exception of kptv the cbofs baroclinic setup during the daily operations had a consistent positive bias the estofs negative bias was similar to previous studies funakoshi et al 2012 while also satisfying the nos criteria peng 2018 in the etss modeling framework the noaa predicted astronomical tides are added to the gfs driven storm surge which eliminates the consistent tidal amplitude error that may be present in other guidance systems water levels predicted by etss are further corrected with a post bias correction scheme therefore the evaluation of etss resulted in least forecast bias fig 10 shows the spatial distribution of the error from water levels predicted by iflood denoted by rmse against the other operational guidance systems the ability of iflood to predict floods was demonstrated with the rmse values in the range of 0 1 0 20 m similar to etss 0 1 0 25 m inside the study area the rmse for cbofs was in the orders of 0 1 0 25 m while estofs had the highest rmse of all the guidance systems in the order of 0 25 0 35 m the rmse calculated during the period of analyses for the operational guidance systems were similar to previously published studies funakoshi et al 2012 kim et al 1996 lanerolle et al 2010 for their water levels forecast evaluation the probability of detection pod metric was used to evaluate the capability of iflood in predicting floods in comparison to the other operational flood guidance systems etss estofs and cbofs as shown in fig 11 the observed flood peaks action level usgs flood stage per advisory were compared against the predicted flood levels from the guidance systems modeled flood peaks above the designated flood levels at each noaa monitoring station were isolated and analyzed for each observed peak signals iflood detected the flood levels 90 of the times in cbbv and nearly 94 in the yktv demonstrating the similar pod compared to bias corrected etss guidance systems at the mouth of the bay iflood and estofs had the lowest pod for wasd when compared to other guidance systems the pod values for cbofs ranged between 60 and 80 at majority of stations except wasd pod value of 30 in the bay etss showed an overall highest ability to predict floods compared to other guidance systems at all the stations interestingly the guidance systems were able to predict the flooding more accurately at the entrance of bay compared to the stations in the upper bay and the potomac river several reasons can be attributed to missing some of the flood events for example the uncertainty in the weather forcing consistent noise in the modeling tidal amplitude error and absence of river discharge to account for rainfall induced flooding these contributing factors disrupt the ability to predict floods and might be more prevalent under some events and less in others waves wave modeling inside the iflood domain is dependent on the nam 12 km meteorological forcing and was able to predict wave heights with a bias of 0 05 m averaged over all the six stations for the 6 months of analyses significant wave heights forecasted by iflood were compared against the observed datasets from the national data buoy center ndbc buoys are shown as a scatter plot in fig 12 the recorded peak wave heights at the buoys during the analysis period did not exceed 5 5 m the mean value of the bias for the significant wave height predicted by iflood at the ndbc buoys was in the range of 0 01 0 12 m over prediction results at the vbva and dfnc stations had a positive bias of 0 02 m in the significant wave height while dbde tsva wiva and chva showed a negative prediction of 0 05 m the negative bias under prediction in the predicted winds fig 7 during the daily forecast operations resulted in a slight decrease in the predicted peaks of significant wave heights wave period and direction followed the same trend as of significant wave height fig 12 also suggests that the wind driven wave heights can be predicted by the iflood coupled modeling system with the rmse values ranging between 0 26 and 0 84 m for all the six stations inside and around the chesapeake bay fig 13 shows the comparison between the significant wave height forecasted by iflood and the noaa operational wave guidance system ww3 global ww3 regional and nwps six ndbc buoys inside and outside the bay had continuous record of observed wave height for the same period the lower resolution of wavewatch3 global 3 stations out of 6 and regional model 4 stations out of 6 in the chesapeake bay only provided forecasts for a few stations in the study area for wave forecast evaluation the nwps model domain baltimore washington weather forecast office only covers the chesapeake bay and includes tsva flva chva and wiva buoys for model comparison the bias in wave height prediction against the observations varied from 0 24 m to 0 19 m iflood wave height forecast compared to other operational system demonstrated the lowest wave height bias for the station chva 0 009m followed by dfnc 0 017 m wiva 0 022 m and vbva 0 024 m a maximum bias of 0 124 m was recorded at the tsva significant wave heights forecasted by nwps showed maximum bias for wiva as this station is almost at the nwps model domain washington baltimore wfo model domain the use of initial conditions from the ww3 global for the open ocean boundary in the ww3 regional and nwps improves the wave modeling westhuysen et al 2014 the wave initial conditions inside the iflood are provided using the nowcast simulation of adcirc swan wavewatch3 global and regional operational wave forecast system are currently using the structured numerical mesh to model the waves while nwps westhuysen et al 2014 and iflood uses an unstructured mesh throughout the modeling domain the evaluation of wave heights modeled using nws forecasted winds and gfs winds in the nwps and ww3 global and regional wave forecast models are calculated to perform equally well with slight variations in the absolute bias values averaged over the ndbc buoys ww3 regional model showed the least bias of 0 05 m followed by nwps 0 11 m and ww3 global 0 17 m averaged over the ndbc buoys it is also interesting to note that ww3 global and regional forecasts are output every 3 h in contrast to iflood and nwps which could result in missing peaks of the forecasted significant heights the results here are shown for significant wave height only however similar results were found for wave direction and period 3 2 2 extreme weather forecasts hurricane dorian emerged off the west coast of africa on august 24th due to a strong tropical wave nhc classified dorian as tropical depression five which rapidly intensified into the tropical storm over the course of the next 2 days and on august 27th brought tropical storm winds and rainfall to barbados dorian progressed from hurricane category 1 to 5 on the saffir simpson wind scale with maximum sustained winds of 203 mph resulting from the favorable environment on august 28th 31st on 1st september dorian made landfall on great abaco islands of bahamas with sustained winds of 185 mph hurricane category 5 and slowly moved to grand bahamas the next day dorian weakened to a category 1 hurricane on 3rd sep but the system regained category 3 hurricane strength on 5th sep while moving over the gulf streams however increasing wind shear caused the storm s winds to weaken to a category 2 hurricane on the same day hurricane dorian made landfall near cape hatteras north carolina on friday 6th sep as a category 1 hurricane and impacted the southeastern virginia coast the hurricane track issued by the nhc for dorian passing near the chesapeake bay is shown in fig 4 winds the forecasted wind speeds from nam and gfs models were compared at five stations fig 4 inside the bay during hurricane dorian between sep 3rd to sep 7th fig 14 the resolution for nam winds was 12 km and the gfs winds was 27 km both the wind forcings were interpolated to the same numerical mesh computational nodes to allow for direct comparison of winds the operational winds for both the models were available hourly however the winds were only extracted for 84 h lead time at three hourly outputs wind speed results at 10 m height for both models showed almost same ability of prediction at all the noaa stations for example the highest measured wind speeds of 23 m s were accurately predicted bias of 0 05 m s by both the models at cbbv the stations in the middle bay lwtv and bism also showed same under prediction bias of 1 m s the spatial distribution of the error in the predicted wind speeds is presented as rmse in fig 15 nam 12 km showed smaller value of rmse 2 45 m s compared to gfs 27 km 3 2 m s for the 5 noaa recording stations over the period of sep 3rd to sep 7th during hurricane dorian wind speeds calculated at cbbv and kptv had the same rmse while it increased for the gfs in the middle bay yktv and lwtv results are only shown here for the wind speed however the wind direction present the similar patterns the winds analysis of the nam vs gfs suggest that lower horizontal resolution of the nam could capture the spatial distribution more accurately than gfs the variations in the approaching angle strength and the size of storm from both the models could also explain the rmse in the forecasted winds from nam and gfs models water levels the peak of the water levels at the noaa recording stations were observed on sep 6 2019 as hurricane dorian passed closely to the chesapeake bay hurricane induced water level forecasts were compared starting 3 days prior to the measured peak at the noaa recording stations the water level calculated from all the four guidance systems were retrieved at 0600 and 1800 utc times from sep 3rd to sep 7th and compared against the measured peak water level at six recording stations fig 4 a scatter plot comparing the maximum water level measured for the six noaa stations in the bay against the predicted maximum water level from iflood and the other operational guidance systems etss estofs and cbofs during the forecast period sep 3rd to sep 7th is displayed in fig 16 fig 16 shows that maximum water levels forecasted by iflood during all the forecast advisories were in agreement with the noaa observations at the lower bay kptv swpv cbbv and yktv the maximum water levels within the red oval forecasted by iflood resulted in a negative bias 0 16 m averaged over all the forecast stations fig 4 as the majority of the forecasts leading towards the landfall 6th sep were under predicting the water levels the etss forecasts also underestimated the water levels with bias of 0 05 m averaged over all the stations but outperformed all the other operational guidance systems for example estofs bias of 0 38 m and cbofs bias of 0 26 m the iflood was noted as the second best guidance system in predicting water levels given by the rmse of 0 15 m after the etss rmse of 0 12 m and performed better than cbofs rmse of 0 24 m and estofs rmse of 0 34 m averaged over forecast stations as shown in fig 17 the wind and pressure spatial temporal and intensity variation predicted by the nam 12 km and gfs weather models could help explain the forecasted storm surges errors by each respective guidance system it is interesting to note that the water levels predicted by the iflood using the coupled model adcirc swan during hurricane dorian also showed similar results to the daily operational evaluation the variation in the predicted water levels by gfs forced etss and estofs and nam forced cbofs and iflood could be the result of variations in the approaching angle strength the size of storm and the horizontal resolution of the weather models in addition the use of coupled adcirc swan modeling setup in the iflood could have captured the wave setup resulting in better predictions compared to estofs and cbofs a bias correction similar to etss could result in lower rmses for iflood estofs and cbofs in predicting the water levels the results from forecasting hurricane dorian storm surges suggested that the coupled modeling framework forced by nam 12 km can predict water levels with a small overall under prediction rmse of 0 21 m further evaluation of the operational winds from gfs and nam was performed later in this section to evaluate the error in predicting the wind speed during hurricane dorian from these systems waves the significant wave height predictions during hurricane dorian were also compared against the ndbc buoy observations and with the other operational wave forecast systems ww3 global ww3 regional and nwps ww3 global and regional forecast models were initialized every 6 h 00 06 12 18 utc and forecasted gfs driven waves at 3 h intervals out to 180 h nwps waves forecast system used nhc prepared parametric winds during the hurricane season to forecast the wave heights for a lead time of 145 h similar to water levels evaluation the wave heights from noaa wave forecast systems ww3 global ww3 regional and nwps were trimmed to 84 h of forecast and compared to ndbc buoys observations for the forecast advisories issued between sep 3rd to sep 7th a scatter plot comparing the maximum wave height measured at the ndbc buoys to the predicted maximum wave height of iflood and the other operational wave forecast systems ww3 global ww3 regional and nwps at ndbc buoys during the forecast period sep 3rd to sep 7th is displayed in fig 18 hurricane dorian generated significant wave heights greater than 6 m at vbva and oinc stations and higher than 2 m at the entrance of the bay chva flva and tsva as it made landfall on sep 6th 2019 fig 18 shows that iflood predicted the significant wave heights more accurately at oinc bias of 0 1 m tsva bias of 0 22 m and vbva bias of 0 25 m however overestimated the significant wave heights by 1 m at vbva and chva in comparison to the other operational wave forecast systems ww3 global ww3 regional and nwps ww3 global and ww3 regional showed an under prediction of waves whereas nwps predicted the waves more accurately at tsva and flva fig 19 shows the spatial distribution of the rmse values at individual stations from iflood ww3 global ww3 regional and nwps the rmse of the wave forecasts issued between sep 3rd to sep 7th showed a value of 1 41 m for ww3 global followed by ww3 regional 1 22 m and nwps 0 63 m during the same duration iflood wave forecasts showed the lowest rmse 0 62 m averaged at all the stations the analysis of the wave heights suggest that iflood can predict the wave heights with a lead time of 3 5 days using the nam forcing significant wave heights predicted by the iflood were underestimated before the landfall which resulted from a cold start simulation of the wave forecasts the opposite trend was noted for the ww3 global and regional domains as the underestimation of forecasted waves started to decrease ww3 global and regional forecasted waves were output every 3 h which could have also resulted in missing peaks for both the guidance systems significant wave heights predicted by nwps showed more variability between the first and last forecast advisory for all the stations ww3 global and regional models makes use of the data assimilation that improves the forecast error by initializing simulation with a better representation of the ocean state therefore to reduce the wave forecasting error in iflood hot starting the simulations for wave forecasting was later incorporated into the iflood workflow analysis here is demonstrated for wave heights only however wave direction and period also showed same results 4 improvements to iflood operational forecasts this section discusses potential improvements to operational water level forecasting in the chesapeake bay based on bias correction schemes and ensemble forecasts of the currently available guidance systems 4 1 post bias correction real time bias correction was applied at all the forecast stations to investigate the effect on the water level predictions fig 20 shows the time series plot of water levels at wasd and apam for the iflood forecasts issued on january 27th 2019 the correction values at both stations for future flood forecasts was computed based on equation 4 and resulting bias corrected iflood forecasts yellow line were compared against observations the under predicting water levels in the previous forecasts for wasd resulted in a negative correction value 0 16 m this correction value when added to the latest iflood forecast improved the raw iflood forecast green line rmse from 0 19 to 0 09 m fig 20 also shows that the bias corrected iflood forecast is matching well with the noaa observations contrary to wasd apam resulted into a positive bias 0 16 m which when removed from the latest iflood forecasts showed an increase in the rmse from 0 11 m to 0 20 m fig 21 presents the rmse of the bias corrected water level forecasts from iflood compared against the raw iflood predictions at each station the bias corrected water levels showed a decrease in the rmse for wasd and bltm stations only the rmse value for wasd was reduced noticeably from 0 22 m to 0 15 m by applying the correction value calculated based on the last 1 day the rmse value for the remaining stations was either increased cbbv swpv or remained constant apam the single correction value did not improve the overall bias since the bias in water level predictions is not always consistent this correction scheme worked well for the wasd station where a consistent bias was present as evaluated from the astronomical tidal analysis in section 4 1 however the same correction scheme for the remaining stations introduced a synthetic error by offsetting the predicted water levels at extended lead times therefore this bias correction scheme was only applied to iflood forecasts at wasd for future forecast advisories 4 2 ensemble forecasts evaluation of ensemble based flood guidance was only performed for three possible ensembles discussed in section 2 4 7 ensemble1 ensemble2 and ensemble3 fig 22 shows the overall rmse considering the 6 months period jan 19 to jun 19 from the these ensembles alternatives bias corrected etss and the ahps official forecast benchmarked against the noaa observations for all the stations inside the bay the rmse for ensemble1 showed the maximum values among all the ensembles compared to the ahps at majority of the stations while it was significantly reduced at cbbv ensemble1 resulted in a rmse of 0 13 m for the stations inside the chesapeake bay while wmva had the least rmse 0 09 m ensemble2 predictions resulted in the lowest rmse of all the three ensembles with the exception of wasd station the rmse for ensemble2 was also lower compared to ahps at the majority of the stations except for ocim wasd cbbv and kptv the rmse value for the wasd was smallest for ensemble3 compared to the other ensembles however it was greater than ahps at wasd the ensemble3 values of rmse ranged between ensemble1 and ensemble2 while it had the highest rmse value of 0 26 m for cbbv fig 22 shows that the ensemble2 has the lowest rmse for nearly all the stations inside the bay the best performing ensemble2 is the result of combining one over predicting cbofs and two slightly under predicting iflood and etss model guidance as expected the choice of the members forming an ensemble greatly influences the overall performance di liberto et al 2011 and the consistent under prediction by one of the members in ensemble1 increases the overall error in the predictions from this ensemble the rmse values for all the flood guidance systems etss estofs and cbofs the bias corrected iflood ensembles ensemble1 ensemble2 and ensemble3 and the official ahps are averaged over all the stations and summarized in fig 23 the results show that the ensemble2 had the lowest value for rmse 0 11 m and produces similar results to the ahps official forecast 0 11 m this improved ability to predict floods resulted from the best performing ensemble members etss iflood and cbofs during the daily operations the results presented below demonstrate that the ensemble forecast guidance for water levels could provide higher accuracy for daily operations than any individual member 5 conclusions this study presented the components of the fully coupled surge wave iflood guidance system for the chesapeake bay the coupled adcirc swan coastal hydrodynamic and wave model was forced with the nam 12 km weather model wind and pressure to forecast water levels and waves inside the chesapeake bay for a lead time of 84 h three existing noaa operational flood guidance systems etss estofs and cbofs in the chesapeake bay are currently supporting the nws to produce the official water level forecasts ahps the iflood guidance system is developed to provide additional model guidance inside chesapeake bay using the coupled surge wave modeling in addition to already existing surge only operational guidance systems etss estofs and cbofs the operational performance of iflood was compared during the daily and extreme weather operations against other existing operational flood etss estofs and cbofs and wave wavewatch3 global regional and nwps guidance systems over 6 months of period in the year 2019 january to june hurricane dorian 2019 was used as an example to evaluate the extreme weather performance of the iflood coupled surge wave guidance system during the daily operations the coupled iflood system performed as the second best guidance system in its ability to predict total water levels rmse of 0 15 m after the bias corrected etss 0 14 m in the bay iflood and etss also demonstrated greater pod 80 85 for flood alerts inside the chesapeake bay the nam 12 km forecasted winds utilized in the iflood showed a mean rmse value of 3 8 m s averaged over all the stations in the chesapeake bay the daily wave forecasts predicted by iflood showed the least bias and rmse 0 25 m 0 60 m compared to the nwps 0 54 m 0 75 m ww3 global 0 42 m 0 74 m and ww3 regional 0 55 m 0 69 m wave guidance systems averaged over all the ndbc buoys assessment of the water levels predicted by iflood during hurricane dorian resulted in the second lowest rmse of 0 15 m compared to the bias corrected etss forecasts 0 12 m averaged over all the stations in the chesapeake bay the significant wave heights showed the lowest rmse 0 62 m among all the operational wave forecast systems wavewatch3 global regional and nwps the operational evaluation of predicted wind speeds at 10 m height from nam 12 km and gfs 27 km weather models resulted in similar error at the lower bay however the rmse in predicted wind speeds from gfs 27 km increased at the middle bay the average rmse of nam 12 km predicted wind speeds during the hurricane dorian showed a lower value of rmse 2 45 m s whereas gfs 27 km wind speeds showed a greater value of rmse 3 2 m s furthermore two post processing schemes are evaluated in the iflood operational water level forecasting in order to improve its ability to predict floods in real time the real time post bias correction using the correction value from previous forecasts minimized the rmse at wasd reduced from 0 22m to 0 15 m reducing a known systematic error while at the other stations it stayed either consistent or increased finally a proposed ensemble forecast scheme considering an equal weightage of flood guidance from the bias corrected etss and raw model outputs from cbofs and iflood resulted in the lowest rmse value 0 1 m for water level predictions inside the chesapeake bay this demonstrates the value added to the current state of the art operational flood guidance systems in the chesapeake bay improving our current ability to predict water levels winds and waves in a large estuary during daily and potentially extreme weather operations a web based data portal for the iflood forecasted products is provided for the general public and scientific community through url https iflood vse gmu edu in addition a text and email based alert system is also established to utilize iflood predictions in order to inform communities on the forecasted flood scenarios in the chesapeake bay the iflood forecast setup is undergoing further improvements to include the complex hydrodynamics of seasonal variability of water levels river discharges effect of gulf streams and offshore swells the future work will focus on implementing these complex hydrodynamics in the iflood operational framework and improving the tidal modeling in the shallower parts of the bay while making use of data assimilation to improve flood and wave forecasting in the bay author contributions the first author under the supervision of the second author carried out the major portion of the work declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was made possible by the research assistantship funding from the george mason university the research resources from the flood hazards research lab fhrl and the collaboration with the washington baltimore nws forecast office on the insight of ahps official flood forecasting procedure additionally this research work was partially supported by virginia sea grant program grant number id 204985 any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the nws this work used the extreme science and engineering discovery environment xsede stampede2 resources through allocation id tg bcs130009 which is supported by national science foundation grant number aci 1548562 towns et al 2014 the authors acknowledge the texas advanced computing center tacc at the university of texas at austin for providing hpc resources that have contributed to the model calibration results reported within this paper http www tacc utexas edu the authors are deeply grateful to senior service hydrologist jason elliot for his guidance mentorship and insights on the forecasting operations at the nws and streamlining the integration of iflood into nws daily workflow the authors will also like to thank the invaluable contributions of the students from thomas jefferson high school for science and technology mentorship program kristine wang for implementing the prototype flood forecasting platform at fhrl and william o connell for developing the interface for iflood and gmu phd students juan luis garzon hervas ali mohammad rezaie and tyler miesse for continuous technical support appendix d supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix d supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104748 appendix a the astronomical tidal analysis was performed for 8 major tidal constituents m2 n2 k1 k2 o1 p1 q1 s2 for 120 days in the year of 2018 m2 and s2 were the most prominent in the bay as noted by the amplitude already presented in the main manuscript while the remaining tidal constituents k1 k2 o1 p1 q1 s2 from the analysis is shown in fig a1 and fig a2 fig a 1 comparison of iflood modeled tidal amplitude of tidal constituents k1 k2 o1 p1 q1 s2 vs noaa tidal constituents in the chesapeake bay fig a 1 fig a 2 comparison of iflood modeled tidal phase of tidal constituents k1 k2 o1 p1 q1 s2 vs noaa tidal constituents in the chesapeake bay fig a 2 appendix b the water level predicted by the iflood guidance system were partitioned into various lead times 12 24 36 48 60 72 and 84 the rmse of the water level forecasts against the noaa observations were averaged for various lead times over a period of 6 months jan 19 to jun 19 fig b1 presents the mean value of rmse over the 6 months period at 12 24 36 48 60 72 and 84 lead time fig b 1 rmse plot for iflood water level forecasts over lead times 6 12 24 48 72 84 h for stations in the bay averaged over the period of 6 months jan 19 to jun 19 fig b 1 appendix c screenshot of real time visualization of iflood web based portal for display of flood information in the chesapeake bay water level waves and iflood iot sensor stations are displayed from a sample forecast advisory fig c 1 real time visualization of water levels and waves monitoring stations visualizing the flooded station location using google maps api https iflood vse gmu edu map fig c 1 
25995,real time flood forecasting computational frameworks that can dynamically integrate oceanic coastal and estuarine processes are becoming essential to provide accurate and timely information for emergency response and planning in largely populated estuaries during extreme events this study presents a newly developed real time total water flood guidance system that is fully automated based on the coupled surge wave adcirc swan model and provides water level forecasts in the chesapeake bay for a lead time of 84 h twice a day displayed on a web based public interface this system improved the current total water level predictions in the bay rmse 0 12 m when compared to the existing operational forecasting systems over the period of 6 months jan 19 jun 19 furthermore we demonstrated that a bias correction scheme and a multi member ensemble forecast improve the overall flood prediction results suggests that this framework can improve our current capacity to predict total water levels in large estuaries keywords surge wave guidance system flood forecasting in estuaries adcirc swan model ensemble guidance 1 introduction flooding due to extreme weather events poses an increasing threat to the east coast of the united states us along with devastating economic consequences and significant loss of lives for example super storm sandy in 2012 caused an estimated loss of 50 billion in infrastructure damages throughout the mid atlantic region noaa 2013 hurricane maria 2017 florence 2018 and dorian 2019 are more recent examples of the direct impacts to the us east coast in addition increasing global temperatures are likely to increase the number of severe events easterling et al 2000 van aalst 2006 and significantly impact weather driven global economic losses although oceanfront areas are at the greatest risk of storm surge flooding regions along rivers and estuaries are also extremely susceptible to flooding for instance the chesapeake bay and its tributaries are particularly vulnerable to flooding from hurricanes tropical storms and nor easters as exemplified by hurricanes isabel in 2003 sheng et al 2010 wang et al 2015 and irene in 2011 spring 2011 flooding conditions in the north atlantic ocean are mainly impacted by storm surges astronomical tides winds waves precipitation dresback et al 2013a and indirect effects from the gulf streams and ultimately the rise in the coastal sea levels ezer et al 2013 large scale processes such as the gulf stream have been identified to increase the number of flooding days in hampton roads virginia ezer et al 2013 global climate change rising sea levels and land subsidence are already affecting coastal flooding in major parts of the chesapeake bay boon et al 2010 almost 40 of the us population lives in counties near coastal and tidal areas relying on accurate prediction of coastal flood levels from hours to days of lead time to support emergency preparedness management and responses while several flood guidance systems are currently operational around the world focusing on global regional and local scales kourafalou et al 2015 the weather forecast office wfo from the national weather service nws is responsible for providing flood forecasting and alerts within the us the wfo relies on the storm surge guidance products produced by coastal models from the national center for environmental prediction ncep and the center for operational oceanographic products and services co ops the national oceanic and atmospheric agency noaa currently operates continental scale coastal flooding guidance systems specifically the extra tropical storm surge etss kim et al 1996 and the extratropical surge and tide operational forecast system estofs funakoshi et al 2012 supports the national weather service nws on a daily basis in addition to etss and estofs noaa has regional guidance systems like the chesapeake bay operational forecast system cbofs lanerolle et al 2010 which primarily focuses on providing forecast guidance in the chesapeake bay besides the government operated guidance systems research groups like the virginia institute of marine sciences vims and the university of north carolina unc maintain their own operational forecast systems e g tidewatch loftis et al 2019 and adcirc surge guidance system asgs fleming et al 2008a for flood guidance in the chesapeake bay the operational guidance systems developed by noaa e g etss estofs and cbofs are currently simulating storm surges without dynamically accounting for wave effects which can represent a contribution of 10 15 percent of water levels in the chesapeake bay due to wave setup garzon and ferreira 2016 while the asgs is the only fully coupled surge wave system kourafalou et al 2015 in operation today in the chesapeake bay it was designed to focus in the states of north carolina nc louisiana la and texas tx to provide decision support services to the public sector during active storms in the hurricane season additionally a station based static forecast correction used to improve the initial water level offset in those states leads to an exceptionally large offset of water levels at least 30 cm in the asgs forecasts in the chesapeake bay while the tidewatch was designed for the chesapeake bay it only provides the total water level predictions for a 36 h lead time which is shorter than the duration of the nws official flood forecasts in the chesapeake bay in addition water level forecasts from tidewatch are only available in the lower half of the chesapeake bay stations and are not publicly available to download for model comparison furthermore we hypothesize that water level guidance from a single guidance system may not always accurately represent the best predictions of total water levels in the chesapeake bay that an ensemble based guidance from different models inputs and configurations might provide a better overall total water level prediction in this paper we present a new flood operational guidance system for the chesapeake bay the iflood http iflood vse gmu edu which is a fully coupled surge wave guidance system based on the 2d depth integrated adcirc and the swan phase averaged spectral wave model and forced by the nam weather model this is the first flood guidance system to predict total water levels that incorporates wave set up dedicated to the chesapeake bay the iflood guidance system is currently initialized twice a day for a forecast span of 84 h the utility of the iflood system is demonstrated by its early adoption by the nws baltimore washington wfo lwx providing alternative flood related information at all the official forecasts stations in the chesapeake bay coastal water level forecasting in estuaries is particularly challenging and requires the integration of complex coastal and riverine processes cluckie et al 2000 accurate water level forecasting is highly dependent on correct atmospheric forcing and representation of complex coastline geometry in estuaries therefore the iflood was developed based on the best available topo bathymetric datasets and utilizes high resolution surface weather forcing to model water levels and waves in the bay in order to improve flood forecasting in large estuaries we investigate the use of real time bias correcting schemes salmun and molod 2015 and evaluate model ensembles in an effort to provide a unified flood guidance product to forecasters this manuscript presents the components of iflood guidance system its daily and extreme weather evaluation and the proposed improvements to operational water level forecasting in the chesapeake bay 2 methods 2 1 study area this study focuses on the chesapeake bay and its tributaries the largest estuary of the us scott phillips and blomquist 2015 xiong and berger 2010 the chesapeake bay is located in the mid atlantic region of the us along the east coast of united states and connected to the atlantic ocean the bay is approximately 320 km in length from the mouth of the bay to its head at the mouth of the susquehanna river the main stem has a surface area of 11 601 km2 and it runs entirely in the states of maryland and virginia the bay width varies throughout its axis with the mouth of the bay being 20 km and at the mouth of the potomac river being 45 km the depth of the bay is mostly shallow and almost half of the bay is less than 6 0 m deep only about 8 of the bay has a depth greater than 18 3 m xiong and berger 2010 the main stem and the tributaries of the chesapeake bay are defined as a tidal zone the bay is a complex estuary with more than 7000 km of shoreline and as many as 50 tributaries draining freshwater directly into it low lying areas along the shoreline of the chesapeake bay are more prone to periodic flooding and pose a threat to communities in the event of a tropical storm e g dorchester county maryland and accomack county virginia fig 1 shows the location of the chesapeake bay in reference to the mid atlantic region of the us noaa has several monitoring stations installed throughout the chesapeake bay and the atlantic ocean that record water level waves e g wave height wave period and wave direction and meteorological conditions e g winds speed wind direction and atmospheric pressure fifteen water level and eight wave monitoring stations as listed in table 1 are used to validate the water levels and waves forecasts from the iflood guidance system 2 2 state of the art 2 2 1 water level guidance systems the advanced hydrological prediction system ahps provides flood related forecast information from the nws https water weather gov ahps for the entire us the official water level forecasts issued by the nws hereafter referred as ahps are produced by expert judgment supported by existing surge based model guidance systems and flow forecasts from river forecast centers rfc etss estofs and cbofs are primarily responsible for providing water levels forecasts while the river flow forecast are provided by middle atlantic rfc in the chesapeake bay the etss model is based on two dimensional 2d depth integrated shallow water equations from the sea lake and overland surges from hurricanes slosh model jelesnianski and chen 1992 this model is forced by wind and pressure fields from the nws s global forecast system gfs at 0 25 resolution 28 km forecast cycles are initialized four times a day for an extended period of 102 h the estofs model is based on the most commonly used barotropic version of the advanced circulation adcirc 2d depth integrated 2ddi model luettich et al 1992a similar to etss estofs also makes use of gfs 0 25 28 km surface forcing to forecast storm surge for up to 180 h into the future and it is also initialized four times a day the cbofs is a regional guidance system specifically designed for the chesapeake bay and it is based on a three dimensional 3d baroclinic model the regional ocean modeling system roms shchepetkin and mcwilliams 2005 from rutgers university two meteorological outputs wind and pressure from the north american mesoscale s nam model are utilized as surface forcing to the cbofs model that is initialized four times a day and produce forecast guidance for 48 h into the future lanerolle et al 2010 the adcirc surge guidance system asgs fleming et al 2008a provides a framework supporting the use of the adcirc model for real time storm surge forecasting applications the asgs based forecast system is triggered by storm track advisories issued from the national hurricane center nhc and an expert team during the hurricane season and nam 12km meteorological data is used to simulate storm surge and waves during daily operations the coastal emergency risks assessment cera web based platform utilizes the asgs to support emergency operations for the state of north carolina the tidewatch system uses the semi implicit cross scale hydro science integrated system model schism a derivative product built from the original selfe zhang and baptista 2008 this system is forced by nam nest 5 km atmospheric forecast model to predict storm tide for a 36 h lead time around us east coast and gulf of mexico loftis et al 2019 2 2 2 waves guidance systems a number of noaa operational wave guidance systems provide forecasted information on the waves in the chesapeake bay which includes wavewatchiii global ww3 global wavewatchiii regional ww3 regional and near shore wave prediction system nwps in the atlantic ocean wave forecasts are not directly utilized to produce the official water level forecasts however official wave forecast are provide through nws marine forecast in the chesapeake bay ww3 global and regional models are based on the wavewatchiii model tolman 1989 which is a third generation wave model ww3 global and ww3 regional models are based on rectilinear nested grid ww3 global wave model provides boundary condition for a higher resolution ww3 regional wave model around the us east coast the input meteorological forcing in the ww3 global and ww3 regional models are provided by gfs wind model forecast cycles are initialized four times a day for an extended period of 180 h nwps van der westhuysen et al 2013 is based on the swan booij et al 1999 wave model which is also a third generation wave model ww3 and swan are both third generation wave models with similar source term packages however different numeric schemes ponce de león et al 2018 the nws forecaster s generated winds is used in the nwps model during the daily operations hudlow 1988 which is replaced with nhc forecasted winds during the hurricane season similar to ww3 global and ww3 regional forecast cycles for nwps are also initialized four times a day for an extended period of 180 h 2 3 iflood components 2 3 1 storm surge and wave prediction model hydrodynamic models coupled to phase average wave models are widely used to simulate storm surge and coastal inundation around the world casulli and walters 2000 hubbert et al 1999 n d luettich et al 1992a murty et al 1986 these models are regularly used for historic validation of storm flooding in coastal areas lakes and estuaries blain et al 2010 dresback et al 2013b garzon et al 2018 hanson et al 2013 shen et al 2006b in addition to real time coastal inundation guidance systems akbar et al 2013 dresback et al 2013b fleming et al 2008b funakoshi et al 2012 mattocks and forbes 2008 in this study the advanced circulation adcirc model is applied to simulate hydrodynamic conditions luettich et al 1992a which is a finite element hydrodynamic model based on an unstructured grid and used to simulate water levels currents storm surge and tides forced by meteorological fields e g wind and pressure and tidal constituents adcirc is an open source numerical model written in fortran and is very well documented both in the published scientific literature luettich et al 1992a and on the adcirc web site http adcirc org it has been used extensively for tidal and storm surge prediction bilskie et al 2015 blain et al 1994 garzon et al 2018 haddad et al 2015 li et al 2013 lin et al 2010 shen et al 2006a westerink et al 2008 the two dimensional depth integrated version of adcirc known as adcirc 2ddi is utilized in the iflood guidance system to simulate water levels storm surge tides forced by the forecasted meteorological information winds and pressure and astronomical tides the surge only adcirc only iflood guidance system was updated in november of 2018 to include a coupled surge wave forecast system adcirc swan dietrich et al 2011 by incorporating wave processes using the simulating waves nearshore swan model booij et al 1999 in the forecast operations as well swan is a third generation phase averaged wave numerical model used to calculate wave energy generation propagation and dissipation in shallow waters the adcirc 2ddi model computes water elevation using a vertically integrated continuity equation in the generalized wave continuity equation gwce form depth average current velocities are obtained from the solution of momentum equations whereas swan computes the wave energy from the wind radiation stresses computed by adcirc detailed information on the governing equations of adcirc and swan numerical models can be found at dietrich et al 2011 wave setup can contribute to an additional increase in water levels during storm surges the tightly coupled adcirc swan incorporates the wave setup contribution to total water level predictions dietrich et al 2011 based on local bathymetry and wave conditions wave setup can significantly contribute to storm surges sheng et al 2010 previous studies have demonstrated an increase of 0 1 0 2m in water levels inside the chesapeake bay from wave setup garzon and ferreira 2016 hanson et al 2013 therefore supporting the use of coupled surge wave modeling system in the bay the iflood hydrodynamic model grid domain which extends from 97 85 to 60 04 w and from 7 90 to 45 83 n encompasses the western atlantic the gulf of mexico and the caribbean sea the unstructured computational mesh was created by modifying the fema s region 3 numerical mesh garzon and ferreira 2016 it consists of 381 164 nodes and 750 479 elements in the iflood configuration and allows for higher horizontal resolution in the chesapeake bay each node in the mesh represents its longitude latitude depth and elements which are the group of nodes forming a triangle it ranges from hundreds of kilometers km offshore to tens of meters m in the bay and its tributaries minimum mesh size is 25 m the numerical mesh extends well beyond the wet areas in order to account for topography effects on the storm surge modeling the bathymetry was interpolated from the fema region 3 unstructured grid garzon and ferreira 2016 and merged with data from the national geophysical data center s coastal relief model crm and noaa digital nautical charts which have a horizontal resolution of 3 arc seconds 90 m topography from the united states geological services usgs digital elevation model dem archives was used to enhance the representation of land features dem from usgs has a resolution of 30 m over the chesapeake bay topography besides the chesapeake bay was interpolated from the fema region 3 unstructured grid all data was converted to the north american navd88 vertical datum forte et al 2011 2 3 2 model forcing 2 3 2 1 astronomical tides about 75 80 of the sea height variability during daily operations is represented by the astronomical tides as demonstrated by the observations made possible by the topex poseidon satellite altimetry le provost et al 1994 an accurate representation of the astronomical tidal variation in the flood guidance system is crucial for correct water level forecasting inside estuaries since errors in astronomical tidal simulations can introduce a significant noise in the total water level prediction various tidal databases are available around the world and can provide model boundaries and forcing for astronomical tidal simulations the le provost database le provost et al 1994 has been available for decades while tpxo developed by oregon state university egbert and erofeeva 2002 and the adcirc tidal database mukai et al 2002 are examples of most recent databases developed for the us east coast the accuracy of christian le provost le provost et al 1994 tidal simulations were found to be in the orders of 3 5 cm when compared with observations from nearly 103 coastal and island tidal gauges the open ocean boundary inside the iflood modeling domain was forced by the amplitude and phase of the eight major constituents k1 k2 o1 m2 n2 p1 q1 and s2 extracted from the le provost database the propagation of astronomical tides in large estuaries are also highly influenced by the bottom surface roughness resio and westerink 2008 which is represented by the manning s n coefficient in the adcirc model version used in this study previous studies in the chesapeake bay developed optimal spatial bottom surface roughness values that are utilized in the current formulation of iflood garzon and ferreira 2016 the iflood tidal analysis was performed for eight 8 major tidal constituents for 120 days in the year of 2018 jan 18 to apr 18 in order to analyze the model tidal prediction error the amplitudes and phases of major tidal constituents were studied for 12 stations inside the chesapeake bay fig 1 tidal simulations were forced with open ocean boundary forcing from the major tidal constituents k1 k2 m2 n2 o1 p1 q1 and s2 five principal tidal constituents including m2 n2 s2 o1 and k1 have been reported to be sufficient for accurately modeling tides in the mid atlantic region garzon and ferreira 2016 hanson et al 2013 a 5 day spin up was used to hot start the tidal simulation lasting for 4 months 120 days time series data of modeled astronomical tides was analyzed with a python package pytides based on the algorithm listed in the national ocean service nos publication schureman 1982 the modeled tides were decomposed into its constituents and compared against the noaa harmonic tidal constituents database at recording stations the amplitude is given in meters and the phase is relative to the greenwich meridian in units of degrees 2 3 2 2 north american mesoscale nam 12 km model the north american mesoscale forecast system nam is one of the main weather models operated by the ncep for producing weather forecasts for the us the nam is a non hydrostatic multiscale numerical weather prediction nwp model on the b grid and it utilizes a hybrid sigma pressure vertical grid to solve vertical momentum equations for weather modeling janjic 2005 the nam is based on the noaa earth modeling system nesm which is further based on the earth system modeling framework da silva et al 2002 several weather variables are available from the nam grids ranging from temperature and precipitation to lightning and turbulent kinetic energy the nam generates multiple grids or domains of weather forecasts over the north american continent at various horizontal resolutions high resolution forecasts are generated within the nam using additional numerical weather models these high resolution forecast windows are prepared over fixed regions and are triggered on an event based scales to follow significant weather events such as hurricanes the nam forecasts are made four times daily at 00 06 12 and 18 utc hours with a duration of 84 h into the future the nam 12km conus grid 218 best represents the atlantic ocean and the gulf of mexico with the highest resolution of 12 km the iflood utilizes the 10 m wind velocity u component of wind height above ground v component of wind height above ground and pressure fields pressure reduced to msl msl over the contiguous us to force the coupled adcirc swan model the nam 12km conus grid does not extend below 15 north therefore in order to accommodate the missing data points in the model grid the meteorological surface forcing is set to zero these forecasted products become available at the noaa ncep server http www ftp ncep noaa gov data nccf com nam prod as they are produced water levels in the chesapeake bay are significantly impacted by offshore winds and the astronomical tides shen et al 2006a thus the errors associated with forecasting winds and tides undermine the overall forecasting of water levels in the bay significant wave heights are highly correlated with local wind speeds and incoming swells cavaleri et al 2012 lin et al 2002 semedo et al 2015 the storm surge simulations driven by nam produced more accurate results in comparison to simulations forced by gfs during the hurricane isabel irene and sandy garzon et al 2018 in the chesapeake bay therefore the surface weather forcing in the iflood guidance system uses the high resolution nam model to forecast the storm surges 2 4 iflood guidance system operational workflow the following sections describe the methods used to produce the flood guidance this includes the retrieval and construction of input data the setup of the numerical models the structure of the workflow used to produce results and the web publishing of forecast information for the community 2 4 1 meteorological forcing processing the first step in the iflood forecast operation is to search for the newly available meteorological files from ncep on the noaa portal ncep uploads forecasted weather conditions raw files on their server daily every 6 h an automated python script looks for newly uploaded files and downloads it to the iflood server the iflood guidance system is designed to utilize nam gfs and the nhc best track data however the current iflood operational setup described here is utilizing the nam forcing to simulate water levels currents and waves the forecasted winds at a height of 10 m u10 v10 and pressure at mean sea level msl produced by the nam model are utilized to force the iflood guidance system two times a day 06 and 18utc during the forecast operation a python script downloads the recent forecasted nam files extending up to 84 h the original nam files contain voluminous gridded meteorological data in the gridded binary grib2 file and are projected in lambert conformal conic lcc projections nam files are converted to binary format netcdf network common data form maintained by unidata and are re projected to the geographical coordinate system gcs using wgrib2 utility developed by ncep 2 4 2 hindcast short term forecast and forecast setup this subsection explains the workflow of the hindcast short term forecast and forecast operation inside the iflood framework the hindcast setup of the iflood is initialized as a cold run meaning that the simulation starts with initial water levels set to zero across the entire model domain initial conditions and only tidal forcing is applied for several days e g 10 15 days until the calculated water levels match the noaa astronomical tidal predictions this process is only repeated again if the system needs to be re started the hot start files are written at the end of simulation to save the current state of the ocean the hindcast setup is usually required when the iflood guidance system is shut down due to power outages the hot start files will serve as the initial boundary condition file for a new forecast simulation in order to estimate the model initial conditions that best represent the current state of the ocean for each forecast cycle a short term forecast is produced within each forecast cycle this is necessary as the current version of adcirc swan used in this system does not save hot starting files along the simulation runtime the short term forecast is created by running the adcirc swan numerical model to simulate water levels for a period of 12 h into the future forced by the wind and pressure fields from the 6 hourly data assimilated da cycle of nam along with astronomical tidal forcing the water levels generated at the end of the 12 h simulation period are used as a hot start file for initializing the next forecast cycle the first cycle of short term forecast starts at 06 utc and saves the information for the 18 utc forecast and likewise for ongoing forecasts note that the short term forecast procedure will become obsolete once the adcirc swan model is updated to output and archive hot start files at user defined time steps the forecast simulation begins at the conclusion of the short term forecast therefore the forecast is on the same cycle as the preceding short term forecast starting from the same hot start file and forecasted information about wind atmospheric pressure and astronomical tides this simulation will run for three and a half days 3 5 which is the entire duration of the nam forecast released by ncep no hot start file is generated from the forecast the completion of the forecast simulations results in predicted conditions of water levels currents and waves for next 84 h fig 2 shows the workflow for hindcast short term forecast and forecast setup 2 4 3 high performance computing hpc a 56 core linux server zeus at the mason flood hazards research lab is dedicated for the iflood forecasting zeus is parallelized using hyper threading to split the computational tasks into 56 sub process while each dedicated processor takes part in the numerical calculations of the adcirc swan and reduces the time required to complete the simulation zeus takes about 1 75 h of computational time to complete a forecasted simulation of 84 h for adcirc swan in addition to daily operational forecast cycles instances of amazon web services aws are also initialized during hurricane season to run multiple scenarios of active hurricanes for probabilistic storm surge predictions aws forecast manager t3 low cost burstable amazon elastic compute cloud instance ec2 serves as the resources manager to execute and monitor the forecast instances of iflood for nhc based multiple track simulations 2 4 4 validation metrics the validation of the iflood modeled wind speeds water levels and wave heights is done in real time and offline by comparing the forecasted variables against observations and operational model guidance systems inside the study area the water level predictions from etss estofs and cbofs and wave forecasts from ww3 global ww3 regional and nwps were retrieved twice a day and compared against the iflood predictions for an overall comparison of the existing guidance systems in the chesapeake bay individual assessment of the forecast performances are made routinely by the respective modeling teams however limited records are available of those studies the validation scheme is operational for 14 gage stations nine meteorological stations and six wave buoys an analysis of the system performance for a period of 6 months is presented in the results section the average bias eq 1 measures the mean deviation in the predictions from the observed data the positive values of bias represents the over prediction of the model guidance while the negative values of bias means under prediction the root mean square error rmse eq 2 reported is between the modeled and observed time series for the entire 84 h lead time period for each forecast cycle for the extreme weather evaluation the rmse values are computed between the single forecasted peaks and the measured peaks the probability of detection pod eq 3 is used in the categorical forecast evaluation to determine the model s accuracy in predicting the nws flood levels assigned as action the nws has specified flood levels at the water level recording gages that corresponds to a certain level of threat with the action flood level being the smallest of them all action minor moderate and major this flood level is used as a threshold in determining the pod bias and rmse are in meters m while pod is presented in percentage 1 bias 1 n i 1 n p i o i 2 rmse i 1 n p i o i 2 n 3 pod d e t e c t e d f l o o d s t o t a l n u m b e r o f r e c o r d s 100 where oi are the observations and pi are the predicted values from the model whereas i is the time interval and n are the total number of datasets detected floods is the number of times a model issued forecast guidance was noted more than or equal to the action level height m at that station 2 4 5 web based flood guidance data portal flood warning systems are increasingly using cloud based technology like aws for pre processing the inputs execution of numerical programs and post processing of forecasts morsy et al 2018 a dedicated web portal for the iflood data visualization situation awareness and data download appendix c is available at the following url https iflood vse gmu edu the meteorological forcing from the most recent meteorological forecast cycle are preprocessed in the local server as described in section 2 4 1 the binary outputs netcdf from the most current iflood simulations are converted to various data formats geojson esri shapefile kml text files etc using a set of python and bash the local server takes an additional 35 min to complete these post processing tasks compared to the forecast simulation time 1 75 h mentioned in section 2 4 3 and all the processed data is copied to the amazon s3 for public sharing and storage in addition to forecasted time series data at the stations map based products of water levels wave components atmospheric pressure and wind time series are generated to support the forecast visualization a prototyped network of internet of things iot water level monitoring sensors is also deployed around the belmont bay area va that records the water levels every 6 min these iot based sensors provide additional validation information for the iflood forecast system besides the noaa observations additionally an alert service based on sms messages and email is provided using amazon s lambda function and python scripts for the dissemination of information to users subscribed to iflood the iflood web portal allows users to select alert levels for different water levels or wave stations in the chesapeake bay the overall iflood workflow is summarized in fig 3 2 4 6 case study hurricane dorian 2019 hurricane dorian aug was the first major hurricane of the 2019 hurricane season jun sep which caused significant storm damages along the eastern sea board of the u s the iflood forecasts for the six water levels and meteorological stations and six wave buoys inside and around the bay fig 4 were evaluated against observations of measured hourly water levels winds and significant wave heights the nam 12 km meteorological forcing was used to force the iflood system to forecast the storm surges and waves resulting from the tropical cyclone the iflood forecasts have a maximum lead time of 84 h therefore the noaa operational forecasts were trimmed to 84 h of forecast for this analyses water level winds and waves guidance predicted by iflood were compared with the other noaa operational flood guidance systems etss estofs and cbofs and wave forecast systems ww3 global ww3 regional and nwps from 3rd september to 7th september additionally nam 12 km and gfs 27 km operational wind forcing is compared against the noaa observations for evaluating the accuracy of operational winds 10 m height in simulating water levels and waves over chesapeake bay during the extreme weather 2 4 7 real time bias correction ensemble guidance various guidance systems utilize a post bias correction scheme for minimizing the forecast error resulting from missing physical processes nws also uses a similar correction factor called fudge factor in order to create official flood forecasts among noaa guidance system in the chesapeake bay only etss model results are post processed kim et al 1996 at all the forecast stations by applying a bias correction scheme to the water level predictions the etss model guidance for its 24 h lead time prediction is compared against the noaa observations at individual stations to calculate the forecast anomaly etss predictions minus observations at every forecast the forecasted anomaly is averaged over last 5 days and added to the latest etss water level guidance to minimize the prediction errors at the observing stations salmun and molod 2015 the water levels predicted by iflood were compared against the observations to compute its bias based on the previous 12 h of forecast the computed model bias for each forecast station was used as a correction value to the future forecast advisories a bias correction scheme similar to etss was tested on the calculation for the correction value at each individual station the correction value is based on the raw bias between the predictions and observed data equation 4 for the first 12 h of previous forecasts this bias was computed for the last 1 2 5 10 25 and 50 days average and applied as a correction to the latest water level forecasts equation 5 4 correction value j 1 d i 1 n o i p i d 5 bias corrected iflood raw iflood model outputs correction value where i is the hourly forecast n is the total number of hourly forecast data points compared against observations from the previous forecasts also j is the number of days considered in the previous forecast and d is the total number of days for which the bias was averaged for each station p is the water level predictions and o is the water level observations single deterministic forecast from one model could be misleading as they may fail to predict accurately therefore nws is interested in utilizing a suite of model generated water level predictions ensembles to forecast official flood forecasts the ensemble guidance for water levels prediction was produced by combining the individual guidance systems to improve the overall flood forecasts an equally weighted average scheme was applied to produce three alternative ensemble forecasts using equation 6 6 ensemble w 1 p 1 w 2 p 2 w 3 p 3 w n p n n where w is the weighting factor used for each model guidance prediction and p is the model prediction from selected guidance system also n is the total number of guidance systems 1 ensemble1 was produced by combining all four existing guidance systems etss cbofs estofs iflood representing a balance between nam and gfs forced coastal systems for this case the iflood estofs and cbofs were not bias corrected whereas the etss outputs were bias corrected by noaa before public distribution 2 ensemble2 was created by merging only the etss cbofs and iflood predictions representing the systems that had the least rmse of all the guidance systems when analyzed in section 4 2 1 similarly to ensemble1 the etss guidance is bias corrected while the iflood and cbofs guidance are the raw model outputs 3 ensemble3 was formed by grouping only the bias corrected etss and raw model outputs of cbofs 3 results and discussion this section presents an evaluation of the performance of the iflood guidance system during daily and extreme weather operations the predicted water levels winds and waves were compared against the noaa observations and other existing operational guidance systems in the chesapeake bay 3 1 astronomical tidal prediction evaluation results of the major tidal constituents of m2 and n2 are presented in fig 5 the amplitude and the time of arrival for crest and troughs was predicted satisfactorily and the tidal constituents were in good agreement with tidal constituents at noaa stations except for a few stations upstream the potomac river as shown in fig 5 a maximum tidal amplitude underestimation of 0 1 m was recorded for m2 constituent at the washington d c station while the stations in the main channel and at the mouth of the bay showed much less discrepancy bias less than 0 01 m between iflood and noaa tidal constituents the tidal phase was accurately predicted at the mouth of the bay however the difference between the modeled phases and the noaa stations varied by approximately 12 20 in the upper parts of the main channel as depicted by bism tbmd bltm and apam stations the detailed analysis on the individual tidal constituents can be found in appendix a fig 6 presents the rmse for the astronomical tidal amplitudes and phase of major constituents m2 n2 a rmse of tidal amplitude smaller than 0 20 m and phase below 22 5 are accepted for operational use according to the nos skill assessment criteria peng 2018 the results of the iflood tidal analysis showed an overall similarity to the findings from previous validation studies funakoshi et al 2012 garzon and ferreira 2016 lanerolle et al 2010 on average the modeled tide error rmse inside the chesapeake bay is found to be in the ranges of 0 01 0 29m garzon and ferreira 2016 0 01 0 38m lanerolle et al 2010 0 15 0 23m funakoshi et al 2012 and 0 01 0 08m hanson et al 2013 while the phase difference for the major tidal constituents was in the range of 10 40 garzon and ferreira 2016 the amplitudes and phases errors modeled by iflood were under the maximum range 0 08 m 20 for m2 and s2 at all the stations inside the bay fig 6 the under prediction of the tidal amplitudes and phases in the shallower parts of the potomac river wasd lwtv can be attributed to discrepancies in the bathymetric datasets and the representation of recent potential channel dredging in the numerical mesh the published literature also documented similar under prediction in the upper reaches of the tidal potomac river funakoshi et al 2012 garzon and ferreira 2016 lanerolle et al 2010 these authors also argued that the shallower depth curvature of the tributaries and bottom roughness could be the possible causes for this misrepresentation furthermore another sources of error could arise from missing baroclinic effects and seasonal variability of water levels in the bay lanerolle et al 2010 3 2 operational iflood predictions the iflood performance is evaluated based on water levels winds and waves from the coupled system adcirc swan produced twice a day for six 6 consecutive months in 2019 january june the extreme weather evaluation was performed during hurricane dorian 2019 evaluating the predicted water levels winds and wave heights from the coupled iflood adcirc swan the iflood forecasts were evaluated by comparing the model results against the noaa observations and other existing operational flood guidance systems etss estofs and cbofs and wave forecast systems wavewatch3 global ww3 regional and nwps daily operations refers to twice a day forecasts at 06 utc and 18 utc hours hereafter referred as daily these forecasted model outputs of water levels waves and winds from the iflood are also referred as forecast advisory in this section 3 2 1 daily forecasts winds the nam 12 km operational winds driving the iflood showed an overall negative bias under prediction of 2 8 m s compared to the observations when averaged over all the recording stations fig 7 presents a scatter plot of wind speed calculated by iflood after interpolation and drag modifications of the nam wind forecast from the coupled adcirc swan numerical model compared with noaa measurement at nine monitoring stations inside the chesapeake bay the scatter plot of predicted and observed wind speed showed that wind forecasts are in good agreement with observations however they tend to under predict larger bias of 10 m s for wind speeds greater than 15 m s in lower bay cbbv yktv kptv the mean rmse for nine meteorological station fig 1 was 3 72 m s and individual values are given on fig 7 the horizontal width of the upper bay varies between 18 and 20 km whereas the resolution of the nam weather model is 12 km this coarser resolution of the nam weather model in relation to the bay geometry resulted into few wind change locations grid points in major parts of the bay making it difficult to accurately represent the forecasted winds at noaa stations in the bay the gfs operational wind forcing used to drive the etss and estofs model were not evaluated against the nam wind forcing used in iflood which limited the direct comparison of gfs and nam operational winds during the daily operations however a hind cast study evaluated reanalysis winds at 10 m height comparing nam and gfs in four buoys around the us and found an average under prediction of 1 75 m s for nam winds speeds and 2 4 m s for gfs wind speeds at diamond shoals nc wang et al 2018 another study also concluded that a storm surge model forced by the nam reanalysis winds were able to capture the maximum storm surges more accurately when compared to the same model forced by the gfs in the chesapeake bay garzon et al 2018 while results are only shown here for the wind speed the wind direction present the similar patterns water levels the water level forecasts of iflood were simulated using the astronomical tides wave radiation stress from swan and nam 12 km wind forcing the highest water level measured at the noaa monitoring stations fig 1 during each forecast advisory were compared against the iflood predicted peaks for 15 stations inside and around the chesapeake bay fig 8 the scatter plots shows that the peaks of the predicted water levels matched well with the observations with 0 2 m variations at the lower bay and increased to a maximum of 0 4 m in wasd the rmse values for each station is averaged for the entire period and shown on fig 8 analysis of predicted water levels in the lower bay cbbv swpv kptv and yktv resulted in a small rmse in the range of 0 10 0 12 m results in the middle bay bism slim and lwtv showed rmse between 0 16 and 0 21 m whereas the maximum rmse of 0 34 m was observed for wasd fig 8 overall the model rmse for water level predictions was noted to be lower than 0 2 m for 84 h lead time for the majority of the stations except for wasd lwtv and bltm and were within the criteria set rmse lower than 0 2 m by nos peng 2018 the under prediction of the forecasted winds by the nam 12 km weather model and tidal amplitude errors discussed in previous section could explain the inaccurate modeling of water levels for stations with rmse greater than 0 2 m post processing bias correction due to consistent errors from winds and tidal amplitude and any seasonal variability on the water levels could lead to an improved forecast performance in the shallower parts of the bay this correction becomes necessary as demonstrated by tidal errors in the wasd station which had a consistent under prediction bias of 0 1 m in its m2 tidal amplitude and if removed can reduce the water level bias from 0 32 m to 0 22 m in order to evaluate the model performance based on different lead times the station based lead time guidance was computed for 12 24 36 48 60 72 and 84 h of forecasted information and is presented in appendix b the rmse associated to the forecast advisories at individual lead times during the period of 6 months jan 19 to jun 19 presented a small variance 0 05m in rmse from 12 h lead time to 84 h lead the stations at the lower bay kptv cbbv and swpv had lower rmse with the increasing lead times whereas wasd station inside the potomac river demonstrated an increase in rmse for extended lead times 84 h the initial conditions for every forecasts cycle are provided by the short term forecast which does not accurately represent the state of the ocean this leads to a slightly higher forecast error offset at the beginning of forecast lead time resulting in increased rmse the water levels predicted by iflood showed the lower bias for bism 0 08m and yktv 0 015m when compared to other guidance systems fig 9 in addition an increasing bias compared to etss and cbofs was noted for the stations in the shallow parts of the potomac river for example lwtv and wasd with average values of 0 06 and 0 16 m respectively the bias for the iflood water level predictions at lower bay stations was 0 04 m while a larger bias 0 15 m was calculated for the stations at the upper bay initial forecast evaluations of estofs reported lower rmse less than 0 2 m for the stations at the lower bay and higher rmse above 0 2 m for stations in the shallower parts wasd lwtv funakoshi et al 2012 cbofs presented the lowest bias for the lwtv 0 05 m while the higher bias was reported for kptv 0 2 m bias corrected etss exhibited the least forecast bias compared to the other guidance systems with the lowest bias of 0 025 m averaged over all the stations in the bay the estofs guidance system had the maximum forecast bias of 0 18 m averaged for all the stations inside the bay the evaluation of the other guidance systems also provides an insight into the effects of barotropic and baroclinic model configurations in predicting the water levels in the bay the baroclinic scheme utilized by the cbofs forced with the nam 12 km meteorological data during the period of analysis resulted in similar bias patterns than the barotropic models throughout the bay lanerolle et al 2010 it was interesting to note that the baroclinic cbofs water level prediction had a positive bias over prediction at all the stations contrary to barotropic etss estofs and iflood with an overall negative bias under prediction while temperature and density changes inside the chesapeake bay impact the overall bay circulation blumberg 1978 demirbilek et al 2008 majumdar et al 1987 the results suggests that the daily water levels predictions inside the bay produced by operational guidance systems considering the baroclinic effect cbofs are not significantly better than the barotropic models iflood and estofs unlike other noaa guidance systems the iflood water level prediction showed varying bias throughout the bay for example negative bias at shallower parts of the bay upper and middle and positive bias at lower bay with an exception of kptv the cbofs baroclinic setup during the daily operations had a consistent positive bias the estofs negative bias was similar to previous studies funakoshi et al 2012 while also satisfying the nos criteria peng 2018 in the etss modeling framework the noaa predicted astronomical tides are added to the gfs driven storm surge which eliminates the consistent tidal amplitude error that may be present in other guidance systems water levels predicted by etss are further corrected with a post bias correction scheme therefore the evaluation of etss resulted in least forecast bias fig 10 shows the spatial distribution of the error from water levels predicted by iflood denoted by rmse against the other operational guidance systems the ability of iflood to predict floods was demonstrated with the rmse values in the range of 0 1 0 20 m similar to etss 0 1 0 25 m inside the study area the rmse for cbofs was in the orders of 0 1 0 25 m while estofs had the highest rmse of all the guidance systems in the order of 0 25 0 35 m the rmse calculated during the period of analyses for the operational guidance systems were similar to previously published studies funakoshi et al 2012 kim et al 1996 lanerolle et al 2010 for their water levels forecast evaluation the probability of detection pod metric was used to evaluate the capability of iflood in predicting floods in comparison to the other operational flood guidance systems etss estofs and cbofs as shown in fig 11 the observed flood peaks action level usgs flood stage per advisory were compared against the predicted flood levels from the guidance systems modeled flood peaks above the designated flood levels at each noaa monitoring station were isolated and analyzed for each observed peak signals iflood detected the flood levels 90 of the times in cbbv and nearly 94 in the yktv demonstrating the similar pod compared to bias corrected etss guidance systems at the mouth of the bay iflood and estofs had the lowest pod for wasd when compared to other guidance systems the pod values for cbofs ranged between 60 and 80 at majority of stations except wasd pod value of 30 in the bay etss showed an overall highest ability to predict floods compared to other guidance systems at all the stations interestingly the guidance systems were able to predict the flooding more accurately at the entrance of bay compared to the stations in the upper bay and the potomac river several reasons can be attributed to missing some of the flood events for example the uncertainty in the weather forcing consistent noise in the modeling tidal amplitude error and absence of river discharge to account for rainfall induced flooding these contributing factors disrupt the ability to predict floods and might be more prevalent under some events and less in others waves wave modeling inside the iflood domain is dependent on the nam 12 km meteorological forcing and was able to predict wave heights with a bias of 0 05 m averaged over all the six stations for the 6 months of analyses significant wave heights forecasted by iflood were compared against the observed datasets from the national data buoy center ndbc buoys are shown as a scatter plot in fig 12 the recorded peak wave heights at the buoys during the analysis period did not exceed 5 5 m the mean value of the bias for the significant wave height predicted by iflood at the ndbc buoys was in the range of 0 01 0 12 m over prediction results at the vbva and dfnc stations had a positive bias of 0 02 m in the significant wave height while dbde tsva wiva and chva showed a negative prediction of 0 05 m the negative bias under prediction in the predicted winds fig 7 during the daily forecast operations resulted in a slight decrease in the predicted peaks of significant wave heights wave period and direction followed the same trend as of significant wave height fig 12 also suggests that the wind driven wave heights can be predicted by the iflood coupled modeling system with the rmse values ranging between 0 26 and 0 84 m for all the six stations inside and around the chesapeake bay fig 13 shows the comparison between the significant wave height forecasted by iflood and the noaa operational wave guidance system ww3 global ww3 regional and nwps six ndbc buoys inside and outside the bay had continuous record of observed wave height for the same period the lower resolution of wavewatch3 global 3 stations out of 6 and regional model 4 stations out of 6 in the chesapeake bay only provided forecasts for a few stations in the study area for wave forecast evaluation the nwps model domain baltimore washington weather forecast office only covers the chesapeake bay and includes tsva flva chva and wiva buoys for model comparison the bias in wave height prediction against the observations varied from 0 24 m to 0 19 m iflood wave height forecast compared to other operational system demonstrated the lowest wave height bias for the station chva 0 009m followed by dfnc 0 017 m wiva 0 022 m and vbva 0 024 m a maximum bias of 0 124 m was recorded at the tsva significant wave heights forecasted by nwps showed maximum bias for wiva as this station is almost at the nwps model domain washington baltimore wfo model domain the use of initial conditions from the ww3 global for the open ocean boundary in the ww3 regional and nwps improves the wave modeling westhuysen et al 2014 the wave initial conditions inside the iflood are provided using the nowcast simulation of adcirc swan wavewatch3 global and regional operational wave forecast system are currently using the structured numerical mesh to model the waves while nwps westhuysen et al 2014 and iflood uses an unstructured mesh throughout the modeling domain the evaluation of wave heights modeled using nws forecasted winds and gfs winds in the nwps and ww3 global and regional wave forecast models are calculated to perform equally well with slight variations in the absolute bias values averaged over the ndbc buoys ww3 regional model showed the least bias of 0 05 m followed by nwps 0 11 m and ww3 global 0 17 m averaged over the ndbc buoys it is also interesting to note that ww3 global and regional forecasts are output every 3 h in contrast to iflood and nwps which could result in missing peaks of the forecasted significant heights the results here are shown for significant wave height only however similar results were found for wave direction and period 3 2 2 extreme weather forecasts hurricane dorian emerged off the west coast of africa on august 24th due to a strong tropical wave nhc classified dorian as tropical depression five which rapidly intensified into the tropical storm over the course of the next 2 days and on august 27th brought tropical storm winds and rainfall to barbados dorian progressed from hurricane category 1 to 5 on the saffir simpson wind scale with maximum sustained winds of 203 mph resulting from the favorable environment on august 28th 31st on 1st september dorian made landfall on great abaco islands of bahamas with sustained winds of 185 mph hurricane category 5 and slowly moved to grand bahamas the next day dorian weakened to a category 1 hurricane on 3rd sep but the system regained category 3 hurricane strength on 5th sep while moving over the gulf streams however increasing wind shear caused the storm s winds to weaken to a category 2 hurricane on the same day hurricane dorian made landfall near cape hatteras north carolina on friday 6th sep as a category 1 hurricane and impacted the southeastern virginia coast the hurricane track issued by the nhc for dorian passing near the chesapeake bay is shown in fig 4 winds the forecasted wind speeds from nam and gfs models were compared at five stations fig 4 inside the bay during hurricane dorian between sep 3rd to sep 7th fig 14 the resolution for nam winds was 12 km and the gfs winds was 27 km both the wind forcings were interpolated to the same numerical mesh computational nodes to allow for direct comparison of winds the operational winds for both the models were available hourly however the winds were only extracted for 84 h lead time at three hourly outputs wind speed results at 10 m height for both models showed almost same ability of prediction at all the noaa stations for example the highest measured wind speeds of 23 m s were accurately predicted bias of 0 05 m s by both the models at cbbv the stations in the middle bay lwtv and bism also showed same under prediction bias of 1 m s the spatial distribution of the error in the predicted wind speeds is presented as rmse in fig 15 nam 12 km showed smaller value of rmse 2 45 m s compared to gfs 27 km 3 2 m s for the 5 noaa recording stations over the period of sep 3rd to sep 7th during hurricane dorian wind speeds calculated at cbbv and kptv had the same rmse while it increased for the gfs in the middle bay yktv and lwtv results are only shown here for the wind speed however the wind direction present the similar patterns the winds analysis of the nam vs gfs suggest that lower horizontal resolution of the nam could capture the spatial distribution more accurately than gfs the variations in the approaching angle strength and the size of storm from both the models could also explain the rmse in the forecasted winds from nam and gfs models water levels the peak of the water levels at the noaa recording stations were observed on sep 6 2019 as hurricane dorian passed closely to the chesapeake bay hurricane induced water level forecasts were compared starting 3 days prior to the measured peak at the noaa recording stations the water level calculated from all the four guidance systems were retrieved at 0600 and 1800 utc times from sep 3rd to sep 7th and compared against the measured peak water level at six recording stations fig 4 a scatter plot comparing the maximum water level measured for the six noaa stations in the bay against the predicted maximum water level from iflood and the other operational guidance systems etss estofs and cbofs during the forecast period sep 3rd to sep 7th is displayed in fig 16 fig 16 shows that maximum water levels forecasted by iflood during all the forecast advisories were in agreement with the noaa observations at the lower bay kptv swpv cbbv and yktv the maximum water levels within the red oval forecasted by iflood resulted in a negative bias 0 16 m averaged over all the forecast stations fig 4 as the majority of the forecasts leading towards the landfall 6th sep were under predicting the water levels the etss forecasts also underestimated the water levels with bias of 0 05 m averaged over all the stations but outperformed all the other operational guidance systems for example estofs bias of 0 38 m and cbofs bias of 0 26 m the iflood was noted as the second best guidance system in predicting water levels given by the rmse of 0 15 m after the etss rmse of 0 12 m and performed better than cbofs rmse of 0 24 m and estofs rmse of 0 34 m averaged over forecast stations as shown in fig 17 the wind and pressure spatial temporal and intensity variation predicted by the nam 12 km and gfs weather models could help explain the forecasted storm surges errors by each respective guidance system it is interesting to note that the water levels predicted by the iflood using the coupled model adcirc swan during hurricane dorian also showed similar results to the daily operational evaluation the variation in the predicted water levels by gfs forced etss and estofs and nam forced cbofs and iflood could be the result of variations in the approaching angle strength the size of storm and the horizontal resolution of the weather models in addition the use of coupled adcirc swan modeling setup in the iflood could have captured the wave setup resulting in better predictions compared to estofs and cbofs a bias correction similar to etss could result in lower rmses for iflood estofs and cbofs in predicting the water levels the results from forecasting hurricane dorian storm surges suggested that the coupled modeling framework forced by nam 12 km can predict water levels with a small overall under prediction rmse of 0 21 m further evaluation of the operational winds from gfs and nam was performed later in this section to evaluate the error in predicting the wind speed during hurricane dorian from these systems waves the significant wave height predictions during hurricane dorian were also compared against the ndbc buoy observations and with the other operational wave forecast systems ww3 global ww3 regional and nwps ww3 global and regional forecast models were initialized every 6 h 00 06 12 18 utc and forecasted gfs driven waves at 3 h intervals out to 180 h nwps waves forecast system used nhc prepared parametric winds during the hurricane season to forecast the wave heights for a lead time of 145 h similar to water levels evaluation the wave heights from noaa wave forecast systems ww3 global ww3 regional and nwps were trimmed to 84 h of forecast and compared to ndbc buoys observations for the forecast advisories issued between sep 3rd to sep 7th a scatter plot comparing the maximum wave height measured at the ndbc buoys to the predicted maximum wave height of iflood and the other operational wave forecast systems ww3 global ww3 regional and nwps at ndbc buoys during the forecast period sep 3rd to sep 7th is displayed in fig 18 hurricane dorian generated significant wave heights greater than 6 m at vbva and oinc stations and higher than 2 m at the entrance of the bay chva flva and tsva as it made landfall on sep 6th 2019 fig 18 shows that iflood predicted the significant wave heights more accurately at oinc bias of 0 1 m tsva bias of 0 22 m and vbva bias of 0 25 m however overestimated the significant wave heights by 1 m at vbva and chva in comparison to the other operational wave forecast systems ww3 global ww3 regional and nwps ww3 global and ww3 regional showed an under prediction of waves whereas nwps predicted the waves more accurately at tsva and flva fig 19 shows the spatial distribution of the rmse values at individual stations from iflood ww3 global ww3 regional and nwps the rmse of the wave forecasts issued between sep 3rd to sep 7th showed a value of 1 41 m for ww3 global followed by ww3 regional 1 22 m and nwps 0 63 m during the same duration iflood wave forecasts showed the lowest rmse 0 62 m averaged at all the stations the analysis of the wave heights suggest that iflood can predict the wave heights with a lead time of 3 5 days using the nam forcing significant wave heights predicted by the iflood were underestimated before the landfall which resulted from a cold start simulation of the wave forecasts the opposite trend was noted for the ww3 global and regional domains as the underestimation of forecasted waves started to decrease ww3 global and regional forecasted waves were output every 3 h which could have also resulted in missing peaks for both the guidance systems significant wave heights predicted by nwps showed more variability between the first and last forecast advisory for all the stations ww3 global and regional models makes use of the data assimilation that improves the forecast error by initializing simulation with a better representation of the ocean state therefore to reduce the wave forecasting error in iflood hot starting the simulations for wave forecasting was later incorporated into the iflood workflow analysis here is demonstrated for wave heights only however wave direction and period also showed same results 4 improvements to iflood operational forecasts this section discusses potential improvements to operational water level forecasting in the chesapeake bay based on bias correction schemes and ensemble forecasts of the currently available guidance systems 4 1 post bias correction real time bias correction was applied at all the forecast stations to investigate the effect on the water level predictions fig 20 shows the time series plot of water levels at wasd and apam for the iflood forecasts issued on january 27th 2019 the correction values at both stations for future flood forecasts was computed based on equation 4 and resulting bias corrected iflood forecasts yellow line were compared against observations the under predicting water levels in the previous forecasts for wasd resulted in a negative correction value 0 16 m this correction value when added to the latest iflood forecast improved the raw iflood forecast green line rmse from 0 19 to 0 09 m fig 20 also shows that the bias corrected iflood forecast is matching well with the noaa observations contrary to wasd apam resulted into a positive bias 0 16 m which when removed from the latest iflood forecasts showed an increase in the rmse from 0 11 m to 0 20 m fig 21 presents the rmse of the bias corrected water level forecasts from iflood compared against the raw iflood predictions at each station the bias corrected water levels showed a decrease in the rmse for wasd and bltm stations only the rmse value for wasd was reduced noticeably from 0 22 m to 0 15 m by applying the correction value calculated based on the last 1 day the rmse value for the remaining stations was either increased cbbv swpv or remained constant apam the single correction value did not improve the overall bias since the bias in water level predictions is not always consistent this correction scheme worked well for the wasd station where a consistent bias was present as evaluated from the astronomical tidal analysis in section 4 1 however the same correction scheme for the remaining stations introduced a synthetic error by offsetting the predicted water levels at extended lead times therefore this bias correction scheme was only applied to iflood forecasts at wasd for future forecast advisories 4 2 ensemble forecasts evaluation of ensemble based flood guidance was only performed for three possible ensembles discussed in section 2 4 7 ensemble1 ensemble2 and ensemble3 fig 22 shows the overall rmse considering the 6 months period jan 19 to jun 19 from the these ensembles alternatives bias corrected etss and the ahps official forecast benchmarked against the noaa observations for all the stations inside the bay the rmse for ensemble1 showed the maximum values among all the ensembles compared to the ahps at majority of the stations while it was significantly reduced at cbbv ensemble1 resulted in a rmse of 0 13 m for the stations inside the chesapeake bay while wmva had the least rmse 0 09 m ensemble2 predictions resulted in the lowest rmse of all the three ensembles with the exception of wasd station the rmse for ensemble2 was also lower compared to ahps at the majority of the stations except for ocim wasd cbbv and kptv the rmse value for the wasd was smallest for ensemble3 compared to the other ensembles however it was greater than ahps at wasd the ensemble3 values of rmse ranged between ensemble1 and ensemble2 while it had the highest rmse value of 0 26 m for cbbv fig 22 shows that the ensemble2 has the lowest rmse for nearly all the stations inside the bay the best performing ensemble2 is the result of combining one over predicting cbofs and two slightly under predicting iflood and etss model guidance as expected the choice of the members forming an ensemble greatly influences the overall performance di liberto et al 2011 and the consistent under prediction by one of the members in ensemble1 increases the overall error in the predictions from this ensemble the rmse values for all the flood guidance systems etss estofs and cbofs the bias corrected iflood ensembles ensemble1 ensemble2 and ensemble3 and the official ahps are averaged over all the stations and summarized in fig 23 the results show that the ensemble2 had the lowest value for rmse 0 11 m and produces similar results to the ahps official forecast 0 11 m this improved ability to predict floods resulted from the best performing ensemble members etss iflood and cbofs during the daily operations the results presented below demonstrate that the ensemble forecast guidance for water levels could provide higher accuracy for daily operations than any individual member 5 conclusions this study presented the components of the fully coupled surge wave iflood guidance system for the chesapeake bay the coupled adcirc swan coastal hydrodynamic and wave model was forced with the nam 12 km weather model wind and pressure to forecast water levels and waves inside the chesapeake bay for a lead time of 84 h three existing noaa operational flood guidance systems etss estofs and cbofs in the chesapeake bay are currently supporting the nws to produce the official water level forecasts ahps the iflood guidance system is developed to provide additional model guidance inside chesapeake bay using the coupled surge wave modeling in addition to already existing surge only operational guidance systems etss estofs and cbofs the operational performance of iflood was compared during the daily and extreme weather operations against other existing operational flood etss estofs and cbofs and wave wavewatch3 global regional and nwps guidance systems over 6 months of period in the year 2019 january to june hurricane dorian 2019 was used as an example to evaluate the extreme weather performance of the iflood coupled surge wave guidance system during the daily operations the coupled iflood system performed as the second best guidance system in its ability to predict total water levels rmse of 0 15 m after the bias corrected etss 0 14 m in the bay iflood and etss also demonstrated greater pod 80 85 for flood alerts inside the chesapeake bay the nam 12 km forecasted winds utilized in the iflood showed a mean rmse value of 3 8 m s averaged over all the stations in the chesapeake bay the daily wave forecasts predicted by iflood showed the least bias and rmse 0 25 m 0 60 m compared to the nwps 0 54 m 0 75 m ww3 global 0 42 m 0 74 m and ww3 regional 0 55 m 0 69 m wave guidance systems averaged over all the ndbc buoys assessment of the water levels predicted by iflood during hurricane dorian resulted in the second lowest rmse of 0 15 m compared to the bias corrected etss forecasts 0 12 m averaged over all the stations in the chesapeake bay the significant wave heights showed the lowest rmse 0 62 m among all the operational wave forecast systems wavewatch3 global regional and nwps the operational evaluation of predicted wind speeds at 10 m height from nam 12 km and gfs 27 km weather models resulted in similar error at the lower bay however the rmse in predicted wind speeds from gfs 27 km increased at the middle bay the average rmse of nam 12 km predicted wind speeds during the hurricane dorian showed a lower value of rmse 2 45 m s whereas gfs 27 km wind speeds showed a greater value of rmse 3 2 m s furthermore two post processing schemes are evaluated in the iflood operational water level forecasting in order to improve its ability to predict floods in real time the real time post bias correction using the correction value from previous forecasts minimized the rmse at wasd reduced from 0 22m to 0 15 m reducing a known systematic error while at the other stations it stayed either consistent or increased finally a proposed ensemble forecast scheme considering an equal weightage of flood guidance from the bias corrected etss and raw model outputs from cbofs and iflood resulted in the lowest rmse value 0 1 m for water level predictions inside the chesapeake bay this demonstrates the value added to the current state of the art operational flood guidance systems in the chesapeake bay improving our current ability to predict water levels winds and waves in a large estuary during daily and potentially extreme weather operations a web based data portal for the iflood forecasted products is provided for the general public and scientific community through url https iflood vse gmu edu in addition a text and email based alert system is also established to utilize iflood predictions in order to inform communities on the forecasted flood scenarios in the chesapeake bay the iflood forecast setup is undergoing further improvements to include the complex hydrodynamics of seasonal variability of water levels river discharges effect of gulf streams and offshore swells the future work will focus on implementing these complex hydrodynamics in the iflood operational framework and improving the tidal modeling in the shallower parts of the bay while making use of data assimilation to improve flood and wave forecasting in the bay author contributions the first author under the supervision of the second author carried out the major portion of the work declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was made possible by the research assistantship funding from the george mason university the research resources from the flood hazards research lab fhrl and the collaboration with the washington baltimore nws forecast office on the insight of ahps official flood forecasting procedure additionally this research work was partially supported by virginia sea grant program grant number id 204985 any opinions findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the nws this work used the extreme science and engineering discovery environment xsede stampede2 resources through allocation id tg bcs130009 which is supported by national science foundation grant number aci 1548562 towns et al 2014 the authors acknowledge the texas advanced computing center tacc at the university of texas at austin for providing hpc resources that have contributed to the model calibration results reported within this paper http www tacc utexas edu the authors are deeply grateful to senior service hydrologist jason elliot for his guidance mentorship and insights on the forecasting operations at the nws and streamlining the integration of iflood into nws daily workflow the authors will also like to thank the invaluable contributions of the students from thomas jefferson high school for science and technology mentorship program kristine wang for implementing the prototype flood forecasting platform at fhrl and william o connell for developing the interface for iflood and gmu phd students juan luis garzon hervas ali mohammad rezaie and tyler miesse for continuous technical support appendix d supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix d supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104748 appendix a the astronomical tidal analysis was performed for 8 major tidal constituents m2 n2 k1 k2 o1 p1 q1 s2 for 120 days in the year of 2018 m2 and s2 were the most prominent in the bay as noted by the amplitude already presented in the main manuscript while the remaining tidal constituents k1 k2 o1 p1 q1 s2 from the analysis is shown in fig a1 and fig a2 fig a 1 comparison of iflood modeled tidal amplitude of tidal constituents k1 k2 o1 p1 q1 s2 vs noaa tidal constituents in the chesapeake bay fig a 1 fig a 2 comparison of iflood modeled tidal phase of tidal constituents k1 k2 o1 p1 q1 s2 vs noaa tidal constituents in the chesapeake bay fig a 2 appendix b the water level predicted by the iflood guidance system were partitioned into various lead times 12 24 36 48 60 72 and 84 the rmse of the water level forecasts against the noaa observations were averaged for various lead times over a period of 6 months jan 19 to jun 19 fig b1 presents the mean value of rmse over the 6 months period at 12 24 36 48 60 72 and 84 lead time fig b 1 rmse plot for iflood water level forecasts over lead times 6 12 24 48 72 84 h for stations in the bay averaged over the period of 6 months jan 19 to jun 19 fig b 1 appendix c screenshot of real time visualization of iflood web based portal for display of flood information in the chesapeake bay water level waves and iflood iot sensor stations are displayed from a sample forecast advisory fig c 1 real time visualization of water levels and waves monitoring stations visualizing the flooded station location using google maps api https iflood vse gmu edu map fig c 1 
25996,the prediction and control of river sediment yield sy are critical but challenging tasks erosion and sediment transfer in river catchments are controlled by different processes whose relative importance varies in space and time we thus put forward that sy can be estimated more efficiently by using explicitly the information contained in the similarity within groups to test this hypothesis we developed a novel bayesian hierarchical model applied it to a sample of heterogeneous river catchments and compared its fixed effects and mixed effects performance incorporating different group levels namely gauges rivers basins and clusters of catchments with a parsimonious linear model consisting of four variables specific and extreme discharge elevation and retention coefficient we achieved good performance criteria in the calibration nse 0 79 0 85 and in the cross validation for temporal and spatial prediction nse 0 71 and 0 72 respectively these results support the promising potential of this technique keywords bayesian hierarchical modelling erosion sediment transport river sediment yield spatial prediction temporal prediction cluster analysis 1 introduction the delivery of sediments to surface water bodies as a result of soil erosion can exert a critical effect on flood risk lane et al 2007 on the lifetime of reservoirs kondolf et al 2014 and on the health of benthic ecosystems greig et al 2005 it can also be responsible for increased water treatment costs and for the decline of fisheries resources bilotta and brazier 2008 further the transport of sediments is mostly coupled with the transfer of organic carbon phosphorus and a broad spectrum of particle bound contaminants from soil into water which further contribute to the degradation of aquatic environments long 2006 moran et al 2017 prediction and control of riverine sediment transport are thus fundamental goals for water managers worldwide in this context models are essential tools to estimate sediment yield sy e g t y 1 at catchment outlets to interpret spatial and temporal dynamics as well as to quantify and predict the consequences of climate and land use changes however the extreme complexity and variability of the processes linking soil erosion with river sy make these tasks very challenging the need for estimates of sediment yield and for the understanding of the major factors and processes controlling sy from watersheds across spatial scales is a field of research of long standing nature lane et al 1997 yet the reviews of merritt et al 2003 and de vente et al 2013 which compared and critically discussed existing models designed to predict soil erosion and sediment yield at catchment scale revealed a scattered and still unsatisfactory situation based on the classification system proposed by wheater et al 1993 merritt et al 2003 distinguished between empirical conceptual and physics based models similarly according to de vente et al 2013 models can be conceptually classified as follows spatially lumped and spatially distributed empirical regression physics based and factorial scoring models lisem roo et al 1996 pesera kirkby et al 2008 and swat arnold et al 1998 are examples of widely used physics based models which are based on the numerical solution of fundamental physical equations such as transfer of mass momentum and energy empirical and conceptual models e g watem sedem van oost et al 2000 verstraeten and poesen 2001 do not make explicit inference on detailed physical processes and rely instead on observed or stochastic relationships between causal variables and sediment yield this distinction should not be interpreted in absolute terms because there are also models which consist of a mix of physics based and empirical components e g agnps young et al 1989 factorial scoring models like fsm verstraeten et al 2003 are semi quantitative methods which characterise catchments through factors coupled with scoring and which require in general an expert assessment in the field approaches for statistical modelling of sy which are most typically spatially lumped include multiple linear regression e g de vente et al 2011 and non linear regression models e g bqart syvitski and milliman 2007 the main outcome of merritt et al 2003 was that simpler empirical and conceptual approaches were more appropriate for the estimation of sy than physics based or more complex conceptual models because these were limited by the lack of sufficient spatially distributed data by the over dependency of the results on the experience of the modeller and by high computational requirements the more recent review of de vente et al 2013 similarly found that the elevated requirement of calibration parameters for most physics based models often leads to equifinality and limits their applicability for spatial extrapolations and for scenario studies further outcomes of this review were among others that i the accuracy of existing models differs across spatial and temporal scales and that different models should be selected according to the size of the catchments of interest ii a drawback of many models lies in the fact that they depict only selected erosion and sediment transport processes which limits their applicability to catchments where such processes are dominant and in turn requires an extensive prior knowledge of those e g sheet rill and ephemeral gully erosion hillslope erosion and channel erosion iii there is definitely need for further model development and for balancing between model complexity and quality of input data in this respect there is a powerful technique that has been so far overlooked in this field we refer to hierarchical linear models also known as multilevel models or random coefficient models in such an approach data is grouped in a hierarchy of successively higher level units and instead of considering observations as independent from each other it is assumed that groups within each level e g annual sy of gauges gauges of rivers and rivers of basins share certain attributes and show similarities the key idea is to explicitly use this information by considering both the within and between group variances with the goal to improve model efficiency and estimate accuracy from the statistical point of view this means that model parameters vary at more than one level and that inferences made about one group affect inferences about another in other words the model operates a partial pooling within levels providing a balanced approach between complete pooling same intercept and slopes for all data points i e underfitting and no pooling individual intercepts and slopes for each data point i e overfitting major advantages of hierarchical models are improved estimates for repeated and imbalanced samples the explicit modelling of the variation across individuals within groups of the data the fact that there is no need to perform averaging and consequently no associated loss of information as well as an optimal trade off between overfitting and underfitting mcelreath 2016 hierarchical models are a well established method in social and medical sciences to divide subjects into groups and they are increasingly used in environmental and ecological sciences because they enable incorporating cross scales interactions and thus enhance the model effectiveness both in understanding causal effects and in prediction quian et al 2010 further thanks to the exceptional progress of computational power achieved over the last decades it is now technically feasible to develop hierarchical models within a bayesian framework this offers important possibilities among which the explicit incorporation of prior knowledge into the model and the obtainment of probability distributions of both model parameters and estimates the latter in turn allows for a thorough analysis of the significance and the uncertainty of the results gelman and hill 2006 based on these characteristics we hypothesise that this technique holds a considerable potential to efficiently and reliably predict sy at catchment level and that it might help to overcome at least partially the limitation of having to rely on different models depending on the scale and the properties of the catchments the main consideration behind this hypothesis is that on the one hand there are common processes regulating soil erosion and transport of sediments in all catchments and that on the other hand their relative importance changes greatly in space and time we can for instance expect that catchments with similar morphological traits hydrological regimes and land use also show similarities in the dominant processes determining sy at their outlets further individual catchments typically belong to larger groups such as river systems or basins this constitutes an ideal problem for bayesian hierarchical models which are designed to optimally use the information contained in the variability of the data across nested levels a widely used technique in hydrology top kriging skøien et al 2006 relies on a similar idea it also makes use of the fact that information provided by gauges of the same river system helps to predict a streamflow related variable at an unobserved location better than information provided by gauges of other river systems while top kriging incorporates similarity of topological relation and geographical location only bahsym is also capable of incorporating similarity of other factors the goal of this work is to test our hypothesis by developing a bayesian hierarchical model able to describe and predict sy in heterogeneous river catchments in austria for the development and validation of the model we consider both temporal and spatial cross validation according to the outcomes of their review de vente et al 2013 discarded linear regression equations as suitable prediction models since in a number of case studies they proved unstable and unsuitable to extrapolate sediment yield beyond the calibration datasets verstraeten and poesen 2001 grauso et al 2008 haregeweyn et al 2008 de vente et al 2011 we hypothesise that the bayesian hierarchical approach holds the theoretical power to considerably improve the performance and reliability of otherwise unstable linear regressions this is why in this work we develop and test parsimonious linear regression models consisting of few explanatory variables 2 methods 2 1 study area the study area includes 30 austrian river catchments for which data of suspended solids transported at the outlet is available for multiple years with high temporal resolution this sample consists of catchments that are highly heterogeneous in their total area 135 10 660 km2 average elevation 256 2495 m a s l mean slope 9 61 mean discharge 1256 m3s 1 and land use most of the gauges are located in alpine or mountainous areas whereas agricultural catchments in lowland are more sparse fig 1 depicts the geographical location of the gauges whereas the basic properties of the corresponding catchments are reported in table 1 it is important to observe the enormous temporal variability of sediment transport in the dataset with annual sy varying in some cases up to an order of magnitude at the very same gauge 2 2 bahsym bayesian hierarchical sediment yield model general model structure bahsym consists of a linear regression model embedded within a bayesian hierarchical approach the basic level that we consider in the hierarchical model structure are individual gauges i e we assume that at each gauge sy observations in different years are not fully independent and that dominant processes are to a certain degree similar further we hypothesise that a specific gauge shares more similarities with other gauges within the same river system or within the same basin than with gauges located in other river systems or basins therefore we created two model variants in which we nested the first level into a second higher level namely the level of rivers or that of basins respectively to evaluate whether and how much this hierarchical structure improves the model performance we also tested the model without any level which would correspond to an ordinary linear regression model in a non bayesian context we refer to the variant without levels as fixed effects model and to the variants with hierarchical levels i e random effects as mixed effects models in a mixed effects model the fixed effects describe the effect sizes of the overall mean and the random effects the individual effect sizes of the hierarchical levels which have to be either added to or subtracted from the fixed effects annual sy was not modelled as such but instead as the logarithmic transformation of the specific annual sediment yield ssy t km 2y 1 choosing ssy instead of sy enables to overcome the masking effect of different catchment sizes the logarithmic transformation was necessary to meet the assumption of normality the set of equations 1 presents the mathematical formulation of the mixed effects model with two different group levels it is formulated using non centered parametrization i e with subtraction of the mean fixed effects and factored out standard deviations of the random effects 1a log s s y i normal μ i σ 1b μ i x i j b j i 1c b j i β j β j l e v e l 1 i σ j l e v e l 1 β j l e v e l 2 i σ j l e v e l 2 1d β j normal 0 0 5 1e β j l e v e l 1 i mvnormal 0 j p j k l e v e l 1 1f β j l e v e l 2 i mvnormal 0 j p j k l e v e l 2 1g p j k l e v e l 1 lkjcorr 1 1h p j k l e v e l 2 lkjcorr 1 1i σ j l e v e l 1 exponential 1 1j σ j l e v e l 2 exponential 1 1k σ exponential 1 where log ssy i is assumed to be normally distributed with mean μi and standard deviation σ xij represents the matrix of explanatory variables b j i the combined fixed and random effects i e mixed effects βj the slopes of the fixed effect βj level1i and βj level2i the slopes of the random effects of the two different group levels σj level1 and σj level2 the factored out standard deviations of the slopes of the random effects the symbol represents the hadamard product which means variable wise multiplication of the slopes of the random effects with their corresponding standard deviation in the present case furthermore pjk level1 and pjk level2 stand for the correlation matrices of the slopes of the random effects with respect to the matrix vector indices i indexes observations rows of the matrix of explanatory variables whereas j and k index explanatory variables columns of the matrix of explanatory variables vectors of the effect sizes of the explanatory variables rows and columns of the correlation matrices of the explanatory variables due to the fact that the response variable has been centered and scaled and thus has a mean of zero the model is formulated without any intercept adding an intercept in such a case would not significantly improve model predictions equations 1d 1k define standard priors for centered and scaled variables according to mcelreath 2020 see section 2 3 for details on centering and scaling while these standard priors do not improve model predictions they prevent implausible parameter values furthermore having only centered and scaled explanatory variables allows us to use the same standard priors for all of them apart from the possibility to incorporate subjective or prior information into the model in our opinion an underestimated advantage of bayesian statistics is that it can make certain modelling steps simpler for example back transformation of logarithmic response variables i e exponentiation is straight forward and does not require a correction such as the one explained by laurent 1963 2 3 explanatory variables we considered a pool of potential explanatory variables for bahsym which are reported in table 2 since this work focuses on the methodological approach and not so much on identifying the best performing variables we constrained the selection to a set of relatively few attributes which have the theoretical potential to explain the spatial and or temporal variability of erosion and sediment transfer at catchment scale we thus have chosen average and extreme hydrological variables morphometric traits of the catchments as well as main land uses additionally we have considered the sediment retention coefficient ξ a variable conceived by gavrilovic 1976 as sediment delivery component to be combined with an erosion rate in a semi quantitative model to predict sy at basin scale de vente and poesen 2005 it is thus based on characteristics which mainly influence sediment transport and retention processes such as morphometric traits and waterway length it is calculated as follows 2 ξ p e e l p l a l p 10 a where pe is the perimeter of the catchment km e the average elevation in km a s l lp the length of the principal waterway km and la the cumulated length of secondary waterways km the selection of these variables was driven by expert knowledge regarding the specific study area in other regions different variables might be more suitable and relevant all variables including the response variable have been centered and scaled for modelling purposes centering refers to the practice of subtracting the sample mean from all values of a variable whilst scaling describes the practice of dividing all values of a variable by its sample standard deviation as a result all variables have a mean of zero and a standard deviation of one and their effect sizes in the model can be compared independently of the scale of the original variables the additional centering and scaling of the response variable mean that increasing or decreasing an explanatory variable by one standard deviation while keeping all other explanatory variables constant causes the response variable to change by one standard deviation times the effect size of the adjusted explanatory variable increasing an explanatory variable with for example an effect size of plus 0 5 by 0 5 would therefore cause the response variable to change by 0 25 standard deviations thus centering and scaling eases the interpretation of the modelling results when testing different combinations of explanatory variables we selected for each run a maximum of three to four variables to keep the model parsimonious apart from additive effects we also tested multiplicative interactions between the variables 2 4 data for 27 out of 30 gauges data on daily loads of transported suspended solids were provided for the years 2009 2014 by the directorate iv 4 water balance hydrographical central office of the austrian federal ministry for sustainability and tourism delivered in may 2017 for the rivers pinka and wulka data on suspended solids concentrations corresponding to 48 h composite samples were provided for the same period of time by the provincial government of burgenland as for the river raab data for the years 2009 2014 stems from a station equipped with devices for continuous monitoring of water quality parameters which is operated by the institute for water quality and resource management of the tu wien on behalf of the austrian federal ministry for sustainability and tourism camhy et al 2013 fuiko et al 2016 for all gauges daily or more detailed available loads were summed up to obtain annual sediment yields for each year i e the modelled response variable in the proposed bahsym approach daily precipitation data with 1 1 km spatial resolution were extracted from the spartacus dataset of the central institution for meteorology and geodynamics hiebl and frei 2017 daily discharges were obtained from the austrian hydrographical service ehyd 2017 elevation and slope data stem from the official digital terrain model of austria which has a spatial resolution of 10 10 m dem 2016 detailed land use data at catchment scale for the period 2009 2014 including river network length and lakes surface was made available by clara et al 2019 2 5 model combined with catchment clustering should the model be applied for spatial prediction i e to estimate annual sy for unmonitored locations it would not be meaningful to use specific gauges as group level for this purpose we employed and tested a variation of the mixed effects model in which the group level consists of clusters of similar catchments only in this way it shall be possible to predict sy for new gauges by assigning them to one of the identified clusters we formed clusters of catchments by following a two step procedure in the first place we carried out a principal component analysis pca jolliffe 1989 based on a subset of the variables reported in table 2 including topographic attributes traits of the surface water network land use and river discharge the subset of variables together with their value for each catchment are reported by zoboli and hepp 2020 thus in a second step we used the first two principal components which explain approximately 63 of the total variance table 3 to identify clusters of catchments to perform the cluster analysis we applied the partitioning around medoids pam algorithm park and jun 2009 kaufman et al 1987 the identified clusters are described and discussed in section 3 1 2 6 model evaluation as described in the previous sections we tested different combinations of group levels which bring into being the bahsym variants depicted by the set of equations 3 these equations correspond to variants of equation 1c whereas the rest of the general modelling approach is the same for all for the reasons discussed previously we tested equations 3a 3c for temporal prediction e g to fill yearly gaps whereas equation 3d was employed for spatial prediction e g to extrapolate sy for catchments without monitoring of sediment transport 3a b j i β j β j g a u g e i σ j g a u g e 3b b j i β j β j g a u g e i σ j g a u g e β j r i v e r i σ j r i v e r 3c b j i β j β j g a u g e i σ j g a u g e β j b a s i n i σ j b a s i n 3d b j i β j β j c l u s t e r i σ j c l u s t e r the bahsym variants were tested through a k fold cross validation procedure to test the use of the model for spatial prediction to estimate sy for out of sample gauges we applied a 10 fold cross validation leaving out gauges stratified by cluster the available data was split in ten training and test sets given the 30 available gauges each training set consists of approximately 27 sites and each test set of approximately three different sites each time it is important to note that all annual observations at one site stay together each time i e it cannot happen that for example the years 2009 2010 2012 and 2014 of one site are in the training set and the years 2011 and 2013 of the same site are in the test set this type of cross validation is solely used to test the model s capability to predict sy at new sites furthermore stratifying by clusters makes sure that each test set contains approximately one site of each cluster the goodness of fit metrics are then calculated from the collected predictions of all test sets since the available dataset of sy comprises six years to test the performance of the models for temporal prediction to estimate sy for out of sample years we applied a 6 fold leave one year out cross validation the six years of available data was split into six training and test sets each training set consists of the data corresponding to all sites for five years whereas each test set contains the data corresponding to all sites for the remaining year in each fold a different year this type of cross validation is solely used to test the model s capability to predict sy for new years the goodness of fit metrics are then likewise calculated from the collected predictions of all test sets following criteria were selected to quantify the performance of the models in estimating annual sy nash sutcliffe efficiency nse modified nash sutcliffe efficiency mnse and r squared r2 nse measures the goodness of fit of the model with values ranging from for poor predictive power to one for perfect match between modelled values and data nash and sutcliffe 1970 mnse is a modification of nse which is less sensitive to extreme values and is more influenced by low values krause et al 2005 we applied it to better evaluate the model performance for catchments with relatively smaller ssy in addition we selected the root mean square error rmse and percent bias pbias yapo et al 1996 whereas rmse calculates the standard deviation of the model prediction error pbias indicates the average tendency of modelled values to be larger or smaller than the observed ones in addition bahsym was applied to the whole dataset to identify the best fit model the general best fit model is presented and discussed in section 3 2 while the results for temporal and spatial prediction are described in section 3 3 2 7 software the model was developed and tested with r version 3 6 1 r core team 2019 in particular we made use of the brms package version 2 10 0 which is specifically designed to implement bayesian multilevel models in r using the probabilistic programming language stan bürkner 2017 2018 codes and dataset are publicly available on the zenodo platform zoboli and hepp 2020 3 results and discussion 3 1 clusters of catchments the three groups of catchments obtained through the cluster analysis are depicted in fig 1 and their detailed composition and specific attributes are reported on the zenodo platform zoboli and hepp 2020 here their main characteristics are described before analysing their distinctive traits it is important to observe that they largely differ in size clusters no 1 2 and 3 are composed by 15 ten and five catchments respectively this unbalance is mainly caused by the fact that available sy data stems in the majority from gauges located in alpine and mountainous regions whereas lowland areas are rather under represented fig 2 shows for each cluster the range of variation of the variables selected for the pca the first cluster no 1 is characterised by a median elevation of 1911 m and a median slope of 56 other distinctive attributes are the presence of glaciers and large shares of alpine bare areas and alpine grassland the second cluster no 2 is mainly composed of mountainous catchments although with lower median elevation 1181 m and slope 44 in this cluster glaciers and bare areas are present as well though they occupy a much smaller share of land whereas forests and grassland are largely prevalent the third and smallest cluster no 3 has a median elevation of 582 m and a median slope of 23 there are obviously no glaciers and bare areas and grassland are far less important this cluster has a high average share of forest cover but its most distinct trait is the significantly larger presence of steep arable land with respect to specific discharge the first two clusters are quite similar with a median value of 0 03 m3 s 1 km 2 whereas the third cluster presents a significantly lower value of 0 01 m3 s 1 km 2 total catchment area is not a distinctive factor for these clusters we can only observe that whereas cluster no 3 presents a very narrow range of variation around small areas the other two are characterised by broad ranges 3 2 best fit model the best model and cross validation results were obtained with an additive model comprising four variables namely average elevation e specific discharge q extreme discharge qp95 and sediment retention coefficient ξ it performs best in its variant with two group levels gauges and rivers this model is described in the set of equations 4 where priors consist of standard priors for centered and scaled variables according to mcelreath 2020 and are described by equations 1d 1k 4a log s s y i normal μ i σ 4b μ i x i j b j i 4c x i j e i q i q p 95 i ξ i 4d b j i β j β j g a u g e i σ j g a u g e β j r i v e r i σ j r i v e r as reported in table 4 the slope parameters βj of the first three variables are statistically significant with a 99 confidence interval whereas this is true for the sediment retention coefficient ξ with a 90 confidence interval an investigation of the residuals furthermore confirmed that the assumption of normality was met and that the residuals do not show any signs of non linearity or heteroscedasticity the residuals plot is reported in fig a1 in the appendix although catchment area has been considered in the past to be a relevant predictor for ssy the critical review of de vente et al 2007 concluded that depending on scales and regional specificities the relation between a and ssy can vary from positive to negative and is often non linear it is thus in general a poor predictor of ssy this is in line with our outcomes in fact although a is indirectly considered in this work as component of the sediment retention coefficient its inclusion as separate variable does not improve the model performance slope is almost interchangeable with elevation although e performs slightly better the fact that they hold a similar explanatory power can be explained through their very high correlation coefficient of 0 92 likewise high correlation coefficients between most land use variables and e explain to a large extent why these do not bring almost any improvement to the performance of the parsimonious model indicated above the study of gericke and venohr 2012 on erosion in german mountainous catchments similarly found a strong correlation between sy and average elevation for a complete overview of the correlation coefficients between the variables reported in table 2 please refer to fig a2 in the appendix fig 3 graphically shows the comparison between observed and modelled ssy obtained with the best bahsym variant with two group levels gauges and rivers whereas table 5 reports the performance criteria for all bahsym variants applied to the whole dataset in support of our initial hypothesis we achieve a notable improvement of the model performance through the technique of partial pooling this is true for all variants of bahsym with distinct group levels although they lead to partially different outcomes the variant based on catchment clusters as group level brings a notable enhancement compared to the fixed effects model with r2 raised from 0 70 to 0 80 nse from 0 69 to 0 79 and mnse from 0 43 to 0 54 respectively whilst rmse was lowered from 1 33 t ha 1 to 1 08 t ha 1 this improvement comes however at the expense of a bias increase from 6 9 to 11 3 nevertheless it was by partially pooling over gauges and over the combination of gauges with rivers or basins that we achieved the real breakthrough in model performance r2 increased to 0 84 0 85 nse to 0 83 0 85 mnse to 0 62 0 63 whereas rmse was decreased below 1 t ha 1 further these three variants even reduced the bias despite the improvements we can observe that mnse is consistently lower than nse in all model variants in other words the model s estimates are more reliable for catchments with greater ssy independently from their size this is exemplified by the performance of the best bahsym variant with two group levels gauges and rivers for two gauges highlighted in colour in fig 3 the model performed very well for the gauge kössen hütte located in the mountainous river großache and included in cluster no 2 r2 0 93 nse 0 88 mnse 0 56 rmse 106 t km 2 pbias 10 3 the performance was however rather poor for the gauge neumarkt located in the rather lowland river raab and included in cluster no 3 r2 0 32 nse 0 11 mnse 0 07 rmse 6 t km 2 pbias 9 7 while their performance visually might appear comparable the goodness of fit metrics clearly show the difference especially the nse is a sensitive metric for the relative relationship of the magnitude of modelled residual variance and measured data variance in this respect viewed in isolation the model captures the measured data variance of kössen hütte way better than of neumarkt which is also clearly reflected by the r2 metric 3 3 model for temporal and spatial prediction taking into account the huge temporal variability of sy observed in our dataset the fixed effects model does not perform badly for temporal predictions with a nse of 0 65 and a rmse of 1 41 t ha 1 table 6 this means that the selected combination of variables has a high explanatory power with respect to the temporal variability of ssy two of the four variables namely specific and extreme discharge are time dependent and especially the latter largely varies from one year to the next table 6 shows the considerable improvement of the model performance for temporal prediction achieved through partial pooling the three bahsym variants achieve the same nse of 0 71 the combination of gauges with rivers or with basins in a mixed effects model with two group levels slightly improves rmse from 1 41 t ha 1 to 1 27 t ha 1 at the expense of a small bias increase taking individual gauges as single group level instead than the combination of two group levels leads to a better fit for individual catchments also for the ones with smaller ssy which is reflected in a slightly higher mnse of 0 50 nevertheless this improvement comes at a small expense of the general performance since r2 and rmse reflect to a greater extent the dominance of catchments with high ssy the ability of the fixed effects model for temporal prediction is thus enhanced by the structure of bahsym including random effects allows the effect sizes of the explanatory variables to be correlated making use of at least one explanatory variable that varies in time can cause the effect sizes of the other explanatory variables to depend on the effect size of that time dependent variable for example in a year with high extreme discharge the effect size of the retention coefficient which does not vary in time can be different from the one it has in a year with low extreme discharge while such correlations are often not statistically significant they still affect the predictive power of models making use of random effects in other words random effects have the potential to add sometimes complicated it depends structures to a linear regression model de vente et al 2013 state that extrapolating sy for different years for catchments that were used for calibration generally leads to better validation results than extrapolating sy for out of sample catchments since differences in e g land use and dominant erosion processes between calibration and validation datasets are relatively small in our case the model performs almost equally well for both purposes the two hydrological variables in the model are fundamental to describe temporal variability but their combination with elevation and the morphometric variables contained in the sediment retention coefficient also allow capturing to a great extent the spatial variability this model application further supports our hypothesis regarding the potential of bayesian hierarchical models in this field as reported in table 7 adding partial pooling over clusters of catchments notably improves all performance criteria although lowland catchments with generally lower ssy are under represented in the sample available for model training partial pooling over the clusters significantly improves mnse from 0 39 to 0 48 this implies that even though the cluster of this type of catchments is relatively small it conveys a significant amount of information on the different erosion and sediment transport processes that distinguish these catchments from the mountainous and alpine ones that is an exemplary benefit of this technique in case of unbalanced datasets nevertheless it is clear from the difference between nse 0 72 and mnse 0 48 that the good performance of the model is dominated by mountainous and alpine catchments with greater erosion and larger transfer of sediments our outcomes show that the idea of combining bahsym with clusters of catchments as group level holds a great potential for spatial extrapolation but they also reveal its limitations in order to apply the model to make robust predictions it is essential that new catchments share fundamental similarities with the available clusters in our case given the largely heterogeneous sample available elevation slope land use and discharge were all important factors to determine the clustering however if a more homogeneous sample was available more specific and targeted criteria could be used the outcomes of bahsym are very promising when compared to those of ordinary linear regressions for example de vente et al 2011 attempted to spatially extrapolate sy and ssy based on linear regression and on a wide number of variables for a sample of 61 catchments with areas comprised between 30 and 13 000 km2 in spain although they could achieve quite good results for calibration nse 0 58 the model performed very poorly in the validation step with a nse of 0 10 for the same dataset they did achieve better validation results nse of 0 35 0 67 with spatially distributed models and 0 72 with the factorial score model but such models are more complex and require considerably more data and expert assessments than the variants of bahsym presented in this paper de vente et al 2008 de vente and poesen 2005 our benchmark is not the model by de vente et al 2011 per se but rather the use of ordinary linear regressions of which their study is an example nevertheless we tested the performance of their model for our study area to do that we reproduced the bahsym model with the variables selected by de vente et al 2011 to model sy and ssy i two topographic variables namely average slope also termed mean slope gradient based on a 25 25 m digital terrain model and relief ratio m km 2 calculated as e m a x e m i n a ii the climatic variable precipitation concentration index calculated as i 1 12 p i 2 p 2 100 where p i is the average monthly precipitation mm and p is the average annual precipitation mm iii the land use variable matorral and iv three lithological and soil texture type variables namely percentage of acid metamorphic rock limestone and fluvisols esdb 2004 with respect to the land use variable there is no perfect match for matorral in austria which corresponds to a mediterranean and sub mediterranean evergreen bush and scrub land use type instead we tested three variables closely resembling this land use type in the austrian landscape namely percentage of sparsely vegetated area transitional woodland shrub and natural grassland corine land cover codes 333 324 and 321 respectively the data and r code for this version of the model are fully available on the zenodo platform zoboli and hepp 2020 the results of this comparative exercise are reported in table 8 the original fixed effects model overall did not perform well and an important reason might lie in the variables which were selected for catchments very different from the ones included in our study area nevertheless it is interesting to observe that adding group levels i e turning it into a mixed effects model did improve the best fit model nse of 0 29 0 60 instead of 0 10 as well as the model for temporal prediction nse of 0 36 0 38 instead 0 11 considerably however adding group levels failed for the use of the model for spatial prediction nse 10 3 instead of 18 7 the comparison of these two models is thus useful to show that more complex structures such as the one of bahsym can indeed be very beneficial but only in combination with suitable variables for each application otherwise they might even worsen the model performance 4 conclusions and outlook the outcomes of this study support the hypothesis that bayesian hierarchical models hold a great potential to improve the prediction of sediment yield in rivers we have shown that through the implementation of this technique even parsimonious linear regression models can provide relatively robust temporal and spatial extrapolations this means that with a reduced amount of data availability for few variables this technique enables filling annual gaps performing predictions for future scenarios and extrapolating sy for catchments without monitoring of sediment transport we have also shown that through bahsym the limitation of having unbalanced datasets for the model training is partially compensated for nevertheless the power of this technique can overcome the lack of information only to a certain extent the robustness and reliability of the predictions remain constrained by the availability of sediment transport data for the austrian case study for example it is evident that at present an enhanced monitoring network would be required in lowland catchments with dominant erosion on arable land what we put forward is the use of this technique for an enhanced extrapolation of sediment yield across scales but the model per se will likely need to be adapted for each case study bahsym shall be thus seen as a methodological approach specific purpose data availability and required temporal and spatial scales shall determine in each application the most adequate variables and group levels to be used future lines of research include upgrading bahsym via advanced correlation structures e g gaussian processes formulating informative priors as well as extending the application of this technique to the investigation of the selected transfer of particulate bound contaminants in river catchments declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements financial support for this study was provided by the austrian federal ministry of sustainability and tourism we would like to thank the associate editor and three anonymous reviewers for providing exceptionally detailed and constructive feedback to a previous version of the manuscript appendix a residuals and correlation matrix fig a1 plot of residuals for the best fit model fig a1 fig a2 correlation matrix for the explanatory variables tested in the model fig a2 
25996,the prediction and control of river sediment yield sy are critical but challenging tasks erosion and sediment transfer in river catchments are controlled by different processes whose relative importance varies in space and time we thus put forward that sy can be estimated more efficiently by using explicitly the information contained in the similarity within groups to test this hypothesis we developed a novel bayesian hierarchical model applied it to a sample of heterogeneous river catchments and compared its fixed effects and mixed effects performance incorporating different group levels namely gauges rivers basins and clusters of catchments with a parsimonious linear model consisting of four variables specific and extreme discharge elevation and retention coefficient we achieved good performance criteria in the calibration nse 0 79 0 85 and in the cross validation for temporal and spatial prediction nse 0 71 and 0 72 respectively these results support the promising potential of this technique keywords bayesian hierarchical modelling erosion sediment transport river sediment yield spatial prediction temporal prediction cluster analysis 1 introduction the delivery of sediments to surface water bodies as a result of soil erosion can exert a critical effect on flood risk lane et al 2007 on the lifetime of reservoirs kondolf et al 2014 and on the health of benthic ecosystems greig et al 2005 it can also be responsible for increased water treatment costs and for the decline of fisheries resources bilotta and brazier 2008 further the transport of sediments is mostly coupled with the transfer of organic carbon phosphorus and a broad spectrum of particle bound contaminants from soil into water which further contribute to the degradation of aquatic environments long 2006 moran et al 2017 prediction and control of riverine sediment transport are thus fundamental goals for water managers worldwide in this context models are essential tools to estimate sediment yield sy e g t y 1 at catchment outlets to interpret spatial and temporal dynamics as well as to quantify and predict the consequences of climate and land use changes however the extreme complexity and variability of the processes linking soil erosion with river sy make these tasks very challenging the need for estimates of sediment yield and for the understanding of the major factors and processes controlling sy from watersheds across spatial scales is a field of research of long standing nature lane et al 1997 yet the reviews of merritt et al 2003 and de vente et al 2013 which compared and critically discussed existing models designed to predict soil erosion and sediment yield at catchment scale revealed a scattered and still unsatisfactory situation based on the classification system proposed by wheater et al 1993 merritt et al 2003 distinguished between empirical conceptual and physics based models similarly according to de vente et al 2013 models can be conceptually classified as follows spatially lumped and spatially distributed empirical regression physics based and factorial scoring models lisem roo et al 1996 pesera kirkby et al 2008 and swat arnold et al 1998 are examples of widely used physics based models which are based on the numerical solution of fundamental physical equations such as transfer of mass momentum and energy empirical and conceptual models e g watem sedem van oost et al 2000 verstraeten and poesen 2001 do not make explicit inference on detailed physical processes and rely instead on observed or stochastic relationships between causal variables and sediment yield this distinction should not be interpreted in absolute terms because there are also models which consist of a mix of physics based and empirical components e g agnps young et al 1989 factorial scoring models like fsm verstraeten et al 2003 are semi quantitative methods which characterise catchments through factors coupled with scoring and which require in general an expert assessment in the field approaches for statistical modelling of sy which are most typically spatially lumped include multiple linear regression e g de vente et al 2011 and non linear regression models e g bqart syvitski and milliman 2007 the main outcome of merritt et al 2003 was that simpler empirical and conceptual approaches were more appropriate for the estimation of sy than physics based or more complex conceptual models because these were limited by the lack of sufficient spatially distributed data by the over dependency of the results on the experience of the modeller and by high computational requirements the more recent review of de vente et al 2013 similarly found that the elevated requirement of calibration parameters for most physics based models often leads to equifinality and limits their applicability for spatial extrapolations and for scenario studies further outcomes of this review were among others that i the accuracy of existing models differs across spatial and temporal scales and that different models should be selected according to the size of the catchments of interest ii a drawback of many models lies in the fact that they depict only selected erosion and sediment transport processes which limits their applicability to catchments where such processes are dominant and in turn requires an extensive prior knowledge of those e g sheet rill and ephemeral gully erosion hillslope erosion and channel erosion iii there is definitely need for further model development and for balancing between model complexity and quality of input data in this respect there is a powerful technique that has been so far overlooked in this field we refer to hierarchical linear models also known as multilevel models or random coefficient models in such an approach data is grouped in a hierarchy of successively higher level units and instead of considering observations as independent from each other it is assumed that groups within each level e g annual sy of gauges gauges of rivers and rivers of basins share certain attributes and show similarities the key idea is to explicitly use this information by considering both the within and between group variances with the goal to improve model efficiency and estimate accuracy from the statistical point of view this means that model parameters vary at more than one level and that inferences made about one group affect inferences about another in other words the model operates a partial pooling within levels providing a balanced approach between complete pooling same intercept and slopes for all data points i e underfitting and no pooling individual intercepts and slopes for each data point i e overfitting major advantages of hierarchical models are improved estimates for repeated and imbalanced samples the explicit modelling of the variation across individuals within groups of the data the fact that there is no need to perform averaging and consequently no associated loss of information as well as an optimal trade off between overfitting and underfitting mcelreath 2016 hierarchical models are a well established method in social and medical sciences to divide subjects into groups and they are increasingly used in environmental and ecological sciences because they enable incorporating cross scales interactions and thus enhance the model effectiveness both in understanding causal effects and in prediction quian et al 2010 further thanks to the exceptional progress of computational power achieved over the last decades it is now technically feasible to develop hierarchical models within a bayesian framework this offers important possibilities among which the explicit incorporation of prior knowledge into the model and the obtainment of probability distributions of both model parameters and estimates the latter in turn allows for a thorough analysis of the significance and the uncertainty of the results gelman and hill 2006 based on these characteristics we hypothesise that this technique holds a considerable potential to efficiently and reliably predict sy at catchment level and that it might help to overcome at least partially the limitation of having to rely on different models depending on the scale and the properties of the catchments the main consideration behind this hypothesis is that on the one hand there are common processes regulating soil erosion and transport of sediments in all catchments and that on the other hand their relative importance changes greatly in space and time we can for instance expect that catchments with similar morphological traits hydrological regimes and land use also show similarities in the dominant processes determining sy at their outlets further individual catchments typically belong to larger groups such as river systems or basins this constitutes an ideal problem for bayesian hierarchical models which are designed to optimally use the information contained in the variability of the data across nested levels a widely used technique in hydrology top kriging skøien et al 2006 relies on a similar idea it also makes use of the fact that information provided by gauges of the same river system helps to predict a streamflow related variable at an unobserved location better than information provided by gauges of other river systems while top kriging incorporates similarity of topological relation and geographical location only bahsym is also capable of incorporating similarity of other factors the goal of this work is to test our hypothesis by developing a bayesian hierarchical model able to describe and predict sy in heterogeneous river catchments in austria for the development and validation of the model we consider both temporal and spatial cross validation according to the outcomes of their review de vente et al 2013 discarded linear regression equations as suitable prediction models since in a number of case studies they proved unstable and unsuitable to extrapolate sediment yield beyond the calibration datasets verstraeten and poesen 2001 grauso et al 2008 haregeweyn et al 2008 de vente et al 2011 we hypothesise that the bayesian hierarchical approach holds the theoretical power to considerably improve the performance and reliability of otherwise unstable linear regressions this is why in this work we develop and test parsimonious linear regression models consisting of few explanatory variables 2 methods 2 1 study area the study area includes 30 austrian river catchments for which data of suspended solids transported at the outlet is available for multiple years with high temporal resolution this sample consists of catchments that are highly heterogeneous in their total area 135 10 660 km2 average elevation 256 2495 m a s l mean slope 9 61 mean discharge 1256 m3s 1 and land use most of the gauges are located in alpine or mountainous areas whereas agricultural catchments in lowland are more sparse fig 1 depicts the geographical location of the gauges whereas the basic properties of the corresponding catchments are reported in table 1 it is important to observe the enormous temporal variability of sediment transport in the dataset with annual sy varying in some cases up to an order of magnitude at the very same gauge 2 2 bahsym bayesian hierarchical sediment yield model general model structure bahsym consists of a linear regression model embedded within a bayesian hierarchical approach the basic level that we consider in the hierarchical model structure are individual gauges i e we assume that at each gauge sy observations in different years are not fully independent and that dominant processes are to a certain degree similar further we hypothesise that a specific gauge shares more similarities with other gauges within the same river system or within the same basin than with gauges located in other river systems or basins therefore we created two model variants in which we nested the first level into a second higher level namely the level of rivers or that of basins respectively to evaluate whether and how much this hierarchical structure improves the model performance we also tested the model without any level which would correspond to an ordinary linear regression model in a non bayesian context we refer to the variant without levels as fixed effects model and to the variants with hierarchical levels i e random effects as mixed effects models in a mixed effects model the fixed effects describe the effect sizes of the overall mean and the random effects the individual effect sizes of the hierarchical levels which have to be either added to or subtracted from the fixed effects annual sy was not modelled as such but instead as the logarithmic transformation of the specific annual sediment yield ssy t km 2y 1 choosing ssy instead of sy enables to overcome the masking effect of different catchment sizes the logarithmic transformation was necessary to meet the assumption of normality the set of equations 1 presents the mathematical formulation of the mixed effects model with two different group levels it is formulated using non centered parametrization i e with subtraction of the mean fixed effects and factored out standard deviations of the random effects 1a log s s y i normal μ i σ 1b μ i x i j b j i 1c b j i β j β j l e v e l 1 i σ j l e v e l 1 β j l e v e l 2 i σ j l e v e l 2 1d β j normal 0 0 5 1e β j l e v e l 1 i mvnormal 0 j p j k l e v e l 1 1f β j l e v e l 2 i mvnormal 0 j p j k l e v e l 2 1g p j k l e v e l 1 lkjcorr 1 1h p j k l e v e l 2 lkjcorr 1 1i σ j l e v e l 1 exponential 1 1j σ j l e v e l 2 exponential 1 1k σ exponential 1 where log ssy i is assumed to be normally distributed with mean μi and standard deviation σ xij represents the matrix of explanatory variables b j i the combined fixed and random effects i e mixed effects βj the slopes of the fixed effect βj level1i and βj level2i the slopes of the random effects of the two different group levels σj level1 and σj level2 the factored out standard deviations of the slopes of the random effects the symbol represents the hadamard product which means variable wise multiplication of the slopes of the random effects with their corresponding standard deviation in the present case furthermore pjk level1 and pjk level2 stand for the correlation matrices of the slopes of the random effects with respect to the matrix vector indices i indexes observations rows of the matrix of explanatory variables whereas j and k index explanatory variables columns of the matrix of explanatory variables vectors of the effect sizes of the explanatory variables rows and columns of the correlation matrices of the explanatory variables due to the fact that the response variable has been centered and scaled and thus has a mean of zero the model is formulated without any intercept adding an intercept in such a case would not significantly improve model predictions equations 1d 1k define standard priors for centered and scaled variables according to mcelreath 2020 see section 2 3 for details on centering and scaling while these standard priors do not improve model predictions they prevent implausible parameter values furthermore having only centered and scaled explanatory variables allows us to use the same standard priors for all of them apart from the possibility to incorporate subjective or prior information into the model in our opinion an underestimated advantage of bayesian statistics is that it can make certain modelling steps simpler for example back transformation of logarithmic response variables i e exponentiation is straight forward and does not require a correction such as the one explained by laurent 1963 2 3 explanatory variables we considered a pool of potential explanatory variables for bahsym which are reported in table 2 since this work focuses on the methodological approach and not so much on identifying the best performing variables we constrained the selection to a set of relatively few attributes which have the theoretical potential to explain the spatial and or temporal variability of erosion and sediment transfer at catchment scale we thus have chosen average and extreme hydrological variables morphometric traits of the catchments as well as main land uses additionally we have considered the sediment retention coefficient ξ a variable conceived by gavrilovic 1976 as sediment delivery component to be combined with an erosion rate in a semi quantitative model to predict sy at basin scale de vente and poesen 2005 it is thus based on characteristics which mainly influence sediment transport and retention processes such as morphometric traits and waterway length it is calculated as follows 2 ξ p e e l p l a l p 10 a where pe is the perimeter of the catchment km e the average elevation in km a s l lp the length of the principal waterway km and la the cumulated length of secondary waterways km the selection of these variables was driven by expert knowledge regarding the specific study area in other regions different variables might be more suitable and relevant all variables including the response variable have been centered and scaled for modelling purposes centering refers to the practice of subtracting the sample mean from all values of a variable whilst scaling describes the practice of dividing all values of a variable by its sample standard deviation as a result all variables have a mean of zero and a standard deviation of one and their effect sizes in the model can be compared independently of the scale of the original variables the additional centering and scaling of the response variable mean that increasing or decreasing an explanatory variable by one standard deviation while keeping all other explanatory variables constant causes the response variable to change by one standard deviation times the effect size of the adjusted explanatory variable increasing an explanatory variable with for example an effect size of plus 0 5 by 0 5 would therefore cause the response variable to change by 0 25 standard deviations thus centering and scaling eases the interpretation of the modelling results when testing different combinations of explanatory variables we selected for each run a maximum of three to four variables to keep the model parsimonious apart from additive effects we also tested multiplicative interactions between the variables 2 4 data for 27 out of 30 gauges data on daily loads of transported suspended solids were provided for the years 2009 2014 by the directorate iv 4 water balance hydrographical central office of the austrian federal ministry for sustainability and tourism delivered in may 2017 for the rivers pinka and wulka data on suspended solids concentrations corresponding to 48 h composite samples were provided for the same period of time by the provincial government of burgenland as for the river raab data for the years 2009 2014 stems from a station equipped with devices for continuous monitoring of water quality parameters which is operated by the institute for water quality and resource management of the tu wien on behalf of the austrian federal ministry for sustainability and tourism camhy et al 2013 fuiko et al 2016 for all gauges daily or more detailed available loads were summed up to obtain annual sediment yields for each year i e the modelled response variable in the proposed bahsym approach daily precipitation data with 1 1 km spatial resolution were extracted from the spartacus dataset of the central institution for meteorology and geodynamics hiebl and frei 2017 daily discharges were obtained from the austrian hydrographical service ehyd 2017 elevation and slope data stem from the official digital terrain model of austria which has a spatial resolution of 10 10 m dem 2016 detailed land use data at catchment scale for the period 2009 2014 including river network length and lakes surface was made available by clara et al 2019 2 5 model combined with catchment clustering should the model be applied for spatial prediction i e to estimate annual sy for unmonitored locations it would not be meaningful to use specific gauges as group level for this purpose we employed and tested a variation of the mixed effects model in which the group level consists of clusters of similar catchments only in this way it shall be possible to predict sy for new gauges by assigning them to one of the identified clusters we formed clusters of catchments by following a two step procedure in the first place we carried out a principal component analysis pca jolliffe 1989 based on a subset of the variables reported in table 2 including topographic attributes traits of the surface water network land use and river discharge the subset of variables together with their value for each catchment are reported by zoboli and hepp 2020 thus in a second step we used the first two principal components which explain approximately 63 of the total variance table 3 to identify clusters of catchments to perform the cluster analysis we applied the partitioning around medoids pam algorithm park and jun 2009 kaufman et al 1987 the identified clusters are described and discussed in section 3 1 2 6 model evaluation as described in the previous sections we tested different combinations of group levels which bring into being the bahsym variants depicted by the set of equations 3 these equations correspond to variants of equation 1c whereas the rest of the general modelling approach is the same for all for the reasons discussed previously we tested equations 3a 3c for temporal prediction e g to fill yearly gaps whereas equation 3d was employed for spatial prediction e g to extrapolate sy for catchments without monitoring of sediment transport 3a b j i β j β j g a u g e i σ j g a u g e 3b b j i β j β j g a u g e i σ j g a u g e β j r i v e r i σ j r i v e r 3c b j i β j β j g a u g e i σ j g a u g e β j b a s i n i σ j b a s i n 3d b j i β j β j c l u s t e r i σ j c l u s t e r the bahsym variants were tested through a k fold cross validation procedure to test the use of the model for spatial prediction to estimate sy for out of sample gauges we applied a 10 fold cross validation leaving out gauges stratified by cluster the available data was split in ten training and test sets given the 30 available gauges each training set consists of approximately 27 sites and each test set of approximately three different sites each time it is important to note that all annual observations at one site stay together each time i e it cannot happen that for example the years 2009 2010 2012 and 2014 of one site are in the training set and the years 2011 and 2013 of the same site are in the test set this type of cross validation is solely used to test the model s capability to predict sy at new sites furthermore stratifying by clusters makes sure that each test set contains approximately one site of each cluster the goodness of fit metrics are then calculated from the collected predictions of all test sets since the available dataset of sy comprises six years to test the performance of the models for temporal prediction to estimate sy for out of sample years we applied a 6 fold leave one year out cross validation the six years of available data was split into six training and test sets each training set consists of the data corresponding to all sites for five years whereas each test set contains the data corresponding to all sites for the remaining year in each fold a different year this type of cross validation is solely used to test the model s capability to predict sy for new years the goodness of fit metrics are then likewise calculated from the collected predictions of all test sets following criteria were selected to quantify the performance of the models in estimating annual sy nash sutcliffe efficiency nse modified nash sutcliffe efficiency mnse and r squared r2 nse measures the goodness of fit of the model with values ranging from for poor predictive power to one for perfect match between modelled values and data nash and sutcliffe 1970 mnse is a modification of nse which is less sensitive to extreme values and is more influenced by low values krause et al 2005 we applied it to better evaluate the model performance for catchments with relatively smaller ssy in addition we selected the root mean square error rmse and percent bias pbias yapo et al 1996 whereas rmse calculates the standard deviation of the model prediction error pbias indicates the average tendency of modelled values to be larger or smaller than the observed ones in addition bahsym was applied to the whole dataset to identify the best fit model the general best fit model is presented and discussed in section 3 2 while the results for temporal and spatial prediction are described in section 3 3 2 7 software the model was developed and tested with r version 3 6 1 r core team 2019 in particular we made use of the brms package version 2 10 0 which is specifically designed to implement bayesian multilevel models in r using the probabilistic programming language stan bürkner 2017 2018 codes and dataset are publicly available on the zenodo platform zoboli and hepp 2020 3 results and discussion 3 1 clusters of catchments the three groups of catchments obtained through the cluster analysis are depicted in fig 1 and their detailed composition and specific attributes are reported on the zenodo platform zoboli and hepp 2020 here their main characteristics are described before analysing their distinctive traits it is important to observe that they largely differ in size clusters no 1 2 and 3 are composed by 15 ten and five catchments respectively this unbalance is mainly caused by the fact that available sy data stems in the majority from gauges located in alpine and mountainous regions whereas lowland areas are rather under represented fig 2 shows for each cluster the range of variation of the variables selected for the pca the first cluster no 1 is characterised by a median elevation of 1911 m and a median slope of 56 other distinctive attributes are the presence of glaciers and large shares of alpine bare areas and alpine grassland the second cluster no 2 is mainly composed of mountainous catchments although with lower median elevation 1181 m and slope 44 in this cluster glaciers and bare areas are present as well though they occupy a much smaller share of land whereas forests and grassland are largely prevalent the third and smallest cluster no 3 has a median elevation of 582 m and a median slope of 23 there are obviously no glaciers and bare areas and grassland are far less important this cluster has a high average share of forest cover but its most distinct trait is the significantly larger presence of steep arable land with respect to specific discharge the first two clusters are quite similar with a median value of 0 03 m3 s 1 km 2 whereas the third cluster presents a significantly lower value of 0 01 m3 s 1 km 2 total catchment area is not a distinctive factor for these clusters we can only observe that whereas cluster no 3 presents a very narrow range of variation around small areas the other two are characterised by broad ranges 3 2 best fit model the best model and cross validation results were obtained with an additive model comprising four variables namely average elevation e specific discharge q extreme discharge qp95 and sediment retention coefficient ξ it performs best in its variant with two group levels gauges and rivers this model is described in the set of equations 4 where priors consist of standard priors for centered and scaled variables according to mcelreath 2020 and are described by equations 1d 1k 4a log s s y i normal μ i σ 4b μ i x i j b j i 4c x i j e i q i q p 95 i ξ i 4d b j i β j β j g a u g e i σ j g a u g e β j r i v e r i σ j r i v e r as reported in table 4 the slope parameters βj of the first three variables are statistically significant with a 99 confidence interval whereas this is true for the sediment retention coefficient ξ with a 90 confidence interval an investigation of the residuals furthermore confirmed that the assumption of normality was met and that the residuals do not show any signs of non linearity or heteroscedasticity the residuals plot is reported in fig a1 in the appendix although catchment area has been considered in the past to be a relevant predictor for ssy the critical review of de vente et al 2007 concluded that depending on scales and regional specificities the relation between a and ssy can vary from positive to negative and is often non linear it is thus in general a poor predictor of ssy this is in line with our outcomes in fact although a is indirectly considered in this work as component of the sediment retention coefficient its inclusion as separate variable does not improve the model performance slope is almost interchangeable with elevation although e performs slightly better the fact that they hold a similar explanatory power can be explained through their very high correlation coefficient of 0 92 likewise high correlation coefficients between most land use variables and e explain to a large extent why these do not bring almost any improvement to the performance of the parsimonious model indicated above the study of gericke and venohr 2012 on erosion in german mountainous catchments similarly found a strong correlation between sy and average elevation for a complete overview of the correlation coefficients between the variables reported in table 2 please refer to fig a2 in the appendix fig 3 graphically shows the comparison between observed and modelled ssy obtained with the best bahsym variant with two group levels gauges and rivers whereas table 5 reports the performance criteria for all bahsym variants applied to the whole dataset in support of our initial hypothesis we achieve a notable improvement of the model performance through the technique of partial pooling this is true for all variants of bahsym with distinct group levels although they lead to partially different outcomes the variant based on catchment clusters as group level brings a notable enhancement compared to the fixed effects model with r2 raised from 0 70 to 0 80 nse from 0 69 to 0 79 and mnse from 0 43 to 0 54 respectively whilst rmse was lowered from 1 33 t ha 1 to 1 08 t ha 1 this improvement comes however at the expense of a bias increase from 6 9 to 11 3 nevertheless it was by partially pooling over gauges and over the combination of gauges with rivers or basins that we achieved the real breakthrough in model performance r2 increased to 0 84 0 85 nse to 0 83 0 85 mnse to 0 62 0 63 whereas rmse was decreased below 1 t ha 1 further these three variants even reduced the bias despite the improvements we can observe that mnse is consistently lower than nse in all model variants in other words the model s estimates are more reliable for catchments with greater ssy independently from their size this is exemplified by the performance of the best bahsym variant with two group levels gauges and rivers for two gauges highlighted in colour in fig 3 the model performed very well for the gauge kössen hütte located in the mountainous river großache and included in cluster no 2 r2 0 93 nse 0 88 mnse 0 56 rmse 106 t km 2 pbias 10 3 the performance was however rather poor for the gauge neumarkt located in the rather lowland river raab and included in cluster no 3 r2 0 32 nse 0 11 mnse 0 07 rmse 6 t km 2 pbias 9 7 while their performance visually might appear comparable the goodness of fit metrics clearly show the difference especially the nse is a sensitive metric for the relative relationship of the magnitude of modelled residual variance and measured data variance in this respect viewed in isolation the model captures the measured data variance of kössen hütte way better than of neumarkt which is also clearly reflected by the r2 metric 3 3 model for temporal and spatial prediction taking into account the huge temporal variability of sy observed in our dataset the fixed effects model does not perform badly for temporal predictions with a nse of 0 65 and a rmse of 1 41 t ha 1 table 6 this means that the selected combination of variables has a high explanatory power with respect to the temporal variability of ssy two of the four variables namely specific and extreme discharge are time dependent and especially the latter largely varies from one year to the next table 6 shows the considerable improvement of the model performance for temporal prediction achieved through partial pooling the three bahsym variants achieve the same nse of 0 71 the combination of gauges with rivers or with basins in a mixed effects model with two group levels slightly improves rmse from 1 41 t ha 1 to 1 27 t ha 1 at the expense of a small bias increase taking individual gauges as single group level instead than the combination of two group levels leads to a better fit for individual catchments also for the ones with smaller ssy which is reflected in a slightly higher mnse of 0 50 nevertheless this improvement comes at a small expense of the general performance since r2 and rmse reflect to a greater extent the dominance of catchments with high ssy the ability of the fixed effects model for temporal prediction is thus enhanced by the structure of bahsym including random effects allows the effect sizes of the explanatory variables to be correlated making use of at least one explanatory variable that varies in time can cause the effect sizes of the other explanatory variables to depend on the effect size of that time dependent variable for example in a year with high extreme discharge the effect size of the retention coefficient which does not vary in time can be different from the one it has in a year with low extreme discharge while such correlations are often not statistically significant they still affect the predictive power of models making use of random effects in other words random effects have the potential to add sometimes complicated it depends structures to a linear regression model de vente et al 2013 state that extrapolating sy for different years for catchments that were used for calibration generally leads to better validation results than extrapolating sy for out of sample catchments since differences in e g land use and dominant erosion processes between calibration and validation datasets are relatively small in our case the model performs almost equally well for both purposes the two hydrological variables in the model are fundamental to describe temporal variability but their combination with elevation and the morphometric variables contained in the sediment retention coefficient also allow capturing to a great extent the spatial variability this model application further supports our hypothesis regarding the potential of bayesian hierarchical models in this field as reported in table 7 adding partial pooling over clusters of catchments notably improves all performance criteria although lowland catchments with generally lower ssy are under represented in the sample available for model training partial pooling over the clusters significantly improves mnse from 0 39 to 0 48 this implies that even though the cluster of this type of catchments is relatively small it conveys a significant amount of information on the different erosion and sediment transport processes that distinguish these catchments from the mountainous and alpine ones that is an exemplary benefit of this technique in case of unbalanced datasets nevertheless it is clear from the difference between nse 0 72 and mnse 0 48 that the good performance of the model is dominated by mountainous and alpine catchments with greater erosion and larger transfer of sediments our outcomes show that the idea of combining bahsym with clusters of catchments as group level holds a great potential for spatial extrapolation but they also reveal its limitations in order to apply the model to make robust predictions it is essential that new catchments share fundamental similarities with the available clusters in our case given the largely heterogeneous sample available elevation slope land use and discharge were all important factors to determine the clustering however if a more homogeneous sample was available more specific and targeted criteria could be used the outcomes of bahsym are very promising when compared to those of ordinary linear regressions for example de vente et al 2011 attempted to spatially extrapolate sy and ssy based on linear regression and on a wide number of variables for a sample of 61 catchments with areas comprised between 30 and 13 000 km2 in spain although they could achieve quite good results for calibration nse 0 58 the model performed very poorly in the validation step with a nse of 0 10 for the same dataset they did achieve better validation results nse of 0 35 0 67 with spatially distributed models and 0 72 with the factorial score model but such models are more complex and require considerably more data and expert assessments than the variants of bahsym presented in this paper de vente et al 2008 de vente and poesen 2005 our benchmark is not the model by de vente et al 2011 per se but rather the use of ordinary linear regressions of which their study is an example nevertheless we tested the performance of their model for our study area to do that we reproduced the bahsym model with the variables selected by de vente et al 2011 to model sy and ssy i two topographic variables namely average slope also termed mean slope gradient based on a 25 25 m digital terrain model and relief ratio m km 2 calculated as e m a x e m i n a ii the climatic variable precipitation concentration index calculated as i 1 12 p i 2 p 2 100 where p i is the average monthly precipitation mm and p is the average annual precipitation mm iii the land use variable matorral and iv three lithological and soil texture type variables namely percentage of acid metamorphic rock limestone and fluvisols esdb 2004 with respect to the land use variable there is no perfect match for matorral in austria which corresponds to a mediterranean and sub mediterranean evergreen bush and scrub land use type instead we tested three variables closely resembling this land use type in the austrian landscape namely percentage of sparsely vegetated area transitional woodland shrub and natural grassland corine land cover codes 333 324 and 321 respectively the data and r code for this version of the model are fully available on the zenodo platform zoboli and hepp 2020 the results of this comparative exercise are reported in table 8 the original fixed effects model overall did not perform well and an important reason might lie in the variables which were selected for catchments very different from the ones included in our study area nevertheless it is interesting to observe that adding group levels i e turning it into a mixed effects model did improve the best fit model nse of 0 29 0 60 instead of 0 10 as well as the model for temporal prediction nse of 0 36 0 38 instead 0 11 considerably however adding group levels failed for the use of the model for spatial prediction nse 10 3 instead of 18 7 the comparison of these two models is thus useful to show that more complex structures such as the one of bahsym can indeed be very beneficial but only in combination with suitable variables for each application otherwise they might even worsen the model performance 4 conclusions and outlook the outcomes of this study support the hypothesis that bayesian hierarchical models hold a great potential to improve the prediction of sediment yield in rivers we have shown that through the implementation of this technique even parsimonious linear regression models can provide relatively robust temporal and spatial extrapolations this means that with a reduced amount of data availability for few variables this technique enables filling annual gaps performing predictions for future scenarios and extrapolating sy for catchments without monitoring of sediment transport we have also shown that through bahsym the limitation of having unbalanced datasets for the model training is partially compensated for nevertheless the power of this technique can overcome the lack of information only to a certain extent the robustness and reliability of the predictions remain constrained by the availability of sediment transport data for the austrian case study for example it is evident that at present an enhanced monitoring network would be required in lowland catchments with dominant erosion on arable land what we put forward is the use of this technique for an enhanced extrapolation of sediment yield across scales but the model per se will likely need to be adapted for each case study bahsym shall be thus seen as a methodological approach specific purpose data availability and required temporal and spatial scales shall determine in each application the most adequate variables and group levels to be used future lines of research include upgrading bahsym via advanced correlation structures e g gaussian processes formulating informative priors as well as extending the application of this technique to the investigation of the selected transfer of particulate bound contaminants in river catchments declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements financial support for this study was provided by the austrian federal ministry of sustainability and tourism we would like to thank the associate editor and three anonymous reviewers for providing exceptionally detailed and constructive feedback to a previous version of the manuscript appendix a residuals and correlation matrix fig a1 plot of residuals for the best fit model fig a1 fig a2 correlation matrix for the explanatory variables tested in the model fig a2 
25997,landscape simulation is necessary for stakeholders to discuss future landscapes with new designs in order to preserve good landscapes augmented reality can be used to study the future landscape on a large scale by adding a three dimensional design model to the real world on the other hand diminished reality dr can simulate the virtual demolition and removal of structures in redevelopment however it has not been possible to visually remove moving landscape objects such as vehicles and pedestrians in real time for accurate landscape simulation this research develops a dr system that can virtually remove moving landscape objects by implementing real time object detection using deep learning with a game engine as well as immobile objects such as structures in addition to evaluating the performance of detecting the size of moving landscape objects the developed dr system is applied to large scale landscape simulation at two sites and its utility is validated graphical abstract image 1 keywords landscape simulation design support system diminished reality dr augmented reality ar real time object detection deep learning 1 introduction preserving good landscapes is crucial for enhancing our quality of life recently the need for landscape assessment has grown particularly for evaluating the impact of large scale projects on their surroundings the stakeholders in the landscape assessment process consist of experts such as developers planners architects and engineers and non experts such as neighbourhood residents and citizens it is difficult for stakeholders to understand and discuss planned landscape issues especially for three dimensional 3d spatial information that does not yet exist in the design process visualisation of the planned landscape can help stakeholders to understand the project blueprint and build consensus for developing and preserving good landscapes sheppard 1989 bishop and lange 2005 many studies have investigated the development and practical application of computer graphics cg and virtual reality vr as methods for landscape visualisation and simulation using computer technology including lange 1994 lee et al 2001 yan et al 2011 and dorta et al 2016 vr can help stakeholders to review the design targets intuitively and interactively from any viewpoint of interest but for large scale projects it requires a large amount of time and expense to build 3d models of surroundings that consist of the geography existing structures and natural objects augmented reality ar on the other hand allows stakeholders to review 3d design models superimposed onto the real world milgram and kishino 1994 azuma 1997 krevelen and poelman 2010 ar can simulate future landscapes and environments by overlaying textured 3d models of new buildings and structures on the present surroundings and reduce the time and expense required for 3d modelling of the present surroundings yabuki et al 2011 wang 2013 fukuda et al 2014 haynes et al 2018 beyond existing ar approaches that overlay 3d design models on the physical environment advanced ar visualisation is needed that can virtually remove existing structures in redevelopment projects if ar is simulated while old structures and trees that are planned for demolition and removal are still present the 3d virtual objects of the new design structure become intermingled with the existing structures leading to inaccurate visualisation to solve this issue diminished reality dr which is a technique for concealing eliminating and seeing through objects virtually in a perceived environment in real time has been attracting attention mann 1994 mann and fung 2001 mori et al 2017 nakajima et al 2017 dr has advanced to the point of application to various scale simulations including a small scale simulation for interior design sanni 2017 and a large scale simulation for landscape assessment inoue et al 2018 it is necessary to determine the regions where objects will be removed and to observe the background information behind the removal targets to implement dr processing particularly for accurate future landscape simulation a series of studies has developed a dr methodology that realises visual removal of exterior structures for landscape assessment inoue et al 2016 2018 in those methods 3d models of the removal targets are built by manual modelling e g computer aided design and building information modelling data or semi automatic 3d laser scanning and structure from motion sfm modelling software in pre processing these models are overlaid onto the targets for physical removal in the images in order to define the regions of interest rois the roi for each target is a mask that is then covered with a textured 3d model of the recovered hidden background however the previous methods are able to visually remove only objects that do not move between pre processing and run time processes such as exterior structures for example if a bridge is visually removed in a redevelopment project an inaccurate dr will be output in which vehicles and pedestrians are still travelling on the now removed structure of the immobile bridge even if 3d models of cars and humans are created in pre processing it is difficult to extract rois using them because they may have moved during the dr run time therefore it is necessary to visually remove not only immobile exterior structures but also moving objects such as cars and pedestrians in real time for accurate landscape simulation the objective of this research is to develop a novel dr system using real time object detection to perform visual removal of objects including both immobile exterior structures and moving objects for ar based landscape simulation in real time we plan to integrate the real time object detection technique into a dr system for the removal of moving objects in our proposed system rois for immobile exterior structures are determined using pre defined geometric 3d models and rois for moving objects are determined using object detection technology based on deep learning this research proposes a method for realising dr for onsite and large scale landscape simulation a number of methods have been introduced for dr on a small scale enomoto and saito 2007 kawai et al 2014 mori et al 2017 furthermore it is necessary to render the background after virtually removing objects in dr we apply a method for accurately displaying a 3d model of the background surroundings in order to realise highly accurate dr landscape simulation 2 literature review this research aims to develop a dr system that performs real time removal of both immobile exterior structures and moving objects in landscape simulations therefore we review previous studies on dr methodology and object detection techniques 2 1 dr mori et al 2017 reported a procedure for implementing dr that consisted of five methods background observation scene tracking roi detection hidden view generation and composition the present paper addresses the detection of rois including moving objects this section therefore describes previous dr research into background observation and roi detection 2 1 1 background observation many methods have been proposed in which background surroundings are observed using multiple cameras enomoto and saito 2007 zokai et al 2003 meerits and saito 2015 although these methods can acquire background information in real time they need to capture overlapping regions in order to calculate relationships among the captured images making them difficult to apply to large scale simulations image inpainting techniques have been also adopted for dr processing kawai et al 2014 kim et al 2018 this does not require pre processing but does require a few seconds for image processing meaning that real time processing cannot be implemented this also gives rise to the problem of how to create the correct background image for accurate landscape simulation on the other hand pre observation methods have been proposed inoue et al 2016 recreated a textured 3d model of the background surroundings using sfm these methods are time consuming but can be applied to large scale simulation 2 1 2 roi detection enomoto and saito 2007 proposed a dr method for removing obstacles using multiple images captured from different viewpoints this method does not estimate specific rois for removing target objects because the method uses ar tag markers to calibrate multiple cameras and images can be synthesised by sharing all images captured by all the cameras without removing the target objects however the marker needs to remain within the field of view of all cameras which is difficult for outdoor landscape simulation if the cameras and target objects are fixed in the environment the roi can be set manually zokai et al 2003 however the camera used for ar simulation and the moving target objects cannot be fixed when the geometric shapes of the target object are known the corresponding roi can be determined by overlaying the 3d model according to the perspective the 3d model can then be obtained by manual modelling kaneda et al 2004 tsuda et al 2006 inoue et al 2016 this method can determine the roi to be removed for an exterior structure but not for moving objects 2 2 object detection object detection techniques are divided mainly into two types methods using background subtraction and methods using feature sets in an image in this research the background subtraction methodology is not suitable because neither the camera nor the moving objects are fixed hence object detection using feature sets is employed various feature sets such as haar like features viola and jones 2001 histograms of oriented gradients dalal and triggs 2005 edge orientation histograms levi and weiss 2004 and edgelet features wu and nevatia 2005 have been proposed for detecting specific objects in the field of computer vision these feature sets can be used for automatically detecting specific objects but must be customised to match the subject object detection based on deep learning is a fast emerging field convolutional neural networks cnns are a kind of multi layer neural network designed to recognise visual patterns directly from pixel images and many cnn architectures such as alexnet krizhevsky et al 2012 vgg simonyan and zisserman 2014 googlenet szegedy et al 2016 resnet he et al 2016 mobilenets howard et al 2017 have been developed in addition to the development of cnns many cnn based object detection algorithms such as r cnn girshick et al 2014 you only look once yolo redmon et al 2016 and single shot multibox detector ssd liu et al 2016 have been proposed in these methods objects are detected in real time not as a silhouette but as a bounding box that surrounds the object when cnn based object detection is used to determine the roi the roi is defined as a rectangle instead of the silhouette of the object however it has been reported that the roi does not need to be the silhouette of the target object and can be a bounding box that covers the object roughly and sufficiently mori et al 2017 thus it is thought that object detection based on deep learning is effective for determining rois for real time processing in this research the speed of object detection is as important as its accuracy therefore the combination of mobilenet for the cnn architecture and ssd 300 for the object detection model mobilenetssd is employed as the moving object detection technique as huang et al 2016 reported that the fastest model at that time was mobilenetssd considering the balance of speed accuracy and memory use based on the target 2 3 dr using run time object detection little research has used both dr and deep learning to detect objects in real time by an approach similar to the one in this research nakajima et al 2017 developed a dr system that can automatically recognise the region to be diminished using an rgb d camera this system uses part of the simultaneous localisation and mapping framework for hidden background image generation however the system is difficult to use for large scale simulations because the applicable distance between the rgb d camera and the target object is limited kim et al 2018 developed a dr system that diminishes unwanted objects using object detection based on deep learning and an image inpainting technique this system cannot be implemented in real time because the image inpainting process is time consuming however object detection based on deep learning is effective for detecting rois in our research because object detection methodology has advanced rapidly and achieved real time performance 3 methodology 3 1 overview of the proposed system fig 1 shows an overview of the proposed dr system with a real time object detection technique using deep learning in pre processing 3d models both of the exterior structures that will be demolished and the background surroundings that will be visible afterward are modelled by automatic sfm modelling software using photographs and the mobilenetssd model is trained using a dataset to detect the moving objects to remove in real time processing rois are determined using the constructed 3d models and the regions detected by the object detection technique and the mask image is created by combining these the background surroundings are projected onto the mask image by image based rendering and the dr output is rendered excluding the exterior structures to be removed and the moving objects 3 2 background observation to diminish real objects from captured live video background information that is hidden from the viewer is needed in order to replace the object with the background information mori et al 2017 in the proposed dr system if photos of the background surroundings can be taken the background surroundings are reconstructed using sfm software through pre processing based on inoue et al 2016 the sfm technique can also estimate the camera position and orientation of each photograph used for textured 3d model recreation these positions and orientations are used for dr geometric registration mori et al 2015 inoue et al 2016 2018 3 3 roi detection in the proposed dr system the roi detection processes are divided mainly into two types roi detection of immobile exterior structures and roi detection of moving objects the roi of an exterior structure is determined using a 3d model of the structure modelled in the pre processing with sfm software used in mori et al 2015 inoue et al 2016 and inoue et al 2018 the rois of moving objects are determined by cnn based object detection using mobilenetssd in cnn based object detection vgg simonyan and zisserman 2014 and googlenet szegedy et al 2016 are widely used as cnn architectures because of their accurate object detection however they are difficult to use in mobile applications because the models involve heavy processing and need a high end computer for real time object detection mobilenets developed by howard et al 2017 are efficient cnn models for mobile vision applications this system architecture has a lower computational load than standard convolution ssd is an object detection model based on deep learning liu et al 2016 ssd predicts the class and position of an object only once at the end of inference using the feature map acquired from the cnn model this one stage detection is fast there are two types of ssd ssd300 and ssd512 the values 300 and 512 represent the input image size ssd512 can detect objects more accurately than ssd300 however in the proposed dr system ssd300 is employed because it allows for much faster object detection for object detection based on deep learning the deep learning model needs to be trained with datasets in the proposed dr system the pascal voc dataset everingham et al 2015 is used as the training dataset for general object detection the proposed dr system can detect buses cars motorbikes humans and trains because this system is used for outdoor landscape simulations the entire roi is finally determined by merging the rois of the exterior structure and the rois of moving objects 3 4 dr processing in order to achieve accurate dr the roi to be masked is first determined by superimposing the 3d virtual model of the object to be removed on the corresponding region of that object in the real world it is necessary to accurately project onto the roi the background model that appears after removing the 3d model therefore geometric registration between virtual space and real space is required in the proposed dr system registration uses the camera position and orientation that are estimated when the mask model is created by the sfm software as described in mori et al 2015 inoue et al 2016 and inoue et al 2018 in the virtual world the background model and the mask model are set at the correct position and the camera component is set using one of the photographs for reconstruction of the mask model thus dr needs to be started at the position where the photograph used for geometric registration was taken fig 2 after registration the camera position and orientation are computed relatively based on the initial camera position and orientation to virtually remove both existing structures and moving objects the roi is rendered as green r g b 0 255 0 for chroma keying and a mask image is created the model of the background surroundings is superimposed on the roi as an image and removal of the exterior structure and moving objects is realised 3 5 seamless blending when the model of the background surroundings is projected onto the mask image without any processing dr results in clear boundaries between the projected background on the roi and the live image to address this problem it has been reported that alpha blending processing provides a computationally low cost and sufficient solution cosco et al 2009 mori et al 2015 alpha blending is the process of combining pixels of a foreground image with pixels of a background image by weighted addition thereby producing new blended pixels if the weight value is represented by α the new blended pixel is calculated by eq 1 1 g 1 α f₁ diα f₂ where g is the new blended pixel and f₁ and f₂ are the pixels of the input images in this research the green value of the blurred mask image is defined as a weight value and the alpha blending process is implemented on the captured and background image the roi of the mask image then must be defined to be slightly larger than the original region since the edge of the target object can be transparent fig 3 therefore the 3d model of the removal target is created to be slightly larger than the original the roi of moving objects is also detected to be slightly larger than the original region in the proposed dr system 4 experiments and results this section describes two experiments the first is for accuracy verification and the second is for field validation the proposed dr system was developed by combining low cost commercially available products such as a laptop computer a webcam and the unity game engine table 1 lists the software and libraries which were used for system development and table 2 lists the specifications of the laptop computer and the webcam which were used for the experiments 4 1 accuracy verification the typical application scenario for the proposed dr system is outdoor landscape simulation during redevelopment in which the targets of moving object removal are assumed to be mainly vehicles and humans when the proposed dr system is used in an actual project the targets of moving object removal are expected to be far from the dr virtual camera and the detection accuracy is expected to be lower moving objects cannot be removed by dr if they are not detected therefore we investigated the correlation between the ratio of the target object region to the entire image and detection accuracy with mobilenetssd in this accuracy verification cars and humans were selected as the target objects and 10 different samples were prepared of each we painted elements other than the target object with a single colour using adobe photoshop software leaving only the target object in each image fig 4 the ratio of objects in the image was varied from 0 25 to 5 increasing by 0 25 because ratios of over 4 have already been investigated for of almost all of the objects in the pascal voc dataset lin et al 2014 and the detection accuracy of objects with a ratio of under 4 needs to be investigated finally we prepared a total of 200 images of cars and humans respectively the resolution of the images was 1024 576 pixels the inference time and the confidence score of each image were measured when input to mobilenetssd the inference time indicates how much time the model required for detection since the proposed dr system needs to perform processing in real time this value is considered important the confidence score indicates the model s judgement on the likelihood that the bounding box contains an object an object detection result with a high confidence score is highly reliable thus we employed this score as an index for accuracy verification however if the object was not detected the confidence score cannot be measured therefore we simultaneously calculated the detection rate defined in eq 2 2 d e t e c t i o n r a t e t p n u m b e r o f g r o u n d t r u t h s where tp true positive is the number of correct detections we then need to define correct detection in the proposed dr system the detection of small objects is thought to be necessary thus the confidence threshold was set to 20 this means that objects are regarded as not detected if the confidence score is under 20 the index of intersection over union iou was also used iou is defined in eq 3 and evaluates the overlap between the predicted bounding box and ground truth bounding box 3 i o u a r e a b p b g t a r e a b p b g t here b p is the predicted bounding box and b g t is the ground truth bounding box in this verification the iou threshold was set to 0 5 therefore if the result of object detection has both a confidence score of over 20 and an iou of over 0 5 the result is regarded as a tp figs 5 8 show the results of the accuracy verification overall stable results were obtained in the detection of cars the confidence score gradually decreased as the ratio of the car images decreased under 1 75 or increased over 4 75 fig 5 the detection rate gradually decreased as the ratio of the car images decreased under 1 5 fig 6 the results for the detection of humans were more varied than the car results the confidence score was not always high ranging from 0 30 to 0 80 fig 7 the detection rate was locally small at 60 between 2 25 and 3 fig 8 the inference time was about 12 13 frames per second fps 4 2 field validation field validation was conducted at two venues the live images captured from the webcam had a resolution of 1024 576 pixels a parking lot at the osaka university suita campus that had no exterior structures to be removed was selected as the first venue and the cars were virtually removed fig 9 a photograph of the existing state is shown in fig 10 a the mask image created by automatically detecting cars is shown in fig 10 b and the dr result is shown in fig 10 c the retouched red line in fig 10 d shows the boundary between the input image and the textured 3d model of the projected background surroundings created by sfm in pre processing the second venue was the site of a two storey welfare centre on poplar street on osaka university suita campus fig 11 the welfare centre was selected as the exterior structure for virtual removal and the information science and technology a and b buildings were located in the background soundings the pedestrian walking on the path in fig 11 was also virtually removed the pedestrian was visually identified on the webcam view 1 s after running the proposed dr system fig 12 shows the existing state images mask images created by automatically detecting the immobile structure and moving pedestrian and dr results along a timeline from 0 to 15 s respectively the retouched red line in the right column of fig 12 indicates the boundary between the input image and the textured 3d model of the projected background surroundings created by sfm in pre processing 5 discussion the field experiment results showed that the dr method proposed in this research is feasible and practical for detecting and removing moving objects such as cars and pedestrians in addition to immobile objects such as exterior structures for accurate landscape simulation during redevelopment however some problems were identified in the accuracy verification described in section 4 1 it was confirmed that cars could be stably detected but the detection of humans was unstable when investigating images in which humans account for about 3 or less of the images in the pascal voc dataset used for training there are more images containing only part of a human in frame compared with images containing entire humans in frame this is considered to be the cause of the decrease in detection accuracy when the ratio of humans in the image is about 3 or less it is necessary to increase the amount of training to improve accuracy in the field experiment the rendering speed was about 4 5 fps this is not sufficient for real time rendering in dr system improvements are necessary such as reducing the resolution using a computer with higher processing performance or implementing a faster algorithm to improve the dr frame rate when the system detects a moving object using mobilenetssd the roi is defined by a bounding box that roughly but sufficiently covers the appropriate region however when a landscape element that should remain is included in the bounding box it is virtually removed a more accurate method for determining the roi in dr is to specify the region along with the silhouette of a target semantic segmentation techniques can achieve this but the processing is more complicated and may affect dr real time rendering speed it is necessary to consider the system architecture while achieving both more accurate roi setting and real time rendering onsite dr simulations can be more flexibly performed on a tablet computer or smartphone than a laptop pc however loading a 3d city model performing accurate object detection and performing dr processing places a heavy load on these mobile devices therefore distributed computing via the internet is necessary to implement the proposed system on mobile devices but this has not been realised yet therefore we used mobilenetssd as a lightweight object detection model for standalone operation on a laptop pc to realise a system with higher flexibility and extensibility support for the internet is a future issue trees are an important subject in landscape design since the appearance and position of trees can change moment to moment depending on time and weather it is necessary to detect trees in real time to realise accurate dr however the pascal voc dataset used in this research does not include trees and we have not constructed a new tree dataset for learning for realising illumination consistency between the 3d model of the background surroundings and the live images captured from the webcam during dr simulation only edge blending at the boundary was used in this research however illumination consistency for all materials such as walls is important for the future development of dr simulation in connection with this issue fukuda et al 2017 realised illumination consistency between the virtual sky rendered by cg after a structure was visually removed and the sky of the live action image by deep learning during the dr simulation however illumination consistency was not applied to all materials in this experiment such as building walls and roads liu et al 2009 achieved illumination consistency between a 3d virtual model and live video in ar and mixed reality by performing ambient light estimation in the real space 6 limitations as shown in the section on accuracy verification small moving objects on the screen cannot be visually removed by dr fig 13 shows a dr frame where a pedestrian who was visually removed by dr appears as a small object the ratio of a person on the screen was 0 23 this is consistent with the results of accuracy verification in which the rate became low when the ratio of a person was 0 25 0 5 fig 8 the physical distance from the dr camera to the pedestrian was 27 m therefore under the conditions of this experiment pedestrians about 27 m or more away from the dr camera were not removed the existence of small objects that were not visually removed was not particularly noticeable in this experiment however it will be necessary in the future to evaluate how users respond to the dr images and such unremoved objects in user tests 7 conclusions landscape simulation is essential for building consensus about future landscapes in order to preserve a good landscape dr can virtually remove old structures and trees that are planned for demolition or removal while they still exist this paper presented the development of a dr system that performs real time removal both of immobile exterior structures and moving objects for onsite landscape simulations the contributions of this research are as follows the developed dr system can virtually remove moving landscape objects such as vehicles and pedestrians along with immobile objects such as structures rois of moving objects are determined using object detection based on deep learning and the rois for immobile objects are determined using pre defined geometric 3d models using sfm we evaluated the object detection performance according to the ratio of the main moving object in the image for landscape simulation the developed dr system was applied to large scale landscape simulation at two sites to validate the utility of the dr method for system validation it is necessary to perform further evaluations under more usage scenarios in the field of architectural and urban redevelopment declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was partly supported by jsps kakenhi grant numbers jp16k00707 and jp19k12681 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104759 
25997,landscape simulation is necessary for stakeholders to discuss future landscapes with new designs in order to preserve good landscapes augmented reality can be used to study the future landscape on a large scale by adding a three dimensional design model to the real world on the other hand diminished reality dr can simulate the virtual demolition and removal of structures in redevelopment however it has not been possible to visually remove moving landscape objects such as vehicles and pedestrians in real time for accurate landscape simulation this research develops a dr system that can virtually remove moving landscape objects by implementing real time object detection using deep learning with a game engine as well as immobile objects such as structures in addition to evaluating the performance of detecting the size of moving landscape objects the developed dr system is applied to large scale landscape simulation at two sites and its utility is validated graphical abstract image 1 keywords landscape simulation design support system diminished reality dr augmented reality ar real time object detection deep learning 1 introduction preserving good landscapes is crucial for enhancing our quality of life recently the need for landscape assessment has grown particularly for evaluating the impact of large scale projects on their surroundings the stakeholders in the landscape assessment process consist of experts such as developers planners architects and engineers and non experts such as neighbourhood residents and citizens it is difficult for stakeholders to understand and discuss planned landscape issues especially for three dimensional 3d spatial information that does not yet exist in the design process visualisation of the planned landscape can help stakeholders to understand the project blueprint and build consensus for developing and preserving good landscapes sheppard 1989 bishop and lange 2005 many studies have investigated the development and practical application of computer graphics cg and virtual reality vr as methods for landscape visualisation and simulation using computer technology including lange 1994 lee et al 2001 yan et al 2011 and dorta et al 2016 vr can help stakeholders to review the design targets intuitively and interactively from any viewpoint of interest but for large scale projects it requires a large amount of time and expense to build 3d models of surroundings that consist of the geography existing structures and natural objects augmented reality ar on the other hand allows stakeholders to review 3d design models superimposed onto the real world milgram and kishino 1994 azuma 1997 krevelen and poelman 2010 ar can simulate future landscapes and environments by overlaying textured 3d models of new buildings and structures on the present surroundings and reduce the time and expense required for 3d modelling of the present surroundings yabuki et al 2011 wang 2013 fukuda et al 2014 haynes et al 2018 beyond existing ar approaches that overlay 3d design models on the physical environment advanced ar visualisation is needed that can virtually remove existing structures in redevelopment projects if ar is simulated while old structures and trees that are planned for demolition and removal are still present the 3d virtual objects of the new design structure become intermingled with the existing structures leading to inaccurate visualisation to solve this issue diminished reality dr which is a technique for concealing eliminating and seeing through objects virtually in a perceived environment in real time has been attracting attention mann 1994 mann and fung 2001 mori et al 2017 nakajima et al 2017 dr has advanced to the point of application to various scale simulations including a small scale simulation for interior design sanni 2017 and a large scale simulation for landscape assessment inoue et al 2018 it is necessary to determine the regions where objects will be removed and to observe the background information behind the removal targets to implement dr processing particularly for accurate future landscape simulation a series of studies has developed a dr methodology that realises visual removal of exterior structures for landscape assessment inoue et al 2016 2018 in those methods 3d models of the removal targets are built by manual modelling e g computer aided design and building information modelling data or semi automatic 3d laser scanning and structure from motion sfm modelling software in pre processing these models are overlaid onto the targets for physical removal in the images in order to define the regions of interest rois the roi for each target is a mask that is then covered with a textured 3d model of the recovered hidden background however the previous methods are able to visually remove only objects that do not move between pre processing and run time processes such as exterior structures for example if a bridge is visually removed in a redevelopment project an inaccurate dr will be output in which vehicles and pedestrians are still travelling on the now removed structure of the immobile bridge even if 3d models of cars and humans are created in pre processing it is difficult to extract rois using them because they may have moved during the dr run time therefore it is necessary to visually remove not only immobile exterior structures but also moving objects such as cars and pedestrians in real time for accurate landscape simulation the objective of this research is to develop a novel dr system using real time object detection to perform visual removal of objects including both immobile exterior structures and moving objects for ar based landscape simulation in real time we plan to integrate the real time object detection technique into a dr system for the removal of moving objects in our proposed system rois for immobile exterior structures are determined using pre defined geometric 3d models and rois for moving objects are determined using object detection technology based on deep learning this research proposes a method for realising dr for onsite and large scale landscape simulation a number of methods have been introduced for dr on a small scale enomoto and saito 2007 kawai et al 2014 mori et al 2017 furthermore it is necessary to render the background after virtually removing objects in dr we apply a method for accurately displaying a 3d model of the background surroundings in order to realise highly accurate dr landscape simulation 2 literature review this research aims to develop a dr system that performs real time removal of both immobile exterior structures and moving objects in landscape simulations therefore we review previous studies on dr methodology and object detection techniques 2 1 dr mori et al 2017 reported a procedure for implementing dr that consisted of five methods background observation scene tracking roi detection hidden view generation and composition the present paper addresses the detection of rois including moving objects this section therefore describes previous dr research into background observation and roi detection 2 1 1 background observation many methods have been proposed in which background surroundings are observed using multiple cameras enomoto and saito 2007 zokai et al 2003 meerits and saito 2015 although these methods can acquire background information in real time they need to capture overlapping regions in order to calculate relationships among the captured images making them difficult to apply to large scale simulations image inpainting techniques have been also adopted for dr processing kawai et al 2014 kim et al 2018 this does not require pre processing but does require a few seconds for image processing meaning that real time processing cannot be implemented this also gives rise to the problem of how to create the correct background image for accurate landscape simulation on the other hand pre observation methods have been proposed inoue et al 2016 recreated a textured 3d model of the background surroundings using sfm these methods are time consuming but can be applied to large scale simulation 2 1 2 roi detection enomoto and saito 2007 proposed a dr method for removing obstacles using multiple images captured from different viewpoints this method does not estimate specific rois for removing target objects because the method uses ar tag markers to calibrate multiple cameras and images can be synthesised by sharing all images captured by all the cameras without removing the target objects however the marker needs to remain within the field of view of all cameras which is difficult for outdoor landscape simulation if the cameras and target objects are fixed in the environment the roi can be set manually zokai et al 2003 however the camera used for ar simulation and the moving target objects cannot be fixed when the geometric shapes of the target object are known the corresponding roi can be determined by overlaying the 3d model according to the perspective the 3d model can then be obtained by manual modelling kaneda et al 2004 tsuda et al 2006 inoue et al 2016 this method can determine the roi to be removed for an exterior structure but not for moving objects 2 2 object detection object detection techniques are divided mainly into two types methods using background subtraction and methods using feature sets in an image in this research the background subtraction methodology is not suitable because neither the camera nor the moving objects are fixed hence object detection using feature sets is employed various feature sets such as haar like features viola and jones 2001 histograms of oriented gradients dalal and triggs 2005 edge orientation histograms levi and weiss 2004 and edgelet features wu and nevatia 2005 have been proposed for detecting specific objects in the field of computer vision these feature sets can be used for automatically detecting specific objects but must be customised to match the subject object detection based on deep learning is a fast emerging field convolutional neural networks cnns are a kind of multi layer neural network designed to recognise visual patterns directly from pixel images and many cnn architectures such as alexnet krizhevsky et al 2012 vgg simonyan and zisserman 2014 googlenet szegedy et al 2016 resnet he et al 2016 mobilenets howard et al 2017 have been developed in addition to the development of cnns many cnn based object detection algorithms such as r cnn girshick et al 2014 you only look once yolo redmon et al 2016 and single shot multibox detector ssd liu et al 2016 have been proposed in these methods objects are detected in real time not as a silhouette but as a bounding box that surrounds the object when cnn based object detection is used to determine the roi the roi is defined as a rectangle instead of the silhouette of the object however it has been reported that the roi does not need to be the silhouette of the target object and can be a bounding box that covers the object roughly and sufficiently mori et al 2017 thus it is thought that object detection based on deep learning is effective for determining rois for real time processing in this research the speed of object detection is as important as its accuracy therefore the combination of mobilenet for the cnn architecture and ssd 300 for the object detection model mobilenetssd is employed as the moving object detection technique as huang et al 2016 reported that the fastest model at that time was mobilenetssd considering the balance of speed accuracy and memory use based on the target 2 3 dr using run time object detection little research has used both dr and deep learning to detect objects in real time by an approach similar to the one in this research nakajima et al 2017 developed a dr system that can automatically recognise the region to be diminished using an rgb d camera this system uses part of the simultaneous localisation and mapping framework for hidden background image generation however the system is difficult to use for large scale simulations because the applicable distance between the rgb d camera and the target object is limited kim et al 2018 developed a dr system that diminishes unwanted objects using object detection based on deep learning and an image inpainting technique this system cannot be implemented in real time because the image inpainting process is time consuming however object detection based on deep learning is effective for detecting rois in our research because object detection methodology has advanced rapidly and achieved real time performance 3 methodology 3 1 overview of the proposed system fig 1 shows an overview of the proposed dr system with a real time object detection technique using deep learning in pre processing 3d models both of the exterior structures that will be demolished and the background surroundings that will be visible afterward are modelled by automatic sfm modelling software using photographs and the mobilenetssd model is trained using a dataset to detect the moving objects to remove in real time processing rois are determined using the constructed 3d models and the regions detected by the object detection technique and the mask image is created by combining these the background surroundings are projected onto the mask image by image based rendering and the dr output is rendered excluding the exterior structures to be removed and the moving objects 3 2 background observation to diminish real objects from captured live video background information that is hidden from the viewer is needed in order to replace the object with the background information mori et al 2017 in the proposed dr system if photos of the background surroundings can be taken the background surroundings are reconstructed using sfm software through pre processing based on inoue et al 2016 the sfm technique can also estimate the camera position and orientation of each photograph used for textured 3d model recreation these positions and orientations are used for dr geometric registration mori et al 2015 inoue et al 2016 2018 3 3 roi detection in the proposed dr system the roi detection processes are divided mainly into two types roi detection of immobile exterior structures and roi detection of moving objects the roi of an exterior structure is determined using a 3d model of the structure modelled in the pre processing with sfm software used in mori et al 2015 inoue et al 2016 and inoue et al 2018 the rois of moving objects are determined by cnn based object detection using mobilenetssd in cnn based object detection vgg simonyan and zisserman 2014 and googlenet szegedy et al 2016 are widely used as cnn architectures because of their accurate object detection however they are difficult to use in mobile applications because the models involve heavy processing and need a high end computer for real time object detection mobilenets developed by howard et al 2017 are efficient cnn models for mobile vision applications this system architecture has a lower computational load than standard convolution ssd is an object detection model based on deep learning liu et al 2016 ssd predicts the class and position of an object only once at the end of inference using the feature map acquired from the cnn model this one stage detection is fast there are two types of ssd ssd300 and ssd512 the values 300 and 512 represent the input image size ssd512 can detect objects more accurately than ssd300 however in the proposed dr system ssd300 is employed because it allows for much faster object detection for object detection based on deep learning the deep learning model needs to be trained with datasets in the proposed dr system the pascal voc dataset everingham et al 2015 is used as the training dataset for general object detection the proposed dr system can detect buses cars motorbikes humans and trains because this system is used for outdoor landscape simulations the entire roi is finally determined by merging the rois of the exterior structure and the rois of moving objects 3 4 dr processing in order to achieve accurate dr the roi to be masked is first determined by superimposing the 3d virtual model of the object to be removed on the corresponding region of that object in the real world it is necessary to accurately project onto the roi the background model that appears after removing the 3d model therefore geometric registration between virtual space and real space is required in the proposed dr system registration uses the camera position and orientation that are estimated when the mask model is created by the sfm software as described in mori et al 2015 inoue et al 2016 and inoue et al 2018 in the virtual world the background model and the mask model are set at the correct position and the camera component is set using one of the photographs for reconstruction of the mask model thus dr needs to be started at the position where the photograph used for geometric registration was taken fig 2 after registration the camera position and orientation are computed relatively based on the initial camera position and orientation to virtually remove both existing structures and moving objects the roi is rendered as green r g b 0 255 0 for chroma keying and a mask image is created the model of the background surroundings is superimposed on the roi as an image and removal of the exterior structure and moving objects is realised 3 5 seamless blending when the model of the background surroundings is projected onto the mask image without any processing dr results in clear boundaries between the projected background on the roi and the live image to address this problem it has been reported that alpha blending processing provides a computationally low cost and sufficient solution cosco et al 2009 mori et al 2015 alpha blending is the process of combining pixels of a foreground image with pixels of a background image by weighted addition thereby producing new blended pixels if the weight value is represented by α the new blended pixel is calculated by eq 1 1 g 1 α f₁ diα f₂ where g is the new blended pixel and f₁ and f₂ are the pixels of the input images in this research the green value of the blurred mask image is defined as a weight value and the alpha blending process is implemented on the captured and background image the roi of the mask image then must be defined to be slightly larger than the original region since the edge of the target object can be transparent fig 3 therefore the 3d model of the removal target is created to be slightly larger than the original the roi of moving objects is also detected to be slightly larger than the original region in the proposed dr system 4 experiments and results this section describes two experiments the first is for accuracy verification and the second is for field validation the proposed dr system was developed by combining low cost commercially available products such as a laptop computer a webcam and the unity game engine table 1 lists the software and libraries which were used for system development and table 2 lists the specifications of the laptop computer and the webcam which were used for the experiments 4 1 accuracy verification the typical application scenario for the proposed dr system is outdoor landscape simulation during redevelopment in which the targets of moving object removal are assumed to be mainly vehicles and humans when the proposed dr system is used in an actual project the targets of moving object removal are expected to be far from the dr virtual camera and the detection accuracy is expected to be lower moving objects cannot be removed by dr if they are not detected therefore we investigated the correlation between the ratio of the target object region to the entire image and detection accuracy with mobilenetssd in this accuracy verification cars and humans were selected as the target objects and 10 different samples were prepared of each we painted elements other than the target object with a single colour using adobe photoshop software leaving only the target object in each image fig 4 the ratio of objects in the image was varied from 0 25 to 5 increasing by 0 25 because ratios of over 4 have already been investigated for of almost all of the objects in the pascal voc dataset lin et al 2014 and the detection accuracy of objects with a ratio of under 4 needs to be investigated finally we prepared a total of 200 images of cars and humans respectively the resolution of the images was 1024 576 pixels the inference time and the confidence score of each image were measured when input to mobilenetssd the inference time indicates how much time the model required for detection since the proposed dr system needs to perform processing in real time this value is considered important the confidence score indicates the model s judgement on the likelihood that the bounding box contains an object an object detection result with a high confidence score is highly reliable thus we employed this score as an index for accuracy verification however if the object was not detected the confidence score cannot be measured therefore we simultaneously calculated the detection rate defined in eq 2 2 d e t e c t i o n r a t e t p n u m b e r o f g r o u n d t r u t h s where tp true positive is the number of correct detections we then need to define correct detection in the proposed dr system the detection of small objects is thought to be necessary thus the confidence threshold was set to 20 this means that objects are regarded as not detected if the confidence score is under 20 the index of intersection over union iou was also used iou is defined in eq 3 and evaluates the overlap between the predicted bounding box and ground truth bounding box 3 i o u a r e a b p b g t a r e a b p b g t here b p is the predicted bounding box and b g t is the ground truth bounding box in this verification the iou threshold was set to 0 5 therefore if the result of object detection has both a confidence score of over 20 and an iou of over 0 5 the result is regarded as a tp figs 5 8 show the results of the accuracy verification overall stable results were obtained in the detection of cars the confidence score gradually decreased as the ratio of the car images decreased under 1 75 or increased over 4 75 fig 5 the detection rate gradually decreased as the ratio of the car images decreased under 1 5 fig 6 the results for the detection of humans were more varied than the car results the confidence score was not always high ranging from 0 30 to 0 80 fig 7 the detection rate was locally small at 60 between 2 25 and 3 fig 8 the inference time was about 12 13 frames per second fps 4 2 field validation field validation was conducted at two venues the live images captured from the webcam had a resolution of 1024 576 pixels a parking lot at the osaka university suita campus that had no exterior structures to be removed was selected as the first venue and the cars were virtually removed fig 9 a photograph of the existing state is shown in fig 10 a the mask image created by automatically detecting cars is shown in fig 10 b and the dr result is shown in fig 10 c the retouched red line in fig 10 d shows the boundary between the input image and the textured 3d model of the projected background surroundings created by sfm in pre processing the second venue was the site of a two storey welfare centre on poplar street on osaka university suita campus fig 11 the welfare centre was selected as the exterior structure for virtual removal and the information science and technology a and b buildings were located in the background soundings the pedestrian walking on the path in fig 11 was also virtually removed the pedestrian was visually identified on the webcam view 1 s after running the proposed dr system fig 12 shows the existing state images mask images created by automatically detecting the immobile structure and moving pedestrian and dr results along a timeline from 0 to 15 s respectively the retouched red line in the right column of fig 12 indicates the boundary between the input image and the textured 3d model of the projected background surroundings created by sfm in pre processing 5 discussion the field experiment results showed that the dr method proposed in this research is feasible and practical for detecting and removing moving objects such as cars and pedestrians in addition to immobile objects such as exterior structures for accurate landscape simulation during redevelopment however some problems were identified in the accuracy verification described in section 4 1 it was confirmed that cars could be stably detected but the detection of humans was unstable when investigating images in which humans account for about 3 or less of the images in the pascal voc dataset used for training there are more images containing only part of a human in frame compared with images containing entire humans in frame this is considered to be the cause of the decrease in detection accuracy when the ratio of humans in the image is about 3 or less it is necessary to increase the amount of training to improve accuracy in the field experiment the rendering speed was about 4 5 fps this is not sufficient for real time rendering in dr system improvements are necessary such as reducing the resolution using a computer with higher processing performance or implementing a faster algorithm to improve the dr frame rate when the system detects a moving object using mobilenetssd the roi is defined by a bounding box that roughly but sufficiently covers the appropriate region however when a landscape element that should remain is included in the bounding box it is virtually removed a more accurate method for determining the roi in dr is to specify the region along with the silhouette of a target semantic segmentation techniques can achieve this but the processing is more complicated and may affect dr real time rendering speed it is necessary to consider the system architecture while achieving both more accurate roi setting and real time rendering onsite dr simulations can be more flexibly performed on a tablet computer or smartphone than a laptop pc however loading a 3d city model performing accurate object detection and performing dr processing places a heavy load on these mobile devices therefore distributed computing via the internet is necessary to implement the proposed system on mobile devices but this has not been realised yet therefore we used mobilenetssd as a lightweight object detection model for standalone operation on a laptop pc to realise a system with higher flexibility and extensibility support for the internet is a future issue trees are an important subject in landscape design since the appearance and position of trees can change moment to moment depending on time and weather it is necessary to detect trees in real time to realise accurate dr however the pascal voc dataset used in this research does not include trees and we have not constructed a new tree dataset for learning for realising illumination consistency between the 3d model of the background surroundings and the live images captured from the webcam during dr simulation only edge blending at the boundary was used in this research however illumination consistency for all materials such as walls is important for the future development of dr simulation in connection with this issue fukuda et al 2017 realised illumination consistency between the virtual sky rendered by cg after a structure was visually removed and the sky of the live action image by deep learning during the dr simulation however illumination consistency was not applied to all materials in this experiment such as building walls and roads liu et al 2009 achieved illumination consistency between a 3d virtual model and live video in ar and mixed reality by performing ambient light estimation in the real space 6 limitations as shown in the section on accuracy verification small moving objects on the screen cannot be visually removed by dr fig 13 shows a dr frame where a pedestrian who was visually removed by dr appears as a small object the ratio of a person on the screen was 0 23 this is consistent with the results of accuracy verification in which the rate became low when the ratio of a person was 0 25 0 5 fig 8 the physical distance from the dr camera to the pedestrian was 27 m therefore under the conditions of this experiment pedestrians about 27 m or more away from the dr camera were not removed the existence of small objects that were not visually removed was not particularly noticeable in this experiment however it will be necessary in the future to evaluate how users respond to the dr images and such unremoved objects in user tests 7 conclusions landscape simulation is essential for building consensus about future landscapes in order to preserve a good landscape dr can virtually remove old structures and trees that are planned for demolition or removal while they still exist this paper presented the development of a dr system that performs real time removal both of immobile exterior structures and moving objects for onsite landscape simulations the contributions of this research are as follows the developed dr system can virtually remove moving landscape objects such as vehicles and pedestrians along with immobile objects such as structures rois of moving objects are determined using object detection based on deep learning and the rois for immobile objects are determined using pre defined geometric 3d models using sfm we evaluated the object detection performance according to the ratio of the main moving object in the image for landscape simulation the developed dr system was applied to large scale landscape simulation at two sites to validate the utility of the dr method for system validation it is necessary to perform further evaluations under more usage scenarios in the field of architectural and urban redevelopment declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was partly supported by jsps kakenhi grant numbers jp16k00707 and jp19k12681 appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104759 
25998,modeling the diameter distribution of trees in forest stands is a frequent application in ecology that supports key biologically and economically relevant management decisions the choice of model used to represent the diameter distribution and how to estimate its parameters has received much attention in the forestry literature however accessible software that facilitates comprehensive comparison of the myriad modeling approaches is not available to this end we developed an r package called forestfit that simplifies estimation of common probability distributions used to model tree diameter distributions including the two and three parameter weibull distributions johnson s sb distribution birnbaum saunders distribution and finite mixture distributions frequentist and bayesian techniques are provided for individual tree diameter data as well as grouped data additional functionality facilitates fitting curves to height diameter data the package also provides a set of functions for computing probability distributions and simulating random realizations from common finite mixture models keywords bayesian forest modeling grouped data parameter estimation mixture models r package statistical distributions 1 introduction the diameter distribution of trees in a stand is a relatively simple measure used to inform forest management decisions that have both biological and economical relevance bailey and dell 1973 determining models and associated parameter estimation techniques to characterize an entire stand s diameter distribution based upon a sample of trees has been an important topic in forest biometrics over the last 50 years green et al 1994 merganič and sterba 2006 using an appropriate model to describe the diameter distribution enables an evaluation of the stand growth as well as potential estimates of stand volume numerous probability distributions have been proposed and used to model the diameter distribution such as the two parameter weibull distribution three parameter weibull distribution bailey and dell 1973 johnson s sb jsb distribution fonseca et al 2009 and finite mixture distributions zasada and cieszewski 2005 forest biometricians have developed a suite of methods for estimating the parameters of these models including simple methods such as the method of moments and method of percentiles wang and keats 1995 maximum likelihood techniques gove and fairweather 1989 and bayesian methods green et al 1994 another important tree and stand characterization used by foresters is the diameter to height relationship because diameter is an inexpensive field measurement relative to acquiring tree height the forest biometrics literature is rich with models to predict height based on diameter temesgen et al 2014 due to the extensive suite of techniques in the forest biometrics literature to model diameter distributions and height diameter relationships there is a need for freely available software that facilitates streamlined model parameter estimation and comparison to this end we develop an open source r package called forestfit that provides a limited set of functions to fit a large variety of popular diameter distributions and height diameter models forestfit is open source software released under gpl2 gpl3 license and available on the comprehensive r archive network cran at https cran r project org web packages forestfit index html importantly the functions are designed to simplify comparison among models and parameter estimation techniques using a variety of model fit criteria without requiring an in depth knowledge of the statistical techniques used in the estimation process forestfit will have immediate use in forest science as well as other ecological and environmental fields where modeling size distributions is an important task e g botany fisheries and hydrology in this paper we describe the functionality of the forestfit package and illustrate its features using the analysis of mixed ponderosa pine pinus ponderosa and western juniper juniperus occidentalis forest plots located in the blue mountains near burns oregon usa kerns et al 2017 2 forestfit components in this section we provide an overview of the features available in the forestfit package along with relevant citations that provide a more rigorous statistical treatment 2 1 estimating parameters of the weibull distribution the weibull distribution is perhaps the most conspicuous distribution used for modeling diameter distributions due to its ability to represent common shapes in diameter distribution data bailey and dell 1973 gorgoso et al 2007 gorgoso varela and rojo alboreca 2014 merganič and sterba 2006 pretzsch 2009 stankova and zlatanov 2010 forestfit offers several methods for estimating the parameters of two and three parameter weibull distributions which are listed in table 1 along with relevant sources for further details regarding each estimation method 2 2 bayesian analysis computational improvements over the last 30 years have led to the increase in popularity of bayesian techniques for estimating model parameters for diameter distributions green et al 1994 in forestfit we provide functionality for estimating the parameters of the three parameter weibull and four parameter jsb distributions using the bayesian paradigm the jsb distribution is becoming increasingly popular for modeling diameter distributions in forestry because of its flexibility to model a wide variety of forest types fonseca et al 2009 george and ramachandran 2011 zhang et al 2003 and is commonly applied in other fields e g hydrology cugerone and de michele 2015 and climatology lu et al 2008 with the exception of the location parameter forestfit uses the same algorithm as green et al 1994 for estimating the shape and scale parameters of the three parameter weibull distribution 2 3 modeling diameter distributions using grouped data data collection protocols in forestry often lead to diameter observations that are recorded in groups or classes of diameters e g 5 m between 5 m and 10 m etc rather than as individual tree measurements more specifically suppose a data set has been classified into m separate groups with lower bounds r 0 r 1 r m where r 0 min x 1 x n and r m max x 1 x n with x i being the individual diameter for tree i out of a total n trees in the sample we define f i for i 1 m as the frequency or number of trees in the i th group a schematic of grouped data is provided in table 2 forestfit provides estimation of diameter distribution parameters for the three parameter birnbaum saunders bs distribution the three parameter generalized exponential ge distribution and the three parameter weibull distribution using grouped data through the use of the vectors r r 0 r 1 r m and f f 1 f 2 f m 2 4 modeling diameter distributions using finite mixture models certain forest stands e g uneven aged or biotically disturbed stands exhibit multimodal diameter distributions that are not adequately represented by a single probability distribution in such settings forest biometricians and statisticians often use finite mixture distributions to characterize complex and multimodal diameter distributions zhang et al 2001 liu et al 2002 zasada and cieszewski 2005 zhang and liu 2006 liu et al 2014 the probability density function pdf of a k component mixture model has the form 1 g x θ k 1 k ω k f x θ k where θ ω θ 1 θ k in which θ k α k β k is the parameter vector of the k th component with pdf f θ k and ω ω 1 ω k is the vector of weight parameters the weight parameters ω k s are non negative and sum to one i e k 1 k ω k 1 forestfit enables f θ k to take the following forms for individual i e ungrouped data burr xii chen fisher fréchet gamma gompertz log logistic log normal lomax skew normal and two parameter weibull pdfs are given in appendix for grouped data the finite mixture distribution can be composed of gamma log normal skew normal and two parameter weibull distributions application of finite mixture models are increasing in popularity in environmental and ecological analyses see e g hobbs and hooten 2015 and jaworski and podlaski 2012 to facilitate testing and simulation using the common mixture models implemented in forestfit the package includes functions to compute the probability density function cumulative density function quantile function and simulate random variables from the finite mixture models composed of the aforementioned probability distributions furthermore forestfit provides specific functions to compute the probability density function cumulative density function quantile function and simulate random variables from the gamma shape mixture model that has received increased attention in forestry for characterizing diameter distributions venturini et al 2008 2 5 estimating the parameters of height diameter models in addition to modeling diameter distributions characterizing the height diameter relationship is an important task for obtaining accurate growth and yield measurements to inform forest management temesgen et al 2007 a variety of mathematical relationships have been proposed to model this relationship the most common of which are implemented in forestfit and listed with associated references in table 3 3 worked examples here we illustrate key features available in forestfit all models and estimation methods assume the data points e g diameter of an individual tree are independent and identically distributed for illustration we use a set of 0 08 ha plots comprising mixed ponderosa pine and western juniper located in the malheur national forest on the southern end of the blue mountains near burns oregon usa kerns et al 2017 these data are freely available https www fs usda gov rds archive catalog rds 2017 0041 under the condition that users cite the reference kerns et al 2017 further we include these data in forestfit so the data can easily be obtained using the command data dbh for illustrative purposes we only use the diameter and height measurements at 1 3 m height of all live trees in plots 72 57 55 and 51 and refer to these values as d72 d57 d55 and d51 respectively the heights of trees in plot 55 are referred to as h55 below we provide the commands for loading the dbh data set and obtaining the data from plot 55 3 1 estimating parameters of the weibull distribution the function fitweibull estimates parameters of the two and three parameter weibull distribution and takes the form where data is a vector of data observations location indicates whether to use a three parameter weibull distribution true or a two parameter weibull distribution false method denotes the estimation method to use and starts is a vector of starting values for the parameters to be estimated available estimation methods and their code in forestfit are shown in table 4 the code below calls fitweibull to fit the three parameter weibull distribution using maximum product spacing mps to the plot 72 data with the function output printed below the first element of the output list estimate contains the point estimates for the model parameters the second element of the list measures contains a series of goodness of fit measures including akaike information criterion aic akaike 1998 consistent akaike information criterion caic hurvich and tsai 1991 bayesian information criterion bic schwarz 1978 hannan quinn information criterion hqic hannan and quinn 1979 anderson darling ad anderson and darling 1952 cramér von misses cvm cramér 1928 kolmogorov smirnov ks smirnov 1948 and log likelihood log likelihood these measures allow for comparison between different models and estimation techniques to determine the best fitting model according to the desired criteria larger values of the log likelihood indicate a better model fit while smaller values of all other model comparison criteria indicate a better model fit 3 2 bayesian analysis bayesian inference for diameter distributions modeled using the jsb and weibull distributions is provided via the fitbayesjsb and fitbayesweibull functions respectively the fitbayesjsb function has the form where data is a vector of observations n burn is an integer representing the number of burn in iterations i e the number of iterations the gibbs sampler takes to reach convergence and n simul is the total number of gibbs sampler iterations by default fitbayesjsb uses 10 000 total iterations and 8000 burn in iterations fitbayesweibull has the same arguments below we call both functions using the diameters of all live trees in plot 72 we see that for plot 72 all goodness of fit measures indicate the jsb model is a better fit for the data fig 1 additional examples using the fitbayesjsb and fitbayesweibull functions with alternative data sets can be found in the manual pages of forestfit 3 3 modeling diameter distributions using grouped data suppose diameter observations are given in m separate groups as shown in table 2 forestfit provides the function fitgrouped for estimating the parameters of the three parameter bs and weibull distributions using grouped data this function takes the form where r is a length m 1 vector of group boundaries as shown in table 2 with the first element of r being the lower bound of the first group and all other m values representing the upper bound of the m groups f is a vector of length m of the group frequencies family represents the distribution used in the model taking values weibull or birnbaum saunders method1 is a string denoting the method of estimation here we enable three methods of estimation approximated maximum likelihood aml the expectation maximization em dempster et al 1977 algorithm em and maximum likelihood ml starts is a numeric vector containing the starting values for the shape scale and location parameters respectively lastly method2 indicates the optimization method of the log likelihood taking values bfgs cg l bfgs b nelder mean and sann if using the em algorithm the stats and method2 arguments do not need to be specified below we manually group the diameter observations from plot 57 into six groups and use the function fitgrouped on this grouped data set fig 2 displays the estimated bs pdf to plot 57 s diameter distribution data 3 4 modeling diameter distributions of grouped data using finite mixture models the fitmixturegrouped function provides an interface for fitting a wide range of finite mixture distributions to grouped data and takes the form where r and f are vectors of the class boundaries and frequencies as previously described in section 3 3 k is a single numeric value denoting the number of component pdfs to include in the finite mixture model all parameters in finite mixture models are estimated using the em algorithm the family argument is a character string denoting the distribution used in the mixture model currently available distributions are the gamma gamma log normal log normal skew normal skew normal and weibull weibull families the initial argument denotes whether or not the user wants to specify starting values for the em algorithm by default the argument is set to false and thus the user does not need to specify a value for the starts argument if initial true the user must specify a sequence of initial values taking the form starts ω α β where the three vectors are of length k with elements ω i being the weight of the i th component and α i and β i are the associated parameters of the i th component if family skew normal the user must specify starting values for the weights and three parameters for each component distribution see appendix in the code that follows we illustrate the fitmixturegrouped function by fitting a two component skew normal mixture model to plot 51 s grouped data similar to previous functions using models with a single pdf the output is a list where the first element i e estimate contains the estimated parameters of the first and second components of the model and the second element i e measures contains the goodness of fit measures fig 3 displays the estimated two component skew normal pdf 3 5 modeling diameter distributions using finite mixture models fitted to ungrouped data as a companion to the grouped data fitmixturegrouped function described in section 3 4 the fitmixture function fits finite mixture models to individual diameter distribution data and takes the form where data is a vector of individual diameter measurements and family denotes the family of probability distributions to use available families of pdfs are the bs birnbaum saunders burr xi burrxii chen chen fisher f fréchet frechet gamma gamma generalized exponential ge gompertz gompertz log normal log normal log logistic log logistic lomax lomax skew normal skew normal and weibull weibull families the remaining arguments are analogous to those used in the fitmixturegrouped function below the fitmixture function is called to fit a two component log normal mixture model to plot 51 s diameterdata interpretation of the first and second components of the output list is analagous to the interpretation provided in section 3 4 the third list element cluster specifies the component of the mixture distribution from which each individual diameter measurement arises fig 4 displays the fitted two component log normal pdf to the diameter distribution data of plot 51 additional examples using the fitmixture function with alternative data sets can be found in the manual pages of forestfit 3 6 distribution functions for finite mixture models analogous with r s standard functions for working with common probability distributions we provide commands for computing the density distribution function quantile function and random generation from the finite mixture models used in forestfit the functions take the following forms where x is a vector of observations n is the number of realizations to be simulated from the mixture model p is a numeric value that satisfies 0 p 1 family is one of the families introduced in section 2 4 k is the number of components and param is the mixture model parameter vector that takes the same form as the starts argument of the fitmixturegrouped function described previously in section 3 4 as an illustration the code below generates 500 realizations from a three component bs mixture model and displays the resulting distribution in fig 5 3 7 the gamma shape mixture gsm model the gamma shape mixture model gsm introduced by venturini et al 2008 has received considerable attention in forestry for characterizing diameter distributions because of its popularity we have implemented specific functions in forestfit for estimating the parameters of the gsm distribution computing the density function computing the distribution function and generating realizations for the gsm distribution the function calls for the aforementioned tasks are where data is a vector of observations n is the number of realizations simulated from the gsm model k is the number of components omega is a vector of mixing parameters beta is the rate parameter log indicates whether to compute the density function log false or the log density function log true log p indicates whether to compute the distribution function log p false or the log transformed distribution function log p true and lower tail indicates whether to compute the upper tail lower tail false or lower tail of the distribution lower tail true as an illustration we generate 500 realizations from a ten component gsm model and display the resulting distribution in fig 6 the output list of the fitgsm function consists of three parts 1 beta estimated rate parameter 2 omega estimated vector of mixing parameters 3 measures sequence of goodness of fit measures includingaic caic hqic ad cvm ks and log likelihood 3 8 modeling height diameter relationships the general function for fitting curves to paired observations of height and diameter is where h and d are vectors of height and diameter observations respectively the model argument denotes the model to fit with options chapman richards chapman richards gompertz gompertz hossfeld iv hossfeldiv korf korf logistic logistic prodan prodan ratkowsky ratkowsky sibbesen sibbesen and weibull weibull here we fit a weibull model to the height diameter relationship of plot 55 the output list consists of five parts 1 summary estimated parameters standard error of the estimators t value and corresponding p value 2 residuals residuals of the fitted curve 3 var cov variance covariance matrix of the model coefficient estimators 4 residual std error residual standard error 5 a scatterplot of the height diameter relationship with the fitted values fig 7 4 future development we are currently working on software functionality that includes 1 a broader set of goodness of fit measures for the bayesian models 2 a function that automates optimal model method selection that searches among all methods in table 4 with selection based on a set of user identified goodness of fit measures 3 expanded set of mixture models e g venturini et al 2008 4 addition of random effects that accommodate grouped distribution and height diameter data e g stands species functional types etc 5 conclusion a common task in forestry is modeling the diameter distribution of trees in a given forest stand in this work we have developed and introduced an r package called forestfit that provides numerous models and estimation techniques for modeling diameter distributions from both individual tree data and grouped data as well as providing additional functionality for simulating finite mixture distributions and fitting curves to height diameter data to help facilitate comparison of the different model estimation methods provided by forestfit we include multiple goodness of fit measures that the user can use to determine the best model technique for their purposes while this package was developed in the context of forestry the models we fit and simulate have numerous applications throughout other ecological and environmental fields such as botany fisheries and hydrology for example the jsb distribution is used in hydrology to model rain drop size distribution cugerone and de michele 2015 and finite mixture distributions are broadly applied in environmental and ecological statistics for numerous examples see hobbs and hooten 2015 suggesting forestfit can have wide use across ecological and environmental fields forestfit is open source software released under gpl2 gpl3 license on the cran declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix probability density functions used in f o r e s t f i t bs f x θ x μ β β x μ 2 α x μ ϕ x μ β β x μ α burr xii f x θ α β x α 1 1 x α β 1 chen f x θ α β x α exp x α β exp x α β fisher f x θ γ α β 2 γ α 2 γ β 2 α β α 2 x α 2 1 1 α β x α β 2 fréchet f x θ α β x β α 1 exp x β α gamma f x θ x α 1 β α γ α exp x β ge f x θ β exp β x μ 1 exp β x μ α 1 gompertz f x θ β exp α x exp β exp α x 1 α johnson s sb f x θ δ λ exp 1 2 γ δ log x ξ λ ξ x 2 2 π x ξ λ ξ x log logistic f x θ α β α x α 1 x β α 1 2 log normal f x θ exp 1 2 log x α β 2 2 π β x lomax f x θ α β 1 α x β 1 skew normal f x θ 2 ϕ x α β φ λ x α β weibull f x θ α β x μ β α 1 exp x μ β α where θ is the family parameter vector ϕ and φ denote the density function and distribution function of the standard normal distribution respectively noticeably the three parameter bs gamma ge and weibull distributions simplify to the two parameter bs gamma ge and weibull distributions respectively when μ 0 
25998,modeling the diameter distribution of trees in forest stands is a frequent application in ecology that supports key biologically and economically relevant management decisions the choice of model used to represent the diameter distribution and how to estimate its parameters has received much attention in the forestry literature however accessible software that facilitates comprehensive comparison of the myriad modeling approaches is not available to this end we developed an r package called forestfit that simplifies estimation of common probability distributions used to model tree diameter distributions including the two and three parameter weibull distributions johnson s sb distribution birnbaum saunders distribution and finite mixture distributions frequentist and bayesian techniques are provided for individual tree diameter data as well as grouped data additional functionality facilitates fitting curves to height diameter data the package also provides a set of functions for computing probability distributions and simulating random realizations from common finite mixture models keywords bayesian forest modeling grouped data parameter estimation mixture models r package statistical distributions 1 introduction the diameter distribution of trees in a stand is a relatively simple measure used to inform forest management decisions that have both biological and economical relevance bailey and dell 1973 determining models and associated parameter estimation techniques to characterize an entire stand s diameter distribution based upon a sample of trees has been an important topic in forest biometrics over the last 50 years green et al 1994 merganič and sterba 2006 using an appropriate model to describe the diameter distribution enables an evaluation of the stand growth as well as potential estimates of stand volume numerous probability distributions have been proposed and used to model the diameter distribution such as the two parameter weibull distribution three parameter weibull distribution bailey and dell 1973 johnson s sb jsb distribution fonseca et al 2009 and finite mixture distributions zasada and cieszewski 2005 forest biometricians have developed a suite of methods for estimating the parameters of these models including simple methods such as the method of moments and method of percentiles wang and keats 1995 maximum likelihood techniques gove and fairweather 1989 and bayesian methods green et al 1994 another important tree and stand characterization used by foresters is the diameter to height relationship because diameter is an inexpensive field measurement relative to acquiring tree height the forest biometrics literature is rich with models to predict height based on diameter temesgen et al 2014 due to the extensive suite of techniques in the forest biometrics literature to model diameter distributions and height diameter relationships there is a need for freely available software that facilitates streamlined model parameter estimation and comparison to this end we develop an open source r package called forestfit that provides a limited set of functions to fit a large variety of popular diameter distributions and height diameter models forestfit is open source software released under gpl2 gpl3 license and available on the comprehensive r archive network cran at https cran r project org web packages forestfit index html importantly the functions are designed to simplify comparison among models and parameter estimation techniques using a variety of model fit criteria without requiring an in depth knowledge of the statistical techniques used in the estimation process forestfit will have immediate use in forest science as well as other ecological and environmental fields where modeling size distributions is an important task e g botany fisheries and hydrology in this paper we describe the functionality of the forestfit package and illustrate its features using the analysis of mixed ponderosa pine pinus ponderosa and western juniper juniperus occidentalis forest plots located in the blue mountains near burns oregon usa kerns et al 2017 2 forestfit components in this section we provide an overview of the features available in the forestfit package along with relevant citations that provide a more rigorous statistical treatment 2 1 estimating parameters of the weibull distribution the weibull distribution is perhaps the most conspicuous distribution used for modeling diameter distributions due to its ability to represent common shapes in diameter distribution data bailey and dell 1973 gorgoso et al 2007 gorgoso varela and rojo alboreca 2014 merganič and sterba 2006 pretzsch 2009 stankova and zlatanov 2010 forestfit offers several methods for estimating the parameters of two and three parameter weibull distributions which are listed in table 1 along with relevant sources for further details regarding each estimation method 2 2 bayesian analysis computational improvements over the last 30 years have led to the increase in popularity of bayesian techniques for estimating model parameters for diameter distributions green et al 1994 in forestfit we provide functionality for estimating the parameters of the three parameter weibull and four parameter jsb distributions using the bayesian paradigm the jsb distribution is becoming increasingly popular for modeling diameter distributions in forestry because of its flexibility to model a wide variety of forest types fonseca et al 2009 george and ramachandran 2011 zhang et al 2003 and is commonly applied in other fields e g hydrology cugerone and de michele 2015 and climatology lu et al 2008 with the exception of the location parameter forestfit uses the same algorithm as green et al 1994 for estimating the shape and scale parameters of the three parameter weibull distribution 2 3 modeling diameter distributions using grouped data data collection protocols in forestry often lead to diameter observations that are recorded in groups or classes of diameters e g 5 m between 5 m and 10 m etc rather than as individual tree measurements more specifically suppose a data set has been classified into m separate groups with lower bounds r 0 r 1 r m where r 0 min x 1 x n and r m max x 1 x n with x i being the individual diameter for tree i out of a total n trees in the sample we define f i for i 1 m as the frequency or number of trees in the i th group a schematic of grouped data is provided in table 2 forestfit provides estimation of diameter distribution parameters for the three parameter birnbaum saunders bs distribution the three parameter generalized exponential ge distribution and the three parameter weibull distribution using grouped data through the use of the vectors r r 0 r 1 r m and f f 1 f 2 f m 2 4 modeling diameter distributions using finite mixture models certain forest stands e g uneven aged or biotically disturbed stands exhibit multimodal diameter distributions that are not adequately represented by a single probability distribution in such settings forest biometricians and statisticians often use finite mixture distributions to characterize complex and multimodal diameter distributions zhang et al 2001 liu et al 2002 zasada and cieszewski 2005 zhang and liu 2006 liu et al 2014 the probability density function pdf of a k component mixture model has the form 1 g x θ k 1 k ω k f x θ k where θ ω θ 1 θ k in which θ k α k β k is the parameter vector of the k th component with pdf f θ k and ω ω 1 ω k is the vector of weight parameters the weight parameters ω k s are non negative and sum to one i e k 1 k ω k 1 forestfit enables f θ k to take the following forms for individual i e ungrouped data burr xii chen fisher fréchet gamma gompertz log logistic log normal lomax skew normal and two parameter weibull pdfs are given in appendix for grouped data the finite mixture distribution can be composed of gamma log normal skew normal and two parameter weibull distributions application of finite mixture models are increasing in popularity in environmental and ecological analyses see e g hobbs and hooten 2015 and jaworski and podlaski 2012 to facilitate testing and simulation using the common mixture models implemented in forestfit the package includes functions to compute the probability density function cumulative density function quantile function and simulate random variables from the finite mixture models composed of the aforementioned probability distributions furthermore forestfit provides specific functions to compute the probability density function cumulative density function quantile function and simulate random variables from the gamma shape mixture model that has received increased attention in forestry for characterizing diameter distributions venturini et al 2008 2 5 estimating the parameters of height diameter models in addition to modeling diameter distributions characterizing the height diameter relationship is an important task for obtaining accurate growth and yield measurements to inform forest management temesgen et al 2007 a variety of mathematical relationships have been proposed to model this relationship the most common of which are implemented in forestfit and listed with associated references in table 3 3 worked examples here we illustrate key features available in forestfit all models and estimation methods assume the data points e g diameter of an individual tree are independent and identically distributed for illustration we use a set of 0 08 ha plots comprising mixed ponderosa pine and western juniper located in the malheur national forest on the southern end of the blue mountains near burns oregon usa kerns et al 2017 these data are freely available https www fs usda gov rds archive catalog rds 2017 0041 under the condition that users cite the reference kerns et al 2017 further we include these data in forestfit so the data can easily be obtained using the command data dbh for illustrative purposes we only use the diameter and height measurements at 1 3 m height of all live trees in plots 72 57 55 and 51 and refer to these values as d72 d57 d55 and d51 respectively the heights of trees in plot 55 are referred to as h55 below we provide the commands for loading the dbh data set and obtaining the data from plot 55 3 1 estimating parameters of the weibull distribution the function fitweibull estimates parameters of the two and three parameter weibull distribution and takes the form where data is a vector of data observations location indicates whether to use a three parameter weibull distribution true or a two parameter weibull distribution false method denotes the estimation method to use and starts is a vector of starting values for the parameters to be estimated available estimation methods and their code in forestfit are shown in table 4 the code below calls fitweibull to fit the three parameter weibull distribution using maximum product spacing mps to the plot 72 data with the function output printed below the first element of the output list estimate contains the point estimates for the model parameters the second element of the list measures contains a series of goodness of fit measures including akaike information criterion aic akaike 1998 consistent akaike information criterion caic hurvich and tsai 1991 bayesian information criterion bic schwarz 1978 hannan quinn information criterion hqic hannan and quinn 1979 anderson darling ad anderson and darling 1952 cramér von misses cvm cramér 1928 kolmogorov smirnov ks smirnov 1948 and log likelihood log likelihood these measures allow for comparison between different models and estimation techniques to determine the best fitting model according to the desired criteria larger values of the log likelihood indicate a better model fit while smaller values of all other model comparison criteria indicate a better model fit 3 2 bayesian analysis bayesian inference for diameter distributions modeled using the jsb and weibull distributions is provided via the fitbayesjsb and fitbayesweibull functions respectively the fitbayesjsb function has the form where data is a vector of observations n burn is an integer representing the number of burn in iterations i e the number of iterations the gibbs sampler takes to reach convergence and n simul is the total number of gibbs sampler iterations by default fitbayesjsb uses 10 000 total iterations and 8000 burn in iterations fitbayesweibull has the same arguments below we call both functions using the diameters of all live trees in plot 72 we see that for plot 72 all goodness of fit measures indicate the jsb model is a better fit for the data fig 1 additional examples using the fitbayesjsb and fitbayesweibull functions with alternative data sets can be found in the manual pages of forestfit 3 3 modeling diameter distributions using grouped data suppose diameter observations are given in m separate groups as shown in table 2 forestfit provides the function fitgrouped for estimating the parameters of the three parameter bs and weibull distributions using grouped data this function takes the form where r is a length m 1 vector of group boundaries as shown in table 2 with the first element of r being the lower bound of the first group and all other m values representing the upper bound of the m groups f is a vector of length m of the group frequencies family represents the distribution used in the model taking values weibull or birnbaum saunders method1 is a string denoting the method of estimation here we enable three methods of estimation approximated maximum likelihood aml the expectation maximization em dempster et al 1977 algorithm em and maximum likelihood ml starts is a numeric vector containing the starting values for the shape scale and location parameters respectively lastly method2 indicates the optimization method of the log likelihood taking values bfgs cg l bfgs b nelder mean and sann if using the em algorithm the stats and method2 arguments do not need to be specified below we manually group the diameter observations from plot 57 into six groups and use the function fitgrouped on this grouped data set fig 2 displays the estimated bs pdf to plot 57 s diameter distribution data 3 4 modeling diameter distributions of grouped data using finite mixture models the fitmixturegrouped function provides an interface for fitting a wide range of finite mixture distributions to grouped data and takes the form where r and f are vectors of the class boundaries and frequencies as previously described in section 3 3 k is a single numeric value denoting the number of component pdfs to include in the finite mixture model all parameters in finite mixture models are estimated using the em algorithm the family argument is a character string denoting the distribution used in the mixture model currently available distributions are the gamma gamma log normal log normal skew normal skew normal and weibull weibull families the initial argument denotes whether or not the user wants to specify starting values for the em algorithm by default the argument is set to false and thus the user does not need to specify a value for the starts argument if initial true the user must specify a sequence of initial values taking the form starts ω α β where the three vectors are of length k with elements ω i being the weight of the i th component and α i and β i are the associated parameters of the i th component if family skew normal the user must specify starting values for the weights and three parameters for each component distribution see appendix in the code that follows we illustrate the fitmixturegrouped function by fitting a two component skew normal mixture model to plot 51 s grouped data similar to previous functions using models with a single pdf the output is a list where the first element i e estimate contains the estimated parameters of the first and second components of the model and the second element i e measures contains the goodness of fit measures fig 3 displays the estimated two component skew normal pdf 3 5 modeling diameter distributions using finite mixture models fitted to ungrouped data as a companion to the grouped data fitmixturegrouped function described in section 3 4 the fitmixture function fits finite mixture models to individual diameter distribution data and takes the form where data is a vector of individual diameter measurements and family denotes the family of probability distributions to use available families of pdfs are the bs birnbaum saunders burr xi burrxii chen chen fisher f fréchet frechet gamma gamma generalized exponential ge gompertz gompertz log normal log normal log logistic log logistic lomax lomax skew normal skew normal and weibull weibull families the remaining arguments are analogous to those used in the fitmixturegrouped function below the fitmixture function is called to fit a two component log normal mixture model to plot 51 s diameterdata interpretation of the first and second components of the output list is analagous to the interpretation provided in section 3 4 the third list element cluster specifies the component of the mixture distribution from which each individual diameter measurement arises fig 4 displays the fitted two component log normal pdf to the diameter distribution data of plot 51 additional examples using the fitmixture function with alternative data sets can be found in the manual pages of forestfit 3 6 distribution functions for finite mixture models analogous with r s standard functions for working with common probability distributions we provide commands for computing the density distribution function quantile function and random generation from the finite mixture models used in forestfit the functions take the following forms where x is a vector of observations n is the number of realizations to be simulated from the mixture model p is a numeric value that satisfies 0 p 1 family is one of the families introduced in section 2 4 k is the number of components and param is the mixture model parameter vector that takes the same form as the starts argument of the fitmixturegrouped function described previously in section 3 4 as an illustration the code below generates 500 realizations from a three component bs mixture model and displays the resulting distribution in fig 5 3 7 the gamma shape mixture gsm model the gamma shape mixture model gsm introduced by venturini et al 2008 has received considerable attention in forestry for characterizing diameter distributions because of its popularity we have implemented specific functions in forestfit for estimating the parameters of the gsm distribution computing the density function computing the distribution function and generating realizations for the gsm distribution the function calls for the aforementioned tasks are where data is a vector of observations n is the number of realizations simulated from the gsm model k is the number of components omega is a vector of mixing parameters beta is the rate parameter log indicates whether to compute the density function log false or the log density function log true log p indicates whether to compute the distribution function log p false or the log transformed distribution function log p true and lower tail indicates whether to compute the upper tail lower tail false or lower tail of the distribution lower tail true as an illustration we generate 500 realizations from a ten component gsm model and display the resulting distribution in fig 6 the output list of the fitgsm function consists of three parts 1 beta estimated rate parameter 2 omega estimated vector of mixing parameters 3 measures sequence of goodness of fit measures includingaic caic hqic ad cvm ks and log likelihood 3 8 modeling height diameter relationships the general function for fitting curves to paired observations of height and diameter is where h and d are vectors of height and diameter observations respectively the model argument denotes the model to fit with options chapman richards chapman richards gompertz gompertz hossfeld iv hossfeldiv korf korf logistic logistic prodan prodan ratkowsky ratkowsky sibbesen sibbesen and weibull weibull here we fit a weibull model to the height diameter relationship of plot 55 the output list consists of five parts 1 summary estimated parameters standard error of the estimators t value and corresponding p value 2 residuals residuals of the fitted curve 3 var cov variance covariance matrix of the model coefficient estimators 4 residual std error residual standard error 5 a scatterplot of the height diameter relationship with the fitted values fig 7 4 future development we are currently working on software functionality that includes 1 a broader set of goodness of fit measures for the bayesian models 2 a function that automates optimal model method selection that searches among all methods in table 4 with selection based on a set of user identified goodness of fit measures 3 expanded set of mixture models e g venturini et al 2008 4 addition of random effects that accommodate grouped distribution and height diameter data e g stands species functional types etc 5 conclusion a common task in forestry is modeling the diameter distribution of trees in a given forest stand in this work we have developed and introduced an r package called forestfit that provides numerous models and estimation techniques for modeling diameter distributions from both individual tree data and grouped data as well as providing additional functionality for simulating finite mixture distributions and fitting curves to height diameter data to help facilitate comparison of the different model estimation methods provided by forestfit we include multiple goodness of fit measures that the user can use to determine the best model technique for their purposes while this package was developed in the context of forestry the models we fit and simulate have numerous applications throughout other ecological and environmental fields such as botany fisheries and hydrology for example the jsb distribution is used in hydrology to model rain drop size distribution cugerone and de michele 2015 and finite mixture distributions are broadly applied in environmental and ecological statistics for numerous examples see hobbs and hooten 2015 suggesting forestfit can have wide use across ecological and environmental fields forestfit is open source software released under gpl2 gpl3 license on the cran declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper appendix probability density functions used in f o r e s t f i t bs f x θ x μ β β x μ 2 α x μ ϕ x μ β β x μ α burr xii f x θ α β x α 1 1 x α β 1 chen f x θ α β x α exp x α β exp x α β fisher f x θ γ α β 2 γ α 2 γ β 2 α β α 2 x α 2 1 1 α β x α β 2 fréchet f x θ α β x β α 1 exp x β α gamma f x θ x α 1 β α γ α exp x β ge f x θ β exp β x μ 1 exp β x μ α 1 gompertz f x θ β exp α x exp β exp α x 1 α johnson s sb f x θ δ λ exp 1 2 γ δ log x ξ λ ξ x 2 2 π x ξ λ ξ x log logistic f x θ α β α x α 1 x β α 1 2 log normal f x θ exp 1 2 log x α β 2 2 π β x lomax f x θ α β 1 α x β 1 skew normal f x θ 2 ϕ x α β φ λ x α β weibull f x θ α β x μ β α 1 exp x μ β α where θ is the family parameter vector ϕ and φ denote the density function and distribution function of the standard normal distribution respectively noticeably the three parameter bs gamma ge and weibull distributions simplify to the two parameter bs gamma ge and weibull distributions respectively when μ 0 
25999,this study aimed to develop an ecological habitat suitability model ehsm for zacco platypus by integrating hydraulic hhs and physiologic phs habitat suitability study sites 117 stations were selected throughout the five major watersheds in south korea where hydraulic water temperature and fish survey data 2008 2015 were available the hhs was determined using preference curves for water depth and velocity while the phs was determined by growth and stress curves the geometric mean of hhs and phs was the most appropriate to calculate the ecological habitat suitability ehs when compared with fish abundance the relation of ehs was lower with phs r 0 750 than with hhs r 0 956 likely due to the thermal tolerance of z platypus ehsm represented the effect of water temperature on fish growth and stress better than a conventional model suggesting a promising tool to estimate the habitat suitability of freshwater fish keywords freshwater fish habitat suitability pale chub streamflow water temperature 1 introduction flow and thermal regimes are the most fundamental components of freshwater ecosystems olden and naiman 2010 thus changes in these regimes will eventually shift the distribution of freshwater fish hooper et al 2013 habitat suitability models are widely used to assess the impact of dams or weirs on fish habitats choi and choi 2018 im et al 2011 yao et al 2015 yi et al 2014 2016a or evaluate restoration projects of streams and rivers yao et al 2017 these models are mostly based on the physical habitat simulation system phabsim to assess the physical habitat e g depth velocity and substrate suitability for fish bovee et al 1998 for instance im et al 2011 reported that the overall habitat suitability of the freshwater pale chub zacco platypus would increase after the removal of a weir because of enhanced hydraulic conditions papadaki et al 2016 predicted that climate change would limit the suitable habitats of the west balkan trout salmo farioides due to the altered flow regimes recently as climate change and thermopeaking received increasing interest the modeling of the suitability of water temperature also drew attention choi and choi 2018 muñoz mas et al 2016 2018 zhang et al 2019 yi et al 2010 predicted the habitat suitability of the chinese sturgeon acipenser sinensis at different flow conditions released from a dam and evaluated the thermal suitability by preference curves zhang et al 2019 evaluated the thermal preference of spawning and juvenile freshwater fish coreius guichenoti under climate change scenarios accompanied by hydropower operations muñoz mas et al 2018 also used the thermal preference of spawning fish to predict climate change effects to the brown trout salmo trutta domestically choi and choi 2018 developed preference curves to evaluate the impact of dam operations on z platypus in terms of flow and temperature in particular several studies reported the synergistic effects of altered flow and temperature on fish arismendi et al 2013 brook et al 2008 which highlights the importance of modeling physiological responses however these habitat suitability models consider water temperature like a physical variable using preference curves regardless of its key role in the physiology of freshwater fish climex climatic index has been widely used to evaluate the physiologic response such as growth and stress against climatic variables e g air temperature and humidity kriticos et al 2015 sutherst 1998 sutherst and maywald 1985 sutherst and maywald 2005 in this model the ecoclimatic index which is the product of growth and stress is used to estimate the climatic suitability for sustaining a population kriticos et al 2015 sutherst and maywald 1985 climex has been mostly used to predict the distribution of insects or plants he et al 2012 demonstrated that cold heat and dry stress were the limiting factors in the distribution of the light brown apple moth epiphyas postvittana paterson et al 2015 also predicted that climate change would negatively affect oil palms elaeis guineensis due to increased cold heat and dry stress however to the best of our knowledge the work by koehn 2004 is the only case in which climex was used to predict the habitat suitability of an invasive freshwater fish cyprinus carpio despite insufficient studies it is believed that the physiologic habitat models such as climex can be applied to simulate the thermal habitats of fish therefore this study aimed to develop an ecological habitat suitability model ehsm that integrates the hydraulic water depth and velocity and physiologic water temperature suitabilities zacco platypus was selected as the model species due to its widespread distribution in the five major watersheds han nakdong geum seomjin and yeongsan river in addition z platypus is the sentinel species most frequently used in habitat suitability modeling in south korea im et al 2011 2 materials and methods 2 1 study site and data collection the study area was represented by the five major river watersheds of south korea namely the han 34 428 10 km2 nakdong 23 690 32 km2 geum 9914 02 km2 seomjin 4914 32 km2 and yeongsan 3469 58 km2 river watersheds fig s1 basin and river maps were downloaded from wamis water resources management information system www wamis go kr the study sites were selected according to the following criteria first the site must have hydraulic water level or flow rate water quality water temperature and fish survey stations second all stations must have been operated at least three years within the study period 2008 2015 finally the hydraulic characteristics including flow rate water level depth and cross section area must have been determined at least 30 times in the study site as a result 117 stations satisfied the conditions with 32 27 29 14 15 sites for the han nakdong geum seomjin and yeongsan river watersheds respectively current climate and environmental data were collected from 2008 to 2016 daily air temperature average maximum and minimum was collected from kma korea meteorological administration https data kma go kr the observation results of water level and flow rate along with station coordinates were collected from wamis http www wamis go kr hydraulic survey data of flow rate water level mean velocity mean depth cross section area and stream width were collected from the korea annual hydrological report which is provided by the han river flood control office http www hrfco go kr water temperature and fish survey data 2008 2016 conducted by the nationwide aquatic ecological monitoring program were collected from weis water environment information system http water nier go kr 2 2 hydraulic and thermal modeling regression models were used to calculate hydraulic variables depth and velocity and water temperature average maximum and minimum the coefficient of determination r2 was calculated to evaluate the model which can be considered as either good 0 75 r2 0 85 or very good r2 0 85 moriasi et al 2015 daily water depth and velocity were derived from flow rate q using a series of empirical equations first flow rate was used to derive water level r2 0 902 0 427 or cross section area r2 0 876 0 170 by eq 1 1 q a l b c where a b and c are constants and l is the water level or cross section area depth was calculated using the linear relationship with water level r2 0 799 0 167 and velocity was calculated by dividing flow rate by the cross section area the average daily water temperature r2 0 928 0 022 was simulated using the data collected from the water quality stations by eq 2 according to park and clough 2009 2 wi tmean 1 0 trange 2 sin 0 0174533 0 987 d j p 30 where the time scale was transferred to julian date dj where 2008 01 01 is the starting date dj 1 and 2015 12 31 as the final date dj 2922 wi is the average daily water temperature at julian date i tmean is the average annual temperature in c trange is the annual temperature difference between the maximum and minimum daily temperatures and p is the time lag in deciding the hottest date daily maximum r2 0 819 0 050 and minimum r2 0 824 0 032 water temperatures were derived using the linear relationship between water temperature and air temperature by eq 3 3 wt a a t b where wt is the maximum or minimum water temperature at is the maximum or minimum air temperature and a and b are constants regression models of water depth water velocity and average water temperature were validated by comparing with observation data in 2016 validation sites were randomly selected in han 3 sites nakdong 2 sites geum 2 sites seomjin 1 site and yeongsan 1 site river watersheds the nash sutcliffe efficiency nse was applied for validation moriasi et al 2015 eq 4 4 nse 1 i 1 n o i p i 2 i 1 n o i o 2 where oi is the ith observation record pi is the ith prediction result and o is the mean of observation data nse ranges from to 1 where 1 is the optimal value 2 3 development of ehsm the ehsm was developed by integrating hydraulic habitat suitability hhs and physiologic habitat suitability phs fig 1 water depth and velocity were selected as the hydraulic parameters to evaluate the hhs the hhs curves for water depth and velocity were taken from the work of kang 2010 which defines the central 50 75 90 and 95 of the data as a suitability of 1 0 5 0 1 and 0 05 respectively gosse 1982 ifasg 1986 fig 2 kang 2010 used monitoring data of the han and geum river watersheds to derive the suitability curves which is to the best of our knowledge the most extensive data set used in south korea water temperature average minimum and maximum was used to evaluate the phs the phs curves for thermal growth and stress were developed using the fish monitoring data of z platypus from 2008 to 2016 fig 3 according to the ifasg method ifasg 1986 the lower g1 and upper g2 optimum temperature of growth 18 6 c and 24 0 c respectively was defined as the central 50 of the fish monitoring data and the lower g0 and upper g3 threshold temperature of growth 13 6 c and 27 0 c respectively was defined as the central 90 of the data the threshold for cold stress c0 was set at 6 0 c by rounding off the lowest temperature of appearance recorded 6 4 c kang 2010 the threshold for heat stress h0 was set at 30 c which is the average value of the reported thermal tolerance chung et al 2011 estimated that z platypus is capable of tolerating up to 29 c while kang et al 2013 estimated the thermal tolerance to be 31 c the rates of cold and heat stress were estimated using the relationship between the relative fish abundance and water temperature fig s2 the lower and upper 25 range of fish abundance data 2008 2016 were used to calculate the rate of cold and heat stress respectively although cold and heat stress can increase infinitely kriticos et al 2015 the minimum water temperature is usually limited to the freezing point toffolon and piccolroaz 2015 wanders et al 2019 in this study cold stress was set to reach its maximum at 4 0 c because this was the minimum water temperature observed in the national monitoring data from 1997 to 2005 nier 2006 the ehs was calculated using the arithmetic mean eq 5 geometric mean eq 6 and product method eq 7 muñoz mas et al 2012 the hhs and phs were not weighted by assuming that they are equally important ahmadi nedushan et al 2006 5 ehs hhs phs 2 6 ehs hhs phs 7 ehs hhs phs the geometric mean eq 6 assumes that good habitat conditions can compensate for unfavorable habitat conditions with interaction among variables fukuda et al 2011 the arithmetic mean eq 5 can consider only the compensation without interaction while the product method eq 7 can consider the interaction without compensation fukuda et al 2011 the annual hhs was calculated as follows papadaki et al 2016 8 hhs i 1 365 si d i si v i 365 where sid i and siv i are suitability indexes at day i for water depth fig 2a and velocity fig 2b respectively the annual phs was derived by multiplying the annual growth and stress kriticos et al 2015 9 phs gi si where gi and si are annual growth and stress indexes respectively annual gi was derived by calculating the arithmetic mean of the daily growth gi of the whole year eq 10 10 gi i 1 365 g i 365 where the gi value was derived from the growth curve in fig 3a with daily average water temperature considering that cold and heat stress are accumulated across the whole year kriticos et al 2015 the annual cold or heat stress indexes csi and hsi were derived by calculating the average of weighted monthly cold or heat stress si of the whole year eq 11 11 csi or hsi i 1 12 s i w i 12 the si value was weighted on a monthly basis wi where the stress of january was weighted as 1 and gradually increased with december being weighted as 12 the monthly cold or heat stress were the arithmetic mean of daily cold or heat stress the daily cold and heat stress were derived from the stress curve in fig 3b with daily minimum and maximum water temperature respectively finally the annual stress index was calculated by eq 12 12 si 1 min csi 1 1 min hsi 1 the ehsm was validated by comparing calculated ehs values at the representative 117 sites in south korea table s1 with z platypus monitoring data 2008 2015 the average cpue catch per unit effort of the monitoring period was used by assuming equivalent sampling conditions or effort among all sites after weighting the cpue by dividing it by the stream width cpuew the relationship between ehs and the natural log scale cpuew was analyzed with a non linear equation van der wal et al 2009 eq 13 13 ln cpuew a 1 exp b ehs c the performance of ehsm was evaluated by comparing with a conventional model that calculates composite habitat suitability chs as the annual average of geometric mean of hydraulic and thermal suitability indexes eq 14 14 chs i 1 365 si d i xsi v i si t i 3 365 where sid i siv i and sit i are suitability indexes at day i for water depth velocity and temperature respectively the suitability curves for water depth and velocity were the same as those used in the ehsm fig 2 and the suitability curve for water temperature was taken from the work of choi and choi 2018 fig s3 moreover lowess locally weighted scatterplot smoothing analysis provided in matlab s curve fitting tool version r2019a the mathworks inc natick ma was applied to visualize the relationship of suitability indexes of ehsm and chsm with fish abundance ln cpuew cleveland and devlin 1988 for the chsm hhs and ths thermal habitat suitability values were calculated by eqs 8 and 15 respectively 15 ths i 1 365 si t i 365 2 4 statistical analysis microsoft excel 2016 microsoft corporation redmond wa usa and sigmaplot version 12 systat software inc san jose ca usa were used for regression analyses spss version 24 ibm corp armonk ny usa was used for partial correlation analyses all statistical results were determined by a significance level of p 0 05 3 results and discussion 3 1 model performance the overall nse values of regression models for water depth water velocity and average water temperature were 0 673 0 278 satisfactory 0 50 nse 0 70 0 747 0 176 good 0 70 nse 0 80 and 0 849 0 249 very good nse 0 80 respectively moriasi et al 2015 these results suggest that the regression models applied in this study are reliable to simulate water depth water velocity and average water temperature all three methods used in ehs calculation successfully produced a non linear relationship between ehs and ln cpuew with an adjusted r2 of 0 4835 0 5181 fig 4 among them the geometric mean of hhs and phs was selected in this study because its adjusted r2 0 5181 was higher than the other methods 0 4840 and 0 4835 for product and arithmetic mean methods respectively considering that compensation among habitat variables is likely to occur for fish the geometric mean is reasonable to calculate ehs fukuda et al 2011 moreover the accuracy of the ehs to predict the presence of z platypus ehs 0 was 91 5 these findings suggest that the ehsm developed in this study is valid for predicting the habitat suitability of z platypus the performance of ehsm was evaluated by comparing with the composite habitat suitability model chsm the non linear relationship eq 13 between chs and ln cpuew was slightly lower than that of ehs with an adjusted r2 of 0 5076 fig s4 moreover lowess analysis suggests that both hhs and phs contributed to the increase in fish abundance for ehsm fig s5 however the contribution of ths to the fish abundance was much less than that of hhs in the case of chsm overall these findings support that ehsm is a promising tool for assessing habitat suitability of freshwater fish 3 2 habitat suitability assessment of z platypus the habitat suitability of the pale chub z platypus in south korea is shown in fig s6 the ehs was classified into four groups by combining the hhs and phs criteria the hhs and phs values were classified according to krieger and diana 2017 and taylor and kumar 2013 respectively table 1 fig 5 presents the cumulative percentage of each habitat group in the studied sites based on the classification of ehs 34 2 of the sites were highly suitable while 31 6 of the sites were either poor or unsuitable additionally the proportion of highly suitable sites of ehs was quite similar to that of hhs 38 5 while a much higher number of sites yielded highly suitable based on phs 49 6 indeed partial correlation analysis confirmed that ehs was more related to hhs r 0 956 than to phs r 0 750 the higher physiologic suitability possibly occurred due to the relatively high thermal tolerance of z platypus chung et al 2011 kang et al 2013 in this study the values of hhs seem to be controlled by the variation in depth fig s7 partial correlation analysis confirmed that hhs was more related to suitability of depth r 0 980 than that of velocity r 0 456 the higher variations in depth than in velocity would be related to the stream order lee et al 2014 showed that stream order and depth have a significantly positive correlation p 0 01 in the namhan river while velocity or water current does not show a significant correlation p 0 05 moges and bhole 2015 also reported that depth has a relatively higher correlation r2 0 646 with stream order than velocity r2 0 194 in the goro river 3 3 implications and limitations of ehsm habitat suitability indexes are commonly derived from the combination of independent preference curves kang 2010 yi et al 2017 like the geometric means of hhs and phs used to calculate ehs in this study however layher and maughan 1985 demonstrated that an unweighted geometric mean of suitability indexes of spotted bass showed little correlation to observed biomass in line with this a fuzzy logic approach can be applied to weighting habitat suitability index noack et al 2013 fukuda 2009 demonstrated that the fuzzy logic approach decreased fluctuations in habitat suitability and reduced prediction errors moreover yi et al 2016b reported that fuzzy logic models yielded higher accuracy than unweighted preference curve models these findings suggest that the fuzzy logic approach can be used to improve ehs evaluation by weighted integrating hhs and phs laboratory experiments or field observations are recommended to simulate growth and stress of species kriticos et al 2015 sutherst and maywald 1985 for instance he et al 2012 developed growth and stress curves of e postvittana using experimental data on development population growth rate and generation time sutherst and maywald 2005 also used field observations of colony growth of the red imported fire ant solenopsis invicta in order to develop growth curves in the case of fish elliott and hurley 1997 estimated the growth rate of atlantic salmon parr depending on temperature 5 0 20 c through laboratory experiments in this study growth and stress curves were developed based on field observation data of z platypus however physiologic data derived from laboratory experiments of z platypus are rare and require more research to refine growth and stress curves physiological suitability is expected to play a key role in determining the habitat suitability of cold water fish because these species are generally sensitive to water temperature stitt et al 2014 given that climate change may enhance the variation of water temperature in streams and rivers isaak et al 2012 webb and nobilis 2007 ehsm can be a promising model to predict the impact of climate change on cold water fish in addition individual suitability indexes water depth and velocity growth and cold and heat stress can provide information on the causes of climate change effects on freshwater fish distributions 4 conclusions ehsm was developed by integrating hydraulic and physiologic habitat suitability and successfully used to estimate the habitat suitability of z platypus on a national scale modeling results have shown that hhs especially the suitability regarding depth played key roles in determining the ehs of z platypus in addition ehsm represents the habitat suitability of freshwater fish better than a conventional model that has no consideration of fish physiology although more studies are required to refine growth and stress curves ehsm can be useful to predict climate change impacts on freshwater fish especially cold water species and this should be further investigated declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 5 acknowledgments this study was supported by the korea ministry of environment moe climate change correspondence program grant number 201400130007 we would also like to thank dr byungwoong choi and two anonymous reviewers for valuable comments on our manuscript appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104760 
25999,this study aimed to develop an ecological habitat suitability model ehsm for zacco platypus by integrating hydraulic hhs and physiologic phs habitat suitability study sites 117 stations were selected throughout the five major watersheds in south korea where hydraulic water temperature and fish survey data 2008 2015 were available the hhs was determined using preference curves for water depth and velocity while the phs was determined by growth and stress curves the geometric mean of hhs and phs was the most appropriate to calculate the ecological habitat suitability ehs when compared with fish abundance the relation of ehs was lower with phs r 0 750 than with hhs r 0 956 likely due to the thermal tolerance of z platypus ehsm represented the effect of water temperature on fish growth and stress better than a conventional model suggesting a promising tool to estimate the habitat suitability of freshwater fish keywords freshwater fish habitat suitability pale chub streamflow water temperature 1 introduction flow and thermal regimes are the most fundamental components of freshwater ecosystems olden and naiman 2010 thus changes in these regimes will eventually shift the distribution of freshwater fish hooper et al 2013 habitat suitability models are widely used to assess the impact of dams or weirs on fish habitats choi and choi 2018 im et al 2011 yao et al 2015 yi et al 2014 2016a or evaluate restoration projects of streams and rivers yao et al 2017 these models are mostly based on the physical habitat simulation system phabsim to assess the physical habitat e g depth velocity and substrate suitability for fish bovee et al 1998 for instance im et al 2011 reported that the overall habitat suitability of the freshwater pale chub zacco platypus would increase after the removal of a weir because of enhanced hydraulic conditions papadaki et al 2016 predicted that climate change would limit the suitable habitats of the west balkan trout salmo farioides due to the altered flow regimes recently as climate change and thermopeaking received increasing interest the modeling of the suitability of water temperature also drew attention choi and choi 2018 muñoz mas et al 2016 2018 zhang et al 2019 yi et al 2010 predicted the habitat suitability of the chinese sturgeon acipenser sinensis at different flow conditions released from a dam and evaluated the thermal suitability by preference curves zhang et al 2019 evaluated the thermal preference of spawning and juvenile freshwater fish coreius guichenoti under climate change scenarios accompanied by hydropower operations muñoz mas et al 2018 also used the thermal preference of spawning fish to predict climate change effects to the brown trout salmo trutta domestically choi and choi 2018 developed preference curves to evaluate the impact of dam operations on z platypus in terms of flow and temperature in particular several studies reported the synergistic effects of altered flow and temperature on fish arismendi et al 2013 brook et al 2008 which highlights the importance of modeling physiological responses however these habitat suitability models consider water temperature like a physical variable using preference curves regardless of its key role in the physiology of freshwater fish climex climatic index has been widely used to evaluate the physiologic response such as growth and stress against climatic variables e g air temperature and humidity kriticos et al 2015 sutherst 1998 sutherst and maywald 1985 sutherst and maywald 2005 in this model the ecoclimatic index which is the product of growth and stress is used to estimate the climatic suitability for sustaining a population kriticos et al 2015 sutherst and maywald 1985 climex has been mostly used to predict the distribution of insects or plants he et al 2012 demonstrated that cold heat and dry stress were the limiting factors in the distribution of the light brown apple moth epiphyas postvittana paterson et al 2015 also predicted that climate change would negatively affect oil palms elaeis guineensis due to increased cold heat and dry stress however to the best of our knowledge the work by koehn 2004 is the only case in which climex was used to predict the habitat suitability of an invasive freshwater fish cyprinus carpio despite insufficient studies it is believed that the physiologic habitat models such as climex can be applied to simulate the thermal habitats of fish therefore this study aimed to develop an ecological habitat suitability model ehsm that integrates the hydraulic water depth and velocity and physiologic water temperature suitabilities zacco platypus was selected as the model species due to its widespread distribution in the five major watersheds han nakdong geum seomjin and yeongsan river in addition z platypus is the sentinel species most frequently used in habitat suitability modeling in south korea im et al 2011 2 materials and methods 2 1 study site and data collection the study area was represented by the five major river watersheds of south korea namely the han 34 428 10 km2 nakdong 23 690 32 km2 geum 9914 02 km2 seomjin 4914 32 km2 and yeongsan 3469 58 km2 river watersheds fig s1 basin and river maps were downloaded from wamis water resources management information system www wamis go kr the study sites were selected according to the following criteria first the site must have hydraulic water level or flow rate water quality water temperature and fish survey stations second all stations must have been operated at least three years within the study period 2008 2015 finally the hydraulic characteristics including flow rate water level depth and cross section area must have been determined at least 30 times in the study site as a result 117 stations satisfied the conditions with 32 27 29 14 15 sites for the han nakdong geum seomjin and yeongsan river watersheds respectively current climate and environmental data were collected from 2008 to 2016 daily air temperature average maximum and minimum was collected from kma korea meteorological administration https data kma go kr the observation results of water level and flow rate along with station coordinates were collected from wamis http www wamis go kr hydraulic survey data of flow rate water level mean velocity mean depth cross section area and stream width were collected from the korea annual hydrological report which is provided by the han river flood control office http www hrfco go kr water temperature and fish survey data 2008 2016 conducted by the nationwide aquatic ecological monitoring program were collected from weis water environment information system http water nier go kr 2 2 hydraulic and thermal modeling regression models were used to calculate hydraulic variables depth and velocity and water temperature average maximum and minimum the coefficient of determination r2 was calculated to evaluate the model which can be considered as either good 0 75 r2 0 85 or very good r2 0 85 moriasi et al 2015 daily water depth and velocity were derived from flow rate q using a series of empirical equations first flow rate was used to derive water level r2 0 902 0 427 or cross section area r2 0 876 0 170 by eq 1 1 q a l b c where a b and c are constants and l is the water level or cross section area depth was calculated using the linear relationship with water level r2 0 799 0 167 and velocity was calculated by dividing flow rate by the cross section area the average daily water temperature r2 0 928 0 022 was simulated using the data collected from the water quality stations by eq 2 according to park and clough 2009 2 wi tmean 1 0 trange 2 sin 0 0174533 0 987 d j p 30 where the time scale was transferred to julian date dj where 2008 01 01 is the starting date dj 1 and 2015 12 31 as the final date dj 2922 wi is the average daily water temperature at julian date i tmean is the average annual temperature in c trange is the annual temperature difference between the maximum and minimum daily temperatures and p is the time lag in deciding the hottest date daily maximum r2 0 819 0 050 and minimum r2 0 824 0 032 water temperatures were derived using the linear relationship between water temperature and air temperature by eq 3 3 wt a a t b where wt is the maximum or minimum water temperature at is the maximum or minimum air temperature and a and b are constants regression models of water depth water velocity and average water temperature were validated by comparing with observation data in 2016 validation sites were randomly selected in han 3 sites nakdong 2 sites geum 2 sites seomjin 1 site and yeongsan 1 site river watersheds the nash sutcliffe efficiency nse was applied for validation moriasi et al 2015 eq 4 4 nse 1 i 1 n o i p i 2 i 1 n o i o 2 where oi is the ith observation record pi is the ith prediction result and o is the mean of observation data nse ranges from to 1 where 1 is the optimal value 2 3 development of ehsm the ehsm was developed by integrating hydraulic habitat suitability hhs and physiologic habitat suitability phs fig 1 water depth and velocity were selected as the hydraulic parameters to evaluate the hhs the hhs curves for water depth and velocity were taken from the work of kang 2010 which defines the central 50 75 90 and 95 of the data as a suitability of 1 0 5 0 1 and 0 05 respectively gosse 1982 ifasg 1986 fig 2 kang 2010 used monitoring data of the han and geum river watersheds to derive the suitability curves which is to the best of our knowledge the most extensive data set used in south korea water temperature average minimum and maximum was used to evaluate the phs the phs curves for thermal growth and stress were developed using the fish monitoring data of z platypus from 2008 to 2016 fig 3 according to the ifasg method ifasg 1986 the lower g1 and upper g2 optimum temperature of growth 18 6 c and 24 0 c respectively was defined as the central 50 of the fish monitoring data and the lower g0 and upper g3 threshold temperature of growth 13 6 c and 27 0 c respectively was defined as the central 90 of the data the threshold for cold stress c0 was set at 6 0 c by rounding off the lowest temperature of appearance recorded 6 4 c kang 2010 the threshold for heat stress h0 was set at 30 c which is the average value of the reported thermal tolerance chung et al 2011 estimated that z platypus is capable of tolerating up to 29 c while kang et al 2013 estimated the thermal tolerance to be 31 c the rates of cold and heat stress were estimated using the relationship between the relative fish abundance and water temperature fig s2 the lower and upper 25 range of fish abundance data 2008 2016 were used to calculate the rate of cold and heat stress respectively although cold and heat stress can increase infinitely kriticos et al 2015 the minimum water temperature is usually limited to the freezing point toffolon and piccolroaz 2015 wanders et al 2019 in this study cold stress was set to reach its maximum at 4 0 c because this was the minimum water temperature observed in the national monitoring data from 1997 to 2005 nier 2006 the ehs was calculated using the arithmetic mean eq 5 geometric mean eq 6 and product method eq 7 muñoz mas et al 2012 the hhs and phs were not weighted by assuming that they are equally important ahmadi nedushan et al 2006 5 ehs hhs phs 2 6 ehs hhs phs 7 ehs hhs phs the geometric mean eq 6 assumes that good habitat conditions can compensate for unfavorable habitat conditions with interaction among variables fukuda et al 2011 the arithmetic mean eq 5 can consider only the compensation without interaction while the product method eq 7 can consider the interaction without compensation fukuda et al 2011 the annual hhs was calculated as follows papadaki et al 2016 8 hhs i 1 365 si d i si v i 365 where sid i and siv i are suitability indexes at day i for water depth fig 2a and velocity fig 2b respectively the annual phs was derived by multiplying the annual growth and stress kriticos et al 2015 9 phs gi si where gi and si are annual growth and stress indexes respectively annual gi was derived by calculating the arithmetic mean of the daily growth gi of the whole year eq 10 10 gi i 1 365 g i 365 where the gi value was derived from the growth curve in fig 3a with daily average water temperature considering that cold and heat stress are accumulated across the whole year kriticos et al 2015 the annual cold or heat stress indexes csi and hsi were derived by calculating the average of weighted monthly cold or heat stress si of the whole year eq 11 11 csi or hsi i 1 12 s i w i 12 the si value was weighted on a monthly basis wi where the stress of january was weighted as 1 and gradually increased with december being weighted as 12 the monthly cold or heat stress were the arithmetic mean of daily cold or heat stress the daily cold and heat stress were derived from the stress curve in fig 3b with daily minimum and maximum water temperature respectively finally the annual stress index was calculated by eq 12 12 si 1 min csi 1 1 min hsi 1 the ehsm was validated by comparing calculated ehs values at the representative 117 sites in south korea table s1 with z platypus monitoring data 2008 2015 the average cpue catch per unit effort of the monitoring period was used by assuming equivalent sampling conditions or effort among all sites after weighting the cpue by dividing it by the stream width cpuew the relationship between ehs and the natural log scale cpuew was analyzed with a non linear equation van der wal et al 2009 eq 13 13 ln cpuew a 1 exp b ehs c the performance of ehsm was evaluated by comparing with a conventional model that calculates composite habitat suitability chs as the annual average of geometric mean of hydraulic and thermal suitability indexes eq 14 14 chs i 1 365 si d i xsi v i si t i 3 365 where sid i siv i and sit i are suitability indexes at day i for water depth velocity and temperature respectively the suitability curves for water depth and velocity were the same as those used in the ehsm fig 2 and the suitability curve for water temperature was taken from the work of choi and choi 2018 fig s3 moreover lowess locally weighted scatterplot smoothing analysis provided in matlab s curve fitting tool version r2019a the mathworks inc natick ma was applied to visualize the relationship of suitability indexes of ehsm and chsm with fish abundance ln cpuew cleveland and devlin 1988 for the chsm hhs and ths thermal habitat suitability values were calculated by eqs 8 and 15 respectively 15 ths i 1 365 si t i 365 2 4 statistical analysis microsoft excel 2016 microsoft corporation redmond wa usa and sigmaplot version 12 systat software inc san jose ca usa were used for regression analyses spss version 24 ibm corp armonk ny usa was used for partial correlation analyses all statistical results were determined by a significance level of p 0 05 3 results and discussion 3 1 model performance the overall nse values of regression models for water depth water velocity and average water temperature were 0 673 0 278 satisfactory 0 50 nse 0 70 0 747 0 176 good 0 70 nse 0 80 and 0 849 0 249 very good nse 0 80 respectively moriasi et al 2015 these results suggest that the regression models applied in this study are reliable to simulate water depth water velocity and average water temperature all three methods used in ehs calculation successfully produced a non linear relationship between ehs and ln cpuew with an adjusted r2 of 0 4835 0 5181 fig 4 among them the geometric mean of hhs and phs was selected in this study because its adjusted r2 0 5181 was higher than the other methods 0 4840 and 0 4835 for product and arithmetic mean methods respectively considering that compensation among habitat variables is likely to occur for fish the geometric mean is reasonable to calculate ehs fukuda et al 2011 moreover the accuracy of the ehs to predict the presence of z platypus ehs 0 was 91 5 these findings suggest that the ehsm developed in this study is valid for predicting the habitat suitability of z platypus the performance of ehsm was evaluated by comparing with the composite habitat suitability model chsm the non linear relationship eq 13 between chs and ln cpuew was slightly lower than that of ehs with an adjusted r2 of 0 5076 fig s4 moreover lowess analysis suggests that both hhs and phs contributed to the increase in fish abundance for ehsm fig s5 however the contribution of ths to the fish abundance was much less than that of hhs in the case of chsm overall these findings support that ehsm is a promising tool for assessing habitat suitability of freshwater fish 3 2 habitat suitability assessment of z platypus the habitat suitability of the pale chub z platypus in south korea is shown in fig s6 the ehs was classified into four groups by combining the hhs and phs criteria the hhs and phs values were classified according to krieger and diana 2017 and taylor and kumar 2013 respectively table 1 fig 5 presents the cumulative percentage of each habitat group in the studied sites based on the classification of ehs 34 2 of the sites were highly suitable while 31 6 of the sites were either poor or unsuitable additionally the proportion of highly suitable sites of ehs was quite similar to that of hhs 38 5 while a much higher number of sites yielded highly suitable based on phs 49 6 indeed partial correlation analysis confirmed that ehs was more related to hhs r 0 956 than to phs r 0 750 the higher physiologic suitability possibly occurred due to the relatively high thermal tolerance of z platypus chung et al 2011 kang et al 2013 in this study the values of hhs seem to be controlled by the variation in depth fig s7 partial correlation analysis confirmed that hhs was more related to suitability of depth r 0 980 than that of velocity r 0 456 the higher variations in depth than in velocity would be related to the stream order lee et al 2014 showed that stream order and depth have a significantly positive correlation p 0 01 in the namhan river while velocity or water current does not show a significant correlation p 0 05 moges and bhole 2015 also reported that depth has a relatively higher correlation r2 0 646 with stream order than velocity r2 0 194 in the goro river 3 3 implications and limitations of ehsm habitat suitability indexes are commonly derived from the combination of independent preference curves kang 2010 yi et al 2017 like the geometric means of hhs and phs used to calculate ehs in this study however layher and maughan 1985 demonstrated that an unweighted geometric mean of suitability indexes of spotted bass showed little correlation to observed biomass in line with this a fuzzy logic approach can be applied to weighting habitat suitability index noack et al 2013 fukuda 2009 demonstrated that the fuzzy logic approach decreased fluctuations in habitat suitability and reduced prediction errors moreover yi et al 2016b reported that fuzzy logic models yielded higher accuracy than unweighted preference curve models these findings suggest that the fuzzy logic approach can be used to improve ehs evaluation by weighted integrating hhs and phs laboratory experiments or field observations are recommended to simulate growth and stress of species kriticos et al 2015 sutherst and maywald 1985 for instance he et al 2012 developed growth and stress curves of e postvittana using experimental data on development population growth rate and generation time sutherst and maywald 2005 also used field observations of colony growth of the red imported fire ant solenopsis invicta in order to develop growth curves in the case of fish elliott and hurley 1997 estimated the growth rate of atlantic salmon parr depending on temperature 5 0 20 c through laboratory experiments in this study growth and stress curves were developed based on field observation data of z platypus however physiologic data derived from laboratory experiments of z platypus are rare and require more research to refine growth and stress curves physiological suitability is expected to play a key role in determining the habitat suitability of cold water fish because these species are generally sensitive to water temperature stitt et al 2014 given that climate change may enhance the variation of water temperature in streams and rivers isaak et al 2012 webb and nobilis 2007 ehsm can be a promising model to predict the impact of climate change on cold water fish in addition individual suitability indexes water depth and velocity growth and cold and heat stress can provide information on the causes of climate change effects on freshwater fish distributions 4 conclusions ehsm was developed by integrating hydraulic and physiologic habitat suitability and successfully used to estimate the habitat suitability of z platypus on a national scale modeling results have shown that hhs especially the suitability regarding depth played key roles in determining the ehs of z platypus in addition ehsm represents the habitat suitability of freshwater fish better than a conventional model that has no consideration of fish physiology although more studies are required to refine growth and stress curves ehsm can be useful to predict climate change impacts on freshwater fish especially cold water species and this should be further investigated declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 5 acknowledgments this study was supported by the korea ministry of environment moe climate change correspondence program grant number 201400130007 we would also like to thank dr byungwoong choi and two anonymous reviewers for valuable comments on our manuscript appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104760 
