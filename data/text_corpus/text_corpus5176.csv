index,text
25880,environmental monitoring has become of paramount importance for researchers and decision makers in order to better protect the environment and achieve sustainable development goals especially with continuous population growth and rapid urbanization and industrialization processes that alter natural components of the environment and degrade the ecosystems hence the use of landsat data is of great significance particularly in environmental monitoring and land management due to their high spatial resolution and their free of cost availability since 1972 however calculation of remotely sensed indices for environmental purpose occurs in several steps and requires minimum knowledge in imagery processing this paper describes the development and design of q lip qgis landsat indices plugin a simple and easy to use tool to download process and automatically calculate environmental indices from landsat imagery q lip has been built under python and designed as a qgis software plugin it is available to download from qgis plugin repository keywords q lip landsat qgis environmental monitoring software availability name of software q lip qgis landsat indices plugin developers boutaina sebbah otmane yazidi alaoui contact information boutainasebbah gmail com year first available 2020 hardware required 8 gb ram software required qgis3 x software availability https github com yazidiotmane q lip qgis software cost free program language python program size 623 kb on disk 1 introduction as the world population continues to develop human activities continue their evolution too and over exploitation of biotic and abiotic resources become greater and greater leading to a drastic degradation of environmental components and alteration of its natural characteristics the need to understand and monitor environmental status and its spatiotemporal changes is of great significance in order to protect and manage natural resources efficiently and therefore achieve sustainable development goals ground based techniques and in situ measurements have been traditionally used in environmental monitoring but unfortunately they have many limitations such as high cost and are not time or labour efficient moreover they are only effective for small and limited regions li et al 2020 the use of remotely sensed data in environmental monitoring and ecological change detection represents an effective surrogate of ground base methods xu et al 2019 since it enables a large scale area coverage and provides regular observations due to its diverse spatial and temporal resolution furthermore it saves time and effort and is cheaper compared to traditional methods de araujo barbosa et al 2015 due to their free of cost availability data acquired from landsat sensors are considered as one of the most used data in global climate change researches environmental monitoring and land management besides they provide the longest running and continuous time series intended for civil purposes since 1972 nasa and usgs 2018 landsat data have been used in diverse fields in relation to environmental and ecological monitoring it has been used in land and water surface temperature assessment ding and elmore 2015 guo et al 2016 in vegetation cover changes and evapotranspiration estimation panda et al 2019 xie et al 2019 in burn severity estimation and deforestation monitoring quintano et al 2015 schultz et al 2016 they have also been used in urban studies as urban sprawl mapping sebbah et al 2017a and widely used in urban heat island studies as they have a high resolution thermal band compared to other existing data liu et al 2020 sebbah et al 2017b quite often these analyses are performed through different remote sensing indices that have been developed to assess and quantify environmental status as normalized difference vegetation index ndvi rouse et al 1974 normalized difference water index ndwi mcfeeters 1996 and land surface temperature lst jimenez munoz et al 2014 etc therefore to calculate these multiple indices and produce variation maps it is recommended to first preprocess landsat images as they are delivered in raw values then calculate the required indices this through image processing software or even geographic information system software that have become commonly used especially with the emergence of open access software such as qgis this has motivated us to develop q lip qgis landsat indices plugin presented in this paper which is a plugin built under python and openly available in qgis to facilitate the processing and reduce its time particularly for users without a strong background in image processing this plugin is dedicated to download landsat images with path 201 and raw 35 that includes the north of morocco and the south of spain on the strait of gibraltar and compute landsat environmental indices in order to help researchers and decision makers to better understand and monitor environmental variation in this region q lip plugin also allows the calculation of landsat environmental indices for any other area provided that images are downloaded separately from existing online platforms 2 technical implementation to facilitate landsat environmental indices computing for a broad audience and for users without a strong knowledge in image processing and to support the open source model q lip plugin is integrated in qgis an open source geographic information system software that permits the creation visualization editing and analysis of geospatial data the use of qgis is highly recommended since it is freely available and its performance can be extended with a wide range of external plugins developed by its large community recently many plugins have been developed under qgis to serve environmental purposes such as evapotranspiration prediction ellsäßer et al 2020 near surface air temperature mapping touati et al 2020 climate change mitigation lindberg et al 2018 and aquatic ecosystems evaluation nielsen et al 2021 the core functionality of q lip is written in python 3 2 while the graphical user interface gui is developed using qt designer q lip is compatible with qgis version 3 10 and all its functionalities can be run on a standard qgis for desktop without requirements of any additional python libraries package however it performs hyper complicated operations requiring a minimum of 8 gb of ram to compute all indices from landsat images with more than 1 gb size 3 process description using landsat images from three different satellites l5 l7 and l8 q lip allows users to automatically compute some of the most utilized indices in environmental studies the main tasks of this plugin can be summarized in three steps as illustrated in fig 1 1 download of landsat images 2 preprocess of landsat images 3 automatic computing of landsat environmental indices q lip uses information from metadata file mtl included in the landsat image folder to extract necessary values for radiometric calibration and then it calculates the chosen environmental indices the output result is a raster layer with tagged image file format tiff the gui presented in fig 2 will appear once the plugin is executed thus the process is divided into three tabs downloading metadata and environmental indices 3 1 downloading tab the downloading tab allows the download of landsat data with path 201 and row 35 from usgs earth explorer platform this image covers the north of morocco and the south of spain on the strait of gibraltar an account in usgs platform must be created in advance so that it can be used to download the required images directly from the plugin firstly the users must enter their login and password then choose date intervals e g searching from 01 08 2020 to 31 08 2020 secondly the user should select the required satellite from the three available choices landsat 5 landsat 7 or landsat 8 and then specify the cloud coverage percentage all found results will appear according to the criteria selected by the user and an overview of the image will appear once an image has been selected fig 2a finally the selected image can be downloaded 3 2 metadata tab metadata tab fig 2b allows navigation to the image directory and loading the mtl file once the mtl is loaded the needed information for preprocessing such as sun elevation earth sun distance radiance and reflectance multiplicative additive scaling factors and band specific thermal conversion constants k1 and k2 will be loaded in the table radiometric calibration of landsat images is performed automatically according to the following steps 3 2 1 conversion of dn to at sensor spectral radiance conversion to at sensor spectral radiance is an essential step to switch from raw digital numbers dn in level 1 products to physical values chander et al 2009 conversion requires the knowledge of bands specific rescaling gain and bias factors the conversion of the dn from the three satellites was carried out with the following equations sited in chander et al 2007 u s geological survey 2019 usgs 2011 3 2 2 conversion to toa reflectance conversion from at sensor spectral radiance to top of atmosphere toa reflectance is an important step especially in multi temporal studies where data from multiple sensors are compared this is so because it eliminates the cosine effect of different solar zenith angles generated by variability between data acquisitions time it also compensates the various values of exo atmospheric solar irradiance resulting from variation of spectral bands as well as adjusting the difference in the earth sun distance which arises due to differences between dates of data acquisition chander et al 2009 toa reflectance for the three sensors was computed according to equations cited in chander et al 2009 u s geological survey 2019 3 2 3 conversion to at sensor brightness temperature spectral radiance of thermal bands from the three satellites band 6 6 2 and 10 of l5 tm l7 etm and l8 oli tirs respectively is converted to at satellite brightness temperature also known as effective at satellite temperatures following equations in chander et al 2009 u s geological survey 2019 in this plugin the use of thermal band 11 from l8 oli tirs was avoided due to the large calibration uncertainty of this band mentioned by us geological survey u s geological survey 2019 3 2 4 environmental indices tab the third tab is reserved for landsat environmental indices fig 2c where the required indices to be computed must be checked indices are divided into four categories vegetation soil water and thermal indices each category contains several indices for example in vegetation category users can compute normalized difference vegetation index ndvi portion of vegetation pv and modified soil adjusted vegetation index msavi all indices proposed by q lip plugin for each category and their formulas are summarized in table 1 the environmental indices tab also contains an index description table that gives information about proposed indices their calculation formula zonal statistics and correlation matrix between indices can also be extracted and it is saved in txt file 4 application and results q lip was tested using a computer with 8 gb of ram for example the total processing time of a landsat 8 image with 1 73 gb size is 3 min for users working with multi temporal landsat data who desire to reduce processing time it is recommended to use devices with 8 gb in ram or more results of calculated indices are saved in the input image file with the same name as input image plus a suffix indicating index abbreviation and its date and processing time once the processing is completed raster data of computed indices will appear in qgis canvas fig 3 allowing users to benefit from all qgis functions and to modify raster proprieties or extract data according to existing shape files this facilitates the production of new maps and interprets them depending on the user s objective indices computed by q lip could be used in several environmental studies such as land cover change detection burn severity pre and post forest fire analyses surface water detection for water resources management urban sprawls detection and in urban heat island delineation and assessment 5 conclusion the present paper introduces q lip qgis landsat indices plugin a simple and easy to use qgis plugin built under python allowing the automatic downloading processing and computing of landsat environmental indices it permits the reduction of processing time to only 3 min for landsat images with more than 1 gb size when using computers with a minimum of 8 gb in ram q lip can be a valuable tool for users without strong knowledge in remote sensing processing but are interested in environmental monitoring and land management funding this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors thank the insightful comments and suggestions from anonymous reviewers 
25880,environmental monitoring has become of paramount importance for researchers and decision makers in order to better protect the environment and achieve sustainable development goals especially with continuous population growth and rapid urbanization and industrialization processes that alter natural components of the environment and degrade the ecosystems hence the use of landsat data is of great significance particularly in environmental monitoring and land management due to their high spatial resolution and their free of cost availability since 1972 however calculation of remotely sensed indices for environmental purpose occurs in several steps and requires minimum knowledge in imagery processing this paper describes the development and design of q lip qgis landsat indices plugin a simple and easy to use tool to download process and automatically calculate environmental indices from landsat imagery q lip has been built under python and designed as a qgis software plugin it is available to download from qgis plugin repository keywords q lip landsat qgis environmental monitoring software availability name of software q lip qgis landsat indices plugin developers boutaina sebbah otmane yazidi alaoui contact information boutainasebbah gmail com year first available 2020 hardware required 8 gb ram software required qgis3 x software availability https github com yazidiotmane q lip qgis software cost free program language python program size 623 kb on disk 1 introduction as the world population continues to develop human activities continue their evolution too and over exploitation of biotic and abiotic resources become greater and greater leading to a drastic degradation of environmental components and alteration of its natural characteristics the need to understand and monitor environmental status and its spatiotemporal changes is of great significance in order to protect and manage natural resources efficiently and therefore achieve sustainable development goals ground based techniques and in situ measurements have been traditionally used in environmental monitoring but unfortunately they have many limitations such as high cost and are not time or labour efficient moreover they are only effective for small and limited regions li et al 2020 the use of remotely sensed data in environmental monitoring and ecological change detection represents an effective surrogate of ground base methods xu et al 2019 since it enables a large scale area coverage and provides regular observations due to its diverse spatial and temporal resolution furthermore it saves time and effort and is cheaper compared to traditional methods de araujo barbosa et al 2015 due to their free of cost availability data acquired from landsat sensors are considered as one of the most used data in global climate change researches environmental monitoring and land management besides they provide the longest running and continuous time series intended for civil purposes since 1972 nasa and usgs 2018 landsat data have been used in diverse fields in relation to environmental and ecological monitoring it has been used in land and water surface temperature assessment ding and elmore 2015 guo et al 2016 in vegetation cover changes and evapotranspiration estimation panda et al 2019 xie et al 2019 in burn severity estimation and deforestation monitoring quintano et al 2015 schultz et al 2016 they have also been used in urban studies as urban sprawl mapping sebbah et al 2017a and widely used in urban heat island studies as they have a high resolution thermal band compared to other existing data liu et al 2020 sebbah et al 2017b quite often these analyses are performed through different remote sensing indices that have been developed to assess and quantify environmental status as normalized difference vegetation index ndvi rouse et al 1974 normalized difference water index ndwi mcfeeters 1996 and land surface temperature lst jimenez munoz et al 2014 etc therefore to calculate these multiple indices and produce variation maps it is recommended to first preprocess landsat images as they are delivered in raw values then calculate the required indices this through image processing software or even geographic information system software that have become commonly used especially with the emergence of open access software such as qgis this has motivated us to develop q lip qgis landsat indices plugin presented in this paper which is a plugin built under python and openly available in qgis to facilitate the processing and reduce its time particularly for users without a strong background in image processing this plugin is dedicated to download landsat images with path 201 and raw 35 that includes the north of morocco and the south of spain on the strait of gibraltar and compute landsat environmental indices in order to help researchers and decision makers to better understand and monitor environmental variation in this region q lip plugin also allows the calculation of landsat environmental indices for any other area provided that images are downloaded separately from existing online platforms 2 technical implementation to facilitate landsat environmental indices computing for a broad audience and for users without a strong knowledge in image processing and to support the open source model q lip plugin is integrated in qgis an open source geographic information system software that permits the creation visualization editing and analysis of geospatial data the use of qgis is highly recommended since it is freely available and its performance can be extended with a wide range of external plugins developed by its large community recently many plugins have been developed under qgis to serve environmental purposes such as evapotranspiration prediction ellsäßer et al 2020 near surface air temperature mapping touati et al 2020 climate change mitigation lindberg et al 2018 and aquatic ecosystems evaluation nielsen et al 2021 the core functionality of q lip is written in python 3 2 while the graphical user interface gui is developed using qt designer q lip is compatible with qgis version 3 10 and all its functionalities can be run on a standard qgis for desktop without requirements of any additional python libraries package however it performs hyper complicated operations requiring a minimum of 8 gb of ram to compute all indices from landsat images with more than 1 gb size 3 process description using landsat images from three different satellites l5 l7 and l8 q lip allows users to automatically compute some of the most utilized indices in environmental studies the main tasks of this plugin can be summarized in three steps as illustrated in fig 1 1 download of landsat images 2 preprocess of landsat images 3 automatic computing of landsat environmental indices q lip uses information from metadata file mtl included in the landsat image folder to extract necessary values for radiometric calibration and then it calculates the chosen environmental indices the output result is a raster layer with tagged image file format tiff the gui presented in fig 2 will appear once the plugin is executed thus the process is divided into three tabs downloading metadata and environmental indices 3 1 downloading tab the downloading tab allows the download of landsat data with path 201 and row 35 from usgs earth explorer platform this image covers the north of morocco and the south of spain on the strait of gibraltar an account in usgs platform must be created in advance so that it can be used to download the required images directly from the plugin firstly the users must enter their login and password then choose date intervals e g searching from 01 08 2020 to 31 08 2020 secondly the user should select the required satellite from the three available choices landsat 5 landsat 7 or landsat 8 and then specify the cloud coverage percentage all found results will appear according to the criteria selected by the user and an overview of the image will appear once an image has been selected fig 2a finally the selected image can be downloaded 3 2 metadata tab metadata tab fig 2b allows navigation to the image directory and loading the mtl file once the mtl is loaded the needed information for preprocessing such as sun elevation earth sun distance radiance and reflectance multiplicative additive scaling factors and band specific thermal conversion constants k1 and k2 will be loaded in the table radiometric calibration of landsat images is performed automatically according to the following steps 3 2 1 conversion of dn to at sensor spectral radiance conversion to at sensor spectral radiance is an essential step to switch from raw digital numbers dn in level 1 products to physical values chander et al 2009 conversion requires the knowledge of bands specific rescaling gain and bias factors the conversion of the dn from the three satellites was carried out with the following equations sited in chander et al 2007 u s geological survey 2019 usgs 2011 3 2 2 conversion to toa reflectance conversion from at sensor spectral radiance to top of atmosphere toa reflectance is an important step especially in multi temporal studies where data from multiple sensors are compared this is so because it eliminates the cosine effect of different solar zenith angles generated by variability between data acquisitions time it also compensates the various values of exo atmospheric solar irradiance resulting from variation of spectral bands as well as adjusting the difference in the earth sun distance which arises due to differences between dates of data acquisition chander et al 2009 toa reflectance for the three sensors was computed according to equations cited in chander et al 2009 u s geological survey 2019 3 2 3 conversion to at sensor brightness temperature spectral radiance of thermal bands from the three satellites band 6 6 2 and 10 of l5 tm l7 etm and l8 oli tirs respectively is converted to at satellite brightness temperature also known as effective at satellite temperatures following equations in chander et al 2009 u s geological survey 2019 in this plugin the use of thermal band 11 from l8 oli tirs was avoided due to the large calibration uncertainty of this band mentioned by us geological survey u s geological survey 2019 3 2 4 environmental indices tab the third tab is reserved for landsat environmental indices fig 2c where the required indices to be computed must be checked indices are divided into four categories vegetation soil water and thermal indices each category contains several indices for example in vegetation category users can compute normalized difference vegetation index ndvi portion of vegetation pv and modified soil adjusted vegetation index msavi all indices proposed by q lip plugin for each category and their formulas are summarized in table 1 the environmental indices tab also contains an index description table that gives information about proposed indices their calculation formula zonal statistics and correlation matrix between indices can also be extracted and it is saved in txt file 4 application and results q lip was tested using a computer with 8 gb of ram for example the total processing time of a landsat 8 image with 1 73 gb size is 3 min for users working with multi temporal landsat data who desire to reduce processing time it is recommended to use devices with 8 gb in ram or more results of calculated indices are saved in the input image file with the same name as input image plus a suffix indicating index abbreviation and its date and processing time once the processing is completed raster data of computed indices will appear in qgis canvas fig 3 allowing users to benefit from all qgis functions and to modify raster proprieties or extract data according to existing shape files this facilitates the production of new maps and interprets them depending on the user s objective indices computed by q lip could be used in several environmental studies such as land cover change detection burn severity pre and post forest fire analyses surface water detection for water resources management urban sprawls detection and in urban heat island delineation and assessment 5 conclusion the present paper introduces q lip qgis landsat indices plugin a simple and easy to use qgis plugin built under python allowing the automatic downloading processing and computing of landsat environmental indices it permits the reduction of processing time to only 3 min for landsat images with more than 1 gb size when using computers with a minimum of 8 gb in ram q lip can be a valuable tool for users without strong knowledge in remote sensing processing but are interested in environmental monitoring and land management funding this research did not receive any specific grant from funding agencies in the public commercial or not for profit sectors declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors thank the insightful comments and suggestions from anonymous reviewers 
25881,solute transport models are widely employed to the predict spatio temporal fate of contaminants in rivers however most previous studies have assumed that contaminants spilled are unreactive they disregard the inherent reactivity of contaminants in water the reactive solute transport model rstm gooseff et al 2005 includes lumped decay terms to reflect the decayability of the chemicals but its applicability was still poor due to dependence on an optimization method the purpose of this study is to perform reactive transport modelling considering chemicals reactivities for sorption volatilization and biodegradation to this end we manipulated the governing equations of the rstm and suggested theoretical and empirical methods of estimating the key parameters of the reaction terms the results showed for example that benzene lost 57 7 of its primary mass after being transported 4 54 km downstream due to its high volatility also the arrival time of toluene was delayed by 10 4 due to adsorption keywords reactive solute transport model analytical solution tracer test sorption volatilization biodegradation 1 introduction worldwide industrialization has caused a dramatic rise in chemical usage in industrial complexes which has in turn led to spill accidents of harmful contaminant in november 2014 a truck overturned and spilled 2 000 l of sulfuric acid into nakdong river the longest river in south korea which resulted in severe damage to the aquatic ecosystem and water quality such unpredictable accidents in rivers are especially serious since most of the water supply in south korea is dependent on surface waters such as rivers and dam reservoirs having recognized the problem many researchers have employed the solute transport models to predict the behavior and fate of contaminants transported in the rivers however even though many solute transport analyses have been conducted in recent decades few of these studies attempted to analyze reactive transport of chemicals as it is difficult to use toxic chemicals in a field test due to safety concerns most chemicals are inherently reactive in water for example methyl isocyanate is extremely toxic but cannot be subjected to study since half of them is consumed in few minutes in excess water castro et al 1985 some hydrogen compounds dissociate or ionize almost completely in water some compounds with high vapor pressure or high henry s law constant may lose their mass when flowing downstream in a river due to their volatility therefore the models need to consider these reaction characteristics to accurately predict the behavior of chemicals in rivers in order to meet these needs the otis p runkel and broshears 1991 runkel and chapra 1993 runkel 1998 gooseff et al 2005 includes a sorption algorithm and first order decay terms on the governing equation of the transient storage model tsm the tsm is the most prominent and popular model because it can accurately simulate the skewness of a breakthrough curve btc bencala and walters 1983 afterwards gooseff et al 2005 referred to the expanded model of tsm as the reactive solute transport model rstm in their study gooseff et al 2005 estimated the first order decay parameters using an optimization method with experimental data on nitrogen dioxide from a tracer test in green creek antarctica the sorption parameter was set to an arbitrary value likewise o connor et al 2010 employed the improved inverse model to estimate the decay parameters haggerty et al 2009 used resazurin as a smart tracer to provide additional biochemical information in streams these efforts to estimate the reaction parameters that yield the best fitting output have confirmed the validity of the first order decay terms however the methods of parameter estimation have a practical limitation in that the parameters optimized for one chemical cannot be applied to another type of chemical to address the above limitation the current study attempts to present practical methods for reaction analysis of chemicals in rivers the governing equations of the rstm were manipulated the numerical model was constructed using the finite difference method fdm and crank nicolson method in addition an analytical solution of the manipulated governing equations was devised in order to verify the numerical model to determine the reactivity parameters the inherent properties of chemicals were conjugated based on the appropriate theoretical and empirical methods since an estimation of conservative transport parameters should precede reactive transport parameter estimation wagner and harvey 1999 tracer tests using rhodamin wt were conducted at gam creek south korea in 2019 and 2020 the test bed was also subjected to the reactive transport simulations by extension the simulation results were used for the sensitivity analysis of the model with respect to the decay parameters a flow chart of model development and validation is shown in fig 1 2 model development 2 1 governing equation when a chemical is accidently introduced into a river the rstm can be used for transport analysis considering advection by current dispersion by turbulence storage effect sorption on the streambed and first order decay in this study the channel was divided into three domains the flow zone storage zone and streambed zone as shown in fig 2 the governing equations corresponding to each domain were formulated by reflecting the phenomena of volatilization and biodegradation as also shown in fig 2 as follows 1a c t q a f c x 1 a f x a f k c x α c s c λ s c b c e q λ v c λ b c 1b c s t α a f a s c c s λ b c s 1c c b t λ s c e q c b λ b c b where c c s and c b g m 3 denote the solute concentration within the flow zone storage zones and sediment on the streambed respectively q m3 s 1 denotes the volumetric flow rate a f and a s m2 denote the cross sectional area of the flow zone and storage zones respectively k m2 s 1 denotes the dispersion coefficient α denotes the mass exchange rate between the flow zone and storage zones s 1 c e q g m 3 denotes the equilibrium sorbate concentration on the streambed λ s λ v and λ b s 1 denote the kinetic exchange rate by sorption volatilization and biodegradation respectively and t is a time variable the x coordinate was taken to be approximately streamwise distance the sorption mechanism was interpreted as a mass transfer between the flow zone and streambed zone depending on the equilibrium sorbate concentration on the streambed and the first order kinetic coefficient the single decay term of the rstm was divided into two components volatilization and biodegradation which are influential in the mass loss of organic chemicals in rivers for these governing equations a numerical model was constructed using the python programming language the continuous solute concentration distributions in the three domains were spatially and temporally discretized using the fdm and the crank nicolson method following the procedures used to build the otis model runkel an broshears 1991 2 2 reactivity parameter estimation for the reactive solute transport simulation the first order kinematic modelling for the reaction mechanisms and the estimation methods for the reactivity parameters were delineated the governing equations of the model contain three reactivity parameters sorption rate λ s volatilization rate λ v and biodegradation rate λ b these parameters represent the chemicals inherent reactivity i e how quickly they are affected by the corresponding mechanisms in this study they were modeled by the explicit methods given below 2 2 1 sorption rate λ s contaminants that spill into a river are adsorbed and desorbed on sediment above the streambed this mechanism can significantly affect the mixing behavior of contaminants in rivers in the sense that it delay solute transport by trapping into a non flowing area the sorption mechanism could be interpreted as the same process as the transient storage effect at the storage zones however whereas the transient storage effect is simply understood as the first order mass transfer driven by the concentration difference between the flow zone and storage zones the rate of adsorption and desorption significantly differ due to the resistance to release of accumulated sorbate on sediment di toro and horzempa 1982 karickhoff and morris 1985 the degree of this resistance varies drastically by type of chemical substance and is represented by the partition coefficient which is conventionally defined as the ratio of sorbate phase concentration to solution phase concentration of the contaminant in the equilibrium state in this study the sorption mechanism was simplified following the model of runkel runkel 1998 gooseff et al 2005 the sorption term was manipulated in this model by saying that the partial derivative of the solute concentration is proportionally affected by the disproportionality of current sorbate concentration and equilibrium sorbate concentration as 2 c t λ s c b c e q c e q k p ρ b c where c e q g m 3 is the equilibrium sorbate concentration on the streambed λ s s 1 is the sorption rate k p m3 g 1 is the partition coefficient of a chemical which can be determined using the organic carbon water partition coefficient k oc and the fraction of organic carbon in sediment f oc karickhoff et al 1979 abdul et al 1986 as 3 k p f o c k o c herein f o c was set at the default value of 0 2 0 002 g g as suggested in the soil screening guidance 1996 developed by the us environmental protection agency us epa to estimate k oc the linear regression between k oc and the octanol water partition coefficient k ow was employed following karickhoff et al 1979 as 4 k o c 0 63 k o w r 2 0 96 ρ b g m 3 is the ratio of sediment mass within the mixing layer on the streambed and the waterbody volume the mixing layer is the column in which sorptive mass exchange occurs ρ b can be approximately defined as 5 ρ b δ m h 1 φ ρ s where δ m m is mixing layer thickness h m is water depth φ is porosity of streambed and ρ s g m 3 is the sediment density in the application of this study the δ m h φ and ρ s were assumed to be constant values of 0 1 0 4 and 2 65 g cm 1 respectively based on the relationship between varying chemicals and sediments the kinetic sorption rate λ s is inversely proportional to the equilibrium partition coefficient k p karickhoff and morris 1985 linear regression analysis using the suite of data characterized by hassett and means 1980 was used to obtain the relationship between the characteristic time and k p as 6 1 λ s h o u r s 0 03 k p in sum once the octanol water partition coefficient of a given contaminant is obtained k p can be empirically estimated so that λ s can be identified for sorption mechanism modelling 2 2 2 volatilization rate λ v in the process of chemical transport in a river the volatilization into the atmosphere and resulting mass degradation occur depending on the chemical s volatility this volatility is normally indicated by the chemical s molecular weight or vapor pressure smith et al 1980 we embodied the volatilization mechanism in the transport model as a first order mass transfer through a two phase boundary layer which is referred by whitman 1923 to as the two film theory this theory holds that volatility is dominated by the diffusion resistance within the boundary layers the two film theory relies on several assumptions first mass gradients within the quiescent boundary layers are linear due to molecular diffusion as illustrated in fig 3 second the ratio of the concentrations on the air water interface as notated in fig 3 with c w i n t and c a i n t respectively can be defined as the henry s law constant lastly there is no mass accumulation at the interface meaning that the mass fluxes in both the liquid phase film and the gas phase film are identical the flux can be estimated by one dimensional fick s law 7 j d m δ c c i n t k c c i n t where j g m 2 sec 1 is the mass flux through the boundary layer d m m2 sec 1 is the molecular diffusivity δ is the boundary layer thickness c g m 3 is the bulk concentration c int g m 3 is the concentration on the interface and k m s 1 is the mass transfer rate since the concentration on the interface cannot be directly measured eq 7 was converted by applying the second assumption 8 j o u t w a r d k c w c a h c c 9 1 k 1 k w 1 h c c k a where the subscripts w and a indicate the value in water and in the atmosphere respectively as shown in fig 3 and h cc is the dimensionless henry s law constant which is the ratio of the aqueous phase molarity of a substance to its gas phase molarity since the volume of the atmosphere is infinitely large compared to that of the waterbody as sufficient time passes c a i n t approaches c a near zero in other words the atmosphere functions as an infinite sink accordingly the change in only c w over time is of interest and the aqueous phase film is used as a reference under these conditions to convert the flux into the concentration change over time eq 8 was multiplied by the area of the air water interface and divided by the domain water volume as 10 c w t λ v c w c i n t 0 where λ v sec 1 is the mass transfer rate hence forth called the volatilization rate the volatilization rate has a linear relationship with the diffusion coefficient of a chemical smith et al 1980 revealed that the ratio of diffusion coefficients of any two substances does not depend on the effects of temperature and viscosity that is the volatilization rate of a certain contaminant can be estimated in relation to the value of oxygen because the oxygen transfer rate referred to as the reaeration rate and the diffusivity of oxygen are relatively easy to obtain from previous studies the relation was proposed as 11 λ v λ r d d o n where λ v s 1 is the volatilization rate of a contaminant λ r s 1 is the reaeration rate d and d o m2 s 1 are the aqueous phase diffusivity of the contaminant and oxygen respectively and n is the modulus of relation which is not 1 because of the apparent incongruity relating to the quiescence assumption of the boundary layer smith et al 1980 in this study the empirical value of 0 61 was applied suggested by smith et al 1980 the oxygen reaeration rates in streams have been reasonably fitted by the following equation o conner and dobbins 1958 12 λ r d o u 0 5 h 1 5 where u m s 1 is mean flow velocity and h m is mean water depth since these two hydraulic variables are involved in the relation volatilization is affected not only by volatility but also by the flow conditions in the river to evaluate the validity of the above estimation method the correlation between the estimated λ r of a substance and its vapor pressure was assessed in consideration of the fact that the vapor pressure can represent a degree of volatility if this comparison reveals a high linear proportionality we can indirectly confirm that λ v reasonably represents the volatility by the transitive relation furthermore cross validation was also conducted with the existing volatilization model for both lakes and rivers referred to as wvolwin in the estimation program interface epi suite developed by the us epa based upon an adaptation of the method suggested by thomas 1982 wvolwin is used to compute the volatilization rate at 25 c 298k considering henry s law constant flow velocity water depth and the molecular weights of the compounds for 21 compounds the properties of volatility and estimated λ v according to both the proposed method and wvolwin are listed in table a1 the comparison yielded a pearson correlation coefficients of 0 851 between the estimated λ v and the vapor pressure and 0 832 between λ v and λ v w v o l w i n because the input data and the estimation method used in this study were apparently different from those used for wvolwin model λ v and λ v w v o l w i n would not be expected to be the same even so the resulting correlation which is plotted in fig 4 shows that the proposed estimation method can reasonably be used to represent the inherent volatility of the compounds 2 2 3 biodegradation rate λ s organic chemicals in natural rivers commonly lose mass due to biochemical reactions and the remaining chemical concentration can be plotted logarithmically with respect to time by first order kinetics this first order kinetics is also referred to as a half life kinetics because the half life represents the elapsed time for a chemical to halve alexander 1999 the biodegradation half life t h has a relation with the first order biodegradation rate λ b 13 λ b ln 2 t h the epi suite from the us epa offers several biodegradation models in which an indication of biodegradation rate is provided in relative terms of time range such as hours days and so on boethling et al 1994 one of the models biohcwin can estimate the biodegradation half life using hydrocarbon content howard et al 2005 the provided half life value was converted to the biodegradation rate using eq 13 3 model validation 3 1 derivation of analytical solution in this study to validate the developed numerical model the analytical solution of the governing equation for flow zone was derived in laplace domain following kazezyılmaz alhan 2008 who derived an analytical solution for tsm without regard of the terms for sorption and decay within the governing equations once the time variable is converted to the laplace variable s the laplace transformed concentration in the storage zone and the streambed zone notated as c s and c b respectively have a linear relationship with respect to the transformed flow zone concentration c 14 k 2 c x 2 u c x α c s α c λ s c b λ s k p ρ c λ v λ b c s c c t 0 0 15 c s β c w h e r e β α a f a s α a f a s λ b s 16 c b γ c w h e r e γ k s k p ρ k s λ b s where 17 c x s l c x t 0 e s t c d t substituting eq 15 and eq 16 into eq 14 the governing equation for the flow zone becomes a form of homogeneous ordinary differential equation ode as 18 k f d 2 c d x 2 u d c d x α β α λ s γ λ s k p ρ b λ v λ b s c 0 kazezyılmaz alhan 2008 derived the analytical solution for the tsm without regard to the sorption and decay terms following the work of kazezyılmaz alhan 2008 the general solution of the ode was obtained as 19 c x s c 0 s exp u u 2 4 k α β α λ s γ λ s k p ρ b λ v λ b s 2 k x the boundary condition was set as the laplace transformed heaviside function signifying that the contaminant is introduced from time t 1 to t 2 with concentration of c i n i 20 c 0 s c i n i s exp t 1 s exp t 2 s for inversion of the laplace transform the approximation polynomials which are precalculated are normally listed in tables however because we are unable to easily increase the order of the polynomials the numerical inversion method based on the bromwich integral was employed in this study to analyze the laplace transform of the solution valsa and brančik 1998 the suggested inversion algorithm was coded in matlab 3 2 validation of numerical model for a comparison of outcomes between the proposed numerical model and the analytical solution under identical conditions a virtual prismatic channel and simulation scenarios with a conservative solute case c and toluene case r were set up table 1 summarizes the input data of the transient storage parameters and the required properties of toluene used for the reactivity parameter estimation here case r was divided into three runs r 1 for sorption only r 2 for volatilization and biodegradation and r 3 for all three reaction processes in these simulations 1 ppm of both conservative contaminant and the toluene were released at the inlet for 30 s and simulated btcs were compared at sections 500 m 1 000 m and 1 500 m downstream of the inlet hereafter s1 s2 and s3 respectively the results revealed the validity of the numerical model showing a determination coefficient r2 of over 0 99 as plotted in fig 5 a as shown in fig 5b the btcs for case c and case r were fairly different while the mass conservation rates of the conservative solute at all sections were nearly 1 not exactly 1 due to truncation error those of toluene at s1 s2 and s3 were 0 98 0 95 and 0 92 respectively indicating that its mass decayed as it flows downstream table 2 lists several results of the comparison with different reaction mechanisms considered case r 1 which considers sorption only showed that sorption did not cause mass loss but delayed solute transport with a time lag before peak concentration on the other hand in case r 2 8 1 of the initial mass decayed and there was no temporal difference in the resulting btc as plotted in fig 5b 4 model application 4 1 experimental data since estimation of the conservative transport parameters is necessary prior to the improved reactive transport parameter estimation wagner and harvey 1999 tracer tests were carried out at gam creek gc the gc basin is located across an urban settlement in gimcheon and an agricultural area in gumi it plays an important role in supplying industrial and agricultural water and is at risk of contaminant spill accidents from nearby industrial complexes the whole reach of gc is gently meandering and the streambed is predominantly composed of sand substrate with little vegetation two of the tracer tests were conducted at the 4 85 km reach of gc in october 2019 gc2019 and the 4 54 km reach in june 2020 gc2020 including the injection point 5 cross sections along the reach were surveyed and the reach was divided into 4 sub reaches as illustrated in fig 6 to apply the one dimensional model the channel characteristics of each sub reach were assumed to be uniform therefore the key to parameter estimation was to determine the appropriate parameters representing in reach features a single sub reach should have relatively uniform flow properties and the corresponding parameters should be representative of the sub reach as a whole in these tests rhodamine wt rwt was used as a conservative tracer due to its visibility and low detection limit the tracer was simultaneously injected at multiple points in a transverse row because it has to be fully mixed in both the horizontal and vertical directions for the one dimensional mixing analysis in addition since the first measurement section s1 must be far enough from the injection point i p to ensure complete mixing the required distance for the multi point injection experiment was calculated using the following equation kilpatrick 1989 21 l 0 0 1 1 n 2 u w 2 e z where l 0 is the distance required for both complete horizontal and vertical complete mixing n is the number of injection points w is the mean stream top width and e z is the lateral mixing coefficient which is estimated from e z h u 0 15 fischer 1979 ysi 600oms fluorometers were used to measure the released rwt all of which were calibrated using rwt solutions in the range of 0 200 ppb the error range of the fluorometer is 5 of the reading values or 1 ppb the sensing rate was set to 0 25 hz to obtain an average concentration for each cross section three or four fluorometers were uniformly installed in the transverse direction in the four measurement sections s1 to s4 the hydraulic and geometric properties including flow rate velocity profiles water depth and bottom slope were measured using a sontek flowtracker acoustic doppler velocimeter and sokkia grx1 of real time kinematic global positioning system rtk gps fig 7 shows a view of the test reach at gam creek and the behavior of the tracer downstream of the injection point fig 7a shows the shape of the tracer cloud flowing downstream there is a bridge in the first sub reach that can cause local settling and scouring of sediment resulting in formation of storage zones the tracer cloud was then transported through a braided channel the channel irregularities of which causes the tracer to become partially trapped as shown in figs 7b and c thus these channel banks and shallow areas caused the storage effect supporting the applicability of the tsm at gc the hydraulic and channel geometric data obtained in the test are summarized in table 3 the mean velocity and channel width in gc 2019 were much greater than those in gc 2020 due to the higher discharge even though the mean depths were almost the same during the two tests 4 2 numerical simulations the reactive transport simulations were conducted in the above test bed the transient storage parameters were estimated fitting the simulated concentration curves to the btcs identified in the tracer tests using the inverse modelling suggested by noh et al 2019 and estimated values of the four parameters for each test were listed in table 3 the optimization results show that the estimated values of the dispersion coefficient in sub reach rc3 for both tests were significantly larger than the values in the other sub reaches whereas the mass exchange rate in rc3 was lower than those of the other sub reaches the results indicated that tracer mixing in this reach was more dominated by shear dispersion in the flow zone than storage effects in the storage zones for reactive chemicals the required reactivity properties can be obtained using chemical property calculators such as epi suite and sparc online calculator developed by the us epa us epa 2012 arp h p h et al 2010 archem in this study in addition to the rwt more scenarios with toluene phenol and sulfuric acid were simulated under the same flow conditions the required properties of the three reactants for reactivity parameters are listed in table 4 figs 8 and 9 illustrate the btcs according to the tracer tests and the simulations for the four scenarios at each section of the creek by way of comparison between the measured btcs and the parameter calibrated btcs the determination coefficients r2 of 0 99 and 0 96 were yielded from cases of gc2019 and gc2020 respectively to quantitatively compare the results of the three reactive transport simulations 8 statistical features were extracted from the resulting btcs and the percentage changes relative to the conservative transport results were calculated as plotted in fig 10 the abbreviations of the features are also noted in the description of fig 10 the results revealed that the features related to mass and concentration such as max c and auc were more sensitive than the others due to the decay terms since the volatilization is proportional to flow velocity the changes in concentration of the cases of gc2020 were lower than those of gc2019 as the latter had a lower flow rate since sorption mechanism delays the transport of the solute cloud similarly to the storage effect compared to the tp changes of sulfuric acid and benzene those of toluene were relatively high owing to its high octanol water partition coefficient this study contains some obvious limitations first in the process of biodegradation rate estimation the biochemical reactivity can be represented with a biodegradation half life and the model requires a specific value for that half life however the half life depends on the surrounding environment such as ph or temperature and is normally suggested in an approximate amount of time for reliability this variance could yield significant errors in the simulation results second for the volatilization mechanism the developed model says that the derivative of solute concentration with respect to time is proportionally affected by the concentration gradient and diffusivity technically however volatilization is normally driven by the relative fugacity gradient which is a measure of how easy or difficult it is to breakaway from one phase to the other the fugacity of a chemical depends on the surrounding pressure and temperature as well as its concentration according to henry s law atkins 2006 be that as it may this study does not consider these conditions third although the developed model is universally applicable to contaminant of various species the simulation results cannot be comparable to actual phenomena due to the impossibility of carrying out a toxic chemical tracer test as mentioned above 4 3 sensitivity analysis for decay parameters although the reactivity parameters were theoretically and empirically estimated to reflect the actual phenomena the parameters might not be compatible to those of the transport model thus we analyzed the sensitivity of the model with respect to the decay parameters in order to evaluate how the parameters function in the model for two decay mechanisms under consideration the degree of decay γ d was set as a target value representing how much of the solute mass is reduced compared to its primary mass as 22 γ d c d t c c d t where c c is the flow zone solute concentration when λ v and λ b are zero the γ d does not only depend on the decay parameters but also on how far and how fast the solute moves with the flow of the stream thus we employed the damkohler number d a i as the ratio of the reaction rate to the advective mass transport rate which normalizes the decay parameters from the flow conditions as 23 d a i λ l u where λ represents a decay parameter λb in here based on simulations of nearly 150 cases we ascertained that the relationships between γ d and d a i are independent of the transport distance and flow velocity as plotted in fig 11 since as was previously known the closer d a i is to 1 the less is the parameter uncertainty in ordinary cases wagner and harvey 1997 we strictly defined the significant regime as being bounded by 1 and 0 1 to evaluate the appropriate decay parameters under certain flow conditions the over reacting regime significant regime and ignorable regime in the significant regime the damkohler number monotonically increased with increasing γ d thus a single relation between the damkohler number and the degree of decay exists as 24 γ d 1 exp d a i 5 conclusion even though there have been several past attempts to validate the reaction mechanism modelling in solute transport models using reactive tracers estimated parameters could not cover a wide range of types of reactants furthermore an actual toxic reactant such as benzene or toluene cannot be subjected to a tracer test due to safety concerns therefore this study employed theoretical and empirical methods to estimate the reactivity parameters which represent the inherent reactivity of a given reactant for reactive transport modelling and applied the developed numerical model to a natural river the governing equations of the numerical model were formulated to analyze the mixing and reactions of reactive contaminants in rivers the analytical solution was derived in laplace domain to verify the numerical model the results for the conservative contaminants revealed that the determination coefficients between the numerical model and the analytical solution was over 0 99 the numerical model was then applied to the natural rivers adopting advantage of its applicability for unconstrained boundary conditions and spatially varying parameters comparison between the measured btcs from tracer tests and the parameter calibrated btcs showed that the determination coefficients of 0 99 and 0 96 were yielded from the cases of gc2019 and gc2020 respectively the results of the application revealed that the simulation for relatively highly reactive reactants yielded a remarkable difference in resulting btcs compared to the conservative transport simulation this is particularly true for mass as benzene lost 57 7 of its initial mass 4 54 km downstream from the injection site this result demonstrated the inappropriateness of the conservative assumptions from the sensitivity analysis of the model with respect to the decay parameters we identified the logarithmic relationship between the damkohler number with input conditions and the resulting degree of decay which was found to be independent of reach length and flow velocity within the ignorable regime r d was less than 10 meaning that the decay mechanism would not be considered for the sake of brevity and economy of computation time on the other hand r d was over 70 within the over reacting regime indicating that the reaction rate was over estimated so that the other mixing mechanisms can be muted to function given these results we recommended that during the parameter estimation stage the decay parameters should be assessed with the damkohler number to simultaneously maintain the accuracy of the rstm and reflect the reaction characteristics of contaminants in the solute transport model declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this research was supported by the korea agency for infrastructure technology advancement grant funded by the ministry of land infrastructure and transport grant 19dpiw c153746 01 and the bk21 plus research program of the national research foundation of korea this research work was conducted at the institute of engineering research and institute of construction and environmental engineering in seoul national university seoul south korea the authors would like to express their sincere gratitude to j s kim d h baek s h yun and j y bang of seoul national university for their valuable contribution to the field work appendix a information used for validation of the volatilization modelling table a1 chemical properties for 21 compounds related to volatilization and estimated volatilization rates at 25 c table a1 cas no name d 1 m2 day 1 h c c 2 log m m p 3 mmhg λ v sec 1 λ v w v o l w i n 4 sec 1 108 95 2 phenol 3 04e 05 4 18 0 35 1 55e 05 3 76e 08 71 43 2 benzene 8 46e 05 0 52 91 23 2 89e 05 3 94e 05 107 07 3 2 choloroethanol 3 30e 05 4 24 7 29 1 63e 05 9 27e 08 107 02 8 acrolein 9 87e 05 2 37 275 10 3 17e 05 1 31e 05 78 53 5 amiton 3 15 e 05 3 52 0 01 1 58e 05 1 87e 11 7726 95 6 bromine 8 90 e 05 0 61 215 21 2 98e 05 2 88e 05 76 06 2 chloropicrin 7 87 e 05 0 67 25 89 2 76e 05 2 47e 05 506 77 4 cyanogen chloride 1 04 e 04 1 35 1238 48 3 28e 05 4 65e 05 75 21 8 ethylene oxide 1 15 e 04 2 00 1305 40 3 48e 05 5 83e 05 74 90 8 hydrogen cyanide 1 27 e 04 1 89 742 03 3 70e 05 6 83e 05 51 75 2 mechlorethamine 4 97 e 05 3 39 0 17 2 09e 05 2 53e 07 74 83 9 methyl bromide 1 11 e 04 0 86 1646 90 3 40e 05 3 63e 05 78 93 3 methyl ethyl ketone 8 18 e 05 2 50 90 02 2 83e 05 6 28e 06 505 60 2 mustard gas 6 52 e 05 2 61 0 11 2 46e 05 6 14e 06 75 44 5 phosgene 1 02 e 04 1 34 1406 07 3 23e 05 3 64e 05 10025 87 3 phosphorus oxychloride 8 34 e 05 0 51 35 73 2 86e 05 7 78e 07 7719 12 2 phosphorus trichloride 8 46 e 05 0 99 119 08 2 89e 05 1 69e 05 77 81 6 sarin 6 17 e 05 0 76 2 86 2 38e 05 5 29e 08 10025 67 9 sulfur monochloride 7 64 e 05 1 94 8 83 2 71e 05 3 16e 05 7719 09 7 thionyl chloride 8 00 e 05 2 28 117 88 2 79e 05 2 44e 07 108 88 3 toluene 7 60 e 05 0 45 28 45 2 70e 05 3 66e 05 1 diffusivity calculated using the sparc online calculator archem arp h p h et al 2010 2 dimensionless henry s law constant which is a ratio of the aqueous phase molarity of a compound and its gas phase molarity 3 vapor pressure calculated using the antoine equation antoine c 1888 4 volatilization rate estimated from wvolwin epi suite 
25881,solute transport models are widely employed to the predict spatio temporal fate of contaminants in rivers however most previous studies have assumed that contaminants spilled are unreactive they disregard the inherent reactivity of contaminants in water the reactive solute transport model rstm gooseff et al 2005 includes lumped decay terms to reflect the decayability of the chemicals but its applicability was still poor due to dependence on an optimization method the purpose of this study is to perform reactive transport modelling considering chemicals reactivities for sorption volatilization and biodegradation to this end we manipulated the governing equations of the rstm and suggested theoretical and empirical methods of estimating the key parameters of the reaction terms the results showed for example that benzene lost 57 7 of its primary mass after being transported 4 54 km downstream due to its high volatility also the arrival time of toluene was delayed by 10 4 due to adsorption keywords reactive solute transport model analytical solution tracer test sorption volatilization biodegradation 1 introduction worldwide industrialization has caused a dramatic rise in chemical usage in industrial complexes which has in turn led to spill accidents of harmful contaminant in november 2014 a truck overturned and spilled 2 000 l of sulfuric acid into nakdong river the longest river in south korea which resulted in severe damage to the aquatic ecosystem and water quality such unpredictable accidents in rivers are especially serious since most of the water supply in south korea is dependent on surface waters such as rivers and dam reservoirs having recognized the problem many researchers have employed the solute transport models to predict the behavior and fate of contaminants transported in the rivers however even though many solute transport analyses have been conducted in recent decades few of these studies attempted to analyze reactive transport of chemicals as it is difficult to use toxic chemicals in a field test due to safety concerns most chemicals are inherently reactive in water for example methyl isocyanate is extremely toxic but cannot be subjected to study since half of them is consumed in few minutes in excess water castro et al 1985 some hydrogen compounds dissociate or ionize almost completely in water some compounds with high vapor pressure or high henry s law constant may lose their mass when flowing downstream in a river due to their volatility therefore the models need to consider these reaction characteristics to accurately predict the behavior of chemicals in rivers in order to meet these needs the otis p runkel and broshears 1991 runkel and chapra 1993 runkel 1998 gooseff et al 2005 includes a sorption algorithm and first order decay terms on the governing equation of the transient storage model tsm the tsm is the most prominent and popular model because it can accurately simulate the skewness of a breakthrough curve btc bencala and walters 1983 afterwards gooseff et al 2005 referred to the expanded model of tsm as the reactive solute transport model rstm in their study gooseff et al 2005 estimated the first order decay parameters using an optimization method with experimental data on nitrogen dioxide from a tracer test in green creek antarctica the sorption parameter was set to an arbitrary value likewise o connor et al 2010 employed the improved inverse model to estimate the decay parameters haggerty et al 2009 used resazurin as a smart tracer to provide additional biochemical information in streams these efforts to estimate the reaction parameters that yield the best fitting output have confirmed the validity of the first order decay terms however the methods of parameter estimation have a practical limitation in that the parameters optimized for one chemical cannot be applied to another type of chemical to address the above limitation the current study attempts to present practical methods for reaction analysis of chemicals in rivers the governing equations of the rstm were manipulated the numerical model was constructed using the finite difference method fdm and crank nicolson method in addition an analytical solution of the manipulated governing equations was devised in order to verify the numerical model to determine the reactivity parameters the inherent properties of chemicals were conjugated based on the appropriate theoretical and empirical methods since an estimation of conservative transport parameters should precede reactive transport parameter estimation wagner and harvey 1999 tracer tests using rhodamin wt were conducted at gam creek south korea in 2019 and 2020 the test bed was also subjected to the reactive transport simulations by extension the simulation results were used for the sensitivity analysis of the model with respect to the decay parameters a flow chart of model development and validation is shown in fig 1 2 model development 2 1 governing equation when a chemical is accidently introduced into a river the rstm can be used for transport analysis considering advection by current dispersion by turbulence storage effect sorption on the streambed and first order decay in this study the channel was divided into three domains the flow zone storage zone and streambed zone as shown in fig 2 the governing equations corresponding to each domain were formulated by reflecting the phenomena of volatilization and biodegradation as also shown in fig 2 as follows 1a c t q a f c x 1 a f x a f k c x α c s c λ s c b c e q λ v c λ b c 1b c s t α a f a s c c s λ b c s 1c c b t λ s c e q c b λ b c b where c c s and c b g m 3 denote the solute concentration within the flow zone storage zones and sediment on the streambed respectively q m3 s 1 denotes the volumetric flow rate a f and a s m2 denote the cross sectional area of the flow zone and storage zones respectively k m2 s 1 denotes the dispersion coefficient α denotes the mass exchange rate between the flow zone and storage zones s 1 c e q g m 3 denotes the equilibrium sorbate concentration on the streambed λ s λ v and λ b s 1 denote the kinetic exchange rate by sorption volatilization and biodegradation respectively and t is a time variable the x coordinate was taken to be approximately streamwise distance the sorption mechanism was interpreted as a mass transfer between the flow zone and streambed zone depending on the equilibrium sorbate concentration on the streambed and the first order kinetic coefficient the single decay term of the rstm was divided into two components volatilization and biodegradation which are influential in the mass loss of organic chemicals in rivers for these governing equations a numerical model was constructed using the python programming language the continuous solute concentration distributions in the three domains were spatially and temporally discretized using the fdm and the crank nicolson method following the procedures used to build the otis model runkel an broshears 1991 2 2 reactivity parameter estimation for the reactive solute transport simulation the first order kinematic modelling for the reaction mechanisms and the estimation methods for the reactivity parameters were delineated the governing equations of the model contain three reactivity parameters sorption rate λ s volatilization rate λ v and biodegradation rate λ b these parameters represent the chemicals inherent reactivity i e how quickly they are affected by the corresponding mechanisms in this study they were modeled by the explicit methods given below 2 2 1 sorption rate λ s contaminants that spill into a river are adsorbed and desorbed on sediment above the streambed this mechanism can significantly affect the mixing behavior of contaminants in rivers in the sense that it delay solute transport by trapping into a non flowing area the sorption mechanism could be interpreted as the same process as the transient storage effect at the storage zones however whereas the transient storage effect is simply understood as the first order mass transfer driven by the concentration difference between the flow zone and storage zones the rate of adsorption and desorption significantly differ due to the resistance to release of accumulated sorbate on sediment di toro and horzempa 1982 karickhoff and morris 1985 the degree of this resistance varies drastically by type of chemical substance and is represented by the partition coefficient which is conventionally defined as the ratio of sorbate phase concentration to solution phase concentration of the contaminant in the equilibrium state in this study the sorption mechanism was simplified following the model of runkel runkel 1998 gooseff et al 2005 the sorption term was manipulated in this model by saying that the partial derivative of the solute concentration is proportionally affected by the disproportionality of current sorbate concentration and equilibrium sorbate concentration as 2 c t λ s c b c e q c e q k p ρ b c where c e q g m 3 is the equilibrium sorbate concentration on the streambed λ s s 1 is the sorption rate k p m3 g 1 is the partition coefficient of a chemical which can be determined using the organic carbon water partition coefficient k oc and the fraction of organic carbon in sediment f oc karickhoff et al 1979 abdul et al 1986 as 3 k p f o c k o c herein f o c was set at the default value of 0 2 0 002 g g as suggested in the soil screening guidance 1996 developed by the us environmental protection agency us epa to estimate k oc the linear regression between k oc and the octanol water partition coefficient k ow was employed following karickhoff et al 1979 as 4 k o c 0 63 k o w r 2 0 96 ρ b g m 3 is the ratio of sediment mass within the mixing layer on the streambed and the waterbody volume the mixing layer is the column in which sorptive mass exchange occurs ρ b can be approximately defined as 5 ρ b δ m h 1 φ ρ s where δ m m is mixing layer thickness h m is water depth φ is porosity of streambed and ρ s g m 3 is the sediment density in the application of this study the δ m h φ and ρ s were assumed to be constant values of 0 1 0 4 and 2 65 g cm 1 respectively based on the relationship between varying chemicals and sediments the kinetic sorption rate λ s is inversely proportional to the equilibrium partition coefficient k p karickhoff and morris 1985 linear regression analysis using the suite of data characterized by hassett and means 1980 was used to obtain the relationship between the characteristic time and k p as 6 1 λ s h o u r s 0 03 k p in sum once the octanol water partition coefficient of a given contaminant is obtained k p can be empirically estimated so that λ s can be identified for sorption mechanism modelling 2 2 2 volatilization rate λ v in the process of chemical transport in a river the volatilization into the atmosphere and resulting mass degradation occur depending on the chemical s volatility this volatility is normally indicated by the chemical s molecular weight or vapor pressure smith et al 1980 we embodied the volatilization mechanism in the transport model as a first order mass transfer through a two phase boundary layer which is referred by whitman 1923 to as the two film theory this theory holds that volatility is dominated by the diffusion resistance within the boundary layers the two film theory relies on several assumptions first mass gradients within the quiescent boundary layers are linear due to molecular diffusion as illustrated in fig 3 second the ratio of the concentrations on the air water interface as notated in fig 3 with c w i n t and c a i n t respectively can be defined as the henry s law constant lastly there is no mass accumulation at the interface meaning that the mass fluxes in both the liquid phase film and the gas phase film are identical the flux can be estimated by one dimensional fick s law 7 j d m δ c c i n t k c c i n t where j g m 2 sec 1 is the mass flux through the boundary layer d m m2 sec 1 is the molecular diffusivity δ is the boundary layer thickness c g m 3 is the bulk concentration c int g m 3 is the concentration on the interface and k m s 1 is the mass transfer rate since the concentration on the interface cannot be directly measured eq 7 was converted by applying the second assumption 8 j o u t w a r d k c w c a h c c 9 1 k 1 k w 1 h c c k a where the subscripts w and a indicate the value in water and in the atmosphere respectively as shown in fig 3 and h cc is the dimensionless henry s law constant which is the ratio of the aqueous phase molarity of a substance to its gas phase molarity since the volume of the atmosphere is infinitely large compared to that of the waterbody as sufficient time passes c a i n t approaches c a near zero in other words the atmosphere functions as an infinite sink accordingly the change in only c w over time is of interest and the aqueous phase film is used as a reference under these conditions to convert the flux into the concentration change over time eq 8 was multiplied by the area of the air water interface and divided by the domain water volume as 10 c w t λ v c w c i n t 0 where λ v sec 1 is the mass transfer rate hence forth called the volatilization rate the volatilization rate has a linear relationship with the diffusion coefficient of a chemical smith et al 1980 revealed that the ratio of diffusion coefficients of any two substances does not depend on the effects of temperature and viscosity that is the volatilization rate of a certain contaminant can be estimated in relation to the value of oxygen because the oxygen transfer rate referred to as the reaeration rate and the diffusivity of oxygen are relatively easy to obtain from previous studies the relation was proposed as 11 λ v λ r d d o n where λ v s 1 is the volatilization rate of a contaminant λ r s 1 is the reaeration rate d and d o m2 s 1 are the aqueous phase diffusivity of the contaminant and oxygen respectively and n is the modulus of relation which is not 1 because of the apparent incongruity relating to the quiescence assumption of the boundary layer smith et al 1980 in this study the empirical value of 0 61 was applied suggested by smith et al 1980 the oxygen reaeration rates in streams have been reasonably fitted by the following equation o conner and dobbins 1958 12 λ r d o u 0 5 h 1 5 where u m s 1 is mean flow velocity and h m is mean water depth since these two hydraulic variables are involved in the relation volatilization is affected not only by volatility but also by the flow conditions in the river to evaluate the validity of the above estimation method the correlation between the estimated λ r of a substance and its vapor pressure was assessed in consideration of the fact that the vapor pressure can represent a degree of volatility if this comparison reveals a high linear proportionality we can indirectly confirm that λ v reasonably represents the volatility by the transitive relation furthermore cross validation was also conducted with the existing volatilization model for both lakes and rivers referred to as wvolwin in the estimation program interface epi suite developed by the us epa based upon an adaptation of the method suggested by thomas 1982 wvolwin is used to compute the volatilization rate at 25 c 298k considering henry s law constant flow velocity water depth and the molecular weights of the compounds for 21 compounds the properties of volatility and estimated λ v according to both the proposed method and wvolwin are listed in table a1 the comparison yielded a pearson correlation coefficients of 0 851 between the estimated λ v and the vapor pressure and 0 832 between λ v and λ v w v o l w i n because the input data and the estimation method used in this study were apparently different from those used for wvolwin model λ v and λ v w v o l w i n would not be expected to be the same even so the resulting correlation which is plotted in fig 4 shows that the proposed estimation method can reasonably be used to represent the inherent volatility of the compounds 2 2 3 biodegradation rate λ s organic chemicals in natural rivers commonly lose mass due to biochemical reactions and the remaining chemical concentration can be plotted logarithmically with respect to time by first order kinetics this first order kinetics is also referred to as a half life kinetics because the half life represents the elapsed time for a chemical to halve alexander 1999 the biodegradation half life t h has a relation with the first order biodegradation rate λ b 13 λ b ln 2 t h the epi suite from the us epa offers several biodegradation models in which an indication of biodegradation rate is provided in relative terms of time range such as hours days and so on boethling et al 1994 one of the models biohcwin can estimate the biodegradation half life using hydrocarbon content howard et al 2005 the provided half life value was converted to the biodegradation rate using eq 13 3 model validation 3 1 derivation of analytical solution in this study to validate the developed numerical model the analytical solution of the governing equation for flow zone was derived in laplace domain following kazezyılmaz alhan 2008 who derived an analytical solution for tsm without regard of the terms for sorption and decay within the governing equations once the time variable is converted to the laplace variable s the laplace transformed concentration in the storage zone and the streambed zone notated as c s and c b respectively have a linear relationship with respect to the transformed flow zone concentration c 14 k 2 c x 2 u c x α c s α c λ s c b λ s k p ρ c λ v λ b c s c c t 0 0 15 c s β c w h e r e β α a f a s α a f a s λ b s 16 c b γ c w h e r e γ k s k p ρ k s λ b s where 17 c x s l c x t 0 e s t c d t substituting eq 15 and eq 16 into eq 14 the governing equation for the flow zone becomes a form of homogeneous ordinary differential equation ode as 18 k f d 2 c d x 2 u d c d x α β α λ s γ λ s k p ρ b λ v λ b s c 0 kazezyılmaz alhan 2008 derived the analytical solution for the tsm without regard to the sorption and decay terms following the work of kazezyılmaz alhan 2008 the general solution of the ode was obtained as 19 c x s c 0 s exp u u 2 4 k α β α λ s γ λ s k p ρ b λ v λ b s 2 k x the boundary condition was set as the laplace transformed heaviside function signifying that the contaminant is introduced from time t 1 to t 2 with concentration of c i n i 20 c 0 s c i n i s exp t 1 s exp t 2 s for inversion of the laplace transform the approximation polynomials which are precalculated are normally listed in tables however because we are unable to easily increase the order of the polynomials the numerical inversion method based on the bromwich integral was employed in this study to analyze the laplace transform of the solution valsa and brančik 1998 the suggested inversion algorithm was coded in matlab 3 2 validation of numerical model for a comparison of outcomes between the proposed numerical model and the analytical solution under identical conditions a virtual prismatic channel and simulation scenarios with a conservative solute case c and toluene case r were set up table 1 summarizes the input data of the transient storage parameters and the required properties of toluene used for the reactivity parameter estimation here case r was divided into three runs r 1 for sorption only r 2 for volatilization and biodegradation and r 3 for all three reaction processes in these simulations 1 ppm of both conservative contaminant and the toluene were released at the inlet for 30 s and simulated btcs were compared at sections 500 m 1 000 m and 1 500 m downstream of the inlet hereafter s1 s2 and s3 respectively the results revealed the validity of the numerical model showing a determination coefficient r2 of over 0 99 as plotted in fig 5 a as shown in fig 5b the btcs for case c and case r were fairly different while the mass conservation rates of the conservative solute at all sections were nearly 1 not exactly 1 due to truncation error those of toluene at s1 s2 and s3 were 0 98 0 95 and 0 92 respectively indicating that its mass decayed as it flows downstream table 2 lists several results of the comparison with different reaction mechanisms considered case r 1 which considers sorption only showed that sorption did not cause mass loss but delayed solute transport with a time lag before peak concentration on the other hand in case r 2 8 1 of the initial mass decayed and there was no temporal difference in the resulting btc as plotted in fig 5b 4 model application 4 1 experimental data since estimation of the conservative transport parameters is necessary prior to the improved reactive transport parameter estimation wagner and harvey 1999 tracer tests were carried out at gam creek gc the gc basin is located across an urban settlement in gimcheon and an agricultural area in gumi it plays an important role in supplying industrial and agricultural water and is at risk of contaminant spill accidents from nearby industrial complexes the whole reach of gc is gently meandering and the streambed is predominantly composed of sand substrate with little vegetation two of the tracer tests were conducted at the 4 85 km reach of gc in october 2019 gc2019 and the 4 54 km reach in june 2020 gc2020 including the injection point 5 cross sections along the reach were surveyed and the reach was divided into 4 sub reaches as illustrated in fig 6 to apply the one dimensional model the channel characteristics of each sub reach were assumed to be uniform therefore the key to parameter estimation was to determine the appropriate parameters representing in reach features a single sub reach should have relatively uniform flow properties and the corresponding parameters should be representative of the sub reach as a whole in these tests rhodamine wt rwt was used as a conservative tracer due to its visibility and low detection limit the tracer was simultaneously injected at multiple points in a transverse row because it has to be fully mixed in both the horizontal and vertical directions for the one dimensional mixing analysis in addition since the first measurement section s1 must be far enough from the injection point i p to ensure complete mixing the required distance for the multi point injection experiment was calculated using the following equation kilpatrick 1989 21 l 0 0 1 1 n 2 u w 2 e z where l 0 is the distance required for both complete horizontal and vertical complete mixing n is the number of injection points w is the mean stream top width and e z is the lateral mixing coefficient which is estimated from e z h u 0 15 fischer 1979 ysi 600oms fluorometers were used to measure the released rwt all of which were calibrated using rwt solutions in the range of 0 200 ppb the error range of the fluorometer is 5 of the reading values or 1 ppb the sensing rate was set to 0 25 hz to obtain an average concentration for each cross section three or four fluorometers were uniformly installed in the transverse direction in the four measurement sections s1 to s4 the hydraulic and geometric properties including flow rate velocity profiles water depth and bottom slope were measured using a sontek flowtracker acoustic doppler velocimeter and sokkia grx1 of real time kinematic global positioning system rtk gps fig 7 shows a view of the test reach at gam creek and the behavior of the tracer downstream of the injection point fig 7a shows the shape of the tracer cloud flowing downstream there is a bridge in the first sub reach that can cause local settling and scouring of sediment resulting in formation of storage zones the tracer cloud was then transported through a braided channel the channel irregularities of which causes the tracer to become partially trapped as shown in figs 7b and c thus these channel banks and shallow areas caused the storage effect supporting the applicability of the tsm at gc the hydraulic and channel geometric data obtained in the test are summarized in table 3 the mean velocity and channel width in gc 2019 were much greater than those in gc 2020 due to the higher discharge even though the mean depths were almost the same during the two tests 4 2 numerical simulations the reactive transport simulations were conducted in the above test bed the transient storage parameters were estimated fitting the simulated concentration curves to the btcs identified in the tracer tests using the inverse modelling suggested by noh et al 2019 and estimated values of the four parameters for each test were listed in table 3 the optimization results show that the estimated values of the dispersion coefficient in sub reach rc3 for both tests were significantly larger than the values in the other sub reaches whereas the mass exchange rate in rc3 was lower than those of the other sub reaches the results indicated that tracer mixing in this reach was more dominated by shear dispersion in the flow zone than storage effects in the storage zones for reactive chemicals the required reactivity properties can be obtained using chemical property calculators such as epi suite and sparc online calculator developed by the us epa us epa 2012 arp h p h et al 2010 archem in this study in addition to the rwt more scenarios with toluene phenol and sulfuric acid were simulated under the same flow conditions the required properties of the three reactants for reactivity parameters are listed in table 4 figs 8 and 9 illustrate the btcs according to the tracer tests and the simulations for the four scenarios at each section of the creek by way of comparison between the measured btcs and the parameter calibrated btcs the determination coefficients r2 of 0 99 and 0 96 were yielded from cases of gc2019 and gc2020 respectively to quantitatively compare the results of the three reactive transport simulations 8 statistical features were extracted from the resulting btcs and the percentage changes relative to the conservative transport results were calculated as plotted in fig 10 the abbreviations of the features are also noted in the description of fig 10 the results revealed that the features related to mass and concentration such as max c and auc were more sensitive than the others due to the decay terms since the volatilization is proportional to flow velocity the changes in concentration of the cases of gc2020 were lower than those of gc2019 as the latter had a lower flow rate since sorption mechanism delays the transport of the solute cloud similarly to the storage effect compared to the tp changes of sulfuric acid and benzene those of toluene were relatively high owing to its high octanol water partition coefficient this study contains some obvious limitations first in the process of biodegradation rate estimation the biochemical reactivity can be represented with a biodegradation half life and the model requires a specific value for that half life however the half life depends on the surrounding environment such as ph or temperature and is normally suggested in an approximate amount of time for reliability this variance could yield significant errors in the simulation results second for the volatilization mechanism the developed model says that the derivative of solute concentration with respect to time is proportionally affected by the concentration gradient and diffusivity technically however volatilization is normally driven by the relative fugacity gradient which is a measure of how easy or difficult it is to breakaway from one phase to the other the fugacity of a chemical depends on the surrounding pressure and temperature as well as its concentration according to henry s law atkins 2006 be that as it may this study does not consider these conditions third although the developed model is universally applicable to contaminant of various species the simulation results cannot be comparable to actual phenomena due to the impossibility of carrying out a toxic chemical tracer test as mentioned above 4 3 sensitivity analysis for decay parameters although the reactivity parameters were theoretically and empirically estimated to reflect the actual phenomena the parameters might not be compatible to those of the transport model thus we analyzed the sensitivity of the model with respect to the decay parameters in order to evaluate how the parameters function in the model for two decay mechanisms under consideration the degree of decay γ d was set as a target value representing how much of the solute mass is reduced compared to its primary mass as 22 γ d c d t c c d t where c c is the flow zone solute concentration when λ v and λ b are zero the γ d does not only depend on the decay parameters but also on how far and how fast the solute moves with the flow of the stream thus we employed the damkohler number d a i as the ratio of the reaction rate to the advective mass transport rate which normalizes the decay parameters from the flow conditions as 23 d a i λ l u where λ represents a decay parameter λb in here based on simulations of nearly 150 cases we ascertained that the relationships between γ d and d a i are independent of the transport distance and flow velocity as plotted in fig 11 since as was previously known the closer d a i is to 1 the less is the parameter uncertainty in ordinary cases wagner and harvey 1997 we strictly defined the significant regime as being bounded by 1 and 0 1 to evaluate the appropriate decay parameters under certain flow conditions the over reacting regime significant regime and ignorable regime in the significant regime the damkohler number monotonically increased with increasing γ d thus a single relation between the damkohler number and the degree of decay exists as 24 γ d 1 exp d a i 5 conclusion even though there have been several past attempts to validate the reaction mechanism modelling in solute transport models using reactive tracers estimated parameters could not cover a wide range of types of reactants furthermore an actual toxic reactant such as benzene or toluene cannot be subjected to a tracer test due to safety concerns therefore this study employed theoretical and empirical methods to estimate the reactivity parameters which represent the inherent reactivity of a given reactant for reactive transport modelling and applied the developed numerical model to a natural river the governing equations of the numerical model were formulated to analyze the mixing and reactions of reactive contaminants in rivers the analytical solution was derived in laplace domain to verify the numerical model the results for the conservative contaminants revealed that the determination coefficients between the numerical model and the analytical solution was over 0 99 the numerical model was then applied to the natural rivers adopting advantage of its applicability for unconstrained boundary conditions and spatially varying parameters comparison between the measured btcs from tracer tests and the parameter calibrated btcs showed that the determination coefficients of 0 99 and 0 96 were yielded from the cases of gc2019 and gc2020 respectively the results of the application revealed that the simulation for relatively highly reactive reactants yielded a remarkable difference in resulting btcs compared to the conservative transport simulation this is particularly true for mass as benzene lost 57 7 of its initial mass 4 54 km downstream from the injection site this result demonstrated the inappropriateness of the conservative assumptions from the sensitivity analysis of the model with respect to the decay parameters we identified the logarithmic relationship between the damkohler number with input conditions and the resulting degree of decay which was found to be independent of reach length and flow velocity within the ignorable regime r d was less than 10 meaning that the decay mechanism would not be considered for the sake of brevity and economy of computation time on the other hand r d was over 70 within the over reacting regime indicating that the reaction rate was over estimated so that the other mixing mechanisms can be muted to function given these results we recommended that during the parameter estimation stage the decay parameters should be assessed with the damkohler number to simultaneously maintain the accuracy of the rstm and reflect the reaction characteristics of contaminants in the solute transport model declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this research was supported by the korea agency for infrastructure technology advancement grant funded by the ministry of land infrastructure and transport grant 19dpiw c153746 01 and the bk21 plus research program of the national research foundation of korea this research work was conducted at the institute of engineering research and institute of construction and environmental engineering in seoul national university seoul south korea the authors would like to express their sincere gratitude to j s kim d h baek s h yun and j y bang of seoul national university for their valuable contribution to the field work appendix a information used for validation of the volatilization modelling table a1 chemical properties for 21 compounds related to volatilization and estimated volatilization rates at 25 c table a1 cas no name d 1 m2 day 1 h c c 2 log m m p 3 mmhg λ v sec 1 λ v w v o l w i n 4 sec 1 108 95 2 phenol 3 04e 05 4 18 0 35 1 55e 05 3 76e 08 71 43 2 benzene 8 46e 05 0 52 91 23 2 89e 05 3 94e 05 107 07 3 2 choloroethanol 3 30e 05 4 24 7 29 1 63e 05 9 27e 08 107 02 8 acrolein 9 87e 05 2 37 275 10 3 17e 05 1 31e 05 78 53 5 amiton 3 15 e 05 3 52 0 01 1 58e 05 1 87e 11 7726 95 6 bromine 8 90 e 05 0 61 215 21 2 98e 05 2 88e 05 76 06 2 chloropicrin 7 87 e 05 0 67 25 89 2 76e 05 2 47e 05 506 77 4 cyanogen chloride 1 04 e 04 1 35 1238 48 3 28e 05 4 65e 05 75 21 8 ethylene oxide 1 15 e 04 2 00 1305 40 3 48e 05 5 83e 05 74 90 8 hydrogen cyanide 1 27 e 04 1 89 742 03 3 70e 05 6 83e 05 51 75 2 mechlorethamine 4 97 e 05 3 39 0 17 2 09e 05 2 53e 07 74 83 9 methyl bromide 1 11 e 04 0 86 1646 90 3 40e 05 3 63e 05 78 93 3 methyl ethyl ketone 8 18 e 05 2 50 90 02 2 83e 05 6 28e 06 505 60 2 mustard gas 6 52 e 05 2 61 0 11 2 46e 05 6 14e 06 75 44 5 phosgene 1 02 e 04 1 34 1406 07 3 23e 05 3 64e 05 10025 87 3 phosphorus oxychloride 8 34 e 05 0 51 35 73 2 86e 05 7 78e 07 7719 12 2 phosphorus trichloride 8 46 e 05 0 99 119 08 2 89e 05 1 69e 05 77 81 6 sarin 6 17 e 05 0 76 2 86 2 38e 05 5 29e 08 10025 67 9 sulfur monochloride 7 64 e 05 1 94 8 83 2 71e 05 3 16e 05 7719 09 7 thionyl chloride 8 00 e 05 2 28 117 88 2 79e 05 2 44e 07 108 88 3 toluene 7 60 e 05 0 45 28 45 2 70e 05 3 66e 05 1 diffusivity calculated using the sparc online calculator archem arp h p h et al 2010 2 dimensionless henry s law constant which is a ratio of the aqueous phase molarity of a compound and its gas phase molarity 3 vapor pressure calculated using the antoine equation antoine c 1888 4 volatilization rate estimated from wvolwin epi suite 
25882,qes winds is a fast response wind modeling platform for simulating high resolution mean wind fields for optimization and prediction the code uses a variational analysis technique to solve the poisson equation for lagrange multipliers to obtain a mean wind field and gpu parallelization to accelerate the numerical solution of the poisson equation qes winds benefits from cuda dynamic parallelism launching the kernel from the gpu to speed up calculations by a factor of 128 compared to the serial solver for a domain with 145 million cells the dynamic parallelism enables qes winds to calculate mean velocity fields for domains with sizes of 10 km 2 and horizontal resolutions of 1 3 m in under 1 min as a result qes winds is a numerical code suitable for computing high resolution wind fields on large domains in real time which can be used to model a wide range of real world problems including wildfires and urban air quality keywords qes winds poisson equation fast response wind modeling iterative method staggered grid 1 introduction the urban population of the world has grown rapidly from 751 million in 1950 to 4 2 billion in 2018 in 2018 cities contained 55 of the world s population and by 2050 urban areas will account for 68 of the world s population united nations and department of economic and social affairs 2019 this rapid urbanization creates multiple meteorological and climate related phenomena linked to negative human health outcomes including air pollution shukla and parikh 1992 and urban heat island effects akbari and kolokotsa 2016 additionally urban population growth expands the wildland urban interface increasing the risk to life and property from wildfires radeloff et al 2018 calkin et al 2014 the risk is compounded by the increased number of wildfires over the past three decades since 2000 at least 10 states in the united states have had their largest fires on record and currently fire seasons are 78 days longer than in the 1970s while over 70 000 communities are at risk of wildfires usda 2019 proper modeling of the physics of wildfires moody et al 2019 linn et al 2020 and pollution dispersion in cities pardyjak and brown 2001 williams et al 2004 singh et al 2008 requires high resolution representation of wind fields in natural and urban areas for the purpose of prediction where model run times should be near or faster than real time or for design optimization problems where thousands of simulations must be performed in a short period of time traditional computational fluid dynamics cfd models such as reynolds averaged navier stokes rans simulations and large eddy simulations les are not fast enough hayati et al 2019 another option is to use a semi empirical fast response approach the complexity of urban and natural land surface geometries along with the complicated resulting wind flow requires sophisticated physical models however more detailed models require a significant increase in computational costs time and computational power as a result fast response wind flow simulators are needed that compromise some accuracy to shorten computation time the quick environmental simulation qes tool is a microclimate simulation platform for computing the transport of three dimensional environmental scalars in urban areas and over complex topography qes is organized into separate components each designed to simulate a different aspect of environmental transport qes winds is the fast response 3d diagnostic wind modeling module written in c based on the often used fortran code quic urb quick urban and industrial complex urban brown et al 2013 pardyjak and brown 2003 qes winds solves a mass conservation equation for the wind field rather than the slower and more physics based solvers that include conservation of momentum while qes winds uses reduced order physics to simulate urban flows the solutions are rapid and compare quite favorably with higher order physics based models in both idealized hayati et al 2017 2019 and realistic cities neophytou et al 2011 the qes winds model is based on the 3d diagnostic urban wind model proposed by röckle 1990 first an initial wind field is prescribed by combining an incident flow with localized flows that account for the effects of building geometries via empirical parameterizations singh et al 2008 conservation of mass is then enforced using a variational analysis a type of data assimilation technique sasaki 1958 sasaki 1970 sasaki 1970 to minimize the differences between the initial guess field and the final mass conserving wind field this technique requires the solution of a poisson equation for lagrange multipliers and results in calculating a quasi time averaged velocity field the resulting complex 3d wind field resembles time averaged experimental data hayati et al 2017 2019 the poisson equation is discretized over the computational domain and rearranged into matrix form creating a system of linear equations the matrix form of the poisson equation can theoretically be solved using sparse direct solvers which should be fast compared to iterative solvers however after applying the complex boundary conditions specific to our case the coefficient matrix a becomes a sparse non diagonal non banded non triangular non symmetric and not real positive diagonal matrix as a result the matrix must be solved using the m a 57 algorithm duff 2004 sparse symmetric system multifrontal method which is numerically expensive and quite slow for our purposes to overcome these shortcomings the poisson equation in qes winds is solved using the successive over relaxation sor method a variant the of gauss seidel method with faster convergence young 1954 varga 1962 the sor method is a sequential iterative method adams 1982 which means that it is not fast enough for the purpose of optimization and prediction for domains with a high number of cells when a large number of iterations is required for convergence to reduce the execution time of qes winds the sor method must be parallelized despite the sequential nature of sor poisson solvers they can be executed in parallel if the discretized equations are ordered according to the classical red black coloring scheme hayes 1974 lambiotte 1975 as illustrated in fig 1 in the red black scheme cells are divided into two partitions such that all of the neighboring cells of a red cell are black and vice versa as a result the discretized equation can be solved for two sub iterations in parallel once for all of the red cells and once for all of the black cells inside the domain adams 1982 evans 1984 utilizing parallel computing on cpus central processing units zapata et al 2018 krupka and šimecek 2010 and gpus graphics processing units helfenstein and koko 2012 li and saad 2013 cotronis et al 2014 konstantinidis and cotronis 2011 itu et al 2011 to accelerate the sor solver has been the subject of extensive research kruptka and šimeček 2010 showed that the achievable speedup by parallelizing on the cpu depends on the number of computational nodes available and size of the computational domain their results indicated that the maximum speedup for red black sor is equal to the number of computational nodes available on the gpu finding the maximum speedup is not as easy since various factors are involved all codes that run on the gpu need to be launched from the cpu and all data required for calculations must be copied from the cpu s memory to the gpu s global memory memory access time on the gpu and the bidirectional data transfer between the cpu and gpu are the most important parameters that affect the potential speedup all of the aforementioned research mainly focused on reducing memory access time on the gpu by using shared memory or padded global memory itu et al 2011 cotronis et al 2014 reported 11 times speedup using the global memory of an nvidia gtx480 gpu with 480 computational cores over a sequential solver on a cpu for a domain size of 2882 2882 cells in this paper we focus on reducing the copy overhead between the cpu and gpu to accelerate the sor solver cuda compute unified device architecture dynamic parallelism has been used in the literature jones 2012 kirk and wen mei 2016 ding and tan 2015 to reduce the bidirectional data transfer between the cpu and gpu another option is to rely on the gpu memory to hold the data during the whole iterative process there are other similar diagnostic wind modeling software packages that have been described in the literature most notably windninja forthofer et al 2014 and micro swift moussafir et al 2004 tinarelli et al 2007 windninja uses a conjugate gradient method with jacobi preconditioning to solve the poisson equation in a terrain following coordinate system forthofer et al 2014 the conjugate gradient method with jacobi preconditioning is computationally expensive and time consuming but it is the best option using terrain following coordinates as a result windninja is not the best option for simulating wind fields with fine grids e g of the order of 1 m in addition the terrain following coordinate system is not suitable for computing flows around buildings which means that windninja is not applicable for modeling urban areas micro swift utilizes the sor method to solve poisson s equation in a three dimensional cartesian coordinate system moussafir et al 2004 tinarelli et al 2007 however micro swift is not able to compute flows over complex terrain since it does not process terrain geometry in addition because micro swift only has a serial solver solving for high resolution wind fields over urban areas is computationally expensive because it has the same basic solver micro swift could benefit from the parallelization method described in this paper there have been few attempts to accelerate diagnostic wind models using gpus in the literature the study most similar to our effort was conducted by pinheiro et al 2017 they utilized gpu parallelization to accelerate a mass consistent wind model used in predicting atmospheric dispersion of radionuclides they used the winds extrapolated from stability and terrain west model which has an irrotational correction to the initial wind field that minimizes the divergence of the initial velocity field the irrotational correction is based on the perturbation velocity potential and transmission coefficients which are defined based on temperature profiles obtained from upper air soundings in this method they calculate the divergence in each cell and the gradient of the perturbation velocity potential they then update the velocity field and repeat the process until convergence which is negligible divergence homicz 2002 reviewed different wind modeling approaches and concluded that the west approach called the noabl model in the paper is different than models based on variational calculus e g qes winds also in order to get a divergence free final velocity field there are restrictions on the values of transmission coefficients which means that the model produces mass consistent wind fields in special circumstances pinheiro et al 2017 reported about 25 times speedup using an nvidia gtx 680 gpu for a domain with their finest grid about 1 5 million cells the gpu parallelization technique dynamic parallelism discussed here can be incorporated into other types of wind solvers and dispersion models to significantly accelerate them singh et al 2011 developed a new dispersion model called gpu plume that utilized the parallel computational capabilities available on the gpu to accelerate the calculations our group is developing an improved version of the gpu plume that has the potential to decrease the execution time by exploiting benefits of the dynamic parallelism also the new model in conjunction with the qes winds could run air quality simulations of large urban areas in near real time other examples include windstation which is software developed by lopes 2003 as a tool to simulate atmospheric flows over complex terrain two models were used to handle the complex topography the first one is a mass conservative wind solver much simpler than qes winds while the second one solved for three dimensional naiver stokes equations incorporating the same gpu technique described here in these models can speed up calculations and allow for high resolution simulations on massive domain including those with complex topography lastly linn et al 2020 recently developed a fast running tool to model complex behavior of wildland fire propagation they used quic urb brown et al 2013 pardyjak and brown 2003 as the wind solver to provide a high resolution wind field for the fire propagation model qes winds which evolved from quic urb can provide a wind to fire propagation models to predict wildfire behavior over larger domains in near real time the overall goal of this research is to significantly decrease the execution time required for qes winds nvidia s parallel gpu computing platform and cuda application programming interface api nvidia 2019 are used to substantially accelerate qes winds by utilizing gpu parallel computing capabilities qes winds will be fast enough to use for prediction and optimization purposes 2 methods qes winds uses a variational analysis technique sasaki 1958 sasaki 1970 sasaki 1970 to obtain a quasi time averaged velocity field this method requires the solution of a poisson equation for the lagrange multipliers λ 1 2 λ x 2 2 λ y 2 α 1 α 2 2 2 λ z 2 r where x y and z are the spatial coordinates in the streamwise spanwise and ground surface normal directions respectively and α 1 and α 2 are gaussian precision moduli to numerically implement eq 1 we discretize the computational domain using a staggered grid where λ and the divergence of the initial velocity field r are cell centered variables and flow components u v and w corresponding to the x y and z directions respectively are cell faced values the divergence of the initial wind field is defined as 2 r i j k 2 α 1 2 u i 1 2 j k 0 u i 1 2 j k 0 δ x v i j 1 2 k 0 v i j 1 2 k 0 δ y w i j k 1 2 0 w i j k 1 2 0 δ z where i j and k are cell indices in the x y and z directions respectively a half index step indicates a cell face value δ x δ y and δ z are the cell dimensions in the x y and z directions respectively and the superscript 0 denotes an initial estimated value equation 1 is solved using the sor method young 1954 varga 1962 resulting in the following relationship for the lagrange multipliers 3 λ i j k ω e i j k f i j k g i j k h i j k m i j k n i j k δ x 2 r i j k e i j k λ i 1 j k f i j k λ i 1 j k a g i j k λ i j 1 k h i j k λ i j 1 k b m i j k λ i j k 1 n i j k λ i j k 1 1 ω λ i j k where e i j k f i j k g i j k h i j k m i j k and n i j k are boundary condition coefficients and a δ x 2 δ y 2 and b η δ x 2 δ y 2 where η α 1 α 2 2 are domain constants gaussian precision moduli α 1 and α 2 are set to unity in this study and the sor over relaxation factor ω 1 78 is based on the recommendation by röckle 1990 neumann boundary conditions λ n 0 are applied to solid surfaces and dirichlet boundary conditions λ 0 are applied to inlet outlet surfaces to implement the solid surface boundary condition the boundary condition coefficient related to the surface is set to zero the boundary condition coefficients e i j k f i j k g i j k h i j k m i j k and n i j k are related to cell surfaces located at i 1 2 i 1 2 j 1 2 j 1 2 k 1 2 and k 1 2 of the cell i j k respectively the final velocity field is then updated through the euler lagrange equations 4 u i j k u i j k 0 1 2 α 1 2 δ x λ i 1 j k λ i j k and 5 v i j k v i j k 0 1 2 α 1 2 δ y λ i j 1 k λ i j k and 6 w i j k w i j k 0 1 2 α 2 2 δ z λ i j k 1 λ i j k to ensure convergence the error for each iteration is calculated as 7 e r r o r m a x λ i j k t λ i j k t 1 where t represents the current iteration and t 1 stands for the previous iteration this guarantees that all calculated lagrange multipliers in the computational domain converge to the same criteria set by the user 2 1 solver options the combination of the divergence eq 2 the sor loop eq 3 and euler lagrange equations eqs 4 6 comprise the qes winds solver qes winds has two options for the solver the first option is to solve the equations on the gpu using cuda kernels and the second option uses the cpu for computations the gpu solver makes use of a red black coloring scheme explained in the following section 2 2 red black coloring scheme in eq 3 the lagrange multiplier for each cell λ i j k depends on the lagrange multiplier values for neighboring cells i 1 i 1 j 1 j 1 k 1 and k 1 the sor method is sequential adams 1982 which means that the lagrange multiplier values for i 1 j 1 and k 1 cells are calculated in the current iteration while the values for i 1 j 1 and k 1 cells are from the previous iteration the dependency and sequence in calculating the lagrange multipliers prevents us from solving for all the cells in parallel adams 1982 several papers including hayes et al hayes 1974 and lamblotte et al lambiotte 1975 suggested the red black coloring scheme as a solution to eliminate this dependency and sequence in the red black scheme the computational domain is divided into two sets of cells colored red and black in which the cell being solved for and its neighboring cells are different colors in qes winds cells are colored red black if i j k is odd even by applying the red black ordering scheme the sor iterations turn into two separate sub iterations each done in parallel across all available gpu cores equation 3 is first solved for all red cells then subsequently for all black cells fig 1 shows a two dimensional domain with the red black coloring order applied and indicates the two sub iterations for red and black cells three different gpu solver implementations are introduced in the following sections 2 3 gpu solver using global memory three kernels divergenceglobal sor rb global and finalvelocityglobal are written to access the global memory to compute the divergence eq 2 solve for the lagrange multipliers using the red black sor method eq 3 and solve the euler lagrange equations eqs 4 6 in addition three other kernels savelambdaglobal applyneumannbcglobal and calculateerrorglobal save a copy of the lagrange multipliers from the previous iteration apply the neumann boundary conditions to the ground surface and calculate maximum relative error for the iteration algorithm 1 shows the details of incorporating the solver on the gpu using global memory kernels data required for calculations are copied once from the cpu s memory to gpu s global memory and must remain there until the end of the process nvidia gpus with pre volta architectures pascal maxwell kepler fermi and tesla use the multi process service mps that does not fully isolate the threads and memory required for an application run nvidia 2020 according to the cuda mps overview nvidia 2020 this means that other applications running concurrently on a shared gpu can possibly allocate over and or modify data allocated for another application without triggering an error pietro et al 2016 reported an illegal memory access memory leakage by two independent host processes while using the global memory of pre volta architecture gpus to guarantee security of qes winds data all data required for each kernel must be copied to the gpu s global memory before launching the kernel this solution means that the lagrange multipliers and boundary condition coefficients must be copied back and forth to the cpu s memory before and after each call to the red black sor kernel which leads to a massive copying overhead and slow down in the solver in the present study we stick to the current version of the solver without copying back and forth since we have access to gpus that are not shared with other applications algorithm 1 gpu solver using global memory image 1 2 4 gpu solver using shared memory global memory accesses on the gpu usually have an associated time delay one way to reduce the delay is to load the required data for calculations from the global memory accessible by all cores to faster shared memory accessible only by threads in a block because the sor rb kernel has the most memory accesses we only applied the shared memory to the sor rb kernel the rest of the kernels and the algorithm are the same as the ones for the global memory solver 2 5 gpu solver using dynamic parallelism another way to address the aforementioned memory leakage issue is to use the cuda dynamic parallelism by utilizing dynamic parallelism the host does not have information about the number of threads and the amount of memory required for the calculation as a result the whole global memory and all the cores are reserved to run the dynamic parallel kernel and the mps does not share the gpu resources nvidia 2020 this means that data on the global memory is secured even for pre volta gpu architectures dynamic parallelism has been used to reduce the copying overhead jones 2012 kirk and wen mei 2016 ding and tan 2015 in this method data required for calculations are copied to the gpu global memory once which makes the solver much faster compared to the most secure version of the global memory solver with copies back and forth to the gpu in this solver the dynamicparallel kernel is called with one thread from the host next all kernels are called from inside the dynamicparallel kernel already on the gpu algorithm 2 details the dynamic parallel method all kernels that are launched from the dynamicparallel kernel are the same as the ones in the global memory solver algorithm 2 gpu solver using dynamic parallelism image 2 3 results and discussion 3 1 convergence criteria a common convergence criterion for iterative methods is when the maximum error in the domain falls below 10 6 to 10 10 adams 1982 kelley 1995 the complex boundary conditions and imposed building parameterizations in qes winds makes convergence difficult to achieve in most cases converging to 10 9 requires up to 200 000 iterations without a significant difference in the final wind field as a result qes winds imposes the maximum number of iterations as a secondary convergence criteria in order to define the maximum number of iterations a test case with 100 100 100 cells and cell size of 2 2 1 m was investigated the test case was a three dimensional flow around a single cubical building with a 20 m edge length located in the middle of domain this is the simplest case in qes winds all building flow parameterizations were applied to the building winds are specified for simulation initialization using a sensor at 10 m height with a measured wind speed of 5 ms 1 at 270 from the north a logarithmic profile has been used to create the initial velocity field based on the sensor data since the qes winds error does not reach 10 9 for this test case the solution after 100 000 iterations was chosen as a reference details from the test case are shown in table 1 fig 2 illustrates the maximum convergence error eq 7 and maximum difference for velocity components compared to the reference solution as a function of iteration count the results in table 1 and fig 2 show that while the error and maximum differences decrease with number of iterations they do not decrease beyond 1500 iterations thus performing more than 1500 iterations does not improve the solution and wastes time and computational resources the values of maximum and relative differences for each velocity component for the solution with 500 iterations are of the order of 10 3 velocity values less than 0 01 ms 1 cannot be measured experimentally which means that 0 01 ms 1 is an acceptable threshold for our calculations in addition converging to the error criterion 10 9 is much harder for realistic cases such flow wind over cities this is a result of multiple buildings and overlapping building parameterizations for cities or irregular geometry in complex terrain flows as a result the maximum number of iterations for the purposes of qes winds is set to 500 iterations 3 2 benchmarking for cpu and gpu solvers the cpu solver is quite efficient but slow in comparison to the gpu solvers especially for large domains since it lacks parallel capabilities a suite of test cases were designed and analyzed to illustrate differences between solvers each case includes a cubical building with edge lengths of 20 m situated in the middle of the domain with all the building parameterizations applied winds are specified for simulation initialization using a sensor at 10 m above ground with a wind speed of 5 ms 1 coming from 270 relative to north a logarithmic profile has been used to create the initial velocity field based on the sensor data details for each are provided in table 2 the cpu solver solution is considered a reference because it is identical to the well validated quic urb solver brown et al 2013 pardyjak and brown 2003 quic urb has compared quite well with higher order physics based models and available experimental results shukla and parikh 1992 hayati et al 2017 2019 neophytou et al 2011 bagal et al 2004 booth and pardyjak 2012 bowker et al 2004 balwinder et al 2006 hence we consider qes winds cpu solver accurate enough for the purposes of diagnostic wind modeling the cpu solver was run for 500 iterations in each test case and then the gpu solvers were run until they reached the same error as the cpu solver after 500 iterations table 2 shows the number of iterations required for the global memory gm the shared memory sm and the dynamic parallel dp solvers to converge in addition to the total time required this benchmarking was performed on a machine with an intel xeon gold 6130 cpu 2 10 ghz cpu with 12 gb ram and an nvidia titan v gpu with cuda 10 1 toolkit nvidia 2019 and 12 gb of global memory since the cpu solver and each of the gpu solvers run for different numbers of iterations the total time for each solver is divided by the number of iterations table 3 displays the time per iteration for the different solvers in addition to the speedup for each of the gpu solvers over the cpu solver fig 3 shows the time per iteration as a function of the number of cells for all test cases and solvers all gpu solvers are much faster than the cpu solver due to parallelization benefits for very large domains 145 million cells all gpu solvers are approximately 128 times faster than the cpu solver differences in time per iteration for all gpu solvers are negligible since they only have the overhead associated with one copy from the cpu s memory to gpu s global memory and back and they have similar kernels with small differences qes winds cannot solve for more than 145 million cells on the titan v gpu due to constraints on its global memory 12 gb according to the description of cuda dynamic parallelism in the cuda c programming guide nvidia 2019 launching kernels from inside the dynamic parallel kernel has the potential to add a large amount of overhead on the application however in the case of qes winds results show that the dynamic parallel solver is less than five percent slower than the global and shared memory solvers the shared memory solver is slightly faster than the global and dynamic parallel solvers for smaller domains number of cells 50 m the only exception is for the case of a total number of cells of 0 1 m because the number of accesses to the global memory is not large enough to realize the advantage of loading the data on memory with less latency shared memory can give generally since there is no reuse of data loaded to the shared memory in the sor rb shared the speedup for the shared memory solver is less than five percent the solver accounts for most of the execution time in qes winds although the single building case with building parameterization is the simplest test case as long as the more realistic cases e g flow over cities or complex terrain have the same number of cells the execution time is almost the same the only difference is the set up time which depends on the case type processing buildings and apply building parameterizations prior to running the sor solver leads to different execution times depending on the specific geometry being simulated assuming 1 m horizontal and 3 m vertical resolution qes winds can compute wind fields on a 1 18 km by 1 21 km by 210 m domain 1180 1210 70 cells the 100 million cell case an area as big as the central business district in downtown oklahoma city with all building parameterizations applied in about 130 s no other wind modeling systems is capable of simulating such a large domain with such a fine resolution in real time note that the cpu solver takes about 55 min kernel execution metrics are required to explain why the gpu solvers behave this way the nvidia visual profiler nvidia 2019 is used for this purpose since the nvidia visual profiler version 10 1 does not support cuda dynamic parallel profiling on gpus with compute capability of 7 0 and higher titan v has compute capability of 7 0 the profiling was performed on a different machine with an nvidia geforce gtx titan x maxwell architecture with the cuda 10 1 toolkit nvidia 2019 and 12 gb of global memory table 4 contains results of profiling on the gpu solvers for three test cases with cell counts of 1 10 and 50 million cells the gpu activities of the dynamic parallel solver that have equivalent parts in the global and shared memory solvers are listed in table 4 in order to provide a fair comparison between the solvers it can be seen that the copying overhead to the gpu cuda memcpy htod and from the gpu cuda memcpy dtoh is slightly higher for the global and shared memory compared to the dynamic parallel solver the reason for this behavior is that the error value must be copied back and forth between the host and the device to check for convergence meanwhile kernels launched from the dynamicparallel kernel have slightly longer execution times which must be related to the execution overhead of the dynamic parallelism nvidia 2019 since all the kernels are identical for the global memory solver and dynamic parallel solver in the most secure versions of the global and shared memory solvers which require copying back and forth during each iteration the copying overhead increases the time per iteration related to cuda memcpy htod by a factor of about four in this case the dynamic parallel solver is much faster than the global and shared memory solvers 3 3 comparing red black sor to serial sor surprisingly to our knowledge there has never been a discussion on how well the red black sor method i e parallel version compares to serial sor in the literature we conducted a test using the same test case described above of flow around an isolated 20 m cube with 1 million cells the third row case in table 2 using the red black sor gpu and serial sor cpu solvers winds are specified for simulation initialization using a sensor at 10 m height with a measured wind speed of 5 ms 1 at 270 from the north a logarithmic profile has been used to create the initial velocity field based on the sensor data the converged solutions from each case are compared in fig 4 fig 4 a and b show velocity vectors for the cpu and the gpu solvers while fig 4 c show the differences in velocity magnitudes between the cpu and the gpu solvers all data in the figure are presented for a horizontal plane at z 10 m the solid black line shows the boundaries of the building there is a notable checkerboard pattern present in the difference field which is caused by the red black sor procedure in which the red cells use the lagrange multiplier values from the previous iteration while the black cells use the lagrange multiplier values of the red cells from the current iteration the flow of data between neighboring cells in the red black sor solver is different from the serial sor gauss seidel and jacobi methods and prevents smoothing and causes the observed checkerboard pattern qes winds outputs the velocity field as an averaged cell centered field for visualization purposes which slightly smooths the checkerboard pattern the maximum difference between the velocity components from the two solutions are 0 0006 0 0008 and 0 001 ms 1 for u v and w respectively this means that for the purpose of qes winds modeling mean wind fields the checkerboard pattern is not a significant issue because one the most important components of the qes winds is speed the accuracy can be sacrificed for a faster running solver however for applications that need the gradient of the velocity field e g computing the turbulence field the checkerboard pattern can pose issues more complex and expensive smoothing techniques may be required for such applications 4 conclusion optimizing and predicting wind fields for fast response applications such as wildfires and urban air quality require modeling high resolution three dimensional mean wind fields in real time for large domains qes winds uses the parallel capabilities of the gpu to accelerate wind field computations three different implementations of the gpu solver were examined while all three solvers were much faster than the cpu serial solver only one of them dynamic parallel demonstrated the ability to guarantee the security of global memory data from potential illegal accesses the dynamic parallel solver reduces the execution time by a factor of 128 compared to the serial solver for a domain with 145 million cells qes winds was able to solve for the wind field on a 10 km 2 domain with a horizontal grid spacing of 1 3 m in less than 1 min the application of qes winds as a fast response wind modeling code can be further enhanced by improving its physics modules nvidia s cuda dynamic parallelism can greatly accelerate iterative methods and other codes that require a large amount of copying overhead between the cpu and the gpu while protecting data on all gpu architectures there were several challenges related to the dynamic parallel implementation first current gpu s global memory limits qes winds to solving domains of no more than 145 million cells second the nvidia visual profiler is unable to run for a code using cuda dynamic parallelism on newer gpus with higher compute capabilities these challenges will be alleviated with the advent of newer gpus with higher global memory that are supported by the cuda visual profiler software availability the quick environmental system qes fast response wind solver qes winds has been developed as a collaboration between the university of utah university of minnesota duluth and pukyong national university the code is written mainly in c and nvidia s cuda language qes winds is publicly accessible currently hosted on github https github com utahefd qes winds public declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was partly supported by a grant from the national institute of environment research nier funded by the ministry of environment moe of the republic of korea nier sp2019 312 the united states department of agriculture national institute for food and agriculture specialty crop research initiative award no 2018 03375 and the united states department of agriculture agricultural research service through research support agreement 58 2072 0 036 
25882,qes winds is a fast response wind modeling platform for simulating high resolution mean wind fields for optimization and prediction the code uses a variational analysis technique to solve the poisson equation for lagrange multipliers to obtain a mean wind field and gpu parallelization to accelerate the numerical solution of the poisson equation qes winds benefits from cuda dynamic parallelism launching the kernel from the gpu to speed up calculations by a factor of 128 compared to the serial solver for a domain with 145 million cells the dynamic parallelism enables qes winds to calculate mean velocity fields for domains with sizes of 10 km 2 and horizontal resolutions of 1 3 m in under 1 min as a result qes winds is a numerical code suitable for computing high resolution wind fields on large domains in real time which can be used to model a wide range of real world problems including wildfires and urban air quality keywords qes winds poisson equation fast response wind modeling iterative method staggered grid 1 introduction the urban population of the world has grown rapidly from 751 million in 1950 to 4 2 billion in 2018 in 2018 cities contained 55 of the world s population and by 2050 urban areas will account for 68 of the world s population united nations and department of economic and social affairs 2019 this rapid urbanization creates multiple meteorological and climate related phenomena linked to negative human health outcomes including air pollution shukla and parikh 1992 and urban heat island effects akbari and kolokotsa 2016 additionally urban population growth expands the wildland urban interface increasing the risk to life and property from wildfires radeloff et al 2018 calkin et al 2014 the risk is compounded by the increased number of wildfires over the past three decades since 2000 at least 10 states in the united states have had their largest fires on record and currently fire seasons are 78 days longer than in the 1970s while over 70 000 communities are at risk of wildfires usda 2019 proper modeling of the physics of wildfires moody et al 2019 linn et al 2020 and pollution dispersion in cities pardyjak and brown 2001 williams et al 2004 singh et al 2008 requires high resolution representation of wind fields in natural and urban areas for the purpose of prediction where model run times should be near or faster than real time or for design optimization problems where thousands of simulations must be performed in a short period of time traditional computational fluid dynamics cfd models such as reynolds averaged navier stokes rans simulations and large eddy simulations les are not fast enough hayati et al 2019 another option is to use a semi empirical fast response approach the complexity of urban and natural land surface geometries along with the complicated resulting wind flow requires sophisticated physical models however more detailed models require a significant increase in computational costs time and computational power as a result fast response wind flow simulators are needed that compromise some accuracy to shorten computation time the quick environmental simulation qes tool is a microclimate simulation platform for computing the transport of three dimensional environmental scalars in urban areas and over complex topography qes is organized into separate components each designed to simulate a different aspect of environmental transport qes winds is the fast response 3d diagnostic wind modeling module written in c based on the often used fortran code quic urb quick urban and industrial complex urban brown et al 2013 pardyjak and brown 2003 qes winds solves a mass conservation equation for the wind field rather than the slower and more physics based solvers that include conservation of momentum while qes winds uses reduced order physics to simulate urban flows the solutions are rapid and compare quite favorably with higher order physics based models in both idealized hayati et al 2017 2019 and realistic cities neophytou et al 2011 the qes winds model is based on the 3d diagnostic urban wind model proposed by röckle 1990 first an initial wind field is prescribed by combining an incident flow with localized flows that account for the effects of building geometries via empirical parameterizations singh et al 2008 conservation of mass is then enforced using a variational analysis a type of data assimilation technique sasaki 1958 sasaki 1970 sasaki 1970 to minimize the differences between the initial guess field and the final mass conserving wind field this technique requires the solution of a poisson equation for lagrange multipliers and results in calculating a quasi time averaged velocity field the resulting complex 3d wind field resembles time averaged experimental data hayati et al 2017 2019 the poisson equation is discretized over the computational domain and rearranged into matrix form creating a system of linear equations the matrix form of the poisson equation can theoretically be solved using sparse direct solvers which should be fast compared to iterative solvers however after applying the complex boundary conditions specific to our case the coefficient matrix a becomes a sparse non diagonal non banded non triangular non symmetric and not real positive diagonal matrix as a result the matrix must be solved using the m a 57 algorithm duff 2004 sparse symmetric system multifrontal method which is numerically expensive and quite slow for our purposes to overcome these shortcomings the poisson equation in qes winds is solved using the successive over relaxation sor method a variant the of gauss seidel method with faster convergence young 1954 varga 1962 the sor method is a sequential iterative method adams 1982 which means that it is not fast enough for the purpose of optimization and prediction for domains with a high number of cells when a large number of iterations is required for convergence to reduce the execution time of qes winds the sor method must be parallelized despite the sequential nature of sor poisson solvers they can be executed in parallel if the discretized equations are ordered according to the classical red black coloring scheme hayes 1974 lambiotte 1975 as illustrated in fig 1 in the red black scheme cells are divided into two partitions such that all of the neighboring cells of a red cell are black and vice versa as a result the discretized equation can be solved for two sub iterations in parallel once for all of the red cells and once for all of the black cells inside the domain adams 1982 evans 1984 utilizing parallel computing on cpus central processing units zapata et al 2018 krupka and šimecek 2010 and gpus graphics processing units helfenstein and koko 2012 li and saad 2013 cotronis et al 2014 konstantinidis and cotronis 2011 itu et al 2011 to accelerate the sor solver has been the subject of extensive research kruptka and šimeček 2010 showed that the achievable speedup by parallelizing on the cpu depends on the number of computational nodes available and size of the computational domain their results indicated that the maximum speedup for red black sor is equal to the number of computational nodes available on the gpu finding the maximum speedup is not as easy since various factors are involved all codes that run on the gpu need to be launched from the cpu and all data required for calculations must be copied from the cpu s memory to the gpu s global memory memory access time on the gpu and the bidirectional data transfer between the cpu and gpu are the most important parameters that affect the potential speedup all of the aforementioned research mainly focused on reducing memory access time on the gpu by using shared memory or padded global memory itu et al 2011 cotronis et al 2014 reported 11 times speedup using the global memory of an nvidia gtx480 gpu with 480 computational cores over a sequential solver on a cpu for a domain size of 2882 2882 cells in this paper we focus on reducing the copy overhead between the cpu and gpu to accelerate the sor solver cuda compute unified device architecture dynamic parallelism has been used in the literature jones 2012 kirk and wen mei 2016 ding and tan 2015 to reduce the bidirectional data transfer between the cpu and gpu another option is to rely on the gpu memory to hold the data during the whole iterative process there are other similar diagnostic wind modeling software packages that have been described in the literature most notably windninja forthofer et al 2014 and micro swift moussafir et al 2004 tinarelli et al 2007 windninja uses a conjugate gradient method with jacobi preconditioning to solve the poisson equation in a terrain following coordinate system forthofer et al 2014 the conjugate gradient method with jacobi preconditioning is computationally expensive and time consuming but it is the best option using terrain following coordinates as a result windninja is not the best option for simulating wind fields with fine grids e g of the order of 1 m in addition the terrain following coordinate system is not suitable for computing flows around buildings which means that windninja is not applicable for modeling urban areas micro swift utilizes the sor method to solve poisson s equation in a three dimensional cartesian coordinate system moussafir et al 2004 tinarelli et al 2007 however micro swift is not able to compute flows over complex terrain since it does not process terrain geometry in addition because micro swift only has a serial solver solving for high resolution wind fields over urban areas is computationally expensive because it has the same basic solver micro swift could benefit from the parallelization method described in this paper there have been few attempts to accelerate diagnostic wind models using gpus in the literature the study most similar to our effort was conducted by pinheiro et al 2017 they utilized gpu parallelization to accelerate a mass consistent wind model used in predicting atmospheric dispersion of radionuclides they used the winds extrapolated from stability and terrain west model which has an irrotational correction to the initial wind field that minimizes the divergence of the initial velocity field the irrotational correction is based on the perturbation velocity potential and transmission coefficients which are defined based on temperature profiles obtained from upper air soundings in this method they calculate the divergence in each cell and the gradient of the perturbation velocity potential they then update the velocity field and repeat the process until convergence which is negligible divergence homicz 2002 reviewed different wind modeling approaches and concluded that the west approach called the noabl model in the paper is different than models based on variational calculus e g qes winds also in order to get a divergence free final velocity field there are restrictions on the values of transmission coefficients which means that the model produces mass consistent wind fields in special circumstances pinheiro et al 2017 reported about 25 times speedup using an nvidia gtx 680 gpu for a domain with their finest grid about 1 5 million cells the gpu parallelization technique dynamic parallelism discussed here can be incorporated into other types of wind solvers and dispersion models to significantly accelerate them singh et al 2011 developed a new dispersion model called gpu plume that utilized the parallel computational capabilities available on the gpu to accelerate the calculations our group is developing an improved version of the gpu plume that has the potential to decrease the execution time by exploiting benefits of the dynamic parallelism also the new model in conjunction with the qes winds could run air quality simulations of large urban areas in near real time other examples include windstation which is software developed by lopes 2003 as a tool to simulate atmospheric flows over complex terrain two models were used to handle the complex topography the first one is a mass conservative wind solver much simpler than qes winds while the second one solved for three dimensional naiver stokes equations incorporating the same gpu technique described here in these models can speed up calculations and allow for high resolution simulations on massive domain including those with complex topography lastly linn et al 2020 recently developed a fast running tool to model complex behavior of wildland fire propagation they used quic urb brown et al 2013 pardyjak and brown 2003 as the wind solver to provide a high resolution wind field for the fire propagation model qes winds which evolved from quic urb can provide a wind to fire propagation models to predict wildfire behavior over larger domains in near real time the overall goal of this research is to significantly decrease the execution time required for qes winds nvidia s parallel gpu computing platform and cuda application programming interface api nvidia 2019 are used to substantially accelerate qes winds by utilizing gpu parallel computing capabilities qes winds will be fast enough to use for prediction and optimization purposes 2 methods qes winds uses a variational analysis technique sasaki 1958 sasaki 1970 sasaki 1970 to obtain a quasi time averaged velocity field this method requires the solution of a poisson equation for the lagrange multipliers λ 1 2 λ x 2 2 λ y 2 α 1 α 2 2 2 λ z 2 r where x y and z are the spatial coordinates in the streamwise spanwise and ground surface normal directions respectively and α 1 and α 2 are gaussian precision moduli to numerically implement eq 1 we discretize the computational domain using a staggered grid where λ and the divergence of the initial velocity field r are cell centered variables and flow components u v and w corresponding to the x y and z directions respectively are cell faced values the divergence of the initial wind field is defined as 2 r i j k 2 α 1 2 u i 1 2 j k 0 u i 1 2 j k 0 δ x v i j 1 2 k 0 v i j 1 2 k 0 δ y w i j k 1 2 0 w i j k 1 2 0 δ z where i j and k are cell indices in the x y and z directions respectively a half index step indicates a cell face value δ x δ y and δ z are the cell dimensions in the x y and z directions respectively and the superscript 0 denotes an initial estimated value equation 1 is solved using the sor method young 1954 varga 1962 resulting in the following relationship for the lagrange multipliers 3 λ i j k ω e i j k f i j k g i j k h i j k m i j k n i j k δ x 2 r i j k e i j k λ i 1 j k f i j k λ i 1 j k a g i j k λ i j 1 k h i j k λ i j 1 k b m i j k λ i j k 1 n i j k λ i j k 1 1 ω λ i j k where e i j k f i j k g i j k h i j k m i j k and n i j k are boundary condition coefficients and a δ x 2 δ y 2 and b η δ x 2 δ y 2 where η α 1 α 2 2 are domain constants gaussian precision moduli α 1 and α 2 are set to unity in this study and the sor over relaxation factor ω 1 78 is based on the recommendation by röckle 1990 neumann boundary conditions λ n 0 are applied to solid surfaces and dirichlet boundary conditions λ 0 are applied to inlet outlet surfaces to implement the solid surface boundary condition the boundary condition coefficient related to the surface is set to zero the boundary condition coefficients e i j k f i j k g i j k h i j k m i j k and n i j k are related to cell surfaces located at i 1 2 i 1 2 j 1 2 j 1 2 k 1 2 and k 1 2 of the cell i j k respectively the final velocity field is then updated through the euler lagrange equations 4 u i j k u i j k 0 1 2 α 1 2 δ x λ i 1 j k λ i j k and 5 v i j k v i j k 0 1 2 α 1 2 δ y λ i j 1 k λ i j k and 6 w i j k w i j k 0 1 2 α 2 2 δ z λ i j k 1 λ i j k to ensure convergence the error for each iteration is calculated as 7 e r r o r m a x λ i j k t λ i j k t 1 where t represents the current iteration and t 1 stands for the previous iteration this guarantees that all calculated lagrange multipliers in the computational domain converge to the same criteria set by the user 2 1 solver options the combination of the divergence eq 2 the sor loop eq 3 and euler lagrange equations eqs 4 6 comprise the qes winds solver qes winds has two options for the solver the first option is to solve the equations on the gpu using cuda kernels and the second option uses the cpu for computations the gpu solver makes use of a red black coloring scheme explained in the following section 2 2 red black coloring scheme in eq 3 the lagrange multiplier for each cell λ i j k depends on the lagrange multiplier values for neighboring cells i 1 i 1 j 1 j 1 k 1 and k 1 the sor method is sequential adams 1982 which means that the lagrange multiplier values for i 1 j 1 and k 1 cells are calculated in the current iteration while the values for i 1 j 1 and k 1 cells are from the previous iteration the dependency and sequence in calculating the lagrange multipliers prevents us from solving for all the cells in parallel adams 1982 several papers including hayes et al hayes 1974 and lamblotte et al lambiotte 1975 suggested the red black coloring scheme as a solution to eliminate this dependency and sequence in the red black scheme the computational domain is divided into two sets of cells colored red and black in which the cell being solved for and its neighboring cells are different colors in qes winds cells are colored red black if i j k is odd even by applying the red black ordering scheme the sor iterations turn into two separate sub iterations each done in parallel across all available gpu cores equation 3 is first solved for all red cells then subsequently for all black cells fig 1 shows a two dimensional domain with the red black coloring order applied and indicates the two sub iterations for red and black cells three different gpu solver implementations are introduced in the following sections 2 3 gpu solver using global memory three kernels divergenceglobal sor rb global and finalvelocityglobal are written to access the global memory to compute the divergence eq 2 solve for the lagrange multipliers using the red black sor method eq 3 and solve the euler lagrange equations eqs 4 6 in addition three other kernels savelambdaglobal applyneumannbcglobal and calculateerrorglobal save a copy of the lagrange multipliers from the previous iteration apply the neumann boundary conditions to the ground surface and calculate maximum relative error for the iteration algorithm 1 shows the details of incorporating the solver on the gpu using global memory kernels data required for calculations are copied once from the cpu s memory to gpu s global memory and must remain there until the end of the process nvidia gpus with pre volta architectures pascal maxwell kepler fermi and tesla use the multi process service mps that does not fully isolate the threads and memory required for an application run nvidia 2020 according to the cuda mps overview nvidia 2020 this means that other applications running concurrently on a shared gpu can possibly allocate over and or modify data allocated for another application without triggering an error pietro et al 2016 reported an illegal memory access memory leakage by two independent host processes while using the global memory of pre volta architecture gpus to guarantee security of qes winds data all data required for each kernel must be copied to the gpu s global memory before launching the kernel this solution means that the lagrange multipliers and boundary condition coefficients must be copied back and forth to the cpu s memory before and after each call to the red black sor kernel which leads to a massive copying overhead and slow down in the solver in the present study we stick to the current version of the solver without copying back and forth since we have access to gpus that are not shared with other applications algorithm 1 gpu solver using global memory image 1 2 4 gpu solver using shared memory global memory accesses on the gpu usually have an associated time delay one way to reduce the delay is to load the required data for calculations from the global memory accessible by all cores to faster shared memory accessible only by threads in a block because the sor rb kernel has the most memory accesses we only applied the shared memory to the sor rb kernel the rest of the kernels and the algorithm are the same as the ones for the global memory solver 2 5 gpu solver using dynamic parallelism another way to address the aforementioned memory leakage issue is to use the cuda dynamic parallelism by utilizing dynamic parallelism the host does not have information about the number of threads and the amount of memory required for the calculation as a result the whole global memory and all the cores are reserved to run the dynamic parallel kernel and the mps does not share the gpu resources nvidia 2020 this means that data on the global memory is secured even for pre volta gpu architectures dynamic parallelism has been used to reduce the copying overhead jones 2012 kirk and wen mei 2016 ding and tan 2015 in this method data required for calculations are copied to the gpu global memory once which makes the solver much faster compared to the most secure version of the global memory solver with copies back and forth to the gpu in this solver the dynamicparallel kernel is called with one thread from the host next all kernels are called from inside the dynamicparallel kernel already on the gpu algorithm 2 details the dynamic parallel method all kernels that are launched from the dynamicparallel kernel are the same as the ones in the global memory solver algorithm 2 gpu solver using dynamic parallelism image 2 3 results and discussion 3 1 convergence criteria a common convergence criterion for iterative methods is when the maximum error in the domain falls below 10 6 to 10 10 adams 1982 kelley 1995 the complex boundary conditions and imposed building parameterizations in qes winds makes convergence difficult to achieve in most cases converging to 10 9 requires up to 200 000 iterations without a significant difference in the final wind field as a result qes winds imposes the maximum number of iterations as a secondary convergence criteria in order to define the maximum number of iterations a test case with 100 100 100 cells and cell size of 2 2 1 m was investigated the test case was a three dimensional flow around a single cubical building with a 20 m edge length located in the middle of domain this is the simplest case in qes winds all building flow parameterizations were applied to the building winds are specified for simulation initialization using a sensor at 10 m height with a measured wind speed of 5 ms 1 at 270 from the north a logarithmic profile has been used to create the initial velocity field based on the sensor data since the qes winds error does not reach 10 9 for this test case the solution after 100 000 iterations was chosen as a reference details from the test case are shown in table 1 fig 2 illustrates the maximum convergence error eq 7 and maximum difference for velocity components compared to the reference solution as a function of iteration count the results in table 1 and fig 2 show that while the error and maximum differences decrease with number of iterations they do not decrease beyond 1500 iterations thus performing more than 1500 iterations does not improve the solution and wastes time and computational resources the values of maximum and relative differences for each velocity component for the solution with 500 iterations are of the order of 10 3 velocity values less than 0 01 ms 1 cannot be measured experimentally which means that 0 01 ms 1 is an acceptable threshold for our calculations in addition converging to the error criterion 10 9 is much harder for realistic cases such flow wind over cities this is a result of multiple buildings and overlapping building parameterizations for cities or irregular geometry in complex terrain flows as a result the maximum number of iterations for the purposes of qes winds is set to 500 iterations 3 2 benchmarking for cpu and gpu solvers the cpu solver is quite efficient but slow in comparison to the gpu solvers especially for large domains since it lacks parallel capabilities a suite of test cases were designed and analyzed to illustrate differences between solvers each case includes a cubical building with edge lengths of 20 m situated in the middle of the domain with all the building parameterizations applied winds are specified for simulation initialization using a sensor at 10 m above ground with a wind speed of 5 ms 1 coming from 270 relative to north a logarithmic profile has been used to create the initial velocity field based on the sensor data details for each are provided in table 2 the cpu solver solution is considered a reference because it is identical to the well validated quic urb solver brown et al 2013 pardyjak and brown 2003 quic urb has compared quite well with higher order physics based models and available experimental results shukla and parikh 1992 hayati et al 2017 2019 neophytou et al 2011 bagal et al 2004 booth and pardyjak 2012 bowker et al 2004 balwinder et al 2006 hence we consider qes winds cpu solver accurate enough for the purposes of diagnostic wind modeling the cpu solver was run for 500 iterations in each test case and then the gpu solvers were run until they reached the same error as the cpu solver after 500 iterations table 2 shows the number of iterations required for the global memory gm the shared memory sm and the dynamic parallel dp solvers to converge in addition to the total time required this benchmarking was performed on a machine with an intel xeon gold 6130 cpu 2 10 ghz cpu with 12 gb ram and an nvidia titan v gpu with cuda 10 1 toolkit nvidia 2019 and 12 gb of global memory since the cpu solver and each of the gpu solvers run for different numbers of iterations the total time for each solver is divided by the number of iterations table 3 displays the time per iteration for the different solvers in addition to the speedup for each of the gpu solvers over the cpu solver fig 3 shows the time per iteration as a function of the number of cells for all test cases and solvers all gpu solvers are much faster than the cpu solver due to parallelization benefits for very large domains 145 million cells all gpu solvers are approximately 128 times faster than the cpu solver differences in time per iteration for all gpu solvers are negligible since they only have the overhead associated with one copy from the cpu s memory to gpu s global memory and back and they have similar kernels with small differences qes winds cannot solve for more than 145 million cells on the titan v gpu due to constraints on its global memory 12 gb according to the description of cuda dynamic parallelism in the cuda c programming guide nvidia 2019 launching kernels from inside the dynamic parallel kernel has the potential to add a large amount of overhead on the application however in the case of qes winds results show that the dynamic parallel solver is less than five percent slower than the global and shared memory solvers the shared memory solver is slightly faster than the global and dynamic parallel solvers for smaller domains number of cells 50 m the only exception is for the case of a total number of cells of 0 1 m because the number of accesses to the global memory is not large enough to realize the advantage of loading the data on memory with less latency shared memory can give generally since there is no reuse of data loaded to the shared memory in the sor rb shared the speedup for the shared memory solver is less than five percent the solver accounts for most of the execution time in qes winds although the single building case with building parameterization is the simplest test case as long as the more realistic cases e g flow over cities or complex terrain have the same number of cells the execution time is almost the same the only difference is the set up time which depends on the case type processing buildings and apply building parameterizations prior to running the sor solver leads to different execution times depending on the specific geometry being simulated assuming 1 m horizontal and 3 m vertical resolution qes winds can compute wind fields on a 1 18 km by 1 21 km by 210 m domain 1180 1210 70 cells the 100 million cell case an area as big as the central business district in downtown oklahoma city with all building parameterizations applied in about 130 s no other wind modeling systems is capable of simulating such a large domain with such a fine resolution in real time note that the cpu solver takes about 55 min kernel execution metrics are required to explain why the gpu solvers behave this way the nvidia visual profiler nvidia 2019 is used for this purpose since the nvidia visual profiler version 10 1 does not support cuda dynamic parallel profiling on gpus with compute capability of 7 0 and higher titan v has compute capability of 7 0 the profiling was performed on a different machine with an nvidia geforce gtx titan x maxwell architecture with the cuda 10 1 toolkit nvidia 2019 and 12 gb of global memory table 4 contains results of profiling on the gpu solvers for three test cases with cell counts of 1 10 and 50 million cells the gpu activities of the dynamic parallel solver that have equivalent parts in the global and shared memory solvers are listed in table 4 in order to provide a fair comparison between the solvers it can be seen that the copying overhead to the gpu cuda memcpy htod and from the gpu cuda memcpy dtoh is slightly higher for the global and shared memory compared to the dynamic parallel solver the reason for this behavior is that the error value must be copied back and forth between the host and the device to check for convergence meanwhile kernels launched from the dynamicparallel kernel have slightly longer execution times which must be related to the execution overhead of the dynamic parallelism nvidia 2019 since all the kernels are identical for the global memory solver and dynamic parallel solver in the most secure versions of the global and shared memory solvers which require copying back and forth during each iteration the copying overhead increases the time per iteration related to cuda memcpy htod by a factor of about four in this case the dynamic parallel solver is much faster than the global and shared memory solvers 3 3 comparing red black sor to serial sor surprisingly to our knowledge there has never been a discussion on how well the red black sor method i e parallel version compares to serial sor in the literature we conducted a test using the same test case described above of flow around an isolated 20 m cube with 1 million cells the third row case in table 2 using the red black sor gpu and serial sor cpu solvers winds are specified for simulation initialization using a sensor at 10 m height with a measured wind speed of 5 ms 1 at 270 from the north a logarithmic profile has been used to create the initial velocity field based on the sensor data the converged solutions from each case are compared in fig 4 fig 4 a and b show velocity vectors for the cpu and the gpu solvers while fig 4 c show the differences in velocity magnitudes between the cpu and the gpu solvers all data in the figure are presented for a horizontal plane at z 10 m the solid black line shows the boundaries of the building there is a notable checkerboard pattern present in the difference field which is caused by the red black sor procedure in which the red cells use the lagrange multiplier values from the previous iteration while the black cells use the lagrange multiplier values of the red cells from the current iteration the flow of data between neighboring cells in the red black sor solver is different from the serial sor gauss seidel and jacobi methods and prevents smoothing and causes the observed checkerboard pattern qes winds outputs the velocity field as an averaged cell centered field for visualization purposes which slightly smooths the checkerboard pattern the maximum difference between the velocity components from the two solutions are 0 0006 0 0008 and 0 001 ms 1 for u v and w respectively this means that for the purpose of qes winds modeling mean wind fields the checkerboard pattern is not a significant issue because one the most important components of the qes winds is speed the accuracy can be sacrificed for a faster running solver however for applications that need the gradient of the velocity field e g computing the turbulence field the checkerboard pattern can pose issues more complex and expensive smoothing techniques may be required for such applications 4 conclusion optimizing and predicting wind fields for fast response applications such as wildfires and urban air quality require modeling high resolution three dimensional mean wind fields in real time for large domains qes winds uses the parallel capabilities of the gpu to accelerate wind field computations three different implementations of the gpu solver were examined while all three solvers were much faster than the cpu serial solver only one of them dynamic parallel demonstrated the ability to guarantee the security of global memory data from potential illegal accesses the dynamic parallel solver reduces the execution time by a factor of 128 compared to the serial solver for a domain with 145 million cells qes winds was able to solve for the wind field on a 10 km 2 domain with a horizontal grid spacing of 1 3 m in less than 1 min the application of qes winds as a fast response wind modeling code can be further enhanced by improving its physics modules nvidia s cuda dynamic parallelism can greatly accelerate iterative methods and other codes that require a large amount of copying overhead between the cpu and the gpu while protecting data on all gpu architectures there were several challenges related to the dynamic parallel implementation first current gpu s global memory limits qes winds to solving domains of no more than 145 million cells second the nvidia visual profiler is unable to run for a code using cuda dynamic parallelism on newer gpus with higher compute capabilities these challenges will be alleviated with the advent of newer gpus with higher global memory that are supported by the cuda visual profiler software availability the quick environmental system qes fast response wind solver qes winds has been developed as a collaboration between the university of utah university of minnesota duluth and pukyong national university the code is written mainly in c and nvidia s cuda language qes winds is publicly accessible currently hosted on github https github com utahefd qes winds public declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was partly supported by a grant from the national institute of environment research nier funded by the ministry of environment moe of the republic of korea nier sp2019 312 the united states department of agriculture national institute for food and agriculture specialty crop research initiative award no 2018 03375 and the united states department of agriculture agricultural research service through research support agreement 58 2072 0 036 
25883,the variogram analysis of response surfaces vars has been proposed by razavi and gupta as a new comprehensive framework in sensitivity analysis according to these authors vars provides a more intuitive notion of sensitivity and is much more computationally efficient than sobol indices here we review these arguments and critically compare the performance of vars to for total order index against the total order jansen estimator we argue that unlike classic variance based methods vars lacks a clear definition of what an important factor is and we show that the alleged computational superiority of vars does not withstand scrutiny we conclude that while vars enriches the spectrum of existing methods for sensitivity analysis especially for a diagnostic use of mathematical models it complements rather than replaces classic estimators used in variance based sensitivity analysis keywords uncertainty sensitivity analysis modeling statistics design of experiment 1 introduction sensitivity analysis sa explores how uncertainty in the output of a model numerical or otherwise can be apportioned to different sources of uncertainty in the model input space saltelli 2002 1 1 this article is part of a si on sensitivity analysis for environmental modeling sa is especially needed when complex models which often formalize partially known processes and include non linear relations are used to guide policies in the real world this is generally the case of models in the environmental sciences domain e g on crop water requirements water availability under climate change weather forecasting surface runoff or precipitation and evaporation processes döll and siebert 2002 pappenberger et al 2011 vieux and vieux 2016 wang et al 2020 the uncertainties in these models might be either parametric i e exact values for parameters might be unknown there might be errors in the measurement or structural i e lack of knowledge on the underlying processes multiple ways of modeling the same phenomenon and their combined effect on the model output should be understood to guarantee a robust inference for policy making in this context sa jointly with uncertainty analysis is regarded as an unavoidable step to ensure the quality of the modeling process borgonovo and plischke 2016 eker et al 2018 jakeman et al 2006 saltelli 2019 saltelli et al 2020 tarantola et al 2002 in sa as in all fields of computational research different strategies and methods compete to establish themselves as good recommended or best practices while variance based methods and sobol indices are deemed to belong to the class of recommended methods saltelli et al 2008 other approaches have been proposed to complement or overcome their limitations i e entropy based methods liu et al 2006 the δ measure borgonovo 2007 the kuiper metric baucells and borgonovo 2013 or the pawn index pianosi and wagener 2015 2018 one of the most recent competitors is the variogram analysis of response surfaces vars proposed by razavi and gupta 2016a 2016b according to google scholar and as of november 2020 the two foundational vars papers have been cited 86 times and seem to have been especially embraced by hydrologists and water scientists jayathilake and smith 2020a 2020b lilhare et al 2020 krogh et al 2017 razavi and gupta 2016a 2016b report that vars outperforms sobol indices in two main aspects 1 it provides a more intuitive assessment of sensitivities and the importance of model inputs in determining the model output 2 it computes the total order effect with a much higher computational efficiency up to two orders of magnitude more efficient in the present work we explore these results and benchmark vars against one of the best sobol indices estimator that of jansen 1999 before engaging in the discussion we briefly recall hereafter some useful formulae needed to understand the two approaches 1 1 sobol indices the apparatus of variance based sensitivity indices described by sobol 1993 and extended by homma and saltelli 1996 is currently considered as the recommended practice in sa saltelli et al 2008 for a model of k factors f x x 1 x 2 x k r k the first order sensitivity index s i can be written as 1 s i v x i e x i y x i v y the inner mean in equation 1 is taken over all factors but x i x i while the outer variance is taken over x i v y is the unconditional variance of the output variable y when the factors are independent s i can be defined as a first order term in the variance decomposition of y 2 1 i 1 k s i i i j s i j s 1 2 k s i lends itself to be expressed in plain english as the fractional reduction in the variance of y which would be obtained on average if x i could be fixed this is because 3 v y v x i e x i y x i e x i v x i y x i e x i v x i y x i is the average variance that would be left after fixing x i to a given value in its uncertainty range for this reason v x i e x i y x i must be the average reduction in variance as discussed above while v x i y x i can be greater than v y e x i v x i y x i is always smaller than v y as per equation 3 another useful variance based measure is the total order index t i homma and saltelli 1996 which measures the first order effect of a model input jointly with its interactions up to the k th order 4 t i e x i v x i y x i v y the index is called total because it includes all factors in the variance decomposition see equation 2 that include the index i for instance for a model with three factors t 1 s 1 s 1 2 s 1 3 s 1 2 3 and likewise for t 2 or t 3 the meaning of t i is the fraction of variance that would remain on average if x i is left to vary over its uncertainty range while all other factors are fixed note that the theory of variance based measures is as flexible as to accommodate group or set sensitivities these are simply the first order effect of a set of factors if u is the set of factors x 1 x 2 then s u s 1 s 2 s 1 2 1 2 vars vars is based on variogram analysis to characterise the spatial structure and variability of a given model output across the input space razavi and gupta 2016a 2016b let us again consider a function of factors f x x 1 x 2 x k r k if x a and x b are two generic points separated by a distance h then the variogram γ is calculated as 5 γ x a x b 1 2 v y x a y x b and the covariogram c as 6 c x a x b c o v y x a y x b note that 7 v y x a y x b v y x a v y x b 2 c o v y x a y x b given that v y x a v y x b then 8 γ x a x b v y x c x a x b as mentioned the points x a x b are spaced by a fixed distance and v c o v are the variance and covariance respectively note that γ is defined by the interval separating x a x b to make this clearer one can write h x a x b with h h 1 h 2 h n so that 9 γ h 1 2 e y x h y x 2 where the term e 2 in the expression of the variance as the expectation of the square minus the square of the expectation v e 2 e 2 is assumed to be zero the practical formula for computing a multidimensional variogram is 10 γ h 1 2 n h y x a y x b 2 where the sum is extended to all n h couples of points x a x b such that their modulo distance x a x b is h razavi and gupta 2016a 2016b suggest some integral measures based on variogram γ i e the integrated variogram γ h i 11 γ h i 0 h i γ h i d h i and recommend the use of ivars10 ivars30 and ivars50 computed for h equal to 10 30 and 50 of the factor range respectively to explore larger fractions of the variation space of the function with ivars50 corresponding to the entire interval in variogram analysis the maximum meaningful range is one half of the factor range cressie 2015 of important practical use as we shall see is the directional variogram along one of the axes of the factors space 12 γ h i 1 2 e y x 1 x i 1 h i x n y x 1 x i x n 2 which is evidently computed on all couples of points spaced h i along the x i axis with all other factors being kept fixed note that the difference in brackets is what is called in saltelli et al 2010 a step along the x i direction which is fungible to compute the total sensitivity index t i the equivalent of equation 8 for the case of the unidirectional variogram γ h i is 13 γ x i h i v y x i c x i h i where x i is a fixed point in the space of non x i in order for vars to compute the total order index t i labeled as vars to by razavi and gupta 2016a the authors suggest taking the mean value across the factors space on both sides of equation 13 thus obtaining 14 e x i γ x i h i e x i v y x i e x i c x i h i which can also be written as 15 e x i γ x i h i v y t i e x i c x i h i and therefore 16 t i vars to e x i γ x i h i e x i c x i h i v y 2 the issue of intuitiveness and importance in a paper immediately preceding vars razavi and gupta 2015 already stressed two main drawbacks of global sensitivity analysis 1 the incapacity of variance based sobol indices to appraise the spatial distribution of the model response 2 the dependence of the morris 1991 approach on the step size defined by the analyst which can significantly condition the final sensitivity value vars was presented as a comprehensive approach which overcomes these drawbacks by encapsulating in a single sensitivity framework a unified assessment of local and global sensitivity razavi and gupta 2015 p 3090 the fact that integrated variogram measures such as ivars10 ivars30 and ivars50 are able to differentiate sensitivities as a function of scale h whereas sobol indices do not is taken as proof of the limitations of the latter according to razavi and gupta 2016a pp 427 428 433 434 this endows vars with a more intuitive appraisal of sensitivities razavi and gupta 2016a construct their case using several functions which we reproduce hereafter in fig 1 a sobol indices do not differentiate f 3 from f 1 whereas vars points towards f 3 as the most sensitive function in fig 1b variance based methods equate f 1 with f 2 because they have identical variance according to razavi and gupta 2016a p 428 this runs counter to our intuitive notion of sensitivity given the multimodality of f 2 if vars is used f 2 is identified as more sensitive than f 1 for 0 h 0 2 in fig 1c sobol indices do not detect the periodicities of f 2 which razavi and gupta suggest might be important in evaluating the impact of a factor from the perspective of model calibration in fig 1d variance based methods regard f 2 as more sensitive than f 1 razavi and gupta 2016a p 433 argue that this is contrary to intuition because the effect of f 1 is more complex bi modal ivars10 and ivars30 in contrast characterise f 1 as more sensitive than f 2 it is apparent that for razavi and gupta 2016a a sensitivity measure should be able to appraise the function structure our impression is that this perception of sensitivity is relevant to specific contexts e g a diagnostic setting in which one is interested in the topology of a given function however the key lies in the definition of importance pointed to by vars in which sense is f 2 more important than f 1 in fig 1b or f 3 more important than f 1 and f 2 in fig 1a if sa is used in an information quality setting kenett and shmueli 2013 when the aim is to determine which factor has the highest potential to reduce the uncertainty in the inference i e how much is gained by discovering the true value of an uncertain factor these functions might be regarded as equally sensitive the same applies to fig 1d given that f 2 changes more decidedly over the interval range than f 1 a larger reduction in uncertainty can be achieved by learning first about f 2 than about f 1 given that sa quantifies the relative influence of each model input in the model output the concept of sensitivity is ultimately linked to that of importance this is why it should be clear what do we mean when we say that a model input is important or that a model output is very sensitive to a given model input variance based methods meet this requirement by linking sa to statistical theory via anova archer et al 1997 thus defining sa as the study of how the variance in the model output is apportioned to different sources of uncertainty in the model input saltelli et al 2002 the use of variance based methods such as sobol indices are well defined and associated with clear settings saltelli and tarantola 2002 1 factors prioritization the aim is to identify the single factor that if determined i e fixed to its true but unknown value would lead to the greatest reduction in the variance of the output this is met by the first order sensitivity index s i 2 variance reduction the aim is to identify the sets of factors couples triplets and so on leading to the reduction of the output variance below a given threshold and doing this by fixing the smallest number of factors this is achieved by using set group sensitivity indices 3 factors fixing the objective is to identify factors that can be fixed anywhere in their range of variation without affecting the variance of the output this is met by the total order sensitivity index t i variance based methods clearly resolve what is meant by importance of a factor however this is not as apparent in the case of vars if a decision needs to be taken based on the inference provided by a model which of the variogram based measures ivars10 ivars30 vars50 vars to should be finally used to characterise the factors importance and what does importance mean for vars razavi and gupta 2016a s statement of vars being more intuitive than sobol indices is open to debate intuition is in the eyes of the beholder while solid criteria underpin the methodological quality of sobol indices one way of gaining a factual insight into the alleged intuitiveness of vars is through the analysis of its use by the 86 studies that have cited razavi and gupta 2016a 2016b up until november 2020 if adopted and used by practitioners other than the vars authors themselves and if the vars framework is applied as recommended by their designers i e by exploring different ranges of the spatial structure of the model response through integrated variograms ivars and vars to then the claim by razavi and gupta 2016a 2016b of vars being an instinctive user friendly framework will find empirical support we observed that 53 studies 62 cite razavi and gupta 2016a 2016b but do not implement vars in any specific sensitivity analysis of the 33 studies that do apply vars 13 40 include either razavi and or gupta as lead author s or co authors hence the number of papers that use vars and are not contributed by vars authors amounts to 20 23 of all vars citations fig 2 a and b out of the 33 studies that do use vars there were nine from which we could not retrieve precise information on the vars metric s used as for the remaining 24 15 studies used just one vars metric 11 ivars50 and four vars to two used two ivars50 and vars to three used three ivars10 ivars30 ivars50 x 2 ivars10 ivars50 vars to and four studies used all four metrics the contributions by authors other than razavi and gupta have strongly leaned towards the use of a single summary measure out of the 12 works for which we could retrieve information on the vars metric used nine relied merely on one metric six on ivars50 and three on vars to with one study using two three and all four vars measures with regard to the sensitivity settings vars has largely been applied to models with up to 20 parameters with mesh being the model with the highest dimensionality 111 the number of stars has been mostly set between 20 and 50 with a single study raising it to 1000 a large number of works have used h 0 1 with the minimum and maximum h values being 0 01 and 0 3 fig 2c e as yet such results place the intuitive nature of vars in a disputable position although cited its use as a sensitivity measure has been comparatively moderate and most authors have preferred a single summary vars metric ivars50 or vars to both very similar to the sobol total order index razavi and gupta 2016a p 434 rather than implementing and interpreting the whole integrated variogram approach the discussion above leads to another aspect listed by razavi and gupta 2016a p 423 as a motivation for developing vars an ambiguous characterization of sensitivity different sa methods are based in different philosophies and theoretical definitions of sensitivity the absence of a unique definition for sensitivity can result in different even conflicting assessments of the underlying sensitivities for a given problem 2 2 the extent to which this points to an ambiguity is unclear in any discipline including statistics different methods may naturally exist which become useful in different applications for instance the linear relation between two variables x and y might be modelled with ordinary least squares ols if x causes y or with standard major axis sma if it is unclear which variable is the predictor and which one is the response smith 2009 does this mean that the characterisation of residuals in regression analysis is an ambiguous branch of statistics we argue that the source of ambiguity in sensitivity analysis is not the lack of a unifying theory or the fact that many sensitivity measures are available but in the definition of importance unless the analyst stipulates what she means when she says that a variable is important different methods can be thrown at the model resulting in different ordering of importance of the input variables whereby the analyst could be tempted to cherry pick the method most conforming to one s own bias by linking the definition of importance to clear settings sobol indices resolve this quandary clearly and transparently and provide end users with a plain english description of the results this comes in handy when the receiver customer of the analysis is not another practitioner the expedient to produce functions where the validity of sobol indices is downplayed is quite common this approach was also taken by liu et al 2006 and pianosi and wagener 2015 using liu s highly skewed function y x 1 x 2 where x 1 χ 2 10 and x 2 χ 2 13 978 fig 3 the reader might wonder why one of the degrees of freedom is expressed with two digit precision and the other with a five digits one the reason is that with these crisp numbers t 1 and t 2 are identical and equal to 0 5462 while inspection of fig 3b should convince us that x 1 is more important than x 2 by virtue of its longer tail the liu function is thus what lakatos et al 1976 would have called a monster example designed on purpose to invalidate variance based methods however based on the definition of importance of sobol indices the fact that they are equally influential appears totally reasonable we conclude by stating that rather than hinting at what should or not should be intuitive a sensitivity index should pin down its definition of importance in unambiguous terms 3 the issue of efficiency razavi and gupta 2016a 2016b claim that vars to is much more computationally efficient than the total order estimator of saltelli et al 2008 eq 4 23 up to two orders of magnitude which is taken as a state of the art implementation of the sobol approach they make their case with three different models 1 the six dimensional response surface displayed in fig 1d which is a purely additive model vars to accurately ranks the model inputs with just 60 simulations beating the saltelli et al 2008 estimator of total order indices at 6 000 simulations razavi and gupta 2016a pp 435 436 2 the five dimensional conceptual rainfall runoff model hymod vrugt et al 2003 vars to detects the true ranking of the model inputs at 500 simulations while the saltelli et al 2008 estimator requires 10 000 simulations razavi and gupta 2016b pp 443 444 3 the 45 dimensional land surface scheme hydrology model mesh pietroniro et al 2007 the vars to estimate of the total order effect stabilizes at 5000 simulations whereas the saltelli et al 2008 estimator requires more than 100 000 simulations razavi and gupta 2016b pp 453 454 do these examples truly prove that vars to is between 20 and 100 times more efficient than the sobol based approach to total order indices 3 1 the case of the six dimensional response surface model to properly answer this question in the case of the six dimensional model fig 1d whose functional form reads as 17 g 1 x 1 sin π x 1 0 3 sin 3 33 π x 1 g 2 x 2 0 76 sin π x 2 0 2 0 315 g 3 x 3 0 12 sin 1 05 π x 3 0 2 0 02 sin 95 24 π x 3 0 96 g 4 x 4 0 12 sin 1 05 π x 4 0 2 0 96 g 5 x 5 0 05 sin π x 5 0 2 1 02 g 6 x 6 1 08 y f g 1 x 1 g 2 x 2 g 6 x 6 we should first focus on the sampling design of vars and sobol indices the computation of vars relies on stars and is referred to as star vars by razavi and gupta 2016b the analyst first randomly selects n s t a r points across the factor space i e via random numbers latin hypercube sampling lhs or sobol quasi random numbers qrn these are the star centres and their location can be denoted as s v s v 1 s v i s v k where v 1 2 n s t a r then for each star centre a cross section of equally spaced points δ h apart needs to be generated for each of the k factors including and passing through the star centre fig 4 left side plot the cross section is produced by fixing s v i and varying s i finally for each factor all pairs of points with h values of δ h 2 δ h 3 δ h and so on should be extracted the total computational cost of this design is n t n s t a r k 1 δ h 1 1 sobol indices also rely on a star based sampling strategy they require a n 2 k base sample matrix designed via lhs or qrn in which the rightmost k columns are allocated to an a matrix and the leftmost k columns to a b matrix then k extra n k a b i matrices are created where all columns come from a except the i th which comes from b this design creates stars with centres and points a step away in the x i direction fig 4 right side plot the cost of this design for t i is n t n k 1 where n is the row dimension of the base sample matrix when the function or the model under study is fully additive as in the six dimensional surface model mentioned above fig 1d the computation of vars to can be done with a single cross section in the space of x i for each model input vars to thus becomes a first order index de facto as one model input remains constant while all the others vary the natural term of comparison is thus the sobol first order index and not the total in that sense and for any function which behaves non additively for at least one factor i e f f x i g x i the first order effect s i can be computed very easily since 18 s i e x i f x i 2 e x i 2 f x i v f i e s i is only a function of x i and hence it can be computed with a single trajectory along x i irrespective of its position in x i we provide the proof in section 2 1 of the supplementary materials we used equation 18 to compute s i for the six dimensional model aiming at replicating the results by razavi and gupta 2016a see their fig 6 for vars to and sobol based indices they tested their probability of failure defined as the probability of obtaining erroneous ranks for the model inputs of the six dimensional model fig 1d and equation 17 we observed that if equation 18 is used to compute sobol based indices all model inputs are accurately ranked at n t 896 fig 5a contrasting with the n t 6 000 obtained by razavi and gupta 2016a this example suggests that vars to is indeed more efficient than a sobol based approach when the model is fully additive and the aim is to rank the parameters but significantly less than what the authors claimed it to be it is also worth noting that there are other approaches that might permit a more efficient computation of first order indices plischke 2010 mara et al 2017 strong et al 2012 however why do razavi and gupta rely exclusively on the probability of failure in the ranking as a performance metric sorting the parameters by their influence in the model output is indeed a common setting in sensitivity analysis however other goals may exist the analyst might be more interested in getting exact values for the sensitivity indices in order to ascertain for instance how much the uncertainty would be reduced if the true value of an uncertain factor is discovered in such context a performance measure such as the mean absolute error mae between the estimated t ˆ and the analytical t values might be more appropriate the mae has been a very widespread performance measure in sensitivity analysis saltelli et al 2010 lo piano et al 2020 and is computed as 19 mae 1 p v 1 p i 1 k t i t ˆ i k where p is the number of replicas of the sample matrix and t i and t ˆ i the analytical and the estimated total order index of the i th input had razavi and gupta relied on the mae rather than on the probability of failure as a performance measure their assessment of the efficiency of vars to and sobol based indices would have been very different throughout the range of explored model runs vars to never approaches equation 18 in terms of accuracy fig 5b these results exemplify how sensitive the outcome of a benchmarking exercise can be to the particular settings defined by the analyst merely the use of a different performance measure can completely tip the balance from one estimator to another in the section below we show how to minimize this source of bias to get a more accurate picture of the true performance of vars to compared to sobol based estimators puy et al 2020a 3 2 the case of the hymod and mesh models unlike the six dimensional model hymod and mesh are non additive models hence a single trajectory is not enough and several cross sections in the space of x i should be drawn to fully explore the hypercube under such settings the comparison between vars to and a sobol based estimator of the total order index is the appropriate methodological choice but does the higher accuracy of vars to reported by razavi and gupta 2016a 2016b for these two models truly evidence its superiority over sobol based total order indices we argue that the following issues make razavi and gupta 2016a 2016b s claim controversial the use of the saltelli et al 2008 total order estimator as state of the art amongst all estimators available for computing t i that of saltelli et al 2008 ranks close to last on accuracy and performance and is significantly outperformed by the jansen or the janon monod estimators jansen 1999 puy et al 2020a monod et al 2006 janon et al 2014 furthermore saltelli et al 2010 demonstrated that configurations based on b b a i matrices as is the case of the saltelli estimator were surpassed in performance by those relying on a a b i matrices i e the jansen estimator when quasi random numbers were used to create the sample matrix the extrapolation of the results obtained with hymod and mesh to mean that vars to is generally better than sobol based indices puy et al 2020a recently showed that once the benchmark settings are randomised i e the model and its dimensionality the sampling method the total number of model runs the fraction of active second and third order effects the distribution of the model inputs and the performance measure vars to loses much of its purported computational superiority it only very slightly outperforms the sobol based estimators jansen 1999 and janon monod monod et al 2006 janon et al 2014 when there are serious constraints on the number of model runs that can be allocated to each model input i e 2 10 at larger sample sizes the performances of vars and jansen and janon monod are exactly the same puy et al 2020a this randomisation is required to reduce the dependence of the results on the choices taken by the analyst as we have seen in the case of the six dimensional model see section 3 1 even the use of a given performance measure over another might completely swap the outcome of an analysis in order to obtain a more comprehensive view of the performance of vars to against sobol indices we have reproduced the work by puy et al 2020a with the following changes and or additions 1 we have tested vars to against the jansen 1999 formula one of the most precise and accurate sobol total order estimators puy et al 2020a 2 we have increased the range of the proportion of active second and third order effects in the test functions i e between 50 100 and 30 100 respectively in puy et al 2020a they ranged between 30 50 and 10 30 respectively this aimed at checking how vars to performs under serious non additivities 3 we have taken into account the algorithmic uncertainties of vars to i e the number of stars n s t a r and the distance between pairs h which ultimately condition its computational cost these design parameters need to be set by the analyst before executing the algorithm and the specific value that might work best is unclear different authors have used different values for h δ h 0 002 δ h 0 1 δ h 0 3 19 20 23 47 see fig 2e razavi et al 2019 recommend h 0 1 and h 0 1 if more accurate results are needed as shown by puy et al 2020b for pawn the uncertainty in the design parameters of a sensitivity index might contribute appreciably to its volatility we compared the performance of vars to and the jansen estimator by treating the main benchmark settings listed in table 1 as uncertain parameters described by probability distributions we created a 2 12 k sample matrix using sobol 1967 1976 quasi random numbers in which each row is a sample point and each column an uncertain parameter for v 1 2 2 12 rows we computed vars to and the jansen total order index according to the specifications set by n s t a r v h v δ v the final model output was r v the correlation coefficient between the indices estimated by vars to and jansen t ˆ v and the true indices t v computed with a large sample size n 2 12 and the jansen 1999 estimator the larger the r v the better the estimation of the true sensitivity indices by vars to or jansen we argue that this approach allows us to examine the accuracy of vars to more comprehensively given the enormous range of sensitivity problems that it is able to explore puy et al 2020a becker 2020 potentially more than 3 billion scenarios in this case if vars to outperforms sobol based indices unequivocally as asserted by razavi and gupta 2016a 2016b its computational advantage should emerge against jansen as well the supplementary materials thoroughly detail the rationale and the execution of the experiment fig 6a shows that both jansen and vars to are very accurate as the empirical distribution of r is highly right skewed if anything jansen seems to outperform vars to overall due to its slightly narrower distribution 95 ci 0 93 0 99 median of 0 99 for jansen 95 ci 0 87 0 99 median of 0 97 for vars this is also apparent in fig 6b with vars to presenting more simulations with redder orange colours approx r 0 85 a closer look at the performance of both approaches reveals that jansen maintains a higher median accuracy at higher dimensions fig 7 a whereas vars to confirms its slightly higher efficiency only when the number of runs that can be allocated per model input n t k is considerably constrained 50 in this case fig 7b puy et al 2020a vars to also displays a larger volatility at 100 n t k fig 7b suggesting that jansen might become more stable in a larger number of sensitivity problems if the number of model runs per input is increased these results rest on solid grounds as the number of simulations for which we have computed the median n t k is almost identical for jansen and vars to fig 7c overall this proves that both estimators have a very similar efficiency and reliability we also computed sobol indices to assess which uncertain parameter most influences the performance of vars to fig 8 we observed that c 30 the variance in its performance is driven by the underlying probability distribution of the model inputs φ which appears as the most influential parameter the other parameters are important through interactions especially the functional form of the model ε the sampling method τ the model dimensionality k and the performance measure selected δ in that order the proportion of second and third order effects k 2 k 3 has no effect which means that vars to is very robust against non additivities compared to jansen vars to significantly underperformed when the model inputs were normally distributed e g when φ 2 figs s5 9 we observed that this was caused by high order interactions between the sampling design of vars to fig 4 left side and at least five different uncertain parameters n s t a r h k φ τ to understand these interactions let us first assume that we use random numbers τ 1 to sample our star centres which razavi et al 2019 table 1 list as a possible sampling strategy to compute vars to these star centres are located at s v s v 1 s v i s v k where v 1 2 n s t a r the higher the n s t a r and k the higher the chances that a value at the boundary of 0 1 is included in s v given that vars to requires fixing s v i while varying s i at a step defined by h this value at the periphery of 0 1 will be repeated in the 1 h 1 k 1 coordinates which can be manifold if k is high and h is low once the model inputs are transformed into a normal distribution it will turn into an extreme value and will disrupt both the model output and the computation of vars to for the x i parameters let us now assume that we do not use random numbers to sample the star centres but sobol quasi random qrn number sequences τ 2 they are also contemplated by razavi et al 2019 table 1 as a sampling strategy to compute vars to although the design of qrn makes the sampling of star centres at the very periphery 0 1 very unlikely cross sections can indeed sample the boundary of the domain for instance if h 0 1 and s v i 0 5 the cross section of the x i parameter will be the vector x i 0 1 0 2 s v i 1 this will cause vars to to crash as 1 becomes infinity under a normal distribution even if the star vars algorithm is modified to prevent 1 from being sampled e g by replacing 1 by 0 999 as we did in this study some cross sections will still sample values very close to 1 by design especially if h is set at a small value these values will be extreme values under a normal distribution disrupting again the computation of the model output and the vars to index in this case for the x i parameter we believe that this explains the high order interactions involving n s t a r h k φ τ which are non negligible fig 8 the effect of n s t a r and h in vars to was not explored by puy et al 2020a nor by becker 2020 who documented a slightly higher performance of vars to against jansen and janon monod our results indicate that vars to loses this marginal edge once these internal uncertainties jointly with the uncertainties in the benchmark settings are considered in the computations even if the use of vars to is restricted to non normal distributions φ 2 its performance would still be slightly outdone by jansen 95 ci 0 96 0 99 median of 0 99 for jansen 95 ci 0 94 0 99 median of 0 97 for vars 4 conclusions we have revised the variogram analysis of response surfaces vars a new framework for sensitivity analysis developed by razavi and gupta 2016a 2016b we have specifically focused on two aspects that according to razavi and gupta 2016a 2016b make vars outperform sobol indices its more intuitive appraisal of sensitivities and of the importance of model inputs and its 20 100 times higher computational efficiency the claim that vars is more intuitive than sobol indices can hardly be reversed as it ultimately is a matter of personal taste disciplinary orientation and objective of the modeling activity a geographer working in a diagnostic model setting might indeed prefer vars s approach to the model structure due to its capacity to distinguish response surfaces however the professed higher intuitive nature of vars ties in poorly with the available evidence less than one half of the studies citing vars actually implement it in a case study and almost half of those that use it include the vars authors themselves furthermore most works do not explore the full range of the response surface but rely exclusively on summary metrics such as ivars50 or vars to which are very similar to the sobol total order index we argue that sobol indices provide a clearer description of what an important model input is given its connection to statistical theory and anova the use of sobol first or total order indices is associated with clear research settings and their meaning can be easily conveyed to stakeholders or non specialists which adds to their transparency vars in contrast allows the analyst to zoom into the structure of the model output and assess its dependency on the model inputs through the integrated variograms ivars10 ivars30 and ivars50 as well as through the variance based total order effect vars to but what is their definition of importance how useful is it for a stakeholder to know that a parameter is important under ivars10 and not as much under ivars30 for instance which ivars measure should she finally rely onto before making a policy decision if the answer is the summary measure vars to then it is unclear how vars advances sobol indices given the reliance of vars to on variance and covariance matrices the purported much higher efficiency of vars to is very contentious the observation that it is more than 100 times more efficient than sobol based total order indices rests on an exercise with a fully additive model in which vars to is compared against one of the less accurate total order estimators saltelli et al 2008 see puy et al 2020a and the performance measure chosen is the probability of failure of properly ranking the model inputs if the comparison is instead conducted between vars to and a lightweight sobol based first order estimator equation 18 the advantage of the former shrinks to being only 15 times more efficient vars to completely loses all its edge if the mean absolute error mae replaces the probability of failure as a performance measure in this setting equation 18 is the one showing an accuracy up to 100 times higher than vars to nevertheless the advantage of vars to over sobol based indices is still remarkable if the goal is to rank parameters and suggests that vars to should be the sensitivity measure of choice if computational efficiency is a priority and the model is additive however this condition is unlikely to apply to models of the earth and environmental domain either because they encompass multiplicative terms and exponentials or because their mathematical complexity prevents the analyst from knowing their behavior before running the simulations the assertion that vars to is at least 20 times more efficient than sobol based total order indices is not confirmed by our results vars to only very slightly outperforms one of the most accurate sobol total order estimators that of jansen 1999 when the number of model runs per model input is very small however it comes second to jansen at increasing dimensionalities and in overall performance such results have been obtained after randomising the benchmark settings thus creating a set of sensitivity problems much wider than those represented by the hymod and mesh models and by simultaneously examining the internal uncertainties of vars to n s t a r h its sampling design makes it especially vulnerable to the high order interactions between the sampling method τ the number of stars n s t a r the function dimensionality k the distance between pairs h and the underlying distribution of the model inputs φ especially if they follow a normal distribution vars nonetheless represents a relevant addition to the family of sensitivity analysis methods with the additional merit of having been developed to appraise the response surface of a model furthermore the conceptual framework of vars comes with a software described as next generation by razavi 2019 time will tell whether vars ends up unseating sobol based indices as the recommended best practice in sensitivity analysis code availability fully documented code is freely available in puy 2020 and in github https github com arnaldpuy vars paper data availability a csv file of the studies citing vars as of november 2020 can be found in github https github com arnaldpuy vars paper declaration of competing interest we do not have any conflict of interest acknowledgements this work has been funded by the european commission marie skłodowska curie global fellowship grant number 792178 to a p appendix a supplementary data the following is are the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 104960 
25883,the variogram analysis of response surfaces vars has been proposed by razavi and gupta as a new comprehensive framework in sensitivity analysis according to these authors vars provides a more intuitive notion of sensitivity and is much more computationally efficient than sobol indices here we review these arguments and critically compare the performance of vars to for total order index against the total order jansen estimator we argue that unlike classic variance based methods vars lacks a clear definition of what an important factor is and we show that the alleged computational superiority of vars does not withstand scrutiny we conclude that while vars enriches the spectrum of existing methods for sensitivity analysis especially for a diagnostic use of mathematical models it complements rather than replaces classic estimators used in variance based sensitivity analysis keywords uncertainty sensitivity analysis modeling statistics design of experiment 1 introduction sensitivity analysis sa explores how uncertainty in the output of a model numerical or otherwise can be apportioned to different sources of uncertainty in the model input space saltelli 2002 1 1 this article is part of a si on sensitivity analysis for environmental modeling sa is especially needed when complex models which often formalize partially known processes and include non linear relations are used to guide policies in the real world this is generally the case of models in the environmental sciences domain e g on crop water requirements water availability under climate change weather forecasting surface runoff or precipitation and evaporation processes döll and siebert 2002 pappenberger et al 2011 vieux and vieux 2016 wang et al 2020 the uncertainties in these models might be either parametric i e exact values for parameters might be unknown there might be errors in the measurement or structural i e lack of knowledge on the underlying processes multiple ways of modeling the same phenomenon and their combined effect on the model output should be understood to guarantee a robust inference for policy making in this context sa jointly with uncertainty analysis is regarded as an unavoidable step to ensure the quality of the modeling process borgonovo and plischke 2016 eker et al 2018 jakeman et al 2006 saltelli 2019 saltelli et al 2020 tarantola et al 2002 in sa as in all fields of computational research different strategies and methods compete to establish themselves as good recommended or best practices while variance based methods and sobol indices are deemed to belong to the class of recommended methods saltelli et al 2008 other approaches have been proposed to complement or overcome their limitations i e entropy based methods liu et al 2006 the δ measure borgonovo 2007 the kuiper metric baucells and borgonovo 2013 or the pawn index pianosi and wagener 2015 2018 one of the most recent competitors is the variogram analysis of response surfaces vars proposed by razavi and gupta 2016a 2016b according to google scholar and as of november 2020 the two foundational vars papers have been cited 86 times and seem to have been especially embraced by hydrologists and water scientists jayathilake and smith 2020a 2020b lilhare et al 2020 krogh et al 2017 razavi and gupta 2016a 2016b report that vars outperforms sobol indices in two main aspects 1 it provides a more intuitive assessment of sensitivities and the importance of model inputs in determining the model output 2 it computes the total order effect with a much higher computational efficiency up to two orders of magnitude more efficient in the present work we explore these results and benchmark vars against one of the best sobol indices estimator that of jansen 1999 before engaging in the discussion we briefly recall hereafter some useful formulae needed to understand the two approaches 1 1 sobol indices the apparatus of variance based sensitivity indices described by sobol 1993 and extended by homma and saltelli 1996 is currently considered as the recommended practice in sa saltelli et al 2008 for a model of k factors f x x 1 x 2 x k r k the first order sensitivity index s i can be written as 1 s i v x i e x i y x i v y the inner mean in equation 1 is taken over all factors but x i x i while the outer variance is taken over x i v y is the unconditional variance of the output variable y when the factors are independent s i can be defined as a first order term in the variance decomposition of y 2 1 i 1 k s i i i j s i j s 1 2 k s i lends itself to be expressed in plain english as the fractional reduction in the variance of y which would be obtained on average if x i could be fixed this is because 3 v y v x i e x i y x i e x i v x i y x i e x i v x i y x i is the average variance that would be left after fixing x i to a given value in its uncertainty range for this reason v x i e x i y x i must be the average reduction in variance as discussed above while v x i y x i can be greater than v y e x i v x i y x i is always smaller than v y as per equation 3 another useful variance based measure is the total order index t i homma and saltelli 1996 which measures the first order effect of a model input jointly with its interactions up to the k th order 4 t i e x i v x i y x i v y the index is called total because it includes all factors in the variance decomposition see equation 2 that include the index i for instance for a model with three factors t 1 s 1 s 1 2 s 1 3 s 1 2 3 and likewise for t 2 or t 3 the meaning of t i is the fraction of variance that would remain on average if x i is left to vary over its uncertainty range while all other factors are fixed note that the theory of variance based measures is as flexible as to accommodate group or set sensitivities these are simply the first order effect of a set of factors if u is the set of factors x 1 x 2 then s u s 1 s 2 s 1 2 1 2 vars vars is based on variogram analysis to characterise the spatial structure and variability of a given model output across the input space razavi and gupta 2016a 2016b let us again consider a function of factors f x x 1 x 2 x k r k if x a and x b are two generic points separated by a distance h then the variogram γ is calculated as 5 γ x a x b 1 2 v y x a y x b and the covariogram c as 6 c x a x b c o v y x a y x b note that 7 v y x a y x b v y x a v y x b 2 c o v y x a y x b given that v y x a v y x b then 8 γ x a x b v y x c x a x b as mentioned the points x a x b are spaced by a fixed distance and v c o v are the variance and covariance respectively note that γ is defined by the interval separating x a x b to make this clearer one can write h x a x b with h h 1 h 2 h n so that 9 γ h 1 2 e y x h y x 2 where the term e 2 in the expression of the variance as the expectation of the square minus the square of the expectation v e 2 e 2 is assumed to be zero the practical formula for computing a multidimensional variogram is 10 γ h 1 2 n h y x a y x b 2 where the sum is extended to all n h couples of points x a x b such that their modulo distance x a x b is h razavi and gupta 2016a 2016b suggest some integral measures based on variogram γ i e the integrated variogram γ h i 11 γ h i 0 h i γ h i d h i and recommend the use of ivars10 ivars30 and ivars50 computed for h equal to 10 30 and 50 of the factor range respectively to explore larger fractions of the variation space of the function with ivars50 corresponding to the entire interval in variogram analysis the maximum meaningful range is one half of the factor range cressie 2015 of important practical use as we shall see is the directional variogram along one of the axes of the factors space 12 γ h i 1 2 e y x 1 x i 1 h i x n y x 1 x i x n 2 which is evidently computed on all couples of points spaced h i along the x i axis with all other factors being kept fixed note that the difference in brackets is what is called in saltelli et al 2010 a step along the x i direction which is fungible to compute the total sensitivity index t i the equivalent of equation 8 for the case of the unidirectional variogram γ h i is 13 γ x i h i v y x i c x i h i where x i is a fixed point in the space of non x i in order for vars to compute the total order index t i labeled as vars to by razavi and gupta 2016a the authors suggest taking the mean value across the factors space on both sides of equation 13 thus obtaining 14 e x i γ x i h i e x i v y x i e x i c x i h i which can also be written as 15 e x i γ x i h i v y t i e x i c x i h i and therefore 16 t i vars to e x i γ x i h i e x i c x i h i v y 2 the issue of intuitiveness and importance in a paper immediately preceding vars razavi and gupta 2015 already stressed two main drawbacks of global sensitivity analysis 1 the incapacity of variance based sobol indices to appraise the spatial distribution of the model response 2 the dependence of the morris 1991 approach on the step size defined by the analyst which can significantly condition the final sensitivity value vars was presented as a comprehensive approach which overcomes these drawbacks by encapsulating in a single sensitivity framework a unified assessment of local and global sensitivity razavi and gupta 2015 p 3090 the fact that integrated variogram measures such as ivars10 ivars30 and ivars50 are able to differentiate sensitivities as a function of scale h whereas sobol indices do not is taken as proof of the limitations of the latter according to razavi and gupta 2016a pp 427 428 433 434 this endows vars with a more intuitive appraisal of sensitivities razavi and gupta 2016a construct their case using several functions which we reproduce hereafter in fig 1 a sobol indices do not differentiate f 3 from f 1 whereas vars points towards f 3 as the most sensitive function in fig 1b variance based methods equate f 1 with f 2 because they have identical variance according to razavi and gupta 2016a p 428 this runs counter to our intuitive notion of sensitivity given the multimodality of f 2 if vars is used f 2 is identified as more sensitive than f 1 for 0 h 0 2 in fig 1c sobol indices do not detect the periodicities of f 2 which razavi and gupta suggest might be important in evaluating the impact of a factor from the perspective of model calibration in fig 1d variance based methods regard f 2 as more sensitive than f 1 razavi and gupta 2016a p 433 argue that this is contrary to intuition because the effect of f 1 is more complex bi modal ivars10 and ivars30 in contrast characterise f 1 as more sensitive than f 2 it is apparent that for razavi and gupta 2016a a sensitivity measure should be able to appraise the function structure our impression is that this perception of sensitivity is relevant to specific contexts e g a diagnostic setting in which one is interested in the topology of a given function however the key lies in the definition of importance pointed to by vars in which sense is f 2 more important than f 1 in fig 1b or f 3 more important than f 1 and f 2 in fig 1a if sa is used in an information quality setting kenett and shmueli 2013 when the aim is to determine which factor has the highest potential to reduce the uncertainty in the inference i e how much is gained by discovering the true value of an uncertain factor these functions might be regarded as equally sensitive the same applies to fig 1d given that f 2 changes more decidedly over the interval range than f 1 a larger reduction in uncertainty can be achieved by learning first about f 2 than about f 1 given that sa quantifies the relative influence of each model input in the model output the concept of sensitivity is ultimately linked to that of importance this is why it should be clear what do we mean when we say that a model input is important or that a model output is very sensitive to a given model input variance based methods meet this requirement by linking sa to statistical theory via anova archer et al 1997 thus defining sa as the study of how the variance in the model output is apportioned to different sources of uncertainty in the model input saltelli et al 2002 the use of variance based methods such as sobol indices are well defined and associated with clear settings saltelli and tarantola 2002 1 factors prioritization the aim is to identify the single factor that if determined i e fixed to its true but unknown value would lead to the greatest reduction in the variance of the output this is met by the first order sensitivity index s i 2 variance reduction the aim is to identify the sets of factors couples triplets and so on leading to the reduction of the output variance below a given threshold and doing this by fixing the smallest number of factors this is achieved by using set group sensitivity indices 3 factors fixing the objective is to identify factors that can be fixed anywhere in their range of variation without affecting the variance of the output this is met by the total order sensitivity index t i variance based methods clearly resolve what is meant by importance of a factor however this is not as apparent in the case of vars if a decision needs to be taken based on the inference provided by a model which of the variogram based measures ivars10 ivars30 vars50 vars to should be finally used to characterise the factors importance and what does importance mean for vars razavi and gupta 2016a s statement of vars being more intuitive than sobol indices is open to debate intuition is in the eyes of the beholder while solid criteria underpin the methodological quality of sobol indices one way of gaining a factual insight into the alleged intuitiveness of vars is through the analysis of its use by the 86 studies that have cited razavi and gupta 2016a 2016b up until november 2020 if adopted and used by practitioners other than the vars authors themselves and if the vars framework is applied as recommended by their designers i e by exploring different ranges of the spatial structure of the model response through integrated variograms ivars and vars to then the claim by razavi and gupta 2016a 2016b of vars being an instinctive user friendly framework will find empirical support we observed that 53 studies 62 cite razavi and gupta 2016a 2016b but do not implement vars in any specific sensitivity analysis of the 33 studies that do apply vars 13 40 include either razavi and or gupta as lead author s or co authors hence the number of papers that use vars and are not contributed by vars authors amounts to 20 23 of all vars citations fig 2 a and b out of the 33 studies that do use vars there were nine from which we could not retrieve precise information on the vars metric s used as for the remaining 24 15 studies used just one vars metric 11 ivars50 and four vars to two used two ivars50 and vars to three used three ivars10 ivars30 ivars50 x 2 ivars10 ivars50 vars to and four studies used all four metrics the contributions by authors other than razavi and gupta have strongly leaned towards the use of a single summary measure out of the 12 works for which we could retrieve information on the vars metric used nine relied merely on one metric six on ivars50 and three on vars to with one study using two three and all four vars measures with regard to the sensitivity settings vars has largely been applied to models with up to 20 parameters with mesh being the model with the highest dimensionality 111 the number of stars has been mostly set between 20 and 50 with a single study raising it to 1000 a large number of works have used h 0 1 with the minimum and maximum h values being 0 01 and 0 3 fig 2c e as yet such results place the intuitive nature of vars in a disputable position although cited its use as a sensitivity measure has been comparatively moderate and most authors have preferred a single summary vars metric ivars50 or vars to both very similar to the sobol total order index razavi and gupta 2016a p 434 rather than implementing and interpreting the whole integrated variogram approach the discussion above leads to another aspect listed by razavi and gupta 2016a p 423 as a motivation for developing vars an ambiguous characterization of sensitivity different sa methods are based in different philosophies and theoretical definitions of sensitivity the absence of a unique definition for sensitivity can result in different even conflicting assessments of the underlying sensitivities for a given problem 2 2 the extent to which this points to an ambiguity is unclear in any discipline including statistics different methods may naturally exist which become useful in different applications for instance the linear relation between two variables x and y might be modelled with ordinary least squares ols if x causes y or with standard major axis sma if it is unclear which variable is the predictor and which one is the response smith 2009 does this mean that the characterisation of residuals in regression analysis is an ambiguous branch of statistics we argue that the source of ambiguity in sensitivity analysis is not the lack of a unifying theory or the fact that many sensitivity measures are available but in the definition of importance unless the analyst stipulates what she means when she says that a variable is important different methods can be thrown at the model resulting in different ordering of importance of the input variables whereby the analyst could be tempted to cherry pick the method most conforming to one s own bias by linking the definition of importance to clear settings sobol indices resolve this quandary clearly and transparently and provide end users with a plain english description of the results this comes in handy when the receiver customer of the analysis is not another practitioner the expedient to produce functions where the validity of sobol indices is downplayed is quite common this approach was also taken by liu et al 2006 and pianosi and wagener 2015 using liu s highly skewed function y x 1 x 2 where x 1 χ 2 10 and x 2 χ 2 13 978 fig 3 the reader might wonder why one of the degrees of freedom is expressed with two digit precision and the other with a five digits one the reason is that with these crisp numbers t 1 and t 2 are identical and equal to 0 5462 while inspection of fig 3b should convince us that x 1 is more important than x 2 by virtue of its longer tail the liu function is thus what lakatos et al 1976 would have called a monster example designed on purpose to invalidate variance based methods however based on the definition of importance of sobol indices the fact that they are equally influential appears totally reasonable we conclude by stating that rather than hinting at what should or not should be intuitive a sensitivity index should pin down its definition of importance in unambiguous terms 3 the issue of efficiency razavi and gupta 2016a 2016b claim that vars to is much more computationally efficient than the total order estimator of saltelli et al 2008 eq 4 23 up to two orders of magnitude which is taken as a state of the art implementation of the sobol approach they make their case with three different models 1 the six dimensional response surface displayed in fig 1d which is a purely additive model vars to accurately ranks the model inputs with just 60 simulations beating the saltelli et al 2008 estimator of total order indices at 6 000 simulations razavi and gupta 2016a pp 435 436 2 the five dimensional conceptual rainfall runoff model hymod vrugt et al 2003 vars to detects the true ranking of the model inputs at 500 simulations while the saltelli et al 2008 estimator requires 10 000 simulations razavi and gupta 2016b pp 443 444 3 the 45 dimensional land surface scheme hydrology model mesh pietroniro et al 2007 the vars to estimate of the total order effect stabilizes at 5000 simulations whereas the saltelli et al 2008 estimator requires more than 100 000 simulations razavi and gupta 2016b pp 453 454 do these examples truly prove that vars to is between 20 and 100 times more efficient than the sobol based approach to total order indices 3 1 the case of the six dimensional response surface model to properly answer this question in the case of the six dimensional model fig 1d whose functional form reads as 17 g 1 x 1 sin π x 1 0 3 sin 3 33 π x 1 g 2 x 2 0 76 sin π x 2 0 2 0 315 g 3 x 3 0 12 sin 1 05 π x 3 0 2 0 02 sin 95 24 π x 3 0 96 g 4 x 4 0 12 sin 1 05 π x 4 0 2 0 96 g 5 x 5 0 05 sin π x 5 0 2 1 02 g 6 x 6 1 08 y f g 1 x 1 g 2 x 2 g 6 x 6 we should first focus on the sampling design of vars and sobol indices the computation of vars relies on stars and is referred to as star vars by razavi and gupta 2016b the analyst first randomly selects n s t a r points across the factor space i e via random numbers latin hypercube sampling lhs or sobol quasi random numbers qrn these are the star centres and their location can be denoted as s v s v 1 s v i s v k where v 1 2 n s t a r then for each star centre a cross section of equally spaced points δ h apart needs to be generated for each of the k factors including and passing through the star centre fig 4 left side plot the cross section is produced by fixing s v i and varying s i finally for each factor all pairs of points with h values of δ h 2 δ h 3 δ h and so on should be extracted the total computational cost of this design is n t n s t a r k 1 δ h 1 1 sobol indices also rely on a star based sampling strategy they require a n 2 k base sample matrix designed via lhs or qrn in which the rightmost k columns are allocated to an a matrix and the leftmost k columns to a b matrix then k extra n k a b i matrices are created where all columns come from a except the i th which comes from b this design creates stars with centres and points a step away in the x i direction fig 4 right side plot the cost of this design for t i is n t n k 1 where n is the row dimension of the base sample matrix when the function or the model under study is fully additive as in the six dimensional surface model mentioned above fig 1d the computation of vars to can be done with a single cross section in the space of x i for each model input vars to thus becomes a first order index de facto as one model input remains constant while all the others vary the natural term of comparison is thus the sobol first order index and not the total in that sense and for any function which behaves non additively for at least one factor i e f f x i g x i the first order effect s i can be computed very easily since 18 s i e x i f x i 2 e x i 2 f x i v f i e s i is only a function of x i and hence it can be computed with a single trajectory along x i irrespective of its position in x i we provide the proof in section 2 1 of the supplementary materials we used equation 18 to compute s i for the six dimensional model aiming at replicating the results by razavi and gupta 2016a see their fig 6 for vars to and sobol based indices they tested their probability of failure defined as the probability of obtaining erroneous ranks for the model inputs of the six dimensional model fig 1d and equation 17 we observed that if equation 18 is used to compute sobol based indices all model inputs are accurately ranked at n t 896 fig 5a contrasting with the n t 6 000 obtained by razavi and gupta 2016a this example suggests that vars to is indeed more efficient than a sobol based approach when the model is fully additive and the aim is to rank the parameters but significantly less than what the authors claimed it to be it is also worth noting that there are other approaches that might permit a more efficient computation of first order indices plischke 2010 mara et al 2017 strong et al 2012 however why do razavi and gupta rely exclusively on the probability of failure in the ranking as a performance metric sorting the parameters by their influence in the model output is indeed a common setting in sensitivity analysis however other goals may exist the analyst might be more interested in getting exact values for the sensitivity indices in order to ascertain for instance how much the uncertainty would be reduced if the true value of an uncertain factor is discovered in such context a performance measure such as the mean absolute error mae between the estimated t ˆ and the analytical t values might be more appropriate the mae has been a very widespread performance measure in sensitivity analysis saltelli et al 2010 lo piano et al 2020 and is computed as 19 mae 1 p v 1 p i 1 k t i t ˆ i k where p is the number of replicas of the sample matrix and t i and t ˆ i the analytical and the estimated total order index of the i th input had razavi and gupta relied on the mae rather than on the probability of failure as a performance measure their assessment of the efficiency of vars to and sobol based indices would have been very different throughout the range of explored model runs vars to never approaches equation 18 in terms of accuracy fig 5b these results exemplify how sensitive the outcome of a benchmarking exercise can be to the particular settings defined by the analyst merely the use of a different performance measure can completely tip the balance from one estimator to another in the section below we show how to minimize this source of bias to get a more accurate picture of the true performance of vars to compared to sobol based estimators puy et al 2020a 3 2 the case of the hymod and mesh models unlike the six dimensional model hymod and mesh are non additive models hence a single trajectory is not enough and several cross sections in the space of x i should be drawn to fully explore the hypercube under such settings the comparison between vars to and a sobol based estimator of the total order index is the appropriate methodological choice but does the higher accuracy of vars to reported by razavi and gupta 2016a 2016b for these two models truly evidence its superiority over sobol based total order indices we argue that the following issues make razavi and gupta 2016a 2016b s claim controversial the use of the saltelli et al 2008 total order estimator as state of the art amongst all estimators available for computing t i that of saltelli et al 2008 ranks close to last on accuracy and performance and is significantly outperformed by the jansen or the janon monod estimators jansen 1999 puy et al 2020a monod et al 2006 janon et al 2014 furthermore saltelli et al 2010 demonstrated that configurations based on b b a i matrices as is the case of the saltelli estimator were surpassed in performance by those relying on a a b i matrices i e the jansen estimator when quasi random numbers were used to create the sample matrix the extrapolation of the results obtained with hymod and mesh to mean that vars to is generally better than sobol based indices puy et al 2020a recently showed that once the benchmark settings are randomised i e the model and its dimensionality the sampling method the total number of model runs the fraction of active second and third order effects the distribution of the model inputs and the performance measure vars to loses much of its purported computational superiority it only very slightly outperforms the sobol based estimators jansen 1999 and janon monod monod et al 2006 janon et al 2014 when there are serious constraints on the number of model runs that can be allocated to each model input i e 2 10 at larger sample sizes the performances of vars and jansen and janon monod are exactly the same puy et al 2020a this randomisation is required to reduce the dependence of the results on the choices taken by the analyst as we have seen in the case of the six dimensional model see section 3 1 even the use of a given performance measure over another might completely swap the outcome of an analysis in order to obtain a more comprehensive view of the performance of vars to against sobol indices we have reproduced the work by puy et al 2020a with the following changes and or additions 1 we have tested vars to against the jansen 1999 formula one of the most precise and accurate sobol total order estimators puy et al 2020a 2 we have increased the range of the proportion of active second and third order effects in the test functions i e between 50 100 and 30 100 respectively in puy et al 2020a they ranged between 30 50 and 10 30 respectively this aimed at checking how vars to performs under serious non additivities 3 we have taken into account the algorithmic uncertainties of vars to i e the number of stars n s t a r and the distance between pairs h which ultimately condition its computational cost these design parameters need to be set by the analyst before executing the algorithm and the specific value that might work best is unclear different authors have used different values for h δ h 0 002 δ h 0 1 δ h 0 3 19 20 23 47 see fig 2e razavi et al 2019 recommend h 0 1 and h 0 1 if more accurate results are needed as shown by puy et al 2020b for pawn the uncertainty in the design parameters of a sensitivity index might contribute appreciably to its volatility we compared the performance of vars to and the jansen estimator by treating the main benchmark settings listed in table 1 as uncertain parameters described by probability distributions we created a 2 12 k sample matrix using sobol 1967 1976 quasi random numbers in which each row is a sample point and each column an uncertain parameter for v 1 2 2 12 rows we computed vars to and the jansen total order index according to the specifications set by n s t a r v h v δ v the final model output was r v the correlation coefficient between the indices estimated by vars to and jansen t ˆ v and the true indices t v computed with a large sample size n 2 12 and the jansen 1999 estimator the larger the r v the better the estimation of the true sensitivity indices by vars to or jansen we argue that this approach allows us to examine the accuracy of vars to more comprehensively given the enormous range of sensitivity problems that it is able to explore puy et al 2020a becker 2020 potentially more than 3 billion scenarios in this case if vars to outperforms sobol based indices unequivocally as asserted by razavi and gupta 2016a 2016b its computational advantage should emerge against jansen as well the supplementary materials thoroughly detail the rationale and the execution of the experiment fig 6a shows that both jansen and vars to are very accurate as the empirical distribution of r is highly right skewed if anything jansen seems to outperform vars to overall due to its slightly narrower distribution 95 ci 0 93 0 99 median of 0 99 for jansen 95 ci 0 87 0 99 median of 0 97 for vars this is also apparent in fig 6b with vars to presenting more simulations with redder orange colours approx r 0 85 a closer look at the performance of both approaches reveals that jansen maintains a higher median accuracy at higher dimensions fig 7 a whereas vars to confirms its slightly higher efficiency only when the number of runs that can be allocated per model input n t k is considerably constrained 50 in this case fig 7b puy et al 2020a vars to also displays a larger volatility at 100 n t k fig 7b suggesting that jansen might become more stable in a larger number of sensitivity problems if the number of model runs per input is increased these results rest on solid grounds as the number of simulations for which we have computed the median n t k is almost identical for jansen and vars to fig 7c overall this proves that both estimators have a very similar efficiency and reliability we also computed sobol indices to assess which uncertain parameter most influences the performance of vars to fig 8 we observed that c 30 the variance in its performance is driven by the underlying probability distribution of the model inputs φ which appears as the most influential parameter the other parameters are important through interactions especially the functional form of the model ε the sampling method τ the model dimensionality k and the performance measure selected δ in that order the proportion of second and third order effects k 2 k 3 has no effect which means that vars to is very robust against non additivities compared to jansen vars to significantly underperformed when the model inputs were normally distributed e g when φ 2 figs s5 9 we observed that this was caused by high order interactions between the sampling design of vars to fig 4 left side and at least five different uncertain parameters n s t a r h k φ τ to understand these interactions let us first assume that we use random numbers τ 1 to sample our star centres which razavi et al 2019 table 1 list as a possible sampling strategy to compute vars to these star centres are located at s v s v 1 s v i s v k where v 1 2 n s t a r the higher the n s t a r and k the higher the chances that a value at the boundary of 0 1 is included in s v given that vars to requires fixing s v i while varying s i at a step defined by h this value at the periphery of 0 1 will be repeated in the 1 h 1 k 1 coordinates which can be manifold if k is high and h is low once the model inputs are transformed into a normal distribution it will turn into an extreme value and will disrupt both the model output and the computation of vars to for the x i parameters let us now assume that we do not use random numbers to sample the star centres but sobol quasi random qrn number sequences τ 2 they are also contemplated by razavi et al 2019 table 1 as a sampling strategy to compute vars to although the design of qrn makes the sampling of star centres at the very periphery 0 1 very unlikely cross sections can indeed sample the boundary of the domain for instance if h 0 1 and s v i 0 5 the cross section of the x i parameter will be the vector x i 0 1 0 2 s v i 1 this will cause vars to to crash as 1 becomes infinity under a normal distribution even if the star vars algorithm is modified to prevent 1 from being sampled e g by replacing 1 by 0 999 as we did in this study some cross sections will still sample values very close to 1 by design especially if h is set at a small value these values will be extreme values under a normal distribution disrupting again the computation of the model output and the vars to index in this case for the x i parameter we believe that this explains the high order interactions involving n s t a r h k φ τ which are non negligible fig 8 the effect of n s t a r and h in vars to was not explored by puy et al 2020a nor by becker 2020 who documented a slightly higher performance of vars to against jansen and janon monod our results indicate that vars to loses this marginal edge once these internal uncertainties jointly with the uncertainties in the benchmark settings are considered in the computations even if the use of vars to is restricted to non normal distributions φ 2 its performance would still be slightly outdone by jansen 95 ci 0 96 0 99 median of 0 99 for jansen 95 ci 0 94 0 99 median of 0 97 for vars 4 conclusions we have revised the variogram analysis of response surfaces vars a new framework for sensitivity analysis developed by razavi and gupta 2016a 2016b we have specifically focused on two aspects that according to razavi and gupta 2016a 2016b make vars outperform sobol indices its more intuitive appraisal of sensitivities and of the importance of model inputs and its 20 100 times higher computational efficiency the claim that vars is more intuitive than sobol indices can hardly be reversed as it ultimately is a matter of personal taste disciplinary orientation and objective of the modeling activity a geographer working in a diagnostic model setting might indeed prefer vars s approach to the model structure due to its capacity to distinguish response surfaces however the professed higher intuitive nature of vars ties in poorly with the available evidence less than one half of the studies citing vars actually implement it in a case study and almost half of those that use it include the vars authors themselves furthermore most works do not explore the full range of the response surface but rely exclusively on summary metrics such as ivars50 or vars to which are very similar to the sobol total order index we argue that sobol indices provide a clearer description of what an important model input is given its connection to statistical theory and anova the use of sobol first or total order indices is associated with clear research settings and their meaning can be easily conveyed to stakeholders or non specialists which adds to their transparency vars in contrast allows the analyst to zoom into the structure of the model output and assess its dependency on the model inputs through the integrated variograms ivars10 ivars30 and ivars50 as well as through the variance based total order effect vars to but what is their definition of importance how useful is it for a stakeholder to know that a parameter is important under ivars10 and not as much under ivars30 for instance which ivars measure should she finally rely onto before making a policy decision if the answer is the summary measure vars to then it is unclear how vars advances sobol indices given the reliance of vars to on variance and covariance matrices the purported much higher efficiency of vars to is very contentious the observation that it is more than 100 times more efficient than sobol based total order indices rests on an exercise with a fully additive model in which vars to is compared against one of the less accurate total order estimators saltelli et al 2008 see puy et al 2020a and the performance measure chosen is the probability of failure of properly ranking the model inputs if the comparison is instead conducted between vars to and a lightweight sobol based first order estimator equation 18 the advantage of the former shrinks to being only 15 times more efficient vars to completely loses all its edge if the mean absolute error mae replaces the probability of failure as a performance measure in this setting equation 18 is the one showing an accuracy up to 100 times higher than vars to nevertheless the advantage of vars to over sobol based indices is still remarkable if the goal is to rank parameters and suggests that vars to should be the sensitivity measure of choice if computational efficiency is a priority and the model is additive however this condition is unlikely to apply to models of the earth and environmental domain either because they encompass multiplicative terms and exponentials or because their mathematical complexity prevents the analyst from knowing their behavior before running the simulations the assertion that vars to is at least 20 times more efficient than sobol based total order indices is not confirmed by our results vars to only very slightly outperforms one of the most accurate sobol total order estimators that of jansen 1999 when the number of model runs per model input is very small however it comes second to jansen at increasing dimensionalities and in overall performance such results have been obtained after randomising the benchmark settings thus creating a set of sensitivity problems much wider than those represented by the hymod and mesh models and by simultaneously examining the internal uncertainties of vars to n s t a r h its sampling design makes it especially vulnerable to the high order interactions between the sampling method τ the number of stars n s t a r the function dimensionality k the distance between pairs h and the underlying distribution of the model inputs φ especially if they follow a normal distribution vars nonetheless represents a relevant addition to the family of sensitivity analysis methods with the additional merit of having been developed to appraise the response surface of a model furthermore the conceptual framework of vars comes with a software described as next generation by razavi 2019 time will tell whether vars ends up unseating sobol based indices as the recommended best practice in sensitivity analysis code availability fully documented code is freely available in puy 2020 and in github https github com arnaldpuy vars paper data availability a csv file of the studies citing vars as of november 2020 can be found in github https github com arnaldpuy vars paper declaration of competing interest we do not have any conflict of interest acknowledgements this work has been funded by the european commission marie skłodowska curie global fellowship grant number 792178 to a p appendix a supplementary data the following is are the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 104960 
25884,land subsidence provides important information about the spatial and temporal changes occurring in the subsurface e g groundwater levels geology etc however sufficient subsidence data are difficult to obtain using only one sensor or survey often resulting in a tradeoff between spatial resolution and temporal coverage this study aims to estimate the high spatio temporal resolution land subsidence by using a kernel based vector data fusion approach between annual leveling and monthly subsidence monitoring well data while invoking an invariant relation of subsidence information subsidence patterns and processes can be identified when spatio temporal fusion of sensor data are implemented in this subsidence investigation in yunlin and chunghua counties taiwan the root mean square error rmse is 0 52 cm in the fusion stage and the mapping rmse is 0 53 cm in the interpolation the fused subsidence data readily show that the subsidence hotspot varies with time and space the subsidence hotspots are in the western region during the winter related to aquaculture activities but move to the inland areas of yunlin county during the following spring related to agricultural activities the proposed approach can help explain the spatio temporal variability of the subsidence pattern keywords spatio temporal data fusion subsidence spatial regression kernel weight 1 introduction many regions of the earth face land subsidence crises due to over exploitation of groundwater for agriculture and other anthropogenic activities hu et al 2004 galloway and burbey 2011 minderhoud et al 2017 mahmoudpour et al 2016 extraction of large volumes of groundwater increases the effective stress of the aquifer system resulting in compaction and subsequent land subsidence galloway and burbey 2011 minderhoud et al 2017 a spatiotemporal model has been developed an accurate representations of annual land subsidence changes from groundwater exploitation ali et al 2020 however the high spatio temporal resolution of subsidence monitoring data can help us summarize the processes and patterns of land subsidence subsidence monitoring which aims to characterize the spatial distribution of land subsidence generally includes leveling surveys borehole monitoring data gps and insar hung et al 2010 however the sensors and surveys used in these investigations produce measurements with different spatial and temporal resolutions data with high spatial and temporal characteristics are difficult to acquire because of the tradeoff in sensor and survey designs which balances spatial resolution and temporal coverage zhang et al 2013 for instance sensor data can be applied at fine spatial resolutions but can rarely be used to capture the temporal changes due to their low temporal resolutions the available subsidence leveling data still cannot satisfy our requirement of high frequency land subsidence change measurements caused by groundwater extraction from highly heterogeneous aquifer systems leveling data are usually measured at low frequencies due to the high cost of obtaining this information leveling can deliver pointwise vertical displacements that are accurate to few mm compaction measurements are typically measured at monthly intervals compaction monitoring wells can provide depth dependent measurements of compaction at various intervals within a single borehole by noting the depth of individual magnetic rings that are emplaced at various depths throughout the depth of the well hung et al 2010 the measurement of compaction monitoring wells provide monthly subsidence information however the depth of the compaction monitoring well is limited due to the installation cost and geological conditions high temporal resolution data are provided by sensors but cannot reach the requirements of interpretation and visualization due to their low spatial resolution therefore spatiotemporal data fusion represents a feasible solution for the aforementioned problem zhu et al 2018 spatiotemporal data fusion was originally designed for blending reflectance bandwidths from landsat and modis data to produce daily landsat like surface reflectance chen et al 2015 spatiotemporal data fusion is performed to fuse satellite images from two independent sensors namely a sensor with very high frequency but coarse spatial resolution such as modis with very high spatial resolution but low frequency sensor such as landsat zhang et al 2013 chen et al 2015 in this study spatiotemporal data fusion can generate high spatial temporal resolution subsidence combining leveling data and compaction monitoring well data typical spatiotemporal data fusion methods are categorized into five groups by using the specific techniques used to link coarse and fine data sets namely unmixing based weight function based bayesian based learning based and hybrid based methods zhu et al 2018 weight function based methods are the most popular approach the weight function is empirical and provides estimates at a fine pixel scale by combining information from all input images using weight functions the non spatial weighting assumption is invalid for heterogeneous aquifer systems in this study the data fusion method is spatial regression based but also uses kernel weights fotheringham et al 2003 our study assumes that the relation between leveling data and subsidence monitoring wells is invariant with time thus on the basis of the spatial relations between the leveling data and subsidence information the high spatial temporal resolution fused data can be determined in addition few studies have shown the spatio temporal subsidence changes using multiple sensors tangdamrongsub et al 2019 our fused subsidence data are expected to show where and when subsidence hotspots will occur this study aims to provide high spatio temporal resolution land subsidence estimates by using a data fusion approach between annual leveling data and monthly compaction observations we hypothesize that data fusion will maximize the spatio temporal resolution of observations the subsidence information between annual leveling data and compaction observations are constant with time their temporal transformation is a spatial function e g location based linear transformation moreover we will provide interpolated subsidence maps obtained from data fusion the data fusion approach proposed here will provide a reliable representation of the seasonal subsidence in spite of the temporal and spatial heterogeneity associated with subsidence in addition the feasibility of the subsidence hotspot movement detection from the fused data is presented 2 methods 2 1 definition of subsidence data fusion y s t represents the subsidence obtained from leveling data primary variable with location set s and time set t in this investigation two sets of leveling data are used may 2014 and may 2015 x s t is the subsidence from compaction monitoring wells auxiliary variable with location set s and time set t these data x s t are collected monthly s s t t m and n are the maximum number of temporal and spatial observations i e t and s respectively the estimated y ˆ s t can be determined considering spatial temporal data fusion the spatial temporal data fusion model method of quantifying subsidence can be separated into four steps fig 1 steps 1 and 2 represent the main steps of the data fusion focusing on spatial dimension and expansion and step 3 is the interpolation for mapping model validation is the final step the process of each step is outlined as follows step 1 spatial dimension expansion the spatial dimension of auxiliary variable x s t can be expanded as x s t i e x s t x s t by using the nearest neighbor strategy the nearest neighbor strategy assigns the nearest point value to an unknown point without considering any neighboring points to yield a piecewise constant interpolation no parameters are required by this approach here the dimension number includes the following t 1 t 12 s 31 and s 671 step 2 kernel based transfer function development and estimation with time this study assumes that the relation between primary and auxiliary variables leveling data and compaction measurement data are invariant with time the model y t f x t is built in the training stage at time t in a set t after model development y ˆ t can be estimated at time t with the set t t y t represents the subsidence from the leveling data at time t may 2015 x t is the subsidence of the nearest neighboring monitoring well from leveling stations at time t their transfer function is a location based linear transformation the model used is from kernel based spatial regression fotheringham et al 2003 the transfer function at time m in the training stage is expressed as follows 1 y i m β u i v i x i m ε i where x i m is the subsidence of the nearest neighboring monitoring well at leveling station i at the time m y i m is the subsidence of the leveling data at station i at the time m β 1 u i v i is the slope of the spatial regression parameters and varies with the spatial coordinates u i v i at observation i variables u i and v i are the longitude and latitude at observation i respectively estimated model coefficient β ˆ u i v i is derived from the weighted least squares and is defined as follows 2 β ˆ u i v i x t w u i v i x 1 x t w u i v i y where y y 1 m y n m t and x are the corresponding matrix of covariates x x 1 m x n m t the elements in a kernel based weight matrix w i j are the weighted elements between observations i and j the weight element can be computed as a kernel function as follows 3 w i j e x p d i j 2 b 2 where b is a nonnegative parameter known as bandwidth and d i j is the gaussian distance decay based function based on the geographical distance the optimal bandwidth is determined by cross validation for data fusion the auxiliary variable for subsidence from the nearest neighboring monitoring well at leveling station i and time t which is x i t in x s t is known the estimated y ˆ i t following the existing regression and its parameter β u i v i from eq 1 can be expressed as follows 4 y ˆ i t β u i v i x i t ε i step 3 interpolation mapping considering steps 1 and 2 fusion y ˆ s t can be determined y ˆ i t y ˆ s t in this step the interpolated points y ˆ s t can be further generated on the basis of y ˆ s t if s s here s 75 51 2d map the inverse distance weighting idw method is a straightforward and represents a low computational approach for spatial interpolation procedures of the subsidence map y ˆ s t where s is the interpolated point set the idw method is used to generate a 1 km resolution for subsidence estimation the parameter power value is 2 as the inverse distance squared weighted interpolation the weights of the observations in the idw are based on the inverse distances of the unknown point and observations step 4 validation to evaluate the generated and related fusion models the commonly used root mean square error rmse between the estimated and measured subsidence is adopted as follows 5 rmse i 1 v n e s t i o b s i 2 v n where e s t i and o b s i represent the estimated and measured subsidence at observation i v n is the validation number the estimated subsidence is from the fusion result points or mapping grid data based on the idw the model rmses are used in the validation including the fusion and mapping rmses 2 2 study area fig 2 a shows the location map of the study area the choshui river fan encompasses yunlin and chunghua counties the choshui river alluvial fan sediments originate from the rock formations in the upstream watershed to the east and are comprised of mudstone metamorphic quartzite sandstone shale and slate liu et al 2004 land subsidence has become a serious issue in the area in both counties yunlin and chunghua the prominence of fine grained deposits within the alluvial fan is responsible for the significant amount of land subsidence in the region creating a substantial geologic hazard particularly for the high speed rail system that extends north south through the study area measured from the land surface downward aquifers i ii 1 ii 2 iii and iv are separated by aquitards t1 t2 and t3 aquitards are most prevalent in the distal fan and mid fan areas and gradually diminish in thickness toward the east the proximal fan represents the major recharge area of the aquifer system jang et al 2008 yu and chu 2010 the distribution of sediments in the alluvial fan transitions from sediments dominated by gravel to sand and then to clay moving from the proximal fan in the east to the distal fan in the west moreover aquifer ii is the primary aquifer of the choshui alluvial plain for agriculture and domestic use because of its large spatial extent and acceptable depth for groundwater extraction 3 data 3 1 leveling data annual subsidence rates from 671 leveling points locations in fig 2b are used in this investigation and are shown in fig 2d a leveling network with over 1000 km in length is used to calculate subsidence for every 1 5 km interval along the leveling routes leveling specifications satisfy a loop closure of less than 3 k mm where k is the length of the leveling circuit in kilometers the vertical accuracy of the leveling data is generally within 1 cm hung et al 2010 leveling has a high degree of accuracy but is time consuming and expensive compared with gps generally the leveling data are measured each may the leveling data from may of 2014 and 2015 are used in this case study 3 2 data from subsidence monitoring wells the subsidence monitoring wells are constructed with magnetic rings that are emplaced at specific depth intervals within the sedimentary layers enc a device is lowered into the monitoring well that measures the precise location of the magnetic rings fig 2c these measurements are made on a monthly basis to obtain the relative compaction of the sedimentary layers over time which commenced in 2012 the number of compaction monitoring wells is relatively low at 31 because the cost of installing these subsidence measuring compaction at different depth intervals monitoring wells is high however this information is critical in exploiting the relationship between the groundwater extraction and subsidence mechanisms hsu et al 2015 the advantage of the monitoring wells is stability and high accuracy approximately 1 5 mm however the disadvantage is that the device is costly and incapable of determining the subsidence below the depth of the well maximum of 300 m hung et al 2010 in this study the subsidence monitoring well information only measures compaction to a depth of 300 m or less the analyzed data for this investigation are from june 2014 to may 2015 1 year fig 3 shows that spatial plots of the monthly subsidence from the monitoring wells at the third sixth ninth 10th 11th and 12th month i e august and november 2014 and february march april and may 2015 4 results 4 1 monthly subsidence from monitoring wells subsidence between leveling points and subsidence monitoring wells are found to be highly correlated the correlation coefficient for the annual subsidence between leveling points and subsidence monitoring wells with collocated locations is approximately 0 80 fig 4 shows the monthly subsidence from the 31 subsidence monitoring wells the results imply that the subsidence varies with season the average subsidence is less than 1 5 cm at the first eight months but increases dramatically from february to may 2015 table 1 as a result of increased groundwater usage for agricultural activity chu et al 2020 the standard deviation of subsidence from the monitoring wells at the 10th 11th and 12th months february to may is larger than 1 cm table 1 the high standard deviation indicates that the subsidence is spatially variant and subsidence hotspots exist especially during these months fig 5 shows the monthly variations of subsidence at kecuo tuku yuanchang and xinxing the temporal trends of monthly subsidence are similar at kecuo tuku and yuanchang but differ at xinxing the subsidence process at kecuo tuku and yuanchang the peaks are from february to may is related to agricultural activity but that from xinxing the peaks are during summer e g may and june and winter e g november and december is related to aquaculture activities in aquaculture activities groundwater is used to adjust the temperature of the fish ponds for example groundwater is pumped into the pond for water cooling during the summer months 4 2 fused subsidence fig 6 shows the high spatio temporal resolution fused monthly subsidence for the third sixth ninth 10th 11th and 12th months the subsidence data points can be refined in spatial and temporal resolutions using the first two steps of the proposed method the resulting r2 from the regression function eq 1 is 0 94 the fusion rmse is approximately 0 52 cm for leveling observations in may 2015 results show the vector based fusion but most algorithms focus on image based fusion zhu et al 2018 small amounts of subsidence occur during the third month august 2014 as observed from the subsidence data points in november 2014 the sixth month a subsidence hotspot occurred at the western coastal area in yunlin county the hotspot coincides with many fishing ponds during the winter fish farmers use groundwater to fill and maintain the water temperature in the ponds because the groundwater discharge is typically warm moreover the fused subsidence data show the evident subsidence bowl close to inland area of yunlin during february and may the period from february to may is the first rice cropping of the season in taiwan from february to may large amounts of subsidence occurs in the inland area because farmers use groundwater for irrigation the model also helps in understanding the influence of human activity on land and groundwater usage 4 3 subsidence maps fig 7 shows the estimated subsidence in august and november 2014 and february march april and may 2015 the subsidence hotspots can be identified easily from the fusion interpolation step 3 the mapping rmse in 2015 may is 0 53 cm when compared with the leveling data mapping performance when compared with the hotspot movement is acceptable the fusion and mapping residuals in the study area are shown in fig 8 the fusion and mapping subsidence residuals in the year measured on may 2015 are randomly distributed with ranges between 3 3 and 3 2 and 2 8 2 2 cm the maps fig 7 clearly show that the subsidence hotspot changes over time and the heterogeneous nature of the subsidence bowl is evident within the study area the results show that the hotspots of the monthly subsidence are not always in the inland areas of yunlin county the variability of subsidence is attributed to the various groundwater usages for different human activities the subsidence hotspot movement occurs seasonally from year by year and is related to climate and human activities currently groundwater is not extensively used by aquaculture ponds in the coastal area a long term subsidence hotspot migrates on the choushui alluvial fan from the coast to the inland area of the study because the groundwater usage and human activities have changed over the past decades hung et al 2012 5 discussion subsidence data provided by only one sensor are greatly limited in usefulness due to the tradeoff in sensor designs that balance spatial resolutions and temporal coverage chen et al 2015 as a result integrated use of data from multiple sensors has become important for understanding subsidence trends from human activities and precipitation patterns fused data with high spatial resolution is critical for monitoring subsidence dynamics in heterogeneous aquifer systems climate and human activities subsidence due to natural and anthropogenic causes requires integrated and fused monitoring data tosi et al 2013 the fusion results are not only time variant but also generate large scale spatial patterns of subsidence because the spatial patterns obtained from sparse monitoring stations is time variant the spatial temporal distribution from fused data points is more complete and reasonable results match the observed trends of fig 3 moreover the spatial distribution of fused subsidence varies in terms of details even at the local scale considering spatio temporal fusion of sensor data the high spatial temporal resolution of subsidence can be estimated within the study area the patterns show that the subsidence hotspot varies with time the hotspots of subsidence migrates over time first occurring in the western area close to the region where aquaculture ponds are used during winter months then migrating to the inland areas of yunlin county during the spring february to may the greatest quantities of subsidence occur during months with low rainfall and high pumping rates and not during the wet season from june to september generally large subsidence occurs from january to may especially february and march chu et al 2020 the subsidence pattern varies with time and space but the leveling data alone cannot explain the monthly variations of the subsidence annual variations of subsidence are poorly understood thereby inhibiting our ability to fully explain the groundwater exploitation induced land subsidence tsai and hsu 2018 accordingly a highly reliable method is proposed to generate subsidence data with high spatial and temporal resolution the subsidence hotspots accumulate from local hotspots and become the regional hotspots from march to may a highly reliable method incorporating leveling and compaction monitoring data is proposed the model that considers spatio temporal heterogeneity can sufficiently explain the spatial and temporal variation of subsidence land subsidence is mainly affected by groundwater drawdown but its progression is characterized by the properties and subsequent response of the fine grained sediments that make up the interbeds and confining layers within the aquifer system miller et al 2017 the groundwater pumping history intensity duration and frequency also affect the distribution of land subsidence eventually understanding the pattern variations in subsidence and its driving factors is important for mitigating its adverse consequences the driving factors such as aquaculture and agricultural activities based on locality and seasonality can be identified using the spatio temporal subsidence method proposed here further study using the data fusion approach can be applied to other sites that integrate multiple subsidence monitoring data for example gps data can be applied for increasing temporal resolution however raw gps data may contain vertical deformation noise that would need to be removed 6 conclusions subsidence data fusion which integrates both leveling data and monthly subsidence information from compaction monitoring wells is used to provide better understanding of subsidence patterns influenced by human activities and natural factors leveling data contain high spatial resolution but low temporal resolution and compaction monitoring well data have high temporal resolution but low spatial resolution however together these two data sets provide a powerful way to analyze subsidence the study accounts for the high spatio temporal resolution of land subsidence caused by compaction in an aquifer system this study assumes that the relation between leveling data and compaction monitoring well data is invariant with time the proposed model considers spatial kernel regression for data fusion and idw interpolation for visualization this final work yields high spatio temporal resolution subsidence maps the spatio temporal fusion method is used to enhance the spatial resolution and temporal frequency simultaneously most previous algorithms have focused on image based data fusion but the proposed method represents a vector data based fusion the leveling and compaction data are fused in the points with high temporal and spatial resolution this study has exploited the spatio temporal process of subsidence using a real world case study of observed subsidence data after interpolation of the fused data fused subsidence maps are developed and reveal where serious hotspots of subsidence movement occurs in the study area results show that the proposed data fusion approach can help explain the spatio temporal variability of the subsidence patterns which are spatially and temporally heterogeneous this spatio temporal fusion approach exhibits high potential for providing a better understanding of complex subsidence processes in addition various interpolation approaches will be applied and tested for the spatial dimension expansion in the future declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we acknowledge the financial support from the ministry of science and technology most and the data support from water resources agency in taiwan this research was funded by most grant number 105 2621 m 006 011 we also thank dr bo huang for scientific advisors the authors would like to thank the editors and anonymous reviewers for providing suggestions of paper improvement 
25884,land subsidence provides important information about the spatial and temporal changes occurring in the subsurface e g groundwater levels geology etc however sufficient subsidence data are difficult to obtain using only one sensor or survey often resulting in a tradeoff between spatial resolution and temporal coverage this study aims to estimate the high spatio temporal resolution land subsidence by using a kernel based vector data fusion approach between annual leveling and monthly subsidence monitoring well data while invoking an invariant relation of subsidence information subsidence patterns and processes can be identified when spatio temporal fusion of sensor data are implemented in this subsidence investigation in yunlin and chunghua counties taiwan the root mean square error rmse is 0 52 cm in the fusion stage and the mapping rmse is 0 53 cm in the interpolation the fused subsidence data readily show that the subsidence hotspot varies with time and space the subsidence hotspots are in the western region during the winter related to aquaculture activities but move to the inland areas of yunlin county during the following spring related to agricultural activities the proposed approach can help explain the spatio temporal variability of the subsidence pattern keywords spatio temporal data fusion subsidence spatial regression kernel weight 1 introduction many regions of the earth face land subsidence crises due to over exploitation of groundwater for agriculture and other anthropogenic activities hu et al 2004 galloway and burbey 2011 minderhoud et al 2017 mahmoudpour et al 2016 extraction of large volumes of groundwater increases the effective stress of the aquifer system resulting in compaction and subsequent land subsidence galloway and burbey 2011 minderhoud et al 2017 a spatiotemporal model has been developed an accurate representations of annual land subsidence changes from groundwater exploitation ali et al 2020 however the high spatio temporal resolution of subsidence monitoring data can help us summarize the processes and patterns of land subsidence subsidence monitoring which aims to characterize the spatial distribution of land subsidence generally includes leveling surveys borehole monitoring data gps and insar hung et al 2010 however the sensors and surveys used in these investigations produce measurements with different spatial and temporal resolutions data with high spatial and temporal characteristics are difficult to acquire because of the tradeoff in sensor and survey designs which balances spatial resolution and temporal coverage zhang et al 2013 for instance sensor data can be applied at fine spatial resolutions but can rarely be used to capture the temporal changes due to their low temporal resolutions the available subsidence leveling data still cannot satisfy our requirement of high frequency land subsidence change measurements caused by groundwater extraction from highly heterogeneous aquifer systems leveling data are usually measured at low frequencies due to the high cost of obtaining this information leveling can deliver pointwise vertical displacements that are accurate to few mm compaction measurements are typically measured at monthly intervals compaction monitoring wells can provide depth dependent measurements of compaction at various intervals within a single borehole by noting the depth of individual magnetic rings that are emplaced at various depths throughout the depth of the well hung et al 2010 the measurement of compaction monitoring wells provide monthly subsidence information however the depth of the compaction monitoring well is limited due to the installation cost and geological conditions high temporal resolution data are provided by sensors but cannot reach the requirements of interpretation and visualization due to their low spatial resolution therefore spatiotemporal data fusion represents a feasible solution for the aforementioned problem zhu et al 2018 spatiotemporal data fusion was originally designed for blending reflectance bandwidths from landsat and modis data to produce daily landsat like surface reflectance chen et al 2015 spatiotemporal data fusion is performed to fuse satellite images from two independent sensors namely a sensor with very high frequency but coarse spatial resolution such as modis with very high spatial resolution but low frequency sensor such as landsat zhang et al 2013 chen et al 2015 in this study spatiotemporal data fusion can generate high spatial temporal resolution subsidence combining leveling data and compaction monitoring well data typical spatiotemporal data fusion methods are categorized into five groups by using the specific techniques used to link coarse and fine data sets namely unmixing based weight function based bayesian based learning based and hybrid based methods zhu et al 2018 weight function based methods are the most popular approach the weight function is empirical and provides estimates at a fine pixel scale by combining information from all input images using weight functions the non spatial weighting assumption is invalid for heterogeneous aquifer systems in this study the data fusion method is spatial regression based but also uses kernel weights fotheringham et al 2003 our study assumes that the relation between leveling data and subsidence monitoring wells is invariant with time thus on the basis of the spatial relations between the leveling data and subsidence information the high spatial temporal resolution fused data can be determined in addition few studies have shown the spatio temporal subsidence changes using multiple sensors tangdamrongsub et al 2019 our fused subsidence data are expected to show where and when subsidence hotspots will occur this study aims to provide high spatio temporal resolution land subsidence estimates by using a data fusion approach between annual leveling data and monthly compaction observations we hypothesize that data fusion will maximize the spatio temporal resolution of observations the subsidence information between annual leveling data and compaction observations are constant with time their temporal transformation is a spatial function e g location based linear transformation moreover we will provide interpolated subsidence maps obtained from data fusion the data fusion approach proposed here will provide a reliable representation of the seasonal subsidence in spite of the temporal and spatial heterogeneity associated with subsidence in addition the feasibility of the subsidence hotspot movement detection from the fused data is presented 2 methods 2 1 definition of subsidence data fusion y s t represents the subsidence obtained from leveling data primary variable with location set s and time set t in this investigation two sets of leveling data are used may 2014 and may 2015 x s t is the subsidence from compaction monitoring wells auxiliary variable with location set s and time set t these data x s t are collected monthly s s t t m and n are the maximum number of temporal and spatial observations i e t and s respectively the estimated y ˆ s t can be determined considering spatial temporal data fusion the spatial temporal data fusion model method of quantifying subsidence can be separated into four steps fig 1 steps 1 and 2 represent the main steps of the data fusion focusing on spatial dimension and expansion and step 3 is the interpolation for mapping model validation is the final step the process of each step is outlined as follows step 1 spatial dimension expansion the spatial dimension of auxiliary variable x s t can be expanded as x s t i e x s t x s t by using the nearest neighbor strategy the nearest neighbor strategy assigns the nearest point value to an unknown point without considering any neighboring points to yield a piecewise constant interpolation no parameters are required by this approach here the dimension number includes the following t 1 t 12 s 31 and s 671 step 2 kernel based transfer function development and estimation with time this study assumes that the relation between primary and auxiliary variables leveling data and compaction measurement data are invariant with time the model y t f x t is built in the training stage at time t in a set t after model development y ˆ t can be estimated at time t with the set t t y t represents the subsidence from the leveling data at time t may 2015 x t is the subsidence of the nearest neighboring monitoring well from leveling stations at time t their transfer function is a location based linear transformation the model used is from kernel based spatial regression fotheringham et al 2003 the transfer function at time m in the training stage is expressed as follows 1 y i m β u i v i x i m ε i where x i m is the subsidence of the nearest neighboring monitoring well at leveling station i at the time m y i m is the subsidence of the leveling data at station i at the time m β 1 u i v i is the slope of the spatial regression parameters and varies with the spatial coordinates u i v i at observation i variables u i and v i are the longitude and latitude at observation i respectively estimated model coefficient β ˆ u i v i is derived from the weighted least squares and is defined as follows 2 β ˆ u i v i x t w u i v i x 1 x t w u i v i y where y y 1 m y n m t and x are the corresponding matrix of covariates x x 1 m x n m t the elements in a kernel based weight matrix w i j are the weighted elements between observations i and j the weight element can be computed as a kernel function as follows 3 w i j e x p d i j 2 b 2 where b is a nonnegative parameter known as bandwidth and d i j is the gaussian distance decay based function based on the geographical distance the optimal bandwidth is determined by cross validation for data fusion the auxiliary variable for subsidence from the nearest neighboring monitoring well at leveling station i and time t which is x i t in x s t is known the estimated y ˆ i t following the existing regression and its parameter β u i v i from eq 1 can be expressed as follows 4 y ˆ i t β u i v i x i t ε i step 3 interpolation mapping considering steps 1 and 2 fusion y ˆ s t can be determined y ˆ i t y ˆ s t in this step the interpolated points y ˆ s t can be further generated on the basis of y ˆ s t if s s here s 75 51 2d map the inverse distance weighting idw method is a straightforward and represents a low computational approach for spatial interpolation procedures of the subsidence map y ˆ s t where s is the interpolated point set the idw method is used to generate a 1 km resolution for subsidence estimation the parameter power value is 2 as the inverse distance squared weighted interpolation the weights of the observations in the idw are based on the inverse distances of the unknown point and observations step 4 validation to evaluate the generated and related fusion models the commonly used root mean square error rmse between the estimated and measured subsidence is adopted as follows 5 rmse i 1 v n e s t i o b s i 2 v n where e s t i and o b s i represent the estimated and measured subsidence at observation i v n is the validation number the estimated subsidence is from the fusion result points or mapping grid data based on the idw the model rmses are used in the validation including the fusion and mapping rmses 2 2 study area fig 2 a shows the location map of the study area the choshui river fan encompasses yunlin and chunghua counties the choshui river alluvial fan sediments originate from the rock formations in the upstream watershed to the east and are comprised of mudstone metamorphic quartzite sandstone shale and slate liu et al 2004 land subsidence has become a serious issue in the area in both counties yunlin and chunghua the prominence of fine grained deposits within the alluvial fan is responsible for the significant amount of land subsidence in the region creating a substantial geologic hazard particularly for the high speed rail system that extends north south through the study area measured from the land surface downward aquifers i ii 1 ii 2 iii and iv are separated by aquitards t1 t2 and t3 aquitards are most prevalent in the distal fan and mid fan areas and gradually diminish in thickness toward the east the proximal fan represents the major recharge area of the aquifer system jang et al 2008 yu and chu 2010 the distribution of sediments in the alluvial fan transitions from sediments dominated by gravel to sand and then to clay moving from the proximal fan in the east to the distal fan in the west moreover aquifer ii is the primary aquifer of the choshui alluvial plain for agriculture and domestic use because of its large spatial extent and acceptable depth for groundwater extraction 3 data 3 1 leveling data annual subsidence rates from 671 leveling points locations in fig 2b are used in this investigation and are shown in fig 2d a leveling network with over 1000 km in length is used to calculate subsidence for every 1 5 km interval along the leveling routes leveling specifications satisfy a loop closure of less than 3 k mm where k is the length of the leveling circuit in kilometers the vertical accuracy of the leveling data is generally within 1 cm hung et al 2010 leveling has a high degree of accuracy but is time consuming and expensive compared with gps generally the leveling data are measured each may the leveling data from may of 2014 and 2015 are used in this case study 3 2 data from subsidence monitoring wells the subsidence monitoring wells are constructed with magnetic rings that are emplaced at specific depth intervals within the sedimentary layers enc a device is lowered into the monitoring well that measures the precise location of the magnetic rings fig 2c these measurements are made on a monthly basis to obtain the relative compaction of the sedimentary layers over time which commenced in 2012 the number of compaction monitoring wells is relatively low at 31 because the cost of installing these subsidence measuring compaction at different depth intervals monitoring wells is high however this information is critical in exploiting the relationship between the groundwater extraction and subsidence mechanisms hsu et al 2015 the advantage of the monitoring wells is stability and high accuracy approximately 1 5 mm however the disadvantage is that the device is costly and incapable of determining the subsidence below the depth of the well maximum of 300 m hung et al 2010 in this study the subsidence monitoring well information only measures compaction to a depth of 300 m or less the analyzed data for this investigation are from june 2014 to may 2015 1 year fig 3 shows that spatial plots of the monthly subsidence from the monitoring wells at the third sixth ninth 10th 11th and 12th month i e august and november 2014 and february march april and may 2015 4 results 4 1 monthly subsidence from monitoring wells subsidence between leveling points and subsidence monitoring wells are found to be highly correlated the correlation coefficient for the annual subsidence between leveling points and subsidence monitoring wells with collocated locations is approximately 0 80 fig 4 shows the monthly subsidence from the 31 subsidence monitoring wells the results imply that the subsidence varies with season the average subsidence is less than 1 5 cm at the first eight months but increases dramatically from february to may 2015 table 1 as a result of increased groundwater usage for agricultural activity chu et al 2020 the standard deviation of subsidence from the monitoring wells at the 10th 11th and 12th months february to may is larger than 1 cm table 1 the high standard deviation indicates that the subsidence is spatially variant and subsidence hotspots exist especially during these months fig 5 shows the monthly variations of subsidence at kecuo tuku yuanchang and xinxing the temporal trends of monthly subsidence are similar at kecuo tuku and yuanchang but differ at xinxing the subsidence process at kecuo tuku and yuanchang the peaks are from february to may is related to agricultural activity but that from xinxing the peaks are during summer e g may and june and winter e g november and december is related to aquaculture activities in aquaculture activities groundwater is used to adjust the temperature of the fish ponds for example groundwater is pumped into the pond for water cooling during the summer months 4 2 fused subsidence fig 6 shows the high spatio temporal resolution fused monthly subsidence for the third sixth ninth 10th 11th and 12th months the subsidence data points can be refined in spatial and temporal resolutions using the first two steps of the proposed method the resulting r2 from the regression function eq 1 is 0 94 the fusion rmse is approximately 0 52 cm for leveling observations in may 2015 results show the vector based fusion but most algorithms focus on image based fusion zhu et al 2018 small amounts of subsidence occur during the third month august 2014 as observed from the subsidence data points in november 2014 the sixth month a subsidence hotspot occurred at the western coastal area in yunlin county the hotspot coincides with many fishing ponds during the winter fish farmers use groundwater to fill and maintain the water temperature in the ponds because the groundwater discharge is typically warm moreover the fused subsidence data show the evident subsidence bowl close to inland area of yunlin during february and may the period from february to may is the first rice cropping of the season in taiwan from february to may large amounts of subsidence occurs in the inland area because farmers use groundwater for irrigation the model also helps in understanding the influence of human activity on land and groundwater usage 4 3 subsidence maps fig 7 shows the estimated subsidence in august and november 2014 and february march april and may 2015 the subsidence hotspots can be identified easily from the fusion interpolation step 3 the mapping rmse in 2015 may is 0 53 cm when compared with the leveling data mapping performance when compared with the hotspot movement is acceptable the fusion and mapping residuals in the study area are shown in fig 8 the fusion and mapping subsidence residuals in the year measured on may 2015 are randomly distributed with ranges between 3 3 and 3 2 and 2 8 2 2 cm the maps fig 7 clearly show that the subsidence hotspot changes over time and the heterogeneous nature of the subsidence bowl is evident within the study area the results show that the hotspots of the monthly subsidence are not always in the inland areas of yunlin county the variability of subsidence is attributed to the various groundwater usages for different human activities the subsidence hotspot movement occurs seasonally from year by year and is related to climate and human activities currently groundwater is not extensively used by aquaculture ponds in the coastal area a long term subsidence hotspot migrates on the choushui alluvial fan from the coast to the inland area of the study because the groundwater usage and human activities have changed over the past decades hung et al 2012 5 discussion subsidence data provided by only one sensor are greatly limited in usefulness due to the tradeoff in sensor designs that balance spatial resolutions and temporal coverage chen et al 2015 as a result integrated use of data from multiple sensors has become important for understanding subsidence trends from human activities and precipitation patterns fused data with high spatial resolution is critical for monitoring subsidence dynamics in heterogeneous aquifer systems climate and human activities subsidence due to natural and anthropogenic causes requires integrated and fused monitoring data tosi et al 2013 the fusion results are not only time variant but also generate large scale spatial patterns of subsidence because the spatial patterns obtained from sparse monitoring stations is time variant the spatial temporal distribution from fused data points is more complete and reasonable results match the observed trends of fig 3 moreover the spatial distribution of fused subsidence varies in terms of details even at the local scale considering spatio temporal fusion of sensor data the high spatial temporal resolution of subsidence can be estimated within the study area the patterns show that the subsidence hotspot varies with time the hotspots of subsidence migrates over time first occurring in the western area close to the region where aquaculture ponds are used during winter months then migrating to the inland areas of yunlin county during the spring february to may the greatest quantities of subsidence occur during months with low rainfall and high pumping rates and not during the wet season from june to september generally large subsidence occurs from january to may especially february and march chu et al 2020 the subsidence pattern varies with time and space but the leveling data alone cannot explain the monthly variations of the subsidence annual variations of subsidence are poorly understood thereby inhibiting our ability to fully explain the groundwater exploitation induced land subsidence tsai and hsu 2018 accordingly a highly reliable method is proposed to generate subsidence data with high spatial and temporal resolution the subsidence hotspots accumulate from local hotspots and become the regional hotspots from march to may a highly reliable method incorporating leveling and compaction monitoring data is proposed the model that considers spatio temporal heterogeneity can sufficiently explain the spatial and temporal variation of subsidence land subsidence is mainly affected by groundwater drawdown but its progression is characterized by the properties and subsequent response of the fine grained sediments that make up the interbeds and confining layers within the aquifer system miller et al 2017 the groundwater pumping history intensity duration and frequency also affect the distribution of land subsidence eventually understanding the pattern variations in subsidence and its driving factors is important for mitigating its adverse consequences the driving factors such as aquaculture and agricultural activities based on locality and seasonality can be identified using the spatio temporal subsidence method proposed here further study using the data fusion approach can be applied to other sites that integrate multiple subsidence monitoring data for example gps data can be applied for increasing temporal resolution however raw gps data may contain vertical deformation noise that would need to be removed 6 conclusions subsidence data fusion which integrates both leveling data and monthly subsidence information from compaction monitoring wells is used to provide better understanding of subsidence patterns influenced by human activities and natural factors leveling data contain high spatial resolution but low temporal resolution and compaction monitoring well data have high temporal resolution but low spatial resolution however together these two data sets provide a powerful way to analyze subsidence the study accounts for the high spatio temporal resolution of land subsidence caused by compaction in an aquifer system this study assumes that the relation between leveling data and compaction monitoring well data is invariant with time the proposed model considers spatial kernel regression for data fusion and idw interpolation for visualization this final work yields high spatio temporal resolution subsidence maps the spatio temporal fusion method is used to enhance the spatial resolution and temporal frequency simultaneously most previous algorithms have focused on image based data fusion but the proposed method represents a vector data based fusion the leveling and compaction data are fused in the points with high temporal and spatial resolution this study has exploited the spatio temporal process of subsidence using a real world case study of observed subsidence data after interpolation of the fused data fused subsidence maps are developed and reveal where serious hotspots of subsidence movement occurs in the study area results show that the proposed data fusion approach can help explain the spatio temporal variability of the subsidence patterns which are spatially and temporally heterogeneous this spatio temporal fusion approach exhibits high potential for providing a better understanding of complex subsidence processes in addition various interpolation approaches will be applied and tested for the spatial dimension expansion in the future declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments we acknowledge the financial support from the ministry of science and technology most and the data support from water resources agency in taiwan this research was funded by most grant number 105 2621 m 006 011 we also thank dr bo huang for scientific advisors the authors would like to thank the editors and anonymous reviewers for providing suggestions of paper improvement 
