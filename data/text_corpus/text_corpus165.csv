index,text
825,we perform spontaneous imbibition experiments on three carbonates estaillades ketton and portland which are three quarry limestones that have very different pore structures and span wide range of permeability we measure the mass of water imbibed in air saturated cores as a function of time under strongly water wet conditions specifically we perform co current spontaneous experiments using a highly sensitive balance to measure the mass imbibed as a function of time for the three rocks we use cores measuring 37 mm in diameter and three lengths of approximately 76 mm 204 mm and 290 mm we show that the amount imbibed scales as the square root of time and find the parameter c where the volume imbibed per unit cross sectional area at time t is ct 1 2 we find higher c values for higher permeability rocks employing semi analytical solutions for one dimensional flow and using reasonable estimates of relative permeability and capillary pressure we can match the experimental data we finally discuss how in combination with conventional measurements we can use theoretical solutions and imbibition measurements to find or constrain relative permeability and capillary pressure keywords spontaneous imbibition capillary dominated flow relative permeability capillary pressure semi analytical solution 1 introduction spontaneous water imbibition is the invasion of the water into a porous medium due to capillary forces and can only occur in water wet and mixed wet systems morrow and mason 2001 spontaneous imbibition si has two modes co current and counter current see fig 1 counter current imbibition occurs when the oil and brine flow in opposite directions from the same inlet whereas co current is when the brine and oil flow in the same direction the rate of water imbibition into the porous medium is a function of permeability relative permeability capillary pressure initial water saturation boundary conditions viscosity interfacial tension and wettability graue and fernø 2011 mason and morrow 2013 zhang et al 1996 spontaneous imbibition is an important recovery mechanism in naturally and artificially induced fractured reservoirs morrow and mason 2001 in addition imbibition is the process rendering carbon dioxide immobile in the process of carbon capture and storage ccs alyafei and blunt 2016 recently the use of imbibition experiments to estimate multi phase flow parameters such as relative permeability and capillary pressure has been proposed alyafei et al 2016 haugen et al 2014 li and horne 2005 several studies proposed analytical solutions for spontaneous imbibition cil 1996 kashchiev and firoozabadi 2003 however these solutions made additional assumptions that were not physically valid in contrast the solution derived by schmid et al 2016 2011 based on the work of mcwhorter and sunada 1990 1992 is a general solution applicable for any combination of relative permeability and capillary pressure a detailed mathematical description of this solution is provided elsewhere alyafei et al 2016 schmid et al 2016 for co current flow the conservation equations can be expressed as 1 f f f ϕ 2 c 2 d and for counter current flow 2 f f ϕ 2 c 2 d where 3 d s w k λ w λ n w λ t p c s w is the non linear dispersion coefficient m2 s f is the capillary dominated fractional flow f is the second derivative of the capillary dominated fractional flow ϕ is porosity c is a constant that quantifies the rock s ability to imbibe m s f is the buckley leverett fractional flow λw is the wetting phase mobility 1 pa s λnw is the non wetting phase mobility 1 pa s λt is the total mobility 1 pa s and pc sw is the derivative of the capillary pressure with respect to saturation pa the volume of water imbibed per unit area per unit time qw t is defined as 4 q w t 2 c t where c is the imbibition constant m s and t is time s schmid et al 2016 2011 presented the formal solution to the co current flow as 5 f ϕ 2 c 2 d f f d 2 s w and similarly with f 0 for counter current flow this equation is implicit in f and so can only be solved iteratively open source spreadsheets have been provided to analyze the semi analytical solution for co and counter current spontaneous imbibition and can be found here alyafei et al 2016 schmid et al 2016 the resultant solution is a function of both imbibition relative permeability and capillary pressure which is different than the buckley leverett solution for flow with an imposed pressure difference which is a function of relative permeability only note the solution assumes that the amount imbibed scales as the square root of time in addition it is only valid at early time where the flow is entirely governed by capillary forces in the absence of constraining boundaries li and horne 2001 olafuyi et al 2007 suzanne et al 2003 the late time is governed by boundary diffusion where the water front reaches the boundary and the recovery rate decays exponentially in this paper we will address the following examine the validity of using t as a scaling parameter and observe the imbibition behavior on different rocks with varying lengths provide a procedure to obtain the c constant from mass imbibition data and discuss its application develop an understanding of spontaneous imbibition of uniformly water wet media experimentally and compare it with the semi analytical solution discuss how to use spontaneous imbibition experiments in combination with other more traditional measurements to determine imbibition capillary pressure and relative permeability in addition this paper gives a systematic procedure to extract imbibition relative permeability and capillary pressure from simple mass imbibition experiments 2 experimental procedure 2 1 rocks we use three carbonate rocks in our study estaillades is a bioclastic limestone which contains 99 calcite caco3 and traces of dolomite and silica and comes from france wright et al 1995 the measured porosity of estaillades ranges from 27 5 28 2 with permeability 1 19 3 94 10 13 m2 ketton is an oolitic limestone of 99 1 calcite and 0 9 quartz and comes from the uk ashton 1980 the porosity ranges from 20 5 23 4 and the permeability from 1 37 2 54 10 12 m2 portland is a skeletal peloidal limestone of 96 6 calcite and 3 4 quartz and comes from the uk brenchley and rawson 2006 the porosity ranges from 16 1 20 0 and the permeability from 0 65 3 5 10 14 m2 in our study we use cores measuring 37 mm in diameter and three lengths of approximately 76 mm 204 mm and 290 mm two samples of each length were studied to assess experimental reproducibility table 1 shows further geological information about the rocks used fig 2 shows the measured mercury injection capillary pressure micp of samples of the three rocks using an autopore iv 9520 measured at weatherford laboratories in east grinstead uk we can see that portland has the highest capillary entry pressure minimum value of p 2σcos θ of approximately 0 18 µm 1 while estaillades has a lower capillary entry pressure of approximately 0 029 µm 1 ketton has the lowest capillary entry pressure of approximately 0 017 µm 1 2 2 fluids and conditions we conduct our experiments at ambient conditions of atmospheric pressure and room temperature of 20 1 c we use air as the non wetting phase and brine with 5 wt sodium chloride nacl and 1 wt potassium chloride kcl mixed with deionised water as the wetting liquid phase in addition we equilibrate the brine with the carbonate samples for 48 h by mixing them using magnetic stirrer to eliminate any reaction between the brine and the rock surface which might alter the rock morphology then we leave the brine for additional 48 hours to settle and finally we filter it using a fine filter paper to remove the particles that might block the flow pathways of the rocks the density of brine is 1 040 8 kg m3 measured using anton paar dma 5000 m and the viscosity is reported as 1 0085 mpa s lide 2004 the air brine interfacial tension is 0 073 n m measured using ramé hart model 590 device and the air viscosity is reported as 0 0018 mpa s tavassoli et al 2005 2 3 mass imbibition before we start the si experiment we perform our routine analysis by taking the dimensions of the core measuring the dry weight of the core and measuring the helium porosity however we use the mass balance technique to measure the porosity for the 204 mm and 209 mm length cores where we take the dry weight and compare it to the fully saturated core with degassed brine after the experiments since the helium porosimeter cell is too small to fit them we measure the permeability of the core using either gas before starting the spontaneous imbibition experiment or brine after finishing the spontaneous imbibition experiment and measuring the fluid saturation where we use a hassler type cell with a cylindrical confining fluid we use three cell lengths to fit our different core lengths to start our si experiment we apply heat shrink wrapping to confine the outer boundaries of the core and make sure that only co current imbibition is applied then we weigh the core again with the heat shrink and after that we attach the core to a mettler toledo xp5003s balance with 0 001 g accuracy and we lift the brine reservoir at the bottom of the core surface to be in contact with the core fig 3 before that moment we start recording the weight changes over time as the balance is connected to the computer we have three recording settings we record 10 points per second 5 points per second and 2 points per second we use the 10 points per second for ketton as it has the highest permeability and the imbibition process is the quickest 5 points per second for estaillades and 2 points per second for portland after imbibition has finished we weigh the core again and by that we can measure the residual gas saturation sgr using material balance we assume that since we run the experiments at ambient conditions there is no compression of the gas after it is trapped we then vacuum saturate the cores for 24 h to make sure that there is no air in the system then we insert the core into the hassler cell to measure the permeability where we keep injecting degassed brine until we reach a steady state flow regime finally we take the core out and weigh it to measure the porosity using the mass balance technique for consistency we replicate each experiment with a core from the same block of the same size table 2 summarizes the properties of the cores used in the experiments 3 results and discussion fig 4 shows the mass imbibed as a function of square root of time of the various lengths of each rock we can see that the early imbibition shows a rapid increase of brine flow rate which then decays at a later time to find the value of c which is the parameter that quantifies the rock s ability to imbibe we plot the mass imbibed as a function of t instead of t we divide the mass imbibed by the brine density and by the area of the core open to water flow to obtain a volume per unit area then by taking the slope of the curves we obtain the parameter 2c m s from eq 4 fig 5 we see a sudden rise in the mass imbibed at the beginning of the experiment this affects high permeability rocks more which is caused by a meniscus jump when the core is first put in contact with the brine lababjos broncano et al 2001 washburn 1921 other possible origins of this affect include an imbibition incubation time or non equilibrium effects see barenblatt et al 1990 2003 for a fuller discussion we take the slope after this jump to find c see fig 5 at a later time when the water reaches the end of the core again we can see a deviation from t scaling this is ignored in our calculation of c we can see that the estimated value of c is roughly constant regardless of the length of the core fig 6 the error bars in this plot indicate the uncertainty from ignoring the meniscus jump region they show the range of different slopes possible from the results the insensitivity of the results to core length implies that gravitational forces are negligible in these experiments the gravitational pressure drop across the core is at most δρgl where δρ is the density contrast g is the acceleration due to gravity and l is the length of the core this is at most 2 985 pa for the largest cores for comparison the air entry capillary pressure derived from the micp measurements is 5 515 pa 3 447 pa and 33 784 pa for estaillades ketton and portland respectively in all cases the capillary pressure is larger than the gravitational pressure difference although the gravitational pressure difference does have contribution to the flow we have ignored it in our analysis for the purpose of simplicity from darcy s law the flow rate is proportional to permeability k however here the driving force is capillary pressure which using the leverett j function equation scales as 1 k 6 p c σ cos θ ϕ k j s w where pc is capillary pressure pa σ is interfacial tension n m θ is the contact angle ϕ is porosity k is permeability m2 and j sw is the dimensionless j function the end result is an imbibition rate that theoretically scales as k from the non linear capillary dispersion eq 3 we expect the rate at which the mass is imbibed indicated by the parameter c to scale as the square root of permeability k as discussed previously fig 7 shows the c plotted as a function of k for the three rocks the rock type with the largest permeability ketton tends to have the highest imbibition rate while the lowest permeability rock portland have the lowest rate several studies have estimated relative permeability and capillary pressure from spontaneous imbibition measurements alyafei et al 2016 haugen et al 2014 li and horne 2005 in this paper we show that we can estimate the relative permeability and capillary pressure from matching the semi analytical solution with the experimental data we compare the volume imbibed from the experiments and the semi analytical solution for this comparison we use data for one rock of each type e2 k1 p2 as shown in fig 8 therefore we use the c value to calculate the volume imbibed which is described in eq 4 and multiplied by the area of the core sample used the calculation of the volumes imbibed are based on the c values from the semi analytical solution 14 2 10 5 m s 16 1 10 5 m s and 2 7 10 5 m s for estaillades ketton and portland respectively in our theoretical analysis we assume corey or power law expressions for relative permeability and capillary pressure 7 k r w k r w m a x s w s w i 1 s w r s g r n where krw is the water relative permeability k rw max is the maximum water relative permeability sw is the water saturation swi is the initial water saturation sgr is the residual gas saturation and n is the corey water exponent 8 k r g k r g m a x 1 s w s w i 1 s w r s g r m where krg is the gas relative permeability k rg max is the maximum gas relative permeability sw is the water saturation swi is the initial water saturation sgr is the residual gas saturation and m is the corey gas exponent 9 p c p c e n t r y s w s w i 1 s w i s g r α where pc is the capillary pressure p c entry is the entry capillary pressure pa sw is the water saturation swi is the initial water saturation sgr is the residual gas saturation and α is the capillary pressure exponent then we adjust the following parameters k rw max n k ro max m p c entry and α so that the experimental results and analytical predictions match the original spreadsheets were modified to handle mass imbibition and the updated version can be found here we found that the water relative permeability and capillary pressure have the most impact on the theoretical solution while the air relative permeability had little impact on the results this makes physical sense as the air has a low viscosity and is easily displaced the movement of the water front is essentially controlled entirely by the water relative permeability the ability to flow and the capillary pressure the driving force our best match is when the krw exponent 6 since the core is initially dry the water relative permeability is low as water will first preferentially fill the largely immobile micro porosity giving a large change in saturation but little increase in relative permeability indicative of a high corey exponent the relative permeabilities and capillary pressures used for the matching are shown in figs 9 and 10 respectively the water saturation will have to increase to a large value in order to gain conductivity through the macro porosity fernøet al 2013 the presence of an initial water saturation may provide better conductivity with smaller corey exponents li et al 2002 zhou et al 2000 the mass imbibition and the theoretical solution do not show good agreement with ketton due to the meniscus jump our aim for this case is to match the mid time recovery to avoid the uncertainty with the meniscus jump at the early time of imbibition the matching becomes better as the permeability decreases further studies in improving experimental procedure to reduce the meniscus jump effect needs to be addressed overall when matching the experimental data with the semi analytical solution the c values are in agreement with the experimental c values within experimental error the solution is not unique since we have only one measured function and three saturation dependent properties two relative permeabilities and the imbibition capillary pressure however these experiments could be used in conjunction with traditional coreflooding to determine all three functions together for instance if we had measured the two relative permeability functions we should be able to find uniquely the capillary pressure that gave the measured imbibition profile hence by using conventional measurements of relative permeability steady state or using buckley leverett theory in an unsteady state experiment and the spontaneous imbibition saturation profile and or mass imbibition data we can measure the imbibition capillary pressure we could also determine the imbibition relative permeability from a measured capillary pressure and the spontaneous imbibition data as it stands we can match the data but the functions used are not uniquely determined furthermore this approach is only possible if we see t scaling of the imbibition front a different method is needed if this is not the case nooruddin and blunt 2016 4 conclusions we have used the solution for spontaneous imbibition derived by schmid et al 2011 to compare to direct experimental measurements and have shown its current applications and discussed its potential future applications we have shown how to obtain the constant c which determines the imbibition rate from a simple mass imbibition experiment which is an important input parameter in the solution we show that regardless of the length of the core the measured value of c appears to be constant the value is a function of permeability with higher c values for higher permeability rocks we also show how to estimate imbibition relative permeability and capillary pressure from mass imbibition experiments these estimations are not unique as we deal with several unknowns however we used reasonable estimate of relative permeability and capillary pressure as well as constraining the c values to experimental measurements future work could use measured saturation profiles using different fluid pairs and boundary conditions during imbibition to further help constrain the relative permeabilities and capillary pressures acknowledgments we would like to acknowledge the qatar carbonates and carbon storage research centre qccsrc which is supported jointly by qatar petroleum shell and the qatar science technology park and qatar national research fund qnrf project number nprp10 0101 170086 for funding this project 
825,we perform spontaneous imbibition experiments on three carbonates estaillades ketton and portland which are three quarry limestones that have very different pore structures and span wide range of permeability we measure the mass of water imbibed in air saturated cores as a function of time under strongly water wet conditions specifically we perform co current spontaneous experiments using a highly sensitive balance to measure the mass imbibed as a function of time for the three rocks we use cores measuring 37 mm in diameter and three lengths of approximately 76 mm 204 mm and 290 mm we show that the amount imbibed scales as the square root of time and find the parameter c where the volume imbibed per unit cross sectional area at time t is ct 1 2 we find higher c values for higher permeability rocks employing semi analytical solutions for one dimensional flow and using reasonable estimates of relative permeability and capillary pressure we can match the experimental data we finally discuss how in combination with conventional measurements we can use theoretical solutions and imbibition measurements to find or constrain relative permeability and capillary pressure keywords spontaneous imbibition capillary dominated flow relative permeability capillary pressure semi analytical solution 1 introduction spontaneous water imbibition is the invasion of the water into a porous medium due to capillary forces and can only occur in water wet and mixed wet systems morrow and mason 2001 spontaneous imbibition si has two modes co current and counter current see fig 1 counter current imbibition occurs when the oil and brine flow in opposite directions from the same inlet whereas co current is when the brine and oil flow in the same direction the rate of water imbibition into the porous medium is a function of permeability relative permeability capillary pressure initial water saturation boundary conditions viscosity interfacial tension and wettability graue and fernø 2011 mason and morrow 2013 zhang et al 1996 spontaneous imbibition is an important recovery mechanism in naturally and artificially induced fractured reservoirs morrow and mason 2001 in addition imbibition is the process rendering carbon dioxide immobile in the process of carbon capture and storage ccs alyafei and blunt 2016 recently the use of imbibition experiments to estimate multi phase flow parameters such as relative permeability and capillary pressure has been proposed alyafei et al 2016 haugen et al 2014 li and horne 2005 several studies proposed analytical solutions for spontaneous imbibition cil 1996 kashchiev and firoozabadi 2003 however these solutions made additional assumptions that were not physically valid in contrast the solution derived by schmid et al 2016 2011 based on the work of mcwhorter and sunada 1990 1992 is a general solution applicable for any combination of relative permeability and capillary pressure a detailed mathematical description of this solution is provided elsewhere alyafei et al 2016 schmid et al 2016 for co current flow the conservation equations can be expressed as 1 f f f ϕ 2 c 2 d and for counter current flow 2 f f ϕ 2 c 2 d where 3 d s w k λ w λ n w λ t p c s w is the non linear dispersion coefficient m2 s f is the capillary dominated fractional flow f is the second derivative of the capillary dominated fractional flow ϕ is porosity c is a constant that quantifies the rock s ability to imbibe m s f is the buckley leverett fractional flow λw is the wetting phase mobility 1 pa s λnw is the non wetting phase mobility 1 pa s λt is the total mobility 1 pa s and pc sw is the derivative of the capillary pressure with respect to saturation pa the volume of water imbibed per unit area per unit time qw t is defined as 4 q w t 2 c t where c is the imbibition constant m s and t is time s schmid et al 2016 2011 presented the formal solution to the co current flow as 5 f ϕ 2 c 2 d f f d 2 s w and similarly with f 0 for counter current flow this equation is implicit in f and so can only be solved iteratively open source spreadsheets have been provided to analyze the semi analytical solution for co and counter current spontaneous imbibition and can be found here alyafei et al 2016 schmid et al 2016 the resultant solution is a function of both imbibition relative permeability and capillary pressure which is different than the buckley leverett solution for flow with an imposed pressure difference which is a function of relative permeability only note the solution assumes that the amount imbibed scales as the square root of time in addition it is only valid at early time where the flow is entirely governed by capillary forces in the absence of constraining boundaries li and horne 2001 olafuyi et al 2007 suzanne et al 2003 the late time is governed by boundary diffusion where the water front reaches the boundary and the recovery rate decays exponentially in this paper we will address the following examine the validity of using t as a scaling parameter and observe the imbibition behavior on different rocks with varying lengths provide a procedure to obtain the c constant from mass imbibition data and discuss its application develop an understanding of spontaneous imbibition of uniformly water wet media experimentally and compare it with the semi analytical solution discuss how to use spontaneous imbibition experiments in combination with other more traditional measurements to determine imbibition capillary pressure and relative permeability in addition this paper gives a systematic procedure to extract imbibition relative permeability and capillary pressure from simple mass imbibition experiments 2 experimental procedure 2 1 rocks we use three carbonate rocks in our study estaillades is a bioclastic limestone which contains 99 calcite caco3 and traces of dolomite and silica and comes from france wright et al 1995 the measured porosity of estaillades ranges from 27 5 28 2 with permeability 1 19 3 94 10 13 m2 ketton is an oolitic limestone of 99 1 calcite and 0 9 quartz and comes from the uk ashton 1980 the porosity ranges from 20 5 23 4 and the permeability from 1 37 2 54 10 12 m2 portland is a skeletal peloidal limestone of 96 6 calcite and 3 4 quartz and comes from the uk brenchley and rawson 2006 the porosity ranges from 16 1 20 0 and the permeability from 0 65 3 5 10 14 m2 in our study we use cores measuring 37 mm in diameter and three lengths of approximately 76 mm 204 mm and 290 mm two samples of each length were studied to assess experimental reproducibility table 1 shows further geological information about the rocks used fig 2 shows the measured mercury injection capillary pressure micp of samples of the three rocks using an autopore iv 9520 measured at weatherford laboratories in east grinstead uk we can see that portland has the highest capillary entry pressure minimum value of p 2σcos θ of approximately 0 18 µm 1 while estaillades has a lower capillary entry pressure of approximately 0 029 µm 1 ketton has the lowest capillary entry pressure of approximately 0 017 µm 1 2 2 fluids and conditions we conduct our experiments at ambient conditions of atmospheric pressure and room temperature of 20 1 c we use air as the non wetting phase and brine with 5 wt sodium chloride nacl and 1 wt potassium chloride kcl mixed with deionised water as the wetting liquid phase in addition we equilibrate the brine with the carbonate samples for 48 h by mixing them using magnetic stirrer to eliminate any reaction between the brine and the rock surface which might alter the rock morphology then we leave the brine for additional 48 hours to settle and finally we filter it using a fine filter paper to remove the particles that might block the flow pathways of the rocks the density of brine is 1 040 8 kg m3 measured using anton paar dma 5000 m and the viscosity is reported as 1 0085 mpa s lide 2004 the air brine interfacial tension is 0 073 n m measured using ramé hart model 590 device and the air viscosity is reported as 0 0018 mpa s tavassoli et al 2005 2 3 mass imbibition before we start the si experiment we perform our routine analysis by taking the dimensions of the core measuring the dry weight of the core and measuring the helium porosity however we use the mass balance technique to measure the porosity for the 204 mm and 209 mm length cores where we take the dry weight and compare it to the fully saturated core with degassed brine after the experiments since the helium porosimeter cell is too small to fit them we measure the permeability of the core using either gas before starting the spontaneous imbibition experiment or brine after finishing the spontaneous imbibition experiment and measuring the fluid saturation where we use a hassler type cell with a cylindrical confining fluid we use three cell lengths to fit our different core lengths to start our si experiment we apply heat shrink wrapping to confine the outer boundaries of the core and make sure that only co current imbibition is applied then we weigh the core again with the heat shrink and after that we attach the core to a mettler toledo xp5003s balance with 0 001 g accuracy and we lift the brine reservoir at the bottom of the core surface to be in contact with the core fig 3 before that moment we start recording the weight changes over time as the balance is connected to the computer we have three recording settings we record 10 points per second 5 points per second and 2 points per second we use the 10 points per second for ketton as it has the highest permeability and the imbibition process is the quickest 5 points per second for estaillades and 2 points per second for portland after imbibition has finished we weigh the core again and by that we can measure the residual gas saturation sgr using material balance we assume that since we run the experiments at ambient conditions there is no compression of the gas after it is trapped we then vacuum saturate the cores for 24 h to make sure that there is no air in the system then we insert the core into the hassler cell to measure the permeability where we keep injecting degassed brine until we reach a steady state flow regime finally we take the core out and weigh it to measure the porosity using the mass balance technique for consistency we replicate each experiment with a core from the same block of the same size table 2 summarizes the properties of the cores used in the experiments 3 results and discussion fig 4 shows the mass imbibed as a function of square root of time of the various lengths of each rock we can see that the early imbibition shows a rapid increase of brine flow rate which then decays at a later time to find the value of c which is the parameter that quantifies the rock s ability to imbibe we plot the mass imbibed as a function of t instead of t we divide the mass imbibed by the brine density and by the area of the core open to water flow to obtain a volume per unit area then by taking the slope of the curves we obtain the parameter 2c m s from eq 4 fig 5 we see a sudden rise in the mass imbibed at the beginning of the experiment this affects high permeability rocks more which is caused by a meniscus jump when the core is first put in contact with the brine lababjos broncano et al 2001 washburn 1921 other possible origins of this affect include an imbibition incubation time or non equilibrium effects see barenblatt et al 1990 2003 for a fuller discussion we take the slope after this jump to find c see fig 5 at a later time when the water reaches the end of the core again we can see a deviation from t scaling this is ignored in our calculation of c we can see that the estimated value of c is roughly constant regardless of the length of the core fig 6 the error bars in this plot indicate the uncertainty from ignoring the meniscus jump region they show the range of different slopes possible from the results the insensitivity of the results to core length implies that gravitational forces are negligible in these experiments the gravitational pressure drop across the core is at most δρgl where δρ is the density contrast g is the acceleration due to gravity and l is the length of the core this is at most 2 985 pa for the largest cores for comparison the air entry capillary pressure derived from the micp measurements is 5 515 pa 3 447 pa and 33 784 pa for estaillades ketton and portland respectively in all cases the capillary pressure is larger than the gravitational pressure difference although the gravitational pressure difference does have contribution to the flow we have ignored it in our analysis for the purpose of simplicity from darcy s law the flow rate is proportional to permeability k however here the driving force is capillary pressure which using the leverett j function equation scales as 1 k 6 p c σ cos θ ϕ k j s w where pc is capillary pressure pa σ is interfacial tension n m θ is the contact angle ϕ is porosity k is permeability m2 and j sw is the dimensionless j function the end result is an imbibition rate that theoretically scales as k from the non linear capillary dispersion eq 3 we expect the rate at which the mass is imbibed indicated by the parameter c to scale as the square root of permeability k as discussed previously fig 7 shows the c plotted as a function of k for the three rocks the rock type with the largest permeability ketton tends to have the highest imbibition rate while the lowest permeability rock portland have the lowest rate several studies have estimated relative permeability and capillary pressure from spontaneous imbibition measurements alyafei et al 2016 haugen et al 2014 li and horne 2005 in this paper we show that we can estimate the relative permeability and capillary pressure from matching the semi analytical solution with the experimental data we compare the volume imbibed from the experiments and the semi analytical solution for this comparison we use data for one rock of each type e2 k1 p2 as shown in fig 8 therefore we use the c value to calculate the volume imbibed which is described in eq 4 and multiplied by the area of the core sample used the calculation of the volumes imbibed are based on the c values from the semi analytical solution 14 2 10 5 m s 16 1 10 5 m s and 2 7 10 5 m s for estaillades ketton and portland respectively in our theoretical analysis we assume corey or power law expressions for relative permeability and capillary pressure 7 k r w k r w m a x s w s w i 1 s w r s g r n where krw is the water relative permeability k rw max is the maximum water relative permeability sw is the water saturation swi is the initial water saturation sgr is the residual gas saturation and n is the corey water exponent 8 k r g k r g m a x 1 s w s w i 1 s w r s g r m where krg is the gas relative permeability k rg max is the maximum gas relative permeability sw is the water saturation swi is the initial water saturation sgr is the residual gas saturation and m is the corey gas exponent 9 p c p c e n t r y s w s w i 1 s w i s g r α where pc is the capillary pressure p c entry is the entry capillary pressure pa sw is the water saturation swi is the initial water saturation sgr is the residual gas saturation and α is the capillary pressure exponent then we adjust the following parameters k rw max n k ro max m p c entry and α so that the experimental results and analytical predictions match the original spreadsheets were modified to handle mass imbibition and the updated version can be found here we found that the water relative permeability and capillary pressure have the most impact on the theoretical solution while the air relative permeability had little impact on the results this makes physical sense as the air has a low viscosity and is easily displaced the movement of the water front is essentially controlled entirely by the water relative permeability the ability to flow and the capillary pressure the driving force our best match is when the krw exponent 6 since the core is initially dry the water relative permeability is low as water will first preferentially fill the largely immobile micro porosity giving a large change in saturation but little increase in relative permeability indicative of a high corey exponent the relative permeabilities and capillary pressures used for the matching are shown in figs 9 and 10 respectively the water saturation will have to increase to a large value in order to gain conductivity through the macro porosity fernøet al 2013 the presence of an initial water saturation may provide better conductivity with smaller corey exponents li et al 2002 zhou et al 2000 the mass imbibition and the theoretical solution do not show good agreement with ketton due to the meniscus jump our aim for this case is to match the mid time recovery to avoid the uncertainty with the meniscus jump at the early time of imbibition the matching becomes better as the permeability decreases further studies in improving experimental procedure to reduce the meniscus jump effect needs to be addressed overall when matching the experimental data with the semi analytical solution the c values are in agreement with the experimental c values within experimental error the solution is not unique since we have only one measured function and three saturation dependent properties two relative permeabilities and the imbibition capillary pressure however these experiments could be used in conjunction with traditional coreflooding to determine all three functions together for instance if we had measured the two relative permeability functions we should be able to find uniquely the capillary pressure that gave the measured imbibition profile hence by using conventional measurements of relative permeability steady state or using buckley leverett theory in an unsteady state experiment and the spontaneous imbibition saturation profile and or mass imbibition data we can measure the imbibition capillary pressure we could also determine the imbibition relative permeability from a measured capillary pressure and the spontaneous imbibition data as it stands we can match the data but the functions used are not uniquely determined furthermore this approach is only possible if we see t scaling of the imbibition front a different method is needed if this is not the case nooruddin and blunt 2016 4 conclusions we have used the solution for spontaneous imbibition derived by schmid et al 2011 to compare to direct experimental measurements and have shown its current applications and discussed its potential future applications we have shown how to obtain the constant c which determines the imbibition rate from a simple mass imbibition experiment which is an important input parameter in the solution we show that regardless of the length of the core the measured value of c appears to be constant the value is a function of permeability with higher c values for higher permeability rocks we also show how to estimate imbibition relative permeability and capillary pressure from mass imbibition experiments these estimations are not unique as we deal with several unknowns however we used reasonable estimate of relative permeability and capillary pressure as well as constraining the c values to experimental measurements future work could use measured saturation profiles using different fluid pairs and boundary conditions during imbibition to further help constrain the relative permeabilities and capillary pressures acknowledgments we would like to acknowledge the qatar carbonates and carbon storage research centre qccsrc which is supported jointly by qatar petroleum shell and the qatar science technology park and qatar national research fund qnrf project number nprp10 0101 170086 for funding this project 
826,non invasive laboratory based x ray microtomography has been widely applied in many industrial and research disciplines however the main barrier to the use of laboratory systems compared to a synchrotron beamline is its much longer image acquisition time hours per scan compared to seconds to minutes at a synchrotron which results in limited application for dynamic in situ processes therefore the majority of existing laboratory x ray microtomography is limited to static imaging relatively fast imaging tens of minutes per scan can only be achieved by sacrificing imaging quality e g reducing exposure time or number of projections to alleviate this barrier we introduce an optimized implementation of a well known iterative reconstruction algorithm that allows users to reconstruct tomographic images with reasonable image quality but requires lower x ray signal counts and fewer projections than conventional methods quantitative analysis and comparison between the iterative and the conventional filtered back projection reconstruction algorithm was performed using a sandstone rock sample with and without liquid phases in the pore space overall by implementing the iterative reconstruction algorithm the required image acquisition time for samples such as this with sparse object structure can be reduced by a factor of up to 4 without measurable loss of sharpness or signal to noise ratio keywords x ray mictomography iterative reconstruction in situ imaging image quality 1 introduction non invasive high resolution x ray microtomography has found a wide range of applications in materials science and engineering bertei et al 2016 eastwood et al 2014 kinney et al 1994 landis and keane 2010 maire and withers 2014 puncreobutr et al 2012 stock 2008 including geological related studies gouze and luquot 2011 ketcham and carlson 2001 lin et al 2016b saif et al 2017 for example the oil and gas industry has seen a transformation of the application of pore scale imaging and modelling from a primarily research focussed pursuit concerned with the investigation of fundamental flow and transport properties in porous media to an increasingly crucial industrial tool for the characterization of a range of geological and petrophysical properties including porosity pore connectivity permeability mineralogy geomechanical response geological facies type and diagenetic history andrä et al 2013 blunt et al 2013 cnudde and boone 2013 lin et al 2016a lindquist et al 2000 recent years have seen the rapid technological development of the ability to not only image rock structures but also in situ fluid flow and heterogeneous reaction through these pore structures at conditions of temperature and pressure representative of subsurface processes applied to measure fluid saturations saturation distributions capillary trapping localized capillary pressure measurement and even spatially resolved contact angle aghaei and piri 2015 alratrout et al 2017 andrew et al 2014a 2014b armstrong et al 2014 arns et al 2005 berg et al 2013 blunt 2017 lin et al 2017 norouzi apourvari and arns 2016 schlüter et al 2014 when using x ray microtomographic techniques two crucial parameters are image quality and acquisition time frequently however these two parameters are juxtaposed better image quality can be achieved by increasing image acquisition time principally by increasing the number of projections or by increasing individual projection exposure time absolute image acquisition speed is also of critical importance when performing in situ experimentation as more rapid image acquisition enables the imaging of dynamic processes there are an increasing number of studies and applications requiring fast and dynamic imaging in different disciplines andrew et al 2015 berg et al 2013 gao et al 2017 garcea et al 2017 reynolds et al 2017 robinson et al 2014 saif et al 2016 singh et al 2017 which can be achieved by using a synchrotron based x ray tomography with shorter total tomography times approximately 1 30 s in modern beamline light sources the main barrier for laboratory based high resolution dynamic x ray microtomography is its much longer acquisition time which can be on the order of several hours when compared to a synchrotron beamline to significantly reduce the acquisition time when using a laboratory instrument most studies to date have sacrificed image quality for example reducing exposure time reducing the number of projections or using a coarser voxel size al khulaifi et al 2018 2017 bultreys et al 2016 menke et al 2015 significant increases in throughput decrease in acquisition time can be achieved without sacrificing these if we consider each step in the tomographic workflow typically sample mounting scan setup image acquisition 3d computational reconstruction image processing and segmentation and quantitative analysis to optimize the entire imaging workflow each step in the process must be examined the simplest way to increase image throughput while maintaining image quality is to increase the power of the x ray source this allows for the same number of x rays to be acquired in a shorter period of time allowing for exposure times to be decreased and so acquisition is sped up this is the fundamental reason why much of the work on dynamic tomography has focussed on extremely powerful synchrotron light sources the availability of beamtime at synchrotrons however is very limited and expensive in a laboratory setting source power is fundamentally limited by spot size for a given spot size limited system architecture a smaller spot will give an improved resolution an alternative approach is to improve the reconstruction algorithm such that it requires less information i e a lower signal to noise image at the detector achieved using a shorter exposure time or a smaller number of acquired projections when compared with conventional reconstruction algorithms i e feldkamp davis and kress fdk filtered back projection feldkamp et al 1984 iterative reconstruction has the potential of greatly increasing reconstructed image quality for rapid acquisitions at low projection numbers or short exposure times iterative techniques have been investigated extensively for reducing radiation dose in medical ct applications de man and fessler 2010 fessler 2000 pan et al 2009 sidky and pan 2008 more recently there has been much active research into iterative algorithms for microtomography with a particular focus on techniques for time resolved dynamic imaging kazantsev et al 2016 2015 myers et al 2015 2011 van eyndhoven et al 2015 a significant reduction of the number of projections has been demonstrated through techniques such as discrete tomography zhuge et al 2016 for samples consisting of a finite number of materials and neural networks pelt and batenburg 2013 the development of freely available open source software packages such as astra van aarle et al 2016 and tomopy gürsoy et al 2014 has speeded the development and application of these techniques the main focus of our work is to develop an implementation of iterative reconstruction to enable it to be used routinely and reliably as part of a scientific or industrial workflow several challenges must be overcome before this can happen the first of these is that iterative reconstruction is much more computationally demanding than traditional analytical reconstruction techniques an iterative algorithm is typically at least an order of magnitude slower than an analytical algorithm such as fdk this means that while the acquisition stage of the workflow may be sped up dramatically this comes at the cost of significantly increased reconstruction times secondly iterative reconstruction requires the optimization of several tuneable parameters for effective reconstruction which can be challenging for medium or low skill operators in this paper we will demonstrate recent advances in the application of iterative reconstruction techniques applied on the laboratory based versa 520 x ray microscope carl zeiss x ray microscopy to solve the challenge of the computational cost of iterative reconstruction we implemented both algorithmic and computational optimizations we use a statistical iterative algorithm including ordered subsets os and nesterov s momentum technique similar to that described by kim et al 2015 the combination of the quadratic convergence rate provided by the momentum term and the additional linear convergence rate increase provided by os reduces the number of iterations required for acceptable results from many hundreds to approximately 10 20 on the computational side we use a highly parallelized unmatched forward and back projection code implemented on nvidia gpus using cuda we compare this implementation of iterative reconstruction to traditional analytical reconstruction for a homogenous reservoir sandstone both in a dry state with its pore space filled with only air and in a wet state with its pore space partially saturated with oil and water the comparison is mainly made of quantitative metrics of image quality including signal to noise ratio and edge sharpness we show that for a defined processing and segmentation workflow equivalent or better results can be obtained using as few as one quarter the projections and so one quarter the scan time increasing throughput by up to a factor of 4 although the case studies demonstrated in this work are in geological samples with multiphase fluid in the pore spaces the iterative reconstruction algorithm presented here can be applied to other tomographic applications with a similarly sparse image structure it should be noted that this includes many common applications in materials science and non destructive testing the exact amount of throughput increase that is possible in any given application depends on the object s structure and also on what information is sought from the reconstructed volume quantifying this is still a largely open problem which has been studied in the framework of compressed sensing jorgensen et al 2013 jørgensen et al 2017 jorgensen and sidky 2015 2 materials and methods 2 1 materials to demonstrate the comparison between conventional fdk filtered back projection and iterative reconstruction algorithms we selected a bentheimer sandstone sample which contains 95 quartz 4 feldspar and approximately 1 fine clay andrew et al 2014c a cylindrical sample was prepared having 4 95 mm in diameter 10 mm in length the total porosity measured by helium porosimetry is around 20 andrew et al 2014c bentheimer sandstone was selected due to its low mineralogy complexity and high level of porosity resolvable by micro ct this is ideal to compare and assess different reconstruction algorithms 2 2 image acquisition in this study two cases were designed to compare the different reconstruction algorithms dry scans pore space filled with air and multiphase scans both brine doped with 3 5 wt potassium iodide and decane exist within the pore space both dry scans and multiphase scans for the sample were performed by placing the sample in a carbon fibre core holder with a confining pressure of 20 mpa to avoid fluid bypass when injecting brine and decane the same region of the sample was scanned at an energy of 80 kev power of 7 w and a voxel size of 6 0 µm the dimension for each reconstructed image is 1024 1024 1024 voxels to assess the two reconstruction algorithms the exposure time for each scan was controlled to quantify the impact of the average signal counts and the number of projections on image quality the counts and number of projections for each scan are listed in table 1 and the scanning time for each combination is shown in table 2 the scans with high counts and many projections that were reconstructed using the fdk algorithm were used as reference scans against which other scans were compared two sets of scanning times are reported in table 2 first the acquisition time for a scan where the sample is stopped during projection acquisition then repositioned between projections this is optimized to minimize image motion and ring artefacts second a theoretical acquisition time is reported corresponding to the case where the sample is rotated during each projection 2 3 description of the filtered back projection and iterative reconstruction algorithms the conventional technique for reconstruction of x ray microtomographic datasets is the algorithm of feldkamp davies and kress fdk feldkamp et al 1984 this is based on a continuous mathematical model of the x ray projection process which is then solved analytically using transform methods this analytical solution is then discretized to yield a filtered back projection algorithm this is a one shot process that can be implemented highly efficiently the fdk algorithm is known to perform well at low cone angles and with low noise datasets consisting of a sufficiently large number of projections brokish and bresler 2006 however the performance of the algorithm with noisy and or under sampled data may reduce dramatically with reconstructed images suffering from increased noise reduced resolution and potentially severe artefacts boas and fleischmann 2012 joseph and schulz 1980 statistical iterative reconstruction sir algorithms fessler 2000 offer an alternative to analytical algorithms such as fdk and are known to offer increased tolerance to noisy or under sampled data de man and fessler 2010 beginning with a discrete representation of the projection process this is used to formulate a cost function which is then minimized using an iterative algorithm our implementation uses a cost function of the form 1 φ x 1 2 a x b w 2 α r x where the vector x represents the reconstructed volume vector b represents the post log corrected absorption projection data and matrix a represents the projection process the diagonal matrix w provides a statistical weighting of the projection data to more accurately reflect noise statistics using a gaussian noise model for the absorption data the weighting factor for the ith ray is given by wi exp bi the effect of this is to more heavily weight rays with higher signal to noise ratio during the reconstruction thus reducing the impact of the noise the first term in the cost function measures the data fit while the function r x acts as a regularization penalty rewarding smoothness on the reconstructed volume the parameter α controls the balance between data fit and regularization the regularization penalty is defined as 2 r x r 1 n r 1 d r ϕ c x r where c x r j 1 n v c r j x j and the matrix c is a finite difference operator encapsulating differences between each voxel and its 26 first order neighbours in such a way as each difference is counted only once the matrix c is of size nr nv where nv is the total number of voxels and nr is the total number of first order finite difference pairs over the entire reconstructed volume dr is the euclidean distance between each finite difference pair this provides the correct spatial weighting for the penalty function ϕ we use the edge preserving huber function huber 1964 defined as 3 ϕ t 1 2 t 2 t δ δ t δ 2 otherwise the huber parameter δ controls the trade off between edge preservation and noise reduction for sufficiently small δ the huber penalty approximates the well known total variation tv penalty for differences between voxels of magnitude greater than δ the linear portion of the curve penalizes these values relatively less heavily than the quadratic portion applied to differences smaller than δ the assumption is that small differences are due to noise while larger differences are most likely edges and so implicitly that the edges are well resolved in the reconstructed image therefore in our implementation the value of δ is set based on the noise level in a test fdk reconstruction of the data for minimization of the cost function we use the separable quadratic surrogates sqs algorithm combined with ordered subsets os and nesterov s momentum kim et al 2015 direct minimization of the cost function of eq 1 is not possible so the sqs algorithm instead minimizes a sequence of much simpler quadratic surrogate functions each of which is solved using a single newton iteration yielding a gradient descent type algorithm this has an update step 4 x n 1 x n d 1 a t w a x n b α r x n where x n represents the reconstructed volume at the nth iteration the matrix d is known as a diagonal majorizing matrix its inverse can be pre calculated and stored prior to the iteration sequence convergence of sqs is mathematically proven to be monotonic but convergence is slow the convergence rate is 1 n where n is the number of iterations fessler 2000 therefore it may take many hundreds of iterations to achieve satisfactory results to accelerate this sqs is combined with nesterov s momentum nesterov 1983 which increases the convergence rate to o 1 n 2 giving satisfactory results in approximately 100 iterations additionally we apply the ordered subsets technique hudson and larkin 1994 which sub divides each iteration into approximately equally sized sets of projection angles and increases convergence rate of the early iterations to o 1 mn 2 where m is the number of subsets using ordered subsets no longer guarantees monotonic convergence of the algorithm but in practical use cases it is not necessary to obtain a solution to a high degree of accuracy in most cases using 8 subsets satisfactory results are achieved in approximately 10 20 iterations though the exact number required depends on the individual dataset to demonstrate this fig 1 shows the standard euclidian 2 norm of the difference em diff between successive iterations in the reconstructed volume for the few projection high count datasets for both the wet and dry cases at the nth iteration em diff is given by 5 e m diff n j 1 n v x j n x j n 1 2 where x j n and x j n 1 are the reconstructed grey scale values for the current and previous iterations respectively and nv is the total number of voxels em diff is a measure of how much the reconstructed image changes at each iteration and therefore how close it is to convergence note that differences after around iteration 20 are of the order 10 2 compared to the differences after initial iterations and contribute no significant visible change to the image one of the reasons iterative algorithms have not yet found widespread use is their computational expense the main computationally intensive steps are the matrix vector products of the form ax and atb which represent respectively forward and back projection each iteration requires one of each operation in contrast analytical algorithms such as fdk typically require only a single back projection in practice the projection matrix a is not calculated explicitly instead its coefficients are implicitly calculated on the fly as needed during forward and back projection for forward projection we use a ray driven approach which is effectively a 3d extension of joseph s algorithm joseph 1982 although this algorithm is known to cause a systematic variation in the sampling rate along the rays this has not been observed to cause any problems turbell 2001 our back projection uses a voxel driven approach with coefficients calculated using bilinear interpolation on the detector plane using different algorithms for forward and back projection results in a so called unmatched implementation guedouar and zarrad 2010 zeng and gullberg 2000 where the matrix implicitly used in back projection is not the exact transpose of that used in forward projection although this is theoretically not compliant with the derivation of the algorithm in practice it allows for a much more computationally efficient implementation we have also found that it improves stability and significantly reduces aliasing artefacts in the reconstructed volume our iterative reconstruction software is an optimized heterogeneous solution capable of running across multiple cpu threads and multiple nvidia gpus the computationally expensive operations of forward and back projection and calculation of the regularization term are performed on the gpus while less demanding operations such as updates of the reconstructed volume run in parallel in cpu threads the gpu code is written in cuda making full use of asynchronous operations to hide memory transfer latency behind computation our code makes use of built in hardware operations for bilinear interpolation in the gpu texture pipeline the reconstruction time for a typical dataset consisting of 400 projections of size 1024 1024 pixels and a reconstructed volume of size 1024 1024 1024 voxels is approximately 6 seconds per iteration plus a 25 s setup time the test hardware consisted of 2 intel xeon e5 2650 v3 cpus clocked at 2 3 ghz 192 gb ram and 2 nvidia p6000 gpus running windows 7 professional the timing scales approximately linearly with the number of projections 2 4 workflow for selection of optimal iterative reconstruction parameters to obtain optimal reconstructed image quality using the proposed iterative algorithm it is necessary to tune two parameters the total regularization amount α and the huber parameter δ the optimal parameter choice is dependent on the individual dataset and is highly variable to enable parameter tuning by inexperienced operators we have defined a parameter optimization workflow implemented in the gui of our iterative reconstruction software our workflow begins by setting the value of δ based on the noise level in a test fdk reconstruction of the data once this is done a series of images are reconstructed for a range of α values and presented to the user the choice of final α value is left to user preference and therefore somewhat subjective to save computational cost the α scan series is performed on a reduced size version of the dataset with projections truncated to the central 128 slices in the vertical direction for further speedup it is possible to compute the scan series for the different α values in parallel it should be noted that for applications where similar samples of the same class are repetitively imaged once the optimal parameters have been determined for a single sample they can be reused for other samples of the same class 3 results and discussion we demonstrate the superior performance of the iterative reconstruction algorithm by comparing it to the conventional fdk filtered back projection algorithm fdk reconstructions were performed with the vendor s software using the conventional ram lak filter with a gaussian pre filter applied to the projection data with a standard deviation of 0 5 iterative reconstructions used 8 subsets and 15 iterations the images analysed by both methods were registered and resampled to have the same orientation and a sub volume of a field of view 5003 cube voxels from the original 10243 cube voxels was used for comparison and assessment in addition we compare the reconstructed images from both methods with and without using non local means edge preserving filter typical of traditional image processing and analysis workflows schlüter et al 2014 we perform two case studies multiphase scans in section 3 1 and dry scans which show similar results can be found in the supporting information finally in section 3 2 we propose optimization of fast scanning with the iterative algorithm 3 1 case study 1 multiphase scans in this section scans 5003 cube voxels sub volume with two phases in the pore space are examined the scans with theoretically the lowest quality low counts and few projections and highest image quality high counts and many projections are highlighted in fig 2 the images showing larger field of views are shown in figs s1 and s2 in the supporting information once again this corresponds to the best and worst case rather than the optimized case which is examined in section 3 2 it can be seen when multiple fluid phases exist in the pore space the image quality is much worse compared to the corresponding dry scans with the same number of projections and counts from a visual assessment generally the images reconstructed by the iterative algorithm have lower noise levels the difference in image quality between the two reconstruction algorithms is much more obvious when the image quality is low 3 1 1 quantitative analysis signal to noise ratio signal to noise ratio within an image is a key quantitative performance metric when determining image quality and the ease and accuracy for image segmentation in fig 3 the oil brine and grain phases were segmented into green oil blue brine and red grain the boundary of the oil phase is shown in black the segmented image is generated by applying a histogram based thresholding for each phase followed by applying an erosion filter the phase boundary sharpness is quantified in section 3 1 2 in this study we define the phase contrast as the phase signal level for the oil phase brine phase and the grain phase which are determined by the histogram plot of the grey scale values fig 4 shows the histogram for oil brine and grain grey scale values for each scan with different numbers of projections signal counts and the two reconstruction algorithms the image quality reflected by width of the histogram of the image reconstructed by the iterative algorithm is generally much better than for the one that uses the fdk algorithm when the counts and number of projections are the same although applying a non local means filter can significantly improve the image quality for the images with the fdk algorithm generally the image quality is still not as good as when using the iterative algorithm moreover the non local means filter does not have a strong impact when using the iterative reconstruction algorithm this indicates that this portion of the distribution in x ray grey scale values is not attributable to random noise but rather to point to point grey scale variation either due to mineralogical variation in the sample or other artefacts this image quality phenomenon with different scanning conditions and different reconstruction algorithms is generally applicable to almost all scans for the low quality images e g images with few projections and low counts after applying the non local means filter the shape of the histogram plot for the brine phase becomes skewed this is mainly due to the high noise level and poor phase contrast in the multiphase scans the signal to noise ratio snr is defined as follows 6 s n r μ o i l μ g r a i n 1 2 σ o i l σ g r a i n where μ oil is the mean grey scale value for the oil phase μ grain is the mean grey scale value of the grain phase σ oil and σ grain are the standard deviation values for the oil phase and the grain phase the computed signal to noise ratio values for all the multiphase scans are shown in table 3 the snr value from iterative algorithms without applying filters are much higher than those from fdk values and are comparable to those from fdk values with filters this indicates that the phase noise which also reflects the general image quality from the iterative algorithm is much lower than the noise from the fdk algorithm and is at similar level compared to the noise from the fdk algorithm with image filters 3 1 2 quantitative analysis phase boundary sharpness fig 2 shows slices from the highest high counts and many projections and the lowest low counts and few projections quality images using both fdk and iterative reconstruction algorithms the images after applying non local means edge preserving filters are also displayed for comparison to assess the phase boundary sharpness the grey scale values along a line selected between points a and b in fig 2 which passes the oil brine and the grain were plotted as shown in fig 5 from fig 5 it can be observed that when using the fdk algorithm if the image quality is not sufficient e g fig 5a the boundary between two phases is not clear generally applying filters can reduce the image noise and the phase boundary becomes visible however applying a filter can also decrease the boundary sharpness e g fig 5e and f to quantify the sharpness of the phase boundary which is indicated by the steepness of the slope a logistic type of distribution can be used this is a novel application of this function allowing for a quantitative estimation of the length scale over which a phase transition within an image takes place a general expression of a logistic distribution can be defined as 7 f x c l 1 e k x x 0 where l corresponds to the difference between two phases c is the grey scale value of the least attenuating phase x 0 is the value of the sigmoid s midpoint and k is a parameter determining the steepness of the slope the sharpness of the phase boundary increases when the value of k increases and allows for the spatial length scale for an interface transition to be quantitatively and objectively measured the graph shown in fig 5f is used to demonstrate how to compute k values for a multiphase scan by fitting portions of the plot to eq 6 the graph was divided into three sections including three sub profiles passing from the grain phase to the oil phase k1 the oil phase to the brine phase k2 and the brine phase to the grain phase k3 each profile was fitted by a logistic distribution eq 6 and the mean value of k 1 k 2 and k 3 was used to quantify the phase average boundary sharpness kmean shown in fig 6 the k values with different scanning and reconstruction conditions are shown in table 4 generally the images reconstructed by the iterative algorithm are characterized by a sharper phase boundary compared with the images reconstructed by the fdk algorithm although applying theoretically edge preserving filters can significantly reduce the image noise this generally has a negative effect on the sharpness of the phase boundary 3 2 optimization of fast scanning with the iterative algorithm the key parameters affecting the scanning time are the number of projections and the exposure time as source brightness for a given system is usually fixed faster imaging can be achieved by sacrificing one or both of these parameters this also has the effect of reducing image quality to obtain the same or better quality reconstructed images from the same input projection data a different reconstruction algorithm can be used the assessment for the multiphase section 3 1 images and dry scans see supporting information with different reconstruction algorithms shows that by using an iterative reconstruction algorithm image quality can be significantly improved as compared to the image with the same scanning settings which was reconstructed by the conventional fdk filtered back projection algorithm therefore iterative reconstruction has the potential to significantly reduce scanning time while maintaining image quality in this section the image reconstructed by the iterative algorithm with either few projections but high counts or many projections but low counts are compared with a reference scan reconstructed by the fdk algorithm with many projections and high counts both images reconstructed by the iterative algorithm were about 4 times faster to acquire than the reference scan fig 7 shows a comparison of the scans both dry and partially saturated scans reconstructed using the iterative technique the images showing a larger field of view can be found in fig s7 in the supporting information the reference scan many projections high counts reconstructed using the fdk algorithm is also shown it can be observed that by using the iterative reconstruction algorithm the image is similar to the reference scan after either significantly reducing the number of projections using 401 projections rather than 1601 or lowering the x ray counts using 2000 counts rather than 10 000 the grey scale histograms for both dry and multiphase scans show that the noise levels indicated by the snr values from the images using the iterative reconstruction algorithm are much lower than the reference scan reconstructed by the fdk algorithm with many projections and high counts fig 8 the noise levels of the images using the iterative algorithms reconstructed using either few projections but high counts and many projections but low counts are similar the detailed images showing the phase boundary are displayed in fig 7 and the line profile analysis with the corresponding k values for phase sharpness is shown in fig 9 it can be seen that the iterative reconstruction algorithm can significantly reduce the scanning time and the image quality can be maintained or even improved compared to a standard high quality scan the two scans with an intermediate quality of input data many projections low counts or few projections high counts represent somewhat of a sweet spot for image acquisition significant acquisition time decreases can be seen while still maintaining both subjective and objective image quality the comparison between these two cases where image quality is approximately the same when reconstructed using the iterative algorithm is interesting as the effective throughput of each technique is not the same table 2 overall scan time is not simply proportional to exposure time each projection requires the sample to be rotated translated and for data to be read off the x ray detector all this creates a projection to projection overhead which is still present even at a short exposure time scan time is however simply related to the number of projections if acquisition speed is the priority reducing the number of projections is the best strategy as this will have the biggest impact on resulting acquisition throughput 4 conclusions the imaging of dynamic processes using laboratory based x ray microtomography is one of the primary emerging areas of ongoing scientific interest in different disciplines for example material science and engineering and single multiphase flow and transport in porous media however its application is limited by the time required to acquire images in this paper we have shown how iterative reconstruction techniques can be used to achieve the same benchmark image quality as measured by quantitative image performance metrics of signal to noise and edge sharpness in one quarter of the time taken compared to traditional reconstruction methods this was the case both for scanning of just the rock structure and the pore space containing multiple fluid phases it should be noted that both of these cases have a sparse image structure with the reconstructed volume consisting mainly of piecewise constant regions we would expect to be able to achieve comparable reduction of acquisition time in other use cases with similarly sparse image structure but the exact amount of achievable reduction is highly application dependent in particular in use cases where preservation of fine detail is important the amount of achievable throughput increase may be less this quadrupling of acquisition throughput has significant implications for the processes accessible to laboratory examination in the academic sphere research in different areas such as processes reaction and fluid dynamics are available with a coupled temporal and spatial resolution never before accessible in the industrial field particularly that associated with digital rock analysis for the oil and gas industry these developments have the potential of greatly reducing the acquisition time per sample and so associated cost for the analysis in a price constrained environment this could be critical for the future adoption of this technology there are still many opportunities to improve iterative reconstruction both in terms of reconstruction quality with new regularization techniques giving the same quality image for noisier input data ease of use completely automating the reconstruction process and computational efficiency acknowledgements we acknowledge the engineering and physical science research council for financial support through grant ep l012227 1 the images acquired in this study can be downloaded from https doi org 10 6084 m9 figshare 5962834 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2018 03 007 appendix supplementary materials image application 1 
826,non invasive laboratory based x ray microtomography has been widely applied in many industrial and research disciplines however the main barrier to the use of laboratory systems compared to a synchrotron beamline is its much longer image acquisition time hours per scan compared to seconds to minutes at a synchrotron which results in limited application for dynamic in situ processes therefore the majority of existing laboratory x ray microtomography is limited to static imaging relatively fast imaging tens of minutes per scan can only be achieved by sacrificing imaging quality e g reducing exposure time or number of projections to alleviate this barrier we introduce an optimized implementation of a well known iterative reconstruction algorithm that allows users to reconstruct tomographic images with reasonable image quality but requires lower x ray signal counts and fewer projections than conventional methods quantitative analysis and comparison between the iterative and the conventional filtered back projection reconstruction algorithm was performed using a sandstone rock sample with and without liquid phases in the pore space overall by implementing the iterative reconstruction algorithm the required image acquisition time for samples such as this with sparse object structure can be reduced by a factor of up to 4 without measurable loss of sharpness or signal to noise ratio keywords x ray mictomography iterative reconstruction in situ imaging image quality 1 introduction non invasive high resolution x ray microtomography has found a wide range of applications in materials science and engineering bertei et al 2016 eastwood et al 2014 kinney et al 1994 landis and keane 2010 maire and withers 2014 puncreobutr et al 2012 stock 2008 including geological related studies gouze and luquot 2011 ketcham and carlson 2001 lin et al 2016b saif et al 2017 for example the oil and gas industry has seen a transformation of the application of pore scale imaging and modelling from a primarily research focussed pursuit concerned with the investigation of fundamental flow and transport properties in porous media to an increasingly crucial industrial tool for the characterization of a range of geological and petrophysical properties including porosity pore connectivity permeability mineralogy geomechanical response geological facies type and diagenetic history andrä et al 2013 blunt et al 2013 cnudde and boone 2013 lin et al 2016a lindquist et al 2000 recent years have seen the rapid technological development of the ability to not only image rock structures but also in situ fluid flow and heterogeneous reaction through these pore structures at conditions of temperature and pressure representative of subsurface processes applied to measure fluid saturations saturation distributions capillary trapping localized capillary pressure measurement and even spatially resolved contact angle aghaei and piri 2015 alratrout et al 2017 andrew et al 2014a 2014b armstrong et al 2014 arns et al 2005 berg et al 2013 blunt 2017 lin et al 2017 norouzi apourvari and arns 2016 schlüter et al 2014 when using x ray microtomographic techniques two crucial parameters are image quality and acquisition time frequently however these two parameters are juxtaposed better image quality can be achieved by increasing image acquisition time principally by increasing the number of projections or by increasing individual projection exposure time absolute image acquisition speed is also of critical importance when performing in situ experimentation as more rapid image acquisition enables the imaging of dynamic processes there are an increasing number of studies and applications requiring fast and dynamic imaging in different disciplines andrew et al 2015 berg et al 2013 gao et al 2017 garcea et al 2017 reynolds et al 2017 robinson et al 2014 saif et al 2016 singh et al 2017 which can be achieved by using a synchrotron based x ray tomography with shorter total tomography times approximately 1 30 s in modern beamline light sources the main barrier for laboratory based high resolution dynamic x ray microtomography is its much longer acquisition time which can be on the order of several hours when compared to a synchrotron beamline to significantly reduce the acquisition time when using a laboratory instrument most studies to date have sacrificed image quality for example reducing exposure time reducing the number of projections or using a coarser voxel size al khulaifi et al 2018 2017 bultreys et al 2016 menke et al 2015 significant increases in throughput decrease in acquisition time can be achieved without sacrificing these if we consider each step in the tomographic workflow typically sample mounting scan setup image acquisition 3d computational reconstruction image processing and segmentation and quantitative analysis to optimize the entire imaging workflow each step in the process must be examined the simplest way to increase image throughput while maintaining image quality is to increase the power of the x ray source this allows for the same number of x rays to be acquired in a shorter period of time allowing for exposure times to be decreased and so acquisition is sped up this is the fundamental reason why much of the work on dynamic tomography has focussed on extremely powerful synchrotron light sources the availability of beamtime at synchrotrons however is very limited and expensive in a laboratory setting source power is fundamentally limited by spot size for a given spot size limited system architecture a smaller spot will give an improved resolution an alternative approach is to improve the reconstruction algorithm such that it requires less information i e a lower signal to noise image at the detector achieved using a shorter exposure time or a smaller number of acquired projections when compared with conventional reconstruction algorithms i e feldkamp davis and kress fdk filtered back projection feldkamp et al 1984 iterative reconstruction has the potential of greatly increasing reconstructed image quality for rapid acquisitions at low projection numbers or short exposure times iterative techniques have been investigated extensively for reducing radiation dose in medical ct applications de man and fessler 2010 fessler 2000 pan et al 2009 sidky and pan 2008 more recently there has been much active research into iterative algorithms for microtomography with a particular focus on techniques for time resolved dynamic imaging kazantsev et al 2016 2015 myers et al 2015 2011 van eyndhoven et al 2015 a significant reduction of the number of projections has been demonstrated through techniques such as discrete tomography zhuge et al 2016 for samples consisting of a finite number of materials and neural networks pelt and batenburg 2013 the development of freely available open source software packages such as astra van aarle et al 2016 and tomopy gürsoy et al 2014 has speeded the development and application of these techniques the main focus of our work is to develop an implementation of iterative reconstruction to enable it to be used routinely and reliably as part of a scientific or industrial workflow several challenges must be overcome before this can happen the first of these is that iterative reconstruction is much more computationally demanding than traditional analytical reconstruction techniques an iterative algorithm is typically at least an order of magnitude slower than an analytical algorithm such as fdk this means that while the acquisition stage of the workflow may be sped up dramatically this comes at the cost of significantly increased reconstruction times secondly iterative reconstruction requires the optimization of several tuneable parameters for effective reconstruction which can be challenging for medium or low skill operators in this paper we will demonstrate recent advances in the application of iterative reconstruction techniques applied on the laboratory based versa 520 x ray microscope carl zeiss x ray microscopy to solve the challenge of the computational cost of iterative reconstruction we implemented both algorithmic and computational optimizations we use a statistical iterative algorithm including ordered subsets os and nesterov s momentum technique similar to that described by kim et al 2015 the combination of the quadratic convergence rate provided by the momentum term and the additional linear convergence rate increase provided by os reduces the number of iterations required for acceptable results from many hundreds to approximately 10 20 on the computational side we use a highly parallelized unmatched forward and back projection code implemented on nvidia gpus using cuda we compare this implementation of iterative reconstruction to traditional analytical reconstruction for a homogenous reservoir sandstone both in a dry state with its pore space filled with only air and in a wet state with its pore space partially saturated with oil and water the comparison is mainly made of quantitative metrics of image quality including signal to noise ratio and edge sharpness we show that for a defined processing and segmentation workflow equivalent or better results can be obtained using as few as one quarter the projections and so one quarter the scan time increasing throughput by up to a factor of 4 although the case studies demonstrated in this work are in geological samples with multiphase fluid in the pore spaces the iterative reconstruction algorithm presented here can be applied to other tomographic applications with a similarly sparse image structure it should be noted that this includes many common applications in materials science and non destructive testing the exact amount of throughput increase that is possible in any given application depends on the object s structure and also on what information is sought from the reconstructed volume quantifying this is still a largely open problem which has been studied in the framework of compressed sensing jorgensen et al 2013 jørgensen et al 2017 jorgensen and sidky 2015 2 materials and methods 2 1 materials to demonstrate the comparison between conventional fdk filtered back projection and iterative reconstruction algorithms we selected a bentheimer sandstone sample which contains 95 quartz 4 feldspar and approximately 1 fine clay andrew et al 2014c a cylindrical sample was prepared having 4 95 mm in diameter 10 mm in length the total porosity measured by helium porosimetry is around 20 andrew et al 2014c bentheimer sandstone was selected due to its low mineralogy complexity and high level of porosity resolvable by micro ct this is ideal to compare and assess different reconstruction algorithms 2 2 image acquisition in this study two cases were designed to compare the different reconstruction algorithms dry scans pore space filled with air and multiphase scans both brine doped with 3 5 wt potassium iodide and decane exist within the pore space both dry scans and multiphase scans for the sample were performed by placing the sample in a carbon fibre core holder with a confining pressure of 20 mpa to avoid fluid bypass when injecting brine and decane the same region of the sample was scanned at an energy of 80 kev power of 7 w and a voxel size of 6 0 µm the dimension for each reconstructed image is 1024 1024 1024 voxels to assess the two reconstruction algorithms the exposure time for each scan was controlled to quantify the impact of the average signal counts and the number of projections on image quality the counts and number of projections for each scan are listed in table 1 and the scanning time for each combination is shown in table 2 the scans with high counts and many projections that were reconstructed using the fdk algorithm were used as reference scans against which other scans were compared two sets of scanning times are reported in table 2 first the acquisition time for a scan where the sample is stopped during projection acquisition then repositioned between projections this is optimized to minimize image motion and ring artefacts second a theoretical acquisition time is reported corresponding to the case where the sample is rotated during each projection 2 3 description of the filtered back projection and iterative reconstruction algorithms the conventional technique for reconstruction of x ray microtomographic datasets is the algorithm of feldkamp davies and kress fdk feldkamp et al 1984 this is based on a continuous mathematical model of the x ray projection process which is then solved analytically using transform methods this analytical solution is then discretized to yield a filtered back projection algorithm this is a one shot process that can be implemented highly efficiently the fdk algorithm is known to perform well at low cone angles and with low noise datasets consisting of a sufficiently large number of projections brokish and bresler 2006 however the performance of the algorithm with noisy and or under sampled data may reduce dramatically with reconstructed images suffering from increased noise reduced resolution and potentially severe artefacts boas and fleischmann 2012 joseph and schulz 1980 statistical iterative reconstruction sir algorithms fessler 2000 offer an alternative to analytical algorithms such as fdk and are known to offer increased tolerance to noisy or under sampled data de man and fessler 2010 beginning with a discrete representation of the projection process this is used to formulate a cost function which is then minimized using an iterative algorithm our implementation uses a cost function of the form 1 φ x 1 2 a x b w 2 α r x where the vector x represents the reconstructed volume vector b represents the post log corrected absorption projection data and matrix a represents the projection process the diagonal matrix w provides a statistical weighting of the projection data to more accurately reflect noise statistics using a gaussian noise model for the absorption data the weighting factor for the ith ray is given by wi exp bi the effect of this is to more heavily weight rays with higher signal to noise ratio during the reconstruction thus reducing the impact of the noise the first term in the cost function measures the data fit while the function r x acts as a regularization penalty rewarding smoothness on the reconstructed volume the parameter α controls the balance between data fit and regularization the regularization penalty is defined as 2 r x r 1 n r 1 d r ϕ c x r where c x r j 1 n v c r j x j and the matrix c is a finite difference operator encapsulating differences between each voxel and its 26 first order neighbours in such a way as each difference is counted only once the matrix c is of size nr nv where nv is the total number of voxels and nr is the total number of first order finite difference pairs over the entire reconstructed volume dr is the euclidean distance between each finite difference pair this provides the correct spatial weighting for the penalty function ϕ we use the edge preserving huber function huber 1964 defined as 3 ϕ t 1 2 t 2 t δ δ t δ 2 otherwise the huber parameter δ controls the trade off between edge preservation and noise reduction for sufficiently small δ the huber penalty approximates the well known total variation tv penalty for differences between voxels of magnitude greater than δ the linear portion of the curve penalizes these values relatively less heavily than the quadratic portion applied to differences smaller than δ the assumption is that small differences are due to noise while larger differences are most likely edges and so implicitly that the edges are well resolved in the reconstructed image therefore in our implementation the value of δ is set based on the noise level in a test fdk reconstruction of the data for minimization of the cost function we use the separable quadratic surrogates sqs algorithm combined with ordered subsets os and nesterov s momentum kim et al 2015 direct minimization of the cost function of eq 1 is not possible so the sqs algorithm instead minimizes a sequence of much simpler quadratic surrogate functions each of which is solved using a single newton iteration yielding a gradient descent type algorithm this has an update step 4 x n 1 x n d 1 a t w a x n b α r x n where x n represents the reconstructed volume at the nth iteration the matrix d is known as a diagonal majorizing matrix its inverse can be pre calculated and stored prior to the iteration sequence convergence of sqs is mathematically proven to be monotonic but convergence is slow the convergence rate is 1 n where n is the number of iterations fessler 2000 therefore it may take many hundreds of iterations to achieve satisfactory results to accelerate this sqs is combined with nesterov s momentum nesterov 1983 which increases the convergence rate to o 1 n 2 giving satisfactory results in approximately 100 iterations additionally we apply the ordered subsets technique hudson and larkin 1994 which sub divides each iteration into approximately equally sized sets of projection angles and increases convergence rate of the early iterations to o 1 mn 2 where m is the number of subsets using ordered subsets no longer guarantees monotonic convergence of the algorithm but in practical use cases it is not necessary to obtain a solution to a high degree of accuracy in most cases using 8 subsets satisfactory results are achieved in approximately 10 20 iterations though the exact number required depends on the individual dataset to demonstrate this fig 1 shows the standard euclidian 2 norm of the difference em diff between successive iterations in the reconstructed volume for the few projection high count datasets for both the wet and dry cases at the nth iteration em diff is given by 5 e m diff n j 1 n v x j n x j n 1 2 where x j n and x j n 1 are the reconstructed grey scale values for the current and previous iterations respectively and nv is the total number of voxels em diff is a measure of how much the reconstructed image changes at each iteration and therefore how close it is to convergence note that differences after around iteration 20 are of the order 10 2 compared to the differences after initial iterations and contribute no significant visible change to the image one of the reasons iterative algorithms have not yet found widespread use is their computational expense the main computationally intensive steps are the matrix vector products of the form ax and atb which represent respectively forward and back projection each iteration requires one of each operation in contrast analytical algorithms such as fdk typically require only a single back projection in practice the projection matrix a is not calculated explicitly instead its coefficients are implicitly calculated on the fly as needed during forward and back projection for forward projection we use a ray driven approach which is effectively a 3d extension of joseph s algorithm joseph 1982 although this algorithm is known to cause a systematic variation in the sampling rate along the rays this has not been observed to cause any problems turbell 2001 our back projection uses a voxel driven approach with coefficients calculated using bilinear interpolation on the detector plane using different algorithms for forward and back projection results in a so called unmatched implementation guedouar and zarrad 2010 zeng and gullberg 2000 where the matrix implicitly used in back projection is not the exact transpose of that used in forward projection although this is theoretically not compliant with the derivation of the algorithm in practice it allows for a much more computationally efficient implementation we have also found that it improves stability and significantly reduces aliasing artefacts in the reconstructed volume our iterative reconstruction software is an optimized heterogeneous solution capable of running across multiple cpu threads and multiple nvidia gpus the computationally expensive operations of forward and back projection and calculation of the regularization term are performed on the gpus while less demanding operations such as updates of the reconstructed volume run in parallel in cpu threads the gpu code is written in cuda making full use of asynchronous operations to hide memory transfer latency behind computation our code makes use of built in hardware operations for bilinear interpolation in the gpu texture pipeline the reconstruction time for a typical dataset consisting of 400 projections of size 1024 1024 pixels and a reconstructed volume of size 1024 1024 1024 voxels is approximately 6 seconds per iteration plus a 25 s setup time the test hardware consisted of 2 intel xeon e5 2650 v3 cpus clocked at 2 3 ghz 192 gb ram and 2 nvidia p6000 gpus running windows 7 professional the timing scales approximately linearly with the number of projections 2 4 workflow for selection of optimal iterative reconstruction parameters to obtain optimal reconstructed image quality using the proposed iterative algorithm it is necessary to tune two parameters the total regularization amount α and the huber parameter δ the optimal parameter choice is dependent on the individual dataset and is highly variable to enable parameter tuning by inexperienced operators we have defined a parameter optimization workflow implemented in the gui of our iterative reconstruction software our workflow begins by setting the value of δ based on the noise level in a test fdk reconstruction of the data once this is done a series of images are reconstructed for a range of α values and presented to the user the choice of final α value is left to user preference and therefore somewhat subjective to save computational cost the α scan series is performed on a reduced size version of the dataset with projections truncated to the central 128 slices in the vertical direction for further speedup it is possible to compute the scan series for the different α values in parallel it should be noted that for applications where similar samples of the same class are repetitively imaged once the optimal parameters have been determined for a single sample they can be reused for other samples of the same class 3 results and discussion we demonstrate the superior performance of the iterative reconstruction algorithm by comparing it to the conventional fdk filtered back projection algorithm fdk reconstructions were performed with the vendor s software using the conventional ram lak filter with a gaussian pre filter applied to the projection data with a standard deviation of 0 5 iterative reconstructions used 8 subsets and 15 iterations the images analysed by both methods were registered and resampled to have the same orientation and a sub volume of a field of view 5003 cube voxels from the original 10243 cube voxels was used for comparison and assessment in addition we compare the reconstructed images from both methods with and without using non local means edge preserving filter typical of traditional image processing and analysis workflows schlüter et al 2014 we perform two case studies multiphase scans in section 3 1 and dry scans which show similar results can be found in the supporting information finally in section 3 2 we propose optimization of fast scanning with the iterative algorithm 3 1 case study 1 multiphase scans in this section scans 5003 cube voxels sub volume with two phases in the pore space are examined the scans with theoretically the lowest quality low counts and few projections and highest image quality high counts and many projections are highlighted in fig 2 the images showing larger field of views are shown in figs s1 and s2 in the supporting information once again this corresponds to the best and worst case rather than the optimized case which is examined in section 3 2 it can be seen when multiple fluid phases exist in the pore space the image quality is much worse compared to the corresponding dry scans with the same number of projections and counts from a visual assessment generally the images reconstructed by the iterative algorithm have lower noise levels the difference in image quality between the two reconstruction algorithms is much more obvious when the image quality is low 3 1 1 quantitative analysis signal to noise ratio signal to noise ratio within an image is a key quantitative performance metric when determining image quality and the ease and accuracy for image segmentation in fig 3 the oil brine and grain phases were segmented into green oil blue brine and red grain the boundary of the oil phase is shown in black the segmented image is generated by applying a histogram based thresholding for each phase followed by applying an erosion filter the phase boundary sharpness is quantified in section 3 1 2 in this study we define the phase contrast as the phase signal level for the oil phase brine phase and the grain phase which are determined by the histogram plot of the grey scale values fig 4 shows the histogram for oil brine and grain grey scale values for each scan with different numbers of projections signal counts and the two reconstruction algorithms the image quality reflected by width of the histogram of the image reconstructed by the iterative algorithm is generally much better than for the one that uses the fdk algorithm when the counts and number of projections are the same although applying a non local means filter can significantly improve the image quality for the images with the fdk algorithm generally the image quality is still not as good as when using the iterative algorithm moreover the non local means filter does not have a strong impact when using the iterative reconstruction algorithm this indicates that this portion of the distribution in x ray grey scale values is not attributable to random noise but rather to point to point grey scale variation either due to mineralogical variation in the sample or other artefacts this image quality phenomenon with different scanning conditions and different reconstruction algorithms is generally applicable to almost all scans for the low quality images e g images with few projections and low counts after applying the non local means filter the shape of the histogram plot for the brine phase becomes skewed this is mainly due to the high noise level and poor phase contrast in the multiphase scans the signal to noise ratio snr is defined as follows 6 s n r μ o i l μ g r a i n 1 2 σ o i l σ g r a i n where μ oil is the mean grey scale value for the oil phase μ grain is the mean grey scale value of the grain phase σ oil and σ grain are the standard deviation values for the oil phase and the grain phase the computed signal to noise ratio values for all the multiphase scans are shown in table 3 the snr value from iterative algorithms without applying filters are much higher than those from fdk values and are comparable to those from fdk values with filters this indicates that the phase noise which also reflects the general image quality from the iterative algorithm is much lower than the noise from the fdk algorithm and is at similar level compared to the noise from the fdk algorithm with image filters 3 1 2 quantitative analysis phase boundary sharpness fig 2 shows slices from the highest high counts and many projections and the lowest low counts and few projections quality images using both fdk and iterative reconstruction algorithms the images after applying non local means edge preserving filters are also displayed for comparison to assess the phase boundary sharpness the grey scale values along a line selected between points a and b in fig 2 which passes the oil brine and the grain were plotted as shown in fig 5 from fig 5 it can be observed that when using the fdk algorithm if the image quality is not sufficient e g fig 5a the boundary between two phases is not clear generally applying filters can reduce the image noise and the phase boundary becomes visible however applying a filter can also decrease the boundary sharpness e g fig 5e and f to quantify the sharpness of the phase boundary which is indicated by the steepness of the slope a logistic type of distribution can be used this is a novel application of this function allowing for a quantitative estimation of the length scale over which a phase transition within an image takes place a general expression of a logistic distribution can be defined as 7 f x c l 1 e k x x 0 where l corresponds to the difference between two phases c is the grey scale value of the least attenuating phase x 0 is the value of the sigmoid s midpoint and k is a parameter determining the steepness of the slope the sharpness of the phase boundary increases when the value of k increases and allows for the spatial length scale for an interface transition to be quantitatively and objectively measured the graph shown in fig 5f is used to demonstrate how to compute k values for a multiphase scan by fitting portions of the plot to eq 6 the graph was divided into three sections including three sub profiles passing from the grain phase to the oil phase k1 the oil phase to the brine phase k2 and the brine phase to the grain phase k3 each profile was fitted by a logistic distribution eq 6 and the mean value of k 1 k 2 and k 3 was used to quantify the phase average boundary sharpness kmean shown in fig 6 the k values with different scanning and reconstruction conditions are shown in table 4 generally the images reconstructed by the iterative algorithm are characterized by a sharper phase boundary compared with the images reconstructed by the fdk algorithm although applying theoretically edge preserving filters can significantly reduce the image noise this generally has a negative effect on the sharpness of the phase boundary 3 2 optimization of fast scanning with the iterative algorithm the key parameters affecting the scanning time are the number of projections and the exposure time as source brightness for a given system is usually fixed faster imaging can be achieved by sacrificing one or both of these parameters this also has the effect of reducing image quality to obtain the same or better quality reconstructed images from the same input projection data a different reconstruction algorithm can be used the assessment for the multiphase section 3 1 images and dry scans see supporting information with different reconstruction algorithms shows that by using an iterative reconstruction algorithm image quality can be significantly improved as compared to the image with the same scanning settings which was reconstructed by the conventional fdk filtered back projection algorithm therefore iterative reconstruction has the potential to significantly reduce scanning time while maintaining image quality in this section the image reconstructed by the iterative algorithm with either few projections but high counts or many projections but low counts are compared with a reference scan reconstructed by the fdk algorithm with many projections and high counts both images reconstructed by the iterative algorithm were about 4 times faster to acquire than the reference scan fig 7 shows a comparison of the scans both dry and partially saturated scans reconstructed using the iterative technique the images showing a larger field of view can be found in fig s7 in the supporting information the reference scan many projections high counts reconstructed using the fdk algorithm is also shown it can be observed that by using the iterative reconstruction algorithm the image is similar to the reference scan after either significantly reducing the number of projections using 401 projections rather than 1601 or lowering the x ray counts using 2000 counts rather than 10 000 the grey scale histograms for both dry and multiphase scans show that the noise levels indicated by the snr values from the images using the iterative reconstruction algorithm are much lower than the reference scan reconstructed by the fdk algorithm with many projections and high counts fig 8 the noise levels of the images using the iterative algorithms reconstructed using either few projections but high counts and many projections but low counts are similar the detailed images showing the phase boundary are displayed in fig 7 and the line profile analysis with the corresponding k values for phase sharpness is shown in fig 9 it can be seen that the iterative reconstruction algorithm can significantly reduce the scanning time and the image quality can be maintained or even improved compared to a standard high quality scan the two scans with an intermediate quality of input data many projections low counts or few projections high counts represent somewhat of a sweet spot for image acquisition significant acquisition time decreases can be seen while still maintaining both subjective and objective image quality the comparison between these two cases where image quality is approximately the same when reconstructed using the iterative algorithm is interesting as the effective throughput of each technique is not the same table 2 overall scan time is not simply proportional to exposure time each projection requires the sample to be rotated translated and for data to be read off the x ray detector all this creates a projection to projection overhead which is still present even at a short exposure time scan time is however simply related to the number of projections if acquisition speed is the priority reducing the number of projections is the best strategy as this will have the biggest impact on resulting acquisition throughput 4 conclusions the imaging of dynamic processes using laboratory based x ray microtomography is one of the primary emerging areas of ongoing scientific interest in different disciplines for example material science and engineering and single multiphase flow and transport in porous media however its application is limited by the time required to acquire images in this paper we have shown how iterative reconstruction techniques can be used to achieve the same benchmark image quality as measured by quantitative image performance metrics of signal to noise and edge sharpness in one quarter of the time taken compared to traditional reconstruction methods this was the case both for scanning of just the rock structure and the pore space containing multiple fluid phases it should be noted that both of these cases have a sparse image structure with the reconstructed volume consisting mainly of piecewise constant regions we would expect to be able to achieve comparable reduction of acquisition time in other use cases with similarly sparse image structure but the exact amount of achievable reduction is highly application dependent in particular in use cases where preservation of fine detail is important the amount of achievable throughput increase may be less this quadrupling of acquisition throughput has significant implications for the processes accessible to laboratory examination in the academic sphere research in different areas such as processes reaction and fluid dynamics are available with a coupled temporal and spatial resolution never before accessible in the industrial field particularly that associated with digital rock analysis for the oil and gas industry these developments have the potential of greatly reducing the acquisition time per sample and so associated cost for the analysis in a price constrained environment this could be critical for the future adoption of this technology there are still many opportunities to improve iterative reconstruction both in terms of reconstruction quality with new regularization techniques giving the same quality image for noisier input data ease of use completely automating the reconstruction process and computational efficiency acknowledgements we acknowledge the engineering and physical science research council for financial support through grant ep l012227 1 the images acquired in this study can be downloaded from https doi org 10 6084 m9 figshare 5962834 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2018 03 007 appendix supplementary materials image application 1 
827,accurate characterization of subsurface hydraulic conductivity is vital for modeling of subsurface flow and transport the iterative ensemble smoother ies has been proposed to estimate the heterogeneous parameter field as a monte carlo based method ies requires a relatively large ensemble size to guarantee its performance to improve the computational efficiency we propose an adaptive gaussian process gp based iterative ensemble smoother gpies in this study at each iteration the gp surrogate is adaptively refined by adding a few new base points chosen from the updated parameter realizations then the sensitivity information between model parameters and measurements is calculated from a large number of realizations generated by the gp surrogate with virtually no computational cost since the original model evaluations are only required for base points whose number is much smaller than the ensemble size the computational cost is significantly reduced the applicability of gpies in estimating heterogeneous conductivity is evaluated by the saturated and unsaturated flow problems respectively without sacrificing estimation accuracy gpies achieves about an order of magnitude of speed up compared with the standard ies although subsurface flow problems are considered in this study the proposed method can be equally applied to other hydrological models keywords data assimilation gaussian process model ensemble smoother adaptive method parameter estimation 1 introduction subsurface geological formation is inherently heterogeneous at different scales accurate characterization of formation properties is vital for modeling of subsurface flow and transport chen and zeng 2015 ju et al 2018 nowadays the ensemble smoother es van leeuwen and evensen 1996 and the ensemble kalman filter enkf evensen 2003 are popular data assimilation methods to estimate unknown parameters from state variables carrera et al 2005 oliver and chen 2011 zhou et al 2014 compared to enkf es has the advantage of avoiding the inconsistency between updated parameters and states by converting the parameter state estimation problem to a parameter estimation problem es has been used in hydrologic inverse problems for example bailey and baù 2012 employed es to provide an enhanced estimate of conductivity within a catchment system using a surface subsurface flow model crestani et al 2013 compared the performances of es and enkf in retrieving the hydraulic conductivities by assimilating tracer test data since es is based on the linear estimation theory and gaussian assumption it is only optimal for linear gaussian problems however in subsurface flow problems the governing equations are nonlinear and the observation operator may also be nonlinear this may lead to non gaussian distributed states and observations even though the input parameters are gaussian to improve the performance in strongly nonlinear problems iterations have been introduced to the es algorithm chen and oliver 2012 developed an iterative ensemble smoother ies also called batch enrml in which the updating step length is adaptively adjusted during iterations in the ies developed by emerick and reynolds 2013 the same data are assimilated multiple times by inflating the covariance of observation error it has been shown that ies performs better than the standard es and enkf chen and oliver 2013 emerick and reynolds 2013 as a monte carlo method ies requires a sufficiently large ensemble size to guarantee reliable estimations the iteration process further increases the computational cost one approach to alleviate the computational burden is to employ surrogate models which are constructed to approximate the outputs of a complex model at low computational cost asher et al 2015 it has been shown that surrogate based approaches show promising computational efficiency under certain conditions different surrogate based enkf and es have been developed for data assimilation the polynomial chaos expansion pce has been combined with enkf to estimate the permeability field in subsurface flow problems saad and ghanem 2009 zeng et al 2011 chang et al 2017 implemented an ies using both the pce and the interpolation based surrogates in subsurface flow problems as another data driven surrogate modeling technique gaussian process gp is gaining popularity in hydrologic modeling asher et al 2015 marrel et al 2008 showed that the gp model had some advantages over other metamodels in predicting the radionuclide transport in groundwater sun et al 2014 applied the gp model to forecast the monthly streamflow and found it outperformed both the linear regression and artificial neural network models in most test cases a distinct advantage of the gp surrogate is that it can be refined conveniently through appropriately adding more base points chosen from parameter space then the system outputs with arbitrary input parameters can be obtained via statistical conditioning williams and rasmussen 2006 from a bayesian perspective as the posterior distribution usually occupies only a small proportion of the prior distribution constructing a locally accurate gp surrogate over the posterior distribution is a computationally inexpensive alternative to a globally accurate surrogate for high dimensional nonlinear inverse problems in our previous work zhang et al 2016 integrated gp and markov chain monte carlo mcmc to adaptively construct a locally accurate surrogate for bayesian experimental design in groundwater contaminant source identification problems although the gp based mcmc achieved a significant speed up it still required a large number of model evaluations to obtain reliable posterior statistics if the assumption of linearity and gaussianity is not strongly violated ensemble based kalman filter enkf and its variants are generally more computationally efficient than mcmc and have gained widespread popularity in the field of data assimilation oliver and chen 2011 zhou et al 2014 in this study inspired by the recent progresses of data assimilation and the advantages of gp we develop a new gp based iterative ensemble smoother gpies for data assimilation specifically efforts are made in exploring the best practices i e how to wisely distribute the computational resources when constructing the gp model in contrast these aspects were not discussed in detail in our previous work zhang et al 2016 the rest of the paper is organized as follows the methodology is presented in section 2 the efficiency and accuracy of the proposed algorithm are illustrated with two synthetic case studies in section 3 finally some conclusions are provided in section 4 2 methodology 2 1 iterative ensemble smoother in this section ies is briefly introduced it is assumed that the relationship between the measurements and model parameters can be written in the following form 1 d o b s f m ɛ where d obs is the measurement vector f is the forward model m is the parameter vector ε is a vector containing normally distributed measurement errors with zero mean and covariance c d e εε t then ies can be implemented in the following way chen and oliver 2012 step 1 generate ne parameter realizations from the prior distribution as the initial parameter ensemble 2 m 0 m 1 0 m n e 0 where the subscript denotes the ensemble member index and the superscript denotes the iteration index step 2 at the l th iteration step the parameter ensemble m l is updated with the entire historical measurements using the following formula 3 m l 1 β l m 0 1 β l m l β l c m g l t c d g l c m g l t 1 f m l d o b s g l m l m 0 where β l is a step length parameter varying in 0 1 if β l equals 1 a full step length is taken for the iteration otherwise a damped correction is made at the iteration c m denotes the covariance of parameters before the data assimilation and remains unchanged with iterations c m δm 0 δm 0 t ne 1 δm 0 denotes the matrix of deviations of m 0 to its means g l is the ensemble averaged sensitivity matrix linking the changes in the model parameters to the changes in the model outputs to obtain g l the singular value decomposition is employed to solve the following equation 4 δ d l g l δ m l where δd and δm represent the decentralized model outputs and model parameters at the l th iteration respectively to adjust the value of β l at each iteration a data mismatch term is defined as below 5 s m l j 1 n e f m j l d o b s j t c d 1 f m j l d o b s j if s m l 1 s m l overwrite m l with m l 1 and increase β l otherwise keep m l and decrease β l step 3 repeat step 2 until one of the convergence criteria defined below is satisfied 1 m a x 1 i n e m i l 1 m i l α 1 2 s m l 1 s m l α2 s m l 3 the iteration index exceeds the allowed maximum number i max here α1 and α2 denote the preset ratios one may refer to chen and oliver 2012 for details 2 2 gaussian process surrogate for the given model f m we seek to find a gaussian stochastic process g m to approximate the relationship between the model parameters and the model outputs williams and rasmussen 2006 g m can be specified by a mean function μ m and a covariance kernel function k m m 6 g m n μ m k m m suppose we have the outputs of f m evaluated at nb parameter base points b m b 1 m b n b g m is then obtained through interpolating at these points via statistical conditioning the conditional process denoted as g b m is still a gaussian process and its mean and variance at an arbitrary parameter point m are given as 7 μ b m μ m k m b k b b 1 f b μ b and 8 σ b 2 m k m m k m b k b b 1 k b m the conditional process can be used to approximate the original model f m with eqs 7 and 8 giving the approximation of model outputs and corresponding approximation errors respectively the conditional variance σ b 2 m is a measure of the approximation error which can be simply diminished through adding more base points in the local region the forms of mean and covariance functions in eq 6 can also influence the performance of the gp surrogate in this study the sum of a constant and linear mean functions shown in eq 9 and the square exponential kernel shown in eq 10 are employed as the mean and covariance functions respectively 9 μ m a b i i 1 n m m i 10 k m m σ f 2 exp 1 2 i 1 n m m i m i λ i 2 where mi and m i are the ith elements of m and m respectively nm is the number of elements in m and m a bi σ f λ i are the hyper parameters of the mean and covariance functions 2 3 implementing ies with an adaptive gp surrogate as demonstrated in our previous study zhang et al 2016 a locally accurate gp surrogate can be adaptively constructed by adding more parameter base points close to the posterior confidence intervals this will significantly reduce the computational burden compared to constructing a surrogate with a global accuracy over the prior parameter distribution based on this idea we propose an adaptive approach integrating the gp surrogate construction and the ies method at each iteration of ies new base points chosen from the updated ensemble realizations m l 1 are added to the existing pool of base points to refine the gp surrogate the iteration and refinement procedure is repeated until the preset stop criterion of ies is satisfied the flowchart of the adaptive gpies algorithm is illustrated in fig 1 and described as follows step 1 draw ne parameter realizations from the prior distribution to generate the prior ensemble m 0 then randomly choose nini nini ne base points from the prior parameter ensemble evaluate the original model f at these points and build the initial gp surrogate by conditioning to these base points step 2 generate the system outputs for the parameter realizations with the gp surrogate i e using eqs 7 and 8 update the parameter ensemble with the ies formula i e eq 3 specifically a new error term ε consisting of both the surrogate model error ε gp and measurement error ε i e ε ε ε gp is used in eq 1 to avoid an over confident estimation result zhang et al 2016 step 3 randomly choose a small number nadd of realizations from the updated ensemble m l 1 evaluate the original model f at these points and add them into the set of base points construct a refined gp surrogate by conditioning to the new set of base points step 4 repeat steps 2 3 until one of the stop criteria of ies is met in this study the total number of original model simulations nori is taken as an indicator of the computational cost since the cpu time for running the gp surrogate is negligible compared to that for running the original model in gpies the original model simulations are only needed in constructing the gp surrogate i e generating the system outputs with base points of parameters suppose the number of iterations in ies and gpies is iter and iterg respectively the nori value for the standard ies is ne iter 1 while it is nini nadd iterg for gpies usually the number of initial base points nini and the number of adaptively added base points nadd are much smaller than the ensemble size ne on the other hand due to the approximation error of the gp surrogate the required number of iterations of gpies iterg is slightly larger than that of ies iter on the whole the computational cost of gpies is significantly reduced compared to that of ies this will be demonstrated in the following case studies 3 case studies in this section the performance of gpies in estimating heterogeneous conductivities is evaluated by two synthetic case studies 3 1 case 1 saturated flow problem we first consider the two dimensional 2 d transient groundwater flow which satisfies the following equation 11 s s h t x k s x h x z k s z h z q x z where ss is the specific storage l 1 h is the hydraulic head l t is the time t ksx and ksz are the saturated hydraulic conductivities in the horizontal and vertical direction respectively lt 1 q x z is the pumping rate at the location x z l 3 t 1 the finite element solver comsol multiphysics is employed to solve the governing equation the model domain is 2 l 1 l fig 2 and evenly discretized into 5000 100 50 elements the top boundary is set as the dirichlet boundary with a constant head of 1 l while no flow boundaries are prescribed at the other boundaries the initial hydraulic heads are uniformly set at a level of 1 l the specific storage ss is assumed to be homogeneous with a value of 1 10 4 l 1 all the five pumping wells denoted as filled circles in fig 2 are with constant pumping rates of 1 l 3 t 1 the total simulation period is chosen as 0 02 t the transient hydraulic head measurements are obtained every 0 004 t at the 50 observation locations as denoted by the hollow circles in fig 2 to represent the spatial heterogeneity the ln ks field is modeled as a gaussian random field with the following mean and exponential covariance functions 12 ln k s 2 ln l t 1 13 c ln k s d σ ln k s 2 exp d a where denotes the mean operator σ ln k s 2 1 is the variance d represents the distance between points x 1 z 1 and x 2 z 2 a is the range parameter d a x 1 x 2 2 λ x 2 z 1 z 2 2 λ z 2 λ x lx 0 25 λ z lz 0 35 λ x z and l x z are the correlation length and domain size in the corresponding direction respectively the reference ln ks field and the initial ensemble realizations for ies are randomly generated using the sequential gaussian simulation sgsim tool in the geostatistical simulation library gslib deutsch and journel 1995 to improve the computational efficiency of gpies the karhunen loève kl expansion is used to parameterize the ln ks field zeng and zhang 2010 zhang and lu 2004 zhang et al 2007 with the given statistical characteristics 84 of the total variance of the gaussian random field can be captured by using the first 30 terms thus ln ks is parameterized as 14 ln k s x z ln k s i 1 n k l ξ i τ i f i x z where ln ks is the mean component ξ i are uncorrelated standard gaussian random variables τ i and fi x z are the i th eigenvalue and i th eigenfunction of the covariance function respectively and nkl 30 is the truncated number of kl terms in this way the parameter dimensionality is reduced from the total number of elements 5000 to the truncated number of kl terms 30 i e the random input parameters are m ξ1 ξ2 ξ30 the initial ensemble realizations for gpies are then randomly generated through the kl expansion the head measurements are obtained through running the simulator with the reference field under the prescribed initial and boundary conditions the measurement errors are assumed to follow independent identical gaussian distributions with zero mean and standard deviation of 1 10 3 l the noisy hydraulic head measurements are then used to estimate the ln ks field through both the standard and gp based ies to get reliable estimation results we set α1 1 10 4 α2 1 10 3 and i max 50 as the stop criteria for both ies and gpies in this study to quantitatively compare the estimation accuracy two performance indicators are defined as follows 15 r m s e m i 1 n k i r k i 2 n 16 r m s e h j 1 n o b s p h j r h j 2 n o b s where rmsem and rmseh are the root mean square errors of the estimated parameters k and the predicted hydraulic pressure heads p h respectively r k denotes the true values of parameters on all elements and r h represents head measurements n is the number of elements and i is the corresponding index nobs is the number of measurements with j denoting its index it should be pointed out that since the true parameters are unknown in practical applications one popular performance evaluation is to examine the match between the predicted and actual data as shown in eq 16 if the parameters are better estimated smaller performance indicator values are expected to be obtained since ies is a monto carlo based method its performance is affected by the specific initial ensemble especially when the ensemble size is small to examine the effects of ensemble size on estimation accuracy different ensemble sizes i e ne 50 100 200 300 600 are tested to give a fair comparison we repeat the ies implementation with 10 different initial ensemble sets for each tested ensemble size the rmsem values for estimated parameters as a function of ensemble size are ploted in fig 3 a overall the rmsem values decrease to a small and stable level as the ensemble size increases furthermore the mean number of iterations needed also decreases as the ensemble size increases fig 3 b this is because a larger ensemble size guarantees more accurate sensitivity information resulting in a convergent result with fewer iterations overall ies performs well when the ensemble size increases to 300 thus in this case the ensemble size of 300 could achieve a good balance between the accuracy and computational cost in the following calculations ne 300 is also set as the ensemble size for gpies some discussions about the choice of the optimal ensemble size and techniques used to reduce the ensemble size e g localization in ensemble based methods can be found in rasmussen et al 2015 sun et al 2009 yin et al 2015 increasing the number of base points can improve the performance of gpies however at increased computational cost to illustrate the superiority of the adaptive gp refinement two forms adaptive and non adaptive of gpies are repeatedly implemented with 10 different initial ensembles we first implement gpies in an adaptive way with 10 initial base points and 1 more base point added at each iteration i e nini 10 and nadd 1 while in the non adaptive gpies 40 initial base points are used and the adaptive gp refinement is not implemented i e nini 40 and nadd 0 in the adaptive gpies a surrogate with better local accuracy on the posterior parameter distribution is constructed since the updated ensemble members are successively added as base points while in the non adaptive gpies a surrogate with better global accuracy on the prior parameter distribution is constructed by using more initial base points since the number of iterations may vary in different runs of gpies the average nori value is used to evaluate the computational cost fig 4 shows that with the same computational cost the adaptive gpies with smaller rmsem and rmseh values significantly outperforms the non adaptive gpies this is because the posterior distribution usually occupies only a small proportion of the prior distribution and thus constructing a locally accurate surrogate over the posterior distribution could be more computationally efficient to test the effects of the number of added base points at each iteration different nadd values i e 1 5 10 20 are used while the same 10 base points nini 10 are used to construct the initial gp surrogate we also implement the adaptive gpies with 10 different initial ensemble sets for each tested nadd value with different nadd values performance indicator values for estimated parameters and predicted states are shown in fig 5 it is seen that with increasing nadd value the estimation accuracy improves at increased computational cost this occurs because increasing the nadd value leads to a more accurate gp surrogate and thus to more accurate sensitivity information between model parameters and outputs when nadd increases beyond 10 the estimation accuracy of gpies is comparable to that of the standard ies in terms of both parameter estimation and state prediction it is also noted that with ne 300 nini 10 and nadd 10 gpies needs an average number of 9 iterations to reach convergence in contrast the average number of required iterations for ies with ne 300 is 7 fig 3 b thus more iterations are needed in gpies due to its approximation error overall since the original governing equation is only required to be solved for base points the computational cost of gpies is an order of magnitude lower than that of ies considering both the accuracy and computational cost nadd is chosen as 10 in this study fig 6 illustrates the accuracy of the adaptively refined gp surrogate during the iterations it can be seen that as the iteration continues the errors between the original model outputs and the surrogate outputs keep decreasing and the determination coefficient r 2 increases which indicates the gradually improved accuracy of the surrogate a plot of rmseh values against the iteration number is illustrated in fig 6 d obviously the rmseh decreases to almost 0 as the iteration continues confirming a continuous refinement of the surrogate model as shown in fig 7 the estimated mean ln ks fields given by ies ne 300 and gpies ne 300 nini 10 nadd 10 can both capture the main patterns of the reference field it is also noted that the ln ks field estimated by gpies is much smoother due to the utilization of dimension reduction the kl expansion to further evaluate the accuracy we examine the match between the hydraulic head measurements at all observation locations and the corresponding predicted values with the final updated parameter ensembles of ies and gpies results show that the predicted values by the two algorithms both well match the observations indicating a high estimation accuracy and three sets of representative results at selected observation locations are illustrated in fig 8 3 2 case 2 unsaturated flow problem to further demonstrate the performance of gpies we apply it to a more nonlinear problem which considers 2 d water flow in variably saturated soil the flow process can be described by the richards equation 17 θ ψ t x k x ψ ψ x z k z ψ ψ z 1 where θ ψ is the volumetric water content l 3 l 3 kx ψ and kz ψ are the unsaturated hydraulic conductivities in the horizontal and vertical direction lt 1 respectively the highly nonlinear relationships between the soil hydraulic parameters and the pressure head ψ can be described by the van genuchten mualem model vgm genuchten 1980 18 θ ψ θ r θ s θ r 1 α ψ n m ψ 0 θ s ψ 0 19 k ψ k s s e 0 5 1 1 s e 1 m m 2 where se θ θ r θ s θ r is the effective water content θ s and θ r are the saturated and residual water contents respectively ks is the saturated hydraulic conductivity lt 1 m 1 1 n n 1 α and n are two empirical coefficients related to the soil pore size distribution the study domain shown in fig 9 is a square with a dimension of 2 l 2 l which is evenly discretized into 961 31 31 elements the upper boundary is exposed to the atmosphere with a precipitation of 0 02 lt 1 and the lower boundary is set as free drainage impervious boundaries are prescribed for the two lateral sides initially the pressure head ranges linearly from 2 l near the upper boundary to 0 near the lower boundary in this case the heterogeneous saturated hydraulic conductivity field ks x z is assumed to be estimated while other vgm parameters are given as constants i e θ r 1 10 4 l 3 l 3 θ s 0 339 l 3 l 3 α 1 74 l 1 and n 1 3757 the finite element software hydrus šimůnek et al 2006 is employed to simulate the unsaturated flow the total simulation time is set as 20 t and pressure head data are monitored at the observation locations denoted as round circles in fig 9 at an interval of 1 t the log transformed saturated conductivity field ln ks x z is modeled as a gaussian random field with the following mean and spherical covariance function 20 ln k s 2 ln l t 1 21 c ln k s d σ ln k s 2 1 1 5 d a 0 5 d a 3 d a 0 d a in this case σ ln k s 2 is still chosen as 1 and correlation scales are λ x lx 0 25 λ z lz 0 25 the reference ln ks field and the initial ensemble realizations for ies are also randomly generated using the sgsim tool the pressure head observations are then obtained through running the hydrus solver with the reference field and the measurement errors are assumed to follow independent identical gaussian distributions with zero mean and standard deviation of 1 10 2 l the kl expansion is also employed to parameterize the ln ks fields in gpies and 82 of the total variance of the gaussian random field can be captured by increasing the number of truncated kl terms to nkl 50 as the dimensionality and nonlinearity are significantly increased compared to case 1 more base points are required to guarantee the accuracy of the initial gp surrogate we set nini 100 while other recommended settings are still used i e ne 300 nadd 10 with the observations the ln ks fields are estimated using both ies ne 300 and gpies ne 300 nini 100 nadd 10 the simulations are also repeatedly implemented 10 times with different initial ensemble realizations the average number of iterations required to reach convergence is 9 and 20 for ies and gpies respectively overall the final mean estimations of the two algorithms shown in fig 10 can still capture the main patterns of the reference field fig 11 shows the match between the pressure head observations at three representative observation locations and corresponding predicted values with the final ensembles of gpies and ies it can be seen that the predicted values given by gpies better match the true observations furthermore the confidence intervals given by gpies are much narrower indicating the smaller uncertainty since the observations are all within the confidence intervals the estimated uncertainty given by gpies is reasonable in this case the performance indicators for the two algorithms are compared in fig 12 overall gpies gives comparable parameter estimations and much better data matches compared to ies in terms of computational efficiency however gpies only needs about 1 10 of the computational cost required by ies thus gpies is still superior to ies in this case to further assess the performance of gpies in the presence of smaller correlation scales we reduce both λ x lx and λ z lz to 0 15 the reference field and initial ensemble realizations for ies are also generated using the sgsim tool for the kl expansion in gpies 100 leading terms are required to retain 80 of the total variance of the random field accordingly we increase nini to 200 to construct the initial gp surrogate due to the much higher dimensionality while other settings remain unchanged fig 13 shows that the final mean estimations of ies and gpies can still capture the main patterns of the reference field with the decreased correlation scales the performance indicators for the estimated parameter field and predicted pressure heads from the two algorithms are compared in fig 14 overall gpies still achieves a satisfactory estimation accuracy and about an order of magnitude of speed up compared with ies it is also noted that more iterations are required for both ies 12 and gpies 27 to reach convergence due to the decreased correlation scales 4 conclusions in this study a gaussian process based iterative ensemble smoother gpies has been proposed for data assimilation the key idea is to adaptively refine the gp surrogate using the updated parameters conditional to the available measurements at each iteration then the sensitivity information between model parameters and measurements is calculated from a large number of realizations generated by the gp surrogate with virtually no computational cost to further improve the computational efficiency the karhunen loève kl expansion is employed as a dimension reduction tool to parameterize the heterogeneous parameter field thus the gp surrogate is constructed in a much reduced parameter dimension during the entire gpies simulation the original model evaluations are only needed at the base points whose number is much smaller than the ensemble size thus the computational cost can be significantly reduced two synthetic case studies considering saturated and unsaturated flow problems are tested to validate the gpies algorithm respectively overall gpies achieves at least an order of magnitude of speed up compared with the standard ies without sacrificing the estimation accuracy the strategy in constructing the gp surrogate is also investigated it has been shown that adaptively constructing a locally accurate surrogate i e successively adding the updated ensemble realizations as new base points significantly improves the efficiency compared to the non adaptive approach it should be pointed out that gpies will lose its efficiency if the random dimensionality is extremely large i e nkl 100 for very small correlation scales in contrast the implementation of ies is relatively independent to the random dimensionality furthermore to improve the performance in estimating the non gaussian parameters some other techniques i e the level set parameterization chang et al 2010 and normal score transformation zhou et al 2011 can be combined with gpies in future studies acknowledgments computer codes and data used are available upon request to the corresponding author the authors would like to thank the three anonymous reviewers and the editors for their constructive comments on an earlier version which led to a much improved paper this work is supported by the national natural science foundation of china grants 41771254 41571450 and 41571215 
827,accurate characterization of subsurface hydraulic conductivity is vital for modeling of subsurface flow and transport the iterative ensemble smoother ies has been proposed to estimate the heterogeneous parameter field as a monte carlo based method ies requires a relatively large ensemble size to guarantee its performance to improve the computational efficiency we propose an adaptive gaussian process gp based iterative ensemble smoother gpies in this study at each iteration the gp surrogate is adaptively refined by adding a few new base points chosen from the updated parameter realizations then the sensitivity information between model parameters and measurements is calculated from a large number of realizations generated by the gp surrogate with virtually no computational cost since the original model evaluations are only required for base points whose number is much smaller than the ensemble size the computational cost is significantly reduced the applicability of gpies in estimating heterogeneous conductivity is evaluated by the saturated and unsaturated flow problems respectively without sacrificing estimation accuracy gpies achieves about an order of magnitude of speed up compared with the standard ies although subsurface flow problems are considered in this study the proposed method can be equally applied to other hydrological models keywords data assimilation gaussian process model ensemble smoother adaptive method parameter estimation 1 introduction subsurface geological formation is inherently heterogeneous at different scales accurate characterization of formation properties is vital for modeling of subsurface flow and transport chen and zeng 2015 ju et al 2018 nowadays the ensemble smoother es van leeuwen and evensen 1996 and the ensemble kalman filter enkf evensen 2003 are popular data assimilation methods to estimate unknown parameters from state variables carrera et al 2005 oliver and chen 2011 zhou et al 2014 compared to enkf es has the advantage of avoiding the inconsistency between updated parameters and states by converting the parameter state estimation problem to a parameter estimation problem es has been used in hydrologic inverse problems for example bailey and baù 2012 employed es to provide an enhanced estimate of conductivity within a catchment system using a surface subsurface flow model crestani et al 2013 compared the performances of es and enkf in retrieving the hydraulic conductivities by assimilating tracer test data since es is based on the linear estimation theory and gaussian assumption it is only optimal for linear gaussian problems however in subsurface flow problems the governing equations are nonlinear and the observation operator may also be nonlinear this may lead to non gaussian distributed states and observations even though the input parameters are gaussian to improve the performance in strongly nonlinear problems iterations have been introduced to the es algorithm chen and oliver 2012 developed an iterative ensemble smoother ies also called batch enrml in which the updating step length is adaptively adjusted during iterations in the ies developed by emerick and reynolds 2013 the same data are assimilated multiple times by inflating the covariance of observation error it has been shown that ies performs better than the standard es and enkf chen and oliver 2013 emerick and reynolds 2013 as a monte carlo method ies requires a sufficiently large ensemble size to guarantee reliable estimations the iteration process further increases the computational cost one approach to alleviate the computational burden is to employ surrogate models which are constructed to approximate the outputs of a complex model at low computational cost asher et al 2015 it has been shown that surrogate based approaches show promising computational efficiency under certain conditions different surrogate based enkf and es have been developed for data assimilation the polynomial chaos expansion pce has been combined with enkf to estimate the permeability field in subsurface flow problems saad and ghanem 2009 zeng et al 2011 chang et al 2017 implemented an ies using both the pce and the interpolation based surrogates in subsurface flow problems as another data driven surrogate modeling technique gaussian process gp is gaining popularity in hydrologic modeling asher et al 2015 marrel et al 2008 showed that the gp model had some advantages over other metamodels in predicting the radionuclide transport in groundwater sun et al 2014 applied the gp model to forecast the monthly streamflow and found it outperformed both the linear regression and artificial neural network models in most test cases a distinct advantage of the gp surrogate is that it can be refined conveniently through appropriately adding more base points chosen from parameter space then the system outputs with arbitrary input parameters can be obtained via statistical conditioning williams and rasmussen 2006 from a bayesian perspective as the posterior distribution usually occupies only a small proportion of the prior distribution constructing a locally accurate gp surrogate over the posterior distribution is a computationally inexpensive alternative to a globally accurate surrogate for high dimensional nonlinear inverse problems in our previous work zhang et al 2016 integrated gp and markov chain monte carlo mcmc to adaptively construct a locally accurate surrogate for bayesian experimental design in groundwater contaminant source identification problems although the gp based mcmc achieved a significant speed up it still required a large number of model evaluations to obtain reliable posterior statistics if the assumption of linearity and gaussianity is not strongly violated ensemble based kalman filter enkf and its variants are generally more computationally efficient than mcmc and have gained widespread popularity in the field of data assimilation oliver and chen 2011 zhou et al 2014 in this study inspired by the recent progresses of data assimilation and the advantages of gp we develop a new gp based iterative ensemble smoother gpies for data assimilation specifically efforts are made in exploring the best practices i e how to wisely distribute the computational resources when constructing the gp model in contrast these aspects were not discussed in detail in our previous work zhang et al 2016 the rest of the paper is organized as follows the methodology is presented in section 2 the efficiency and accuracy of the proposed algorithm are illustrated with two synthetic case studies in section 3 finally some conclusions are provided in section 4 2 methodology 2 1 iterative ensemble smoother in this section ies is briefly introduced it is assumed that the relationship between the measurements and model parameters can be written in the following form 1 d o b s f m ɛ where d obs is the measurement vector f is the forward model m is the parameter vector ε is a vector containing normally distributed measurement errors with zero mean and covariance c d e εε t then ies can be implemented in the following way chen and oliver 2012 step 1 generate ne parameter realizations from the prior distribution as the initial parameter ensemble 2 m 0 m 1 0 m n e 0 where the subscript denotes the ensemble member index and the superscript denotes the iteration index step 2 at the l th iteration step the parameter ensemble m l is updated with the entire historical measurements using the following formula 3 m l 1 β l m 0 1 β l m l β l c m g l t c d g l c m g l t 1 f m l d o b s g l m l m 0 where β l is a step length parameter varying in 0 1 if β l equals 1 a full step length is taken for the iteration otherwise a damped correction is made at the iteration c m denotes the covariance of parameters before the data assimilation and remains unchanged with iterations c m δm 0 δm 0 t ne 1 δm 0 denotes the matrix of deviations of m 0 to its means g l is the ensemble averaged sensitivity matrix linking the changes in the model parameters to the changes in the model outputs to obtain g l the singular value decomposition is employed to solve the following equation 4 δ d l g l δ m l where δd and δm represent the decentralized model outputs and model parameters at the l th iteration respectively to adjust the value of β l at each iteration a data mismatch term is defined as below 5 s m l j 1 n e f m j l d o b s j t c d 1 f m j l d o b s j if s m l 1 s m l overwrite m l with m l 1 and increase β l otherwise keep m l and decrease β l step 3 repeat step 2 until one of the convergence criteria defined below is satisfied 1 m a x 1 i n e m i l 1 m i l α 1 2 s m l 1 s m l α2 s m l 3 the iteration index exceeds the allowed maximum number i max here α1 and α2 denote the preset ratios one may refer to chen and oliver 2012 for details 2 2 gaussian process surrogate for the given model f m we seek to find a gaussian stochastic process g m to approximate the relationship between the model parameters and the model outputs williams and rasmussen 2006 g m can be specified by a mean function μ m and a covariance kernel function k m m 6 g m n μ m k m m suppose we have the outputs of f m evaluated at nb parameter base points b m b 1 m b n b g m is then obtained through interpolating at these points via statistical conditioning the conditional process denoted as g b m is still a gaussian process and its mean and variance at an arbitrary parameter point m are given as 7 μ b m μ m k m b k b b 1 f b μ b and 8 σ b 2 m k m m k m b k b b 1 k b m the conditional process can be used to approximate the original model f m with eqs 7 and 8 giving the approximation of model outputs and corresponding approximation errors respectively the conditional variance σ b 2 m is a measure of the approximation error which can be simply diminished through adding more base points in the local region the forms of mean and covariance functions in eq 6 can also influence the performance of the gp surrogate in this study the sum of a constant and linear mean functions shown in eq 9 and the square exponential kernel shown in eq 10 are employed as the mean and covariance functions respectively 9 μ m a b i i 1 n m m i 10 k m m σ f 2 exp 1 2 i 1 n m m i m i λ i 2 where mi and m i are the ith elements of m and m respectively nm is the number of elements in m and m a bi σ f λ i are the hyper parameters of the mean and covariance functions 2 3 implementing ies with an adaptive gp surrogate as demonstrated in our previous study zhang et al 2016 a locally accurate gp surrogate can be adaptively constructed by adding more parameter base points close to the posterior confidence intervals this will significantly reduce the computational burden compared to constructing a surrogate with a global accuracy over the prior parameter distribution based on this idea we propose an adaptive approach integrating the gp surrogate construction and the ies method at each iteration of ies new base points chosen from the updated ensemble realizations m l 1 are added to the existing pool of base points to refine the gp surrogate the iteration and refinement procedure is repeated until the preset stop criterion of ies is satisfied the flowchart of the adaptive gpies algorithm is illustrated in fig 1 and described as follows step 1 draw ne parameter realizations from the prior distribution to generate the prior ensemble m 0 then randomly choose nini nini ne base points from the prior parameter ensemble evaluate the original model f at these points and build the initial gp surrogate by conditioning to these base points step 2 generate the system outputs for the parameter realizations with the gp surrogate i e using eqs 7 and 8 update the parameter ensemble with the ies formula i e eq 3 specifically a new error term ε consisting of both the surrogate model error ε gp and measurement error ε i e ε ε ε gp is used in eq 1 to avoid an over confident estimation result zhang et al 2016 step 3 randomly choose a small number nadd of realizations from the updated ensemble m l 1 evaluate the original model f at these points and add them into the set of base points construct a refined gp surrogate by conditioning to the new set of base points step 4 repeat steps 2 3 until one of the stop criteria of ies is met in this study the total number of original model simulations nori is taken as an indicator of the computational cost since the cpu time for running the gp surrogate is negligible compared to that for running the original model in gpies the original model simulations are only needed in constructing the gp surrogate i e generating the system outputs with base points of parameters suppose the number of iterations in ies and gpies is iter and iterg respectively the nori value for the standard ies is ne iter 1 while it is nini nadd iterg for gpies usually the number of initial base points nini and the number of adaptively added base points nadd are much smaller than the ensemble size ne on the other hand due to the approximation error of the gp surrogate the required number of iterations of gpies iterg is slightly larger than that of ies iter on the whole the computational cost of gpies is significantly reduced compared to that of ies this will be demonstrated in the following case studies 3 case studies in this section the performance of gpies in estimating heterogeneous conductivities is evaluated by two synthetic case studies 3 1 case 1 saturated flow problem we first consider the two dimensional 2 d transient groundwater flow which satisfies the following equation 11 s s h t x k s x h x z k s z h z q x z where ss is the specific storage l 1 h is the hydraulic head l t is the time t ksx and ksz are the saturated hydraulic conductivities in the horizontal and vertical direction respectively lt 1 q x z is the pumping rate at the location x z l 3 t 1 the finite element solver comsol multiphysics is employed to solve the governing equation the model domain is 2 l 1 l fig 2 and evenly discretized into 5000 100 50 elements the top boundary is set as the dirichlet boundary with a constant head of 1 l while no flow boundaries are prescribed at the other boundaries the initial hydraulic heads are uniformly set at a level of 1 l the specific storage ss is assumed to be homogeneous with a value of 1 10 4 l 1 all the five pumping wells denoted as filled circles in fig 2 are with constant pumping rates of 1 l 3 t 1 the total simulation period is chosen as 0 02 t the transient hydraulic head measurements are obtained every 0 004 t at the 50 observation locations as denoted by the hollow circles in fig 2 to represent the spatial heterogeneity the ln ks field is modeled as a gaussian random field with the following mean and exponential covariance functions 12 ln k s 2 ln l t 1 13 c ln k s d σ ln k s 2 exp d a where denotes the mean operator σ ln k s 2 1 is the variance d represents the distance between points x 1 z 1 and x 2 z 2 a is the range parameter d a x 1 x 2 2 λ x 2 z 1 z 2 2 λ z 2 λ x lx 0 25 λ z lz 0 35 λ x z and l x z are the correlation length and domain size in the corresponding direction respectively the reference ln ks field and the initial ensemble realizations for ies are randomly generated using the sequential gaussian simulation sgsim tool in the geostatistical simulation library gslib deutsch and journel 1995 to improve the computational efficiency of gpies the karhunen loève kl expansion is used to parameterize the ln ks field zeng and zhang 2010 zhang and lu 2004 zhang et al 2007 with the given statistical characteristics 84 of the total variance of the gaussian random field can be captured by using the first 30 terms thus ln ks is parameterized as 14 ln k s x z ln k s i 1 n k l ξ i τ i f i x z where ln ks is the mean component ξ i are uncorrelated standard gaussian random variables τ i and fi x z are the i th eigenvalue and i th eigenfunction of the covariance function respectively and nkl 30 is the truncated number of kl terms in this way the parameter dimensionality is reduced from the total number of elements 5000 to the truncated number of kl terms 30 i e the random input parameters are m ξ1 ξ2 ξ30 the initial ensemble realizations for gpies are then randomly generated through the kl expansion the head measurements are obtained through running the simulator with the reference field under the prescribed initial and boundary conditions the measurement errors are assumed to follow independent identical gaussian distributions with zero mean and standard deviation of 1 10 3 l the noisy hydraulic head measurements are then used to estimate the ln ks field through both the standard and gp based ies to get reliable estimation results we set α1 1 10 4 α2 1 10 3 and i max 50 as the stop criteria for both ies and gpies in this study to quantitatively compare the estimation accuracy two performance indicators are defined as follows 15 r m s e m i 1 n k i r k i 2 n 16 r m s e h j 1 n o b s p h j r h j 2 n o b s where rmsem and rmseh are the root mean square errors of the estimated parameters k and the predicted hydraulic pressure heads p h respectively r k denotes the true values of parameters on all elements and r h represents head measurements n is the number of elements and i is the corresponding index nobs is the number of measurements with j denoting its index it should be pointed out that since the true parameters are unknown in practical applications one popular performance evaluation is to examine the match between the predicted and actual data as shown in eq 16 if the parameters are better estimated smaller performance indicator values are expected to be obtained since ies is a monto carlo based method its performance is affected by the specific initial ensemble especially when the ensemble size is small to examine the effects of ensemble size on estimation accuracy different ensemble sizes i e ne 50 100 200 300 600 are tested to give a fair comparison we repeat the ies implementation with 10 different initial ensemble sets for each tested ensemble size the rmsem values for estimated parameters as a function of ensemble size are ploted in fig 3 a overall the rmsem values decrease to a small and stable level as the ensemble size increases furthermore the mean number of iterations needed also decreases as the ensemble size increases fig 3 b this is because a larger ensemble size guarantees more accurate sensitivity information resulting in a convergent result with fewer iterations overall ies performs well when the ensemble size increases to 300 thus in this case the ensemble size of 300 could achieve a good balance between the accuracy and computational cost in the following calculations ne 300 is also set as the ensemble size for gpies some discussions about the choice of the optimal ensemble size and techniques used to reduce the ensemble size e g localization in ensemble based methods can be found in rasmussen et al 2015 sun et al 2009 yin et al 2015 increasing the number of base points can improve the performance of gpies however at increased computational cost to illustrate the superiority of the adaptive gp refinement two forms adaptive and non adaptive of gpies are repeatedly implemented with 10 different initial ensembles we first implement gpies in an adaptive way with 10 initial base points and 1 more base point added at each iteration i e nini 10 and nadd 1 while in the non adaptive gpies 40 initial base points are used and the adaptive gp refinement is not implemented i e nini 40 and nadd 0 in the adaptive gpies a surrogate with better local accuracy on the posterior parameter distribution is constructed since the updated ensemble members are successively added as base points while in the non adaptive gpies a surrogate with better global accuracy on the prior parameter distribution is constructed by using more initial base points since the number of iterations may vary in different runs of gpies the average nori value is used to evaluate the computational cost fig 4 shows that with the same computational cost the adaptive gpies with smaller rmsem and rmseh values significantly outperforms the non adaptive gpies this is because the posterior distribution usually occupies only a small proportion of the prior distribution and thus constructing a locally accurate surrogate over the posterior distribution could be more computationally efficient to test the effects of the number of added base points at each iteration different nadd values i e 1 5 10 20 are used while the same 10 base points nini 10 are used to construct the initial gp surrogate we also implement the adaptive gpies with 10 different initial ensemble sets for each tested nadd value with different nadd values performance indicator values for estimated parameters and predicted states are shown in fig 5 it is seen that with increasing nadd value the estimation accuracy improves at increased computational cost this occurs because increasing the nadd value leads to a more accurate gp surrogate and thus to more accurate sensitivity information between model parameters and outputs when nadd increases beyond 10 the estimation accuracy of gpies is comparable to that of the standard ies in terms of both parameter estimation and state prediction it is also noted that with ne 300 nini 10 and nadd 10 gpies needs an average number of 9 iterations to reach convergence in contrast the average number of required iterations for ies with ne 300 is 7 fig 3 b thus more iterations are needed in gpies due to its approximation error overall since the original governing equation is only required to be solved for base points the computational cost of gpies is an order of magnitude lower than that of ies considering both the accuracy and computational cost nadd is chosen as 10 in this study fig 6 illustrates the accuracy of the adaptively refined gp surrogate during the iterations it can be seen that as the iteration continues the errors between the original model outputs and the surrogate outputs keep decreasing and the determination coefficient r 2 increases which indicates the gradually improved accuracy of the surrogate a plot of rmseh values against the iteration number is illustrated in fig 6 d obviously the rmseh decreases to almost 0 as the iteration continues confirming a continuous refinement of the surrogate model as shown in fig 7 the estimated mean ln ks fields given by ies ne 300 and gpies ne 300 nini 10 nadd 10 can both capture the main patterns of the reference field it is also noted that the ln ks field estimated by gpies is much smoother due to the utilization of dimension reduction the kl expansion to further evaluate the accuracy we examine the match between the hydraulic head measurements at all observation locations and the corresponding predicted values with the final updated parameter ensembles of ies and gpies results show that the predicted values by the two algorithms both well match the observations indicating a high estimation accuracy and three sets of representative results at selected observation locations are illustrated in fig 8 3 2 case 2 unsaturated flow problem to further demonstrate the performance of gpies we apply it to a more nonlinear problem which considers 2 d water flow in variably saturated soil the flow process can be described by the richards equation 17 θ ψ t x k x ψ ψ x z k z ψ ψ z 1 where θ ψ is the volumetric water content l 3 l 3 kx ψ and kz ψ are the unsaturated hydraulic conductivities in the horizontal and vertical direction lt 1 respectively the highly nonlinear relationships between the soil hydraulic parameters and the pressure head ψ can be described by the van genuchten mualem model vgm genuchten 1980 18 θ ψ θ r θ s θ r 1 α ψ n m ψ 0 θ s ψ 0 19 k ψ k s s e 0 5 1 1 s e 1 m m 2 where se θ θ r θ s θ r is the effective water content θ s and θ r are the saturated and residual water contents respectively ks is the saturated hydraulic conductivity lt 1 m 1 1 n n 1 α and n are two empirical coefficients related to the soil pore size distribution the study domain shown in fig 9 is a square with a dimension of 2 l 2 l which is evenly discretized into 961 31 31 elements the upper boundary is exposed to the atmosphere with a precipitation of 0 02 lt 1 and the lower boundary is set as free drainage impervious boundaries are prescribed for the two lateral sides initially the pressure head ranges linearly from 2 l near the upper boundary to 0 near the lower boundary in this case the heterogeneous saturated hydraulic conductivity field ks x z is assumed to be estimated while other vgm parameters are given as constants i e θ r 1 10 4 l 3 l 3 θ s 0 339 l 3 l 3 α 1 74 l 1 and n 1 3757 the finite element software hydrus šimůnek et al 2006 is employed to simulate the unsaturated flow the total simulation time is set as 20 t and pressure head data are monitored at the observation locations denoted as round circles in fig 9 at an interval of 1 t the log transformed saturated conductivity field ln ks x z is modeled as a gaussian random field with the following mean and spherical covariance function 20 ln k s 2 ln l t 1 21 c ln k s d σ ln k s 2 1 1 5 d a 0 5 d a 3 d a 0 d a in this case σ ln k s 2 is still chosen as 1 and correlation scales are λ x lx 0 25 λ z lz 0 25 the reference ln ks field and the initial ensemble realizations for ies are also randomly generated using the sgsim tool the pressure head observations are then obtained through running the hydrus solver with the reference field and the measurement errors are assumed to follow independent identical gaussian distributions with zero mean and standard deviation of 1 10 2 l the kl expansion is also employed to parameterize the ln ks fields in gpies and 82 of the total variance of the gaussian random field can be captured by increasing the number of truncated kl terms to nkl 50 as the dimensionality and nonlinearity are significantly increased compared to case 1 more base points are required to guarantee the accuracy of the initial gp surrogate we set nini 100 while other recommended settings are still used i e ne 300 nadd 10 with the observations the ln ks fields are estimated using both ies ne 300 and gpies ne 300 nini 100 nadd 10 the simulations are also repeatedly implemented 10 times with different initial ensemble realizations the average number of iterations required to reach convergence is 9 and 20 for ies and gpies respectively overall the final mean estimations of the two algorithms shown in fig 10 can still capture the main patterns of the reference field fig 11 shows the match between the pressure head observations at three representative observation locations and corresponding predicted values with the final ensembles of gpies and ies it can be seen that the predicted values given by gpies better match the true observations furthermore the confidence intervals given by gpies are much narrower indicating the smaller uncertainty since the observations are all within the confidence intervals the estimated uncertainty given by gpies is reasonable in this case the performance indicators for the two algorithms are compared in fig 12 overall gpies gives comparable parameter estimations and much better data matches compared to ies in terms of computational efficiency however gpies only needs about 1 10 of the computational cost required by ies thus gpies is still superior to ies in this case to further assess the performance of gpies in the presence of smaller correlation scales we reduce both λ x lx and λ z lz to 0 15 the reference field and initial ensemble realizations for ies are also generated using the sgsim tool for the kl expansion in gpies 100 leading terms are required to retain 80 of the total variance of the random field accordingly we increase nini to 200 to construct the initial gp surrogate due to the much higher dimensionality while other settings remain unchanged fig 13 shows that the final mean estimations of ies and gpies can still capture the main patterns of the reference field with the decreased correlation scales the performance indicators for the estimated parameter field and predicted pressure heads from the two algorithms are compared in fig 14 overall gpies still achieves a satisfactory estimation accuracy and about an order of magnitude of speed up compared with ies it is also noted that more iterations are required for both ies 12 and gpies 27 to reach convergence due to the decreased correlation scales 4 conclusions in this study a gaussian process based iterative ensemble smoother gpies has been proposed for data assimilation the key idea is to adaptively refine the gp surrogate using the updated parameters conditional to the available measurements at each iteration then the sensitivity information between model parameters and measurements is calculated from a large number of realizations generated by the gp surrogate with virtually no computational cost to further improve the computational efficiency the karhunen loève kl expansion is employed as a dimension reduction tool to parameterize the heterogeneous parameter field thus the gp surrogate is constructed in a much reduced parameter dimension during the entire gpies simulation the original model evaluations are only needed at the base points whose number is much smaller than the ensemble size thus the computational cost can be significantly reduced two synthetic case studies considering saturated and unsaturated flow problems are tested to validate the gpies algorithm respectively overall gpies achieves at least an order of magnitude of speed up compared with the standard ies without sacrificing the estimation accuracy the strategy in constructing the gp surrogate is also investigated it has been shown that adaptively constructing a locally accurate surrogate i e successively adding the updated ensemble realizations as new base points significantly improves the efficiency compared to the non adaptive approach it should be pointed out that gpies will lose its efficiency if the random dimensionality is extremely large i e nkl 100 for very small correlation scales in contrast the implementation of ies is relatively independent to the random dimensionality furthermore to improve the performance in estimating the non gaussian parameters some other techniques i e the level set parameterization chang et al 2010 and normal score transformation zhou et al 2011 can be combined with gpies in future studies acknowledgments computer codes and data used are available upon request to the corresponding author the authors would like to thank the three anonymous reviewers and the editors for their constructive comments on an earlier version which led to a much improved paper this work is supported by the national natural science foundation of china grants 41771254 41571450 and 41571215 
828,we present an investigation on the combined effect of fluid rheology and permeability variations on the propagation of porous gravity currents in axisymmetric geometry the fluid is taken to be of power law type with behaviour index n and the permeability to depend from the distance from the source as a power law function of exponent β the model represents the injection of a current of non newtonian fluid along a vertical bore hole in porous media with space dependent properties the injection is either instantaneous α 0 or continuous α 0 a self similar solution describing the rate of propagation and the profile of the current is derived under the assumption of small aspect ratio between the current average thickness and length the limitations on model parameters imposed by the model assumptions are discussed in depth considering currents of increasing decreasing velocity thickness and aspect ratio and the sensitivity of the radius thickness and aspect ratio to model parameters several critical values of α and β discriminating between opposite tendencies are thus determined experimental validation is performed using shear thinning suspensions and newtonian mixtures in different regimes a box filled with ballotini of different diameter is used to reproduce the current with observations from the side and bottom most experimental results for the radius and profile of the current agree well with the self similar solution except at the beginning of the process due to the limitations of the 2 d assumption and to boundary effects near the injection zone the results for this specific case corroborate a general model for currents with constant or time varying volume of power law fluids propagating in porous domains of plane or radial geometry with uniform or varying permeability and the possible effect of channelization all results obtained in the present and previous papers for the key parameters governing the dynamics of power law gravity currents are summarized and compared to infer the combinations of parameters leading to the fastest lowest rate of propagation and of variation of thickness and aspect ratio keywords gravity current self similar non newtonian experiment review 1 introduction the propagation of gravity driven flows in porous media is but a chapter of the fascinating book on gravity currents hereinafter gcs which has received considerable attention ungarish 2009 with new chapters being continuously added yet also porous gcs by themselves originating from such diverse applications carbon dioxide sequestration mining engineering environmental pollution and remediation seawater intrusion to name but a few constitute such a wide topic that a comprehensive summary is arduous in the authors view the recent advancements on gravity driven porous flow belong to two broad categories the first group of contributions has as a common feature the modelling of the spatial variations of properties and or of boundary conditions in natural geologic media and the description of their topographical features examples of such contributions are huppert et al 2013 sahu and flynn 2017 and ngo et al 2016 where heterogeneity is modelled via discrete layers or intrusions of finite extent islam et al 2016 who introduce explicitly small scale heterogeneity yu et al 2017 who account simultaneously for drainage from a permeable substrate and an edge and huber et al 2016 who aim at reproducing the effect of diverse co2 injection strategies the second broad group of gc themed contributions presents an improved description of fundamental mechanisms via a more sophisticated modelling some relevant examples are the effects of a change in flux ball et al 2017 or of stratification in an intruding current pegler et al 2016 the investigation of the co2 sequestration mechanisms into deep saline aquifers involving two phase flow guo et al 2016 or with possible background hydrological flow unwin et al 2016 the interactions between gravity currents and convective dissolution elenius et al 2015 or geomechanics bjornara et al 2016 the adoption of realistic rheological models in the study of non newtonian gcs di federico et al 2017 some recent contributions belong to both categories and are associated for example with the modelling of co2 sequestration ngo et al 2016 or the simultaneous presence of non newtonian flow and spatial heterogeneity or specific topographical features the latter topic has been investigated in depth in several papers considering deterministic heterogeneity and radial di federico et al 2014 or plane geometry ciriello et al 2016 and channelized flow di federico et al 2014 the motivation for these studies lies in a multiplicity of applications involving complex fluids flowing in geologic media characterized by spatial heterogeneity at various scales oil and displacing suspensions in reservoir flow remediation carriers and liquid contaminants in the subsurface environments drilling and grouting fluids earlier works ciriello et al 2016 di federico et al 2014 list specific references to these applications studies of flows of non newtonian gcs rely on a body of knowledge accumulated for newtonian currents the reference works of huppert and woods 1995 for plane geometry and by lyle et al 2005 for axisymmetric geometry were extended to power law fluid flow by di federico et al 2012a b which in turn set the stage for the more complex setups cited earlier variations of properties along vertical and horizontal direction were considered in the context of newtonian gcs by zheng et al 2014 2013 while vertical variations mimic the effect of stratification horizontal variations may represent e g the effect induced by the drilling of a well and thus are of interest especially in the context of axisymmetric propagation a review of existing studies on non newtonian porous gcs reveals the lack of a study coupling power law rheology and permeability gradients along the flow direction in axisymmetric flow such a study is presented here in sections 2 5 considering the usual hypothesis of a gc of time variable inflow the exposition is organized as follows the mathematical problem is formulated in section 2 for a radial injection and solved in section 3 in self similar form generalizing the results of di federico et al 2012b section 4 discusses the dependency of key time exponents governing the propagation of the current on problem parameters along with the limitations imposed by modelling assumptions experimental results are presented in section 5 first the experimental set up is described with special attention on the difficulties implied by simulating heterogeneity then results from a series of tests are compared with the theory in constant and variable flux regime the theory and experiments presented complete a first picture on porous gravity currents of power law fluid flowing in different geometries plane and axisymmetric in domains exhibiting permeability variations in different directions vertical and horizontal taking into account the influence of the channel cross section for plane flow a general overview and comparison of these self similar solutions seems timely and is presented in section 6 concluding remarks are formulated in section 7 together with perspectives for future work 2 problem formulation consider a non newtonian power law fluid of density ρ consistency index m and flow behaviour index n that spreads axisymmetrically over a horizontal bed into a porous medium of height h 0 initially saturated with a lighter fluid of density ρ δ ρ see fig 1 under the sharp interface and thin current approximations and in the absence of capillary effects see the recent paper by chiapponi 2017 for an indication of the fluid retention in a glass beads porous medium the pressure within the current is hydrostatic and given by p r z t p 1 δ ρ g h r t ρ g z where r and z represent radial and vertical coordinates p 1 p 0 ρ δ ρ g h 0 is a constant p 0 is the constant pressure at z h 0 and g is gravity under the additional assumption that the current thickness is small compared to that of the ambient fluid the velocity of the latter and the vertical velocity in the intruding fluid can be neglected allowing to describe the current behaviour by means of its horizontal velocity u r t thickness h r t and maximum extension rn t for given time t the expression of the horizontal velocity can be deduced from the following general equation valid for the motion of a power law fluid in a porous medium cristopher and middleman 1965 1 p ρ g μ e f f k u n 1 u in which p is the pressure u is the darcy velocity field g is the gravity vector k the permeability and μeff is the effective viscosity dimensions ml n t n 2 the mobility μ e f f k is given by di federico et al 2012b 2 k μ e f f 1 2 c t 1 m n ϕ 3 n 1 n 50 k 3 ϕ n 1 2 where ϕ is the porosity and c t c t n the tortuosity factor the modified darcy s law 1 is based on a capillary bundle model first proposed by bird et al 1960 and later modified to include tortuosity for which different formulations are available e g shenoy 1995 in the following the formulation by pascal 1983 c t 25 12 n 1 2 is adopted macroscopic laws having the same structure of eq 1 were obtained via direct simulation at the pore scale by e g balhoff and thompson 2006 and vakilha and manzari 2008 experimental verification was provided among others by cristopher and middleman 1965 and yilmaz et al 2009 additional references on the use of eq 1 are reported in di federico et al 2012b the model is unable to handle viscoelastic effects and thixotropy and needs to be modified to include yield stress or newtonian behaviour at low shear rates local mass conservation implies that 3 1 r r r u h ϕ h t and in addition two boundary conditions are needed to formulate the problem the first b c is the global mass conservation condition 4 2 π ϕ 0 r n t r h r t d r q t α expressing the total volume of the current as a function of time t and parameters q dimensions l 3 t α and α this formulation includes the instantaneous injection with constant volume α 0 and the continuous injection with increasing volume α 0 the second boundary condition states that the thickness at the current front is null i e 5 h r n t t 0 further the horizontal permeability variation needs to be described the following law of variation is adopted for the medium permeability k ciriello et al 2013 zheng et al 2014 6 k r k 0 r σ r β where k 0 is a characteristic permeability r is a length scale σ is a coefficient introduced for convenience and β is a constant the coefficient σ is necessary in order to recover the dependency of the permeability only on the characteristics of the porous medium and assumes different values for different length scales r in order to keep the denominator σr independent on the fluid properties and on the injection power law for β 0 the permeability decreases or increases with the distance from the injection well respectively β 0 represents a medium with constant permeability k 0 and the simpler model of di federico et al 2012b is recovered for β 0 the behaviour of 6 is singular for r 0 but this does not affect the overall behaviour of the current further we require that β β 0 2 n 3 n 1 this upper limitation to the increase of the permeability with distance from the origin guarantees the validity of our solution from a theoretical point of view as demonstrated in section 4 1 for a newtonian fluid n 1 β 0 4 ciriello et al 2013 found β 0 3 in plane geometry for the two limit cases n 1 and n 1 very shear thinning or shear thickening fluids β 0 6 and β 0 2 respectively in a related study on newtonian gravity currents in hele shaw cells with a gap thickness varying in the flow direction zheng et al 2014 showed that the upper limit for the validity of the lubrication approximation is β 3 in terms of the present paper they then derived results for β 0 6 1 5 and 2 4 a range of values including the actual β value simulated in our experiments described in section 5 as far as field values are concerned realistic exponents for vertical power law variations of properties jiang et al 2010 marsily 1980 tend to be much lower than the upper limit value β 0 more importantly also horizontal variations of permeability are often modelled with negative exponential or decreasing power law functions to represent the steadily decreasing permeability altered by the drilling process of the aquifer around a large diameter well altunkaynak and sen 2011 slider 1983 or of the reservoir surrounding a fracture li et al 2017 in both cases there is a simplification of the actual behaviour where probably a constant and lower value of permeability is reached at a certain distance from the well or fracture substituting eq 6 in the one dimensional version of eq 1 and expressing the pressure gradient as a function of the unknown free surface as p r δ ρ g h r yields the following equation of motion 7 u r z t λ δ ρ g 1 n k 0 n 1 2 n r σ r β n 1 2 n h r 1 n 1 h r where 8 λ λ ϕ m n 1 2 c t 50 3 n 1 2 n 3 n 1 n ϕ n 1 2 m which for a newtonian fluid n 1 is the inverse of dynamic viscosity μ the mathematical problem constituted by eqs 7 and 3 with boundary conditions 4 and 5 may be rendered non dimensional upon defining time space and velocity scales as 9 t q ϕ v 3 1 3 α r v t v λ δ ρ g 1 n k 0 1 n 2 n ϕ σ β n 1 2 n and dimensionless coordinates as t t t r r r r n r n r and h h r note that the time scale t is defined for α 3 the particular case α 3 requires a partially different non dimensional formulation which can be easily derived following e g di federico et al 2012a vella and huppert 2006 for all other cases the dimensionless problem reads 10 1 r r r f 1 1 h h r 1 n 1 h r h t obtained by combining the dimensionless versions of 7 and 3 in eq 10 11 f 1 β n 1 2 n is a factor which reduces to zero in the homogeneous case the global mass balance 4 becomes 12 2 π 0 r n r h d r t α while the condition at the front 5 becomes h r n 0 in dimensionless form 3 solution it is desirable to obtain a self similar solution to the system formed by eqs 10 and 12 with 5 to capture the long term evolution of the current once the influence of initial and boundary conditions fades as illustrated in the appendix a first kind similarity solution for the extension and thickness of the current is derived in the form r n t η n t f 2 and h r t η n f 5 t f 3 ψ ζ where the thickness factor ψ ζ is the solution of the non linear ordinary differential equation 13 ζ f 1 1 ψ ψ ψ 1 n 1 f 2 ζ 2 ψ f 3 ζ ψ 0 in which the prime indicates d dζ and subject to the condition 14 ψ 1 0 while the similarity variable at the front of the current ηn is given by 15 η n 2 π 0 1 ζ ψ ζ d ζ 1 f 5 2 and the factors f 2 f 3 and f 5 are given by a 2 a 3 and a 7 respectively for a homogeneous medium β 0 results reduce to the simpler case of di federico et al 2012b with f 1 0 f 2 α n n 3 f 3 α n 1 2 n n 3 and f 5 n 1 for a newtonian fluid n 1 one obtains f 1 β f 2 α 1 4 β f 3 α 2 β 2 4 β and f 5 2 β when both simplifications apply the homogeneous newtonian case studied by lyle et al 2005 is recovered and f 1 0 f 2 α 1 4 f 3 α 1 2 and f 5 2 for the instantaneous injection case α 0 eqs 13 and 15 subject to 14 and ψ 0 0 the latter condition derives from a no flux boundary condition for r 0 valid for constant volume are amenable to the closed form solution 16 ψ ζ f 20 n f 5 1 ζ f 5 17 η n π f 20 n f 5 2 1 f 5 2 where f 20 f 2 α 0 2 n 2 n 3 β n 1 the constraint f 5 0 equivalent to β 2 applies when β 0 eq 17 of di federico et al 2012b is recovered when n 1 16 and 17 transform into 18 ψ ζ 1 4 β 2 β 1 ζ 2 β 19 η n 4 β 2 π 1 4 β finally when both n 1 and β 0 ψ ζ 1 ζ 2 8 and η n 2 π 1 4 pattle 1959 for the continuous injection case α 0 eq 13 needs to be integrated numerically with 14 and a second boundary condition is obtained expanding the solution in power frobenius series and balancing the lower order terms for ζ 1 this yields 20 ψ ζ 1 a 0 b ϵ b 1 a 0 f 2 n b 1 where ϵ 1 ζ is a small quantity and f 2 is non negative if β 2 n 3 n 1 integrating 13 with 14 and 20 with a runge kutta scheme yields the thickness profile ψ ζ and the similarity variable ηn sample results are shown in fig 2 a f for α 0 1 and selected values of n and β the analytical solution for α 0 and the results obtained by lyle et al 2005 for the newtonian homogeneous case are well reproduced the thickness profile ψ ζ increases with the injected volume α for given fluid and medium n and β is an increasing function of β and a decreasing function of n for constant volume currents the dependency on n for constant flux currents is more complex the prefactor ηn 15 whose value influences the current thickness via a 8 is illustrated in fig 2 g versus α for different n β ηn increases with n and decreases with α and β while its sensitivity is largest for smaller α and n and larger β these dependencies are reversed with respect to the thickness profile so that the dimensionless thickness results from the interplay of ψ and ηn other quantities of interest are the aspect ratio of the current the ratio between its average thickness h and radius rn and the average free surface gradient driving the motion h r these are given respectively by 21 h r n η n f 5 1 t f 3 f 2 ψ 22 h r η n f 5 1 t f 3 f 2 d ψ d ζ f 3 f 2 α 2 n β n 1 6 n 2 n 3 β n 1 where ψ and d ψ d ζ are respectively the average value of the thickness profile and of its derivative over the interval 0 1 furthermore the velocity field u is given in dimensionless form u u v by 23 u ϕ r f 1 η n f 5 1 n t f 3 f 2 n ψ ζ 1 n 1 ψ ζ while for the velocity of advancement of the front of the current v d r n t d t the dimensionless expression v v v is 24 v η n f 2 t f 2 1 with f 2 1 2 α 6 β n 1 n 1 2 β 4 4 discussion of results 4 1 behaviour of key time exponents the power law time exponents f 2 f 2 1 f 3 and f 3 f 2 eqs a 2 24 a 3 and 22 of the radius velocity thickness and aspect ratio of the gravity current are the key factors to understand the evolution of the current over time in the present section we explore their dependency on model parameters α the time rate of change of the fluid volume n the flow behaviour index and β the rate of change of the permeability along the direction of propagation by evaluating their sign and their partial derivatives with respect to model parameters the results obtained for f 2 together with f 2 1 f 3 and f 3 f 2 are listed in tables 1 3 respectively various limit values of α and in some instances of other parameters emerge each limit value is listed below the respective condition on f 2 f 3 f 3 f 2 or their partial derivative these threshold values of model parameters discriminate between a positive null or negative value of f 2 f 3 f 3 f 2 and of their partial derivative with respect to α n and β inspection of table 1 reveals that for a physically meaningful solution the permeability must decrease over space or increase not too sharply β βe for a newtonian fluid n 1 β e 4 the current front accelerates f 2 1 0 for any α under a sharp increase in permeability β βa or beyond a threshold value αa of α for permeability decreasing or increasing moderately over space β βa otherwise the current is decelerated for a newtonian fluid n 1 the threshold values reduce to β a 3 α a 3 β for a homogeneous medium β 0 and any fluid α a 3 moreover f 2 increases with α for any combination of β n as a larger fluid injection rate implies an increase in the current velocity regardless of the permeability variation and fluid nature similarly f 2 increases with β for any combination of α n as an increase less marked decrease of the permeability favours the current advancement the functional dependency of f 2 on n is more complex as the velocity of the current increases with n for any α under a sharp increase in permeability β βen or below a threshold value αen of α for permeability decreasing or increasing moderately over space β βen the current velocity decreases with n when combining a large injection rate α αen with a permeability decreasing or increasing moderately over space β βen for a newtonian fluid n 1 the threshold value of α reduces to α e n 3 inspection of table 2 shows that the thickness of the current increases with time at a given point f 3 0 only when a large injection rate α αt is combined with permeability decreasing or increasing moderately over space β βt for decreasing permeability the current encounters more resistance as it advances while for a moderately increasing permeability the decrease in medium resistance is more than compensated by the volume increase of the current in all other cases the thickness decreases over time and does so for any α when the permeability increase is marked for a newtonian fluid n 1 the threshold value of α reduces to α t 2 2 β for a homogeneous medium β 0 and any fluid α t 2 n n 1 furthermore it is noted that f 3 decreases or increases with α depending whether a threshold value αt is exceeded or not or equivalently depending whether the increase in the volume of the current prevails over the permeability increase along the flow direction the functional dependency of f 3 on n is the opposite of f 2 and the same threshold values due to mass balance finally f 3 decreases with β for any combination of α n as an increase less marked decrease of the permeability increases the radius of the current thus implying a decrease in thickness due to mass balance for the same reasons the dependence of f 3 upon n is the opposite of f 2 with the threshold value αtn being equal to αen inspection of table 3 indicates that the aspect ratio average spatial gradient of the current increases with time f 3 f 2 0 only when a large injection rate α αg is combined with permeability decreasing or increasing moderately over space β βg this behaviour can be understood noting that the average spatial gradient is proportional to the resistance encountered by the current in its advancement otherwise the aspect ratio decreases with time and the current grows progressively more elongated for a newtonian fluid n 1 the threshold values reduce to α g 2 2 β β g 1 for a homogeneous medium β 0 and any fluid α g 3 the dependence of f 3 f 2 on α is governed by a threshold value βgα for β βgα f 3 f 2 decreases with increasing α the reverse is true for β βgα this is so because unless the permeability increase is marked the aspect ratio of the current increases with the injection rate the threshold is β β g α 1 for a newtonian fluid the behaviour of f 3 f 2 as a function of n is analogous to f 2 with the same threshold values finally f 3 f 2 decreases with β for any combination of α n as a more permeable medium implies less resistance to the flow and a reduced average spatial gradient to visually illustrate the behaviour of the key exponent fig 3 a f depict how f 2 f 3 and f 3 f 2 depend on β for fixed n 0 5 and on n for fixed β 0 5 results for various values of α including the critical ones are shown the two reference values n 0 5 and β 0 5 are selected for illustrative purposes and represent common cases in natural porous media i e a shear thinning fluid and a permeability decreasing with distance from the source a comparison of the threshold values of α and β reveals that i for a homogeneous medium β 0 all threshold values of α coalesce into 3 except for αt ii for a newtonian fluid n 1 the threshold values of α are β dependent iii for newtonian flow in a homogeneous medium α t 1 a plot of the limit αg is shown in fig 4 for n 0 5 1 1 5 the limiting value of α increases with β the increase is more rapid for β 0 the influence of n on αg is mixed in that this limit value increases with n for β 0 and decreases with n for β 0 for a homogeneous medium the limit αg is independent of the behaviour index n 4 2 limits of validity limitations on the parameters emerge when considering the validity of model assumptions at any time conditions for the radius of the current to increase with time must hold f 2 0 as noted in the previous subsection furthermore for t 1 the thin current approximation requires the intruding current to be thin compared to both its height f 3 f 2 0 and the characteristic height h 0 of the porous medium f 3 0 otherwise at large times i the current thickness would exceed a reasonable portion of the porous domain total height rendering invalid the assumption of immobile ambient fluid ii the aspect ratio of the current would increase without bounds contrary to the assumption of negligible vertical velocities combining these limitations the parameters domain satisfying all model assumptions asymptotically the most restrictive condition is obtained an example is illustrated in fig 5 where the two limits βe and βg are depicted the first to ensure f 2 0 the second to ensure f 3 f 2 0 in all cases of practical interest n 3 the latter limitation is more stringent than the former it is seen that a too sharp increase in the permeability along the flow direction renders the current steeper with time the limit β value is 0 67 for n 0 5 and 1 for n 1 4 3 limitations of the model for in situ applications as to in situ applications there is still room to improve the connection between the present model and the field conditions the model is based on a monotonic permeability variation from the well to infinity porosity variations can be easily added and is not presently able to handle composite and more complex spatial variations when the permeability variation is due to fracturing rearrangement of grains during drilling or due to sealing or to mud injection in the medium a cutoff is expected at a certain distance from the well in addition in the latter cases the most relevant variations of permeability and porosity happen at a short distance from the well where the model itself is questionable due to several effects earlier highlighted however in other cases the permeability reduction is more gradual for example when it is associated to clogging of pore space resulting from deposition of fine material or escape of dissolved gases in water aquifers nevertheless the results are promising and indicate that further steps and advancements can be based on the present approach which can function as a benchmark solution for more complex situations more on this in the conclusions 5 laboratory experiments 5 1 experimental setup a series of experiments were conducted at the hydraulic laboratory of the university of parma to test the validity of the theoretical solution a 90 sector glass tank 25 cm 25 cm 25 cm in size was filled with transparent glass ballotini with nominal diameters of d 1 0 2 0 3 0 4 0 and 5 0 mm to reproduce a porous medium the continuous horizontal gradient of the permeability required by eq 6 was approximately reproduced by using a plastic framework that allowed to create separate neighbouring sectors each filled with beads of uniform diameter and having uniform permeability given by the kozeny carman equation the thickness of each sector was determined according to the procedure outlined in appendix b of di federico et al 2014 which provides the connection between the geometry of the stepwise distribution of diameters and the theoretical parameters k 0 and β of the continuous distribution 6 the plastic framework shown in fig 6 consists of thin plastic sheets 0 5 mm curved in order to reproduce four quarters of cylinder with radius equal to 3 2 cm 6 cm 9 cm and 12 2 cm with two radial diaphragms plane plastic sheets after filling the sectors with the beads the framework is gently removed by lifting it fig 7 shows the radial distribution of the diameters and the permeability for β 1 65 the diameters adopted for the beads are in the upper range for natural porous media this choice mainly reflects commercial availability and ease of sieving nevertheless the solution is applicable to porous media with grains of any size as long as the underlying assumptions are respected the horizontality of the bottom of the tank was checked by an electronic level the intruding current was a shear thinning fluid made of softened water water without cations like ca and mg glycerine and xanthan gum mixed in two different proportions i 40 vol of water 60 vol of glycerine and 0 10 weight of xanthan gum ii 95 vol of water 5 vol of glycerine and 0 15 weight of xanthan gum ink was added to the final mixture for an easy visualization and detection of the interface we used a commercial xanthan gum for food use from a local supplier and glycerine was added to increase the consistency index without adding too much xanthan gum the mixing was performed in a low speed stirrer by adding small quantities of xanthan gum to pure water and then adding glycerine after mixing lumps were removed with a small colander and the mixture was left at rest for several hours the overall result is that mixtures with the same ingredients but prepared in different days show different rheological parameters the rheological parameters flow behaviour index n and consistency index m were measured by a strain controlled rheometer dynamic shear rheometer anton paar physica mcr 101 with parallel plates roughened by sandpaper p 60 glued onto both smooth surfaces the distance between the plates was 1 mm and the testing temperature of the rheometer was t 25 c equal to the one measured in the laboratory during the experiments with expected fluctuations of 1 c the range of shear rate during measurements was chosen in order to overlap the range of shear rate expected during flow in the porous medium following the criterion reported in longo et al 2013b according to this criterion the effective shear rate should be evaluated at the pore scale by using e g the expression given by savins 1969 25 γ u 2 10 4 k ϕ c with u the darcian velocity and c 2 1 2 4 a coefficient related to tortuosity the result is a low effective shear rate in most part of the body of the current see longo et al 2013b fig 5 for an estimation of the shear rate in experiments similar to the present experiments indeed in some part of the current like the injection area the shear rate is much larger than in the body of the current however it has been experimentally demonstrated that the evolution of a viscous buoyancy gravity current is not influenced by the local disturbances near the inlet section see e g lyle et al 2005 fig 8 shows the stress strain measurements for two fluids adopted in the experiments with the interpolating power law function we bear in mind that the power law approximation hides a much more complex rheological behaviour of the mixture see e g amundarain et al 2009 zhong et al 2013 which is also influenced by ions and chemicals hence the power law is adopted as a pragmatic working tool for a simple and synthetic description of the local rheology of the fluid the intruding fluid was injected with a syringe pump into the tank through a quarter cylinder volume similar to a well having radius of 0 8 cm obtained with a brass net which was located in one corner of the tank this configuration reproduces an axisymmetric spreading due to the symmetry along the vertical axis and with negligible influence of the wall boundary layers the syringe pump was controlled by an analog electric signal to generate a constant α 1 or waxing α 1 5 2 0 influx rate during the injection the lateral current profile was recorded by a high resolution video camera canon legria hf 20 1920 1080 pixels working at 25 frames per second while the bottom view was reflected by a mirror and captured by a photo camera shooting every 2 s the videos and images were post processed using a software to transform the pixel positions into metric coordinates a grid stuck on the wall and on the bottom of the tank was used to reconstruct the correspondence between image and physical plane surfaces with the use of interpolating polynomials functions the position of the front of the current was detected by selecting the nose on the image and then converting the pixels coordinates into metric coordinates with an overall accuracy of 1 mm fig 9 shows two typical images of the side and bottom view during one of the experiments 5 2 experimental results and discussion a total of 10 experiments were performed with the experimental parameters summarized in table 4 the horizontal permeability is controlled by the value of β which was kept constant for all data sets while the injection rate α the fluid rheology m and n and the fluid density ρ varied among the tests fig 10 depicts the non dimensional front position rn of the current for the various tests compared with the theoretical prediction for most tests with the exception of a1 and a4 experimental results indicate a front position below the theoretical counterpart before reaching it asymptotically in all cases the good agreement between theoretical and experimental data over time asymptotically within 5 is due to the balance of buoyancy and viscous forces while the disturbing effects due to injection with significant vertical velocity influence the position of the front only at the beginning of motion the comparison between tests a4 and b3 which only differ in the type of fluid having respectively n 0 43 and n 0 57 leads to the conclusion that the more shear thinning fluid a4 best fits the theoretical model and this result holds true since the beginning of the test furthermore shear thinning fluids advance slower with decreasing values of n as shown upon comparing tests a5 and b4 for α 2 the results of tests a2 and a3 characterized by different values of q i e 2 4 and 4 0 cm 3 s 1 clearly show the same behaviour demonstrating that all other parameters being equal q is not relevant in the evaluation of the dimensionless front position rn indeed a little variation of density e g between tests b2 and b5 proves that the fluid density ρ significantly affects the dimensionless position of the front the comparison between the actual position of the front end among different experiments is best performed in dimensional form as the time and velocity scales are function of experimental parameters which differ among the tests conducted fig 11 a and b shows the shape of the current at different times for two experiments with constant and waxing influx rate respectively the agreement between experiments and model is fairly good in particular at late times near the origin the experimental shape of the current is below the theoretical profile even though this effect does not affect significantly the front end position and the shape of the main body 6 overview on non newtonian gravity currents in porous media the present section is devoted to an overview of self similar solutions governing the propagation of non newtonian currents of variable volume with power law rheology in porous media the overview is performed by comparing the key parameters governing the propagation i e f 2 f 3 and f 3 f 2 equal to the time exponents of the extension thickness and slope of the current derivation of the exponent of the velocity of the front end of the current f 2 1 is trivial for a variety of combinations of geometries and laws of variation of properties for the case covered in the present paper radial propagation in an horizontally heterogeneous media f 2 f 3 and f 3 f 2 are reported in eqs a 2 a 3 and 22 respectively results for other geometries and or laws of variation were derived in previous papers ciriello et al 2016 di federico et al 2012a 2012b 2014 longo et al 2015 always with the parameter α equal to the time exponent of the volume of the current table 5 covers results for plane geometry the base unbounded case di federico et al 2012a is compared to the channelized case of parameter κ longo et al 2015 to vertical heterogeneity of parameter ω ciriello et al 2016 and to horizontal heterogeneity of parameter β ciriello et al 2016 see the table caption for additional details table 6 lists results for radial geometry the base case di federico et al 2012b is compared to vertical heterogeneity of parameter ω di federico et al 2014 and to horizontal heterogeneity of parameter β the present paper again see caption for details fig 12 depicts the behaviour of each key parameter for the homogeneous case as a function of geometry n and α for all cases analyzed the radial geometry implies lower values of all key parameters with the exception of a continuous injection of very shear thinning fluids in narrow cross sections for an instantaneous fluid release α 0 an increase of the rheological parameter n in radial geometry leads to f 2 values lower than other geometries due to mass balance considerations among the plane cases f 2 tends to decrease as the shape factor κ increases tending to the unbounded case κ as the volume of the current remains constant and the front of fluid is forced to move further for lower κ in constant flux regime α 1 radial geometry and n 0 5 f 2 behaves as in the constant volume regime while for plane geometries it shows an opposite behaviour i e f 2 increases with higher values of κ in constant volume regime α 0 the parameter f 3 is negative for all the analyzed geometries in general this exponent tends to decrease when moving to plane unbounded geometry in constant flux regime f 3 is negative only for radial geometry and dilatant fluids n 1 whilst in plane geometries f 3 tends to increase with the shape factor κ as does f 2 for all geometries the parameter f 3 f 2 is always negative for α 1 because of the higher limit of validity for shear thinning fluids the parameter reaches lower values in constant volume regime α 0 and is larger for plane than for radial geometry the influence of κ on results is more limited as κ increases fig 13 illustrates the trend of parameters f 2 f 3 and f 3 f 2 considering vertical permeability variations in plane and radial geometry the homogeneous case with ω 1 ciriello et al 2016 di federico et al 2014 is depicted in fig 12 in both plane and radial geometry for an instantaneous fluid release α 0 and ω 1 f 2 is higher than the homogeneous case depicted in fig 12 whilst it is lower if ω 1 this trend changes for a constant flux regime α 1 for ω 1 and plane geometry f 2 is lower than the homogeneous case and it becomes higher if ω 1 in the radial case for ω 1 f 2 is lower than the homogeneous case but only for a shear thinning fluid n 1 while it becomes higher for a dilatant fluid n 1 on the contrary if ω 1 f 2 shows an opposite behaviour concerning the parameter f 3 in constant volume regime α 0 and both geometries this parameter is lower than the homogeneous case for ω 1 while it becomes higher if ω 1 for continuous injection α 1 in plane unbounded geometry f 3 is higher than the homogeneous case if ω 1 and it reverses its behaviour with ω 1 in radial case for ω 1 f 3 is higher than homogeneous case only for a shear thinning fluid n 1 while it becomes lower for a dilatant fluid n 1 on the contrary if ω 1 f 3 has an opposite trend for an instantaneous release α 0 in both geometries f 3 f 2 is lower than the homogeneous case for ω 1 while it reverses its behaviour if ω 1 for continuous injection α 1 in plane unbounded geometry f 3 f 2 is higher than the homogeneous case if ω 1 reversing for ω 1 in radial geometry instead the behaviour is similar to f 3 for both geometries independently on vertical permeability variations the deviation between homogeneous fig 12 and heterogeneous values fig 13 tends to increase if n increases for α 0 and it decreases for constant injections only in plane geometry fig 14 depicts the behaviour of parameters f 2 f 3 and f 3 f 2 considering horizontal permeability variations in plane and radial geometry the homogeneous case β 0 is depicted in fig 12 ciriello et al 2016 in both geometries and regimes i e constant volume and constant flux for β 0 f 2 is lower than homogeneous case while its behaviour is reversed if β 0 for β 0 the parameter f 3 is higher than the homogeneous case for all geometries and regimes whilst it becomes lower if β 0 finally f 3 f 2 follows the same trend of f 3 for both releases geometries and horizontal permeability variations the deviation between homogeneous and heterogeneous case tends to increase for higher n values 7 conclusions we have presented a novel model describing the propagation of axisymmetric power law gcs in porous media with an horizontal permeability variation the problem is amenable to a self similar solution of the first kind yielding the position of the front end and the thickness of the current as functions of dimensionless parameters describing the volume of the gc α the fluid rheological behaviour n and the power law permeability variation along the horizontal coordinate β depending on the value of β the permeability increases or decreases with the distance from the origin in the latter case this conceptual simplification captures the essential behaviour of the radial variation of permeability around a well with the additional convenience of an easy to implement self similar solution which can be used as a benchmark for numerical modelling the special case of constant volume currents has a closed form solution the behaviour of key time exponents governing the rate of propagation thickness and aspect ratio of the current was discussed in detail yielding a number of threshold value of model parameters α and β which discriminate between opposite trends in the behaviour of the current over time and govern the sensitivity to model parameters themselves in turn these parameters allow to discriminate the conditions for the validity of our solution at large times a specific laboratory set up was devised to directly reproduce horizontal permeability variations overcoming the difficulties inherent in the horizontal juxtaposition of layers of glass beads of different diameter theoretical results were confirmed by our experiments with a fairly good agreement except for the early time regime it is confirmed that also in presence of a deterministic spatial variation of permeability disturbances or anomalies near the injection line and near the front of the current do not affect the current evolution in the intermediate asymptotic regime also the discretization of the porous medium to mimic a continuous variation of permeability and capillary effects do not significantly affect the behaviour of the gcs at least for constant influx rate condition α 1 more important disturbances are expected for constant volume experiments α 0 and in general for waning gcs in real applications model parameters are obtained as follows i rheological fluid parameters n and m need to be determined experimentally bearing in mind that the power law model is an approximation of the real fluid behaviour ii the strength of the injection α depends on its type which is usually known and is equal to 0 or 1 for instantaneous or continuous injection iii the parameter β reflecting the intensity of the permeability variation needs to be determined experimentally on the basis of available measurements at different locations note that two measurements k 1 and k 2 at two locations r 1 and r 2 allow the determination of β by means of eq 6 the theory and experiments herein presented complete a first picture on porous gravity currents of power law fluid flowing in plane and axisymmetric geometry the reference solutions are derived by di federico et al 2012a for plane and by di federico et al 2012b for radial flow the influence of channel shape on plane flow is covered in longo et al 2015 heterogeneous deterministic variations of properties are examined by ciriello et al 2016 considering vertical and horizontal grading in 2 d flows and by di federico et al 2014 considering vertical grading in radial flow horizontal grading is covered in the present paper an overview of the key time exponents for these cases revealed the combination of geometries and model parameters yielding the fastest lowest currents and those having the fastest decrease of thickness and aspect ratio over time our study has several connections to geological flows and industrial flows including flows during fracking procedures shale gas recovery drilling wells and may be relevant for co2 sequestration as solvents which proved effective in co2 capture exhibit shear rate dependent viscosities sze et al 2014 in all these applications fluids exhibiting non newtonian effects often approximated by power law fluids are used almost always in heterogeneous porous media at the pore scale it is worth noting that the effect of heterogeneity prevails over the non linearity due to rheology in shaping the flow pattern zami pierre et al 2016 with a relatively minor influence of the specific rheological equation chevalier et al 2014 it remains an open question whether this is true at darcy s scale in sum several avenues of investigation remain open in the area of non newtonian gcs e g inclusion of fluid drainage injection at the bottom of the current either distributed or concentrated in single multiple fissure s inclusion of stratification effects in the advancing current consideration of more complex permeability variations including cutoffs and discontinuities in the medium properties e g inclusions adoption of more realistic rheological models to describe complex fluids such as carreau or truncated power law stochastic modelling of heterogeneity we are investigating these fascinating topics and hope to report on them in the near future acknowledgements we thank luca chiapponi for assistance with the laboratory experiments appendix a self similar solution inspection of eq 10 yields the following time scalings for the length r and thickness h of the current a 1 r t f 2 h t f 3 where a 2 f 2 2 α n 2 n 3 β n 1 a 3 f 3 α n 1 2 β 4 n 2 n 3 β n 1 this suggests the adoption of the similarity variable a 4 η r t f 2 which in turn leads to the expression of the position of the front and of the thickness respectively as a 5 r n t η n t f 2 a 6 h r t t f 3 f η where ηn is the η value at the front r r n t the function f η may be recast as f η η n f 5 ψ ζ via the introduction of the normalized similarity variable ζ η η n where a 7 f 5 n 1 2 β 2 and ψ ζ is the thickness profile substituting f η in a 6 gives a 8 h r t η n f 5 t f 3 ψ ζ and adoption of the latter expression for the thickness transforms i eq 10 into the ode 13 ii the condition 12 into 15 iii the boundary condition 5 into 14 these three equations are reported in the main body of the manuscript supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2018 03 008 appendix b supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
828,we present an investigation on the combined effect of fluid rheology and permeability variations on the propagation of porous gravity currents in axisymmetric geometry the fluid is taken to be of power law type with behaviour index n and the permeability to depend from the distance from the source as a power law function of exponent β the model represents the injection of a current of non newtonian fluid along a vertical bore hole in porous media with space dependent properties the injection is either instantaneous α 0 or continuous α 0 a self similar solution describing the rate of propagation and the profile of the current is derived under the assumption of small aspect ratio between the current average thickness and length the limitations on model parameters imposed by the model assumptions are discussed in depth considering currents of increasing decreasing velocity thickness and aspect ratio and the sensitivity of the radius thickness and aspect ratio to model parameters several critical values of α and β discriminating between opposite tendencies are thus determined experimental validation is performed using shear thinning suspensions and newtonian mixtures in different regimes a box filled with ballotini of different diameter is used to reproduce the current with observations from the side and bottom most experimental results for the radius and profile of the current agree well with the self similar solution except at the beginning of the process due to the limitations of the 2 d assumption and to boundary effects near the injection zone the results for this specific case corroborate a general model for currents with constant or time varying volume of power law fluids propagating in porous domains of plane or radial geometry with uniform or varying permeability and the possible effect of channelization all results obtained in the present and previous papers for the key parameters governing the dynamics of power law gravity currents are summarized and compared to infer the combinations of parameters leading to the fastest lowest rate of propagation and of variation of thickness and aspect ratio keywords gravity current self similar non newtonian experiment review 1 introduction the propagation of gravity driven flows in porous media is but a chapter of the fascinating book on gravity currents hereinafter gcs which has received considerable attention ungarish 2009 with new chapters being continuously added yet also porous gcs by themselves originating from such diverse applications carbon dioxide sequestration mining engineering environmental pollution and remediation seawater intrusion to name but a few constitute such a wide topic that a comprehensive summary is arduous in the authors view the recent advancements on gravity driven porous flow belong to two broad categories the first group of contributions has as a common feature the modelling of the spatial variations of properties and or of boundary conditions in natural geologic media and the description of their topographical features examples of such contributions are huppert et al 2013 sahu and flynn 2017 and ngo et al 2016 where heterogeneity is modelled via discrete layers or intrusions of finite extent islam et al 2016 who introduce explicitly small scale heterogeneity yu et al 2017 who account simultaneously for drainage from a permeable substrate and an edge and huber et al 2016 who aim at reproducing the effect of diverse co2 injection strategies the second broad group of gc themed contributions presents an improved description of fundamental mechanisms via a more sophisticated modelling some relevant examples are the effects of a change in flux ball et al 2017 or of stratification in an intruding current pegler et al 2016 the investigation of the co2 sequestration mechanisms into deep saline aquifers involving two phase flow guo et al 2016 or with possible background hydrological flow unwin et al 2016 the interactions between gravity currents and convective dissolution elenius et al 2015 or geomechanics bjornara et al 2016 the adoption of realistic rheological models in the study of non newtonian gcs di federico et al 2017 some recent contributions belong to both categories and are associated for example with the modelling of co2 sequestration ngo et al 2016 or the simultaneous presence of non newtonian flow and spatial heterogeneity or specific topographical features the latter topic has been investigated in depth in several papers considering deterministic heterogeneity and radial di federico et al 2014 or plane geometry ciriello et al 2016 and channelized flow di federico et al 2014 the motivation for these studies lies in a multiplicity of applications involving complex fluids flowing in geologic media characterized by spatial heterogeneity at various scales oil and displacing suspensions in reservoir flow remediation carriers and liquid contaminants in the subsurface environments drilling and grouting fluids earlier works ciriello et al 2016 di federico et al 2014 list specific references to these applications studies of flows of non newtonian gcs rely on a body of knowledge accumulated for newtonian currents the reference works of huppert and woods 1995 for plane geometry and by lyle et al 2005 for axisymmetric geometry were extended to power law fluid flow by di federico et al 2012a b which in turn set the stage for the more complex setups cited earlier variations of properties along vertical and horizontal direction were considered in the context of newtonian gcs by zheng et al 2014 2013 while vertical variations mimic the effect of stratification horizontal variations may represent e g the effect induced by the drilling of a well and thus are of interest especially in the context of axisymmetric propagation a review of existing studies on non newtonian porous gcs reveals the lack of a study coupling power law rheology and permeability gradients along the flow direction in axisymmetric flow such a study is presented here in sections 2 5 considering the usual hypothesis of a gc of time variable inflow the exposition is organized as follows the mathematical problem is formulated in section 2 for a radial injection and solved in section 3 in self similar form generalizing the results of di federico et al 2012b section 4 discusses the dependency of key time exponents governing the propagation of the current on problem parameters along with the limitations imposed by modelling assumptions experimental results are presented in section 5 first the experimental set up is described with special attention on the difficulties implied by simulating heterogeneity then results from a series of tests are compared with the theory in constant and variable flux regime the theory and experiments presented complete a first picture on porous gravity currents of power law fluid flowing in different geometries plane and axisymmetric in domains exhibiting permeability variations in different directions vertical and horizontal taking into account the influence of the channel cross section for plane flow a general overview and comparison of these self similar solutions seems timely and is presented in section 6 concluding remarks are formulated in section 7 together with perspectives for future work 2 problem formulation consider a non newtonian power law fluid of density ρ consistency index m and flow behaviour index n that spreads axisymmetrically over a horizontal bed into a porous medium of height h 0 initially saturated with a lighter fluid of density ρ δ ρ see fig 1 under the sharp interface and thin current approximations and in the absence of capillary effects see the recent paper by chiapponi 2017 for an indication of the fluid retention in a glass beads porous medium the pressure within the current is hydrostatic and given by p r z t p 1 δ ρ g h r t ρ g z where r and z represent radial and vertical coordinates p 1 p 0 ρ δ ρ g h 0 is a constant p 0 is the constant pressure at z h 0 and g is gravity under the additional assumption that the current thickness is small compared to that of the ambient fluid the velocity of the latter and the vertical velocity in the intruding fluid can be neglected allowing to describe the current behaviour by means of its horizontal velocity u r t thickness h r t and maximum extension rn t for given time t the expression of the horizontal velocity can be deduced from the following general equation valid for the motion of a power law fluid in a porous medium cristopher and middleman 1965 1 p ρ g μ e f f k u n 1 u in which p is the pressure u is the darcy velocity field g is the gravity vector k the permeability and μeff is the effective viscosity dimensions ml n t n 2 the mobility μ e f f k is given by di federico et al 2012b 2 k μ e f f 1 2 c t 1 m n ϕ 3 n 1 n 50 k 3 ϕ n 1 2 where ϕ is the porosity and c t c t n the tortuosity factor the modified darcy s law 1 is based on a capillary bundle model first proposed by bird et al 1960 and later modified to include tortuosity for which different formulations are available e g shenoy 1995 in the following the formulation by pascal 1983 c t 25 12 n 1 2 is adopted macroscopic laws having the same structure of eq 1 were obtained via direct simulation at the pore scale by e g balhoff and thompson 2006 and vakilha and manzari 2008 experimental verification was provided among others by cristopher and middleman 1965 and yilmaz et al 2009 additional references on the use of eq 1 are reported in di federico et al 2012b the model is unable to handle viscoelastic effects and thixotropy and needs to be modified to include yield stress or newtonian behaviour at low shear rates local mass conservation implies that 3 1 r r r u h ϕ h t and in addition two boundary conditions are needed to formulate the problem the first b c is the global mass conservation condition 4 2 π ϕ 0 r n t r h r t d r q t α expressing the total volume of the current as a function of time t and parameters q dimensions l 3 t α and α this formulation includes the instantaneous injection with constant volume α 0 and the continuous injection with increasing volume α 0 the second boundary condition states that the thickness at the current front is null i e 5 h r n t t 0 further the horizontal permeability variation needs to be described the following law of variation is adopted for the medium permeability k ciriello et al 2013 zheng et al 2014 6 k r k 0 r σ r β where k 0 is a characteristic permeability r is a length scale σ is a coefficient introduced for convenience and β is a constant the coefficient σ is necessary in order to recover the dependency of the permeability only on the characteristics of the porous medium and assumes different values for different length scales r in order to keep the denominator σr independent on the fluid properties and on the injection power law for β 0 the permeability decreases or increases with the distance from the injection well respectively β 0 represents a medium with constant permeability k 0 and the simpler model of di federico et al 2012b is recovered for β 0 the behaviour of 6 is singular for r 0 but this does not affect the overall behaviour of the current further we require that β β 0 2 n 3 n 1 this upper limitation to the increase of the permeability with distance from the origin guarantees the validity of our solution from a theoretical point of view as demonstrated in section 4 1 for a newtonian fluid n 1 β 0 4 ciriello et al 2013 found β 0 3 in plane geometry for the two limit cases n 1 and n 1 very shear thinning or shear thickening fluids β 0 6 and β 0 2 respectively in a related study on newtonian gravity currents in hele shaw cells with a gap thickness varying in the flow direction zheng et al 2014 showed that the upper limit for the validity of the lubrication approximation is β 3 in terms of the present paper they then derived results for β 0 6 1 5 and 2 4 a range of values including the actual β value simulated in our experiments described in section 5 as far as field values are concerned realistic exponents for vertical power law variations of properties jiang et al 2010 marsily 1980 tend to be much lower than the upper limit value β 0 more importantly also horizontal variations of permeability are often modelled with negative exponential or decreasing power law functions to represent the steadily decreasing permeability altered by the drilling process of the aquifer around a large diameter well altunkaynak and sen 2011 slider 1983 or of the reservoir surrounding a fracture li et al 2017 in both cases there is a simplification of the actual behaviour where probably a constant and lower value of permeability is reached at a certain distance from the well or fracture substituting eq 6 in the one dimensional version of eq 1 and expressing the pressure gradient as a function of the unknown free surface as p r δ ρ g h r yields the following equation of motion 7 u r z t λ δ ρ g 1 n k 0 n 1 2 n r σ r β n 1 2 n h r 1 n 1 h r where 8 λ λ ϕ m n 1 2 c t 50 3 n 1 2 n 3 n 1 n ϕ n 1 2 m which for a newtonian fluid n 1 is the inverse of dynamic viscosity μ the mathematical problem constituted by eqs 7 and 3 with boundary conditions 4 and 5 may be rendered non dimensional upon defining time space and velocity scales as 9 t q ϕ v 3 1 3 α r v t v λ δ ρ g 1 n k 0 1 n 2 n ϕ σ β n 1 2 n and dimensionless coordinates as t t t r r r r n r n r and h h r note that the time scale t is defined for α 3 the particular case α 3 requires a partially different non dimensional formulation which can be easily derived following e g di federico et al 2012a vella and huppert 2006 for all other cases the dimensionless problem reads 10 1 r r r f 1 1 h h r 1 n 1 h r h t obtained by combining the dimensionless versions of 7 and 3 in eq 10 11 f 1 β n 1 2 n is a factor which reduces to zero in the homogeneous case the global mass balance 4 becomes 12 2 π 0 r n r h d r t α while the condition at the front 5 becomes h r n 0 in dimensionless form 3 solution it is desirable to obtain a self similar solution to the system formed by eqs 10 and 12 with 5 to capture the long term evolution of the current once the influence of initial and boundary conditions fades as illustrated in the appendix a first kind similarity solution for the extension and thickness of the current is derived in the form r n t η n t f 2 and h r t η n f 5 t f 3 ψ ζ where the thickness factor ψ ζ is the solution of the non linear ordinary differential equation 13 ζ f 1 1 ψ ψ ψ 1 n 1 f 2 ζ 2 ψ f 3 ζ ψ 0 in which the prime indicates d dζ and subject to the condition 14 ψ 1 0 while the similarity variable at the front of the current ηn is given by 15 η n 2 π 0 1 ζ ψ ζ d ζ 1 f 5 2 and the factors f 2 f 3 and f 5 are given by a 2 a 3 and a 7 respectively for a homogeneous medium β 0 results reduce to the simpler case of di federico et al 2012b with f 1 0 f 2 α n n 3 f 3 α n 1 2 n n 3 and f 5 n 1 for a newtonian fluid n 1 one obtains f 1 β f 2 α 1 4 β f 3 α 2 β 2 4 β and f 5 2 β when both simplifications apply the homogeneous newtonian case studied by lyle et al 2005 is recovered and f 1 0 f 2 α 1 4 f 3 α 1 2 and f 5 2 for the instantaneous injection case α 0 eqs 13 and 15 subject to 14 and ψ 0 0 the latter condition derives from a no flux boundary condition for r 0 valid for constant volume are amenable to the closed form solution 16 ψ ζ f 20 n f 5 1 ζ f 5 17 η n π f 20 n f 5 2 1 f 5 2 where f 20 f 2 α 0 2 n 2 n 3 β n 1 the constraint f 5 0 equivalent to β 2 applies when β 0 eq 17 of di federico et al 2012b is recovered when n 1 16 and 17 transform into 18 ψ ζ 1 4 β 2 β 1 ζ 2 β 19 η n 4 β 2 π 1 4 β finally when both n 1 and β 0 ψ ζ 1 ζ 2 8 and η n 2 π 1 4 pattle 1959 for the continuous injection case α 0 eq 13 needs to be integrated numerically with 14 and a second boundary condition is obtained expanding the solution in power frobenius series and balancing the lower order terms for ζ 1 this yields 20 ψ ζ 1 a 0 b ϵ b 1 a 0 f 2 n b 1 where ϵ 1 ζ is a small quantity and f 2 is non negative if β 2 n 3 n 1 integrating 13 with 14 and 20 with a runge kutta scheme yields the thickness profile ψ ζ and the similarity variable ηn sample results are shown in fig 2 a f for α 0 1 and selected values of n and β the analytical solution for α 0 and the results obtained by lyle et al 2005 for the newtonian homogeneous case are well reproduced the thickness profile ψ ζ increases with the injected volume α for given fluid and medium n and β is an increasing function of β and a decreasing function of n for constant volume currents the dependency on n for constant flux currents is more complex the prefactor ηn 15 whose value influences the current thickness via a 8 is illustrated in fig 2 g versus α for different n β ηn increases with n and decreases with α and β while its sensitivity is largest for smaller α and n and larger β these dependencies are reversed with respect to the thickness profile so that the dimensionless thickness results from the interplay of ψ and ηn other quantities of interest are the aspect ratio of the current the ratio between its average thickness h and radius rn and the average free surface gradient driving the motion h r these are given respectively by 21 h r n η n f 5 1 t f 3 f 2 ψ 22 h r η n f 5 1 t f 3 f 2 d ψ d ζ f 3 f 2 α 2 n β n 1 6 n 2 n 3 β n 1 where ψ and d ψ d ζ are respectively the average value of the thickness profile and of its derivative over the interval 0 1 furthermore the velocity field u is given in dimensionless form u u v by 23 u ϕ r f 1 η n f 5 1 n t f 3 f 2 n ψ ζ 1 n 1 ψ ζ while for the velocity of advancement of the front of the current v d r n t d t the dimensionless expression v v v is 24 v η n f 2 t f 2 1 with f 2 1 2 α 6 β n 1 n 1 2 β 4 4 discussion of results 4 1 behaviour of key time exponents the power law time exponents f 2 f 2 1 f 3 and f 3 f 2 eqs a 2 24 a 3 and 22 of the radius velocity thickness and aspect ratio of the gravity current are the key factors to understand the evolution of the current over time in the present section we explore their dependency on model parameters α the time rate of change of the fluid volume n the flow behaviour index and β the rate of change of the permeability along the direction of propagation by evaluating their sign and their partial derivatives with respect to model parameters the results obtained for f 2 together with f 2 1 f 3 and f 3 f 2 are listed in tables 1 3 respectively various limit values of α and in some instances of other parameters emerge each limit value is listed below the respective condition on f 2 f 3 f 3 f 2 or their partial derivative these threshold values of model parameters discriminate between a positive null or negative value of f 2 f 3 f 3 f 2 and of their partial derivative with respect to α n and β inspection of table 1 reveals that for a physically meaningful solution the permeability must decrease over space or increase not too sharply β βe for a newtonian fluid n 1 β e 4 the current front accelerates f 2 1 0 for any α under a sharp increase in permeability β βa or beyond a threshold value αa of α for permeability decreasing or increasing moderately over space β βa otherwise the current is decelerated for a newtonian fluid n 1 the threshold values reduce to β a 3 α a 3 β for a homogeneous medium β 0 and any fluid α a 3 moreover f 2 increases with α for any combination of β n as a larger fluid injection rate implies an increase in the current velocity regardless of the permeability variation and fluid nature similarly f 2 increases with β for any combination of α n as an increase less marked decrease of the permeability favours the current advancement the functional dependency of f 2 on n is more complex as the velocity of the current increases with n for any α under a sharp increase in permeability β βen or below a threshold value αen of α for permeability decreasing or increasing moderately over space β βen the current velocity decreases with n when combining a large injection rate α αen with a permeability decreasing or increasing moderately over space β βen for a newtonian fluid n 1 the threshold value of α reduces to α e n 3 inspection of table 2 shows that the thickness of the current increases with time at a given point f 3 0 only when a large injection rate α αt is combined with permeability decreasing or increasing moderately over space β βt for decreasing permeability the current encounters more resistance as it advances while for a moderately increasing permeability the decrease in medium resistance is more than compensated by the volume increase of the current in all other cases the thickness decreases over time and does so for any α when the permeability increase is marked for a newtonian fluid n 1 the threshold value of α reduces to α t 2 2 β for a homogeneous medium β 0 and any fluid α t 2 n n 1 furthermore it is noted that f 3 decreases or increases with α depending whether a threshold value αt is exceeded or not or equivalently depending whether the increase in the volume of the current prevails over the permeability increase along the flow direction the functional dependency of f 3 on n is the opposite of f 2 and the same threshold values due to mass balance finally f 3 decreases with β for any combination of α n as an increase less marked decrease of the permeability increases the radius of the current thus implying a decrease in thickness due to mass balance for the same reasons the dependence of f 3 upon n is the opposite of f 2 with the threshold value αtn being equal to αen inspection of table 3 indicates that the aspect ratio average spatial gradient of the current increases with time f 3 f 2 0 only when a large injection rate α αg is combined with permeability decreasing or increasing moderately over space β βg this behaviour can be understood noting that the average spatial gradient is proportional to the resistance encountered by the current in its advancement otherwise the aspect ratio decreases with time and the current grows progressively more elongated for a newtonian fluid n 1 the threshold values reduce to α g 2 2 β β g 1 for a homogeneous medium β 0 and any fluid α g 3 the dependence of f 3 f 2 on α is governed by a threshold value βgα for β βgα f 3 f 2 decreases with increasing α the reverse is true for β βgα this is so because unless the permeability increase is marked the aspect ratio of the current increases with the injection rate the threshold is β β g α 1 for a newtonian fluid the behaviour of f 3 f 2 as a function of n is analogous to f 2 with the same threshold values finally f 3 f 2 decreases with β for any combination of α n as a more permeable medium implies less resistance to the flow and a reduced average spatial gradient to visually illustrate the behaviour of the key exponent fig 3 a f depict how f 2 f 3 and f 3 f 2 depend on β for fixed n 0 5 and on n for fixed β 0 5 results for various values of α including the critical ones are shown the two reference values n 0 5 and β 0 5 are selected for illustrative purposes and represent common cases in natural porous media i e a shear thinning fluid and a permeability decreasing with distance from the source a comparison of the threshold values of α and β reveals that i for a homogeneous medium β 0 all threshold values of α coalesce into 3 except for αt ii for a newtonian fluid n 1 the threshold values of α are β dependent iii for newtonian flow in a homogeneous medium α t 1 a plot of the limit αg is shown in fig 4 for n 0 5 1 1 5 the limiting value of α increases with β the increase is more rapid for β 0 the influence of n on αg is mixed in that this limit value increases with n for β 0 and decreases with n for β 0 for a homogeneous medium the limit αg is independent of the behaviour index n 4 2 limits of validity limitations on the parameters emerge when considering the validity of model assumptions at any time conditions for the radius of the current to increase with time must hold f 2 0 as noted in the previous subsection furthermore for t 1 the thin current approximation requires the intruding current to be thin compared to both its height f 3 f 2 0 and the characteristic height h 0 of the porous medium f 3 0 otherwise at large times i the current thickness would exceed a reasonable portion of the porous domain total height rendering invalid the assumption of immobile ambient fluid ii the aspect ratio of the current would increase without bounds contrary to the assumption of negligible vertical velocities combining these limitations the parameters domain satisfying all model assumptions asymptotically the most restrictive condition is obtained an example is illustrated in fig 5 where the two limits βe and βg are depicted the first to ensure f 2 0 the second to ensure f 3 f 2 0 in all cases of practical interest n 3 the latter limitation is more stringent than the former it is seen that a too sharp increase in the permeability along the flow direction renders the current steeper with time the limit β value is 0 67 for n 0 5 and 1 for n 1 4 3 limitations of the model for in situ applications as to in situ applications there is still room to improve the connection between the present model and the field conditions the model is based on a monotonic permeability variation from the well to infinity porosity variations can be easily added and is not presently able to handle composite and more complex spatial variations when the permeability variation is due to fracturing rearrangement of grains during drilling or due to sealing or to mud injection in the medium a cutoff is expected at a certain distance from the well in addition in the latter cases the most relevant variations of permeability and porosity happen at a short distance from the well where the model itself is questionable due to several effects earlier highlighted however in other cases the permeability reduction is more gradual for example when it is associated to clogging of pore space resulting from deposition of fine material or escape of dissolved gases in water aquifers nevertheless the results are promising and indicate that further steps and advancements can be based on the present approach which can function as a benchmark solution for more complex situations more on this in the conclusions 5 laboratory experiments 5 1 experimental setup a series of experiments were conducted at the hydraulic laboratory of the university of parma to test the validity of the theoretical solution a 90 sector glass tank 25 cm 25 cm 25 cm in size was filled with transparent glass ballotini with nominal diameters of d 1 0 2 0 3 0 4 0 and 5 0 mm to reproduce a porous medium the continuous horizontal gradient of the permeability required by eq 6 was approximately reproduced by using a plastic framework that allowed to create separate neighbouring sectors each filled with beads of uniform diameter and having uniform permeability given by the kozeny carman equation the thickness of each sector was determined according to the procedure outlined in appendix b of di federico et al 2014 which provides the connection between the geometry of the stepwise distribution of diameters and the theoretical parameters k 0 and β of the continuous distribution 6 the plastic framework shown in fig 6 consists of thin plastic sheets 0 5 mm curved in order to reproduce four quarters of cylinder with radius equal to 3 2 cm 6 cm 9 cm and 12 2 cm with two radial diaphragms plane plastic sheets after filling the sectors with the beads the framework is gently removed by lifting it fig 7 shows the radial distribution of the diameters and the permeability for β 1 65 the diameters adopted for the beads are in the upper range for natural porous media this choice mainly reflects commercial availability and ease of sieving nevertheless the solution is applicable to porous media with grains of any size as long as the underlying assumptions are respected the horizontality of the bottom of the tank was checked by an electronic level the intruding current was a shear thinning fluid made of softened water water without cations like ca and mg glycerine and xanthan gum mixed in two different proportions i 40 vol of water 60 vol of glycerine and 0 10 weight of xanthan gum ii 95 vol of water 5 vol of glycerine and 0 15 weight of xanthan gum ink was added to the final mixture for an easy visualization and detection of the interface we used a commercial xanthan gum for food use from a local supplier and glycerine was added to increase the consistency index without adding too much xanthan gum the mixing was performed in a low speed stirrer by adding small quantities of xanthan gum to pure water and then adding glycerine after mixing lumps were removed with a small colander and the mixture was left at rest for several hours the overall result is that mixtures with the same ingredients but prepared in different days show different rheological parameters the rheological parameters flow behaviour index n and consistency index m were measured by a strain controlled rheometer dynamic shear rheometer anton paar physica mcr 101 with parallel plates roughened by sandpaper p 60 glued onto both smooth surfaces the distance between the plates was 1 mm and the testing temperature of the rheometer was t 25 c equal to the one measured in the laboratory during the experiments with expected fluctuations of 1 c the range of shear rate during measurements was chosen in order to overlap the range of shear rate expected during flow in the porous medium following the criterion reported in longo et al 2013b according to this criterion the effective shear rate should be evaluated at the pore scale by using e g the expression given by savins 1969 25 γ u 2 10 4 k ϕ c with u the darcian velocity and c 2 1 2 4 a coefficient related to tortuosity the result is a low effective shear rate in most part of the body of the current see longo et al 2013b fig 5 for an estimation of the shear rate in experiments similar to the present experiments indeed in some part of the current like the injection area the shear rate is much larger than in the body of the current however it has been experimentally demonstrated that the evolution of a viscous buoyancy gravity current is not influenced by the local disturbances near the inlet section see e g lyle et al 2005 fig 8 shows the stress strain measurements for two fluids adopted in the experiments with the interpolating power law function we bear in mind that the power law approximation hides a much more complex rheological behaviour of the mixture see e g amundarain et al 2009 zhong et al 2013 which is also influenced by ions and chemicals hence the power law is adopted as a pragmatic working tool for a simple and synthetic description of the local rheology of the fluid the intruding fluid was injected with a syringe pump into the tank through a quarter cylinder volume similar to a well having radius of 0 8 cm obtained with a brass net which was located in one corner of the tank this configuration reproduces an axisymmetric spreading due to the symmetry along the vertical axis and with negligible influence of the wall boundary layers the syringe pump was controlled by an analog electric signal to generate a constant α 1 or waxing α 1 5 2 0 influx rate during the injection the lateral current profile was recorded by a high resolution video camera canon legria hf 20 1920 1080 pixels working at 25 frames per second while the bottom view was reflected by a mirror and captured by a photo camera shooting every 2 s the videos and images were post processed using a software to transform the pixel positions into metric coordinates a grid stuck on the wall and on the bottom of the tank was used to reconstruct the correspondence between image and physical plane surfaces with the use of interpolating polynomials functions the position of the front of the current was detected by selecting the nose on the image and then converting the pixels coordinates into metric coordinates with an overall accuracy of 1 mm fig 9 shows two typical images of the side and bottom view during one of the experiments 5 2 experimental results and discussion a total of 10 experiments were performed with the experimental parameters summarized in table 4 the horizontal permeability is controlled by the value of β which was kept constant for all data sets while the injection rate α the fluid rheology m and n and the fluid density ρ varied among the tests fig 10 depicts the non dimensional front position rn of the current for the various tests compared with the theoretical prediction for most tests with the exception of a1 and a4 experimental results indicate a front position below the theoretical counterpart before reaching it asymptotically in all cases the good agreement between theoretical and experimental data over time asymptotically within 5 is due to the balance of buoyancy and viscous forces while the disturbing effects due to injection with significant vertical velocity influence the position of the front only at the beginning of motion the comparison between tests a4 and b3 which only differ in the type of fluid having respectively n 0 43 and n 0 57 leads to the conclusion that the more shear thinning fluid a4 best fits the theoretical model and this result holds true since the beginning of the test furthermore shear thinning fluids advance slower with decreasing values of n as shown upon comparing tests a5 and b4 for α 2 the results of tests a2 and a3 characterized by different values of q i e 2 4 and 4 0 cm 3 s 1 clearly show the same behaviour demonstrating that all other parameters being equal q is not relevant in the evaluation of the dimensionless front position rn indeed a little variation of density e g between tests b2 and b5 proves that the fluid density ρ significantly affects the dimensionless position of the front the comparison between the actual position of the front end among different experiments is best performed in dimensional form as the time and velocity scales are function of experimental parameters which differ among the tests conducted fig 11 a and b shows the shape of the current at different times for two experiments with constant and waxing influx rate respectively the agreement between experiments and model is fairly good in particular at late times near the origin the experimental shape of the current is below the theoretical profile even though this effect does not affect significantly the front end position and the shape of the main body 6 overview on non newtonian gravity currents in porous media the present section is devoted to an overview of self similar solutions governing the propagation of non newtonian currents of variable volume with power law rheology in porous media the overview is performed by comparing the key parameters governing the propagation i e f 2 f 3 and f 3 f 2 equal to the time exponents of the extension thickness and slope of the current derivation of the exponent of the velocity of the front end of the current f 2 1 is trivial for a variety of combinations of geometries and laws of variation of properties for the case covered in the present paper radial propagation in an horizontally heterogeneous media f 2 f 3 and f 3 f 2 are reported in eqs a 2 a 3 and 22 respectively results for other geometries and or laws of variation were derived in previous papers ciriello et al 2016 di federico et al 2012a 2012b 2014 longo et al 2015 always with the parameter α equal to the time exponent of the volume of the current table 5 covers results for plane geometry the base unbounded case di federico et al 2012a is compared to the channelized case of parameter κ longo et al 2015 to vertical heterogeneity of parameter ω ciriello et al 2016 and to horizontal heterogeneity of parameter β ciriello et al 2016 see the table caption for additional details table 6 lists results for radial geometry the base case di federico et al 2012b is compared to vertical heterogeneity of parameter ω di federico et al 2014 and to horizontal heterogeneity of parameter β the present paper again see caption for details fig 12 depicts the behaviour of each key parameter for the homogeneous case as a function of geometry n and α for all cases analyzed the radial geometry implies lower values of all key parameters with the exception of a continuous injection of very shear thinning fluids in narrow cross sections for an instantaneous fluid release α 0 an increase of the rheological parameter n in radial geometry leads to f 2 values lower than other geometries due to mass balance considerations among the plane cases f 2 tends to decrease as the shape factor κ increases tending to the unbounded case κ as the volume of the current remains constant and the front of fluid is forced to move further for lower κ in constant flux regime α 1 radial geometry and n 0 5 f 2 behaves as in the constant volume regime while for plane geometries it shows an opposite behaviour i e f 2 increases with higher values of κ in constant volume regime α 0 the parameter f 3 is negative for all the analyzed geometries in general this exponent tends to decrease when moving to plane unbounded geometry in constant flux regime f 3 is negative only for radial geometry and dilatant fluids n 1 whilst in plane geometries f 3 tends to increase with the shape factor κ as does f 2 for all geometries the parameter f 3 f 2 is always negative for α 1 because of the higher limit of validity for shear thinning fluids the parameter reaches lower values in constant volume regime α 0 and is larger for plane than for radial geometry the influence of κ on results is more limited as κ increases fig 13 illustrates the trend of parameters f 2 f 3 and f 3 f 2 considering vertical permeability variations in plane and radial geometry the homogeneous case with ω 1 ciriello et al 2016 di federico et al 2014 is depicted in fig 12 in both plane and radial geometry for an instantaneous fluid release α 0 and ω 1 f 2 is higher than the homogeneous case depicted in fig 12 whilst it is lower if ω 1 this trend changes for a constant flux regime α 1 for ω 1 and plane geometry f 2 is lower than the homogeneous case and it becomes higher if ω 1 in the radial case for ω 1 f 2 is lower than the homogeneous case but only for a shear thinning fluid n 1 while it becomes higher for a dilatant fluid n 1 on the contrary if ω 1 f 2 shows an opposite behaviour concerning the parameter f 3 in constant volume regime α 0 and both geometries this parameter is lower than the homogeneous case for ω 1 while it becomes higher if ω 1 for continuous injection α 1 in plane unbounded geometry f 3 is higher than the homogeneous case if ω 1 and it reverses its behaviour with ω 1 in radial case for ω 1 f 3 is higher than homogeneous case only for a shear thinning fluid n 1 while it becomes lower for a dilatant fluid n 1 on the contrary if ω 1 f 3 has an opposite trend for an instantaneous release α 0 in both geometries f 3 f 2 is lower than the homogeneous case for ω 1 while it reverses its behaviour if ω 1 for continuous injection α 1 in plane unbounded geometry f 3 f 2 is higher than the homogeneous case if ω 1 reversing for ω 1 in radial geometry instead the behaviour is similar to f 3 for both geometries independently on vertical permeability variations the deviation between homogeneous fig 12 and heterogeneous values fig 13 tends to increase if n increases for α 0 and it decreases for constant injections only in plane geometry fig 14 depicts the behaviour of parameters f 2 f 3 and f 3 f 2 considering horizontal permeability variations in plane and radial geometry the homogeneous case β 0 is depicted in fig 12 ciriello et al 2016 in both geometries and regimes i e constant volume and constant flux for β 0 f 2 is lower than homogeneous case while its behaviour is reversed if β 0 for β 0 the parameter f 3 is higher than the homogeneous case for all geometries and regimes whilst it becomes lower if β 0 finally f 3 f 2 follows the same trend of f 3 for both releases geometries and horizontal permeability variations the deviation between homogeneous and heterogeneous case tends to increase for higher n values 7 conclusions we have presented a novel model describing the propagation of axisymmetric power law gcs in porous media with an horizontal permeability variation the problem is amenable to a self similar solution of the first kind yielding the position of the front end and the thickness of the current as functions of dimensionless parameters describing the volume of the gc α the fluid rheological behaviour n and the power law permeability variation along the horizontal coordinate β depending on the value of β the permeability increases or decreases with the distance from the origin in the latter case this conceptual simplification captures the essential behaviour of the radial variation of permeability around a well with the additional convenience of an easy to implement self similar solution which can be used as a benchmark for numerical modelling the special case of constant volume currents has a closed form solution the behaviour of key time exponents governing the rate of propagation thickness and aspect ratio of the current was discussed in detail yielding a number of threshold value of model parameters α and β which discriminate between opposite trends in the behaviour of the current over time and govern the sensitivity to model parameters themselves in turn these parameters allow to discriminate the conditions for the validity of our solution at large times a specific laboratory set up was devised to directly reproduce horizontal permeability variations overcoming the difficulties inherent in the horizontal juxtaposition of layers of glass beads of different diameter theoretical results were confirmed by our experiments with a fairly good agreement except for the early time regime it is confirmed that also in presence of a deterministic spatial variation of permeability disturbances or anomalies near the injection line and near the front of the current do not affect the current evolution in the intermediate asymptotic regime also the discretization of the porous medium to mimic a continuous variation of permeability and capillary effects do not significantly affect the behaviour of the gcs at least for constant influx rate condition α 1 more important disturbances are expected for constant volume experiments α 0 and in general for waning gcs in real applications model parameters are obtained as follows i rheological fluid parameters n and m need to be determined experimentally bearing in mind that the power law model is an approximation of the real fluid behaviour ii the strength of the injection α depends on its type which is usually known and is equal to 0 or 1 for instantaneous or continuous injection iii the parameter β reflecting the intensity of the permeability variation needs to be determined experimentally on the basis of available measurements at different locations note that two measurements k 1 and k 2 at two locations r 1 and r 2 allow the determination of β by means of eq 6 the theory and experiments herein presented complete a first picture on porous gravity currents of power law fluid flowing in plane and axisymmetric geometry the reference solutions are derived by di federico et al 2012a for plane and by di federico et al 2012b for radial flow the influence of channel shape on plane flow is covered in longo et al 2015 heterogeneous deterministic variations of properties are examined by ciriello et al 2016 considering vertical and horizontal grading in 2 d flows and by di federico et al 2014 considering vertical grading in radial flow horizontal grading is covered in the present paper an overview of the key time exponents for these cases revealed the combination of geometries and model parameters yielding the fastest lowest currents and those having the fastest decrease of thickness and aspect ratio over time our study has several connections to geological flows and industrial flows including flows during fracking procedures shale gas recovery drilling wells and may be relevant for co2 sequestration as solvents which proved effective in co2 capture exhibit shear rate dependent viscosities sze et al 2014 in all these applications fluids exhibiting non newtonian effects often approximated by power law fluids are used almost always in heterogeneous porous media at the pore scale it is worth noting that the effect of heterogeneity prevails over the non linearity due to rheology in shaping the flow pattern zami pierre et al 2016 with a relatively minor influence of the specific rheological equation chevalier et al 2014 it remains an open question whether this is true at darcy s scale in sum several avenues of investigation remain open in the area of non newtonian gcs e g inclusion of fluid drainage injection at the bottom of the current either distributed or concentrated in single multiple fissure s inclusion of stratification effects in the advancing current consideration of more complex permeability variations including cutoffs and discontinuities in the medium properties e g inclusions adoption of more realistic rheological models to describe complex fluids such as carreau or truncated power law stochastic modelling of heterogeneity we are investigating these fascinating topics and hope to report on them in the near future acknowledgements we thank luca chiapponi for assistance with the laboratory experiments appendix a self similar solution inspection of eq 10 yields the following time scalings for the length r and thickness h of the current a 1 r t f 2 h t f 3 where a 2 f 2 2 α n 2 n 3 β n 1 a 3 f 3 α n 1 2 β 4 n 2 n 3 β n 1 this suggests the adoption of the similarity variable a 4 η r t f 2 which in turn leads to the expression of the position of the front and of the thickness respectively as a 5 r n t η n t f 2 a 6 h r t t f 3 f η where ηn is the η value at the front r r n t the function f η may be recast as f η η n f 5 ψ ζ via the introduction of the normalized similarity variable ζ η η n where a 7 f 5 n 1 2 β 2 and ψ ζ is the thickness profile substituting f η in a 6 gives a 8 h r t η n f 5 t f 3 ψ ζ and adoption of the latter expression for the thickness transforms i eq 10 into the ode 13 ii the condition 12 into 15 iii the boundary condition 5 into 14 these three equations are reported in the main body of the manuscript supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2018 03 008 appendix b supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
829,in recent years proper orthogonal decomposition pod has become a popular model reduction method in the field of groundwater modeling it is used to mitigate the problem of long run times that are often associated with physically based modeling of natural systems especially for parameter estimation and uncertainty analysis pod based techniques reproduce groundwater head fields sufficiently accurate for a variety of applications however no study has investigated how pod techniques affect the accuracy of different boundary conditions found in groundwater models we show that the current treatment of boundary conditions in pod causes inaccuracies for these boundaries in the reduced models we provide an improved method that splits the pod projection space into a subspace orthogonal to the boundary conditions and a separate subspace that enforces the boundary conditions to test the method for dirichlet neumann and cauchy boundary conditions four simple transient 1d groundwater models as well as a more complex 3d model are set up and reduced both by standard pod and pod with the new extension we show that in contrast to standard pod the new method satisfies both dirichlet and neumann boundary conditions it can also be applied to cauchy boundaries where the flux error of standard pod is reduced by its head independent contribution the extension essentially shifts the focus of the projection towards the boundary conditions therefore we see a slight trade off between errors at model boundaries and overall accuracy of the reduced model the proposed pod extension is recommended where exact treatment of boundary conditions is required keywords model reduction proper orthogonal decomposition boundary condition treatment 1 introduction the building and application of groundwater models is one of the major fields supporting modern groundwater management as these models often couple well understood physical principles with insufficiently known and highly spatially variable aquifer properties computerized calibration of parameters and robust uncertainty analysis of model predictions are mandatory for meaningful interpretation of the results unfortunately calibration and uncertainty analysis often require many model runs modern advances in computer performance and parallelization can alleviate these run times only to a limited extent over the years many researchers tackled this problem with model reduction many different methods have been applied to reduce run times of physically based groundwater models usually model run time reduction comes at a cost of model accuracy balancing this trade off has been the main focus of research model reduction techniques in groundwater can be roughly put into three categories data driven methods projection based methods and structural reduction methods the data driven methods include artificial neural networks taormina et al 2012 certain forms of polynomial chaos expansion oladyshkin and nowak 2012 or bayesian networks fienen et al 2013 the most prominent projection based method is proper orthogonal decomposition pod vermeulen et al 2004b mcphee and yeh 2008 or siade et al 2010 for example followed by the fourier model reduction willcox and megretski 2005 structural reduction methods directly reduce the size of the model discretization or simplify the representation of the processes von gunten et al 2014 structural simplification can go along with parameter reduction like doherty and christensen 2011 did with the inversion based upscaling approach proper orthogonal decomposition projects the discretized model equations onto a subspace usually by the galerkin method the subspace is mostly defined via singular vectors of simulations of the full model for pre defined times and forcings so called snapshots see winton et al 2011 for an example of pod in groundwater not based on snapshots recent research in pod for groundwater models focused on non linear reduction e g stanko et al 2016 snapshot selection e g siade et al 2010 or inverse modeling boyce and yeh 2014 or ushijima and yeh 2015 for example but did not focus on the effects of pod on the boundary conditions of the system the full physically based groundwater model depends on the statement of and adherence to certain boundary conditions while the pod method incorporates the original model equations it does not explicitly adhere to the original boundary conditions the approximation of groundwater heads in the reduced order model rom obtained by pod may lead to significant errors in boundary heads or fluxes this is especially relevant if the rom is used to predict boundary fluxes groundwater discharge into rivers or low land springs for example this problem has been analyzed in other research areas gunzburger et al 2007 developed a method to treat dirichlet boundaries explicitly by modifying the pod subspace to be orthogonal to the boundary conditions this leads to an underdetermined system of equations that is then filled up with the corresponding projections of the original boundary equations cosimo et al 2016 built on this work by proposing different methods of building the rom but still only with respect to dirichlet boundaries we transfer the idea proposed by gunzburger et al 2007 into the area of groundwater modeling furthermore we significantly extend the method now allowing the explicit treatment of all three types of boundary conditions dirichlet neumann and cauchy in pod reduction of groundwater models we analyze the performance of the new method with several test cases section 3 and study the trade off between overall model performance and accuracy at the boundary conditions section 4 2 methods in this section we will first give a short summary on the mathematical basis of the groundwater models to which pod is typically applied second we briefly revisit the common pod method we will then develop the extension of pod for explicit boundary treatment eb pod and explain the implications of its use finally we will present different synthetic models used in this study to depict and test the new method as well as the chosen methodology for snapshot selection 2 1 full groundwater model in this work we focus on groundwater models based on the groundwater flow equations these equations have been presented in numerous studies pinder and celia 2006 thus we only summarized them here as required for developing the new method more in depth formulations can be found in the modflow manual harbaugh 2005 for example 3d groundwater flow is usually described by the following partial differential equation based on darcy s 1856 law 1 x k x x h x y k y y h y z k z z h z w s h t with k being hydraulic conductivity l t h the piezometric head l w a source sink term 1 t and s the storage term 1 l the solution of this differential equation requires initial and boundary conditions initial conditions are defined by specifying the head h x y z 0 f 0 x y z at time zero for the boundary conditions three different types are most common 2 h f 1 t x y z dirichlet type i n h f 2 t x y z neumann type ii n h c t x y z h f 3 t x y z cauchy type iii where x y z n is the normal vector to the boundary and f 1 2 3 and c can be functions in space and or time conceptually dirichlet boundary conditions represent specified heads neumann boundaries are specified fluxes and cauchy boundaries are specified leakages spatial and temporal discretization of eq 1 and its boundary conditions e g by finite differences finite volumes or finite elements leads to the following linear system of equations provided here for implicit time stepping in matrix notation 3 r l t h t 1 r h t t q where r f s l f k and q f w after renaming for convenience r l t a and r h t t q b eq 3 becomes 4 a h t 1 b this system of linear equations is solved for every time step the size of the system of equations depends on the number of model cells nm as follows a ϵ r n m n m and b h ϵ r n m 1 the boundary conditions from eq 2 can be found explicitly in boundary related rows of the system in eq 4 the solution of the equations for groundwater levels h requires iterative manipulation of a a computationally and therefore time demanding process 2 2 proper orthogonal decomposition pod the pod method aims to reduce the size of eq 4 by projection onto a suitable subspace this subspace is created by singular value decomposition of a collection of previously collected system states so called snapshots these snapshots are built by calculating the vectors of groundwater heads h at specific times t s t s 1 t s 2 t s n s in the full model and saving them in the matrix h s h s 1 h s 2 h s n s the number of snapshots ns results in h s ϵ r n m n s the process of snapshot selection is detailed further in section 2 5 now the snapshot matrix h s is treated by singular value decomposition 5 h s u σ v t resulting in the matrix of left singular vectors u ϵ r n m n m the diagonal matrix of singular values σ ϵ r n m n s and the matrix of right singular vectors v ϵ r n s n s the projection matrix p u 1 u 2 u n r is a selection of left singular vectors from u using the singular values σ as selection criteria similar to a principal component analysis the number of retained singular values nr is chosen as follows 6 i 1 n r σ i i 1 r a n k s σ i 100 rv where rv is a chosen specific relative retained variance i e of 99 99 since singular values are ranked in descending order this assures that the chosen rv of the snapshot set is achieved with the lowest possible number of singular vectors nr is the size of the reduced model now the galerkin 1968 method is used to project eq 4 onto the subspace spanned by the columns of p 7 p t a p r t 1 p t b solving eq 7 is much more time efficient compared to the original eq 4 because nm nr the groundwater heads in the original model are calculated from this as follows 8 h p r this approximation captures the rv of the dynamics represented by the snapshots in h s however no explicit lines can be found anymore that enforce the boundary conditions 2 3 explicit boundary pod 2 3 1 general form the original eq 4 handles boundary conditions explicitly to assure the correct solution in affected cells this does not hold for the reduced model in eq 7 where boundary conditions are only approximated due to the projection onto the subspace we propose an explicit boundary pod method eb pod which modifies the original pod equations to enforce the boundary conditions in the reduced model the simplified handling of dirichlet boundaries section 2 3 2 is taken directly from gunzburger et al 2007 in this contribution we generalize their approach for the application to all three types of boundaries see eq 2 which is subsequently presented in our new method we replace part of the reduced model with the equations for the original boundary conditions for this we need to modify the projection matrix p of the reduced model to remove the approximation regarding these boundaries first we collect the rows of p corresponding to the independent boundary conditions bc we want to treat explicitly into n independent boundaries means that for several rows and therefore model cells that are all subjected to the same forcing for example by representing the same constant head boundary along a model edge only one row has to be collected into n 9 n p b c in our notation the first index of a matrix stands for the position of rows while the second index stands for the position of columns p bc therefore denotes a subset of p with the bc rows and all columns respectively likewise q n b c 1 n r denotes a subset of q of all rows and n b c 1 to nr columns the transpose of matrix n is now decomposed by qr factorization 10 q r q r n t the columns of the resulting matrix q of size nr nr form an orthonormal basis for n the number of independent boundary conditions nbc must be smaller than the reduced model size nr then the n b c 1 n r columns of q hold the coefficients of the linear combinations of p that are independent of the boundaries as the 1 nbc columns of q retain all information regarding the boundaries multiplication of these n b c 1 n r columns of q with p leads to 11 ψ pq n b c 1 n r the matrix ψ has the size nm n r2 where n r 2 n r n b c it contains linear combinations of the pod basis functions that vanish at the boundaries thus ψ is a reduced version of p that is cleaned from the inaccurate boundary related information contained in the original projection matrix we can now substitute eq 11 into eq 7 12 ψ t a p r t 1 ψ t b because the size nr of p and r is larger by nbc than the number of columns n r2 of ψ eq 12 is underdetermined as ψ is boundary independent and the number of missing equations is nbc we can now add the accurate boundary conditions from the original model 13 a b c h t 1 b b c as eq 12 works with the reduced solution r instead of h we use the same approximation eq 8 in eq 13 14 a b c p r t 1 b b c combining the new reduced model eq 12 with the reduced form of the boundary eq 14 leads to the final system of equations for the eb pod reduced model 15 ψ t a p a b c p r t 1 ψ t b b b c where the approximation of groundwater heads is again h pr 2 3 2 dirichlet boundaries due to the way dirichlet boundaries are implemented into groundwater models their handling with the eb pod simplifies the eq 15 for dirichlet boundary cells dc the corresponding system matrix diagonals a dc dc are 1 while the off diagonal elements are 0 this leads to h t 1 d c b d c where b dc is the fixed groundwater head at the dirichlet boundary this simplifies a bc p of eq 15 for the dirichlet boundaries leading to 16 ψ t a p a n c p p d c r t 1 ψ t b b n c b d c where nc denotes the explicitly treated boundaries that are not of dirichlet type neumann and cauchy boundaries and dc represents the dirichlet boundary cells if only dirichlet boundaries are to be handled by eb pod eq 16 simplifies to the method presented in the field of applied mechanical engineering by gunzburger et al 2007 therefore eq 16 is the generalization of that method to the three featured boundary condition types again the approximation of groundwater heads is still h pr note that dirichlet boundaries can potentially be handled correctly by standard pod with several restrictions to the rom building process the model must simulate drawdown instead of groundwater heads for drawdown dirichlet boundary conditions are always zero thus not affected by the subspace projection of pod as has been shown by siade et al 2010 groundwater heads and fluxes can be calculated from the drawdown simulations by simple superposition of drawdown with the groundwater heads without pumping which are subsequently termed pseudo steady state however this can only be applied to linear groundwater models dirichlet boundary cells must be omitted in the calculation of the pod rom by excluding its rows from the snapshot matrix h s and thus from p instead these cells are set to the correct values manually this is sometimes done in groundwater pod in general presumably for better accuracy at the dirichlet boundaries as is stated in vermeulen et al 2004a preprocessing of the snapshots is necessary see section 2 5 to compute drawdowns by subtraction of the pseudo steady state for variable dirichlet boundaries this pseudo steady state heads are time dependent and have to be computed for every time step thus an additional full model run without pumping is necessary offline to create the corresponding pseudo steady state the additional computational requirements reduce the efficiency of the pod model reduction these limitations in the handling of dirichlet boundaries are overcome by our proposed eb pod method 2 3 3 neumann boundaries neumann boundaries are directly handled by eb pod as shown in section 2 3 1 eb pod allows the explicit treatment of the neumann boundaries in the rom this ensures the correct boundary fluxes in the rom in contrast to standard pod where they are approximated 2 3 4 cauchy boundaries the head dependency of cauchy boundaries has noteworthy effects on the handling with eb pod while all information regarding dirichlet and neumann boundaries is found in the right hand side of eq 16 in the respective rows of the b vector cauchy boundaries are different as explained below on the example of a river hydraulically connected to the aquifer the flow from a connected river into the underlying cell is calculated in modflow for example as q r i v c r i v h r i v h where qriv is the flow l3 t into the cell from the river criv is the conductance l2 t of the river bed and hriv is the head in the river l this can be rewritten as q r i v c r i v h r i v c r i v h resulting in a groundwater head independent part crivhriv that is added to the right hand side b of eq 16 as for neumann boundaries and a head dependent part c r i v h that is added to the diagonal of the left hand side a of eq 16 the eb pod method adds the original boundary equations to the reduced equation system in eq 16 but does so in the reduced space this means that fluxes through head dependent boundaries i e cauchy boundaries are not computed error free in the eb pod model while the head independent part stored in b is correct and error free through its enforcement similar to a neumann boundary in eb pod the head dependent part in a is calculated from the approximated heads ah apr and is therefore not entirely error free this is demonstrated in section 3 4 2 3 5 implications of eb pod looking at the left hand side matrix of eq 15 several things are important first the overall size of the eb pod reduced order model does not change compared to the pod model both handle full matrices of size nr nr to find the solution for each time step thus the explicit treatment of boundary conditions in eb pod comes at no additional computational cost compared to the original pod method what changed in size however are the corresponding projection matrices while the original pod matrix p ϵ r n m n r uses the information of nr singular vectors to span the reduced model subspace the projection matrix ψ ϵ r n m n r 2 retains less information since n r 2 n r n b c practically this means that the reduced model size nr has to be chosen larger than the number of independent boundary conditions nbc treated this way replacing original pod rows by boundary rows reduces in theory the accuracy of the solution within the domain however the accurate representation of the boundaries typically counteracts this loss of accuracy or forces the user to decide on the relative importance of different model aspects this potential trade off will be analyzed further in the following sections another potential implementation of the eb pod method would be the addition of the boundary rows to the original pod projection matrix let the original pod matrix be p o r i g ϵ r n m n r now we create a new bigger projection matrix p b i g ϵ r n m n b i g following the procedure detailed in section 2 2 but retaining as many additional singular values as we have independent boundaries n b i g n r n b c we subject this matrix p big to the steps detailed in section 2 3 1 resulting in a new eb pod projection matrix ψ a d d ϵ r n m n r 2 note that for this matrix n r 2 n b i g n b c and thus n r 2 n r this means there is no loss of information within the domain by going from p orig to ψ add application of this alternative implementation not shown proved that this is indeed the case and the accuracy of the eb pod model within the domain is in agreement with the original pod model while retaining the feature of accurate boundary representation it is important to note though that the overall size of the eb pod rom is n r 2 n b c and thus nbc bigger than the original pod rom therefore this new eb pod model is while more accurate slower than the original pod model thus the question of subtraction or addition of the nbc boundary condition rows to the rom is the same trade off between run time vs accuracy as the original choice of the reduced model size nr for the sake of comparison between pod and eb pod it seemed more practical if the reduced model size for both methods is the same thus fixing run times and exploring the trade off in accuracy instead of in run time 2 4 synthetic model setup 2 4 1 one dimensional 1d models to demonstrate our new method we use four simple synthetic 1d models with different types of transient boundary conditions as well as a more complex synthetic 3d model while differing in their boundary conditions all four 1d models have the same geometry time settings and aquifer properties following earlier pod benchmarking studies siade et al 2010 we set up 1d confined groundwater models that consist of 1 row 100 columns and 1 layer a total of 100 model cells all model cells are 100 m wide δx 1 m deep δy and 100 m high δz the synthetic aquifer s dimensions are therefore 10 000 1 100 m the model simulation time is one year with daily time steps leading to 365 time steps two zones of hydraulic conductivity are used to incorporate some complexity and asymmetry into the model two thirds of the model domain cells 1 67 have hydraulic conductivity values of 1500 m d while the rest of the domain cells 68 100 has a hydraulic conductivity of 500 m d the entire model domain has a specific storage value of 0 005 1 m fig 1 shows the different 1d model schematics with the boundary conditions and hydraulic conductivity zones table 2 summarizes the differences of the four 1d models and states which boundary conditions are treated explicitly by the eb pod method the pumping regime of all models can be seen in table 1 the first 1d model termed m1 serves as a simple proof of concept for the method the model is set up analogous to the 1d models used in groundwater pod literature see for example mcphee and yeh 2008 the left and right model edges are set as constant head boundaries at 1 and 2 m respectively the initial groundwater head follows the linear gradient interpolated between these two constant head boundaries a single pumping well is added to cell 33 x 3250 m with a pumping rate of 100 m3 d which is turned on in the interval t p 100 250 days for this model only the single neumann boundary cell the pumping well is treated explicitly by eb pod the second 1d model m2 is more complex here we replace the left constant head boundary with an unconnected river neumann type boundary with a hydrograph taken with slight adjustments from a real world river see fig 4 e the initial groundwater head is identical to the remaining constant boundary on the right model edge for the entire domain furthermore a second pumping well is added to the model domain at cell 67 x 6650 m and both wells are pumped with changing pumping rates and partly separate partly overlapping pumping regimes ranging from 150 m3 d to 250 m3 d in this model all three neumann boundaries the river and both pumping wells are treated explicitly by eb pod m3 the third 1d model is used to show the application to a variable dirichlet boundary the model set up is similar to m1 but the left boundary is changed to a variable head simulating a seasonal head change with a sine function with period 1 year and amplitude of 1 m only this variable dirichlet boundary is handled explicitly by eb pod the last 1d model m4 is similar to m2 but connecting the river to the aquifer which then is a cauchy type boundary as in m2 both river and pumping wells one cauchy two neumann boundaries are explicitly treated by eb pod 2 4 2 three dimensional 3d model a fifth synthetic test model m5 is presented here to show the applicability of the method for a more complex model m5 is a 3d confined groundwater model with 30 rows 100 columns and 5 layers i e a total of 15 000 model cells the size of the model cells is 10 10 m horizontally δx and δy respectively and 5 m vertically δz resulting in aquifer dimensions of 1000 300 50 m model simulation time is again one year with daily time steps 365 hydraulic conductivity is assumed to be different for odd and even numbered layers resulting in four zones labeled a d with different hydraulic conductivity values in x y and z direction fig 2 and table 3 the specific storage value of the entire model domain is assumed to be 0 005 1 m with respect to the definition of the boundary conditions model m5 follows the build of m2 with a hydraulically unconnected river neumann boundary at the left model domain fig 2 exhibiting the same hydrograph as used in m2 the right model boundary is a constant head of 0 m the initial groundwater head is set to 0 m across the entire model domain two pumping wells are added in layer 5 fig 2 pumping rates and intervals for these wells are listed in table 1 as before all three neumann boundaries one river two pumping wells are treated explicitly by eb pod as can be seen in table 2 2 5 snapshot selection technique snapshot selection is an important factor in pod based reduced order modeling and has been investigated in the literature cf siade et al 2010 the aim of this study however is the comparison of standard pod and an extension to that method the eb pod we assume that this comparison is meaningful regardless of the snapshot selection method provided the applied process is following tested standard procedures and both pod and eb pod reduced order models use the same set of snapshots for each model on that basis we used a snapshot selection approach modified from gunzburger et al 2007 that can be described by the following three steps 1 identify all drivers of the groundwater system dirichlet boundaries neumann boundaries wells rivers or recharge for example cauchy type boundaries nd 2 compute a steady state system response to all time independent boundaries of the groundwater system constant heads for example h ini and add this steady state to the snapshot matrix h s h i n i 3 run the following loop for each driver i 1 n d a store the current set of snapshots as h s o l d h s b set the chosen driver to an arbitrary but reasonable value constant in time pumping in the well river head etc c turn off all other drivers set pumping to zero river head to river bottom variable head to constant etc d set h 0 h i n i e run the model until steady state is reached generating system responses i e groundwater heads at each time step t 0 n t d for this driver h i ϵ r n m n t d f append these responses to the overall snapshot matrix h s h s o l d h i while the original snapshot selection in gunzburger et al 2007 extends this approach by running the model with all time variable drivers turned on simultaneously after each step e of the loop our experiments revealed that these additional runs have no influence on the precision or stability of either pod or eb pod reduced order models results not shown since they add a significant portion to offline computing time however we chose to remove this step from our snapshot selection approach the chosen approach also follows general guidelines for snapshot selection for pod in groundwater modeling e g vermeulen et al 2004a mcphee and yeh 2008 in the literature one often finds several techniques for snapshot pre processing such as mean centering and normalization cf boyce et al 2015 we could not see any improvement of the pod reduced order models for our test cases by applying these techniques in contrary mean centering of the snapshots before applying the explicit boundary handling for eb pod resulted in an non functioning rom for our test cases this is because eb pod s handling of boundary conditions is dependent on the time variant information contained in the snapshot set that mean centering removes the snapshot selection approach described above was used for all different test models the time for each driver model run to reach steady state ntd was chosen to be 100 days table 4 summarizes the number of drivers in the system nd resulting snapshot numbers and reduced order model sizes with a chosen retained variance rv of 99 99 note that the number of explicitly treated boundaries nbc is not the same as the number of drivers in the system nd as only specific boundaries were handled explicitly by eb pod as described in section 2 4 for model m5 n b c 3 one river and two pumping wells even though the river spans the whole left hand side of the first layer 30 cells since all river cells are subjected to the same boundary forcing though only one single row of the original projection matrix has to be collected for the eb pod handling of the river 3 results in this section we apply the eb pod method to the four 1d models and the 3d model outlined in section 2 4 we compare original pod and eb pod in their precision for the original boundary conditions and the general approximation of the groundwater head fields note that we will provide run times only for the 3d model as the 1d models are too simplistic for a meaningful comparison 3 1 model m1 fig 3 a shows the head distribution every ten cells for all three realizations at two different time steps one can see a good fit for both roms this can be verified by fig 3 b which shows the absolute maximum head error of both roms at each time step these head errors are very small for most of the simulation with maximum values of less than 1 cm for the pod and of about 3 cm for the eb pod model table 5 states these errors along with normalized root mean squared errors nrmse in time and space for the groundwater heads for all four models the biggest errors coincide with the start and stop of pumping at the well meaning that radical boundary changes are the biggest challenge for the roms fig 3 c shows the relative flux error at the neumann boundary the pumping well for pod and eb pod while the flux error at the start of the pumping period is greater than 5 for the pod model it is effectively zero for the eb pod note that flux errors for the eb pod model are not exactly zero due to the accuracy of the applied numerical solvers clearly the eb pod method successfully integrated the neumann boundary condition into the rom this can also be seen from the low nrmse values of the neumann boundary flux at the well shown in table 6 flux errors at the boundary only occurred shortly after the start of the pumping phase for this simple model though this improvement at the boundary comes at a cost though as was indicated in section 2 3 5 there is a trade off between the reduction of the flux error at the boundary and the overall head error for the eb pod model in this case that is true for the magnitude of the head errors at major system disturbances caused by changes in the boundary conditions while the head errors after some time of continuous pumping are actually smaller for eb pod than for pod 3 2 model m2 model m2 was built to see whether a more complex setup results in different patterns of pod flux errors or in more significant changes in eb pod head errors fig 4 a shows the head distribution for the two reduced and the full model at different time steps both reduced models are in good agreement with the full model regarding the water table the absolute maximum head errors depicted in fig 4 b are about 2 cm for the pod model and peaking to about 9 cm for the eb pod model it is easily recognizable that these errors are larger than for model m1 fig 4 c and d show the relative flux errors at the two pumping wells flux errors at the wells are up to 5 for the pod model and negligible for the eb pod model while this is very similar to model m1 the patterns of the flux errors are significantly different the highest errors are at the start of a pumping phase but they do not diminish in the same manner as in m1 this can be explained by the pod flux error at the river boundary being highly dynamic mainly being governed by the input hydrograph fig 4 e comparing the dynamic shapes of the pumping well flux errors with the river flux errors one can see that the river flux errors strongly influence the pumping well errors especially for well 2 the eb pod model treated all three neumann boundaries accurately eliminating the flux errors at these boundaries as is confirmed by low nrmse values table 6 the trade off compared to maximum head errors is apparent again though as the eb pod has a consistently lower performance in this regard 3 3 model m3 we compare only the eb pod with the full model for model m3 as the standard pod implementation is not applicable for variable dirichlet boundaries in most cases as was stated in section 2 3 2 correct implementation of dirichlet boundaries in standard pod is dependent on several restrictions regarding the rom fig 5 a shows the water table and fig 5 b the head error at the variable dirichlet boundary note that the scale of fig 5 b is orders of magnitude smaller than the similar graphs for the other test cases the eb pod model reproduces the full model very well and the errors at the variable dirichlet boundary are negligible absolute maximum head errors of about 1 5 cm are comparable with results from model m1 as is the nrmse value cf table 5 3 4 model m4 the difference between m4 and m2 is the river bed elevation see fig 1 in model m4 the river is connected directly to the aquifer resulting in a cauchy type boundary condition head distribution head errors and pumping well flux errors of the pod and eb pod models are very similar to model m2 and therefore not shown fig 6 shows the flux error at the cauchy boundary overall the flux errors for both roms at the cauchy boundary are larger than they were for the disconnected river boundary neumann in model m2 flux errors of the eb pod model at the cauchy boundary are not zero contrary to the previous models this is confirmed by the nrmse values in table 6 where the eb pod model shows significant errors at the cauchy boundaries compared to the errors at neumann boundaries discussed before this can be explained by the difference between neumann and cauchy boundaries neumann boundaries are independent of the groundwater head and therefore only part of the right hand side vector b in eq 16 cauchy boundaries however are head dependent as was detailed in section 2 3 4 this means that information regarding their boundary fluxes is stored in both the right hand side b and the left hand side a of eq 16 while the first part can be enforced error free by the eb pod method the second part is calculated from the approximated groundwater heads and thus contains errors resulting from the approximation via the subspace projection this means that the flux error through cauchy boundaries cannot be eliminated completely but compared to the original pod model it leads to a reduction of the error in most cases as can be seen in fig 6 3 5 model m5 model m5 was used for an application of both pod and eb pod to a larger more complex 3d model fig 7 shows the groundwater levels in the model domain at two selected times t 1 175 d t 2 225 d for the full model along with errors for the two roms as head differences to the heads of the original model fig 7 a and b depicts the depression cones of the two pumping wells in the full model for two times respectively along with the different groundwater head values at the left boundary which are influenced by the river stage the head errors for both roms at the two time steps are very similar in their structure fig 7 c e and d f the absolute values are different though with slightly larger errors for the eb pod model this is in agreement with the results of the other test cases and is caused by the information loss through the reduction of retained singular values in the projection matrix for eb pod this is also confirmed by fig 8 a which shows the maximum head errors of both roms over time please note that these errors are still small and at most in the range of typical head measurement errors this is confirmed by head errors and nrmse values reported in table 5 fig 8 b and c shows the relative flux errors at the two pumping wells which are relatively small for the pod model and almost zero for the eb pod model the relative flux errors at the river boundary are depicted in fig 8 d again the flux errors at this neumann type boundary are significant for the pod model while the errors for the eb pod model are virtually zero as confirmed by the nrmse values in table 6 note that for the 3d model this river boundary spans 30 cells which could all be represented in the eb pod matrix in a single line see section 2 5 this shows the potential of the eb pod method for complex groundwater models since boundary conditions implemented into several cells can be accurately represented in an eb pod reduced model the run times of the 3d models are summarized in table 7 a run time reduction by a factor of 6 was observed for the online portions of the roms this is even higher more than factor 30 if considering just the timings of the matrix inversion step reductions are the same for both roms due to their identical sizes in our test case the offline portion of calculating the snapshots and creating the projection matrix takes as much time as one full model run since it is often not necessary to repeat this step for rom applications calculation with different boundaries for example this is an acceptable time for pre processing note that run times are taken on a standard desktop pc for model implementations in matlab there is potential to further improve actual run times by for example compilation of the code 4 discussion the results presented in section 3 show that the eb pod method can be applied for all three types of boundary conditions originally pod models adhere to constant dirichlet boundaries through pre processing of snapshots while variable dirichlet boundaries could not be implemented this way at all alternatively dirichlet boundaries can be handled correctly by application of pod to drawdown simulations and calculation of groundwater heads through superposition but only for linear models neumann and cauchy boundaries are only implicitly handled this leads to flux errors through these boundaries which even in simple 1d models can become quite significant 10 relative error one can also see that changes at certain boundaries influence the flux errors at other boundaries the proposed eb pod method extends the general pod for all three boundary types in a straightforward manner that does not necessitate complicated computations model m3 shows the application to a variable dirichlet boundary which can only be reproduced by standard pod in special cases the resulting errors of the eb pod rom at the head boundary are negligible models m1 m2 and m5 are applications of the method for different neumann boundaries the explicit treatment of these boundaries through eb pod leads to a reduction of the flux errors through the boundaries to effectively zero model m2 shows that the interplay of several boundaries can lead to substantial miscalculation of these fluxes in pod reduced order models thus one can assume that justification for the eb pod method increases with the complexity of the model model m5 shows that the method works satisfactory in 3d allowing the representation of a 30 cell river boundary through one row in the eb pod projection matrix the roms exhibit a run time reduction by a factor of 6 while model m4 shows that the method can also be applied for cauchy boundaries the accompanying flux errors through this boundary type cannot be eliminated completely cauchy boundaries are composed of a head independent i e linear and a head dependent i e non linear part where the latter is calculated from the approximated head instead of explicitly enforced in eb pod and is therefore error prone nonetheless the eb pod method still explicitly adheres to the head independent part of the cauchy boundary thus diminishing the resulting flux error compared to the original pod all in all the new eb pod method successfully handles rom errors at boundaries this elimination or reduction for cauchy type boundaries of rom flux errors through eb pod comes at a cost the overall accuracy of the head field in the aquifer is somewhat lower for eb pod models compared to original pod models of the same build formed from the same snapshots with the same number of retained singular vectors as is explained in section 2 3 5 this is due to the method s handling of the original projection matrix p which is projected onto a smaller subspace ψ of size n r 2 n r n b c to allow space for the independent boundary condition equations this projection is accompanied by a loss of information this also means that the more independent boundary conditions are treated explicitly by eb pod the larger the error of the new rom with respect to the head field furthermore the theoretical maximum of such independent boundaries would be n b c n r 1 as a higher number of boundaries would deduct all information of the original projection matrix p practically this limit is even smaller as a number of nbc very close to nr would probably result in a highly inaccurate rom note that this limit refers to independent boundaries though in groundwater modeling multiple cells in the model domain are often described by a single boundary in addition this only limits the number of explicitly treated boundary conditions not the overall number of boundaries in the full model essentially the eb pod method provides a trade off for reduced order modeling with pod the elimination dirichlet and neumann or reduction cauchy of boundary errors comes at a cost of increasing inaccuracy of the overall head approximation with increasing number of independent boundaries treated by eb pod fortunately this trade off can be easily quantified implementation of different boundaries via eb pod is straight forward and computationally inexpensive therefore the modeler can test different scenarios regarding boundary condition treatment by eb pod and compare both reduction of flux errors and increase in head errors with the original pod model and other scenarios furthermore models are often built with a specific purpose in mind and the accuracy may be required at certain locations and in certain outputs only if for example the purpose of the model is the quantification of river aquifer exchange fluxes the correct calculation of these fluxes is of higher interest than the general head field in contrast if head approximations of the reduced order model at specific measurement points is the main target of the model one can analyze the distribution of change in head error through application of eb pod this targeted analysis allows the modeler an informed decision on which if any boundary conditions to treat this way similar choices have to be made for general pod reduction as well the choice of specific relative retained variance rv see section 2 2 is one of computational speed versus precision of the reduced order model in comparison the decision regarding boundary condition treatment through eb pod is one about accuracy at specific boundaries versus accuracy in the overall head field fortunately this is a choice that can be made with quantifiable information on its ramifications thus handing all power to the user for target oriented reduced order modeling 5 summary and conclusion the newly proposed eb pod method allows the explicit handling of dirichlet neumann and cauchy boundary conditions for pod based groundwater model reduction pod s projection of the system equations onto a subspace does not account for the boundary conditions in the explicit way the original model equations do by modifying the projection matrix to be orthogonal to the boundary conditions and then adding the original boundary equations to the final system of equations the eb pod method treats the boundary conditions in the same explicit way as the original model the method is applied to five synthetic test cases to test it against all three types of boundary conditions boundary errors for dirichlet and neumann boundaries are successfully eliminated through our new pod extension eb pod reduces flux errors at cauchy boundaries compared to pod though not to zero due to their head dependence the test cases highlight the new method s ability to severely enhance accuracy of pod rom s boundary representation even in 3d while retaining computational time savings of the original pod method the implementation of the method results in slightly larger errors in the overall head field though this stems from the method s reduction in size of the original pod projection matrix containing the snapshot information fortunately the effect of this trade off can easily be quantified we therefore provide a tool for target orientated decision making on intrinsic boundary accuracy with the eb pod method this adds a direct and specific way of controlling the rom accuracy to the present choice of snapshot selection and retained variance while the method has been applied to simple synthetic test cases in groundwater modeling in this study the application to and its effects on real world models is an area of future investigation furthermore the method does not deal with the shortcomings of general pod reduction for non linear models it could potentially be combined with techniques like pod deim see stanko et al 2016 to improve its computational performance for non linear groundwater models acknowledgments this work was funded by the german research foundation grant wo 1781 1 1 the author would like to thank michael sinsbeck at simtech universität stuttgart for helpful insights and discussions 
829,in recent years proper orthogonal decomposition pod has become a popular model reduction method in the field of groundwater modeling it is used to mitigate the problem of long run times that are often associated with physically based modeling of natural systems especially for parameter estimation and uncertainty analysis pod based techniques reproduce groundwater head fields sufficiently accurate for a variety of applications however no study has investigated how pod techniques affect the accuracy of different boundary conditions found in groundwater models we show that the current treatment of boundary conditions in pod causes inaccuracies for these boundaries in the reduced models we provide an improved method that splits the pod projection space into a subspace orthogonal to the boundary conditions and a separate subspace that enforces the boundary conditions to test the method for dirichlet neumann and cauchy boundary conditions four simple transient 1d groundwater models as well as a more complex 3d model are set up and reduced both by standard pod and pod with the new extension we show that in contrast to standard pod the new method satisfies both dirichlet and neumann boundary conditions it can also be applied to cauchy boundaries where the flux error of standard pod is reduced by its head independent contribution the extension essentially shifts the focus of the projection towards the boundary conditions therefore we see a slight trade off between errors at model boundaries and overall accuracy of the reduced model the proposed pod extension is recommended where exact treatment of boundary conditions is required keywords model reduction proper orthogonal decomposition boundary condition treatment 1 introduction the building and application of groundwater models is one of the major fields supporting modern groundwater management as these models often couple well understood physical principles with insufficiently known and highly spatially variable aquifer properties computerized calibration of parameters and robust uncertainty analysis of model predictions are mandatory for meaningful interpretation of the results unfortunately calibration and uncertainty analysis often require many model runs modern advances in computer performance and parallelization can alleviate these run times only to a limited extent over the years many researchers tackled this problem with model reduction many different methods have been applied to reduce run times of physically based groundwater models usually model run time reduction comes at a cost of model accuracy balancing this trade off has been the main focus of research model reduction techniques in groundwater can be roughly put into three categories data driven methods projection based methods and structural reduction methods the data driven methods include artificial neural networks taormina et al 2012 certain forms of polynomial chaos expansion oladyshkin and nowak 2012 or bayesian networks fienen et al 2013 the most prominent projection based method is proper orthogonal decomposition pod vermeulen et al 2004b mcphee and yeh 2008 or siade et al 2010 for example followed by the fourier model reduction willcox and megretski 2005 structural reduction methods directly reduce the size of the model discretization or simplify the representation of the processes von gunten et al 2014 structural simplification can go along with parameter reduction like doherty and christensen 2011 did with the inversion based upscaling approach proper orthogonal decomposition projects the discretized model equations onto a subspace usually by the galerkin method the subspace is mostly defined via singular vectors of simulations of the full model for pre defined times and forcings so called snapshots see winton et al 2011 for an example of pod in groundwater not based on snapshots recent research in pod for groundwater models focused on non linear reduction e g stanko et al 2016 snapshot selection e g siade et al 2010 or inverse modeling boyce and yeh 2014 or ushijima and yeh 2015 for example but did not focus on the effects of pod on the boundary conditions of the system the full physically based groundwater model depends on the statement of and adherence to certain boundary conditions while the pod method incorporates the original model equations it does not explicitly adhere to the original boundary conditions the approximation of groundwater heads in the reduced order model rom obtained by pod may lead to significant errors in boundary heads or fluxes this is especially relevant if the rom is used to predict boundary fluxes groundwater discharge into rivers or low land springs for example this problem has been analyzed in other research areas gunzburger et al 2007 developed a method to treat dirichlet boundaries explicitly by modifying the pod subspace to be orthogonal to the boundary conditions this leads to an underdetermined system of equations that is then filled up with the corresponding projections of the original boundary equations cosimo et al 2016 built on this work by proposing different methods of building the rom but still only with respect to dirichlet boundaries we transfer the idea proposed by gunzburger et al 2007 into the area of groundwater modeling furthermore we significantly extend the method now allowing the explicit treatment of all three types of boundary conditions dirichlet neumann and cauchy in pod reduction of groundwater models we analyze the performance of the new method with several test cases section 3 and study the trade off between overall model performance and accuracy at the boundary conditions section 4 2 methods in this section we will first give a short summary on the mathematical basis of the groundwater models to which pod is typically applied second we briefly revisit the common pod method we will then develop the extension of pod for explicit boundary treatment eb pod and explain the implications of its use finally we will present different synthetic models used in this study to depict and test the new method as well as the chosen methodology for snapshot selection 2 1 full groundwater model in this work we focus on groundwater models based on the groundwater flow equations these equations have been presented in numerous studies pinder and celia 2006 thus we only summarized them here as required for developing the new method more in depth formulations can be found in the modflow manual harbaugh 2005 for example 3d groundwater flow is usually described by the following partial differential equation based on darcy s 1856 law 1 x k x x h x y k y y h y z k z z h z w s h t with k being hydraulic conductivity l t h the piezometric head l w a source sink term 1 t and s the storage term 1 l the solution of this differential equation requires initial and boundary conditions initial conditions are defined by specifying the head h x y z 0 f 0 x y z at time zero for the boundary conditions three different types are most common 2 h f 1 t x y z dirichlet type i n h f 2 t x y z neumann type ii n h c t x y z h f 3 t x y z cauchy type iii where x y z n is the normal vector to the boundary and f 1 2 3 and c can be functions in space and or time conceptually dirichlet boundary conditions represent specified heads neumann boundaries are specified fluxes and cauchy boundaries are specified leakages spatial and temporal discretization of eq 1 and its boundary conditions e g by finite differences finite volumes or finite elements leads to the following linear system of equations provided here for implicit time stepping in matrix notation 3 r l t h t 1 r h t t q where r f s l f k and q f w after renaming for convenience r l t a and r h t t q b eq 3 becomes 4 a h t 1 b this system of linear equations is solved for every time step the size of the system of equations depends on the number of model cells nm as follows a ϵ r n m n m and b h ϵ r n m 1 the boundary conditions from eq 2 can be found explicitly in boundary related rows of the system in eq 4 the solution of the equations for groundwater levels h requires iterative manipulation of a a computationally and therefore time demanding process 2 2 proper orthogonal decomposition pod the pod method aims to reduce the size of eq 4 by projection onto a suitable subspace this subspace is created by singular value decomposition of a collection of previously collected system states so called snapshots these snapshots are built by calculating the vectors of groundwater heads h at specific times t s t s 1 t s 2 t s n s in the full model and saving them in the matrix h s h s 1 h s 2 h s n s the number of snapshots ns results in h s ϵ r n m n s the process of snapshot selection is detailed further in section 2 5 now the snapshot matrix h s is treated by singular value decomposition 5 h s u σ v t resulting in the matrix of left singular vectors u ϵ r n m n m the diagonal matrix of singular values σ ϵ r n m n s and the matrix of right singular vectors v ϵ r n s n s the projection matrix p u 1 u 2 u n r is a selection of left singular vectors from u using the singular values σ as selection criteria similar to a principal component analysis the number of retained singular values nr is chosen as follows 6 i 1 n r σ i i 1 r a n k s σ i 100 rv where rv is a chosen specific relative retained variance i e of 99 99 since singular values are ranked in descending order this assures that the chosen rv of the snapshot set is achieved with the lowest possible number of singular vectors nr is the size of the reduced model now the galerkin 1968 method is used to project eq 4 onto the subspace spanned by the columns of p 7 p t a p r t 1 p t b solving eq 7 is much more time efficient compared to the original eq 4 because nm nr the groundwater heads in the original model are calculated from this as follows 8 h p r this approximation captures the rv of the dynamics represented by the snapshots in h s however no explicit lines can be found anymore that enforce the boundary conditions 2 3 explicit boundary pod 2 3 1 general form the original eq 4 handles boundary conditions explicitly to assure the correct solution in affected cells this does not hold for the reduced model in eq 7 where boundary conditions are only approximated due to the projection onto the subspace we propose an explicit boundary pod method eb pod which modifies the original pod equations to enforce the boundary conditions in the reduced model the simplified handling of dirichlet boundaries section 2 3 2 is taken directly from gunzburger et al 2007 in this contribution we generalize their approach for the application to all three types of boundaries see eq 2 which is subsequently presented in our new method we replace part of the reduced model with the equations for the original boundary conditions for this we need to modify the projection matrix p of the reduced model to remove the approximation regarding these boundaries first we collect the rows of p corresponding to the independent boundary conditions bc we want to treat explicitly into n independent boundaries means that for several rows and therefore model cells that are all subjected to the same forcing for example by representing the same constant head boundary along a model edge only one row has to be collected into n 9 n p b c in our notation the first index of a matrix stands for the position of rows while the second index stands for the position of columns p bc therefore denotes a subset of p with the bc rows and all columns respectively likewise q n b c 1 n r denotes a subset of q of all rows and n b c 1 to nr columns the transpose of matrix n is now decomposed by qr factorization 10 q r q r n t the columns of the resulting matrix q of size nr nr form an orthonormal basis for n the number of independent boundary conditions nbc must be smaller than the reduced model size nr then the n b c 1 n r columns of q hold the coefficients of the linear combinations of p that are independent of the boundaries as the 1 nbc columns of q retain all information regarding the boundaries multiplication of these n b c 1 n r columns of q with p leads to 11 ψ pq n b c 1 n r the matrix ψ has the size nm n r2 where n r 2 n r n b c it contains linear combinations of the pod basis functions that vanish at the boundaries thus ψ is a reduced version of p that is cleaned from the inaccurate boundary related information contained in the original projection matrix we can now substitute eq 11 into eq 7 12 ψ t a p r t 1 ψ t b because the size nr of p and r is larger by nbc than the number of columns n r2 of ψ eq 12 is underdetermined as ψ is boundary independent and the number of missing equations is nbc we can now add the accurate boundary conditions from the original model 13 a b c h t 1 b b c as eq 12 works with the reduced solution r instead of h we use the same approximation eq 8 in eq 13 14 a b c p r t 1 b b c combining the new reduced model eq 12 with the reduced form of the boundary eq 14 leads to the final system of equations for the eb pod reduced model 15 ψ t a p a b c p r t 1 ψ t b b b c where the approximation of groundwater heads is again h pr 2 3 2 dirichlet boundaries due to the way dirichlet boundaries are implemented into groundwater models their handling with the eb pod simplifies the eq 15 for dirichlet boundary cells dc the corresponding system matrix diagonals a dc dc are 1 while the off diagonal elements are 0 this leads to h t 1 d c b d c where b dc is the fixed groundwater head at the dirichlet boundary this simplifies a bc p of eq 15 for the dirichlet boundaries leading to 16 ψ t a p a n c p p d c r t 1 ψ t b b n c b d c where nc denotes the explicitly treated boundaries that are not of dirichlet type neumann and cauchy boundaries and dc represents the dirichlet boundary cells if only dirichlet boundaries are to be handled by eb pod eq 16 simplifies to the method presented in the field of applied mechanical engineering by gunzburger et al 2007 therefore eq 16 is the generalization of that method to the three featured boundary condition types again the approximation of groundwater heads is still h pr note that dirichlet boundaries can potentially be handled correctly by standard pod with several restrictions to the rom building process the model must simulate drawdown instead of groundwater heads for drawdown dirichlet boundary conditions are always zero thus not affected by the subspace projection of pod as has been shown by siade et al 2010 groundwater heads and fluxes can be calculated from the drawdown simulations by simple superposition of drawdown with the groundwater heads without pumping which are subsequently termed pseudo steady state however this can only be applied to linear groundwater models dirichlet boundary cells must be omitted in the calculation of the pod rom by excluding its rows from the snapshot matrix h s and thus from p instead these cells are set to the correct values manually this is sometimes done in groundwater pod in general presumably for better accuracy at the dirichlet boundaries as is stated in vermeulen et al 2004a preprocessing of the snapshots is necessary see section 2 5 to compute drawdowns by subtraction of the pseudo steady state for variable dirichlet boundaries this pseudo steady state heads are time dependent and have to be computed for every time step thus an additional full model run without pumping is necessary offline to create the corresponding pseudo steady state the additional computational requirements reduce the efficiency of the pod model reduction these limitations in the handling of dirichlet boundaries are overcome by our proposed eb pod method 2 3 3 neumann boundaries neumann boundaries are directly handled by eb pod as shown in section 2 3 1 eb pod allows the explicit treatment of the neumann boundaries in the rom this ensures the correct boundary fluxes in the rom in contrast to standard pod where they are approximated 2 3 4 cauchy boundaries the head dependency of cauchy boundaries has noteworthy effects on the handling with eb pod while all information regarding dirichlet and neumann boundaries is found in the right hand side of eq 16 in the respective rows of the b vector cauchy boundaries are different as explained below on the example of a river hydraulically connected to the aquifer the flow from a connected river into the underlying cell is calculated in modflow for example as q r i v c r i v h r i v h where qriv is the flow l3 t into the cell from the river criv is the conductance l2 t of the river bed and hriv is the head in the river l this can be rewritten as q r i v c r i v h r i v c r i v h resulting in a groundwater head independent part crivhriv that is added to the right hand side b of eq 16 as for neumann boundaries and a head dependent part c r i v h that is added to the diagonal of the left hand side a of eq 16 the eb pod method adds the original boundary equations to the reduced equation system in eq 16 but does so in the reduced space this means that fluxes through head dependent boundaries i e cauchy boundaries are not computed error free in the eb pod model while the head independent part stored in b is correct and error free through its enforcement similar to a neumann boundary in eb pod the head dependent part in a is calculated from the approximated heads ah apr and is therefore not entirely error free this is demonstrated in section 3 4 2 3 5 implications of eb pod looking at the left hand side matrix of eq 15 several things are important first the overall size of the eb pod reduced order model does not change compared to the pod model both handle full matrices of size nr nr to find the solution for each time step thus the explicit treatment of boundary conditions in eb pod comes at no additional computational cost compared to the original pod method what changed in size however are the corresponding projection matrices while the original pod matrix p ϵ r n m n r uses the information of nr singular vectors to span the reduced model subspace the projection matrix ψ ϵ r n m n r 2 retains less information since n r 2 n r n b c practically this means that the reduced model size nr has to be chosen larger than the number of independent boundary conditions nbc treated this way replacing original pod rows by boundary rows reduces in theory the accuracy of the solution within the domain however the accurate representation of the boundaries typically counteracts this loss of accuracy or forces the user to decide on the relative importance of different model aspects this potential trade off will be analyzed further in the following sections another potential implementation of the eb pod method would be the addition of the boundary rows to the original pod projection matrix let the original pod matrix be p o r i g ϵ r n m n r now we create a new bigger projection matrix p b i g ϵ r n m n b i g following the procedure detailed in section 2 2 but retaining as many additional singular values as we have independent boundaries n b i g n r n b c we subject this matrix p big to the steps detailed in section 2 3 1 resulting in a new eb pod projection matrix ψ a d d ϵ r n m n r 2 note that for this matrix n r 2 n b i g n b c and thus n r 2 n r this means there is no loss of information within the domain by going from p orig to ψ add application of this alternative implementation not shown proved that this is indeed the case and the accuracy of the eb pod model within the domain is in agreement with the original pod model while retaining the feature of accurate boundary representation it is important to note though that the overall size of the eb pod rom is n r 2 n b c and thus nbc bigger than the original pod rom therefore this new eb pod model is while more accurate slower than the original pod model thus the question of subtraction or addition of the nbc boundary condition rows to the rom is the same trade off between run time vs accuracy as the original choice of the reduced model size nr for the sake of comparison between pod and eb pod it seemed more practical if the reduced model size for both methods is the same thus fixing run times and exploring the trade off in accuracy instead of in run time 2 4 synthetic model setup 2 4 1 one dimensional 1d models to demonstrate our new method we use four simple synthetic 1d models with different types of transient boundary conditions as well as a more complex synthetic 3d model while differing in their boundary conditions all four 1d models have the same geometry time settings and aquifer properties following earlier pod benchmarking studies siade et al 2010 we set up 1d confined groundwater models that consist of 1 row 100 columns and 1 layer a total of 100 model cells all model cells are 100 m wide δx 1 m deep δy and 100 m high δz the synthetic aquifer s dimensions are therefore 10 000 1 100 m the model simulation time is one year with daily time steps leading to 365 time steps two zones of hydraulic conductivity are used to incorporate some complexity and asymmetry into the model two thirds of the model domain cells 1 67 have hydraulic conductivity values of 1500 m d while the rest of the domain cells 68 100 has a hydraulic conductivity of 500 m d the entire model domain has a specific storage value of 0 005 1 m fig 1 shows the different 1d model schematics with the boundary conditions and hydraulic conductivity zones table 2 summarizes the differences of the four 1d models and states which boundary conditions are treated explicitly by the eb pod method the pumping regime of all models can be seen in table 1 the first 1d model termed m1 serves as a simple proof of concept for the method the model is set up analogous to the 1d models used in groundwater pod literature see for example mcphee and yeh 2008 the left and right model edges are set as constant head boundaries at 1 and 2 m respectively the initial groundwater head follows the linear gradient interpolated between these two constant head boundaries a single pumping well is added to cell 33 x 3250 m with a pumping rate of 100 m3 d which is turned on in the interval t p 100 250 days for this model only the single neumann boundary cell the pumping well is treated explicitly by eb pod the second 1d model m2 is more complex here we replace the left constant head boundary with an unconnected river neumann type boundary with a hydrograph taken with slight adjustments from a real world river see fig 4 e the initial groundwater head is identical to the remaining constant boundary on the right model edge for the entire domain furthermore a second pumping well is added to the model domain at cell 67 x 6650 m and both wells are pumped with changing pumping rates and partly separate partly overlapping pumping regimes ranging from 150 m3 d to 250 m3 d in this model all three neumann boundaries the river and both pumping wells are treated explicitly by eb pod m3 the third 1d model is used to show the application to a variable dirichlet boundary the model set up is similar to m1 but the left boundary is changed to a variable head simulating a seasonal head change with a sine function with period 1 year and amplitude of 1 m only this variable dirichlet boundary is handled explicitly by eb pod the last 1d model m4 is similar to m2 but connecting the river to the aquifer which then is a cauchy type boundary as in m2 both river and pumping wells one cauchy two neumann boundaries are explicitly treated by eb pod 2 4 2 three dimensional 3d model a fifth synthetic test model m5 is presented here to show the applicability of the method for a more complex model m5 is a 3d confined groundwater model with 30 rows 100 columns and 5 layers i e a total of 15 000 model cells the size of the model cells is 10 10 m horizontally δx and δy respectively and 5 m vertically δz resulting in aquifer dimensions of 1000 300 50 m model simulation time is again one year with daily time steps 365 hydraulic conductivity is assumed to be different for odd and even numbered layers resulting in four zones labeled a d with different hydraulic conductivity values in x y and z direction fig 2 and table 3 the specific storage value of the entire model domain is assumed to be 0 005 1 m with respect to the definition of the boundary conditions model m5 follows the build of m2 with a hydraulically unconnected river neumann boundary at the left model domain fig 2 exhibiting the same hydrograph as used in m2 the right model boundary is a constant head of 0 m the initial groundwater head is set to 0 m across the entire model domain two pumping wells are added in layer 5 fig 2 pumping rates and intervals for these wells are listed in table 1 as before all three neumann boundaries one river two pumping wells are treated explicitly by eb pod as can be seen in table 2 2 5 snapshot selection technique snapshot selection is an important factor in pod based reduced order modeling and has been investigated in the literature cf siade et al 2010 the aim of this study however is the comparison of standard pod and an extension to that method the eb pod we assume that this comparison is meaningful regardless of the snapshot selection method provided the applied process is following tested standard procedures and both pod and eb pod reduced order models use the same set of snapshots for each model on that basis we used a snapshot selection approach modified from gunzburger et al 2007 that can be described by the following three steps 1 identify all drivers of the groundwater system dirichlet boundaries neumann boundaries wells rivers or recharge for example cauchy type boundaries nd 2 compute a steady state system response to all time independent boundaries of the groundwater system constant heads for example h ini and add this steady state to the snapshot matrix h s h i n i 3 run the following loop for each driver i 1 n d a store the current set of snapshots as h s o l d h s b set the chosen driver to an arbitrary but reasonable value constant in time pumping in the well river head etc c turn off all other drivers set pumping to zero river head to river bottom variable head to constant etc d set h 0 h i n i e run the model until steady state is reached generating system responses i e groundwater heads at each time step t 0 n t d for this driver h i ϵ r n m n t d f append these responses to the overall snapshot matrix h s h s o l d h i while the original snapshot selection in gunzburger et al 2007 extends this approach by running the model with all time variable drivers turned on simultaneously after each step e of the loop our experiments revealed that these additional runs have no influence on the precision or stability of either pod or eb pod reduced order models results not shown since they add a significant portion to offline computing time however we chose to remove this step from our snapshot selection approach the chosen approach also follows general guidelines for snapshot selection for pod in groundwater modeling e g vermeulen et al 2004a mcphee and yeh 2008 in the literature one often finds several techniques for snapshot pre processing such as mean centering and normalization cf boyce et al 2015 we could not see any improvement of the pod reduced order models for our test cases by applying these techniques in contrary mean centering of the snapshots before applying the explicit boundary handling for eb pod resulted in an non functioning rom for our test cases this is because eb pod s handling of boundary conditions is dependent on the time variant information contained in the snapshot set that mean centering removes the snapshot selection approach described above was used for all different test models the time for each driver model run to reach steady state ntd was chosen to be 100 days table 4 summarizes the number of drivers in the system nd resulting snapshot numbers and reduced order model sizes with a chosen retained variance rv of 99 99 note that the number of explicitly treated boundaries nbc is not the same as the number of drivers in the system nd as only specific boundaries were handled explicitly by eb pod as described in section 2 4 for model m5 n b c 3 one river and two pumping wells even though the river spans the whole left hand side of the first layer 30 cells since all river cells are subjected to the same boundary forcing though only one single row of the original projection matrix has to be collected for the eb pod handling of the river 3 results in this section we apply the eb pod method to the four 1d models and the 3d model outlined in section 2 4 we compare original pod and eb pod in their precision for the original boundary conditions and the general approximation of the groundwater head fields note that we will provide run times only for the 3d model as the 1d models are too simplistic for a meaningful comparison 3 1 model m1 fig 3 a shows the head distribution every ten cells for all three realizations at two different time steps one can see a good fit for both roms this can be verified by fig 3 b which shows the absolute maximum head error of both roms at each time step these head errors are very small for most of the simulation with maximum values of less than 1 cm for the pod and of about 3 cm for the eb pod model table 5 states these errors along with normalized root mean squared errors nrmse in time and space for the groundwater heads for all four models the biggest errors coincide with the start and stop of pumping at the well meaning that radical boundary changes are the biggest challenge for the roms fig 3 c shows the relative flux error at the neumann boundary the pumping well for pod and eb pod while the flux error at the start of the pumping period is greater than 5 for the pod model it is effectively zero for the eb pod note that flux errors for the eb pod model are not exactly zero due to the accuracy of the applied numerical solvers clearly the eb pod method successfully integrated the neumann boundary condition into the rom this can also be seen from the low nrmse values of the neumann boundary flux at the well shown in table 6 flux errors at the boundary only occurred shortly after the start of the pumping phase for this simple model though this improvement at the boundary comes at a cost though as was indicated in section 2 3 5 there is a trade off between the reduction of the flux error at the boundary and the overall head error for the eb pod model in this case that is true for the magnitude of the head errors at major system disturbances caused by changes in the boundary conditions while the head errors after some time of continuous pumping are actually smaller for eb pod than for pod 3 2 model m2 model m2 was built to see whether a more complex setup results in different patterns of pod flux errors or in more significant changes in eb pod head errors fig 4 a shows the head distribution for the two reduced and the full model at different time steps both reduced models are in good agreement with the full model regarding the water table the absolute maximum head errors depicted in fig 4 b are about 2 cm for the pod model and peaking to about 9 cm for the eb pod model it is easily recognizable that these errors are larger than for model m1 fig 4 c and d show the relative flux errors at the two pumping wells flux errors at the wells are up to 5 for the pod model and negligible for the eb pod model while this is very similar to model m1 the patterns of the flux errors are significantly different the highest errors are at the start of a pumping phase but they do not diminish in the same manner as in m1 this can be explained by the pod flux error at the river boundary being highly dynamic mainly being governed by the input hydrograph fig 4 e comparing the dynamic shapes of the pumping well flux errors with the river flux errors one can see that the river flux errors strongly influence the pumping well errors especially for well 2 the eb pod model treated all three neumann boundaries accurately eliminating the flux errors at these boundaries as is confirmed by low nrmse values table 6 the trade off compared to maximum head errors is apparent again though as the eb pod has a consistently lower performance in this regard 3 3 model m3 we compare only the eb pod with the full model for model m3 as the standard pod implementation is not applicable for variable dirichlet boundaries in most cases as was stated in section 2 3 2 correct implementation of dirichlet boundaries in standard pod is dependent on several restrictions regarding the rom fig 5 a shows the water table and fig 5 b the head error at the variable dirichlet boundary note that the scale of fig 5 b is orders of magnitude smaller than the similar graphs for the other test cases the eb pod model reproduces the full model very well and the errors at the variable dirichlet boundary are negligible absolute maximum head errors of about 1 5 cm are comparable with results from model m1 as is the nrmse value cf table 5 3 4 model m4 the difference between m4 and m2 is the river bed elevation see fig 1 in model m4 the river is connected directly to the aquifer resulting in a cauchy type boundary condition head distribution head errors and pumping well flux errors of the pod and eb pod models are very similar to model m2 and therefore not shown fig 6 shows the flux error at the cauchy boundary overall the flux errors for both roms at the cauchy boundary are larger than they were for the disconnected river boundary neumann in model m2 flux errors of the eb pod model at the cauchy boundary are not zero contrary to the previous models this is confirmed by the nrmse values in table 6 where the eb pod model shows significant errors at the cauchy boundaries compared to the errors at neumann boundaries discussed before this can be explained by the difference between neumann and cauchy boundaries neumann boundaries are independent of the groundwater head and therefore only part of the right hand side vector b in eq 16 cauchy boundaries however are head dependent as was detailed in section 2 3 4 this means that information regarding their boundary fluxes is stored in both the right hand side b and the left hand side a of eq 16 while the first part can be enforced error free by the eb pod method the second part is calculated from the approximated groundwater heads and thus contains errors resulting from the approximation via the subspace projection this means that the flux error through cauchy boundaries cannot be eliminated completely but compared to the original pod model it leads to a reduction of the error in most cases as can be seen in fig 6 3 5 model m5 model m5 was used for an application of both pod and eb pod to a larger more complex 3d model fig 7 shows the groundwater levels in the model domain at two selected times t 1 175 d t 2 225 d for the full model along with errors for the two roms as head differences to the heads of the original model fig 7 a and b depicts the depression cones of the two pumping wells in the full model for two times respectively along with the different groundwater head values at the left boundary which are influenced by the river stage the head errors for both roms at the two time steps are very similar in their structure fig 7 c e and d f the absolute values are different though with slightly larger errors for the eb pod model this is in agreement with the results of the other test cases and is caused by the information loss through the reduction of retained singular values in the projection matrix for eb pod this is also confirmed by fig 8 a which shows the maximum head errors of both roms over time please note that these errors are still small and at most in the range of typical head measurement errors this is confirmed by head errors and nrmse values reported in table 5 fig 8 b and c shows the relative flux errors at the two pumping wells which are relatively small for the pod model and almost zero for the eb pod model the relative flux errors at the river boundary are depicted in fig 8 d again the flux errors at this neumann type boundary are significant for the pod model while the errors for the eb pod model are virtually zero as confirmed by the nrmse values in table 6 note that for the 3d model this river boundary spans 30 cells which could all be represented in the eb pod matrix in a single line see section 2 5 this shows the potential of the eb pod method for complex groundwater models since boundary conditions implemented into several cells can be accurately represented in an eb pod reduced model the run times of the 3d models are summarized in table 7 a run time reduction by a factor of 6 was observed for the online portions of the roms this is even higher more than factor 30 if considering just the timings of the matrix inversion step reductions are the same for both roms due to their identical sizes in our test case the offline portion of calculating the snapshots and creating the projection matrix takes as much time as one full model run since it is often not necessary to repeat this step for rom applications calculation with different boundaries for example this is an acceptable time for pre processing note that run times are taken on a standard desktop pc for model implementations in matlab there is potential to further improve actual run times by for example compilation of the code 4 discussion the results presented in section 3 show that the eb pod method can be applied for all three types of boundary conditions originally pod models adhere to constant dirichlet boundaries through pre processing of snapshots while variable dirichlet boundaries could not be implemented this way at all alternatively dirichlet boundaries can be handled correctly by application of pod to drawdown simulations and calculation of groundwater heads through superposition but only for linear models neumann and cauchy boundaries are only implicitly handled this leads to flux errors through these boundaries which even in simple 1d models can become quite significant 10 relative error one can also see that changes at certain boundaries influence the flux errors at other boundaries the proposed eb pod method extends the general pod for all three boundary types in a straightforward manner that does not necessitate complicated computations model m3 shows the application to a variable dirichlet boundary which can only be reproduced by standard pod in special cases the resulting errors of the eb pod rom at the head boundary are negligible models m1 m2 and m5 are applications of the method for different neumann boundaries the explicit treatment of these boundaries through eb pod leads to a reduction of the flux errors through the boundaries to effectively zero model m2 shows that the interplay of several boundaries can lead to substantial miscalculation of these fluxes in pod reduced order models thus one can assume that justification for the eb pod method increases with the complexity of the model model m5 shows that the method works satisfactory in 3d allowing the representation of a 30 cell river boundary through one row in the eb pod projection matrix the roms exhibit a run time reduction by a factor of 6 while model m4 shows that the method can also be applied for cauchy boundaries the accompanying flux errors through this boundary type cannot be eliminated completely cauchy boundaries are composed of a head independent i e linear and a head dependent i e non linear part where the latter is calculated from the approximated head instead of explicitly enforced in eb pod and is therefore error prone nonetheless the eb pod method still explicitly adheres to the head independent part of the cauchy boundary thus diminishing the resulting flux error compared to the original pod all in all the new eb pod method successfully handles rom errors at boundaries this elimination or reduction for cauchy type boundaries of rom flux errors through eb pod comes at a cost the overall accuracy of the head field in the aquifer is somewhat lower for eb pod models compared to original pod models of the same build formed from the same snapshots with the same number of retained singular vectors as is explained in section 2 3 5 this is due to the method s handling of the original projection matrix p which is projected onto a smaller subspace ψ of size n r 2 n r n b c to allow space for the independent boundary condition equations this projection is accompanied by a loss of information this also means that the more independent boundary conditions are treated explicitly by eb pod the larger the error of the new rom with respect to the head field furthermore the theoretical maximum of such independent boundaries would be n b c n r 1 as a higher number of boundaries would deduct all information of the original projection matrix p practically this limit is even smaller as a number of nbc very close to nr would probably result in a highly inaccurate rom note that this limit refers to independent boundaries though in groundwater modeling multiple cells in the model domain are often described by a single boundary in addition this only limits the number of explicitly treated boundary conditions not the overall number of boundaries in the full model essentially the eb pod method provides a trade off for reduced order modeling with pod the elimination dirichlet and neumann or reduction cauchy of boundary errors comes at a cost of increasing inaccuracy of the overall head approximation with increasing number of independent boundaries treated by eb pod fortunately this trade off can be easily quantified implementation of different boundaries via eb pod is straight forward and computationally inexpensive therefore the modeler can test different scenarios regarding boundary condition treatment by eb pod and compare both reduction of flux errors and increase in head errors with the original pod model and other scenarios furthermore models are often built with a specific purpose in mind and the accuracy may be required at certain locations and in certain outputs only if for example the purpose of the model is the quantification of river aquifer exchange fluxes the correct calculation of these fluxes is of higher interest than the general head field in contrast if head approximations of the reduced order model at specific measurement points is the main target of the model one can analyze the distribution of change in head error through application of eb pod this targeted analysis allows the modeler an informed decision on which if any boundary conditions to treat this way similar choices have to be made for general pod reduction as well the choice of specific relative retained variance rv see section 2 2 is one of computational speed versus precision of the reduced order model in comparison the decision regarding boundary condition treatment through eb pod is one about accuracy at specific boundaries versus accuracy in the overall head field fortunately this is a choice that can be made with quantifiable information on its ramifications thus handing all power to the user for target oriented reduced order modeling 5 summary and conclusion the newly proposed eb pod method allows the explicit handling of dirichlet neumann and cauchy boundary conditions for pod based groundwater model reduction pod s projection of the system equations onto a subspace does not account for the boundary conditions in the explicit way the original model equations do by modifying the projection matrix to be orthogonal to the boundary conditions and then adding the original boundary equations to the final system of equations the eb pod method treats the boundary conditions in the same explicit way as the original model the method is applied to five synthetic test cases to test it against all three types of boundary conditions boundary errors for dirichlet and neumann boundaries are successfully eliminated through our new pod extension eb pod reduces flux errors at cauchy boundaries compared to pod though not to zero due to their head dependence the test cases highlight the new method s ability to severely enhance accuracy of pod rom s boundary representation even in 3d while retaining computational time savings of the original pod method the implementation of the method results in slightly larger errors in the overall head field though this stems from the method s reduction in size of the original pod projection matrix containing the snapshot information fortunately the effect of this trade off can easily be quantified we therefore provide a tool for target orientated decision making on intrinsic boundary accuracy with the eb pod method this adds a direct and specific way of controlling the rom accuracy to the present choice of snapshot selection and retained variance while the method has been applied to simple synthetic test cases in groundwater modeling in this study the application to and its effects on real world models is an area of future investigation furthermore the method does not deal with the shortcomings of general pod reduction for non linear models it could potentially be combined with techniques like pod deim see stanko et al 2016 to improve its computational performance for non linear groundwater models acknowledgments this work was funded by the german research foundation grant wo 1781 1 1 the author would like to thank michael sinsbeck at simtech universität stuttgart for helpful insights and discussions 
