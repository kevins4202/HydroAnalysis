index,text
25755,soil temperature is a key driver of several physical chemical and biological processes the environmental policy integrated climate epic is a comprehensive ecosystem model that simulates soil temperature dynamics using a cosine function approach driven by daily air temperature and average annual soil temperature at damping depth which may erroneously predict lower soil temperatures in winter a new cosine model and a pseudo heat transfer model were therefore developed and implemented for simulating soil temperature the two methods were evaluated by comparing simulated daily soil temperatures with observed data at 24 study sites results showed that the two new methods had similar performance and the better statistical results obtained with these new methods demonstrated the ability to better predict the soil temperature for a wide range of pedoclimatic conditions land management and land uses the main reason for the improved performance was due to a better prediction of soil temperature during the winter period keywords apex model cosine function soil heat transfer damping depth 1 introduction soil temperature greatly affects the biological physical and chemical processes that take place in terrestrial ecosystems which are key to many environmental studies and the management of earth surface resources schoonover and crim 2015 for instance soil temperature plays an important role in facilitating or inhibiting soil respiration when the temperature changes and goes above or below certain threshold values lellei kovács et al 2011 low temperatures can lead to soil freezing which usually inactivate below ground biochemical processes affecting not only water subsurface flow but also nutrient dynamics fitzhugh et al 2001 for temperatures ranging from 0 to 5 c root growth of most plants and germination of most seeds are inhibited soil survey staff 1999 with all being equal seeds germinate earlier in the season with high soil temperature and the fluctuations of soil temperature may initiate or accelerate the germination process alvarado and bradford 2002 baskin and baskin 1998 bewley and black 1994 moreover plant growth and biomass production can also be affected by soil temperature heinze et al 2016 soil temperature along with soil water content is the main driver of soil microbial growth and activities including the mineralization of organic matter with consequent co2 production in this way soil temperature is a critical variable controlling below ground processes that affects the carbon cycle the direct correlation between soil temperature and the microbial activity has been demonstrated by many studies andresen and jensen 2001 delogu et al 2017 han et al 2019 kurganova et al 2007 lai et al 2019 pietikainen et al 2005 in addition soil temperature influences the physical and chemical processes that take place in the pedosphere and consequently affects the transport and fate of contaminants in the subsurface environment biswas et al 2018 finally surface temperature controls thermodynamic equilibrium between the solid the liquid and the gas phases hence acting on surface fluxes of solutes like pesticides and especially ammonia which compensation point doubles every 5 c flechard et al 2013 although soil temperature is a key driver of several processes which are relevant in agroecosystems long term quality data of soil temperature are not available at most standard weather stations hu and feng 2003 and are often limited to research sites donatelli et al 2014 direct measurements of soil temperature are often limited to specific studies and locations and for relatively short periods the natural resources conservation service soil climate analysis network scan schaefer et al 2007 and the fluxnet dataset https fluxnet fluxdata org accessed november 18 2020 have established some long term measuring stations but most historical soil temperature databases have been compiled from various field experiments with limited temporal and spatial coverage and at various soil depths for specific applications only holmes et al 2008 therefore using modeling approaches to determine the soil temperature at different depths would be very helpful in several agricultural fields and for many related applications donatelli et al 2014 sándor and fodor 2012 sharma et al 2010 considering the importance of processes affected by soil temperature the correct soil temperature prediction at different depths is necessary for studies that rely on process based agro ecological models for instance the accurate simulation of soil temperatures allows a better prediction of seed germination allen 2003 plant growth chandrvavanshi et al 2019 and the timing and rate of biophysical processes kim et al 2015 with substantial consequences on nutrients dynamics and availability wegehenkel et al 2019 on the contrary a poor prediction of soil temperature would over or under estimate the emission of trace gases such as carbon dioxide co2 and nitrous oxide n2o oertel et al 2016 this aspect is important even for short periods when for example the wrong simulation of a sudden change of the soil temperature can generate unrealistic spikes in trace gases emissions the main drivers of soil temperature are solar radiation and air temperature while secondary drivers are soil texture bulk density soil moisture content soil surface cover such as litter and vegetation etc du et al 2013 ni et al 2019 ochsner et al 2001 considerable research has been conducted to develop models relating soil temperature to easily measured and readily available variables such as air temperature solar radiation and soil physical properties dardo et al 2001 elias et al 2004 harris 2007 lei et al 2010 the importance of the plant canopy and the mass of litter in estimating the soil temperature has been proven by several studies such as in paul et al 2004 the complexity of these models varies and so does the accuracy required for given applications although these models capture the seasonal pattern and describe the temperature profile at various soil depths they are restricted by limited representation of weather and soil conditions temperature of soils depends mainly on the surface heat balance and on the ratio of absorbed heat energy to energy lost in soils routines for practical and flexible soil temperature models are mostly based on heat conduction equation and the energy balance at the ground surface hiller 1982 marshall and holmes 1988 wijk and vries 1966 these models can give sufficiently accurate estimates provided the required input data are available which might limit their utilization mahrer and katan 1981 developed a two dimensional numerical model for predicting the soil temperature spatial regime however it also needs a significant number of input variables which might be not readily available for wide scale applications mihalakakou 2002 presented a neural network soil temperature model which depends strongly on the training data more recently approaches based on machine learning informed by meteorological data such as wind speed air temperature relative humidity solar radiation and vapor pressure deficit were developed and tested for soil temperature estimations in china feng et al 2019 the use of machine learning models was found to be much more successful than neural networks sattari et al 2020 however the dilemma remains that the major challenge for accurate soil temperature prediction is the unavailability of the needed meteorological data in most cases talaee 2014 the environmental policy integrated climate epic model williams 1990 1995 is a comprehensive mechanistic ecosystem model widely used in studies ranging from crop growth and yield to irrigation nutrient cycling and losses wind and water erosion soil moisture and temperature regimes gassman et al 2005 wang et al 2015 and soil carbon sequestration causarano et al 2008 while epic has been used as an important tool to support agricultural and environmental management in a variety of conditions in the u s abrahamson et al 2009 zhang et al 2015 and beyond assefa et al 2020 dono et al 2016 ehrhardt et al 2018 le et al 2018 sándor et al 2017 2020 including a global level investigation liu et al 2007 the model has been actively maintained and continuously improved and detailed information on the development of the epic model over time are provided by wang et al 2011 and gassman et al 2005 in epic a cosine function approach based on the drivers of soil temperature which are daily air temperature and average annual soil temperature at damping depth is applied to simulate daily soil temperature fluctuations this simplified approach may lead to less than optimal results as revealed for instance by roloff et al 1998 who underestimated soil temperature during cold winters in central eastern canada and doro et al 2017 who observed a similar problem while simulating the soil temperature in european experimental sites doro et al 2017 therefore pointed out the need to improve epic soil temperature simulation because it unrealistically estimates freezing soil temperature which consequently leads to unrealistic predictions of soil water and nutrient dynamics the goal of this study was to improve the soil temperature simulation using the epic model as the framework specific objectives were 1 to develop an enhanced cosine function approach and a new method based on the heat transfer concept and 2 to evaluate the new approaches using field data from a variety of climates soils and land uses worldwide i e 24 cropland and grassland monitoring sites across seven countries as stated above in this work the epic model was used as the framework to test the new approaches to estimate the soil temperature but with minimal modifications they can be integrated and used in any daily time step simulation model 2 methods 2 1 original approach and model improvements epic was developed in the early 1980s to simulate the effects of soil erosion on soil productivity i e crop yield the model has been actively maintained ever since and expanded to simulate different coupled physical and biochemical processes the major components are weather hydrology erosion sedimentation plant growth nutrient cycling pesticide fate soil temperature tillage and plant environment control williams 1990 the original epic soil temperature module estimates the temperature at the center of each soil layer as variation between the soil surface temperature and the soil temperature at damping depth as a function of the depth of each soil layer and a lag coefficient that weights the effect of yesterday s temperature sharpley and williams 1990 a basic cosine function was lately introduced to simulate the soil temperature based on the variation of the temperature between the soil surface and the damping depth dd in meters in this case the surface temperature was estimated as a function of daily air temperature and solar radiation williams et al 1984 the basic equation to calculate the soil temperature ts at the center of each layer i was as below 1 t s i d t a y t a m p 2 e z d d c o s 2 π d d t s m a x 365 25 z d d where z is the depth from the soil surface to the center of the soil layer i in m d is the day of the year during the simulation ta y is the long term average annual air temperature in c t amp is the annual amplitude of the average air temperature in c d d tsmax 365 25 is the fraction of the year with d tsmax as the average day of year when the average high soil temperature is reached which equals to 200 july 19 in the northern hemisphere and 20 january 20 in the southern hemisphere and the term 2π converts the fraction in radians ta y and t amp are estimated using the long term monthly average maximum and minimum air temperatures from the nearest weather station calculated for the entire weather series available in this way eq 1 produces a smooth temperature curve the same every year based on the long term average and the amplitude of air temperature to allow the model accounting for variations in temperature and solar radiation either estimated or user input on a daily basis the basic equation was modified as follow 2 t s i d t s i d t s 0 d t s 0 d e z d d where t s i d is the temperature in c of soil layer i on day d calculated from eq 1 t s 0 d is the surface soil temperature c calculated considering the mean air temperature c of the day d the solar radiation mj m 2 d 1 and the soil albedo while t s 0 d is the surface soil temperature c calculated with eq 1 the value of t s 0 d used to adjust eq 2 is a 5 day moving average of the soil temperature estimated for the day of interest and the four preceding days two new submodels the enhanced cosine submodel ecosine and the pseudo heat transfer submodel pht developed in this study are described in the following subsections a graphical representation that depicts the submodels development and simplifies the interaction between the factors used in the original and in the new approaches is shown in fig 1 as an overview of the approaches soil radiation air temperature and soil albedo have a direct effect on soil surface temperature in all three approaches fig 1 soil covers of snow f c s n o w plant f c p l a n t and litter f c b i o m affect the soil surface temperature in the ecosine and pht submodels additionally the snow cover affects the surface transfer coefficient u 0 the bulk density factor f b d i and the soil water content factor f s w i of each soil layer i affect the heat transfer coefficient between soil layers in all three submodels the damping depth is used in a cosine function to estimate the soil temperature at the bottom of the soil profile 2 1 1 submodel i the enhanced cosine because soil temperature depends mainly on the soil surface heat balance and on the ratio of absorbed heat energy to energy lost in soils this study further improved the cosine approach to provide a better simulation of the soil surface temperature the new enhanced cosine submodel ecosine includes the following equations which are empirically developed considering the effect of soil cover factors snow cover total above ground plant material cover and the fraction of ground covered by the leaf area index on the soil surface temperature 3 f c s n o w m i n p 87 z s n o w z s n o w e 1 44 0 37 z s n o w 4 f c b i o m m i n p 95 m a x f c p l a n t f c r s d f c r s d e 0 12 0 12 f c r s d 5 f c p l a n t l a i l a i e 1 75 1 75 l a i where f c s n o w is the snow cover factor p87 is a parameter for setting an upper limit on the snow cover factor z s n o w is the actual snow cover expressed in mm of water equivalent f c b i o m is the plant material cover factor p95 is a parameter for setting an upper limit on the vegetative cover factor f c p l a n t is the fraction of soil surface covered by plants in f c r s d is the fraction of soil surface covered by plant residue in mg ha 1 and lai is leaf area index in this way the soil surface temperature is directly affected and regulated by the presence of snow plant residues and plant cover simulated on each day during the warm season the surface soil temperature t s 0 d is calculated as follows 6 t s 0 d 1 f c b i o m t a m a x d t a m i n d 0 03 g t a a v d where t a m a x d and t a m i n d are the maximum and minimum air temperature in c on day d respectively g is the solar radiation in mj m 2 d 1 on day of the year d the term of t a m a x d t a m i n d is used as an indicator of relative humidity t a a v d is the average air temperature c on day d for the cool season the surface soil temperature is calculated as follows 7 t t a a v d t a m i n d 2 then if s r a d 10 t of eq 7 is adjusted for solar radiation 8 t t t a m a x d t a m i n d 0 03 g g a l b g a l b e 12 997 0 610 g a l b where g a l b is the solar radiation that affects the soil surface considering the soil albedo t is then adjusted for snow cover to get the value of soil temperature t s 0 d 9 t s 0 d 1 f c s n o w t the final t s 0 d used in the ecosine is a 5 day moving average of the soil surface temperature the effect of actual daily weather in estimating the soil surface temperature is further modified by using an exponent p85 for the term t s 0 d t s 0 d in eq 2 this modification smooths the impact of the difference between this calculated soil surface temperature t s 0 d and the soil surface temperature t s 0 d calculated with the original cosine approach eq 1 on daily soil temperature estimation the term t s 0 d t s 0 d in eq 2 is replaced by tt that is calculated as follows t t t s 0 d t s 0 d p 85 i f t s 0 d t s 0 d 0 10 t t t s 0 d t s 0 d p 85 i f t s 0 d t s 0 d 0 therefore eq 2 is replaced by the modified exponential function in the ecosine submodel and to provide improved predictions of soil temperature with depth a power of two is added as follows 11 t s i d t s i d t t e z d d 2 2 1 2 submodel ii the pseudo heat transfer the pseudo heat transfer submodel pht is a new more physically based approach compared to the cosine method elias et al 2004 lei et al 2010 the submodel is based on the concept of buffered thermal transfer between soil layers where both water and soil density play a role the soil surface temperature t s 0 d is calculated using the same eqs 3 8 as in the ecosine method however for the cool season instead of adjusting for snow cover as in eq 9 t s 0 d 1 f c s n o w t the snow cover factor f c s n o w is used for calculating surface transfer coefficient u 0 when snow is present the relationship between air and soil temperatures is different from that without snow due to the insulation property of the snow the transfer coefficient u is also adjusted by soil bulk density and water content giving faster heat transfer for wet soils with high bulk density 12 u i 0 6 f b d i f s w i 1 f c s n o w f c s n o w 0 w h e n i 1 where f c s n o w is the snow cover factor calculated as in eq 3 f b d i is bulk density factor and f s w i is soil water factor of soil layer i they are calculated as follow 13 s o c i 0 1 w o c i m s i 14 s o m i 1 72 s o c i 15 a d 1 i s a n i s i l i c l a i s o m i 16 f b d i 1 6 s a n i 1 3 s i l i 1 1 c l a i 0 224 s o m i a d 1 i 1 25 17 f s w i m i n 1 2 0 8 e 0 4055 θ i θ w p i θ f c i θ w p i i f θ i θ w p i where w o c i is the organic carbon of layer i in kg ha 1 m s i is the mass of soil layer i in mg ha 1 s o c i is the soil organic carbon in layer i in kg ha 1 s o m i is the soil organic matter of layer i in kg ha 1 s a n i s i l i and c l a i are sand silt and clay content of layer i in θ f c i θ w p i and θ i are the water contents in mm for layer i at field capacity wilting point and current conditions eq 12 indicates the importance of soil bulk density soil water content and snow cover in the calculation of the transfer coefficient and consequently in the simulation of soil temperature this highlights that a correct simulation of the soil water content is important to obtain satisfactory simulation of the soil temperature for the first day of the simulation the soil temperature at the center of each soil layer i is first simulated using the basic soil temperature equation as described in williams et al 1984 in pht the initial soil temperatures t s i 0 see below where d 1 0 provides the initial soil temperature are adjusted then for every following day the previous day d 1 soil temperature and the soil surface temperature on the current day are used to calculate the heat transferred t q i c from one soil layer to the next one soil temperature in layer 1 is adjusted using temperature transfer t q 1 which is between soil surface and soil layer 1 18 t q 1 u 0 t s 0 d t s 1 d 1 19 t s 1 d t s 1 d 1 t q 1 then adjustment is conducted for each layer 20 t q i u i t s i 1 d t s i d 1 i 1 21 t s i 1 d t s i 1 d 1 0 9 t q i 22 t s i d t s i d 1 t q i equation 21 adjusts the temperature of a soil layer to account for the heat transferred to the following layer soil temperature in the bottom layer i n is adjusted based on the stable temperature at the bottom of the soil profile s t i n d which is estimated using the equation 23 s t i n d t a y t a m p 2 exp 1 5 z d d c o s 2 π d d t s m a x 365 25 z d d 24 t s i n d 0 5 s t i n d t s i n d 1 2 2 study sites the new soil temperature submodels ecosine and pht implemented in epic were evaluated using field data from 24 sites in seven countries to ensure a wide spectrum of pedo climatic conditions table 1 five sites were located in europe france 2 germany 1 italy 1 switzerland 1 one in australia and 18 in north america united states 17 and canada 1 for france one site was a semi natural grazed upland grassland located in the massif central of france laqueuille on a silt loam soil 16 clay 56 silt 28 sand and 15 organic matter in the top 10 cm while the second site was a cropland located in paris basin grignon on a silt loam soil with 19 clay 71 silt 10 sand and 3 4 organic matter in the top 15 cm klumpp et al 2011 loubet et al 2011 the crop rotation was maize winter wheat winter barley mustard the german site grillenburg was a mown grassland with a mix of perennial species on a silty loam clay loam medium clay silt soil with 6 clay 80 silt 14 sand and 3 0 organic matter in the top 10 cm prescher et al 2010 the italian site monte bondone located in north italy was a semi natural mown 1 cut year upland grassland on a loam soil 18 clay 43 silt 39 sand and 4 8 organic matter in the top 10 cm wohlfahrt et al 2008 the swiss site located in the central swiss plateau oensingen was a semi natural mown grass clover permanent grassland on a soil with 43 clay 33 silt 24 sand and 1 8 organic matter in the top 10 cm ammann et al 2007 outside europe one site was located near ottawa canada sansoulet et al 2014 the site was a cropland cultivated with corn spring wheat spring canola and soybean in rotation on a silty clay loam soil 31 clay 49 silt and 20 sand with 5 2 organic matter in the top 15 cm the australian site kingaroy was a cropland cultivated with corn winter wheat rotation the soil has 53 clay 15 silt and 32 sand resulting in a clay texture with 2 2 organic matter in the top 10 cm de antoni migliorati et al 2014 data for the sites in australia canada and europe were made available through the research projects macsur and cn mip field data in the united states were from 17 historical scan sites table 1 characterized by perennial grasses as land cover 1 1 details on soil properties and attributes by layer can be found from the scan site soils pedon report e g for walnut gulch 1 site id 2026 available at http www wcc nrcs usda gov nwcc site sitenum 2026 accessed 18 november 2020 replace the id number to access other scan sites for all the study sites data on precipitation daily maximum and minimum air temperatures soil temperature at different depths i e 4 8 20 and 40 inches 10 20 50 and 100 mm for the scan sites were available because the scan sites generally have significant missing daily weather data the nearest weather stations were acquired for a 21 yr simulation 1990 2010 considering all the testing sites average annual precipitation ranged from 289 nunn 1 id 2017 in co usa to 1404 mm geneva 1 id 2011 in ny usa table 1 average monthly maximum temperature ranged from 18 3 c monte bondone italy to 35 c little washita 1 id 2023 in ok usa and average monthly minimum temperature ranged from 21 c glacial ridge 1 id 2050 in mn usa to 3 9 c prairie view 1 id 2016 in tx usa table 1 input files for the epic model were prepared using the epic version of the apexeditor leyton 2019 2 3 model evaluation to compare the results of the methods and evaluate model accuracy four statistical indices were calculated the akaike information criterion aic akaike 1974 coefficient of determination r2 nash sutcliffe efficiency nse nash and sutcliffe 1970 and percent bias pbias aic is a commonly used criteria in model selection asl rousta et al 2017 perez sanchez et al 2019 symmonds and moussalli 2011 and is based on information theory as a criterion it seeks a model that has a good fit to the observations but few parameters aic can be calculated as in eq 25 by using residual sum of squares rss 25 a i c n l n r s s n 2 k where n is the sample size k is the number of parameters the root mean squared error of the model output rmse was used in this study to replace the term rss n in eq 25 26 r m s e i 1 n t s i m i t o b s i 2 n where t obs i and t sim i are the observed and simulated soil temperature on day i respectively aic criteria classify the best model when the value achieved is lower summary statistics and goodness of fit measures were selected for the model evaluation based on suggestions given by loague and green 1991 wallach 2006 and bellocchi et al 2010 the r2 was used to evaluate how accurately the model tracks the variation of observed values it ranges from 0 to 1 best with higher values indicating less error variance the nse was used to compare the model error against the variability around the mean of the observed data it ranges from 1 best to minus infinite worse with negative values meaning that the simple average of the observed data is a better predictor than the model tested pbias was used to assess under positive value or over estimation negative value and the magnitude of error it ranges from positive infinite to negative infinite with the best result obtained at zero 27 r 2 i 1 n t o b s i t o b s t s i m i t s i m 2 i 1 n t o b s i t o b s 2 i 1 n t s i m i t s i m 2 28 n s e 1 i 1 n t o b s i t s i m i 2 i 1 n t o b s i t o b s 2 29 p b i a s i 1 n t o b s i t s i m i 100 i 1 n t o b s i these model performance statistics were calculated using r r core team 2019 and the hydrogof package zambrano bigiarini 2020 model performance was considered good with r2 0 75 nse 0 65 and pbias within 20 considering the guideline proposed by moriasi for model evaluation moriasi et al 2007 as introduced above three parameters were added to this new development p87 eq 3 and p95 eq 4 used in both soil temperature submodels for adjusting soil cover factor while calculating soil surface temperature and p85 eq 10 used in ecosine to regulate the effect of actual daily weather on soil surface temperature table 2 the default values of the parameters were defined to give realistic results on a broader testing dataset and based on developers experience while developing the methods no site specific calibration was conducted for the sites where the default value of the three parameters directly related to soil temperature estimation gave good model performance grade otherwise the time series of each site specific dataset was split into two segments of equal length or with the validation set one year longer that the calibration one in the case of time series with an odd number of years in all the cases the first part of each time series was used for calibration and the second part for validation calibration of the three relevant parameters was performed manually until achieving good model performance grade 3 results and discussion 3 1 original epic approach compared to ecosine and pht methods the two new soil temperature methods were compared with the original epic approach and evaluated by comparing simulated and measured daily values of soil temperatures at different soil depths using the default parameter values listed in table 2 to test the robustness of the methods although the aic penalizes the number of model parameters it provided no evidence that the original method was better than the two methods developed in this study the original method has no parameter involved yet its aic achieved bigger values in all the 24 testing sites across 7 countries table 3 except for the top two soil layers in grignon indicating better results when using the new ecosine and pht methods no indication was observed that one of the new methods was consistently better than the other across the study sites however the ecosine method performed slightly better for the deeper depth and warmer environments while the pht provided better performance in simulating soil temperature of upper soil layers depth 20 cm and in colder sites this is probably because near surface and the shallow depth soil temperature is most affected by atmospheric variations in warmer environment the day to day temperature fluctuation can be significant and the pht approach can better simulate the transfer of heat form the soil surface to the upper soil layers also when snow is present the relationship between air and soil temperatures is different from that without snow due to insulation provided by the snow for the cold condition the pht performed better because the snow cover factor together with soil bulk density and water content are used for calculating surface transfer coefficient in addition to the aic we calculated r2 nse and pbias for the 24 sites using default values of the 3 parameters table 2 while comparing with the original method based on the comparison of observed and simulated daily soil temperatures at different soil depths the original method produced good modeling results for 11 out of 24 study sites table 4 with r2 ranging from 0 75 to 0 98 nse ranging from 0 65 to 0 96 and pbias from 18 5 to 15 6 however 18 out of the 24 study sites had good model performance at all the measurement depths using the ecosine and 20 out of the 24 sites had good model performance using the pth methods with r2 ranging from 0 80 to 0 98 nse ranging from 0 66 to 0 98 and pbias within 14 7 and 18 7 table 4 the results demonstrated the robustness of the two new methods which were able to provide good model performance in simulating the soil temperature under various soils and climates conditions it was encouraging to obtain good performance grade by using default parameter values related to the two new submodels described here additionally these model parameters provide a convenient way to calibrate the model to improve the modeling results for example there were 4 out of the 24 study sites where the model performance statistics were under the good grade as set in the model evaluation section using all three soil temperature methods as indicated by the lowest nse values of 0 50 0 52 and 0 53 and largest pbias values of 34 1 29 4 and 27 3 while using the original ecosine and pht methods respectively table 4 and there were two additional sites mt 2019 and nm 2107 that had improved performance while using the ecosine method compared to the original method but still were under the good grade of model performance we chose to calibrate the six scan sites for the ecosine and pht methods see next section yet there was no modular parameter in the original soil temperature method to facilitate the calibration process figs 2 and 3 clearly show the significant under estimation obtained with the original approach during winter periods at the grillenburg site for instance in january 2006 the original epic approach simulated approximately 14 c at 10 cm soil depth while the corresponding observed soil temperature was about 0 c fig 2 similar results were observed also at the italian and swiss sites where the simulated soil temperature was significantly lower than the observed soil temperature during the coldest months see fig 1 s to 4 s included in the supplemental material this was caused by the simplified approach in the original cosine function method which is based on the drivers of daily air temperature and average annual soil temperature at damping depth also noted by other researchers for example roloff et al 1998 showed pronounced underestimated soil temperature during cold winters in central eastern canada the ecosine and pht submodel improved the capability of epic to simulate the soil temperature by eliminating the underestimation in the winter period estimated with the original approach figs 2 and 3 moreover both new methods produced a smother variation of the daily soil temperature resulting in a better representation of the observed soil temperature dynamics as reflected by better model performance statistics the accurate prediction of soil surface temperature is critical for deriving realistic soil temperature profile prediction among the study sites only the french site of grignon reported soil surface temperature observations both the ecosine and pht submodels performed well in simulating the soil surface temperature as indicated by r2 and nse 0 80 and pbias within 5 table 4 both methods over estimated a peak in the soil surface temperature in the middle of may 2008 and at the beginning of october 2011 figs 4 and 5 the high and unusual solar radiation on bare soil affected model prediction and differences were observed for short periods for the temperatures simulated at the soil surface by the two methods saito and simunek 2009 and mahfouf and noilhan 1991 also showed that simulated soil surface temperatures could easily be altered by 50 or more because of inaccurate prediction of the dynamics of the surface energy components while these variations and overestimations were no longer present at 30 cm depth by using the ecosine the overestimations propagated to deeper soil layers and their impact was noticeable at 30 and 50 cm depths using the pht submodel this is because the heat transfer method pht is sensitive to soil surface temperature and temperature transfer which is affected by soil bulk density and water content chalhoub et al 2017 also noted that a proper account of the soil surface cover and site specific soil properties is necessary to obtain accurate soil temperature predictions with uncertainties associated with model inputs e g soil properties model structure e g simplifying assumptions in mathematical terms to represent complex soil plant atmosphere systems and soil temperature measurement it is expected that the soil temperature predictions cannot fully replicate observations with the two new soil temperature submodel options users could select the most suitable option with regards to data availability and agricultural settings for the grignon site the ecosine approach might be preferable to minimize the impact of nonoptimal estimation of the soil surface temperature 3 2 further calibration and validation for six scan sites in the previous study of the scan sites where the original epic soil temperature approach was used significant under and over predictions were obtained pbias ranged from 18 2 to 34 1 the r2 values ranged from 0 73 to 0 98 and the nse values ranged from 0 50 to 0 96 wang et al 2015 because no relevant model parameter in the original epic soil temperature submodel for soil temperature calibration was available we could not improve its performance while it was possible for this study to meet good model performance grade with r2 0 75 nse 0 65 and pbias within 20 the two new soil temperature methods proposed in this study provides the possibility to calibrate the model by adjusting the three new parameters related to soil surface temperature prediction to improve model performance for the 6 scan sites table 5 as shown in table 4 the original method produced the lowest nse value of 0 50 and the largest pbias value of 34 1 among the 6 study sites the ecosine and pht methods had better modeling performance with nse 0 52 and pbias values ranging from 14 7 to 27 3 while using default parameter values table 2 for the calibration and validation processes observation data at the 6 sites were split in two p85 used in the ecosine method for regulating the effect of daily weather on soil surface temperature was adjusted for all the 6 sites p87 and p95 used to adjust daily snow and plant cover factors could be adjusted in both new methods after calibration the simulated daily soil temperatures were in close agreement with observed values at all tested depths with r2 ranging from 0 85 to 0 98 nse from 0 66 to 0 94 and pbias within 18 table 6 indicating good model performance this demonstrates that after refining the parameters related to soil surface temperature estimation both methods could be used with good accuracy at different soil depths the aic criteria indicate that the pht submodel was able to provide better results compare to the ecosine approach when the model was site calibrated table 6 as an example fig 6 illustrates the time series comparison for daily soil temperature at the 50 cm soil depth for the lind 1 wa 2021 site it shows a generally good agreement between the simulated and observed values there were both under and over estimations larger deviations were observed during some winter and summer months in some cases the performance of the model could be distorted by observation error and uncertainty like what we observed on november 1 1996 on that date a sudden drop in soil temperature at 50 cm was recorded with the three consecutive daily observed soil temperatures equal to 8 8 c 0 3 c and 9 5 c such abrupt change of the soil temperature at 50 cm depth is very unlikely in reality and the corresponding simulated soil temperatures were 5 9 c 5 6 c and 5 3 c by the ecosine method and 9 9 c 9 6 c and 9 4 c by the pht on top of possible errors in the observations the use of weather data from nearby weather stations may have contributed to the deviation between simulated and observed values 3 3 impact of soil temperature submodels on plant emergence and crop yield predictions of emergence dates at the study sites in australia canada and europe were compared with available observations in all cases the emergence dates simulated with the three soil temperature approaches were very close to observations differences in simulated germination dates were found in a few years at the australian site the observed winter wheat germinated on the 20th of july in 2011 but epic simulated emergence earlier with all three soil temperature submodels the original epic approach estimated 10 days earlier than the observed emergence date and the ecosine and pht models simulated 6 days earlier these results were in line with those reported by de antoni migliorati et al 2015 who simulated the same dataset with the daycent model in grignon emergence of maize took place on 14th may of 2008 while the simulated emergence dates were may 15 2008 with the ecosine and pht models and may 17 2008 with the original approach in canada spring wheat emerged on may 25 2007 the new models simulated the emergence two days earlier while the original approach in epic simulated one day earlier variations in emergence prediction were caused by soil temperature prediction difference in the three soil temperature approaches along with the difference observed in the emergence date small differences were observed for crop yield simulated with the three soil temperature approaches when considering all the data available for the sites in europe canada and australia the epic model performed well with any of the three soil temperature approaches figs 7 and 5 s a slightly higher pbias was obtained for the crop yield estimated with the original epic soil temperature approach for two years 2007 08 at laqueuille france a grassland site with grazing management epic underestimated yield using any soil temperature approach 4 conclusions although the epic model has been widely applied throughout the u s and worldwide continuous evolution and improvement of major process such as soil temperature is essential for broad and critical applications of this comprehensive agro ecosystem model by considering the soil snow and the plant material cover factors and using a physically based heat transfer algorithm the development of two new soil temperature modules and their evaluation at 24 sites showed increased ability of epic to better simulate soil temperature under varying environmental conditions compared to the original approach the new methods provided more accurate and realistic simulations of the soil temperature for broad agroecosystems conditions this study did not offer a fully conclusive answer as for which of the two newly developed methods was a better choice overall however it outlined the robustness of the ecosine method which produced better results for most of the tested sites using default parameter values on the other hand the pht provided better performance in simulating soil temperature of top soil layers depth 20 cm and when the soil was covered by snow thus the pht method would be better suited for sites having snow cover in winter while the ecosine approach could be successfully used with default parameter values in most cases this aspect is especially important for sites that do not have measured soil temperature available for model calibration by using the improved epic ecosine module the soil temperature could be well predicted at different depths reducing the prediction uncertainty in other soil temperature driven biological physical and chemical processes of terrestrial ecosystems since both ecosine and pht modules were incorporated in the epic model users have now the choice to select a relevant soil temperature module for their modeling applications with these promising results the two new soil temperature modules have been incorporated into the agricultural policy environmental extender apex model williams and izaurralde 2006 to replace the original soil temperature method this improvement is believed to describe the many interactions of agricultural production systems most accurately however further evaluation is needed about the effects of the new methods on other variables simulated by the epic and apex models such as mineralization nitrification volatilization soil water content crop yield and runoff opportunities exist to test them in different climatic and regional conditions in our conservation effects assessment project ceap cropland national assessment program duriancik et al 2008 author contributions l d x w m l n and j r w developed and tested the soil temperature prediction submodels performed the statistical evaluation of the results and developed the manuscript c a m dea m t g k k b l e p and g w provided the observed data for the experimental sites in canada europe and australia and contributed editing and improving the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work was supported by the cn mip and macsur projects of the joint programming initiative facce jpi https www faccejpi com the funding by eu projects carboeurope ip nitroeurope ip and icos by german bmbf project icos d by the state of saxony tu dresden lfulg and by agriculture and agri food canada is greatly appreciated this study received partial funding by the province of südtirol alto adige within the frame of the project cycling of carbon and water in mountain ecosystems under changing climate and land use cyclamen k klumpp acknowledges the support of the anaee france anr 11 inbs 0001 in natura soere acbb permanent grassland site laqueuille research was also conducted with support from the usda nrcs conservation effects assessment project ceap appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105140 
25755,soil temperature is a key driver of several physical chemical and biological processes the environmental policy integrated climate epic is a comprehensive ecosystem model that simulates soil temperature dynamics using a cosine function approach driven by daily air temperature and average annual soil temperature at damping depth which may erroneously predict lower soil temperatures in winter a new cosine model and a pseudo heat transfer model were therefore developed and implemented for simulating soil temperature the two methods were evaluated by comparing simulated daily soil temperatures with observed data at 24 study sites results showed that the two new methods had similar performance and the better statistical results obtained with these new methods demonstrated the ability to better predict the soil temperature for a wide range of pedoclimatic conditions land management and land uses the main reason for the improved performance was due to a better prediction of soil temperature during the winter period keywords apex model cosine function soil heat transfer damping depth 1 introduction soil temperature greatly affects the biological physical and chemical processes that take place in terrestrial ecosystems which are key to many environmental studies and the management of earth surface resources schoonover and crim 2015 for instance soil temperature plays an important role in facilitating or inhibiting soil respiration when the temperature changes and goes above or below certain threshold values lellei kovács et al 2011 low temperatures can lead to soil freezing which usually inactivate below ground biochemical processes affecting not only water subsurface flow but also nutrient dynamics fitzhugh et al 2001 for temperatures ranging from 0 to 5 c root growth of most plants and germination of most seeds are inhibited soil survey staff 1999 with all being equal seeds germinate earlier in the season with high soil temperature and the fluctuations of soil temperature may initiate or accelerate the germination process alvarado and bradford 2002 baskin and baskin 1998 bewley and black 1994 moreover plant growth and biomass production can also be affected by soil temperature heinze et al 2016 soil temperature along with soil water content is the main driver of soil microbial growth and activities including the mineralization of organic matter with consequent co2 production in this way soil temperature is a critical variable controlling below ground processes that affects the carbon cycle the direct correlation between soil temperature and the microbial activity has been demonstrated by many studies andresen and jensen 2001 delogu et al 2017 han et al 2019 kurganova et al 2007 lai et al 2019 pietikainen et al 2005 in addition soil temperature influences the physical and chemical processes that take place in the pedosphere and consequently affects the transport and fate of contaminants in the subsurface environment biswas et al 2018 finally surface temperature controls thermodynamic equilibrium between the solid the liquid and the gas phases hence acting on surface fluxes of solutes like pesticides and especially ammonia which compensation point doubles every 5 c flechard et al 2013 although soil temperature is a key driver of several processes which are relevant in agroecosystems long term quality data of soil temperature are not available at most standard weather stations hu and feng 2003 and are often limited to research sites donatelli et al 2014 direct measurements of soil temperature are often limited to specific studies and locations and for relatively short periods the natural resources conservation service soil climate analysis network scan schaefer et al 2007 and the fluxnet dataset https fluxnet fluxdata org accessed november 18 2020 have established some long term measuring stations but most historical soil temperature databases have been compiled from various field experiments with limited temporal and spatial coverage and at various soil depths for specific applications only holmes et al 2008 therefore using modeling approaches to determine the soil temperature at different depths would be very helpful in several agricultural fields and for many related applications donatelli et al 2014 sándor and fodor 2012 sharma et al 2010 considering the importance of processes affected by soil temperature the correct soil temperature prediction at different depths is necessary for studies that rely on process based agro ecological models for instance the accurate simulation of soil temperatures allows a better prediction of seed germination allen 2003 plant growth chandrvavanshi et al 2019 and the timing and rate of biophysical processes kim et al 2015 with substantial consequences on nutrients dynamics and availability wegehenkel et al 2019 on the contrary a poor prediction of soil temperature would over or under estimate the emission of trace gases such as carbon dioxide co2 and nitrous oxide n2o oertel et al 2016 this aspect is important even for short periods when for example the wrong simulation of a sudden change of the soil temperature can generate unrealistic spikes in trace gases emissions the main drivers of soil temperature are solar radiation and air temperature while secondary drivers are soil texture bulk density soil moisture content soil surface cover such as litter and vegetation etc du et al 2013 ni et al 2019 ochsner et al 2001 considerable research has been conducted to develop models relating soil temperature to easily measured and readily available variables such as air temperature solar radiation and soil physical properties dardo et al 2001 elias et al 2004 harris 2007 lei et al 2010 the importance of the plant canopy and the mass of litter in estimating the soil temperature has been proven by several studies such as in paul et al 2004 the complexity of these models varies and so does the accuracy required for given applications although these models capture the seasonal pattern and describe the temperature profile at various soil depths they are restricted by limited representation of weather and soil conditions temperature of soils depends mainly on the surface heat balance and on the ratio of absorbed heat energy to energy lost in soils routines for practical and flexible soil temperature models are mostly based on heat conduction equation and the energy balance at the ground surface hiller 1982 marshall and holmes 1988 wijk and vries 1966 these models can give sufficiently accurate estimates provided the required input data are available which might limit their utilization mahrer and katan 1981 developed a two dimensional numerical model for predicting the soil temperature spatial regime however it also needs a significant number of input variables which might be not readily available for wide scale applications mihalakakou 2002 presented a neural network soil temperature model which depends strongly on the training data more recently approaches based on machine learning informed by meteorological data such as wind speed air temperature relative humidity solar radiation and vapor pressure deficit were developed and tested for soil temperature estimations in china feng et al 2019 the use of machine learning models was found to be much more successful than neural networks sattari et al 2020 however the dilemma remains that the major challenge for accurate soil temperature prediction is the unavailability of the needed meteorological data in most cases talaee 2014 the environmental policy integrated climate epic model williams 1990 1995 is a comprehensive mechanistic ecosystem model widely used in studies ranging from crop growth and yield to irrigation nutrient cycling and losses wind and water erosion soil moisture and temperature regimes gassman et al 2005 wang et al 2015 and soil carbon sequestration causarano et al 2008 while epic has been used as an important tool to support agricultural and environmental management in a variety of conditions in the u s abrahamson et al 2009 zhang et al 2015 and beyond assefa et al 2020 dono et al 2016 ehrhardt et al 2018 le et al 2018 sándor et al 2017 2020 including a global level investigation liu et al 2007 the model has been actively maintained and continuously improved and detailed information on the development of the epic model over time are provided by wang et al 2011 and gassman et al 2005 in epic a cosine function approach based on the drivers of soil temperature which are daily air temperature and average annual soil temperature at damping depth is applied to simulate daily soil temperature fluctuations this simplified approach may lead to less than optimal results as revealed for instance by roloff et al 1998 who underestimated soil temperature during cold winters in central eastern canada and doro et al 2017 who observed a similar problem while simulating the soil temperature in european experimental sites doro et al 2017 therefore pointed out the need to improve epic soil temperature simulation because it unrealistically estimates freezing soil temperature which consequently leads to unrealistic predictions of soil water and nutrient dynamics the goal of this study was to improve the soil temperature simulation using the epic model as the framework specific objectives were 1 to develop an enhanced cosine function approach and a new method based on the heat transfer concept and 2 to evaluate the new approaches using field data from a variety of climates soils and land uses worldwide i e 24 cropland and grassland monitoring sites across seven countries as stated above in this work the epic model was used as the framework to test the new approaches to estimate the soil temperature but with minimal modifications they can be integrated and used in any daily time step simulation model 2 methods 2 1 original approach and model improvements epic was developed in the early 1980s to simulate the effects of soil erosion on soil productivity i e crop yield the model has been actively maintained ever since and expanded to simulate different coupled physical and biochemical processes the major components are weather hydrology erosion sedimentation plant growth nutrient cycling pesticide fate soil temperature tillage and plant environment control williams 1990 the original epic soil temperature module estimates the temperature at the center of each soil layer as variation between the soil surface temperature and the soil temperature at damping depth as a function of the depth of each soil layer and a lag coefficient that weights the effect of yesterday s temperature sharpley and williams 1990 a basic cosine function was lately introduced to simulate the soil temperature based on the variation of the temperature between the soil surface and the damping depth dd in meters in this case the surface temperature was estimated as a function of daily air temperature and solar radiation williams et al 1984 the basic equation to calculate the soil temperature ts at the center of each layer i was as below 1 t s i d t a y t a m p 2 e z d d c o s 2 π d d t s m a x 365 25 z d d where z is the depth from the soil surface to the center of the soil layer i in m d is the day of the year during the simulation ta y is the long term average annual air temperature in c t amp is the annual amplitude of the average air temperature in c d d tsmax 365 25 is the fraction of the year with d tsmax as the average day of year when the average high soil temperature is reached which equals to 200 july 19 in the northern hemisphere and 20 january 20 in the southern hemisphere and the term 2π converts the fraction in radians ta y and t amp are estimated using the long term monthly average maximum and minimum air temperatures from the nearest weather station calculated for the entire weather series available in this way eq 1 produces a smooth temperature curve the same every year based on the long term average and the amplitude of air temperature to allow the model accounting for variations in temperature and solar radiation either estimated or user input on a daily basis the basic equation was modified as follow 2 t s i d t s i d t s 0 d t s 0 d e z d d where t s i d is the temperature in c of soil layer i on day d calculated from eq 1 t s 0 d is the surface soil temperature c calculated considering the mean air temperature c of the day d the solar radiation mj m 2 d 1 and the soil albedo while t s 0 d is the surface soil temperature c calculated with eq 1 the value of t s 0 d used to adjust eq 2 is a 5 day moving average of the soil temperature estimated for the day of interest and the four preceding days two new submodels the enhanced cosine submodel ecosine and the pseudo heat transfer submodel pht developed in this study are described in the following subsections a graphical representation that depicts the submodels development and simplifies the interaction between the factors used in the original and in the new approaches is shown in fig 1 as an overview of the approaches soil radiation air temperature and soil albedo have a direct effect on soil surface temperature in all three approaches fig 1 soil covers of snow f c s n o w plant f c p l a n t and litter f c b i o m affect the soil surface temperature in the ecosine and pht submodels additionally the snow cover affects the surface transfer coefficient u 0 the bulk density factor f b d i and the soil water content factor f s w i of each soil layer i affect the heat transfer coefficient between soil layers in all three submodels the damping depth is used in a cosine function to estimate the soil temperature at the bottom of the soil profile 2 1 1 submodel i the enhanced cosine because soil temperature depends mainly on the soil surface heat balance and on the ratio of absorbed heat energy to energy lost in soils this study further improved the cosine approach to provide a better simulation of the soil surface temperature the new enhanced cosine submodel ecosine includes the following equations which are empirically developed considering the effect of soil cover factors snow cover total above ground plant material cover and the fraction of ground covered by the leaf area index on the soil surface temperature 3 f c s n o w m i n p 87 z s n o w z s n o w e 1 44 0 37 z s n o w 4 f c b i o m m i n p 95 m a x f c p l a n t f c r s d f c r s d e 0 12 0 12 f c r s d 5 f c p l a n t l a i l a i e 1 75 1 75 l a i where f c s n o w is the snow cover factor p87 is a parameter for setting an upper limit on the snow cover factor z s n o w is the actual snow cover expressed in mm of water equivalent f c b i o m is the plant material cover factor p95 is a parameter for setting an upper limit on the vegetative cover factor f c p l a n t is the fraction of soil surface covered by plants in f c r s d is the fraction of soil surface covered by plant residue in mg ha 1 and lai is leaf area index in this way the soil surface temperature is directly affected and regulated by the presence of snow plant residues and plant cover simulated on each day during the warm season the surface soil temperature t s 0 d is calculated as follows 6 t s 0 d 1 f c b i o m t a m a x d t a m i n d 0 03 g t a a v d where t a m a x d and t a m i n d are the maximum and minimum air temperature in c on day d respectively g is the solar radiation in mj m 2 d 1 on day of the year d the term of t a m a x d t a m i n d is used as an indicator of relative humidity t a a v d is the average air temperature c on day d for the cool season the surface soil temperature is calculated as follows 7 t t a a v d t a m i n d 2 then if s r a d 10 t of eq 7 is adjusted for solar radiation 8 t t t a m a x d t a m i n d 0 03 g g a l b g a l b e 12 997 0 610 g a l b where g a l b is the solar radiation that affects the soil surface considering the soil albedo t is then adjusted for snow cover to get the value of soil temperature t s 0 d 9 t s 0 d 1 f c s n o w t the final t s 0 d used in the ecosine is a 5 day moving average of the soil surface temperature the effect of actual daily weather in estimating the soil surface temperature is further modified by using an exponent p85 for the term t s 0 d t s 0 d in eq 2 this modification smooths the impact of the difference between this calculated soil surface temperature t s 0 d and the soil surface temperature t s 0 d calculated with the original cosine approach eq 1 on daily soil temperature estimation the term t s 0 d t s 0 d in eq 2 is replaced by tt that is calculated as follows t t t s 0 d t s 0 d p 85 i f t s 0 d t s 0 d 0 10 t t t s 0 d t s 0 d p 85 i f t s 0 d t s 0 d 0 therefore eq 2 is replaced by the modified exponential function in the ecosine submodel and to provide improved predictions of soil temperature with depth a power of two is added as follows 11 t s i d t s i d t t e z d d 2 2 1 2 submodel ii the pseudo heat transfer the pseudo heat transfer submodel pht is a new more physically based approach compared to the cosine method elias et al 2004 lei et al 2010 the submodel is based on the concept of buffered thermal transfer between soil layers where both water and soil density play a role the soil surface temperature t s 0 d is calculated using the same eqs 3 8 as in the ecosine method however for the cool season instead of adjusting for snow cover as in eq 9 t s 0 d 1 f c s n o w t the snow cover factor f c s n o w is used for calculating surface transfer coefficient u 0 when snow is present the relationship between air and soil temperatures is different from that without snow due to the insulation property of the snow the transfer coefficient u is also adjusted by soil bulk density and water content giving faster heat transfer for wet soils with high bulk density 12 u i 0 6 f b d i f s w i 1 f c s n o w f c s n o w 0 w h e n i 1 where f c s n o w is the snow cover factor calculated as in eq 3 f b d i is bulk density factor and f s w i is soil water factor of soil layer i they are calculated as follow 13 s o c i 0 1 w o c i m s i 14 s o m i 1 72 s o c i 15 a d 1 i s a n i s i l i c l a i s o m i 16 f b d i 1 6 s a n i 1 3 s i l i 1 1 c l a i 0 224 s o m i a d 1 i 1 25 17 f s w i m i n 1 2 0 8 e 0 4055 θ i θ w p i θ f c i θ w p i i f θ i θ w p i where w o c i is the organic carbon of layer i in kg ha 1 m s i is the mass of soil layer i in mg ha 1 s o c i is the soil organic carbon in layer i in kg ha 1 s o m i is the soil organic matter of layer i in kg ha 1 s a n i s i l i and c l a i are sand silt and clay content of layer i in θ f c i θ w p i and θ i are the water contents in mm for layer i at field capacity wilting point and current conditions eq 12 indicates the importance of soil bulk density soil water content and snow cover in the calculation of the transfer coefficient and consequently in the simulation of soil temperature this highlights that a correct simulation of the soil water content is important to obtain satisfactory simulation of the soil temperature for the first day of the simulation the soil temperature at the center of each soil layer i is first simulated using the basic soil temperature equation as described in williams et al 1984 in pht the initial soil temperatures t s i 0 see below where d 1 0 provides the initial soil temperature are adjusted then for every following day the previous day d 1 soil temperature and the soil surface temperature on the current day are used to calculate the heat transferred t q i c from one soil layer to the next one soil temperature in layer 1 is adjusted using temperature transfer t q 1 which is between soil surface and soil layer 1 18 t q 1 u 0 t s 0 d t s 1 d 1 19 t s 1 d t s 1 d 1 t q 1 then adjustment is conducted for each layer 20 t q i u i t s i 1 d t s i d 1 i 1 21 t s i 1 d t s i 1 d 1 0 9 t q i 22 t s i d t s i d 1 t q i equation 21 adjusts the temperature of a soil layer to account for the heat transferred to the following layer soil temperature in the bottom layer i n is adjusted based on the stable temperature at the bottom of the soil profile s t i n d which is estimated using the equation 23 s t i n d t a y t a m p 2 exp 1 5 z d d c o s 2 π d d t s m a x 365 25 z d d 24 t s i n d 0 5 s t i n d t s i n d 1 2 2 study sites the new soil temperature submodels ecosine and pht implemented in epic were evaluated using field data from 24 sites in seven countries to ensure a wide spectrum of pedo climatic conditions table 1 five sites were located in europe france 2 germany 1 italy 1 switzerland 1 one in australia and 18 in north america united states 17 and canada 1 for france one site was a semi natural grazed upland grassland located in the massif central of france laqueuille on a silt loam soil 16 clay 56 silt 28 sand and 15 organic matter in the top 10 cm while the second site was a cropland located in paris basin grignon on a silt loam soil with 19 clay 71 silt 10 sand and 3 4 organic matter in the top 15 cm klumpp et al 2011 loubet et al 2011 the crop rotation was maize winter wheat winter barley mustard the german site grillenburg was a mown grassland with a mix of perennial species on a silty loam clay loam medium clay silt soil with 6 clay 80 silt 14 sand and 3 0 organic matter in the top 10 cm prescher et al 2010 the italian site monte bondone located in north italy was a semi natural mown 1 cut year upland grassland on a loam soil 18 clay 43 silt 39 sand and 4 8 organic matter in the top 10 cm wohlfahrt et al 2008 the swiss site located in the central swiss plateau oensingen was a semi natural mown grass clover permanent grassland on a soil with 43 clay 33 silt 24 sand and 1 8 organic matter in the top 10 cm ammann et al 2007 outside europe one site was located near ottawa canada sansoulet et al 2014 the site was a cropland cultivated with corn spring wheat spring canola and soybean in rotation on a silty clay loam soil 31 clay 49 silt and 20 sand with 5 2 organic matter in the top 15 cm the australian site kingaroy was a cropland cultivated with corn winter wheat rotation the soil has 53 clay 15 silt and 32 sand resulting in a clay texture with 2 2 organic matter in the top 10 cm de antoni migliorati et al 2014 data for the sites in australia canada and europe were made available through the research projects macsur and cn mip field data in the united states were from 17 historical scan sites table 1 characterized by perennial grasses as land cover 1 1 details on soil properties and attributes by layer can be found from the scan site soils pedon report e g for walnut gulch 1 site id 2026 available at http www wcc nrcs usda gov nwcc site sitenum 2026 accessed 18 november 2020 replace the id number to access other scan sites for all the study sites data on precipitation daily maximum and minimum air temperatures soil temperature at different depths i e 4 8 20 and 40 inches 10 20 50 and 100 mm for the scan sites were available because the scan sites generally have significant missing daily weather data the nearest weather stations were acquired for a 21 yr simulation 1990 2010 considering all the testing sites average annual precipitation ranged from 289 nunn 1 id 2017 in co usa to 1404 mm geneva 1 id 2011 in ny usa table 1 average monthly maximum temperature ranged from 18 3 c monte bondone italy to 35 c little washita 1 id 2023 in ok usa and average monthly minimum temperature ranged from 21 c glacial ridge 1 id 2050 in mn usa to 3 9 c prairie view 1 id 2016 in tx usa table 1 input files for the epic model were prepared using the epic version of the apexeditor leyton 2019 2 3 model evaluation to compare the results of the methods and evaluate model accuracy four statistical indices were calculated the akaike information criterion aic akaike 1974 coefficient of determination r2 nash sutcliffe efficiency nse nash and sutcliffe 1970 and percent bias pbias aic is a commonly used criteria in model selection asl rousta et al 2017 perez sanchez et al 2019 symmonds and moussalli 2011 and is based on information theory as a criterion it seeks a model that has a good fit to the observations but few parameters aic can be calculated as in eq 25 by using residual sum of squares rss 25 a i c n l n r s s n 2 k where n is the sample size k is the number of parameters the root mean squared error of the model output rmse was used in this study to replace the term rss n in eq 25 26 r m s e i 1 n t s i m i t o b s i 2 n where t obs i and t sim i are the observed and simulated soil temperature on day i respectively aic criteria classify the best model when the value achieved is lower summary statistics and goodness of fit measures were selected for the model evaluation based on suggestions given by loague and green 1991 wallach 2006 and bellocchi et al 2010 the r2 was used to evaluate how accurately the model tracks the variation of observed values it ranges from 0 to 1 best with higher values indicating less error variance the nse was used to compare the model error against the variability around the mean of the observed data it ranges from 1 best to minus infinite worse with negative values meaning that the simple average of the observed data is a better predictor than the model tested pbias was used to assess under positive value or over estimation negative value and the magnitude of error it ranges from positive infinite to negative infinite with the best result obtained at zero 27 r 2 i 1 n t o b s i t o b s t s i m i t s i m 2 i 1 n t o b s i t o b s 2 i 1 n t s i m i t s i m 2 28 n s e 1 i 1 n t o b s i t s i m i 2 i 1 n t o b s i t o b s 2 29 p b i a s i 1 n t o b s i t s i m i 100 i 1 n t o b s i these model performance statistics were calculated using r r core team 2019 and the hydrogof package zambrano bigiarini 2020 model performance was considered good with r2 0 75 nse 0 65 and pbias within 20 considering the guideline proposed by moriasi for model evaluation moriasi et al 2007 as introduced above three parameters were added to this new development p87 eq 3 and p95 eq 4 used in both soil temperature submodels for adjusting soil cover factor while calculating soil surface temperature and p85 eq 10 used in ecosine to regulate the effect of actual daily weather on soil surface temperature table 2 the default values of the parameters were defined to give realistic results on a broader testing dataset and based on developers experience while developing the methods no site specific calibration was conducted for the sites where the default value of the three parameters directly related to soil temperature estimation gave good model performance grade otherwise the time series of each site specific dataset was split into two segments of equal length or with the validation set one year longer that the calibration one in the case of time series with an odd number of years in all the cases the first part of each time series was used for calibration and the second part for validation calibration of the three relevant parameters was performed manually until achieving good model performance grade 3 results and discussion 3 1 original epic approach compared to ecosine and pht methods the two new soil temperature methods were compared with the original epic approach and evaluated by comparing simulated and measured daily values of soil temperatures at different soil depths using the default parameter values listed in table 2 to test the robustness of the methods although the aic penalizes the number of model parameters it provided no evidence that the original method was better than the two methods developed in this study the original method has no parameter involved yet its aic achieved bigger values in all the 24 testing sites across 7 countries table 3 except for the top two soil layers in grignon indicating better results when using the new ecosine and pht methods no indication was observed that one of the new methods was consistently better than the other across the study sites however the ecosine method performed slightly better for the deeper depth and warmer environments while the pht provided better performance in simulating soil temperature of upper soil layers depth 20 cm and in colder sites this is probably because near surface and the shallow depth soil temperature is most affected by atmospheric variations in warmer environment the day to day temperature fluctuation can be significant and the pht approach can better simulate the transfer of heat form the soil surface to the upper soil layers also when snow is present the relationship between air and soil temperatures is different from that without snow due to insulation provided by the snow for the cold condition the pht performed better because the snow cover factor together with soil bulk density and water content are used for calculating surface transfer coefficient in addition to the aic we calculated r2 nse and pbias for the 24 sites using default values of the 3 parameters table 2 while comparing with the original method based on the comparison of observed and simulated daily soil temperatures at different soil depths the original method produced good modeling results for 11 out of 24 study sites table 4 with r2 ranging from 0 75 to 0 98 nse ranging from 0 65 to 0 96 and pbias from 18 5 to 15 6 however 18 out of the 24 study sites had good model performance at all the measurement depths using the ecosine and 20 out of the 24 sites had good model performance using the pth methods with r2 ranging from 0 80 to 0 98 nse ranging from 0 66 to 0 98 and pbias within 14 7 and 18 7 table 4 the results demonstrated the robustness of the two new methods which were able to provide good model performance in simulating the soil temperature under various soils and climates conditions it was encouraging to obtain good performance grade by using default parameter values related to the two new submodels described here additionally these model parameters provide a convenient way to calibrate the model to improve the modeling results for example there were 4 out of the 24 study sites where the model performance statistics were under the good grade as set in the model evaluation section using all three soil temperature methods as indicated by the lowest nse values of 0 50 0 52 and 0 53 and largest pbias values of 34 1 29 4 and 27 3 while using the original ecosine and pht methods respectively table 4 and there were two additional sites mt 2019 and nm 2107 that had improved performance while using the ecosine method compared to the original method but still were under the good grade of model performance we chose to calibrate the six scan sites for the ecosine and pht methods see next section yet there was no modular parameter in the original soil temperature method to facilitate the calibration process figs 2 and 3 clearly show the significant under estimation obtained with the original approach during winter periods at the grillenburg site for instance in january 2006 the original epic approach simulated approximately 14 c at 10 cm soil depth while the corresponding observed soil temperature was about 0 c fig 2 similar results were observed also at the italian and swiss sites where the simulated soil temperature was significantly lower than the observed soil temperature during the coldest months see fig 1 s to 4 s included in the supplemental material this was caused by the simplified approach in the original cosine function method which is based on the drivers of daily air temperature and average annual soil temperature at damping depth also noted by other researchers for example roloff et al 1998 showed pronounced underestimated soil temperature during cold winters in central eastern canada the ecosine and pht submodel improved the capability of epic to simulate the soil temperature by eliminating the underestimation in the winter period estimated with the original approach figs 2 and 3 moreover both new methods produced a smother variation of the daily soil temperature resulting in a better representation of the observed soil temperature dynamics as reflected by better model performance statistics the accurate prediction of soil surface temperature is critical for deriving realistic soil temperature profile prediction among the study sites only the french site of grignon reported soil surface temperature observations both the ecosine and pht submodels performed well in simulating the soil surface temperature as indicated by r2 and nse 0 80 and pbias within 5 table 4 both methods over estimated a peak in the soil surface temperature in the middle of may 2008 and at the beginning of october 2011 figs 4 and 5 the high and unusual solar radiation on bare soil affected model prediction and differences were observed for short periods for the temperatures simulated at the soil surface by the two methods saito and simunek 2009 and mahfouf and noilhan 1991 also showed that simulated soil surface temperatures could easily be altered by 50 or more because of inaccurate prediction of the dynamics of the surface energy components while these variations and overestimations were no longer present at 30 cm depth by using the ecosine the overestimations propagated to deeper soil layers and their impact was noticeable at 30 and 50 cm depths using the pht submodel this is because the heat transfer method pht is sensitive to soil surface temperature and temperature transfer which is affected by soil bulk density and water content chalhoub et al 2017 also noted that a proper account of the soil surface cover and site specific soil properties is necessary to obtain accurate soil temperature predictions with uncertainties associated with model inputs e g soil properties model structure e g simplifying assumptions in mathematical terms to represent complex soil plant atmosphere systems and soil temperature measurement it is expected that the soil temperature predictions cannot fully replicate observations with the two new soil temperature submodel options users could select the most suitable option with regards to data availability and agricultural settings for the grignon site the ecosine approach might be preferable to minimize the impact of nonoptimal estimation of the soil surface temperature 3 2 further calibration and validation for six scan sites in the previous study of the scan sites where the original epic soil temperature approach was used significant under and over predictions were obtained pbias ranged from 18 2 to 34 1 the r2 values ranged from 0 73 to 0 98 and the nse values ranged from 0 50 to 0 96 wang et al 2015 because no relevant model parameter in the original epic soil temperature submodel for soil temperature calibration was available we could not improve its performance while it was possible for this study to meet good model performance grade with r2 0 75 nse 0 65 and pbias within 20 the two new soil temperature methods proposed in this study provides the possibility to calibrate the model by adjusting the three new parameters related to soil surface temperature prediction to improve model performance for the 6 scan sites table 5 as shown in table 4 the original method produced the lowest nse value of 0 50 and the largest pbias value of 34 1 among the 6 study sites the ecosine and pht methods had better modeling performance with nse 0 52 and pbias values ranging from 14 7 to 27 3 while using default parameter values table 2 for the calibration and validation processes observation data at the 6 sites were split in two p85 used in the ecosine method for regulating the effect of daily weather on soil surface temperature was adjusted for all the 6 sites p87 and p95 used to adjust daily snow and plant cover factors could be adjusted in both new methods after calibration the simulated daily soil temperatures were in close agreement with observed values at all tested depths with r2 ranging from 0 85 to 0 98 nse from 0 66 to 0 94 and pbias within 18 table 6 indicating good model performance this demonstrates that after refining the parameters related to soil surface temperature estimation both methods could be used with good accuracy at different soil depths the aic criteria indicate that the pht submodel was able to provide better results compare to the ecosine approach when the model was site calibrated table 6 as an example fig 6 illustrates the time series comparison for daily soil temperature at the 50 cm soil depth for the lind 1 wa 2021 site it shows a generally good agreement between the simulated and observed values there were both under and over estimations larger deviations were observed during some winter and summer months in some cases the performance of the model could be distorted by observation error and uncertainty like what we observed on november 1 1996 on that date a sudden drop in soil temperature at 50 cm was recorded with the three consecutive daily observed soil temperatures equal to 8 8 c 0 3 c and 9 5 c such abrupt change of the soil temperature at 50 cm depth is very unlikely in reality and the corresponding simulated soil temperatures were 5 9 c 5 6 c and 5 3 c by the ecosine method and 9 9 c 9 6 c and 9 4 c by the pht on top of possible errors in the observations the use of weather data from nearby weather stations may have contributed to the deviation between simulated and observed values 3 3 impact of soil temperature submodels on plant emergence and crop yield predictions of emergence dates at the study sites in australia canada and europe were compared with available observations in all cases the emergence dates simulated with the three soil temperature approaches were very close to observations differences in simulated germination dates were found in a few years at the australian site the observed winter wheat germinated on the 20th of july in 2011 but epic simulated emergence earlier with all three soil temperature submodels the original epic approach estimated 10 days earlier than the observed emergence date and the ecosine and pht models simulated 6 days earlier these results were in line with those reported by de antoni migliorati et al 2015 who simulated the same dataset with the daycent model in grignon emergence of maize took place on 14th may of 2008 while the simulated emergence dates were may 15 2008 with the ecosine and pht models and may 17 2008 with the original approach in canada spring wheat emerged on may 25 2007 the new models simulated the emergence two days earlier while the original approach in epic simulated one day earlier variations in emergence prediction were caused by soil temperature prediction difference in the three soil temperature approaches along with the difference observed in the emergence date small differences were observed for crop yield simulated with the three soil temperature approaches when considering all the data available for the sites in europe canada and australia the epic model performed well with any of the three soil temperature approaches figs 7 and 5 s a slightly higher pbias was obtained for the crop yield estimated with the original epic soil temperature approach for two years 2007 08 at laqueuille france a grassland site with grazing management epic underestimated yield using any soil temperature approach 4 conclusions although the epic model has been widely applied throughout the u s and worldwide continuous evolution and improvement of major process such as soil temperature is essential for broad and critical applications of this comprehensive agro ecosystem model by considering the soil snow and the plant material cover factors and using a physically based heat transfer algorithm the development of two new soil temperature modules and their evaluation at 24 sites showed increased ability of epic to better simulate soil temperature under varying environmental conditions compared to the original approach the new methods provided more accurate and realistic simulations of the soil temperature for broad agroecosystems conditions this study did not offer a fully conclusive answer as for which of the two newly developed methods was a better choice overall however it outlined the robustness of the ecosine method which produced better results for most of the tested sites using default parameter values on the other hand the pht provided better performance in simulating soil temperature of top soil layers depth 20 cm and when the soil was covered by snow thus the pht method would be better suited for sites having snow cover in winter while the ecosine approach could be successfully used with default parameter values in most cases this aspect is especially important for sites that do not have measured soil temperature available for model calibration by using the improved epic ecosine module the soil temperature could be well predicted at different depths reducing the prediction uncertainty in other soil temperature driven biological physical and chemical processes of terrestrial ecosystems since both ecosine and pht modules were incorporated in the epic model users have now the choice to select a relevant soil temperature module for their modeling applications with these promising results the two new soil temperature modules have been incorporated into the agricultural policy environmental extender apex model williams and izaurralde 2006 to replace the original soil temperature method this improvement is believed to describe the many interactions of agricultural production systems most accurately however further evaluation is needed about the effects of the new methods on other variables simulated by the epic and apex models such as mineralization nitrification volatilization soil water content crop yield and runoff opportunities exist to test them in different climatic and regional conditions in our conservation effects assessment project ceap cropland national assessment program duriancik et al 2008 author contributions l d x w m l n and j r w developed and tested the soil temperature prediction submodels performed the statistical evaluation of the results and developed the manuscript c a m dea m t g k k b l e p and g w provided the observed data for the experimental sites in canada europe and australia and contributed editing and improving the manuscript declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment this work was supported by the cn mip and macsur projects of the joint programming initiative facce jpi https www faccejpi com the funding by eu projects carboeurope ip nitroeurope ip and icos by german bmbf project icos d by the state of saxony tu dresden lfulg and by agriculture and agri food canada is greatly appreciated this study received partial funding by the province of südtirol alto adige within the frame of the project cycling of carbon and water in mountain ecosystems under changing climate and land use cyclamen k klumpp acknowledges the support of the anaee france anr 11 inbs 0001 in natura soere acbb permanent grassland site laqueuille research was also conducted with support from the usda nrcs conservation effects assessment project ceap appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105140 
25756,parallel computing is a primary way to increase computing efficiency of grid based distributed hydrological models this study proposed an automatic partition based parallel algorithm appa to approach the theoretical maximum speedup ratio tmsr through a combination of flexible partition for the domain decomposition and the load balance of parallel simulation appa optimizes the parallelization of hillslope and channel flow routing processes at sub basin and channel unit level respectively to illustrate appa s performance we embedded it in a distributed ecohydrological model and then applied the updated model to three watersheds at different spatial scales the results indicate that appa effectively promoted parallel performance the estimated speedup ratio approached 93 97 of the tmsr for simulating hillslope processes and 91 98 of the tmsr for simulating channel processes using 26 threads in all three watersheds these improvements justify that appa is effective in accelerating model simulation and thus benefits future model based research keywords parallelization grid based distributed models automatic partition speedup ratio computing efficiency watershed modeling software availability software appa automatic partition based parallel algorithm language c availability appa is open sourced on github https github com xuzhenwu appa 1 introduction using distributed models to simulate coupled hydro ecological processes at large spatial and long term temporal scales can be cumbersome and time consuming with the rapid development of computer science the parallelization of model simulation becomes an effective strategy for speeding up the computing efficiency compared to the serial computing one of the most important approaches for parallel computing is to partition a heavy computation task into several independent loads so that the sub tasks can be allocated to multiple processors simultaneously for watershed modeling parallel simulation often divides the study area into several independent sub basins thus each of sub basins is treated as a single task and processed by an individual thread under the assumption that there are no or relatively low communications between sub basins vivoni et al 2011 however the task partition based parallel computing faces difficulties in finding the best partition solution largely due to the spatial connectedness of coupled hydro ecological process e g water and solutes routing in distributed models in spite of aforementioned disadvantages the parallel algorithm by partitioning a watershed into several sub basins has proved to be effective in speeding up model simulations tian et al 2008 wang et al 2011 qin and zhan 2012 the partitioned based parallel algorithm usually involves spatial domain decomposition computational task allocation and inter process communication zhang and zhou 2019 the spatial domain decomposition is to partition a large domain into smaller individual sub basins that are arranged from upstream to downstream apostolopoulos and georgakakos 1997 arnold et al 1998 li et al 2011 in addition to decomposition at sub basin level liu et al 2014 found that the distributed models can be parallelized at basic simulation unit levels through a layered approach consequently the computational tasks of these parallelized units are allocated for parallel computing however the dependency resulting from flow routing still coexists with concurrency this can be either resolved by simulation from upstream to downstream units at thread level with shared memory programing standard hwang et al 2014 liu et al 2014 or by simulation of units at process level with message passing programming standard tian et al 2008 li et al 2011 vivoni et al 2011 wang et al 2011 in practice the former may be constrained due to the consideration of the task arrangement while the latter may suffer from poor efficiency caused by a heavy load of inter process communications li et al 2011 vivoni et al 2011 even so a joint of them can break through the speedup ratio limit compared with using either of them liu et al 2016 zhu et al 2019 nevertheless the exploration of partition based parallel algorithms has greatly enhanced the computing speed of distributed models apart from the effectiveness i e speedup ratio the existing parallel algorithms need improving since the computational efficiency i e parallel efficiency is still restricted to the unit partition at a task parallelism level zhang and zhou 2019 to quantify the efficiency and guide the parallel algorithm development the theoretical maximum speedup ratio tmsr has been proposed as a target to achieve the potential of parallel computing under a given number of processors wang et al 2012 to approach tmsr in the flow routing systems of hydrological models the critical path heuristic algorithm has proven to be effective in scheduling static tasks of partitioned sub basins shirazi et al 1990 liu et al 2013 through analyzing basin width function wang et al 2012 insisted that there is a maximum speedup curve for an arbitrary drainage network in a preliminary application they found that the more sub basins are partitioned the higher the tmsr is in addition the compact watersheds generally have higher tmsr than long and narrow watersheds liu et al 2013 developed a method to estimate tmsr for parallel computing of hillslope and channel processes using grid based distributed hydrological models efforts are continuously made to maximize the parallel efficiency some researchers noticed that the parallel performance is improved with the increase of the number of partitioned sub basins as a result of better load balance wang et al 2012 liu et al 2016 besides the load balance is proved to significantly improve the parallel speed up with proportionally faster runs as simulation complexity i e domain resolution and channel network extent increases vivoni et al 2011 these explorations help researchers recognize the potential of parallel computing under a given parallel algorithm application of distributed models at large spatial scales driven by high spatial resolution data can involve a huge number of simulated grids compared with models built at coarser units e g sub basins hillslope grid level simulation usually consists of more simulation units grids and thus has a more extended and complex flow network hillslope flow routings and channel flow routings the increasing complexity of simulation not only increases computing burden but also may bring greater potential of parallelization although channel reaches are usually set as the basis for sub basin unit decomposition in prior partition based algorithms arnold et al 1998 li et al 2011 vivoni et al 2011 the grid based models have no such restricts and each grid can be set as the potential outlet for extracting a sub basin as partitioned units are further allocated to different processors the computation efficiency is sensitive to the spatial domain decomposition li et al 2011 zhang and zhou 2019 however previous studies usually consider the spatial domain partition as a preliminary process before allocating computation tasks for parallelization it is worth of investigating how the parallel efficiency will respond to the flexibility of partition for grid based distributed models to maximize the parallel performance we argue that an optimal partition based parallel algorithm should have following features a the utilization of the load balance strategy throughout the preparation of a parallel scheme b the search of potential parallel schemes based on flexible spatial domain partition for the optimal efficiency c an estimate of real performance compared with the theoretical maximum performance and d the applicability to most grid based distributed models to meet these criteria this study proposed an automatic partition based parallel algorithm appa that aims to parallelize grid based distributed models such that the theoretical maximum speedup ratio tmsr is approachable by combining global search for potential partition schemes with the load balance strategy in parallel simulation appa optimizes parallelization of hillslope and channel flow routing processes at both sub basin and channel unit levels respectively to illustrate appa s performance we embedded it into the coupled hydro ecological simulation system chess tang et al 2014 2019 and applied the updated chess to three watersheds using various threads the subsequent computing performances of parallel simulations were evaluated using the tmsr estimated speedup ratio and real speedup ratio overall this study provides a new parallel algorithm that can maximize the computing efficiency of parallel simulation thus contributing to future model based research 2 developing the automatic partition based parallel algorithm 2 1 parallel assumptions of grid based hydrological models the automatic partition based parallel algorithm appa assumes that a typical grid based distributed model has following characteristics 1 grids are basic units for simulating hydrological processes e g infiltration evaporation runoff 2 the overall hydrological processes can be classified as hillslope and channel processes in terms of flow equations e g hillslope surface and sub surface flow and channel flow 3 flow routing among grids is generated by a single flow routing algorithm e g d8 algorithm such that runoff yield in a grid is routed to only a downslope neighboring grid in addition directions of surface and sub surface flow are uniform for each hillslope grid 4 given the continuity and nature of downslope water movement grids located in upslope or upstream areas take precedence during simulation in practice this is realized by arranging all grids in descending order according to their elevations 5 the runtime for a grid simulation is assumed to be equal or is determined by its own type 2 2 general parallelization scheme for the automatic partition based algorithm the proposed appa applies a two level partition scheme for parallel computing so that the computing performance of grid based distributed models can be enhanced first grids are divided into hillslope grids and channel grids corresponding to hillslope processes including surface and sub surface flow and channel processes only containing channel flow second the two processes are further partitioned as sub basin units and channel units that are allocated to different threads for parallel computing by doing so the original serial simulation is divided into several rounds of parallel simulations within each round of parallel simulation the partitioned units are assumed to be independent and allocated to different threads that do not communicate with each other fig 1 depicts the two level parallel scheme and the corresponding parallel algorithm developed for a four core computer daily simulation of overall processes is divided into simulation of hillslope and channel processes for hillslope processes the computation tasks of five partitioned sub basins are eventually allocated to four threads fig 1a for channel processes the parallel computing consists of five sequential layers the first layer contains seven independent units which are further allocated to four threads fig 1b similarly three middle layers are sequentially parallelized the last layer of channel processes is computed by one thread as it has no branches for partitioning based on unit partitions and thread allocations the proposed algorithm adopts a fork join structure to achieve parallel computing and considers the load balance among processors to improve parallel performance fig 1c in addition no communication costs exist among threads in each round of parallel computing e g hillslope processes channel processes in first layer within each thread grids are simulated in descending order of elevation to keep flow movement daily simulation of hydrological processes terminates when the outlet grid is complete 2 3 principles of optimization for automatic partition based parallelization to approach the optimal state of partition based parallelization appa adopts the following principles 1 searching the potential parallel scheme repeatedly to select the optimal scheme as the selected model is grid based the size of partitioned units can vary due to many choices of the potential outlet grids each channel grid has the potential to be the reference point to trace all upstream grids that can be grouped into a sub basin or channel unit given this the accumulated runtimes for grid i t g i figs 2b and 5 b are used to estimate the required time for simulating all grids within a sub basin or channel unit when the runtimes for each grid are specified the total t g accumulates as the flow routes downslope to the outlet in theory if the runtimes for simulating each grid are identical the number of grids is just a proxy of the accumulated runtimes t g due to the flexibility of selecting outlets for sub basin or channel units it is likely to obtain the best parallel scheme by comparing computing performances under different domain partitions 2 utilizing the load balance strategy to composite the partitioned units as the final parallel scheme for example the partitioned sub basin units are assigned to groups with possible balanced runtime so that the final runtime for hillslope processes is minimized fig 1a 3 measuring the parallel performance to approach theoretical performance as the goal of partitioning is to fully utilize the computation power of computers with n available processors this study used three kinds of speedup ratios sr to evaluate the parallel performance the theoretical maximum speedup ratio tmsr the estimated speedup ratio esr and the real speedup ratio rsr sr is defined as the ratio of the time cost of serial computing t s to that of the parallel computing with n processors t n hwang et al 2014 1 s r t s t n the overall processes are divided into hillslope processes hs and channel processes ch and there are three types of sr for each of processes tmsr indicates the full utilization of all allocated threads as there are no communications for hillslope processes among sub basins the tmsr of hillslope processes equals the number of threads n when the load balance among threads is fully achieved tmsr h s n for channel processes tmsr c h is determined by the longest exit length which represents the minimum possible execution time shirazi et al 1990 liu et al 2013 it is computed as the ratio of the serial runtime spent for the longest channel exit length t l s to the required runtime for serial computing of channel grids t c h 2 tmsr c h t c h t l s then the overall tmsr is computed as follows 3 tmsr 1 p h s tmsr h s p c h tmsr c h where p h s and p c h refer to the runtime proportion for hillslope and channel processes respectively relative to the overall simulation values p h s p c h 1 the estimated speedup ratio esr is the estimate of speedup ratio for a given parallel scheme based on the load balance in the fork join structure fig 1c to compute esr t s is defined as the total runtimes of all partitioned grids and t n is defined as the thread that is allocated with the maximum runtime of partitioned grids eq 1 rsr is computed by the testing results of real runtimes of serial and parallel computing in comparison rsr is generally the smallest value since it accounts more costs tmsr esr rsr parallel efficiency ep is used to quantify the efficiency in parallel computing and defined as the speedup ratio to the number of available processors 4 e p s r n 2 4 parallel simulation of hillslope processes the simulation of hillslope processes is most time consuming as it involves the most of grids to be simulated in practice parallel computing of hillslope processes involves sub basin partitions and thread allocation as the partitioned sub basin units are independent the parallel computing of hillslope processes should be finished in one round of parallel computing with optimal thread allocation in addition an extra scheme selection process is provided for obtaining optimal parallel performance the three steps are described in the following paragraphs the sub basin partition starts with the definition of partition goals first we define the total computation time needed for hillslope processes as the required time for simulating all hillslope grids t h s thus the theoretical minimum runtime for all n threads t h s n is expressed as 5 t h s n t h s n specially the maximum computation time demanded for partitioned sub basins s m is determined by t h s n 6 s m t h s n d 1 θ s where the deviation index θ s and decomposition index d indicate the variation of s m θ s represents minor variations of s m which will not significantly increase the number of final partitioned sub basins m and d represents significant changes in s m for example m approximates twice of the value of n when d is 2 in practice a potential outlet grid with satisfactory accumulated runtime t g s will be searched from all channel grids this potential channel grid assures that the subsequently partitioned sub basins are independent in terms of hillslope processes and it will retain a closest value to but lower than s m 7 t g s t g i w h e n 0 s m t g i s m t g s t g s o t h e r s where t g s is initialized as zero before the searching process the value of t g s may be updated as the algorithm searches through all channel grids when a grid is captured t g s 0 it will be set as the reference outlet to trace the corresponding sub basin fig 2c once a sub basin is obtained the accumulated runtime for all grids t g will be recomputed and then the program starts to search for next sub basin until all hillslope grids are grouped into the partitioned sub basins fig 2d the thread allocation process allocates the computation tasks of independent sub basins to available threads in one loop of parallel computing fig 1c to optimize the allocation for obtaining the best load balance appa tries to reduce the maximum runtime among all threads first the thread with maximum runtime and the thread with minimum runtime are chosen respectively then the smallest sub basin in the thread with maximum runtime is moved to the minimum thread if it successfully reduces the maximum runtime among all threads that limits the parallel performance the redistribution process continues otherwise the optimal state of allocation is achieved for example the first to fifth sub basins in fig 1a are assigned to four threads in order i e sub basin 5 is assigned to thread 1 as thread 1 is the thread with the maximum runtime the smallest sub basin in thread 1 sub basin 5 is reassigned to thread 3 with the minimum runtime it retained as the optimal result of allocation where sub basin 5 and sub basin 3 are processed by the same thread fig 1c after all hillslope grids of sub basins are allocated to specific threads the applicable parallel scheme for hillslope processes is completed the strategy for the outer selection process is to choose the optimal scheme in terms of esr fig 2a since the maximum runtime of sub basins s m is subject to changes in the deviation index θ s and decomposition index d the proposed algorithm examines all potential parallel schemes empirically θ s has a value ranging from 0 to 0 4 and iterates by a step of 0 05 the decomposition index d is designed to increase the number of sub basins several times as d iterates from 1 to a larger value it will reveal how the variation of domain decomposition affects parallel performance then the best parallel scheme is selected from all schemes based on the esrs 2 5 parallel simulation of channel processes channel routing processes significantly affect the computing performance of model simulation although channel units are inter connected units with binary structures they can still be considered independent when all channel units are treated in a same layer liu et al 2014 as a result the upstream downstream computation dependence only imposes restrictions on the outer and inner layers where the channel units in outer layers take precedence over those in inner layers during simulation thus the aforementioned partition and allocation processes are applicable in parallelizing channel processes into several layers however the more layers there are the more rounds of parallel computing are needed because appa adopts flexible partition strategy for grid based models the partitioned channel units do not necessarily follow the locations of stream reaches in general similar to hillslope processes the parallel computing of channel processes involves channel unit partition and thread allocation in addition an extra scheme selection process is provided for obtaining the optimal parallel performance in simulating channel processes the channel unit partition and thread allocation are synchronous processes as with simulating hillslope processes we define a maximum runtime of threads for channel processes t c h m in fig 3 and thus the strategy of partition is to fill up t c h m which is assumed to be the same in all threads for load balance specially we defined the available runtime of thread i t a i to limit the size of rest of the runtimes of thread i that can be allocated for a potential channel unit t g s like setting the priority for larger sub basin units in hillslope parallel computing the thread with the largest available runtime t a m should be filled up first with a new channel unit step 1 in fig 3 8 t a m max t a 1 t a n as with the available runtime of a sub basin s m eq 6 t a m limits the size of accumulated runtimes of the partitioned channel unit t g i for channel processes the final outlet should meet the requirement that t g s has a slightly lower value than t a m step 2 in fig 3 9 t g s t g i w h e n 0 t a m t g i t a m t g s t g s o t h e r s where t g s is initialized as zero before the searching process the value of t g s may be updated as the algorithm searches through all unpartitioned channel grids if t g s is greater than zero the corresponding grid is set as the reference outlet to trace the channel unit fig 5c which is further allocated to t a m step 3 in fig 3 to eliminate the influence of this unit all the other channel grids located downstream will be excluded from the subsequent searching of t g s to avoid upstream downstream connection in this layer the accumulated runtimes of other grids t g i will be re computed specifically the available runtime of the maximum thread will be updated using the following equation 10 t a m t a m t g s if t g s remains zero the computing for this layer is finished and the unit partitioning and the thread allocation process is terminated after several rounds of parallel simulation the channel processes are finished and the maximum runtime for each thread t c h m in the layer is determined then the runtime of all threads t o v in each layer is defined as 11 t o v t o i where t o i is the total runtime of thread i and t c h m t o i t a i to search all the possibilities of parallel schemes in the layer t c h m is set as the smallest value in the first iteration i e the runtime for a channel grid t c h m t c g fig 4 b and kept updating as θ g increases 12 t c h m max t c h m 1 θ g t c h m t c g where max is the maximum function that returns the greatest value once a new t c h m is obtained the program records the estimated runtime of all threads t o v and the estimated speed up ratio e s r c h this process will be terminated once t o v has covered all channel grids with undefined layers and threads fig 4d e s r c h is the serial time t o v divided by the maximum runtime in a thread in parallel computing 13 e s r c h t o v max t o i fig 4a shows how e s r c h and t o v vary as the key variable t c h m increases using a four core computer as t c h m increases from 1 to 113 e s r c h remains having the maximum value of 4 and t o v keeps increasing fig 4a however the applicability of parallelization drops when t c h m exceeds 113 and e s r c h drops when t c h m exceeds the maximum accumulated runtime among all channel grids they will be allocated to the same thread and e s r c h is 1 fig 4d as the speedup ratio is the top priority of parallelization the schemes with the largest e s r c h are picked for next round comparisons t c h m ranges from 1 to 113 fig 4a then the scheme with the highest t o v is selected fig 4c this selection process can avoid involving too many layers and thus reduce time spent in allocating threads similar steps in scheme generation processes are performed until all channel grids are allocated to a specific layer and thread fig 5a as the partition processes move to the outlet the applicability of parallelization decreases and the last layer with a channel unit is processed using one thread the black bold line in fig 1b empirically the only tunable parameter θ g is set as 0 05 according to the characteristics of layers several rounds of parallel computing can accelerate the simulation of channel processes model simulation for a given time scale e g day will terminate once the simulation for the outlet of the watershed was complete fig 1c 3 application and testing of appa a case study the coupled hydro ecological simulation system chess tang et al 2014 2019 a grid based distributed model was applied to test the proposed appa chess simulates integrated water carbon nutrient dynamics as well as vegetation growth in terrestrial ecosystems at watershed and regional scales in chess the routing of water and solutes among grids is explicitly simulated when the model is applied at high spatial resolution and large spatial scales the implementation of parallel computing becomes crucial for improving the computation efficiency in this study the parallelization of chess was implemented through the open multi processing openmp library by c c compilation fig a1 first chess reads the thread information and lists all grids of each thread in a descending order of grid elevations in each loop of parallel computing the threads compute corresponding grids whose addresses are stored in a specific array to perform the fork join structure the main thread can continue until all parallelized threads are completed model simulation for a given time scale e g day will terminate once the parallel simulation of all grids is completed three watersheds located in southern china were selected for evaluating the performance of proposed parallel algorithms fig 6 they differ in total number of grids and areal coverage table 1 the two smaller watersheds are actually sub watersheds of the biggest and named corresponding to hydrological stations yuecheng and longchuan they represent ten thousand grid and hundred thousand grid level simulations respectively in contrast the largest watershed dongjiang consists of almost a million of grids in 200 m resolution and represents another level of simulation one super computer which is equipped with two way 14 core intel xeon e5 2683 2 0 ghz cpus and supports the parallel computing by a maximum of 28 processors threads was used to analyze parallel efficiency of the proposed algorithm the parallel simulations are performed using different numbers of processors and the resultant parallel performances based on different numbers of threads that vary from 2 to 32 by an interval of 2 are compared with the original serial simulation to analyze the computing performance of the proposed algorithm 4 results 4 1 optimizing the parallel computing of hillslope processes since sub basins are basic units in the parallel computing of hillslope processes the partition of sub basins significantly affects the performance of parallel computing to find the best parallel scheme appa uses the deviation index θ s and the decomposition index d to adjust the size of estimated runtime for partitioned sub basins s m fig 7 shows the variation of estimated speedup ratio esr and the relative increments of esr under different parallel schemes as the number of involved threads increases in the reference parallel scheme series θ s 0 d 1 esr increases linearly as more threads are involved black line fig 7a when change the size of s m θ s varies from 0 to 0 4 by a step length of 0 05 and the optimal result is selected from 9 parallel scheme series blue lines based on optimal results the maximum esr green line increases by about 20 compared with the reference series when the number of threads ranges from 8 to 32 which approximates half the space between the tmsr red line and the reference esr black line fig 7b the number of partitioned sub basins also poses a great effect on the speedup ratio of parallel computing for comparison the reference parallel scheme series black line in fig 7c are set as optimal results generated by searching different θ s under the condition that the decomposition index d equals 1 and the number of sub basins approximates the number of threads green line in fig 7a to change the number of partitioned sub basins we have d vary from 1 to 8 by a step of 1 and the optimal results are selected from 8 parallel scheme series green lines fig 7c fig 7b illustrates how d promotes the maximum esr by searching potential schemes under different d fig 7d indicates that the increasing partition of sub basins can increase esr when the number of threads exceeds 8 however it does not necessarily increase esr when the number of threads ranges from 2 to 6 in general the maximum esr increases by 4 17 compared with the reference esr by searching different parallel schemes with different θ s and d appa achieves the potential of parallel computing while the maximum esr is raised up to 97 99 of tmsr in dongjiang watershed fig 7c 4 2 optimizing the parallel computing of channel processes appa aims to increase the speedup ratio and reduce rounds of parallel computing in the implementation of the layered approach for simulating channel processes in the case of dongjiang watershed the parallel simulation based on the proposed algorithm generates 18 layers for parallel simulation in contrast the original layered approach generates more than 2898 layers for the channel processes grids in the critical exit length if it simply allocates a grid to a single thread liu et al 2014 although esr is the top priority in each layer of the proposed algorithm the potential of parallelization decreases as more channel units are partitioned fig 8 a as a result the esr for inner layers drops significantly for the last channel unit that has no branches it cannot be parallelized and esr is 1 for all three watersheds in general esr increases as the number of involved threads increases fig 8b however the increase in esr has a ceiling when esr approaches tmsr because the proposed algorithm explores all the possible partition based schemes and considers the load balance of parallelization the final esr of channel processes is comparable with tmsr the results based on three watersheds show that the maximum esr can reach 91 98 of tmsr using 26 threads table 1 4 3 evaluating real parallel performance fig 9 illustrates the variations in execution time speedup ratio parallel efficiency and the efficiency deficit as the number of threads changes for the dongjiang watershed the execution time for hillslope processes drop sharply and the speedup ratio increases as the number of threads increases fig 9a and b the estimated speedup ratio for hillslope processes esr h s reaches 25 1 and the real speedup ratio rsr h s reaches 21 6 86 1 of esr h s under 26 threads for channel processes the real speedup ratio under 26 threads rsr c h 26 is 6 6 approximating 89 2 of the estimated value 7 4 as hillslope grids account for 97 9 of all grids the overall performance is mainly determined by performance of simulating hillslope processes in general the parallel efficiency using more threads is lower fig 9c the estimated parallel efficiency for hillslope processes is relatively stable approximating 97 of the theoretical efficiency after the estimated speed ratio for channel process esr c h peaked at 7 4 the parallel efficiency drops continuously which greatly reduces the parallel efficiency for simulating overall processes the efficiency deficit defined as the difference between the real and estimated parallel efficiencie indicates how the real speedup ratio approximates estimated performance the efficiency deficit is less than 10 for all processes when the number of threads is less than 24 however when the number of threads exceeds 28 the real efficiency decreases dramatically as the super computer used in this study only has 28 cores fig 9d fig 10 shows the overall performance of parallelization applied in three watersheds the esr h s using 26 threads varies from 24 3 to 25 1 for the three watersheds and accounts for 93 97 of tmsr h s the performances among the three watersheds differ mainly in tmsr c h which is quantified by channel width function as a result the parallel simulation of the dongjiang watershed has a larger speedup ratio compared with the two smaller watersheds table 1 meanwhile there is no significant difference in the efficiency deficit among the three watersheds which is less than 10 when the number of threads is less than 24 fig 10d in general appa is efficient in parallel computing and the estimated performance is consistent with real performance 5 discussion grid based distributed mechanistic models are increasingly used for evaluating surface and subsurface hydrological processes parallel computing attempts to tackle the computing challenge for applying distributed models at large spatial and long term temporal scales in general a parallel algorithm is only feasible for models that fit specific parallel assumptions which regulate the implementation of the parallel framework with little or no loss of model s accuracy by setting assumptions as pre conditions a parallel algorithm can better describe ideal state of high performance computation and seek the optimal method to reach the ideal performance in this study the proposed appa algorithm is designed for grid based distributed hydrological models to obtain the optimal computing performance under given hardware condition flow routings which keep water flow along the gravity gradient are the source of challenges in parallelization for hydrological models our proposed appa assumes that models should adopt a single flow routing algorithm by which flow generated in a center grid is routed to only one downslope neighboring grid o callaghan and mark 1984 as a result sub basins are assumed to be independent in terms of hillslope processes otherwise additional approaches are required to meet the demand of message passing among these parallelized units for example vivoni et al 2011 used ghost cells to simulate lateral subsurface flow for ridge grids which are located on shared boundaries between sub basins in fact multiple flow routing algorithms based on topography quinn et al 1991 seibert and mcglynn 2007 or real time water level dai et al 2019 consider flow dispersion at each grid and grids at watershed ridges in which flow can belong to two or more partitioned sub basins nevertheless it is still likely for these models to adopt an independent partition based parallelization without communication among partitioned units if flow generated at ridges of sub basins is only routed in one direction since ridge grids account for a small portion of all simulated grids and model outputs are averaged for the whole watershed differences between parallel and serial computation may be negligible apart from this the use of parallel algorithm can be performed flexibly with better understanding of model structures related to flow routing in this study appa was applied for the whole model simulation as chess has closely coupled runoff production and routing processes at grid level for models that have independent runoff generation and routing processes which are usually simulated in a sequential order appa can also be selectively used for only routing processes liu et al 2014 appa classifies simulated grids as either hillslope grids or channel grids the former indicates where hillslope processes e g surface and sub surface flow routing occur and the later represents where channel processes e g channel flow routing take place this assumption is suitable for most grid based distributed models at regional or large spatial scale e g vic liang et al 1994 rhessys tague and band 2004 for models that have no specification between the two processes users of appa can set a boundary value for accumulated runtime t g bhc to separate hillslope grids from channel grids as suggested by vivoni et al 2011 the best strategy of parallelization for large river basins is to combine a balanced partitioning with an extended channel network as the outlet of sub basins must be a channel grid such that sub basins are independent in terms of hillslope processes the magnitude of t g bhc will limit the size of the smallest sub basin and imposes restriction on uses of more threads used in parallel computing for example when the value of t g bhc is 10 of time for serial computing the estimated speedup ratio for channel processes will approach the limit of 10 in spite of increasing number of threads empirically the value of t g bhc should be within the range of 0 5 5 to tap the potential of hillslope parallelization for grid based models if aforementioned assumptions are meet appa will approach the tmsr by searching for the optimal spatial domain decomposition and computational task allocation to better illustrate the parallel performance appa was tested using a typical grid based model i e chess in experiments the proposed approach is not only efficient as a result of optimization for partition based parallelization but also easy to use since the deployment of parallel simulation with appa requires little knowledge of shared memory programing to achieve the fork join structure fig a1 for parallel performance the estimated speedup ranges from 19 5 to 23 9 using 26 threads for the three watersheds which almost reach the theoretical limit 96 of the tmsr table 1 the corresponding real speedup ranges from 19 4 to 20 6 a comparison between our results and prior work in adoption of high performance using distributed hydrologic models indicate a set of general outcomes in parallel performance vivoni et al 2011 in table 2 we investigated various parallel frameworks and briefly summarized their key findings from case studies the source of differences between those approaches lies in the basic simulation unit in hydrological simulation which influences the complexity of model simulation the larger the simulation unit in terms of the spatial scale e g sub basin hillslope is the less complexity the flow routing network is as a result the enhancement of computing efficiency is limited in spite of increasing parallel resources due to constraints of the binary tree structure of drainage network which generally have fewer simulation units rsr 12 li et al 2011 wang et al 2011 wang et al 2012 in comparison modelling with more simulation units implemented in finer basic units e g grids vs sub basins larger area coverage e g larger watersheds vs smaller watersheds or higher spatial resolutions e g 90 m vs 270 m is proved to have a larger speed up ratio vivoni et al 2011 liu et al 2016 zhang and zhou 2019 so as to better quantify the progress of proposed parallel framework the theoretical maximum speedup ratios are defined for given processers and given model s applications wang et al 2012 liu et al 2013 compared with tmsr the real parallel efficiency is mainly hampered by two main factors the balance of the computation loads and the increase in input output i o costs vivoni et al 2011 zhang and zhou 2019 in this study the combination of spatial domain decomposition and load balanced task allocation is proved effective in reducing the former costs a better load balance is achieved with a different extent of disaggregation fig 7 meanwhile the communication costs are avoided since we adopted the fork join structure of openmp as a result the real performance is close to the estimated with efficiency deficit stably less than 10 when the number of available threads are less than 24 this extra loss of efficiency can be attributed to other factors e g the cost of creating threads and building parallel environment hardware limit data management tools for example the best fit between real and estimated speedup occurs in the smallest yuecheng watershed efficiency deficit 2 across all experiments used different threads indicating that the real performance of appa can be improved with an advanced data management method in parallelization although this study demonstrated the possibility for grid based hydrological models to approach tmsr at thread level the proposed appa can be modified for other types of models for example appa can be modified to facilitate the need of parallelization for models based on other basic units e g tin hru or other routing related computations with huge simulation complexity if the runtime and flow routing of each basic unit are defined appa can be used for spatial decomposition to maximize speedup meanwhile it seems that appa can be deployed at process thread level with mpi programing grepping the power of computer clusters the adoption of multiple parallelization levels e g threads and processes can break through the limit of tmsr at thread level which depends on available cores in a computer rsr 30 vivoni et al 2011 liu et al 2016 however the incorporation of mpi is relatively more complex than that of openmp and may have lower efficiency due to the load of communication among processes zhang and zhou 2019 6 conclusion this study proposed the automatic partition based parallel algorithm appa to achieve the optimal state of parallelization for grid based distributed models the results suggest that the performance of parallel computation using grid based distributed models is sensitive to the spatial domain decomposition by combining flexible partition of units e g sub basins or channel units with the load balance of thread allocation the estimated speedup ratio in appa increases noticeably and reaches 93 97 of tmsr for simulating hillslope processes and 91 98 of tmsr for simulating channel processes using 26 threads the real speedup ratio is greater than 90 of the estimated speedup ratio when the number of threads does not exceed the number of cores in our case study overall our proposed appa is useful for parallelizing grid based models to maximize the parallel performance under a given number of threads and thus increase the computation efficiency declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this study was supported by the national key r d program of china 2017yfc0405900 the chinese national natural science foundation 41671192 and the guangzhou municipal scientific program 42050441 appendix fig a1 c pseudo code of the parallel algorithm fig a1 
25756,parallel computing is a primary way to increase computing efficiency of grid based distributed hydrological models this study proposed an automatic partition based parallel algorithm appa to approach the theoretical maximum speedup ratio tmsr through a combination of flexible partition for the domain decomposition and the load balance of parallel simulation appa optimizes the parallelization of hillslope and channel flow routing processes at sub basin and channel unit level respectively to illustrate appa s performance we embedded it in a distributed ecohydrological model and then applied the updated model to three watersheds at different spatial scales the results indicate that appa effectively promoted parallel performance the estimated speedup ratio approached 93 97 of the tmsr for simulating hillslope processes and 91 98 of the tmsr for simulating channel processes using 26 threads in all three watersheds these improvements justify that appa is effective in accelerating model simulation and thus benefits future model based research keywords parallelization grid based distributed models automatic partition speedup ratio computing efficiency watershed modeling software availability software appa automatic partition based parallel algorithm language c availability appa is open sourced on github https github com xuzhenwu appa 1 introduction using distributed models to simulate coupled hydro ecological processes at large spatial and long term temporal scales can be cumbersome and time consuming with the rapid development of computer science the parallelization of model simulation becomes an effective strategy for speeding up the computing efficiency compared to the serial computing one of the most important approaches for parallel computing is to partition a heavy computation task into several independent loads so that the sub tasks can be allocated to multiple processors simultaneously for watershed modeling parallel simulation often divides the study area into several independent sub basins thus each of sub basins is treated as a single task and processed by an individual thread under the assumption that there are no or relatively low communications between sub basins vivoni et al 2011 however the task partition based parallel computing faces difficulties in finding the best partition solution largely due to the spatial connectedness of coupled hydro ecological process e g water and solutes routing in distributed models in spite of aforementioned disadvantages the parallel algorithm by partitioning a watershed into several sub basins has proved to be effective in speeding up model simulations tian et al 2008 wang et al 2011 qin and zhan 2012 the partitioned based parallel algorithm usually involves spatial domain decomposition computational task allocation and inter process communication zhang and zhou 2019 the spatial domain decomposition is to partition a large domain into smaller individual sub basins that are arranged from upstream to downstream apostolopoulos and georgakakos 1997 arnold et al 1998 li et al 2011 in addition to decomposition at sub basin level liu et al 2014 found that the distributed models can be parallelized at basic simulation unit levels through a layered approach consequently the computational tasks of these parallelized units are allocated for parallel computing however the dependency resulting from flow routing still coexists with concurrency this can be either resolved by simulation from upstream to downstream units at thread level with shared memory programing standard hwang et al 2014 liu et al 2014 or by simulation of units at process level with message passing programming standard tian et al 2008 li et al 2011 vivoni et al 2011 wang et al 2011 in practice the former may be constrained due to the consideration of the task arrangement while the latter may suffer from poor efficiency caused by a heavy load of inter process communications li et al 2011 vivoni et al 2011 even so a joint of them can break through the speedup ratio limit compared with using either of them liu et al 2016 zhu et al 2019 nevertheless the exploration of partition based parallel algorithms has greatly enhanced the computing speed of distributed models apart from the effectiveness i e speedup ratio the existing parallel algorithms need improving since the computational efficiency i e parallel efficiency is still restricted to the unit partition at a task parallelism level zhang and zhou 2019 to quantify the efficiency and guide the parallel algorithm development the theoretical maximum speedup ratio tmsr has been proposed as a target to achieve the potential of parallel computing under a given number of processors wang et al 2012 to approach tmsr in the flow routing systems of hydrological models the critical path heuristic algorithm has proven to be effective in scheduling static tasks of partitioned sub basins shirazi et al 1990 liu et al 2013 through analyzing basin width function wang et al 2012 insisted that there is a maximum speedup curve for an arbitrary drainage network in a preliminary application they found that the more sub basins are partitioned the higher the tmsr is in addition the compact watersheds generally have higher tmsr than long and narrow watersheds liu et al 2013 developed a method to estimate tmsr for parallel computing of hillslope and channel processes using grid based distributed hydrological models efforts are continuously made to maximize the parallel efficiency some researchers noticed that the parallel performance is improved with the increase of the number of partitioned sub basins as a result of better load balance wang et al 2012 liu et al 2016 besides the load balance is proved to significantly improve the parallel speed up with proportionally faster runs as simulation complexity i e domain resolution and channel network extent increases vivoni et al 2011 these explorations help researchers recognize the potential of parallel computing under a given parallel algorithm application of distributed models at large spatial scales driven by high spatial resolution data can involve a huge number of simulated grids compared with models built at coarser units e g sub basins hillslope grid level simulation usually consists of more simulation units grids and thus has a more extended and complex flow network hillslope flow routings and channel flow routings the increasing complexity of simulation not only increases computing burden but also may bring greater potential of parallelization although channel reaches are usually set as the basis for sub basin unit decomposition in prior partition based algorithms arnold et al 1998 li et al 2011 vivoni et al 2011 the grid based models have no such restricts and each grid can be set as the potential outlet for extracting a sub basin as partitioned units are further allocated to different processors the computation efficiency is sensitive to the spatial domain decomposition li et al 2011 zhang and zhou 2019 however previous studies usually consider the spatial domain partition as a preliminary process before allocating computation tasks for parallelization it is worth of investigating how the parallel efficiency will respond to the flexibility of partition for grid based distributed models to maximize the parallel performance we argue that an optimal partition based parallel algorithm should have following features a the utilization of the load balance strategy throughout the preparation of a parallel scheme b the search of potential parallel schemes based on flexible spatial domain partition for the optimal efficiency c an estimate of real performance compared with the theoretical maximum performance and d the applicability to most grid based distributed models to meet these criteria this study proposed an automatic partition based parallel algorithm appa that aims to parallelize grid based distributed models such that the theoretical maximum speedup ratio tmsr is approachable by combining global search for potential partition schemes with the load balance strategy in parallel simulation appa optimizes parallelization of hillslope and channel flow routing processes at both sub basin and channel unit levels respectively to illustrate appa s performance we embedded it into the coupled hydro ecological simulation system chess tang et al 2014 2019 and applied the updated chess to three watersheds using various threads the subsequent computing performances of parallel simulations were evaluated using the tmsr estimated speedup ratio and real speedup ratio overall this study provides a new parallel algorithm that can maximize the computing efficiency of parallel simulation thus contributing to future model based research 2 developing the automatic partition based parallel algorithm 2 1 parallel assumptions of grid based hydrological models the automatic partition based parallel algorithm appa assumes that a typical grid based distributed model has following characteristics 1 grids are basic units for simulating hydrological processes e g infiltration evaporation runoff 2 the overall hydrological processes can be classified as hillslope and channel processes in terms of flow equations e g hillslope surface and sub surface flow and channel flow 3 flow routing among grids is generated by a single flow routing algorithm e g d8 algorithm such that runoff yield in a grid is routed to only a downslope neighboring grid in addition directions of surface and sub surface flow are uniform for each hillslope grid 4 given the continuity and nature of downslope water movement grids located in upslope or upstream areas take precedence during simulation in practice this is realized by arranging all grids in descending order according to their elevations 5 the runtime for a grid simulation is assumed to be equal or is determined by its own type 2 2 general parallelization scheme for the automatic partition based algorithm the proposed appa applies a two level partition scheme for parallel computing so that the computing performance of grid based distributed models can be enhanced first grids are divided into hillslope grids and channel grids corresponding to hillslope processes including surface and sub surface flow and channel processes only containing channel flow second the two processes are further partitioned as sub basin units and channel units that are allocated to different threads for parallel computing by doing so the original serial simulation is divided into several rounds of parallel simulations within each round of parallel simulation the partitioned units are assumed to be independent and allocated to different threads that do not communicate with each other fig 1 depicts the two level parallel scheme and the corresponding parallel algorithm developed for a four core computer daily simulation of overall processes is divided into simulation of hillslope and channel processes for hillslope processes the computation tasks of five partitioned sub basins are eventually allocated to four threads fig 1a for channel processes the parallel computing consists of five sequential layers the first layer contains seven independent units which are further allocated to four threads fig 1b similarly three middle layers are sequentially parallelized the last layer of channel processes is computed by one thread as it has no branches for partitioning based on unit partitions and thread allocations the proposed algorithm adopts a fork join structure to achieve parallel computing and considers the load balance among processors to improve parallel performance fig 1c in addition no communication costs exist among threads in each round of parallel computing e g hillslope processes channel processes in first layer within each thread grids are simulated in descending order of elevation to keep flow movement daily simulation of hydrological processes terminates when the outlet grid is complete 2 3 principles of optimization for automatic partition based parallelization to approach the optimal state of partition based parallelization appa adopts the following principles 1 searching the potential parallel scheme repeatedly to select the optimal scheme as the selected model is grid based the size of partitioned units can vary due to many choices of the potential outlet grids each channel grid has the potential to be the reference point to trace all upstream grids that can be grouped into a sub basin or channel unit given this the accumulated runtimes for grid i t g i figs 2b and 5 b are used to estimate the required time for simulating all grids within a sub basin or channel unit when the runtimes for each grid are specified the total t g accumulates as the flow routes downslope to the outlet in theory if the runtimes for simulating each grid are identical the number of grids is just a proxy of the accumulated runtimes t g due to the flexibility of selecting outlets for sub basin or channel units it is likely to obtain the best parallel scheme by comparing computing performances under different domain partitions 2 utilizing the load balance strategy to composite the partitioned units as the final parallel scheme for example the partitioned sub basin units are assigned to groups with possible balanced runtime so that the final runtime for hillslope processes is minimized fig 1a 3 measuring the parallel performance to approach theoretical performance as the goal of partitioning is to fully utilize the computation power of computers with n available processors this study used three kinds of speedup ratios sr to evaluate the parallel performance the theoretical maximum speedup ratio tmsr the estimated speedup ratio esr and the real speedup ratio rsr sr is defined as the ratio of the time cost of serial computing t s to that of the parallel computing with n processors t n hwang et al 2014 1 s r t s t n the overall processes are divided into hillslope processes hs and channel processes ch and there are three types of sr for each of processes tmsr indicates the full utilization of all allocated threads as there are no communications for hillslope processes among sub basins the tmsr of hillslope processes equals the number of threads n when the load balance among threads is fully achieved tmsr h s n for channel processes tmsr c h is determined by the longest exit length which represents the minimum possible execution time shirazi et al 1990 liu et al 2013 it is computed as the ratio of the serial runtime spent for the longest channel exit length t l s to the required runtime for serial computing of channel grids t c h 2 tmsr c h t c h t l s then the overall tmsr is computed as follows 3 tmsr 1 p h s tmsr h s p c h tmsr c h where p h s and p c h refer to the runtime proportion for hillslope and channel processes respectively relative to the overall simulation values p h s p c h 1 the estimated speedup ratio esr is the estimate of speedup ratio for a given parallel scheme based on the load balance in the fork join structure fig 1c to compute esr t s is defined as the total runtimes of all partitioned grids and t n is defined as the thread that is allocated with the maximum runtime of partitioned grids eq 1 rsr is computed by the testing results of real runtimes of serial and parallel computing in comparison rsr is generally the smallest value since it accounts more costs tmsr esr rsr parallel efficiency ep is used to quantify the efficiency in parallel computing and defined as the speedup ratio to the number of available processors 4 e p s r n 2 4 parallel simulation of hillslope processes the simulation of hillslope processes is most time consuming as it involves the most of grids to be simulated in practice parallel computing of hillslope processes involves sub basin partitions and thread allocation as the partitioned sub basin units are independent the parallel computing of hillslope processes should be finished in one round of parallel computing with optimal thread allocation in addition an extra scheme selection process is provided for obtaining optimal parallel performance the three steps are described in the following paragraphs the sub basin partition starts with the definition of partition goals first we define the total computation time needed for hillslope processes as the required time for simulating all hillslope grids t h s thus the theoretical minimum runtime for all n threads t h s n is expressed as 5 t h s n t h s n specially the maximum computation time demanded for partitioned sub basins s m is determined by t h s n 6 s m t h s n d 1 θ s where the deviation index θ s and decomposition index d indicate the variation of s m θ s represents minor variations of s m which will not significantly increase the number of final partitioned sub basins m and d represents significant changes in s m for example m approximates twice of the value of n when d is 2 in practice a potential outlet grid with satisfactory accumulated runtime t g s will be searched from all channel grids this potential channel grid assures that the subsequently partitioned sub basins are independent in terms of hillslope processes and it will retain a closest value to but lower than s m 7 t g s t g i w h e n 0 s m t g i s m t g s t g s o t h e r s where t g s is initialized as zero before the searching process the value of t g s may be updated as the algorithm searches through all channel grids when a grid is captured t g s 0 it will be set as the reference outlet to trace the corresponding sub basin fig 2c once a sub basin is obtained the accumulated runtime for all grids t g will be recomputed and then the program starts to search for next sub basin until all hillslope grids are grouped into the partitioned sub basins fig 2d the thread allocation process allocates the computation tasks of independent sub basins to available threads in one loop of parallel computing fig 1c to optimize the allocation for obtaining the best load balance appa tries to reduce the maximum runtime among all threads first the thread with maximum runtime and the thread with minimum runtime are chosen respectively then the smallest sub basin in the thread with maximum runtime is moved to the minimum thread if it successfully reduces the maximum runtime among all threads that limits the parallel performance the redistribution process continues otherwise the optimal state of allocation is achieved for example the first to fifth sub basins in fig 1a are assigned to four threads in order i e sub basin 5 is assigned to thread 1 as thread 1 is the thread with the maximum runtime the smallest sub basin in thread 1 sub basin 5 is reassigned to thread 3 with the minimum runtime it retained as the optimal result of allocation where sub basin 5 and sub basin 3 are processed by the same thread fig 1c after all hillslope grids of sub basins are allocated to specific threads the applicable parallel scheme for hillslope processes is completed the strategy for the outer selection process is to choose the optimal scheme in terms of esr fig 2a since the maximum runtime of sub basins s m is subject to changes in the deviation index θ s and decomposition index d the proposed algorithm examines all potential parallel schemes empirically θ s has a value ranging from 0 to 0 4 and iterates by a step of 0 05 the decomposition index d is designed to increase the number of sub basins several times as d iterates from 1 to a larger value it will reveal how the variation of domain decomposition affects parallel performance then the best parallel scheme is selected from all schemes based on the esrs 2 5 parallel simulation of channel processes channel routing processes significantly affect the computing performance of model simulation although channel units are inter connected units with binary structures they can still be considered independent when all channel units are treated in a same layer liu et al 2014 as a result the upstream downstream computation dependence only imposes restrictions on the outer and inner layers where the channel units in outer layers take precedence over those in inner layers during simulation thus the aforementioned partition and allocation processes are applicable in parallelizing channel processes into several layers however the more layers there are the more rounds of parallel computing are needed because appa adopts flexible partition strategy for grid based models the partitioned channel units do not necessarily follow the locations of stream reaches in general similar to hillslope processes the parallel computing of channel processes involves channel unit partition and thread allocation in addition an extra scheme selection process is provided for obtaining the optimal parallel performance in simulating channel processes the channel unit partition and thread allocation are synchronous processes as with simulating hillslope processes we define a maximum runtime of threads for channel processes t c h m in fig 3 and thus the strategy of partition is to fill up t c h m which is assumed to be the same in all threads for load balance specially we defined the available runtime of thread i t a i to limit the size of rest of the runtimes of thread i that can be allocated for a potential channel unit t g s like setting the priority for larger sub basin units in hillslope parallel computing the thread with the largest available runtime t a m should be filled up first with a new channel unit step 1 in fig 3 8 t a m max t a 1 t a n as with the available runtime of a sub basin s m eq 6 t a m limits the size of accumulated runtimes of the partitioned channel unit t g i for channel processes the final outlet should meet the requirement that t g s has a slightly lower value than t a m step 2 in fig 3 9 t g s t g i w h e n 0 t a m t g i t a m t g s t g s o t h e r s where t g s is initialized as zero before the searching process the value of t g s may be updated as the algorithm searches through all unpartitioned channel grids if t g s is greater than zero the corresponding grid is set as the reference outlet to trace the channel unit fig 5c which is further allocated to t a m step 3 in fig 3 to eliminate the influence of this unit all the other channel grids located downstream will be excluded from the subsequent searching of t g s to avoid upstream downstream connection in this layer the accumulated runtimes of other grids t g i will be re computed specifically the available runtime of the maximum thread will be updated using the following equation 10 t a m t a m t g s if t g s remains zero the computing for this layer is finished and the unit partitioning and the thread allocation process is terminated after several rounds of parallel simulation the channel processes are finished and the maximum runtime for each thread t c h m in the layer is determined then the runtime of all threads t o v in each layer is defined as 11 t o v t o i where t o i is the total runtime of thread i and t c h m t o i t a i to search all the possibilities of parallel schemes in the layer t c h m is set as the smallest value in the first iteration i e the runtime for a channel grid t c h m t c g fig 4 b and kept updating as θ g increases 12 t c h m max t c h m 1 θ g t c h m t c g where max is the maximum function that returns the greatest value once a new t c h m is obtained the program records the estimated runtime of all threads t o v and the estimated speed up ratio e s r c h this process will be terminated once t o v has covered all channel grids with undefined layers and threads fig 4d e s r c h is the serial time t o v divided by the maximum runtime in a thread in parallel computing 13 e s r c h t o v max t o i fig 4a shows how e s r c h and t o v vary as the key variable t c h m increases using a four core computer as t c h m increases from 1 to 113 e s r c h remains having the maximum value of 4 and t o v keeps increasing fig 4a however the applicability of parallelization drops when t c h m exceeds 113 and e s r c h drops when t c h m exceeds the maximum accumulated runtime among all channel grids they will be allocated to the same thread and e s r c h is 1 fig 4d as the speedup ratio is the top priority of parallelization the schemes with the largest e s r c h are picked for next round comparisons t c h m ranges from 1 to 113 fig 4a then the scheme with the highest t o v is selected fig 4c this selection process can avoid involving too many layers and thus reduce time spent in allocating threads similar steps in scheme generation processes are performed until all channel grids are allocated to a specific layer and thread fig 5a as the partition processes move to the outlet the applicability of parallelization decreases and the last layer with a channel unit is processed using one thread the black bold line in fig 1b empirically the only tunable parameter θ g is set as 0 05 according to the characteristics of layers several rounds of parallel computing can accelerate the simulation of channel processes model simulation for a given time scale e g day will terminate once the simulation for the outlet of the watershed was complete fig 1c 3 application and testing of appa a case study the coupled hydro ecological simulation system chess tang et al 2014 2019 a grid based distributed model was applied to test the proposed appa chess simulates integrated water carbon nutrient dynamics as well as vegetation growth in terrestrial ecosystems at watershed and regional scales in chess the routing of water and solutes among grids is explicitly simulated when the model is applied at high spatial resolution and large spatial scales the implementation of parallel computing becomes crucial for improving the computation efficiency in this study the parallelization of chess was implemented through the open multi processing openmp library by c c compilation fig a1 first chess reads the thread information and lists all grids of each thread in a descending order of grid elevations in each loop of parallel computing the threads compute corresponding grids whose addresses are stored in a specific array to perform the fork join structure the main thread can continue until all parallelized threads are completed model simulation for a given time scale e g day will terminate once the parallel simulation of all grids is completed three watersheds located in southern china were selected for evaluating the performance of proposed parallel algorithms fig 6 they differ in total number of grids and areal coverage table 1 the two smaller watersheds are actually sub watersheds of the biggest and named corresponding to hydrological stations yuecheng and longchuan they represent ten thousand grid and hundred thousand grid level simulations respectively in contrast the largest watershed dongjiang consists of almost a million of grids in 200 m resolution and represents another level of simulation one super computer which is equipped with two way 14 core intel xeon e5 2683 2 0 ghz cpus and supports the parallel computing by a maximum of 28 processors threads was used to analyze parallel efficiency of the proposed algorithm the parallel simulations are performed using different numbers of processors and the resultant parallel performances based on different numbers of threads that vary from 2 to 32 by an interval of 2 are compared with the original serial simulation to analyze the computing performance of the proposed algorithm 4 results 4 1 optimizing the parallel computing of hillslope processes since sub basins are basic units in the parallel computing of hillslope processes the partition of sub basins significantly affects the performance of parallel computing to find the best parallel scheme appa uses the deviation index θ s and the decomposition index d to adjust the size of estimated runtime for partitioned sub basins s m fig 7 shows the variation of estimated speedup ratio esr and the relative increments of esr under different parallel schemes as the number of involved threads increases in the reference parallel scheme series θ s 0 d 1 esr increases linearly as more threads are involved black line fig 7a when change the size of s m θ s varies from 0 to 0 4 by a step length of 0 05 and the optimal result is selected from 9 parallel scheme series blue lines based on optimal results the maximum esr green line increases by about 20 compared with the reference series when the number of threads ranges from 8 to 32 which approximates half the space between the tmsr red line and the reference esr black line fig 7b the number of partitioned sub basins also poses a great effect on the speedup ratio of parallel computing for comparison the reference parallel scheme series black line in fig 7c are set as optimal results generated by searching different θ s under the condition that the decomposition index d equals 1 and the number of sub basins approximates the number of threads green line in fig 7a to change the number of partitioned sub basins we have d vary from 1 to 8 by a step of 1 and the optimal results are selected from 8 parallel scheme series green lines fig 7c fig 7b illustrates how d promotes the maximum esr by searching potential schemes under different d fig 7d indicates that the increasing partition of sub basins can increase esr when the number of threads exceeds 8 however it does not necessarily increase esr when the number of threads ranges from 2 to 6 in general the maximum esr increases by 4 17 compared with the reference esr by searching different parallel schemes with different θ s and d appa achieves the potential of parallel computing while the maximum esr is raised up to 97 99 of tmsr in dongjiang watershed fig 7c 4 2 optimizing the parallel computing of channel processes appa aims to increase the speedup ratio and reduce rounds of parallel computing in the implementation of the layered approach for simulating channel processes in the case of dongjiang watershed the parallel simulation based on the proposed algorithm generates 18 layers for parallel simulation in contrast the original layered approach generates more than 2898 layers for the channel processes grids in the critical exit length if it simply allocates a grid to a single thread liu et al 2014 although esr is the top priority in each layer of the proposed algorithm the potential of parallelization decreases as more channel units are partitioned fig 8 a as a result the esr for inner layers drops significantly for the last channel unit that has no branches it cannot be parallelized and esr is 1 for all three watersheds in general esr increases as the number of involved threads increases fig 8b however the increase in esr has a ceiling when esr approaches tmsr because the proposed algorithm explores all the possible partition based schemes and considers the load balance of parallelization the final esr of channel processes is comparable with tmsr the results based on three watersheds show that the maximum esr can reach 91 98 of tmsr using 26 threads table 1 4 3 evaluating real parallel performance fig 9 illustrates the variations in execution time speedup ratio parallel efficiency and the efficiency deficit as the number of threads changes for the dongjiang watershed the execution time for hillslope processes drop sharply and the speedup ratio increases as the number of threads increases fig 9a and b the estimated speedup ratio for hillslope processes esr h s reaches 25 1 and the real speedup ratio rsr h s reaches 21 6 86 1 of esr h s under 26 threads for channel processes the real speedup ratio under 26 threads rsr c h 26 is 6 6 approximating 89 2 of the estimated value 7 4 as hillslope grids account for 97 9 of all grids the overall performance is mainly determined by performance of simulating hillslope processes in general the parallel efficiency using more threads is lower fig 9c the estimated parallel efficiency for hillslope processes is relatively stable approximating 97 of the theoretical efficiency after the estimated speed ratio for channel process esr c h peaked at 7 4 the parallel efficiency drops continuously which greatly reduces the parallel efficiency for simulating overall processes the efficiency deficit defined as the difference between the real and estimated parallel efficiencie indicates how the real speedup ratio approximates estimated performance the efficiency deficit is less than 10 for all processes when the number of threads is less than 24 however when the number of threads exceeds 28 the real efficiency decreases dramatically as the super computer used in this study only has 28 cores fig 9d fig 10 shows the overall performance of parallelization applied in three watersheds the esr h s using 26 threads varies from 24 3 to 25 1 for the three watersheds and accounts for 93 97 of tmsr h s the performances among the three watersheds differ mainly in tmsr c h which is quantified by channel width function as a result the parallel simulation of the dongjiang watershed has a larger speedup ratio compared with the two smaller watersheds table 1 meanwhile there is no significant difference in the efficiency deficit among the three watersheds which is less than 10 when the number of threads is less than 24 fig 10d in general appa is efficient in parallel computing and the estimated performance is consistent with real performance 5 discussion grid based distributed mechanistic models are increasingly used for evaluating surface and subsurface hydrological processes parallel computing attempts to tackle the computing challenge for applying distributed models at large spatial and long term temporal scales in general a parallel algorithm is only feasible for models that fit specific parallel assumptions which regulate the implementation of the parallel framework with little or no loss of model s accuracy by setting assumptions as pre conditions a parallel algorithm can better describe ideal state of high performance computation and seek the optimal method to reach the ideal performance in this study the proposed appa algorithm is designed for grid based distributed hydrological models to obtain the optimal computing performance under given hardware condition flow routings which keep water flow along the gravity gradient are the source of challenges in parallelization for hydrological models our proposed appa assumes that models should adopt a single flow routing algorithm by which flow generated in a center grid is routed to only one downslope neighboring grid o callaghan and mark 1984 as a result sub basins are assumed to be independent in terms of hillslope processes otherwise additional approaches are required to meet the demand of message passing among these parallelized units for example vivoni et al 2011 used ghost cells to simulate lateral subsurface flow for ridge grids which are located on shared boundaries between sub basins in fact multiple flow routing algorithms based on topography quinn et al 1991 seibert and mcglynn 2007 or real time water level dai et al 2019 consider flow dispersion at each grid and grids at watershed ridges in which flow can belong to two or more partitioned sub basins nevertheless it is still likely for these models to adopt an independent partition based parallelization without communication among partitioned units if flow generated at ridges of sub basins is only routed in one direction since ridge grids account for a small portion of all simulated grids and model outputs are averaged for the whole watershed differences between parallel and serial computation may be negligible apart from this the use of parallel algorithm can be performed flexibly with better understanding of model structures related to flow routing in this study appa was applied for the whole model simulation as chess has closely coupled runoff production and routing processes at grid level for models that have independent runoff generation and routing processes which are usually simulated in a sequential order appa can also be selectively used for only routing processes liu et al 2014 appa classifies simulated grids as either hillslope grids or channel grids the former indicates where hillslope processes e g surface and sub surface flow routing occur and the later represents where channel processes e g channel flow routing take place this assumption is suitable for most grid based distributed models at regional or large spatial scale e g vic liang et al 1994 rhessys tague and band 2004 for models that have no specification between the two processes users of appa can set a boundary value for accumulated runtime t g bhc to separate hillslope grids from channel grids as suggested by vivoni et al 2011 the best strategy of parallelization for large river basins is to combine a balanced partitioning with an extended channel network as the outlet of sub basins must be a channel grid such that sub basins are independent in terms of hillslope processes the magnitude of t g bhc will limit the size of the smallest sub basin and imposes restriction on uses of more threads used in parallel computing for example when the value of t g bhc is 10 of time for serial computing the estimated speedup ratio for channel processes will approach the limit of 10 in spite of increasing number of threads empirically the value of t g bhc should be within the range of 0 5 5 to tap the potential of hillslope parallelization for grid based models if aforementioned assumptions are meet appa will approach the tmsr by searching for the optimal spatial domain decomposition and computational task allocation to better illustrate the parallel performance appa was tested using a typical grid based model i e chess in experiments the proposed approach is not only efficient as a result of optimization for partition based parallelization but also easy to use since the deployment of parallel simulation with appa requires little knowledge of shared memory programing to achieve the fork join structure fig a1 for parallel performance the estimated speedup ranges from 19 5 to 23 9 using 26 threads for the three watersheds which almost reach the theoretical limit 96 of the tmsr table 1 the corresponding real speedup ranges from 19 4 to 20 6 a comparison between our results and prior work in adoption of high performance using distributed hydrologic models indicate a set of general outcomes in parallel performance vivoni et al 2011 in table 2 we investigated various parallel frameworks and briefly summarized their key findings from case studies the source of differences between those approaches lies in the basic simulation unit in hydrological simulation which influences the complexity of model simulation the larger the simulation unit in terms of the spatial scale e g sub basin hillslope is the less complexity the flow routing network is as a result the enhancement of computing efficiency is limited in spite of increasing parallel resources due to constraints of the binary tree structure of drainage network which generally have fewer simulation units rsr 12 li et al 2011 wang et al 2011 wang et al 2012 in comparison modelling with more simulation units implemented in finer basic units e g grids vs sub basins larger area coverage e g larger watersheds vs smaller watersheds or higher spatial resolutions e g 90 m vs 270 m is proved to have a larger speed up ratio vivoni et al 2011 liu et al 2016 zhang and zhou 2019 so as to better quantify the progress of proposed parallel framework the theoretical maximum speedup ratios are defined for given processers and given model s applications wang et al 2012 liu et al 2013 compared with tmsr the real parallel efficiency is mainly hampered by two main factors the balance of the computation loads and the increase in input output i o costs vivoni et al 2011 zhang and zhou 2019 in this study the combination of spatial domain decomposition and load balanced task allocation is proved effective in reducing the former costs a better load balance is achieved with a different extent of disaggregation fig 7 meanwhile the communication costs are avoided since we adopted the fork join structure of openmp as a result the real performance is close to the estimated with efficiency deficit stably less than 10 when the number of available threads are less than 24 this extra loss of efficiency can be attributed to other factors e g the cost of creating threads and building parallel environment hardware limit data management tools for example the best fit between real and estimated speedup occurs in the smallest yuecheng watershed efficiency deficit 2 across all experiments used different threads indicating that the real performance of appa can be improved with an advanced data management method in parallelization although this study demonstrated the possibility for grid based hydrological models to approach tmsr at thread level the proposed appa can be modified for other types of models for example appa can be modified to facilitate the need of parallelization for models based on other basic units e g tin hru or other routing related computations with huge simulation complexity if the runtime and flow routing of each basic unit are defined appa can be used for spatial decomposition to maximize speedup meanwhile it seems that appa can be deployed at process thread level with mpi programing grepping the power of computer clusters the adoption of multiple parallelization levels e g threads and processes can break through the limit of tmsr at thread level which depends on available cores in a computer rsr 30 vivoni et al 2011 liu et al 2016 however the incorporation of mpi is relatively more complex than that of openmp and may have lower efficiency due to the load of communication among processes zhang and zhou 2019 6 conclusion this study proposed the automatic partition based parallel algorithm appa to achieve the optimal state of parallelization for grid based distributed models the results suggest that the performance of parallel computation using grid based distributed models is sensitive to the spatial domain decomposition by combining flexible partition of units e g sub basins or channel units with the load balance of thread allocation the estimated speedup ratio in appa increases noticeably and reaches 93 97 of tmsr for simulating hillslope processes and 91 98 of tmsr for simulating channel processes using 26 threads the real speedup ratio is greater than 90 of the estimated speedup ratio when the number of threads does not exceed the number of cores in our case study overall our proposed appa is useful for parallelizing grid based models to maximize the parallel performance under a given number of threads and thus increase the computation efficiency declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this study was supported by the national key r d program of china 2017yfc0405900 the chinese national natural science foundation 41671192 and the guangzhou municipal scientific program 42050441 appendix fig a1 c pseudo code of the parallel algorithm fig a1 
25757,extensive efforts to adaptively manage nutrient pollution rely on chesapeake bay program s phase 6 watershed model called chesapeake assessment scenario tool cast which helps decision makers plan and track implementation of best management practices bmps we describe mathematical characteristics of cast and develop a constrained nonlinear bmp subset model software and visualization framework this represents the first publicly available optimization framework for exploring least cost strategies of pollutant load control for the united states largest estuary the optimization identifies implementation options for a bmp subset modeled with load reduction effectiveness factors and the web interface facilitates interactive exploration of 30 000 solutions organized by objective nutrient control level and for 200 counties we assess framework performance and demonstrate modeled cost improvements when comparing optimization suggested proposals with proposals inspired by jurisdiction plans stakeholder feedback highlights the framework s current utility for investigating cost effective tradeoffs and its usefulness as a foundation for future analysis of restoration strategies graphical abstract image 1 keywords water quality stakeholder participation decision support system web application chesapeake bay watershed model watershed management optimization 1 introduction chesapeake bay the largest estuary in the united states is an important focus for restoration and protection efforts due to the past and ongoing threats to its ecological health that are of tremendous public interest as well as its historic social environmental and economic value since at least the 1960s and up to the present recurring issues of poor water quality during the summer have included hypoxic conditions of varying severity extent and duration officer et al 1984 hagy et al 2004 kemp et al 1992 2005 murphy et al 2011 bever et al 2013 numerous studies have shown that low dissolved oxygen and other related water quality issues are largely a result of excess nutrients and sediments gillelan et al 1983 kemp et al 2004 2005 boynton and kemp 2008 li et al 2016 and these excess pollutants are coming from both point and nonpoint sources throughout the watershed boynton et al 1995 da et al 2018 ator et al 2019 under the clean water act of 2004 the chesapeake bay is designated as an impaired water body and since 2010 the chesapeake bay total maximum daily load tmdl and its limits on nutrients and sediment pollution have helped guide restoration efforts usepa 2010a b the 2010 chesapeake tmdl is the largest most complex tmdl in the country covering a 166 000 km2 area across the mid atlantic region the tmdl allocates or places limits on loadings of nitrogen phosphorus and sediment sources in the watershed to remove impairments for aquatic life in the tidal tributaries and embayments of the chesapeake linker et al 2013b the tmdl allocations for nutrients and sediment are the basis of comprehensive restoration plans called watershed implementation plans wips which also include rigorous accountability measures to restore water quality and living resources in the chesapeake bay and its watershed usepa 2021 restoration efforts involve many stakeholders including six states the federal government the district of columbia 1 800 local governments and almost 200 county level jurisdictions other stakeholders include commercial entities academic institutions and non governmental organizations ngos coordination for restoration efforts has been led for several decades by the chesapeake bay program cbp partnership a formally organized collaborative effort that is administered by the chesapeake bay program office cbpo of the u s environmental protection agency usepa in close coordination with other federal state local and ngo partners to meet agreed upon goals for a healthy bay and to address continued restoration challenges such efforts must be both sustained and improved harding et al 2016 2019 kleinman et al 2019 zhang et al 2018 the cbp makes extensive use of models to support decision making for watershed management paolisso et al 2015 these models including the current version phase 6 of the chesapeake bay program s watershed model which is a focus of this paper are maintained by the cbpo and other federal partners in a process overseen by a modeling workgroup which includes representatives from multiple partners the models are used to help plan management actions that can reduce excess nutrient and sediment loads to chesapeake bay these tools also aid the usepa and the seven chesapeake bay jurisdictions in tracking progress towards agreed upon implementation goals for improved water quality such as the tmdl wips as well as other environmental related goals ongoing development and refinements of the suite of modeling tools in conjunction with substantial monitoring efforts has provided cbp and its partners with ever improving lines of evidence with which to evaluate restoration status and guide new restoration efforts although the suite of cbp modeling tools simulates the effect of management actions it has not included an automated system for analyzing and searching among the large range of possible management scenarios that can be simulated and realized because of this lack of automation tools use of the existing modeling suite can be time consuming and difficult for planning purposes in particular there is no automated way to identify the least cost portfolio of management action alternatives that could achieve a target level of nitrogen reduction although an experienced user of the model can find satisfactory scenarios there is often room for improvement and other users sometimes go through many iterations of manual trial and error with educated guessing to succeed such difficulties primarily arise because of the large number of decision variables i e combinations of management action types their placements and their amounts and the nonlinear cascading effects of simulated actions an optimization engine that can algorithmically search the decision space and present cost effective implementation strategies would enhance the utility of the cbp modeling tools increase ease of use and enable interested partners to focus on making more effective policy decisions critical to restoring the bay the cbp partnership has recognized the need for expanding the capabilities of the modeling framework to assist users in planning a science based suite of cost effective best management practices for development and implementation in particular and as further described subsequently in this paper stakeholders have specifically requested and recommended the development of better tools for identifying optimal portfolios of best management practices bmps which are cbp reviewed and approved practices for reducing nitrogen phosphorus and sediment loads to the chesapeake bay cbp 2018 implementation of certain bmps can require substantial investments for construction or appropriate adoption modification and maintenance of these practices such practices are perhaps better referred to as beneficial management practices with the same initialism bmp in recognition of the facts that the effectiveness of practices depends upon multiple factors and that which practice is best will vary with place and time as such any strategy that seeks to optimize investment decisions regarding bmps should consider numerous factors including but not limited to the financial investment effectiveness and feasibility of implementing a practice for a given locale previous simulations and analyses of bmp strategies for the chesapeake bay watershed have demonstrated the potential for substantial cost savings through better optimization of implementation strategies several such efforts have used data from a previous version phase 5 3 2 of the chesapeake bay program s watershed model for example analyses of alternative strategies for achieving tmdl goals including both bmps and wastewater treatment upgrades as options usepa 2012 van houtven 2012 have demonstrated that more widespread use of bmps in particular green infrastructure approaches are often cost beneficial compared to more traditional grey infrastructure approaches an extensive watershed wide analysis by kaufman et al 2014 ordered bmps by marginal abatement cost and found several cost savings through careful selection and spatial targeting of bmps for nitrogen phosphorus and sediment reduction optimization efforts at smaller scales have shown similarly encouraging results collaborative examination of management tradeoffs was enabled by optimizing bmp implementation using an early version phase ii of the chesapeake bay program s watershed model for the interstate region of the potomac river basin results highlighted decision making challenges that arise from numerous and complex management alternatives schwartz 2010 xu et al 2019 found cost reductions by targeting bmp placement in the mahantango watershed of pennsylvania using a soil water assessment tool swat model for historic conditions as well as under conditions associated with projections of climate change the usepa also developed the watershed management optimization support tool wmost a spreadsheet based decision support tool which has been used in a case study for cabin john creek watershed in montgomery county md i e within the chesapeake bay watershed to assess bmps with hydrology time series inputs detenbeck et al 2018 prior efforts have demonstrated the utility and value added by an optimization framework in aiding and improving nutrient and sediment reduction decisions however they do not sufficiently satisfy cbp requirements because they were limited to subsets of the chesapeake bay watershed geography or considered independently derived combinations of land use and bmps that were not approved by the cbp partnership they also do not interface directly with the model software that is used by the cbp partnership in other words previous efforts have not been sufficiently tailored or developed specifically for the unique challenge of providing guidance entirely consistent with the cbp approved data methods and model applied homogeneously at the scale of the entire chesapeake bay watershed the focus of this paper is the development integration and implementation of a linked modeling and optimization framework for planning and appraising cost effective strategies for implementing bmps the approach helps address management relevant questions regarding cost effective bmp implementation in this large multi jurisdictional watershed and as such was developed through a stakeholder driven approach to meet the needs of the cbp partnership our particular contributions are development and documentation of i a simplified model structure necessary for optimization ii a new optimization approach for selecting among a subset of cbp recognized bmps and iii new software for exploring solutions notably these use the same data as the cbp simulation system and are designed as a direct response to the need recognized by stakeholders and recommendations of the cbp scientific and technical advisory committee stac specifically this paper describes strategies for simplifying a watershed simulation structure that are now based on multiple models and lines of evidence section 2 the specifications requirements and implementation of an efficient optimization framework for large scale planning problems with a subset of cbp recognized bmps section 3 and the visualization and analytical capabilities that were developed to deliver optimal county level solutions to stakeholders involved in drafting of watershed implementation plans or wips section 4 finally we provide a discussion of the overall performance of the framework with consideration of its limitations section 5 and offer some concluding remarks section 6 2 watershed modeling framework in this section we provide a general description of the cbp modeling framework structure of the management model incorporation of various load sources and simulation of the impacts of bmps 2 1 motivation and requirements the cbp modeling suite has undergone multiple phases of development and improvement since its inception in the 1980s the watershed and estuary models are cornerstones of the suite the watershed was historically simulated using a semi lumped parameter model based on the widely used hydrological simulation program fortran hspf to estimate nitrogen phosphorus and sediment loadings shenk et al 2012 improvements over time have facilitated large scale efforts of calibration and scenario analysis efforts the estuarine model has also undergone numerous improvements in resolution and representation of processes during this period cerco and cole 1993 1994 cerco 1995 2000 cerco et al 2002 2010 2013 cerco and noel 2004 cerco and noel 2013 although the conventional method for evaluating management scenarios utilizes a time dynamic watershed simulation this approach presents several challenges for a large regional scale model such as for the chesapeake bay watershed first the time dynamic model is computationally intensive and takes a long time to simulate critical water quality processes for such a large watershed second the pre and post processing of modeling files become cumbersome and pose a barrier to efficient use third difficulties in setting up and running model scenarios contribute to stakeholders requesting results from practitioners with the expectation that the models will be run for them lastly when stakeholders do not run models directly they may be less inclined to trust results and more likely to question whether runs were performed correctly early efforts to overcome these challenges with use of the watershed simulation resulted in the development of various auxiliary tools two such tools proved particularly useful and represented significant advances in guiding management decisions and garnering stakeholder interest scenario builder usepa 2010c was a database tool that provided an automated method for rapidly creating scenarios with all the necessary inputs for later evaluation in the watershed model simulation the maryland assessment scenario tool mast developed by j7 llc the interstate commission on the potomac river basin and the maryland department of the environment was an online load estimator that provided results similar but not identical to those of the watershed model the input to mast could be emailed by state agencies to the cbpo for input to the scenario builder database then run through the time dynamic watershed simulation scenario builder and mast addressed challenges inherent in pre processing and allowed stakeholders to directly orchestrate drafting of management scenarios using estimated loads however the challenges of extended run time and inability for stakeholders themselves to execute the full watershed model approved by the partnership remained for these reasons it became clear that a model version that managers and other interested stakeholders could run directly would vastly accelerate simulation and analysis timelines reduce barriers to its application and provide greater transparency to provide such a model the cbp developed a time averaged model with a simplified structure to serve as the primary management tool for evaluating the effectiveness of alternative bmp implementation strategies its development required leveraging the participatory strengths of the partnership including stac workshops workgroups discussions and cross group coordination the result is a comprehensive data heavy time averaged watershed model that utilizes information from many sources to produce cbp partnership approved estimates of nutrient and sediment loads throughout the entire chesapeake bay watershed this current version phase 6 of the chesapeake bay watershed model also known as the chesapeake assessment scenario tool cast provides managers and other interested stakeholders a means to develop and evaluate bmp implementation scenarios themselves each scenario represents annually averaged hydrologic conditions for the user selected baseline year each year represents unique historic or projected land uses wastewater treatments and for proposed levels of bmps for both point and nonpoint sources these in turn change the estimated load in essence cast represents an improvement of mast expanded to the entire chesapeake bay watershed and it has transitioned from what was originally an emulator model of a time dynamic management model to what is now the management model itself 2 2 watershed model structure and simplifications by design cast simplifies the structure of the time dynamic watershed model the current version consists of a relational database a user interface a web server and a scenario processor stored in the database the source data includes most of the static information used for model runs such as bmp definitions methods for estimating bmp effects and geographical specifications the scenario processor executes the logical and mathematical operations starting from user specified scenario inputs and the source data and going through numerous validations to calculate loads delivered to the bay because the focus in this manuscript is on the calculations the cast user interface and web server are not described here and readers are suggested to refer to chesapeake bay program 2020 for additional details an input scenario to cast is at a minimum a choice of year geography bmp options and costs the choice of year provides baseline inputs for land use and wastewater for a given scenario the system performs a stepwise series of calculations to evaluate nutrient and sediment loads delivered to the bay the cast documentation contains many details on the computation and rationalization of inputs cbp 2020 however it does not provide the entirety of the numerical formulation of cast in equation form such a description provided as one or a set of equations detailing the load function i e how the inputs and watershed processes are modeled to estimate load delivered to the bay is indispensable for formulating an efficient optimization model in the remainder of this section we provide a mathematical characterization of the load function see tables 1 and 2 for notation used that is consistent with the cast documentation and we briefly describe the information used to determine values for each term of the load function the descriptions provided here are intended to help the reader understand the general structure of calculations in cast and to provide a representation amenable to optimization 2 2 1 load function the finest scale of spatial segmentation in cast is the land river segment which corresponds to the intersection of a land segment and a river segment land segments primarily represent 197 chesapeake bay counties and river segments represent sub watersheds associated with river reaches cbp 2020 section 11 more specifically the land river segmentation partitions the entire watershed into 1 925 mutually exclusive spatial units representing the intersections of political and watershed geographies a load source is a type of land land use or other artificial construct that is modeled as providing nutrient or sediment loads with potential for delivery of these loads to the bay any selected geographic region is comprised of a set of land river segments s a subset of s for a given baseline year and nutrient nitrogen or phosphorus the total load delivered to the bay from a selected region l s is primarily a sum of contributions from non direct and direct load sources with modification by a stream bed and bank factor ζ s h s t b and shoreline contributions eq 1 l s s s h h n o n d i r e c t l o a d s s h ζ s h s t b d i r e c t l o a d s s s h o r e l i n e l o a d where notations are defined in tables 1 and 2 the set of agencies h differentiates among selected federal government landholders and non federal landholders which include privately held as well as state and local government held land the agency designation allows for accounting of land use management responsibilities and does not affect bmp effectiveness next we expand the three high level terms n o n d i r e c t l o a d s s h d i r e c t l o a d s s and s h o r e l i n e l o a d to reveal more of their structure and meaning in the context of model formulation load density i e nutrient or sediment load per acre from non direct sources u u n is a modified sum of inputs it represents an accounting of changes in load with inputs applied to land for a scenario specifically the difference between input load density for a given scenario i u t k s and the watershed wide long term average input i u t is modified by a sensitivity factor σ u t which quantifies marginal change in runoff with changes in input these weighted differences are summed across all input types and added to a watershed averaged load density for each load source l u w the resulting load densities in lb ac are multiplied by the number of land acres α s h u to get load in lb which is attenuated by efficiency bmps θ s h u see section 3 1 1 and by delivery factors ψ s h u representing load attenuation during transport to the bay eq 2 n o n d i r e c t l o a d s h u u n l u w t t i u t k s i u t σ u t θ s h u α s h u ψ ˆ s h u where ψ ˆ s h u represents a cumulative delivery factor of processing taking place on land streams and rivers ψ ˆ s h u ψ s h u l w ψ s h u s r ψ s h u r b transporting load to the bay note that load densities for each input type i u t k s are calculated at the county k or land segment not land river segment scale which is the scale where inputs are estimated using partnership approved data and methods load densities for each involved land river segment within a county are assigned that same value fig 1 shows a schematic representation of eq 2 for county loads delivered to the bay direct load sources u u d include wastewater point sources septic and rapid infiltration basins nutrient delivery from these sources is generally calculated as follows eq 3 d i r e c t l o a d s u u d l s u ψ ˆ s u where ψ ˆ s u represents a cumulative delivery factor ψ ˆ s u ψ s u l w ψ s u s r ψ s u r b transporting load l s u in lb to the bay loads from the shoreline are added separately from other source terms although shoreline is often considered an additional direct source because it does not have an associated input sensitivity σ u t eq 4 s h o r e l i n e l o a d s s h h l s h u s h o r e l i n e substituting the term expansions of equations 2 4 in eq 1 yields eq 5 l s s s h h u u n l u w t t i u t k s i u t σ u t θ s h u α s h u ψ ˆ s h u ζ s h s t b u u d l s u ψ s u s s h h l s h s h o r e l i n e where ψ ˆ s h u represents a cumulative delivery factor ψ ˆ s h u ψ s h u l w ψ s h u s r ψ s h u r b ψ s u u d represents the delivery factor for a specific direct load source and other notation is defined in tables 1 and 2 2 2 2 watershed model terms multiple lines of evidence values for the terms in the above eq 5 are specified using a combination of multiple models or lines of evidence that could include other simulation models regression models or census data for instance the l u w term represents the watershed wide denoted by superscript w average of loading for load source u the values for this term are estimated by a combination of multi model averaging literature review and large scale mass balance as described in cbp 2020 section 2 the term i u t which represents average input load of type t for load source u is determined through a cbp partnership process using the best available data for manure generation and transport fertilizer sales estimates of soil p storage in agricultural lands and various data model hybrids including for example the community multiscale air quality cmaq model and annual phosphorus loss estimator aple model cbp 2020 section 3 4 linker et al 2013a vadas et al 2009 the term ζ s h s t b which represents the load contribution factor from stream beds and banks is determined from observational floodplain studies cbp 2020 section 8 the σ u t term represents the sensitivity of load source u to loads from input type t and this term is determined by investigations using process based watershed models cbp 2020 section 4 the α s h u term representing the acres of load source u in agency h and land river segment s is determined primarily by a usgs land use model cbp 2020 section 5 the delivery factor terms ψ s h u l w ψ s h u s r ψ s h u r b and ψ s u representing fractions of material transported from land to water stream to river river to bay and directly from point sources respectively are determined by using a sparrow model preston and brakebill 1999 ator et al 2011 and a dynamic watershed simulation based on hspf cbp 2020 section 10 the i u t k s θ s h u α s h u l s u and l s h s h o terms are determined in part by partnership approved bmp parameters and the bmp implementation amounts specified for a scenario at the time of this writing cast simulates approximately three hundred bmps cbp 2020 and calculates their load reduction effects according to a specific set of groupings and corresponding rules although the bmps represented in cast can be categorized in a variety of ways cbp 2018 table 3 shows a grouping based on the way load reductions are calculated in cast section 3 provides additional details on these calculations with a focus on the first grouping efficiency bmps which are so called because of their assigned scalar load reduction effectiveness or efficiency values see section 3 1 1 for more details these bmp effectiveness values vary depending on their location within the watershed based on hydrogeomorphology 2 2 3 unit bmp costs in addition to load and load reduction estimates for bmps cast contains summary level bmp annualized cost estimates based on evidence from existing studies for the efficiency bmps these estimates are expressed as average unit i e per acre costs to correspond with the load and load reduction estimates which are expressed on an annual time step these bmp unit cost estimates are also expressed as annual values that is one time per acre capital and installation costs are amortized over the expected lifespan for each bmp using a 5 percent assumed discount rate and added to the per acre annual operation and maintenance costs for each bmp 3 optimization framework 3 1 motivation as previously noted the optimization effort described here has been largely the result of requests and recommendations from stakeholders within the cbp partnership and particularly of a growing demand from users for an expansion of the modeling framework s capabilities to assist users with finding more optimally cost effective portfolios of bmps in 2016 the cbp s scientific and technical advisory committee stac held a workshop of regional planning managers technical experts and other stakeholders in chesapeake bay watershed modeling and management to discuss the possibility for optimization methods to be applied and used as part of cbp s modeling suite stac recommended to cbp that a bay optimization system be created to enhance the existing suite of decision support tools davis martin et al 2017 the first key objective recognized by the workshop participants was to provide a means of minimizing total costs by identifying optimal portfolios of bmps and of doing so in a way that could be easily understood and implemented by the managers stakeholders and users in the various jurisdictions in the bay watershed in response to this workshop the cbp organized an optimization tool development project involving members of the cbp s modeling team and a group of researchers and scientists managed through the chesapeake research consortium see acknowledgements for details in this section and the next we describe our expansion of the regulatory modeling framework to include a new optimization model and decision support tool these tools have been designed for exploration and identification of cost effective load reduction strategies within the cast simulation space for counties throughout the entire regional scale chesapeake bay watershed an overarching goal for this new optimization framework is to identify and provide stakeholders with least cost bmp options for achieving given load reduction targets in the cbp time averaged watershed model based upon its structure as described above in section 2 numerically this has been represented as a constrained optimization problem with a cost objective subject to nutrient load constraints the following subsections describe the watershed data optimization model and implementation of the optimization approach 3 2 watershed data to solve an optimization problem instance several pieces of data are retrieved from the cast source database and parsed into useable formats for the optimization software the data required for optimization include bmps nutrient loads and geographic delineations additional details simplifications assumptions and scales applied to these data for the optimization model are described in the following subsections 3 2 1 bmps and the efficiency bmp subset as shown in table 3 the largest group of cast bmps are known as efficiency bmps for which examples include cover crops stormwater ponds and conservation tillage efficiency bmps are so called because their individual effects on load are determined in cast by an assigned scalar load reduction effectiveness or efficiency value this group of approximately 190 bmps was selected first for inclusion in the optimization model because it represents a substantial portion approximately two thirds of the total number of bmps in cast table 3 and it is a subset whose effects can be calculated separately from the other non efficiency bmps to create an independent optimization model the effect of efficiency bmps is calculated in isolation from the non efficiency bmps i e assuming a static implementation for non efficiency bmps that is not altered by the optimization framework as such a number of non efficiency bmp types that are both effective and widely considered for implementation by decision makers are not yet being evaluated in the optimization system and are outside the scope of this study see discussion in section 5 3 the effects of efficiency bmps in cast are determined by assigned scalar effectiveness values each of which has been defined by cbp expert panels that consist of regional as well as national scientists researchers and interested stakeholders cbp 2010 devereux and rigelman 2014 as an example in the absence of other bmps an efficiency bmp with a nitrogen effectiveness value of 0 12 would reduce edge of stream nitrogen delivery by 12 i e it has a pass through factor of 0 88 meaning it permits 88 of nitrogen to pass through its modeling segment for calculation purposes efficiency bmps are assigned to groups defining whether any two bmps can overlap on the same acreage to provide cumulative nutrient reduction benefits for example cover crops and enhanced nutrient management can both be applied to the same agricultural land acre however two different types of cover crops cannot these groups impact the permissible implementation options because within a single group bmps compete for the available acres meaning that they are mutually exclusive across groups bmps do not compete for acreage and can overlap when bmps overlap their combined pass through effect is calculated by multiplying the pass through percentage factors for each individual bmp implementation costs for each bmp in the optimization model are the same watershed wide average estimates provided in cast section 2 2 3 3 2 2 nutrient loads and the efficiency focused formulation the optimization model leverages cast output to make use of pre calculated values of load density called base load density φ s h u this composite term represents the weighted input densities and delivery factors for non direct sources along with stream bed and bank contributions eq 6 φ s h u l u w t t i u t k s i u t σ u t ψ ˆ s h u ζ s h s t b where ψ ˆ s h u ψ s h u l w ψ s h u s r and other notation is defined in tables 1 and 2 this base load density is combined with acres and pass through factors to compute total load delivered to the bay by substituting eq 6 into eq 5 and letting loads from direct sources l s u u d and shoreline l s h s h o r e l i n e be zero consistent with their linear independence from the non direct sources we arrive at a modified formulation for load from a selected geography l s eq 7 l s s s h h u u φ s h u θ s h u α s h u where as above α s h u represents acres available for implementation the combined pass through factor θ s h u is a function of the implementation acres of efficiency bmps and their load reduction effectiveness the simplified form of the load function in eq 7 permits a succinct articulation of a nonlinear programming model 3 2 3 geographies and the county scale the optimization tool has been developed to provide least cost or maximum load reduction solutions at county scale counties across the chesapeake bay watershed are composed of between 1 and 34 land river model segments due to access restrictions and privacy concerns much of the data in cast such as manure applications and crop types are not available at geographic resolutions finer than counties therefore modeling analyses have thus far focused on counties as they represent the primary scale of interest nevertheless the optimization model should be scalable to larger geographies such as conservation districts or states as long as they are determined precisely by a collection of land river segments 3 3 optimization model and solver based on the data described above the optimization model is defined primarily by two functions total annual cost of bmp implementation eq 8 and total annual load reduction at edge of tide i e delivered to the bay eq 9 the first is a linear convex function that can be computed simply without a cast simulation and the second is a nonlinear and non convex function depending on the optimization goals the cost and load functions are generally interchangeable as objective or constraint i e swapping eq 8 and eq 9 and replacing the right hand side load target in eq 9 with a budget target the feasible region which is the set of all possible values of the variables that satisfy all constraints simultaneously is defined by the edge of tide nutrient load target eq 9 acreage restriction on implementation of bmps in each group eq 10 and non negativity constraint for each bmp implementation variable eq 11 an optimal solution to the model must lie within this feasible region the model is presented in eqs 8 11 following definition of its sets collections of unique and unordered elements decision variables and parameters constants that must be specified in each optimization problem instance sets s set of land river segments s as in table 1 h set of agencies h as in table 1 u set of load sources u as in table 1 g set of non overlapping bmps b that can all be applied to the same load source g u set of bmp groups g that may be applied to load source u b u set of all bmps b that can be applied to load source u decision variables x s h u b number of acres to which bmp b is applied on load source u for agency h within land river segment s parameters θ p target nutrient load lb for pollutant p α s h u total acres of load source u for agency h within land river segment s ac φ s h u p load density of load source u for agency h within land river segment s lb ac τ b per acre annual cost of bmp b ac η b p s u reduction effectiveness of bmp b when applied to load source u in land river segment s reduction 3 3 1 model minimize the annual cost of implementing bmps eq 8 s s h h u u b b u τ b x s h u b subject to the post treatment load not exceeding a target eq 9 s s h h u u φ s h u p α s h u g g u 1 b g η b p s u x s h u b α s h u θ p the implementation of bmps in each group not exceeding the available acres eq 10 b g x s h u b α s h u s s h h u u g g u eq 11 x s h u b 0 s s h h u u b b u note that the product g g u 1 b g η b p s u x s h u b α s h u in eq 9 represents the combined pass through factor θ s h u as the number of acres of implementation for a bmp increases the total amount of load that passes through to the bay is reduced optimization problem instances hereafter simply referred to as problem instances are formed by specifying all sets and parameters for the model problem instances were solved using an open source solver ipopt interior point optimizer which is a package designed for large scale nonlinear optimization and maintained by the coin or laboratory lougee heimer 2003 ipopt is applicable to problems with continuous decision variables and it utilizes a primal dual interior point algorithm with a filter line search method to provide fast convergence to local solutions wächter and biegler 2006 the ipopt algorithm solves a sequence of auxiliary barrier problems in which the original bound constraints are replaced by a logarithmic barrier term an optimization step is taken after checking convergence of both the original nonlinear problem and the auxiliary barrier problem and for each step a trial point is rejected or filtered if it does not sufficiently reduce the objective function or constraint violation the algorithm repeats until a solution or a point satisfying first order optimality conditions is found wächter 2009 3 4 optimization software implementation to facilitate repeated solving for problem instances of the above nonlinear programming model we created an open source optimization software subsystem named bay optimization tools for analysis bayota the current version performs four main functions i processing of cast source data ii generating problem instances for selected counties iii passing in memory instances to the solver and iv parsing solution outputs it is important to note that the optimization model replicates the simulation logic of cast but does not actually run cast nevertheless outputs of the optimization model were checked against cast outputs to ensure that the same cost and load reduction estimates were generated by both models for common sets of efficiency bmps 3 4 1 structure as shown in fig 2 the optimization subsystem is organized into four components namely the engine workspace storage and public layers bayota s core is the engine layer that contains four packages written in the programming language python version 3 python core team 2020 van rossum and drake 2009 to parse source data create problem instances and interact with the solver to accomplish these tasks the engine manipulates both data and optimization model the workspace layer is the practitioner s access point for interactions with the subsystem specifically the workspace is where a user sets up configuration files and execution specifications e g file paths model structure and temporary files the workspace also provides a natural location for post processing analysis or visualization code files in the workspace are typically manipulated in local storage either attached to a personal computer or a remote access node of a high performance computing environment but they are automatically uploaded to the cloud after execution begins and pulled into a running docker container which is a means of bundling software into minimal size and independently executable packages merkel 2014 the storage layer is responsible for holding a copy of source data and serves as a transfer location between the local workspace and running docker containers it also houses temporary log and solution files the public layer described further in section 4 consists of the web based visualization application further technical details of each layer are provided in appendix a problem instances are generated using open source optimization software instances in bayota are built from smaller components using pyomo an extensible python based open source optimization modeling language hart et al 2011 2017 when executed the software initially constructs a model skeleton that consists of the necessary sets parameters and variables other expressions or model components including objectives constraints and any desired diagnostic outputs are added to the skeleton as designated in the specification file problem instances are then solved using ipopt as previously described section 3 3 3 4 2 batch processing in the cloud populating the web based visualization and analytics tool section 4 entails solving a problem instance for each county in the chesapeake bay watershed for four objective constraint goals cost minimization with a nitrogen or phosphorus load target and nitrogen or phosphorus load minimization with a budget limit and 50 different levels of each constraint totaling 39 400 independent problem instances for computational efficiency multiple instances are solved in parallel see appendix b for additional details regarding execution sequence structure and pseudocode an early version of the engine used the slurm job manager on a local high performance computing hpc cluster as the cbpo transitioned its hpc environment to amazon web services aws optimization subsystem execution migrated to a slurm managed cloudformation cluster utilizing 8 nodes of 64 processing cores each problem instances are solved in the aws batch compute environment with job management handled by docker and aws elastic container registry ecr docker containers are run on aws elastic cloud computing ec2 spot instances that allow flexibility in cpu architecture and pricing both of which are determined by aws batch at run time to take advantage of otherwise unused ec2 capacity in the aws cloud maximum memory available to the containers is set to 4 086 mb results are passed to an aws simple storage service s3 bucket providing easy web server access with appropriate credentials 4 visualization tool 4 1 motivation in addition to the optimization model and its software implementation there was need for a visual interface that would allow ease of understanding and facilitate use of the tool directly by management teams who have less familiarity with the optimization details such an interface would also serve the important role of facilitating ongoing communication and receiving feedback necessary for effectively improving and maintaining the system for this reason user interface development was set and prioritized as an early project goal to explore the type of decision support interface needed and the ways managers or other users might operate such an interface we developed a pilot online visualization tool that went through several revisions based on feedback from the cbp workgroups and stakeholders the visualization interface for chesapeake optimization vico allows users to answer real world optimization questions across the problem domain of efficiency bmps for local governments throughout the watershed especially those that carry stormwater permits the web application provides interested stakeholders with the capability to explore cost load and bmp implementation portfolios in this manifestation the tool draws from data exhaustively precomputed i e optimizations were pre run for each watershed county across ranges of load reduction and cost target objectives of practical interest 4 2 graphical interface results are available for nitrogen n and phosphorous p cost minimization objectives for 50 independent and equally spaced percent load reduction constraints from 1 to 50 and with the objective and constraint functions swapped for load reduction maximization with annual cost constraints from 100 000 to 5 000 000 after selecting an optimization goal of interest by geography and objective a user can identify points of interest from the set of feasible solutions to see bmp implementations realizing that target load reduction or annual investment in the form of a bar chart in the optimization literature such a set of solutions is often referred to as a pareto frontier or non inferior set cohon 1978 each member of this set is efficient in that no other alternative exists that performs as well on each objective i e load reduction for the specified nutrient and cost while being strictly better in at least one objective the cost versus load or objective constraint space for an optimization instance is determined and displayed as the user steps through the interface first the user selects the geography and specifies an optimization objective and constraint combination from the set of four preloaded options fig 3 in accordance with the user selected objective and constraint fifty pre computed optimization solutions are presented as points on a curve showing potential tradeoffs between annual load reductions and total annual bmp costs fig 4 note that all data in this manuscript are provisional and subject to revision they are being provided for illustrative purposes only the data have not received final approval by the u s geological survey usgs and are provided on the condition that neither the usgs nor the u s government shall be held liable for any damages resulting from the authorized or unauthorized use of the data to evaluate the bmp portfolio and acreages of implementation for a given optimization solution the user can select an individual point representing a particular optimization solution and then a bar chart showing the associated implementation details is generated selection of two optimization solution points both on the same county or each on a different county generates a grouped bar chart for easy comparison of bmp strategies for the two solutions of interest fig 5 although parsed as comma separated value csv files for data manipulation and display purposes bmp portfolios for selected points can be downloaded from vico in the particular file tabular txt and organizational format needed for user input via the cast web interface this provides users not only a means of verification but also an option to choose an optimal solution and then incorporate additional modifications prior to further exploration with the regulatory model 4 3 software development and improvement with stakeholder engagement the optimization system was developed over a period of two years with active stakeholder engagement of cbp managers scientists and engineers from the seven jurisdictions of the cbp federal agencies and other groups project leadership shared the latest prototypes of the visualization and analytics tool on monthly to quarterly time intervals with stakeholders at cbp modeling workgroup meetings and conference calls throughout the iterative refinement design and implementation process at each workgroup review the development team reported progress to project managers and workgroup members at large via audiovisual presentations and oral follow up discussions progress updates included discussion of programmatic and high level technical hurdles encountered potential solutions and recommendations received feedback from participants in the workgroup as well as other cbp stakeholder groups such as the water quality goal implementation team played a critical role in steering the tool development a clearer shared vision of the tool emerged from the sharing of questions opinions needs and wishes for the long term project direction as part of the ongoing effort to meet high standards of stakeholder participation such as described by barnhart et al 2018 for example the need for interactivity in conjunction with the prototype development of the optimization model and software directly shaped the form and function of vico to gather more instantaneous feedback scripting code for the hotjar behavior analytics tool was embedded in the current beta release to learn from site usage including mouse click density and scrolling behavior and to streamline the collection of feedback on user experience during this development period vico has had two beta releases the first focused on providing access to the efficiency bmp optimization results computed using the optimization model and software framework described in section 3 and stored in aws s3 by county the second beta release building on the first and on user feedback about the first expanded capabilities for tmdl planners and managers to evaluate tradeoffs the second beta release added the capability to show two county optimization result curves simultaneously and to compare two data points with associated bmp realizations from the same county curve or from distinct county curves selection of two points yields a grouped bar chart for comparison of the kind and amount of each bmp implemented in the solution portfolios vico has been implemented as a web application in r shiny chang et al 2015 and hosted on an internal r shiny server in the cbp chesapeake center for collaborative computing c4 aws cloud infrastructure r was chosen as an effective open source language r core team 2020 in widespread use in the scientific community and the r package shiny was selected as an easy powerful flexible and extensible way to present r computations and visualizations on an interactive web site aws is the cloud infrastructure provider for cbp chosen partly for overall cost effectiveness and vico utilizes just a portion of the compute resources set up in c4 for overall shiny application support the approximately 39 400 precomputed optimization results are stored in aws s3 as csv solution files and txt downloadable files base s3 storage cost for the 6 7 gb of data is only a fraction of a dollar a month with additional modest rates and fees applying for transfers etc initial shiny server hosting has been piggybacked on existing hpc modeling resources using only a small portion of available resources on a c4 8xlarge node to conveniently meet and manage increasing requests for hosting of multiple shiny applications the cbp data center is shifting to dedicated shiny server support in the form of a small ec2 instance or comparable containerized offerings overall maintenance costs will include keeping host operating systems r r shiny environments and source code current across upgrades as with other software apps and supporting layers 5 results and discussion 5 1 solving performance optimization runs can be separated conceptually into two stages a set up stage for data parsing and problem instance generation which is described in section 3 4 1 and a run time solving stage additional detail on the sequence of these stages is provided in appendix b the overall expected computation time can be better understood by bounding the two stages parsing source data and generating the model in memory the instance generation stage takes time proportional to the number of variables in the problem instance for each county fig 6 for even the smallest counties data parsing and model generation took between 30 s and 1 min average model generation time was 95 s which increased by approximately 1 min for every additional 37 500 decision variables the variation in instance sizes is primarily a function of the number of land river segments and agencies i e proportional to the product of s and h but is also determined by the types of load sources and bmps applicable in a given geography half of the county instances were composed of fewer than 18 000 variables and 90 used fewer than 50 000 variables execution time for ipopt was benchmarked for each county and for three nitrogen reduction targets fig 7 average solve time was 19 s for a 20 load reduction target every 12 500 additional variables extended solve time by approximately 10 s for targets of 40 and 1 10 s increases corresponded to roughly 8 000 and 50 000 additional variables respectively 5 2 implementation plans and efficiency bmps 5 2 1 comparing optimized and unoptimized modeled costs to analyze potential benefits of the optimization tool to bmp planning we compared costs between two bmp implementation plans for each county 1 solutions of the optimization model which we refer to as optimization opt suggested plans 2 bmp implementation portfolios adapted from the state level restoration plans which we refer to as watershed implementation plan wip inspired plans to generate each wip inspired plan bmps were drawn from the corresponding state level wip currently on the third iterative phase phase iii as of this writing but restricted to only efficiency bmps to ensure that the same potential investments were possible and same set of variables included as in the optimization model additionally to ensure a fair comparison county level nitrogen reductions for the wip inspired plan were calculated on 2010 no bmp meaning all bmps removed rates and acres which are the base conditions used in the current version of the optimization subsystem therefore variables in the optimization model were initialized to be identical to the wip inspired plan and an equivalent nitrogen reduction amount from efficiency bmps was used as the reduction constraint when the optimization model was solved for lowest cost this cost was compared to the wip inspired implementation cost for those same bmps these comparisons were conducted for 20 counties that span the range and include each decile of number of variables in county level optimization problem instances potential cost improvements were identified across the range of experiments and the percentage reduction in cost for the optimization suggested plans was greater for more costly wip inspired plans fig 8 left panel as examples we describe here results for counties representing the 10th 50th and 90th percentiles based on the number of variables in each county s problem instance for the 10th percentile county the wip includes a total annual cost of 2 5 million for implementing efficiency bmps when evaluated on 2010 no bmp rates and acres these efficiency bmps provide a total county level nitrogen reduction of 12 2 when the optimization model was solved for lowest cost using the same nitrogen reduction as a constraint a solution was found that costs approximately 130 thousand i e approximately 95 lower cost for the 50th percentile county the wip includes a total of 8 9 million in annual efficiency bmp implementation costs for a 29 9 reduction in nitrogen load when the corresponding optimization problem instance was solved for lowest cost a solution was found with a cost of 2 6 million i e approximately 70 lower cost for the 90th percentile county where a 4 county level n reduction is achieved relative to no bmp using 2010 rates and acres the wip includes a total annual efficiency bmp implementation cost of 620 thousand dollars when the optimization model was solved for lowest cost using the 4 nitrogen reduction as a constraint a solution was found that costs 270 thousand i e approximately 67 lower cost however such cost improvements were not always achievable with efficiency bmps for example another county that is approximately the 10th percentile only presented a 26 relative cost improvement in the opt suggested implementation plan as compared to the wip inspired plan this was due in part to there being only a relatively small total cost of planned efficiency bmps in the wip inspired plan for the county differences in the overall cost per pound of nitrogen reduction between the two plans were similar to the differences in total cost fig 8 right panel opt suggested plans generally exhibited lower costs per pound of reduction with a median for the opt suggested plans 8 per pound 77 lower than for the wip inspired plans 36 per pound moreover the maximum overall cost per pound across tested counties for the opt suggested plans 212 per pound which was more than double the next highest value was 62 lower than for the wip inspired plans 563 per pound in fact 25 of the wip inspired plans had higher overall costs per pound than the highest among opt suggested plans 5 2 2 limitations and opportunities of comparing cost the above comparison of opt suggested and wip inspired plans suggests the potential for substantial cost savings in the implementation of efficiency bmps however the comparison must be accompanied by multiple important caveats which also point toward future research directions these caveats are briefly described in the following paragraphs and include the land condition cost averaging nutrient selection unmodeled considerations and the bmp subset load reductions were calculated using 2010 no bmp also called no action loading rates and land use acres therefore the available acres do not reflect either current conditions or conditions after any changes as a result of development or other human modifications and it is inaccurate to project such load reductions to 2025 land uses as built into the wip inspired plans we anticipate that modifications to the model will be required to account for existing bmp implementations and varying land use acres the comparison was conducted with watershed wide average costs which serve as a useful approximation but do not reflect the potential variation in per unit implementation costs across and even within jurisdictions in addition the comparison does not consider transactions costs i e the costs of identifying the location of applicable practices including running cast contracting with agents to install the practices and enforcement to ensure that practices are carried out as contracted however given that the opt suggested plan involves fewer practices it is possible that overall costs would be reduced further if transactions costs were factored into the comparison carpentier et al 1998 nevertheless with modifications to input files the framework developed here is capable of utilizing jurisdiction specific cost information such opt suggested plans could include bmps that a decision maker would want to exclude or conversely could exclude bmps that a decision maker would want to include despite their costliness and for unmodeled reasons to address this we could apply user defined bmp implementation constraints but such decisions would be specific to each jurisdiction more generally accommodating flexible user specified constraints in the optimization tool interface would help with exploration of alternative strategies and incorporation of other desired bmp portfolio attributes that are difficult to quantify such as unmodeled preferences or uncertainties this would likely require adding interactive techniques to the optimization procedure these techniques originated from early work by brill 1979 willis and perlack 1980 and schilling et al 1982 on hard to quantify objectives and are currently being incorporated into modern methods such as human guided or interactive multi objective optimization imo as described by babbar sebens et al 2015 and xin et al 2018 load reduction constraints were set according to nitrogen only phosphorus and sediment reductions or targets that are embedded in the wip inspired plan were not considered in the opt suggested plan also exploration of the composition of bmps and locations of implementation in each plan is beyond the scope of this analysis nevertheless the optimization results do specify bmp implementation at the scale of land river segments as seen in the web application section 4 and so we anticipate that the framework described here will be of value to such future investigations it is worth noting again that this comparison included only the so called efficiency bmps therefore many non efficiency bmps such as forest buffers and land retirement that are also effective and widely considered for implementation by decision makers were not considered this limitation is discussed further in the next section section 5 3 5 3 expanding optimization beyond a bmp subset challenges future development and feedback the time averaged watershed simulation model cast has improved the modeling suite s usefulness for supporting decision makers section 2 1 and facilitated both the development of our optimization framework and its specific application to the efficiency bmp subset section 3 nevertheless the structure of calculations in cast section 2 2 presents several challenges for creating an integrated all encompassing optimization system the modeled effect of bmps on the delivered nutrient and sediment loads in cast is nonlinear and there are many potential local optima the decision variables are also non separable roughly speaking the effects of bmps from different categories table 3 are dependent on one another making it difficult to apply divide and conquer or decomposition optimization techniques moreover the decision space is high dimensional for a single county instance the input space will typically consist of variables numbering from tens of thousands to a million this paper has characterized the cast optimization problem and described the development of an optimization framework and its application to a well defined subproblem i e identifying optimal solutions over the efficiency bmp subset within cast section 3 the decision to first address only this specific bmp category was motivated by several aspects of the problem first in these initial years of investigation the lack of an application programming interface api for cast made it difficult to test methods that require many iterative function evaluations second based on the structure of the problem it is believed that an efficient method for solving the selected subproblem i e the optimization problem for the efficiency bmp subset will be an important component of a more comprehensive optimization algorithm third tackling this simplified problem gave valuable insight that can ultimately be applied to the higher dimensionality and intricateness of the complete load function eq 5 moreover although good solutions were consistently found to instances of this nonlinear model ipopt does not guarantee global optimality and so additional computational advantage may be gained through further study of the model s mathematical characteristics and exploration of effective approximations boddiford et al 2021 including all of the bmps should be a primary focus of future efforts to build upon the work described here such work is essential to obtain broader applicability of the optimization tool in supporting targeted and effective management decisions because many of the yet to be modeled bmps could be critical wip elements for many jurisdictions for example land conversion bmps such as forest or grass buffers can reduce nutrient delivery substantially and would add considerable utility to the cbp optimization tools however the formulation of a mathematical programming model including land conversion bmps presents serious challenges because such a formulation must simultaneously account for the change in available acres the implicit treatment or efficiency value applied to upland acres and the acreage constraints on that upland treatment approaches that forego explicit modeling of the bmp effects will likely be required to address these and similar challenges presented by the non efficiency bmps such approaches which could still use an explicit model as a part or complement to other methods may include model reduction techniques such as surrogate modeling e g sun et al 2015 jia and culver 2006 and meta heuristics such as simulated annealing and evolutionary based algorithms e g marco et al 2019 yoon et al 2019 further expansion of optimization capabilities for cast across all bmp input types and rule sets is underway inclusion of all bmps in the optimization is an ultimate goal along with an interface that will ideally launch the optimization engine on custom queries and collect organize and present query results when done in the more distant future the cast model and optimization tool should be expanded to also include values for certain bmp co benefits at local to regional scales such as flood avoidance stream ecology or carbon sequestration while the necessary mathematical algorithmic and software groundwork to identify and apply an appropriate optimization procedure to the fully general case with all bmps is underway software developers have been concurrently porting and recoding cast to provide an implementation that will execute faster it is hoped that reducing computation time for each call of the cast function on a set of bmp inputs will enable a closer coupling between the watershed and optimization models until full programmatic integration with an accelerated version of cast is possible development of bayota has settled practical detail laden front end issues of getting required data to an optimization engine and allowed solution of subsets of the full problem including the useful category of efficiency bmps such solutions have practical value to county level managers and interested stakeholders and can be both illustrative and instructive preliminary feedback has been largely positive regarding the web application interface section 4 with most users expressing no difficulties when using it cost efficient solutions found by the tool such as presented in section 5 were discussed with stakeholders who found the results encouraging and despite current limitations many of which have been characterized by stakeholders as places for improvement there is nonetheless continued interest for using solutions provided by this optimization for steering development of future management plans user feedback has also included requests for the ability to explore solutions for larger geographic areas and for the optimization to be solved in real time so that a user can change constraints and parameters as they explore optimization solutions for example if a user wants an optimized bmp implementation for multiple counties one cannot be assured that combining solutions of each separately solved county level instance would yield an optimal multi county implementation portfolio instead a multi county instance would need to be generated and solved the reason is that in some cases one would find tradeoffs between counties such that a more cost effective solution included greater investment in one county and less in another such a multi county instance would require a target load that encompasses the delivered load from multiple counties and one would need to optimize for the complete set of land river segments from all the counties simultaneously the aforementioned technical challenges and user feedback including the opportunities for improvement discussed in section 5 2 2 and tackling of the fully general case with non efficiency bmps have made clear the importance of building further improvements moreover as stressors on the chesapeake bay from local watershed changes grow due to climate change ni et al 2019 du et al 2018 irby et al 2018 the need for refining and optimizing management plans will likely increase going forward giuffria et al 2017 bosch et al 2018 therefore further development on the multiple challenges discussed here is critical to continue supporting an effective adaptive management strategy 6 summary and conclusions the chesapeake bay watershed is a large area with many stakeholders and jurisdictions committed to achieving environmental improvement goals a suite of modeling tools is used regularly to track management progress and to consider improved management plans there is urgent need for further development of the suite to enable rapid and assured identification of cost efficient strategies the low cost high return management practices have already been implemented in the three decades of the chesapeake bay program restoration effort yet challenges like climate change require continued nutrient reductions to maintain the water quality standards and ecological health of the bay for cost effective and environmentally protective strategies to restore the chesapeake we need efficient plans a framework has been developed for identifying low cost implementation options for a distinct bmp subset that is for efficiency bmps that are represented by scalar load reduction effectiveness factors in the most recent phase 6 cbp watershed model known as cast the optimization model software and web based visualization tool provide new utility for cast to assist stakeholders in identifying cost effective strategies for a variety of management relevant decisions and they address recommendations of cbp s stac importantly the framework uses a problem formulation that can be consistently applied for all counties across the entire chesapeake bay watershed and uses the same source data as the cast simulation thus guaranteeing that optimization results will provide equivalent load reductions when applied through cast this combined simulation and optimization approach will continue to be essential as modeling capabilities are expanded results from a comparison of optimization suggested bmp implementations with wip inspired plans which were adapted from the state level wips and retain only efficiency bmps demonstrate the framework s potential opportunities for finding cost saving ideas or adjustments to jurisdiction implementation strategies however characteristics of the existing tool limit its ability to answer certain strategic implementation questions for the simulation model including for non efficiency bmps i e those not represented by scalar reduction factors which must be addressed by future development the comparison of efficiency bmps is nevertheless useful in its own right and perhaps more importantly it may be possible to utilize this efficiency bmp model as a subproblem in an optimization approach expanded to include non efficiency bmps as well importantly testing of the tool with managers has revealed that performance of the optimization model in generating solutions is fast enough to be practical and feedback from users to date has been positive and encouraging the stakeholder driven approach in developing this modeling and optimization framework continues to improve the suite of tools that support restoration efforts and planning in this large multi jurisdictional watershed watershed management challenges similar to those for chesapeake bay are also found in many other regions both across the united states as well as internationally and the general conceptual approach described here is transferrable to other watersheds however direct applicability of the methods and tools to other areas is dependent on the presence of a management structure and existence of models that are amenable to optimization as evidenced by the partial history provided in section 2 and by published studies and reviews e g boesch 2019 alamanos et al 2021 the amount of time and resources expended on development of the management structure modeling and monitoring resources that are the foundation for these optimization capabilities is quite substantial this provides good rationale for the use of simpler time averaged stakeholder driven models such as cast software availability a freely and permanently available snapshot of bayota bay optimization tools for analysis source code can be accessed by its doi address via zenodo and an up to date version is available via github https doi org 10 5281 zenodo 4609255 kaufman 2021 https github com dkauf42 bayota the vico visualization interface for chesapeake optimization application can be freely accessed and explored in a web browser and its source code is available via github https shiny apps chesapeakebay net vico https github com kwasplen vico the chesapeake bay program s phase 6 watershed model cast and its documentation can be accessed free of charge in a web browser https cast chesapeakebay net declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this material is based upon work funded wholly or in part by the united states environmental protection agency usepa chesapeake bay program office including direct salary support for multiple partners within the usepa administered chesapeake bay program including co authors shenk and linker as well as numerous assisting support staff assistance agreements cb96350501 to chesapeake research consortium crc inc co authors kaufman ball bosch ellis hobbs van houtven and mcgarity cb96325901 and cb96365601 to the university of maryland center for environmental science co author asplen and cb96351401 to pennsylvania state university co author bhatt the contents of this document do not necessarily reflect the views and policies of the environmental protection agency any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government the authors thank rich batiuk and the modeling team at the chesapeake bay program office cuiyin wu andrew sommerlot richard tian isabella bertani for constructive input throughout the project the authors thank and acknowledge stuart schwartz of the university of maryland baltimore county who was an original member of crc s advisory and support committee and provided important early contributions of both conceptual and practical nature many thanks to john massey for technical support with cloud resources and batch processing and megan mcneilly for user experience improvements to the online decision support tool further thanks to brian burch megan thynge tim paris martin koslof and the entire cbpo software development and information technology team a optimization software layers the optimization engine layer is maintained as a git a software version control system repository and run in a docker container it contains four python module collections 1 castjeeves modules access query and parse source data from cast 2 bayom e is devoted to solving optimization problems involving cast efficiency bmps 3 bayota settings is focused on configuring directory paths output graphics and logging and contains sample configuration files 4 bayota util provides useful functionality not intrinsic to the other sub package roles fig 9 application structure of bayota including the optimization engine blue rectangle user specific workspace green rectangle and storage in s3 orange rectangles fig 9 in addition to these the repository contains execution scripts for handling the logical flow of parallel optimization runs fig 9 provides a more detailed look at the optimization engine and its interactions with the other bayota components the workspace layer is the primary handler of inputs and outputs the minimal necessary workspace consists of subdirectories for configuration files data and specification files configuration files set up file paths and log formats and a subdirectory provides temporary space for cast data specification files allow for selection of geographies model objectives and constraints model modifications and various options appendix b other workspace subdirectories may be generated for control logs temporary files output and notebooks some of which are created automatically during optimization runs in the storage layer the current version of bayota uses a copy of relevant data from the cast source database to update the copy the user executes a script to pull the latest source database tabular files into the local workspace and then into s3 during a run the stored source data in s3 are automatically pulled into the workspace of the running docker container the castjeeves package pulls data into memory to be serialized and written to disk for quick loading across repeated executions and queries the data in memory when generating a problem instance the bayom e package uses castjeeves to access data required for each part of the optimization model data are loaded in this sequence pollutants geographic delineations bmps load sources agencies associations between bmps and load sources bmp costs bmp effectiveness factors base loading rates and available acres b batch processing setup and sequence run processing is organized into studies experiments and trials that correspond to different characteristics of a problem instance that must be specified fig 10 studies refer to the geography and model structure used experiments refer to modifications of the model such as adding or removing constraints trials refer to single value changes e g increasing the value of the load target or decreasing the budget fig 10 schematic example left illustrating the categorization of problem instance types i e study experiment trial which map onto the software processing flow right used when solving multiple problem instances fig 10 optimization runs are set up by modifying three types of specification files batch model and e t a batch file is the top level file for controlling runs and contains options for geography the names of other specification files to use and options such as whether to move and translate solution files upon run completion a model file contains options for setting up objectives constraints and additional parameter names an e t file allows specification of model modifications for particular experiments and or setting of different values for constraints termed trials the run script reads the specification files to determine the structure and number of problem instances to be generated execution sequence pseudocode image 3 
25757,extensive efforts to adaptively manage nutrient pollution rely on chesapeake bay program s phase 6 watershed model called chesapeake assessment scenario tool cast which helps decision makers plan and track implementation of best management practices bmps we describe mathematical characteristics of cast and develop a constrained nonlinear bmp subset model software and visualization framework this represents the first publicly available optimization framework for exploring least cost strategies of pollutant load control for the united states largest estuary the optimization identifies implementation options for a bmp subset modeled with load reduction effectiveness factors and the web interface facilitates interactive exploration of 30 000 solutions organized by objective nutrient control level and for 200 counties we assess framework performance and demonstrate modeled cost improvements when comparing optimization suggested proposals with proposals inspired by jurisdiction plans stakeholder feedback highlights the framework s current utility for investigating cost effective tradeoffs and its usefulness as a foundation for future analysis of restoration strategies graphical abstract image 1 keywords water quality stakeholder participation decision support system web application chesapeake bay watershed model watershed management optimization 1 introduction chesapeake bay the largest estuary in the united states is an important focus for restoration and protection efforts due to the past and ongoing threats to its ecological health that are of tremendous public interest as well as its historic social environmental and economic value since at least the 1960s and up to the present recurring issues of poor water quality during the summer have included hypoxic conditions of varying severity extent and duration officer et al 1984 hagy et al 2004 kemp et al 1992 2005 murphy et al 2011 bever et al 2013 numerous studies have shown that low dissolved oxygen and other related water quality issues are largely a result of excess nutrients and sediments gillelan et al 1983 kemp et al 2004 2005 boynton and kemp 2008 li et al 2016 and these excess pollutants are coming from both point and nonpoint sources throughout the watershed boynton et al 1995 da et al 2018 ator et al 2019 under the clean water act of 2004 the chesapeake bay is designated as an impaired water body and since 2010 the chesapeake bay total maximum daily load tmdl and its limits on nutrients and sediment pollution have helped guide restoration efforts usepa 2010a b the 2010 chesapeake tmdl is the largest most complex tmdl in the country covering a 166 000 km2 area across the mid atlantic region the tmdl allocates or places limits on loadings of nitrogen phosphorus and sediment sources in the watershed to remove impairments for aquatic life in the tidal tributaries and embayments of the chesapeake linker et al 2013b the tmdl allocations for nutrients and sediment are the basis of comprehensive restoration plans called watershed implementation plans wips which also include rigorous accountability measures to restore water quality and living resources in the chesapeake bay and its watershed usepa 2021 restoration efforts involve many stakeholders including six states the federal government the district of columbia 1 800 local governments and almost 200 county level jurisdictions other stakeholders include commercial entities academic institutions and non governmental organizations ngos coordination for restoration efforts has been led for several decades by the chesapeake bay program cbp partnership a formally organized collaborative effort that is administered by the chesapeake bay program office cbpo of the u s environmental protection agency usepa in close coordination with other federal state local and ngo partners to meet agreed upon goals for a healthy bay and to address continued restoration challenges such efforts must be both sustained and improved harding et al 2016 2019 kleinman et al 2019 zhang et al 2018 the cbp makes extensive use of models to support decision making for watershed management paolisso et al 2015 these models including the current version phase 6 of the chesapeake bay program s watershed model which is a focus of this paper are maintained by the cbpo and other federal partners in a process overseen by a modeling workgroup which includes representatives from multiple partners the models are used to help plan management actions that can reduce excess nutrient and sediment loads to chesapeake bay these tools also aid the usepa and the seven chesapeake bay jurisdictions in tracking progress towards agreed upon implementation goals for improved water quality such as the tmdl wips as well as other environmental related goals ongoing development and refinements of the suite of modeling tools in conjunction with substantial monitoring efforts has provided cbp and its partners with ever improving lines of evidence with which to evaluate restoration status and guide new restoration efforts although the suite of cbp modeling tools simulates the effect of management actions it has not included an automated system for analyzing and searching among the large range of possible management scenarios that can be simulated and realized because of this lack of automation tools use of the existing modeling suite can be time consuming and difficult for planning purposes in particular there is no automated way to identify the least cost portfolio of management action alternatives that could achieve a target level of nitrogen reduction although an experienced user of the model can find satisfactory scenarios there is often room for improvement and other users sometimes go through many iterations of manual trial and error with educated guessing to succeed such difficulties primarily arise because of the large number of decision variables i e combinations of management action types their placements and their amounts and the nonlinear cascading effects of simulated actions an optimization engine that can algorithmically search the decision space and present cost effective implementation strategies would enhance the utility of the cbp modeling tools increase ease of use and enable interested partners to focus on making more effective policy decisions critical to restoring the bay the cbp partnership has recognized the need for expanding the capabilities of the modeling framework to assist users in planning a science based suite of cost effective best management practices for development and implementation in particular and as further described subsequently in this paper stakeholders have specifically requested and recommended the development of better tools for identifying optimal portfolios of best management practices bmps which are cbp reviewed and approved practices for reducing nitrogen phosphorus and sediment loads to the chesapeake bay cbp 2018 implementation of certain bmps can require substantial investments for construction or appropriate adoption modification and maintenance of these practices such practices are perhaps better referred to as beneficial management practices with the same initialism bmp in recognition of the facts that the effectiveness of practices depends upon multiple factors and that which practice is best will vary with place and time as such any strategy that seeks to optimize investment decisions regarding bmps should consider numerous factors including but not limited to the financial investment effectiveness and feasibility of implementing a practice for a given locale previous simulations and analyses of bmp strategies for the chesapeake bay watershed have demonstrated the potential for substantial cost savings through better optimization of implementation strategies several such efforts have used data from a previous version phase 5 3 2 of the chesapeake bay program s watershed model for example analyses of alternative strategies for achieving tmdl goals including both bmps and wastewater treatment upgrades as options usepa 2012 van houtven 2012 have demonstrated that more widespread use of bmps in particular green infrastructure approaches are often cost beneficial compared to more traditional grey infrastructure approaches an extensive watershed wide analysis by kaufman et al 2014 ordered bmps by marginal abatement cost and found several cost savings through careful selection and spatial targeting of bmps for nitrogen phosphorus and sediment reduction optimization efforts at smaller scales have shown similarly encouraging results collaborative examination of management tradeoffs was enabled by optimizing bmp implementation using an early version phase ii of the chesapeake bay program s watershed model for the interstate region of the potomac river basin results highlighted decision making challenges that arise from numerous and complex management alternatives schwartz 2010 xu et al 2019 found cost reductions by targeting bmp placement in the mahantango watershed of pennsylvania using a soil water assessment tool swat model for historic conditions as well as under conditions associated with projections of climate change the usepa also developed the watershed management optimization support tool wmost a spreadsheet based decision support tool which has been used in a case study for cabin john creek watershed in montgomery county md i e within the chesapeake bay watershed to assess bmps with hydrology time series inputs detenbeck et al 2018 prior efforts have demonstrated the utility and value added by an optimization framework in aiding and improving nutrient and sediment reduction decisions however they do not sufficiently satisfy cbp requirements because they were limited to subsets of the chesapeake bay watershed geography or considered independently derived combinations of land use and bmps that were not approved by the cbp partnership they also do not interface directly with the model software that is used by the cbp partnership in other words previous efforts have not been sufficiently tailored or developed specifically for the unique challenge of providing guidance entirely consistent with the cbp approved data methods and model applied homogeneously at the scale of the entire chesapeake bay watershed the focus of this paper is the development integration and implementation of a linked modeling and optimization framework for planning and appraising cost effective strategies for implementing bmps the approach helps address management relevant questions regarding cost effective bmp implementation in this large multi jurisdictional watershed and as such was developed through a stakeholder driven approach to meet the needs of the cbp partnership our particular contributions are development and documentation of i a simplified model structure necessary for optimization ii a new optimization approach for selecting among a subset of cbp recognized bmps and iii new software for exploring solutions notably these use the same data as the cbp simulation system and are designed as a direct response to the need recognized by stakeholders and recommendations of the cbp scientific and technical advisory committee stac specifically this paper describes strategies for simplifying a watershed simulation structure that are now based on multiple models and lines of evidence section 2 the specifications requirements and implementation of an efficient optimization framework for large scale planning problems with a subset of cbp recognized bmps section 3 and the visualization and analytical capabilities that were developed to deliver optimal county level solutions to stakeholders involved in drafting of watershed implementation plans or wips section 4 finally we provide a discussion of the overall performance of the framework with consideration of its limitations section 5 and offer some concluding remarks section 6 2 watershed modeling framework in this section we provide a general description of the cbp modeling framework structure of the management model incorporation of various load sources and simulation of the impacts of bmps 2 1 motivation and requirements the cbp modeling suite has undergone multiple phases of development and improvement since its inception in the 1980s the watershed and estuary models are cornerstones of the suite the watershed was historically simulated using a semi lumped parameter model based on the widely used hydrological simulation program fortran hspf to estimate nitrogen phosphorus and sediment loadings shenk et al 2012 improvements over time have facilitated large scale efforts of calibration and scenario analysis efforts the estuarine model has also undergone numerous improvements in resolution and representation of processes during this period cerco and cole 1993 1994 cerco 1995 2000 cerco et al 2002 2010 2013 cerco and noel 2004 cerco and noel 2013 although the conventional method for evaluating management scenarios utilizes a time dynamic watershed simulation this approach presents several challenges for a large regional scale model such as for the chesapeake bay watershed first the time dynamic model is computationally intensive and takes a long time to simulate critical water quality processes for such a large watershed second the pre and post processing of modeling files become cumbersome and pose a barrier to efficient use third difficulties in setting up and running model scenarios contribute to stakeholders requesting results from practitioners with the expectation that the models will be run for them lastly when stakeholders do not run models directly they may be less inclined to trust results and more likely to question whether runs were performed correctly early efforts to overcome these challenges with use of the watershed simulation resulted in the development of various auxiliary tools two such tools proved particularly useful and represented significant advances in guiding management decisions and garnering stakeholder interest scenario builder usepa 2010c was a database tool that provided an automated method for rapidly creating scenarios with all the necessary inputs for later evaluation in the watershed model simulation the maryland assessment scenario tool mast developed by j7 llc the interstate commission on the potomac river basin and the maryland department of the environment was an online load estimator that provided results similar but not identical to those of the watershed model the input to mast could be emailed by state agencies to the cbpo for input to the scenario builder database then run through the time dynamic watershed simulation scenario builder and mast addressed challenges inherent in pre processing and allowed stakeholders to directly orchestrate drafting of management scenarios using estimated loads however the challenges of extended run time and inability for stakeholders themselves to execute the full watershed model approved by the partnership remained for these reasons it became clear that a model version that managers and other interested stakeholders could run directly would vastly accelerate simulation and analysis timelines reduce barriers to its application and provide greater transparency to provide such a model the cbp developed a time averaged model with a simplified structure to serve as the primary management tool for evaluating the effectiveness of alternative bmp implementation strategies its development required leveraging the participatory strengths of the partnership including stac workshops workgroups discussions and cross group coordination the result is a comprehensive data heavy time averaged watershed model that utilizes information from many sources to produce cbp partnership approved estimates of nutrient and sediment loads throughout the entire chesapeake bay watershed this current version phase 6 of the chesapeake bay watershed model also known as the chesapeake assessment scenario tool cast provides managers and other interested stakeholders a means to develop and evaluate bmp implementation scenarios themselves each scenario represents annually averaged hydrologic conditions for the user selected baseline year each year represents unique historic or projected land uses wastewater treatments and for proposed levels of bmps for both point and nonpoint sources these in turn change the estimated load in essence cast represents an improvement of mast expanded to the entire chesapeake bay watershed and it has transitioned from what was originally an emulator model of a time dynamic management model to what is now the management model itself 2 2 watershed model structure and simplifications by design cast simplifies the structure of the time dynamic watershed model the current version consists of a relational database a user interface a web server and a scenario processor stored in the database the source data includes most of the static information used for model runs such as bmp definitions methods for estimating bmp effects and geographical specifications the scenario processor executes the logical and mathematical operations starting from user specified scenario inputs and the source data and going through numerous validations to calculate loads delivered to the bay because the focus in this manuscript is on the calculations the cast user interface and web server are not described here and readers are suggested to refer to chesapeake bay program 2020 for additional details an input scenario to cast is at a minimum a choice of year geography bmp options and costs the choice of year provides baseline inputs for land use and wastewater for a given scenario the system performs a stepwise series of calculations to evaluate nutrient and sediment loads delivered to the bay the cast documentation contains many details on the computation and rationalization of inputs cbp 2020 however it does not provide the entirety of the numerical formulation of cast in equation form such a description provided as one or a set of equations detailing the load function i e how the inputs and watershed processes are modeled to estimate load delivered to the bay is indispensable for formulating an efficient optimization model in the remainder of this section we provide a mathematical characterization of the load function see tables 1 and 2 for notation used that is consistent with the cast documentation and we briefly describe the information used to determine values for each term of the load function the descriptions provided here are intended to help the reader understand the general structure of calculations in cast and to provide a representation amenable to optimization 2 2 1 load function the finest scale of spatial segmentation in cast is the land river segment which corresponds to the intersection of a land segment and a river segment land segments primarily represent 197 chesapeake bay counties and river segments represent sub watersheds associated with river reaches cbp 2020 section 11 more specifically the land river segmentation partitions the entire watershed into 1 925 mutually exclusive spatial units representing the intersections of political and watershed geographies a load source is a type of land land use or other artificial construct that is modeled as providing nutrient or sediment loads with potential for delivery of these loads to the bay any selected geographic region is comprised of a set of land river segments s a subset of s for a given baseline year and nutrient nitrogen or phosphorus the total load delivered to the bay from a selected region l s is primarily a sum of contributions from non direct and direct load sources with modification by a stream bed and bank factor ζ s h s t b and shoreline contributions eq 1 l s s s h h n o n d i r e c t l o a d s s h ζ s h s t b d i r e c t l o a d s s s h o r e l i n e l o a d where notations are defined in tables 1 and 2 the set of agencies h differentiates among selected federal government landholders and non federal landholders which include privately held as well as state and local government held land the agency designation allows for accounting of land use management responsibilities and does not affect bmp effectiveness next we expand the three high level terms n o n d i r e c t l o a d s s h d i r e c t l o a d s s and s h o r e l i n e l o a d to reveal more of their structure and meaning in the context of model formulation load density i e nutrient or sediment load per acre from non direct sources u u n is a modified sum of inputs it represents an accounting of changes in load with inputs applied to land for a scenario specifically the difference between input load density for a given scenario i u t k s and the watershed wide long term average input i u t is modified by a sensitivity factor σ u t which quantifies marginal change in runoff with changes in input these weighted differences are summed across all input types and added to a watershed averaged load density for each load source l u w the resulting load densities in lb ac are multiplied by the number of land acres α s h u to get load in lb which is attenuated by efficiency bmps θ s h u see section 3 1 1 and by delivery factors ψ s h u representing load attenuation during transport to the bay eq 2 n o n d i r e c t l o a d s h u u n l u w t t i u t k s i u t σ u t θ s h u α s h u ψ ˆ s h u where ψ ˆ s h u represents a cumulative delivery factor of processing taking place on land streams and rivers ψ ˆ s h u ψ s h u l w ψ s h u s r ψ s h u r b transporting load to the bay note that load densities for each input type i u t k s are calculated at the county k or land segment not land river segment scale which is the scale where inputs are estimated using partnership approved data and methods load densities for each involved land river segment within a county are assigned that same value fig 1 shows a schematic representation of eq 2 for county loads delivered to the bay direct load sources u u d include wastewater point sources septic and rapid infiltration basins nutrient delivery from these sources is generally calculated as follows eq 3 d i r e c t l o a d s u u d l s u ψ ˆ s u where ψ ˆ s u represents a cumulative delivery factor ψ ˆ s u ψ s u l w ψ s u s r ψ s u r b transporting load l s u in lb to the bay loads from the shoreline are added separately from other source terms although shoreline is often considered an additional direct source because it does not have an associated input sensitivity σ u t eq 4 s h o r e l i n e l o a d s s h h l s h u s h o r e l i n e substituting the term expansions of equations 2 4 in eq 1 yields eq 5 l s s s h h u u n l u w t t i u t k s i u t σ u t θ s h u α s h u ψ ˆ s h u ζ s h s t b u u d l s u ψ s u s s h h l s h s h o r e l i n e where ψ ˆ s h u represents a cumulative delivery factor ψ ˆ s h u ψ s h u l w ψ s h u s r ψ s h u r b ψ s u u d represents the delivery factor for a specific direct load source and other notation is defined in tables 1 and 2 2 2 2 watershed model terms multiple lines of evidence values for the terms in the above eq 5 are specified using a combination of multiple models or lines of evidence that could include other simulation models regression models or census data for instance the l u w term represents the watershed wide denoted by superscript w average of loading for load source u the values for this term are estimated by a combination of multi model averaging literature review and large scale mass balance as described in cbp 2020 section 2 the term i u t which represents average input load of type t for load source u is determined through a cbp partnership process using the best available data for manure generation and transport fertilizer sales estimates of soil p storage in agricultural lands and various data model hybrids including for example the community multiscale air quality cmaq model and annual phosphorus loss estimator aple model cbp 2020 section 3 4 linker et al 2013a vadas et al 2009 the term ζ s h s t b which represents the load contribution factor from stream beds and banks is determined from observational floodplain studies cbp 2020 section 8 the σ u t term represents the sensitivity of load source u to loads from input type t and this term is determined by investigations using process based watershed models cbp 2020 section 4 the α s h u term representing the acres of load source u in agency h and land river segment s is determined primarily by a usgs land use model cbp 2020 section 5 the delivery factor terms ψ s h u l w ψ s h u s r ψ s h u r b and ψ s u representing fractions of material transported from land to water stream to river river to bay and directly from point sources respectively are determined by using a sparrow model preston and brakebill 1999 ator et al 2011 and a dynamic watershed simulation based on hspf cbp 2020 section 10 the i u t k s θ s h u α s h u l s u and l s h s h o terms are determined in part by partnership approved bmp parameters and the bmp implementation amounts specified for a scenario at the time of this writing cast simulates approximately three hundred bmps cbp 2020 and calculates their load reduction effects according to a specific set of groupings and corresponding rules although the bmps represented in cast can be categorized in a variety of ways cbp 2018 table 3 shows a grouping based on the way load reductions are calculated in cast section 3 provides additional details on these calculations with a focus on the first grouping efficiency bmps which are so called because of their assigned scalar load reduction effectiveness or efficiency values see section 3 1 1 for more details these bmp effectiveness values vary depending on their location within the watershed based on hydrogeomorphology 2 2 3 unit bmp costs in addition to load and load reduction estimates for bmps cast contains summary level bmp annualized cost estimates based on evidence from existing studies for the efficiency bmps these estimates are expressed as average unit i e per acre costs to correspond with the load and load reduction estimates which are expressed on an annual time step these bmp unit cost estimates are also expressed as annual values that is one time per acre capital and installation costs are amortized over the expected lifespan for each bmp using a 5 percent assumed discount rate and added to the per acre annual operation and maintenance costs for each bmp 3 optimization framework 3 1 motivation as previously noted the optimization effort described here has been largely the result of requests and recommendations from stakeholders within the cbp partnership and particularly of a growing demand from users for an expansion of the modeling framework s capabilities to assist users with finding more optimally cost effective portfolios of bmps in 2016 the cbp s scientific and technical advisory committee stac held a workshop of regional planning managers technical experts and other stakeholders in chesapeake bay watershed modeling and management to discuss the possibility for optimization methods to be applied and used as part of cbp s modeling suite stac recommended to cbp that a bay optimization system be created to enhance the existing suite of decision support tools davis martin et al 2017 the first key objective recognized by the workshop participants was to provide a means of minimizing total costs by identifying optimal portfolios of bmps and of doing so in a way that could be easily understood and implemented by the managers stakeholders and users in the various jurisdictions in the bay watershed in response to this workshop the cbp organized an optimization tool development project involving members of the cbp s modeling team and a group of researchers and scientists managed through the chesapeake research consortium see acknowledgements for details in this section and the next we describe our expansion of the regulatory modeling framework to include a new optimization model and decision support tool these tools have been designed for exploration and identification of cost effective load reduction strategies within the cast simulation space for counties throughout the entire regional scale chesapeake bay watershed an overarching goal for this new optimization framework is to identify and provide stakeholders with least cost bmp options for achieving given load reduction targets in the cbp time averaged watershed model based upon its structure as described above in section 2 numerically this has been represented as a constrained optimization problem with a cost objective subject to nutrient load constraints the following subsections describe the watershed data optimization model and implementation of the optimization approach 3 2 watershed data to solve an optimization problem instance several pieces of data are retrieved from the cast source database and parsed into useable formats for the optimization software the data required for optimization include bmps nutrient loads and geographic delineations additional details simplifications assumptions and scales applied to these data for the optimization model are described in the following subsections 3 2 1 bmps and the efficiency bmp subset as shown in table 3 the largest group of cast bmps are known as efficiency bmps for which examples include cover crops stormwater ponds and conservation tillage efficiency bmps are so called because their individual effects on load are determined in cast by an assigned scalar load reduction effectiveness or efficiency value this group of approximately 190 bmps was selected first for inclusion in the optimization model because it represents a substantial portion approximately two thirds of the total number of bmps in cast table 3 and it is a subset whose effects can be calculated separately from the other non efficiency bmps to create an independent optimization model the effect of efficiency bmps is calculated in isolation from the non efficiency bmps i e assuming a static implementation for non efficiency bmps that is not altered by the optimization framework as such a number of non efficiency bmp types that are both effective and widely considered for implementation by decision makers are not yet being evaluated in the optimization system and are outside the scope of this study see discussion in section 5 3 the effects of efficiency bmps in cast are determined by assigned scalar effectiveness values each of which has been defined by cbp expert panels that consist of regional as well as national scientists researchers and interested stakeholders cbp 2010 devereux and rigelman 2014 as an example in the absence of other bmps an efficiency bmp with a nitrogen effectiveness value of 0 12 would reduce edge of stream nitrogen delivery by 12 i e it has a pass through factor of 0 88 meaning it permits 88 of nitrogen to pass through its modeling segment for calculation purposes efficiency bmps are assigned to groups defining whether any two bmps can overlap on the same acreage to provide cumulative nutrient reduction benefits for example cover crops and enhanced nutrient management can both be applied to the same agricultural land acre however two different types of cover crops cannot these groups impact the permissible implementation options because within a single group bmps compete for the available acres meaning that they are mutually exclusive across groups bmps do not compete for acreage and can overlap when bmps overlap their combined pass through effect is calculated by multiplying the pass through percentage factors for each individual bmp implementation costs for each bmp in the optimization model are the same watershed wide average estimates provided in cast section 2 2 3 3 2 2 nutrient loads and the efficiency focused formulation the optimization model leverages cast output to make use of pre calculated values of load density called base load density φ s h u this composite term represents the weighted input densities and delivery factors for non direct sources along with stream bed and bank contributions eq 6 φ s h u l u w t t i u t k s i u t σ u t ψ ˆ s h u ζ s h s t b where ψ ˆ s h u ψ s h u l w ψ s h u s r and other notation is defined in tables 1 and 2 this base load density is combined with acres and pass through factors to compute total load delivered to the bay by substituting eq 6 into eq 5 and letting loads from direct sources l s u u d and shoreline l s h s h o r e l i n e be zero consistent with their linear independence from the non direct sources we arrive at a modified formulation for load from a selected geography l s eq 7 l s s s h h u u φ s h u θ s h u α s h u where as above α s h u represents acres available for implementation the combined pass through factor θ s h u is a function of the implementation acres of efficiency bmps and their load reduction effectiveness the simplified form of the load function in eq 7 permits a succinct articulation of a nonlinear programming model 3 2 3 geographies and the county scale the optimization tool has been developed to provide least cost or maximum load reduction solutions at county scale counties across the chesapeake bay watershed are composed of between 1 and 34 land river model segments due to access restrictions and privacy concerns much of the data in cast such as manure applications and crop types are not available at geographic resolutions finer than counties therefore modeling analyses have thus far focused on counties as they represent the primary scale of interest nevertheless the optimization model should be scalable to larger geographies such as conservation districts or states as long as they are determined precisely by a collection of land river segments 3 3 optimization model and solver based on the data described above the optimization model is defined primarily by two functions total annual cost of bmp implementation eq 8 and total annual load reduction at edge of tide i e delivered to the bay eq 9 the first is a linear convex function that can be computed simply without a cast simulation and the second is a nonlinear and non convex function depending on the optimization goals the cost and load functions are generally interchangeable as objective or constraint i e swapping eq 8 and eq 9 and replacing the right hand side load target in eq 9 with a budget target the feasible region which is the set of all possible values of the variables that satisfy all constraints simultaneously is defined by the edge of tide nutrient load target eq 9 acreage restriction on implementation of bmps in each group eq 10 and non negativity constraint for each bmp implementation variable eq 11 an optimal solution to the model must lie within this feasible region the model is presented in eqs 8 11 following definition of its sets collections of unique and unordered elements decision variables and parameters constants that must be specified in each optimization problem instance sets s set of land river segments s as in table 1 h set of agencies h as in table 1 u set of load sources u as in table 1 g set of non overlapping bmps b that can all be applied to the same load source g u set of bmp groups g that may be applied to load source u b u set of all bmps b that can be applied to load source u decision variables x s h u b number of acres to which bmp b is applied on load source u for agency h within land river segment s parameters θ p target nutrient load lb for pollutant p α s h u total acres of load source u for agency h within land river segment s ac φ s h u p load density of load source u for agency h within land river segment s lb ac τ b per acre annual cost of bmp b ac η b p s u reduction effectiveness of bmp b when applied to load source u in land river segment s reduction 3 3 1 model minimize the annual cost of implementing bmps eq 8 s s h h u u b b u τ b x s h u b subject to the post treatment load not exceeding a target eq 9 s s h h u u φ s h u p α s h u g g u 1 b g η b p s u x s h u b α s h u θ p the implementation of bmps in each group not exceeding the available acres eq 10 b g x s h u b α s h u s s h h u u g g u eq 11 x s h u b 0 s s h h u u b b u note that the product g g u 1 b g η b p s u x s h u b α s h u in eq 9 represents the combined pass through factor θ s h u as the number of acres of implementation for a bmp increases the total amount of load that passes through to the bay is reduced optimization problem instances hereafter simply referred to as problem instances are formed by specifying all sets and parameters for the model problem instances were solved using an open source solver ipopt interior point optimizer which is a package designed for large scale nonlinear optimization and maintained by the coin or laboratory lougee heimer 2003 ipopt is applicable to problems with continuous decision variables and it utilizes a primal dual interior point algorithm with a filter line search method to provide fast convergence to local solutions wächter and biegler 2006 the ipopt algorithm solves a sequence of auxiliary barrier problems in which the original bound constraints are replaced by a logarithmic barrier term an optimization step is taken after checking convergence of both the original nonlinear problem and the auxiliary barrier problem and for each step a trial point is rejected or filtered if it does not sufficiently reduce the objective function or constraint violation the algorithm repeats until a solution or a point satisfying first order optimality conditions is found wächter 2009 3 4 optimization software implementation to facilitate repeated solving for problem instances of the above nonlinear programming model we created an open source optimization software subsystem named bay optimization tools for analysis bayota the current version performs four main functions i processing of cast source data ii generating problem instances for selected counties iii passing in memory instances to the solver and iv parsing solution outputs it is important to note that the optimization model replicates the simulation logic of cast but does not actually run cast nevertheless outputs of the optimization model were checked against cast outputs to ensure that the same cost and load reduction estimates were generated by both models for common sets of efficiency bmps 3 4 1 structure as shown in fig 2 the optimization subsystem is organized into four components namely the engine workspace storage and public layers bayota s core is the engine layer that contains four packages written in the programming language python version 3 python core team 2020 van rossum and drake 2009 to parse source data create problem instances and interact with the solver to accomplish these tasks the engine manipulates both data and optimization model the workspace layer is the practitioner s access point for interactions with the subsystem specifically the workspace is where a user sets up configuration files and execution specifications e g file paths model structure and temporary files the workspace also provides a natural location for post processing analysis or visualization code files in the workspace are typically manipulated in local storage either attached to a personal computer or a remote access node of a high performance computing environment but they are automatically uploaded to the cloud after execution begins and pulled into a running docker container which is a means of bundling software into minimal size and independently executable packages merkel 2014 the storage layer is responsible for holding a copy of source data and serves as a transfer location between the local workspace and running docker containers it also houses temporary log and solution files the public layer described further in section 4 consists of the web based visualization application further technical details of each layer are provided in appendix a problem instances are generated using open source optimization software instances in bayota are built from smaller components using pyomo an extensible python based open source optimization modeling language hart et al 2011 2017 when executed the software initially constructs a model skeleton that consists of the necessary sets parameters and variables other expressions or model components including objectives constraints and any desired diagnostic outputs are added to the skeleton as designated in the specification file problem instances are then solved using ipopt as previously described section 3 3 3 4 2 batch processing in the cloud populating the web based visualization and analytics tool section 4 entails solving a problem instance for each county in the chesapeake bay watershed for four objective constraint goals cost minimization with a nitrogen or phosphorus load target and nitrogen or phosphorus load minimization with a budget limit and 50 different levels of each constraint totaling 39 400 independent problem instances for computational efficiency multiple instances are solved in parallel see appendix b for additional details regarding execution sequence structure and pseudocode an early version of the engine used the slurm job manager on a local high performance computing hpc cluster as the cbpo transitioned its hpc environment to amazon web services aws optimization subsystem execution migrated to a slurm managed cloudformation cluster utilizing 8 nodes of 64 processing cores each problem instances are solved in the aws batch compute environment with job management handled by docker and aws elastic container registry ecr docker containers are run on aws elastic cloud computing ec2 spot instances that allow flexibility in cpu architecture and pricing both of which are determined by aws batch at run time to take advantage of otherwise unused ec2 capacity in the aws cloud maximum memory available to the containers is set to 4 086 mb results are passed to an aws simple storage service s3 bucket providing easy web server access with appropriate credentials 4 visualization tool 4 1 motivation in addition to the optimization model and its software implementation there was need for a visual interface that would allow ease of understanding and facilitate use of the tool directly by management teams who have less familiarity with the optimization details such an interface would also serve the important role of facilitating ongoing communication and receiving feedback necessary for effectively improving and maintaining the system for this reason user interface development was set and prioritized as an early project goal to explore the type of decision support interface needed and the ways managers or other users might operate such an interface we developed a pilot online visualization tool that went through several revisions based on feedback from the cbp workgroups and stakeholders the visualization interface for chesapeake optimization vico allows users to answer real world optimization questions across the problem domain of efficiency bmps for local governments throughout the watershed especially those that carry stormwater permits the web application provides interested stakeholders with the capability to explore cost load and bmp implementation portfolios in this manifestation the tool draws from data exhaustively precomputed i e optimizations were pre run for each watershed county across ranges of load reduction and cost target objectives of practical interest 4 2 graphical interface results are available for nitrogen n and phosphorous p cost minimization objectives for 50 independent and equally spaced percent load reduction constraints from 1 to 50 and with the objective and constraint functions swapped for load reduction maximization with annual cost constraints from 100 000 to 5 000 000 after selecting an optimization goal of interest by geography and objective a user can identify points of interest from the set of feasible solutions to see bmp implementations realizing that target load reduction or annual investment in the form of a bar chart in the optimization literature such a set of solutions is often referred to as a pareto frontier or non inferior set cohon 1978 each member of this set is efficient in that no other alternative exists that performs as well on each objective i e load reduction for the specified nutrient and cost while being strictly better in at least one objective the cost versus load or objective constraint space for an optimization instance is determined and displayed as the user steps through the interface first the user selects the geography and specifies an optimization objective and constraint combination from the set of four preloaded options fig 3 in accordance with the user selected objective and constraint fifty pre computed optimization solutions are presented as points on a curve showing potential tradeoffs between annual load reductions and total annual bmp costs fig 4 note that all data in this manuscript are provisional and subject to revision they are being provided for illustrative purposes only the data have not received final approval by the u s geological survey usgs and are provided on the condition that neither the usgs nor the u s government shall be held liable for any damages resulting from the authorized or unauthorized use of the data to evaluate the bmp portfolio and acreages of implementation for a given optimization solution the user can select an individual point representing a particular optimization solution and then a bar chart showing the associated implementation details is generated selection of two optimization solution points both on the same county or each on a different county generates a grouped bar chart for easy comparison of bmp strategies for the two solutions of interest fig 5 although parsed as comma separated value csv files for data manipulation and display purposes bmp portfolios for selected points can be downloaded from vico in the particular file tabular txt and organizational format needed for user input via the cast web interface this provides users not only a means of verification but also an option to choose an optimal solution and then incorporate additional modifications prior to further exploration with the regulatory model 4 3 software development and improvement with stakeholder engagement the optimization system was developed over a period of two years with active stakeholder engagement of cbp managers scientists and engineers from the seven jurisdictions of the cbp federal agencies and other groups project leadership shared the latest prototypes of the visualization and analytics tool on monthly to quarterly time intervals with stakeholders at cbp modeling workgroup meetings and conference calls throughout the iterative refinement design and implementation process at each workgroup review the development team reported progress to project managers and workgroup members at large via audiovisual presentations and oral follow up discussions progress updates included discussion of programmatic and high level technical hurdles encountered potential solutions and recommendations received feedback from participants in the workgroup as well as other cbp stakeholder groups such as the water quality goal implementation team played a critical role in steering the tool development a clearer shared vision of the tool emerged from the sharing of questions opinions needs and wishes for the long term project direction as part of the ongoing effort to meet high standards of stakeholder participation such as described by barnhart et al 2018 for example the need for interactivity in conjunction with the prototype development of the optimization model and software directly shaped the form and function of vico to gather more instantaneous feedback scripting code for the hotjar behavior analytics tool was embedded in the current beta release to learn from site usage including mouse click density and scrolling behavior and to streamline the collection of feedback on user experience during this development period vico has had two beta releases the first focused on providing access to the efficiency bmp optimization results computed using the optimization model and software framework described in section 3 and stored in aws s3 by county the second beta release building on the first and on user feedback about the first expanded capabilities for tmdl planners and managers to evaluate tradeoffs the second beta release added the capability to show two county optimization result curves simultaneously and to compare two data points with associated bmp realizations from the same county curve or from distinct county curves selection of two points yields a grouped bar chart for comparison of the kind and amount of each bmp implemented in the solution portfolios vico has been implemented as a web application in r shiny chang et al 2015 and hosted on an internal r shiny server in the cbp chesapeake center for collaborative computing c4 aws cloud infrastructure r was chosen as an effective open source language r core team 2020 in widespread use in the scientific community and the r package shiny was selected as an easy powerful flexible and extensible way to present r computations and visualizations on an interactive web site aws is the cloud infrastructure provider for cbp chosen partly for overall cost effectiveness and vico utilizes just a portion of the compute resources set up in c4 for overall shiny application support the approximately 39 400 precomputed optimization results are stored in aws s3 as csv solution files and txt downloadable files base s3 storage cost for the 6 7 gb of data is only a fraction of a dollar a month with additional modest rates and fees applying for transfers etc initial shiny server hosting has been piggybacked on existing hpc modeling resources using only a small portion of available resources on a c4 8xlarge node to conveniently meet and manage increasing requests for hosting of multiple shiny applications the cbp data center is shifting to dedicated shiny server support in the form of a small ec2 instance or comparable containerized offerings overall maintenance costs will include keeping host operating systems r r shiny environments and source code current across upgrades as with other software apps and supporting layers 5 results and discussion 5 1 solving performance optimization runs can be separated conceptually into two stages a set up stage for data parsing and problem instance generation which is described in section 3 4 1 and a run time solving stage additional detail on the sequence of these stages is provided in appendix b the overall expected computation time can be better understood by bounding the two stages parsing source data and generating the model in memory the instance generation stage takes time proportional to the number of variables in the problem instance for each county fig 6 for even the smallest counties data parsing and model generation took between 30 s and 1 min average model generation time was 95 s which increased by approximately 1 min for every additional 37 500 decision variables the variation in instance sizes is primarily a function of the number of land river segments and agencies i e proportional to the product of s and h but is also determined by the types of load sources and bmps applicable in a given geography half of the county instances were composed of fewer than 18 000 variables and 90 used fewer than 50 000 variables execution time for ipopt was benchmarked for each county and for three nitrogen reduction targets fig 7 average solve time was 19 s for a 20 load reduction target every 12 500 additional variables extended solve time by approximately 10 s for targets of 40 and 1 10 s increases corresponded to roughly 8 000 and 50 000 additional variables respectively 5 2 implementation plans and efficiency bmps 5 2 1 comparing optimized and unoptimized modeled costs to analyze potential benefits of the optimization tool to bmp planning we compared costs between two bmp implementation plans for each county 1 solutions of the optimization model which we refer to as optimization opt suggested plans 2 bmp implementation portfolios adapted from the state level restoration plans which we refer to as watershed implementation plan wip inspired plans to generate each wip inspired plan bmps were drawn from the corresponding state level wip currently on the third iterative phase phase iii as of this writing but restricted to only efficiency bmps to ensure that the same potential investments were possible and same set of variables included as in the optimization model additionally to ensure a fair comparison county level nitrogen reductions for the wip inspired plan were calculated on 2010 no bmp meaning all bmps removed rates and acres which are the base conditions used in the current version of the optimization subsystem therefore variables in the optimization model were initialized to be identical to the wip inspired plan and an equivalent nitrogen reduction amount from efficiency bmps was used as the reduction constraint when the optimization model was solved for lowest cost this cost was compared to the wip inspired implementation cost for those same bmps these comparisons were conducted for 20 counties that span the range and include each decile of number of variables in county level optimization problem instances potential cost improvements were identified across the range of experiments and the percentage reduction in cost for the optimization suggested plans was greater for more costly wip inspired plans fig 8 left panel as examples we describe here results for counties representing the 10th 50th and 90th percentiles based on the number of variables in each county s problem instance for the 10th percentile county the wip includes a total annual cost of 2 5 million for implementing efficiency bmps when evaluated on 2010 no bmp rates and acres these efficiency bmps provide a total county level nitrogen reduction of 12 2 when the optimization model was solved for lowest cost using the same nitrogen reduction as a constraint a solution was found that costs approximately 130 thousand i e approximately 95 lower cost for the 50th percentile county the wip includes a total of 8 9 million in annual efficiency bmp implementation costs for a 29 9 reduction in nitrogen load when the corresponding optimization problem instance was solved for lowest cost a solution was found with a cost of 2 6 million i e approximately 70 lower cost for the 90th percentile county where a 4 county level n reduction is achieved relative to no bmp using 2010 rates and acres the wip includes a total annual efficiency bmp implementation cost of 620 thousand dollars when the optimization model was solved for lowest cost using the 4 nitrogen reduction as a constraint a solution was found that costs 270 thousand i e approximately 67 lower cost however such cost improvements were not always achievable with efficiency bmps for example another county that is approximately the 10th percentile only presented a 26 relative cost improvement in the opt suggested implementation plan as compared to the wip inspired plan this was due in part to there being only a relatively small total cost of planned efficiency bmps in the wip inspired plan for the county differences in the overall cost per pound of nitrogen reduction between the two plans were similar to the differences in total cost fig 8 right panel opt suggested plans generally exhibited lower costs per pound of reduction with a median for the opt suggested plans 8 per pound 77 lower than for the wip inspired plans 36 per pound moreover the maximum overall cost per pound across tested counties for the opt suggested plans 212 per pound which was more than double the next highest value was 62 lower than for the wip inspired plans 563 per pound in fact 25 of the wip inspired plans had higher overall costs per pound than the highest among opt suggested plans 5 2 2 limitations and opportunities of comparing cost the above comparison of opt suggested and wip inspired plans suggests the potential for substantial cost savings in the implementation of efficiency bmps however the comparison must be accompanied by multiple important caveats which also point toward future research directions these caveats are briefly described in the following paragraphs and include the land condition cost averaging nutrient selection unmodeled considerations and the bmp subset load reductions were calculated using 2010 no bmp also called no action loading rates and land use acres therefore the available acres do not reflect either current conditions or conditions after any changes as a result of development or other human modifications and it is inaccurate to project such load reductions to 2025 land uses as built into the wip inspired plans we anticipate that modifications to the model will be required to account for existing bmp implementations and varying land use acres the comparison was conducted with watershed wide average costs which serve as a useful approximation but do not reflect the potential variation in per unit implementation costs across and even within jurisdictions in addition the comparison does not consider transactions costs i e the costs of identifying the location of applicable practices including running cast contracting with agents to install the practices and enforcement to ensure that practices are carried out as contracted however given that the opt suggested plan involves fewer practices it is possible that overall costs would be reduced further if transactions costs were factored into the comparison carpentier et al 1998 nevertheless with modifications to input files the framework developed here is capable of utilizing jurisdiction specific cost information such opt suggested plans could include bmps that a decision maker would want to exclude or conversely could exclude bmps that a decision maker would want to include despite their costliness and for unmodeled reasons to address this we could apply user defined bmp implementation constraints but such decisions would be specific to each jurisdiction more generally accommodating flexible user specified constraints in the optimization tool interface would help with exploration of alternative strategies and incorporation of other desired bmp portfolio attributes that are difficult to quantify such as unmodeled preferences or uncertainties this would likely require adding interactive techniques to the optimization procedure these techniques originated from early work by brill 1979 willis and perlack 1980 and schilling et al 1982 on hard to quantify objectives and are currently being incorporated into modern methods such as human guided or interactive multi objective optimization imo as described by babbar sebens et al 2015 and xin et al 2018 load reduction constraints were set according to nitrogen only phosphorus and sediment reductions or targets that are embedded in the wip inspired plan were not considered in the opt suggested plan also exploration of the composition of bmps and locations of implementation in each plan is beyond the scope of this analysis nevertheless the optimization results do specify bmp implementation at the scale of land river segments as seen in the web application section 4 and so we anticipate that the framework described here will be of value to such future investigations it is worth noting again that this comparison included only the so called efficiency bmps therefore many non efficiency bmps such as forest buffers and land retirement that are also effective and widely considered for implementation by decision makers were not considered this limitation is discussed further in the next section section 5 3 5 3 expanding optimization beyond a bmp subset challenges future development and feedback the time averaged watershed simulation model cast has improved the modeling suite s usefulness for supporting decision makers section 2 1 and facilitated both the development of our optimization framework and its specific application to the efficiency bmp subset section 3 nevertheless the structure of calculations in cast section 2 2 presents several challenges for creating an integrated all encompassing optimization system the modeled effect of bmps on the delivered nutrient and sediment loads in cast is nonlinear and there are many potential local optima the decision variables are also non separable roughly speaking the effects of bmps from different categories table 3 are dependent on one another making it difficult to apply divide and conquer or decomposition optimization techniques moreover the decision space is high dimensional for a single county instance the input space will typically consist of variables numbering from tens of thousands to a million this paper has characterized the cast optimization problem and described the development of an optimization framework and its application to a well defined subproblem i e identifying optimal solutions over the efficiency bmp subset within cast section 3 the decision to first address only this specific bmp category was motivated by several aspects of the problem first in these initial years of investigation the lack of an application programming interface api for cast made it difficult to test methods that require many iterative function evaluations second based on the structure of the problem it is believed that an efficient method for solving the selected subproblem i e the optimization problem for the efficiency bmp subset will be an important component of a more comprehensive optimization algorithm third tackling this simplified problem gave valuable insight that can ultimately be applied to the higher dimensionality and intricateness of the complete load function eq 5 moreover although good solutions were consistently found to instances of this nonlinear model ipopt does not guarantee global optimality and so additional computational advantage may be gained through further study of the model s mathematical characteristics and exploration of effective approximations boddiford et al 2021 including all of the bmps should be a primary focus of future efforts to build upon the work described here such work is essential to obtain broader applicability of the optimization tool in supporting targeted and effective management decisions because many of the yet to be modeled bmps could be critical wip elements for many jurisdictions for example land conversion bmps such as forest or grass buffers can reduce nutrient delivery substantially and would add considerable utility to the cbp optimization tools however the formulation of a mathematical programming model including land conversion bmps presents serious challenges because such a formulation must simultaneously account for the change in available acres the implicit treatment or efficiency value applied to upland acres and the acreage constraints on that upland treatment approaches that forego explicit modeling of the bmp effects will likely be required to address these and similar challenges presented by the non efficiency bmps such approaches which could still use an explicit model as a part or complement to other methods may include model reduction techniques such as surrogate modeling e g sun et al 2015 jia and culver 2006 and meta heuristics such as simulated annealing and evolutionary based algorithms e g marco et al 2019 yoon et al 2019 further expansion of optimization capabilities for cast across all bmp input types and rule sets is underway inclusion of all bmps in the optimization is an ultimate goal along with an interface that will ideally launch the optimization engine on custom queries and collect organize and present query results when done in the more distant future the cast model and optimization tool should be expanded to also include values for certain bmp co benefits at local to regional scales such as flood avoidance stream ecology or carbon sequestration while the necessary mathematical algorithmic and software groundwork to identify and apply an appropriate optimization procedure to the fully general case with all bmps is underway software developers have been concurrently porting and recoding cast to provide an implementation that will execute faster it is hoped that reducing computation time for each call of the cast function on a set of bmp inputs will enable a closer coupling between the watershed and optimization models until full programmatic integration with an accelerated version of cast is possible development of bayota has settled practical detail laden front end issues of getting required data to an optimization engine and allowed solution of subsets of the full problem including the useful category of efficiency bmps such solutions have practical value to county level managers and interested stakeholders and can be both illustrative and instructive preliminary feedback has been largely positive regarding the web application interface section 4 with most users expressing no difficulties when using it cost efficient solutions found by the tool such as presented in section 5 were discussed with stakeholders who found the results encouraging and despite current limitations many of which have been characterized by stakeholders as places for improvement there is nonetheless continued interest for using solutions provided by this optimization for steering development of future management plans user feedback has also included requests for the ability to explore solutions for larger geographic areas and for the optimization to be solved in real time so that a user can change constraints and parameters as they explore optimization solutions for example if a user wants an optimized bmp implementation for multiple counties one cannot be assured that combining solutions of each separately solved county level instance would yield an optimal multi county implementation portfolio instead a multi county instance would need to be generated and solved the reason is that in some cases one would find tradeoffs between counties such that a more cost effective solution included greater investment in one county and less in another such a multi county instance would require a target load that encompasses the delivered load from multiple counties and one would need to optimize for the complete set of land river segments from all the counties simultaneously the aforementioned technical challenges and user feedback including the opportunities for improvement discussed in section 5 2 2 and tackling of the fully general case with non efficiency bmps have made clear the importance of building further improvements moreover as stressors on the chesapeake bay from local watershed changes grow due to climate change ni et al 2019 du et al 2018 irby et al 2018 the need for refining and optimizing management plans will likely increase going forward giuffria et al 2017 bosch et al 2018 therefore further development on the multiple challenges discussed here is critical to continue supporting an effective adaptive management strategy 6 summary and conclusions the chesapeake bay watershed is a large area with many stakeholders and jurisdictions committed to achieving environmental improvement goals a suite of modeling tools is used regularly to track management progress and to consider improved management plans there is urgent need for further development of the suite to enable rapid and assured identification of cost efficient strategies the low cost high return management practices have already been implemented in the three decades of the chesapeake bay program restoration effort yet challenges like climate change require continued nutrient reductions to maintain the water quality standards and ecological health of the bay for cost effective and environmentally protective strategies to restore the chesapeake we need efficient plans a framework has been developed for identifying low cost implementation options for a distinct bmp subset that is for efficiency bmps that are represented by scalar load reduction effectiveness factors in the most recent phase 6 cbp watershed model known as cast the optimization model software and web based visualization tool provide new utility for cast to assist stakeholders in identifying cost effective strategies for a variety of management relevant decisions and they address recommendations of cbp s stac importantly the framework uses a problem formulation that can be consistently applied for all counties across the entire chesapeake bay watershed and uses the same source data as the cast simulation thus guaranteeing that optimization results will provide equivalent load reductions when applied through cast this combined simulation and optimization approach will continue to be essential as modeling capabilities are expanded results from a comparison of optimization suggested bmp implementations with wip inspired plans which were adapted from the state level wips and retain only efficiency bmps demonstrate the framework s potential opportunities for finding cost saving ideas or adjustments to jurisdiction implementation strategies however characteristics of the existing tool limit its ability to answer certain strategic implementation questions for the simulation model including for non efficiency bmps i e those not represented by scalar reduction factors which must be addressed by future development the comparison of efficiency bmps is nevertheless useful in its own right and perhaps more importantly it may be possible to utilize this efficiency bmp model as a subproblem in an optimization approach expanded to include non efficiency bmps as well importantly testing of the tool with managers has revealed that performance of the optimization model in generating solutions is fast enough to be practical and feedback from users to date has been positive and encouraging the stakeholder driven approach in developing this modeling and optimization framework continues to improve the suite of tools that support restoration efforts and planning in this large multi jurisdictional watershed watershed management challenges similar to those for chesapeake bay are also found in many other regions both across the united states as well as internationally and the general conceptual approach described here is transferrable to other watersheds however direct applicability of the methods and tools to other areas is dependent on the presence of a management structure and existence of models that are amenable to optimization as evidenced by the partial history provided in section 2 and by published studies and reviews e g boesch 2019 alamanos et al 2021 the amount of time and resources expended on development of the management structure modeling and monitoring resources that are the foundation for these optimization capabilities is quite substantial this provides good rationale for the use of simpler time averaged stakeholder driven models such as cast software availability a freely and permanently available snapshot of bayota bay optimization tools for analysis source code can be accessed by its doi address via zenodo and an up to date version is available via github https doi org 10 5281 zenodo 4609255 kaufman 2021 https github com dkauf42 bayota the vico visualization interface for chesapeake optimization application can be freely accessed and explored in a web browser and its source code is available via github https shiny apps chesapeakebay net vico https github com kwasplen vico the chesapeake bay program s phase 6 watershed model cast and its documentation can be accessed free of charge in a web browser https cast chesapeakebay net declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this material is based upon work funded wholly or in part by the united states environmental protection agency usepa chesapeake bay program office including direct salary support for multiple partners within the usepa administered chesapeake bay program including co authors shenk and linker as well as numerous assisting support staff assistance agreements cb96350501 to chesapeake research consortium crc inc co authors kaufman ball bosch ellis hobbs van houtven and mcgarity cb96325901 and cb96365601 to the university of maryland center for environmental science co author asplen and cb96351401 to pennsylvania state university co author bhatt the contents of this document do not necessarily reflect the views and policies of the environmental protection agency any use of trade firm or product names is for descriptive purposes only and does not imply endorsement by the u s government the authors thank rich batiuk and the modeling team at the chesapeake bay program office cuiyin wu andrew sommerlot richard tian isabella bertani for constructive input throughout the project the authors thank and acknowledge stuart schwartz of the university of maryland baltimore county who was an original member of crc s advisory and support committee and provided important early contributions of both conceptual and practical nature many thanks to john massey for technical support with cloud resources and batch processing and megan mcneilly for user experience improvements to the online decision support tool further thanks to brian burch megan thynge tim paris martin koslof and the entire cbpo software development and information technology team a optimization software layers the optimization engine layer is maintained as a git a software version control system repository and run in a docker container it contains four python module collections 1 castjeeves modules access query and parse source data from cast 2 bayom e is devoted to solving optimization problems involving cast efficiency bmps 3 bayota settings is focused on configuring directory paths output graphics and logging and contains sample configuration files 4 bayota util provides useful functionality not intrinsic to the other sub package roles fig 9 application structure of bayota including the optimization engine blue rectangle user specific workspace green rectangle and storage in s3 orange rectangles fig 9 in addition to these the repository contains execution scripts for handling the logical flow of parallel optimization runs fig 9 provides a more detailed look at the optimization engine and its interactions with the other bayota components the workspace layer is the primary handler of inputs and outputs the minimal necessary workspace consists of subdirectories for configuration files data and specification files configuration files set up file paths and log formats and a subdirectory provides temporary space for cast data specification files allow for selection of geographies model objectives and constraints model modifications and various options appendix b other workspace subdirectories may be generated for control logs temporary files output and notebooks some of which are created automatically during optimization runs in the storage layer the current version of bayota uses a copy of relevant data from the cast source database to update the copy the user executes a script to pull the latest source database tabular files into the local workspace and then into s3 during a run the stored source data in s3 are automatically pulled into the workspace of the running docker container the castjeeves package pulls data into memory to be serialized and written to disk for quick loading across repeated executions and queries the data in memory when generating a problem instance the bayom e package uses castjeeves to access data required for each part of the optimization model data are loaded in this sequence pollutants geographic delineations bmps load sources agencies associations between bmps and load sources bmp costs bmp effectiveness factors base loading rates and available acres b batch processing setup and sequence run processing is organized into studies experiments and trials that correspond to different characteristics of a problem instance that must be specified fig 10 studies refer to the geography and model structure used experiments refer to modifications of the model such as adding or removing constraints trials refer to single value changes e g increasing the value of the load target or decreasing the budget fig 10 schematic example left illustrating the categorization of problem instance types i e study experiment trial which map onto the software processing flow right used when solving multiple problem instances fig 10 optimization runs are set up by modifying three types of specification files batch model and e t a batch file is the top level file for controlling runs and contains options for geography the names of other specification files to use and options such as whether to move and translate solution files upon run completion a model file contains options for setting up objectives constraints and additional parameter names an e t file allows specification of model modifications for particular experiments and or setting of different values for constraints termed trials the run script reads the specification files to determine the structure and number of problem instances to be generated execution sequence pseudocode image 3 
25758,digital soil mapping dsm techniques have provided soil information that has revolutionized soil management across multiple spatial extents and scales dsm practitioners have been increasingly reliant on machine learning ml techniques yet methods to generate uncertainty maps from ml predictions are limited to address this issue this study integrates ml based dsm with quantile regression qr in a methodological framework for estimating uncertainty we test the proposed framework on two case study areas in canada 1 a dry forest ecosystem in the kamloops region of british columbia canada and 2 an agricultural system in the ottawa region of ontario canada four ml techniques random forest cubist decision tree k nearest neighbors and support vector machine were compared using repeated cross validation maps showing the 90 prediction interval pi were produced regardless of the case study ml approach and predicted soil variable the uncertainty estimates were reliable and stable according to the pi coverage probability analysis keywords digital soil mapping machine learning uncertainty estimation quantile regression 1 introduction over the past decades the soil science community has witnessed the emergence of digital soil mapping dsm techniques in transforming our understanding of the spatial patterns of soils these maps are designed to provide decision makers with accurate and precise information which is necessary to practice sustainable soil management mitigate soil degradation and improve food security sanchez et al 2009 while the uses of digital soil maps are cross sectorial and its benefits are multi dimensional it is also well recognized that the maps produced using dsm techniques are not error free not only is it necessary to evaluate the accuracy of predictions using validation data within a dsm framework it is also necessary to quantitatively estimate and spatially represent the uncertainty so modellers and decision makers can assess the usefulness of a map folberth et al 2016 heuvelink 1998 mcbratney et al 2003 common forms of error may arise from measurements digitisation data entry interpretation classification generalisation and interpolation arrouays et al 2014b uncertainty in digital soil maps may arise from modeling bias parameterization or even measurement errors associated with the input data minasny and mcbratney 2002 to provide a better understanding of uncertainty nelson et al 2011 suggests performing an error budget to examine the contribution of each error by using a combination of geostatistical and monte carlo simulations it is also important to consider the difference between model error and spatially explicit uncertainty model error often measured as the mean square error mse is the average squared difference between the estimated value and the actual value malone et al 2011 wang and bovik 2009 however spatially explicit uncertainty sometimes referred to as local error refers to the quantification of prediction intervals for a model output feizizadeh et al 2014 malone et al 2011 vaysse and lagacherie 2017 based on the importance of spatially explicit uncertainty quantification in dsm the inclusion of uncertainty layers is often specified in international standards examples of these standards may require the inclusion of the 90 prediction interval pi according to the globalsoilmap net specifications arrouays et al 2014a or the inclusion of prediction standard deviation maps according to global scale digital soil maps produced by the food and agriculture of the united nations fao and gsp 2017 despite these specifications measuring uncertainty in dsm remains challenging and is generally uncommon minasny and mcbratney 2002 vaysse and lagacherie 2017 within the dsm literature there are many methods for producing spatially explicit estimates of uncertainty geostatistical modelling approaches for example can produce uncertainty estimates using the kriging prediction variance where spatial representations of the prediction intervals may be calculated however the uncertainty estimates are specific to the modelling approach itself and not applicable for machine learning techniques which are more commonly used now fouedjio and klump 2019 malone et al 2017 alternatively studies such as karunaratne et al 2014 have used conditional sequential gaussian simulation an approach that has been applied in geostatistical techniques to generate a suite of equally probable map realizations the bootstrapping technique provides another approach for uncertainty estimation ackerson et al 2015 padarian et al 2017 stumpf et al 2017 thomas et al 2015 where a model is trained on a random subset of the full training data and multiple realizations of a soil map are produced through the prediction process by generating these realizations uncertainty estimates are produced by calculating the average mse from the realizations and summing it with the bootstrap prediction variance that is estimated for each pixel malone et al 2017 however computational capabilities may limit the use of this approach when applied to large datasets since the full extent of each map realization needs to be predicted to estimate the prediction variance other uncertainty assessment methods also have limitations for example the bayesian method only measures uncertainty associated with input data while the monte carlo method specifically measures uncertainty in parameters solomatine and shrestha 2009 although the literature review primarily focusses on techniques for evaluating uncertainty when predicting continuous response variables spatial estimates of uncertainty may also be produced for categorical response variables using fuzzy inference qi and zhu 2011 zhu 1997 and bootstrapping chaney et al 2016 heung et al 2017 when concerning categorical variables uncertainty is estimated using metrics such as ignorance uncertainty goodchild et al 1994 leung et al 1992 exaggeration uncertainty zhu 1997 or confidence index burrough et al 1997 the use of machine learning ml techniques for regression purposes has become increasingly common in dsm and this is often tied to the increasing power of computational systems rossiter 2018 some of the ml techniques include random forest rf e g grimm et al 2008 mancini et al 2019 cubist decision tree e g adhikari et al 2019 k nearest neighbors knn merchant et al 2018 and support vector machine svm e g mancini et al 2019 merchant et al 2018 despite the popularity of ml techniques many ml techniques are not capable of generating localized uncertainty maps szatmári and pásztor 2019 vaysse and lagacherie 2017 veronesi and schillaci 2019 among all the ml methods used in dsm quantile regression forest qrf is one of the few that has a built in mechanism for uncertainty quantification qrf meinshausen 2006 is an extension of rf breiman 2001 and is used to calculate the full conditional distribution of a prediction rather than only the conditional mean fouedjio and klump 2019 meinshausen 2006 given some of the limitations of existing approaches a knowledge gap is apparent with regards to uncertainty estimations when using ml techniques hence the objectives of this study are as follows 1 to propose a generic framework using a quantile regression qr approach for estimating the uncertainty of digital soil maps produced from ml 2 to test the framework using common ml techniques for two case studies in contrasting landscapes from the kamloops british columbia and the ottawa ontario regions of canada 3 to assess the uncertainty estimates using metrics such as mean prediction intervals mpi and prediction interval coverage probability picp analysis and 4 to compare the proposed qr framework with an established technique such as qrf 2 theoretical background this section provides an overview of the qr approach methods for evaluating the uncertainty estimates and proposes how qr may be integrated into a generic dsm framework 2 1 on quantile regression quantile regression qr was first introduced by koenker and bassett 1978 and originally appeared in the field of quantitative economics however its use has since been extended to other applications within the soil science literature outside of dsm qr has been used for estimating the uncertainty of models used for assessing nitrate contamination of groundwater using different ml techniques rahmati et al 2019 in terms of the dsm literature however relatively few studies have explored this approach with the exception of lombardo et al 2018 who demonstrated the coupling of a generalized linear model approach with qr for predicting soil organic carbon soc however such couplings may be potentially extended to other ml techniques as part of a generic framework when applying qr for uncertainty estimation we assume that there is a linear relationship between a target variable s observed value and its model predicted value qr consists of a set of linear regression models where the response variable is the selected quantile of the variable s conditional distribution within the hydrological modelling literature dogulu et al 2015 lopez et al 2014 rahmati et al 2019 qr is applied as a post processing technique whereby the prediction of the response variable is dissociated from the uncertainty estimation process because of this dissociation there is the added flexibility in the choice of the predictive model because the uncertainty estimates are generated using the model residuals qr estimates the value for a target variable for any quantile that is needed koenker and hallock 2001 which may then be used to calculate prediction intervals e g 90 using the upper 95 and lower 5 quantile maps here a brief description of qr is provided however detailed descriptions may be found in lópez lópez et al 2014 this framework applies the same qr framework to that of lópez lópez et al 2014 dogulu et al 2015 and rahmati et al 2019 for each quantile τ we assumes a linear relationship between observed values y and predicted values y ˆ 1 y a τ y ˆ b τ where a τ is the slope and b τ is the intercept of the linear regression here a τ and b τ are both determined by minimizing the sum of residuals in the following loss function 2 m i n j 1 j ρ τ y j a τ y ˆ j b τ where y j and y ˆ j are the jth paired samples i e soil measurements with a total of j samples and ρ τ is the qr function for the τ th quantile 3 ρ τ j τ 1 j j 0 τ j j 0 where the model residuals j are the difference between the observed and predicted values acquired from eq 1 for the τ th quantile the qr function is applied to the residual j in eq 3 for the desired quantile τ lópez lópez et al 2014 dogulu et al 2015 rahmati et al 2019 2 2 evaluation of uncertainty estimates the uncertainty estimates may be evaluated using a combination of prediction interval coverage probability picp and mean prediction interval mpi graphs picp graphs sometimes referred to as accuracy plots are used to assess the performance of quantile regression in terms of uncertainty quantification by evaluating the encapsulation of observation values into an associated prediction interval goovaerts 2001 at each particular confidence level we should expect that the same percentage of observations equal to the associated confidence level is encapsulated by the pi for example it is expected that about 90 of the observations used for model validation will fall within the 90 pi here picp is calculated as follows 4 p i c p 1 n τ 1 n c c 1 p τ l o w e r l i m i t y τ p τ u p p e r l i m i t 0 where y τ is the observed value and p τ l o w e r l i m i t and p τ u p p e r l i m i t are the predicted lower and upper limit of the soil variable rahmati et al 2019 therefore to assess the sensitivity of qr uncertainty estimates pis for a series of confidence levels are defined and picp is then calculated for each confidence level ideally the picp should correspond to the confidence level and show a 1 1 relationship malone et al 2017 if picp is less than the confidence level the uncertainty has been underestimated and if picp is greater than the confidence level the uncertainty has been overestimated szatmári and pásztor 2019 mpi is calculated as the average width of the prediction intervals and may be used to quantify overall map uncertainty a wider prediction interval represents higher uncertainty while a narrower prediction interval represents lower uncertainty ding et al 2018 rahmati et al 2019 mpi is calculated as follows 5 m p i 1 n τ 1 n p τ u p p e r l i m i t p τ l o w e r l i m i t when comparing models with similar picp the model with the lower mpi is regarded as the better model muthusamy et al 2016 3 methodology based on the theoretical background we developed a framework that evaluates the performance of the uncertainty estimates using picp and mpi section 3 1 1 and generates maps of predictions and their corresponding uncertainty estimates section 3 1 2 to test the performance of the proposed framework it was applied to two case studies from contrasting environmental systems dry inland forests of kamloops british columbia section 3 2 and the agriculturally dominated landscapes of ottawa ontario section 3 3 in both study areas different sets of environmental predictors were used to predict different soil attributes to evaluate the performance of qr as a post processing technique in machine learning various machine learners were compared section 3 4 furthermore the performance of the framework which couples qr to the outputs of machine learning models was compared to a reference approach using quantile regression forest section 3 5 lastly the accuracy of model predictions and the performance of the uncertainty estimates were evaluated using independent test data section 3 6 3 1 integration of quantile regression for predictive mapping the proposed framework consists of two phases phase 1 testing the predictive model and uncertainty estimates and phase 2 generating predictive maps and uncertainty maps fig 1 all modelling activities were carried out using the r statistical language and the relevant packages are listed in table 1 3 1 1 testing the predictive model and uncertainty estimations in phase 1 the objective is to ascertain the accuracy of the ml model using goodness of fit metrics and to quantitatively evaluate the uncertainty estimates using picp eq 4 and mpi eq 5 graphs here the process consists of seven steps where the only input is a matrix that consists of the observed soil attribute value and the corresponding covariate values for each sample location this is acquired by spatially intersecting the geographical position of each sample point with a suite of environmental covariates representing the scorpan factors mcbratney et al 2003 to generate estimates of model accuracy picp and mpi a nested cross validation procedure is applied which consists of an outer loop and an inner loop the inner loop steps 2 5 calibrates and selects the predictive model with the optimal combination of model hyperparameters using the validation data the outer loop steps 1 7 assesses the ml model s accuracy and estimates the uncertainty in step 1 the matrix with the soil environmental data is randomly partitioned into k outer folds whereby k outer 1 folds are used as the input data to the inner loop the remaining fold is reserved for testing the predictive model and evaluating the uncertainty estimates in step 2 the outer training fold is further partitioned into k inner folds whereby k inner 1 folds are used to build the predictive model here the predictive models are fitted step 3 using different combinations of hyperparameters e g m try for random forest and sigma and cost for support vector machine and the model is predicted on the validation data which then allows the calculation of the model s accuracy the accuracy values are used to select the optimal hyperparameter values step 4 this process is reiterated k inner times so that each fold is used to validate the model once and the optimized model is selected in step 5 it is important to note that the optimized model is calibrated using all the data in the inner loop i e non partitioned lastly the residual distribution i e observed vs predicted values are retained from each validation fold and combined steps 2 5 may be carried out entirely using the caret package table 1 kuhn 2018 in the r statistical language which includes the model parameterization and selection functions step 5 produces a matrix of the residual distribution for the validation data inner loop which is compiled from each validation fold this matrix is then used to fit the qr function using the quantreg package table 1 koenker 2019 in step 6 because an independent test fold was retained in step 1 it is now possible in step 7 to produce an estimate of model accuracy by predicting the optimized model on the test data and generating the residual distribution for the test fold outer loop secondly the fitted qr model developed from the compiled model residuals is applied to the test fold at the desired percentiles from which the picp and mpi are later calculated to complete the outer loop the model testing process is also reiterated k outer times so that each test fold is used to test the model once and generate the percentiles for each sample location it is important to recall that the observed values from the k outer test folds are not used when generating the percentiles for the independent test data only the predicted values from the machine learner are used and therefore the test data remains completely independent in the validation process i e a nested cross validation procedure in step 7 a matrix consisting of the residual distribution values and their percentile values are retained from each test fold and combined this final matrix is used to calculate goodness of fit statistics e g lin s concordance correlation coefficient root mean square error calculate picp for a range of confidence levels using eq 4 and generate the corresponding picp plot and calculate the mpi using eq 5 to assess the reliability of the accuracy picp and mpi metrics phase 1 may be repeated over multiple iterations so the mean of the metrics may be reported as well as their standard deviations to generate the final uncertainty estimate maps using qr the residual distribution values for the test data are all compiled including the repeats and used as the input to step 9 in phase 2 3 1 2 generating predictive maps and uncertainty maps in phase 2 the final digital soil maps and uncertainty maps are produced using all soil observations in step 8 the soil environmental matrix is used to calibrate the predictive model and hyperparameter values are selected using cross validation in the caret package once the optimal hyperparameter values are determined a final model is produced using the entire training dataset and spatial predictions are made using the covariate stack to generate the final maps in step 9 using the raster package hijmans 2019 to generate the uncertainty maps the residual distribution values for the test data from step 7 are used to fit the qr function step 10 where the fitted qr function is then applied to the final predicted maps produced in step 9 to generate the upper and lower limit percentile maps and the pi map is then calculated as the difference between upper and lower limit percentile maps 3 2 kamloops study area this study area is 50 km west of kamloops british columbia where the eagle hill forest is located fig 2 the study area has a spatial extent of approximately 95 km2 and is located between 50 46 50 to 50 55 05 n and 120 46 37 to 120 57 03 w and has an elevation range of 545 m to 1455 m above mean sea level the climate is semi arid due to the rain shadow effect of the coast mountains with an average annual precipitation of 380 mm the annual temperature ranges between 10 c in the winter and 30 c in the summer the eagle hill forest is in the interior douglas fir zone according to british columbia s biogeoclimatic ecosystem classification system hope et al 1991 and is largely covered by mature douglas fir pseudotsuga menziesii stands with areas of grasslands at lower elevations the soils are primarily described as eutric brunisols and gray luvisols according to the canadian system of soil classification soil classification working group 1998 3 2 1 development of training data three sampling strategies were combined for the kamloops region conditioned latin hypercube sampling clhs minasny and mcbratney 2006 randomized road cut sampling and opportunistic sampling os for the clhs approach the environmental covariates were first generated and used to inform the sampling scheme due to limited accessibility to potential sampling locations as a result of the mountainous terrain and forested cover we restricted the clhs locations to areas that were within 200 m of a road road cuts are the locations where soil or rock are removed along a route to establish a road and are therefore suitable locations from which soil thickness can be estimated here randomly generated sampling locations were placed along the forest service roads finally os locations were selected in the field at the discretion of field pedologists this was done to supplement point data collection and to replace clhs points that fell in areas where access was not possible the soil variables included soil thickness depth to carbonates and soil ph at the 0 30 cm depth increment which were largely selected for their relevance in forest soil management soil thickness and depth to carbonates were both measured in the field and soil ph was measured using a 1 1 soil 0 01m cacl2 solution in the lab there were 410 observations for soil thickness 172 observations for depth to carbonates and 206 observations for soil ph only sample points where carbonates were measured via effervescence after dropping 10 hcl on the soil profile were included in the modeling the soil thickness ranged from 0 cm to 800 cm depth to carbonates ranged from 0 cm to 145 cm and soil ph ranged from 4 35 to 8 65 the mean values for soil thickness depth to carbonates and soil ph were 180 cm 63 cm and 5 9 respectively table 2 a total of 15 environmental covariates were selected from various sources to represent the scorpan model factors mcbratney et al 2003 a suite of 12 topographic variables were derived from a 3 m resolution digital elevation model dem which was produced from light detection and ranging lidar data all continuous covariates were centered and scaled in addition two categorical variables were included the biogeoclimatic subzone and bedrock geology for non tree based models categorical variables needed to be transformed into a series of binary variables for each category of the variable using an encoding process kuhn and johnson 2019 the encoding process was carried out using the onehot package graves 2017 in r where the resulting variables were used only for the knn and svm models non encoded categorical variables were used for rf and cubist models a list of environmental covariates is provided in appendix a table a1 3 3 ottawa study area the city of ottawa was formed from the amalgamation of the city of ottawa and the regional municipality of ottawa carleton and is bounded to the north by the ottawa river fig 2 the study area has a spatial extent of 2824 km2 and is located between 44 57 43 to 45 32 02 n and 75 14 45 to 76 21 20 w and had elevational range of 55 m 171 m above mean sea level the study area has an average annual precipitation of 919 mm and an annual average temperature of 6 6 c with january as the coldest month 10 2 c and july as the hottest month 21 2 c environment canada 2020 ottawa is within the lake simcoe rideau ecoregion of the mixedwood plains ecozone and has a mild and moist climate crins et al 2009 forested areas are dominated by deciduous species such as sugar maple acer saccharum var saccharum american beech fagus grandifolia and white ash fraxinus americana in upland sites and black spruce picea mariana and tamarack larix laricina in the low lying peatlands crins et al 2009 the most common agricultural land uses include annual crops corn soybeans winter wheat and forages and pasture for livestock or dairy operations soils are dominantly classified as orthic humic gleysols and orthic melanic brunisols of the canadian system of soil classification soil classification working group 1998 3 3 1 development of training data two sampling strategies were combined for the ottawa region clhs and os os points were used to account for clhs points that fell in areas with restricted access in total 1633 soil profiles were described and sampled according to pedological horizons as per the canadian system of soil classification soil classification working group 1998 soil samples were submitted for full laboratory analysis however only two soil properties were selected for this study soil ph and soc soil ph was determined using a 1 1 soil 0 01m cacl2 solution soc was determined using the combustion method kalembasa and jenkinson 1973 where total c and inorganic c were determined using a leco analyzer total c was determined from combustion of the soil sample while inorganic c was determined by first ashing the soil sample in a muffle furnace at 475 c for 3 h to remove the organic c fraction from the soil samples organic carbon was calculated as the difference between total carbon and inorganic carbon the two soil properties were modeled at two depth increments 0 5 cm and 30 60 cm soil profile data were harmonized from soil profile descriptions with analytical data to the standard depths using an equal area quadratic spline bishop et al 1999 as implemented in the ithir package malone 2017 some portions of the ottawa study area have shallow bedrock and truncated soil profiles for this reason for the 0 5 cm depth increment 1633 records were obtained for the training data however only 1531 records were obtained at the 30 60 cm depth increment for the 0 5 cm depth increment soil ph ranged from 3 1 to 7 7 and soc ranged from 0 01 to 48 1 table 1 the low ph and high soc values are associated with organic deposits in the study area the mean ph and soc for this shallow depth were 5 8 and 6 51 for the 30 60 cm depth increment soil ph ranged from 3 2 to 7 7 and soc ranged from 0 01 to 47 94 table 1 the mean values were 6 2 and 1 53 for soil ph and soc respectively the higher mean ph for the 30 60 cm depth resulted from the predominantly neutral to alkaline parent materials of the study area the lower mean soc for this depth indicated fewer organic deposits present at this depth and overall low soc in the parent materials in total 43 covariates were selected for the ottawa study and listed in appendix a table a2 six of the covariates were categorical represent bedrock geology quaternary surficial deposits soil order and crop or vegetation classification for the years 2016 2018 radiometric data was acquired and six covariates were created which included potassium thorium uranium and their ratios euclidean distance fields behrens et al 2018 were calculated to provide spatial context to the models the remaining covariates were derivatives of the lidar dem and were created in saga gis and whiteboxtools lindsay 2019 all covariates were prepared at a 10 m spatial resolution all continuous variables were centered and scaled and given the large number of predictors principal component analysis was applied for the training data used by the tree based learners qrf rf and cubist a principal component analysis pca was applied to the continuous data where the first 17 principal components pcs that consisted of 95 of the cumulative variance were retained in addition six categorical variables that represented soil type bedrock and surficial geology and satellite derived crop data were included for knn and svm the categorical variables were transformed through the encoding process therefore for the qrf rf and cubist models a total of 23 covariates were used 17 pcs 6 categorical whereas for the knn and svm models due to the encoding of categorical covariates a total of 53 covariates were used 17 pcs 36 encoded 3 4 machine learning models to demonstrate how the qr framework may be easily implemented for any machine learner four learners found in the dsm literature were used rf cubist knn and svm we recognize that there are many other learners that could be tested however a comprehensive comparison amongst ml techniques is beyond the scope of this study descriptions of these algorithms as applied to dsm may be found in publications such as heung et al 2016 and lamichhane et al 2019 as well as from the original sources related to rf breiman 2001 cubist quinlan 1992 1993 and svm cortes and vapnik 1995 in addition qrf was used to provide a comparison for the qr framework uncertainty maps it is important to note that qrf is an extension of rf and as such generates the same predictions within the proposed qr framework the hyperparameters that were optimized included the number of randomly selected predictors m try for rf and qrf the number of committees and neighbors for cubist the number of neighbors k for knn and the sigma and cost parameters for svm with radial basis function for each ml a range of hyperparameter values were tested and the optimal combination of values was selected based on the 10 fold cross validation process for the inner loop of the framework although the optimization of the hyperparameters could be computationally demanding the process may be made more efficient with bayesian optimization using the rbayesianoptimization package in r yan 2016 3 5 comparison to quantile regression forest a commonly used method for estimating prediction uncertainty in dsm studies is qrf for example vaysse and lagacherie 2017 compared regression kriging with qrf to test the ability of estimating the uncertainties of globalsoilmap net soil property grids their results showed that qrf provided more accurate predictions and also the spatial patterns of the uncertainty estimates were more easily interpretable than regression kriging subsequently szatmári and pásztor 2019 further tested the application of qrf for uncertainty estimation in dsm by comparing it to sequential gaussian simulation universal kriging variance rf regression kriging variance and bootstrapping of rf regression kriging variance where it was determined that methods using qrf and sequential gaussian simulation were superior when applying a qrf approach meinshausen 2006 explains that the weighted observations used for estimating the conditional mean in rf could also be used to approximate the full conditional distribution readers interested in the theoretical background of qrf should refer to meinshausen 2006 for additional details most importantly the key difference between rf and qrf is that for each terminal node of each tree rf retains only the mean of the predicted values that fall into each node whereas qrf also retains all predicted values within the terminal node and assesses the conditional distribution to estimate the prediction interval for the node meinshausen 2006 it is also important to reemphasize that qrf is fundamentally different than a coupling of rf and qr despite the similar terminology because the latter does not use information from the terminal nodes of rf to generate uncertainty estimates the qrf approach was implemented using the quantregforest package for the r statistical language 3 6 evaluation of accuracy uncertainty estimations for each outer loops of the qr framework the test data is used to evaluate the predictive model developed in the inner loop the outer loop is iterated 10 times once for each of the k outer folds thus validation statistics were compiled and averaged as described in section 2 3 1 the entire qr framework process can be repeated over multiple iterations for this study the qr framework was repeated 20 times validation statistics were therefore compiled averaged across all iterations and reported in the results the accuracy metrics used for this study included root mean square error rmse coefficient of determination r2 and lin s concordance correlation coefficient ccc the primary metric used for model selection and accuracy assessment was the ccc as it measures how closely the observed and predicted values fall along a 1 1 line lawrence and lin 1989 to summarize each model and its hyperparameter combination was iterated 2000 times whereby 10 fold cross validation was applied for each hyperparameter combination within a model inner loop 10 fold cross validation was applied to assess the uncertainty estimates using independent test data outer loop and 20 repeats of the nested cross validation procedure was applied to ensure the stability of the accuracy metrics through the 20 repeats the residual distribution for the test folds for each repeat was retained here mpi and picp plots were generated where the average mpi and picp values and their standard deviations were calculated over the range of confidence levels that were tested in addition this study also compared the proposed qr framework against a reference qrf approach for estimating uncertainty whereby mpi and picp plots were generated using a completely independent dataset i e test data for each of the k outer folds it is worth noting that the structure of the inner and outer loops may be adjusted according to the needs of the user by adjusting the number of folds or using an alternative cross validation approach such as leave one out cross validation or random holdback cross validation however the comparison of the different configurations of the nested cross validation procedure was beyond the scope of this study hence we selected k 10 fold which is routinely used 4 results discussion 4 1 kamloops study area the cubist model generally produced the best predictions and these are therefore shown in fig 3 however maps for each soil property and machine learner are provided in appendix b figs b1 b3 as expected soil thickness was predicted to be shallower at high elevations e g northeast and southwest of the study area and deeper soils were more common at lower elevations fig 3 soil ph maps for all four models showed a similar pattern of neutral to alkaline ph at lower elevations with a gradual transition to acidic ph values at higher elevations in the northeast and southwest fig 3 the high ph area aligned well with the central creek and flood plain areas in the study area there was a general increase in the depth to carbonates as elevation increased based on a visual assessment the resulting digital soil maps were consistent with our expert understanding of the soil patterns for this area 4 1 1 accuracy assessment overall the accuracy metrics for the soil thickness and soil ph results were promising the rf svm and cubist models had similar accuracies for soil thickness with ccc values ranging from 0 47 to 0 50 table 3 the rmse values were also similar for all models ranging from 103 to 105 cm the soil ph models also performed well with ccc ranging from 0 33 to 0 37 and rmse values ranging from 0 68 to 0 73 although the cubist model has the potential to predict values beyond the range of values found in the training data the accuracy metrics were still highest for all predicted soil properties across all models knn consistently performed the worst overall the depth to carbonates maps had the lowest accuracies regardless of the modelling approach used here it is important to recognize that given qrf is an extension of rf the same predicted maps are generated and hence the accuracy metrics are therefore the same for rf and qrf in table 3 because the nested cross validation process was repeated 20 times the summary statistics for all accuracy metrics across the repetitions are presented in appendix a table a3 when predicting soil thickness using svm negative values were predicted table 4 since the svm learner extrapolated values beyond the range of the training data similarly the cubist model extrapolated large positive numbers for soil thickness and depth to carbonates in both cases extreme values were very limited in distribution furthermore we chose not to restrict the predicted values to the range of values found in the training data to ensure that qr estimated the quantile values from the actual predicted values 4 1 2 uncertainty estimation maps of uncertainty estimates were produced for the 90 prediction intervals fig 4 and in addition picp plots fig 5 and mpi plots fig 6 were generated using the qr framework for the rf learner and the reference qrf approach for comparison all other maps and picp plots are provided in appendix b figs b4 b9 in general all models showed a similar spatial pattern of uncertainty however there were differences in the magnitude of the uncertainty although the accuracy metrics were similar amongst the rf cubist and svm model predictions the uncertainty maps were drastically different except for the soil ph predictions using the cubist model the mean uncertainty values between the learners were similar however the range and variability in uncertainty values differed considerably table 4 which led to the differences in spatial patterns the soil thickness uncertainty maps showed highest uncertainty at the lower elevations and in some pockets at higher elevations in the southwestern portion of the study area higher uncertainty was expected at these lower elevations because as the soil thickness increased at these elevations the more difficult it became to accurately measure the soil thickness in the field uncertainty was highest at high elevations for the depth to carbonates maps when using the rf knn and svm models while in contrast the cubist model showed the highest uncertainty at low elevations fig b5 the lower accuracy of the depth to carbonates maps at high elevations was expected because there were few opportunities to collect observations in these thinner soils coupled with the trend towards deeper carbonate depths there for the soil ph predictions uncertainty was lowest at high elevations found within the northeastern and southwestern regions of the study area the likely reason for the better model performance at higher elevation was the lack of soil carbonates in these shallow soils moreover a comparison was conducted between the two uncertainty estimation methods using the qr framework and qrf the uncertainty estimates for soil thickness using qrf showed a larger range of values when compared to qr coupled with the 4 ml models despite the larger width of the 90 pi with qrf the spatial patterns of uncertainty were quite comparable between qrf and the maps generated from the four ml techniques coupled with the qr framework the largest 90 pi width in the uncertainty maps for depth to carbonates was for the svm model which was similar to the uncertainty estimates generated using qrf fig 4 table 4 the qrf uncertainty map differed from the rf with qr framework uncertainty map with the latter having higher uncertainty over most of the study area the range of the 90 prediction interval values for qrf were again larger than those derived from using qr along with the four ml models fig 4 and table 4 however the qrf uncertainty map had the lowest overall uncertainty spatial trends in uncertainty for soil ph were similar across all maps the picp plots for the qr results showed optimal results across all soil properties and models where the picp values corresponded well to their respective confidence levels by repeating the nested cross validation procedure 20 times it was possible to assess the stability of the picp values overall the greatest variability in picp values occurred at confidence levels less than 50 however the extent of the variability was not particularly large at the 90 confidence level the picp values were optimal and showed little variability when compared to the reference approach using qrf the picp for qrf differed from the picp for the four ml models and show an overestimation of the pi width for soil thickness as indicated by the points above the bisector lines in fig 6 in comparison the overestimation of the pi width when modeling depth to carbonates and soil ph was less apparent the mpis for all models were also similar for soil thickness depth to carbonates and soil ph fig 6 in general the qrf and rf models had slightly lower mpi than cubist knn and svm however the differences were generally marginal e g depth to carbonates at the 95 confidence level overall these results indicate that in this landscape the qr approach provided uncertainty estimates that were reliable and stable regardless of the different ml techniques and soil properties furthermore the qr approach also yielded results that were comparable to or slightly better than the reference qrf approach 4 2 ottawa study area the cubist model generally produced the best predictions and the maps generated using this model are therefore shown in fig 7 however maps for each soil property and machine learner are provided in appendix c figs c1 c4 the soil ph maps for all four models at both depths showed a similar pattern of neutral to alkaline ph in the south and west portions of the study area with a gradual transition to acidic ph values in the northeast and northwest the low ph areas aligned well with known landscape features the carp ridge in the northwest which is of precambrian origin with acidic reaction and the mer bleue organic deposits in the northeast which are a combination of deep organic deposits and coarse sandy deposits located in glacial spillway channels in terms of soc the four models produced similar maps of soc distribution except for the knn model which failed to map organic deposits at the southern region of the study area other than this the soc maps were consistent with observations from the field work especially with regards to the large organic deposits 4 2 1 accuracy assessment the cubist model outputs for both depth intervals showed the largest range in soil ph and soc as indicated from the kamloops study cubist predictions were expected to extend outside the range of the input data similar to results from the kamloops study the cubist model was consistently the best performing model when considering ccc ranging from 0 56 to 0 64 across the two soil properties at the two modeled depths table 3 the rf knn and svm models had similar performance except for the knn model for soc 30 60 cm which performed less effectively ccc 0 50 rmse did not provide much insight into model performance with very little difference between the models for soil ph prediction both depths whereas for soc the knn model had the highest rmse for the 0 5 cm depth and for the 30 60 cm depth similar to the results from the kamloops study area svm predicted negative values for soc however the distribution of those negative values was also limited table 4 summary statistics for the accuracy metrics are presented in appendix a table a3 4 2 2 uncertainty estimation uncertainty estimates were assessed by deriving 90 prediction intervals fig 8 and generating picp plots fig 9 and mpi plots fig 10 using the qr framework for the cubist learner and the reference qrf approach all other maps and picp plots are provided in appendix c figs c5 c12 in general all models showed the same general spatial pattern of uncertainty however there were differences in the magnitude of the uncertainty for the soil ph maps at both depths the uncertainty was lowest in the southern and western regions of the maps and increased primarily in the northeast rf and cubist predictions had lower uncertainty than the knn and svm models especially when comparing the northeast sections of the maps this was most apparent in the soil ph maps for the 30 60 cm depth despite the similar range of the uncertainty between the upper and lower depths the differences in the magnitude of the uncertainty was more apparent across the four ml models at both prediction depths the knn model showed the largest areas of lowest and highest uncertainty the soil ph uncertainty maps generated from the qrf as compared to those from the four ml models using the qr framework showed some drastic differences fig 9 in particular the qrf uncertainty estimates at both depths showed the largest range in the prediction intervals furthermore the qrf uncertainty estimate maps were much more variable across the study area in comparison to the four qr uncertainty maps which showed gradual transitions in the uncertainty aligning well with the predictions fig 7 the soc prediction uncertainty maps showed the highest uncertainty in small pockets scattered throughout the maps which aligned well with areas of organic deposits whereby the organic carbon ranged from 17 to 58 from a visual inspection of the maps cubist seems to have had the highest uncertainty along the large organic deposits in the southwest and northeast sections of the map the soc prediction uncertainty for svm showed a consistently higher uncertainty across the entire study area at both prediction depths suggesting that svm had difficulty making accurate predictions at low concentrations of soc in contrast to the soil ph uncertainty maps from qrf the soc uncertainty maps generated from the qrf were more consistent with those generated using the qr framework however the qrf uncertainty map for the 0 5 cm depth showed consistently higher uncertainty in the southwestern and western regions of the study area fig 8 like the kamloops study area the picp plots for the ottawa study area showed a near 1 1 relationship between picp and confidence level when using qr fig 9 figs c9 c12 furthermore there was far less variability in terms of the picp values across the 20 replicates a result that may be partially due to the ottawa study area having a far larger number of sample observations once again these results indicated that the qr approach to uncertainty estimation was reliable and was not dependent on the modelling approach used when comparing the picp plots generated from the qrf model to plots generated using qr for soil ph there was a marginal overestimation of the pi width when qrf was used similar to the kamloops study area the mpi increased as the confidence level increased the mpis values were similar between all the models for soil ph at both depths fig 10 in general the cubist qrf and rf models had slightly lower mpi than knn and svm the mpi data for the soc predictions showed larger differences between models and more variability for the soc 0 5 cm depth the models all had similar mpi up to the 80 confidence level at which point the cubist qrf and rf models started to differ from the knn and svm models the knn and svm models consistently had larger mpis suggesting that as the level of confidence in the predictions increases these models did not perform as well as the tree based learners at the 30 60 cm depth mpis for the soc predictions showed similar trends as the confidence level increased the cubist qrf and rf models generated lower mpis this was only true for the 98 and 99 confidence levels and cubist had the lowest mpi starting at the 80 confidence level in some cases the differences between the models were significant however the magnitudes were negligible by generating a scatterplot of the observed versus predicted values and superimposing the qr functions at different confidence levels fig 11 for soil ph 0 5 cm and soc 0 5 cm using the cubist model we observed crossing qr lines for soc but not for soil ph the crossing qr lines also explain the occurrence of negative uncertainty estimates that are shown in table 4 this phenomenon has been observed in the qr literature bondell et al 2010 and is largely related to the distribution of the data the distribution of ph values was much narrower and more normally distributed than for soc and thus the fitted qr lines were closer together and more importantly parallel the parallel qr lines indicate that the prediction interval remained fairly constant in comparison it was previously established that the soc data had a high positive skewness because of the inclusion of organic deposits this resulted in a funnel shaped scatterplot and the resulting qr functions derived from these observations showed continuous divergence with increasing confidence levels here the prediction interval width increased as the soc increased furthermore a consequence of this effect was the estimation of negative pi values in areas with very low soc values transformation of the dependent variable to normalize the distribution of the observed values prior to modelling could be a reasonable approach to address this issue attempts to address this potential issue were beyond the scope of this study however studies such as bondell et al 2010 have proposed a constrained qr approach for linear and nonparametric quantile curves 4 3 general discussion future work given that the intention of this study was to demonstrate the novel integration of ml and qr within a dsm framework it was necessary to provide this demonstration using two unique case studies to assess the generalizability of the approach in both case studies the resulting picp plots were optimal where the confidence level and picp showed a 1 1 relationship regardless of study area soil attribute and machine learner although we followed the standard approach for assessing uncertainty estimates in the ml qr and dsm literature ding et al 2018 malone et al 2011 muthusamy et al 2016 shrestha and solomatine 2006 by using picp and mpi recent studies such as szatmári and pásztor 2019 have suggested the use of the g statistic deutsch 1997 which quantifies how close the picp values are to their associated confidence levels although there were differences in the choice of environmental covariates and the application of pca for the ottawa study area we do not believe this to impact our findings furthermore though it is recognized that the use of opportunistic sampling may not seem like an optimal approach by some we do not anticipate this to be an issue when the core objective was to test an approach for estimating uncertainty future research may investigate the use of different sampling approaches and combinations of environmental covariates to minimize the uncertainty estimates while maximizing accuracy related to the sampling approaches this study did not seek to identify the minimum number of sample locations required for a reliable application of the proposed method hence future work investigating how the qr approach as well as other uncertainty estimation approaches respond by systematically reducing the sampling density would be informative from the ottawa study area we raised a potential issue related to qr generating negative estimates of uncertainty and we had attributed it to crossing qr lines bondell et al 2010 which may be related to the distribution of soil attribute values and the model residuals furthermore we also recognize that the assumption of a linear relationship between the observed and predicted values may be a weakness of this approach however nonparametric quantile curves could be a potential solution bondell et al 2010 these issues warrant further investigation as applied to a dsm context 4 3 1 quantile regression forest we also demonstrated that the qr framework generates uncertainty estimates that are comparable to those generated from qrf this was true across both study areas and all soil properties generally the spatial patterns of uncertainty were quite similar between the qr framework and qrf outputs however the qrf uncertainty typically had the largest range this demonstrates that the qr framework generates valid estimates of uncertainty however the advantage is that the qr framework allows the user to integrate any ml model not just rf although the focus of this study was to propose a novel qr framework the evaluation of qrf also yielded some interesting observations throughout both the kamloops and ottawa study areas we observed the tendency of qrf to marginally overestimate the pi width which was most apparent when predicting ph when comparing this study with vaysse and lagacherie 2017 where ph organic carbon and clay were mapped the picp plots for qrf showed the same tendency to marginally overestimate the pi width whereby the overestimation was also the most apparent for soil ph we do not have an explanation for this observation and we are also unclear if this is more than a coincidence with respect to estimating the uncertainty of soil ph predictions in szatmári and pásztor 2019 the qrf approach was similarly applied for estimating the uncertainty of soc predictions and again there appeared to be a marginal tendency to overestimate the uncertainty lastly fouedjio and klump 2019 evaluated the qrf approach using a suite of synthetic and real world spatial data and the tendency to overestimate the uncertainty was more apparent than those observed in this study and vaysse and lagacherie 2017 and szatmári and pásztor 2019 when collectively evaluating the results from this study with vaysse and lagacherie 2017 szatmári and pásztor 2019 and fouedjio and klump 2019 it appears as though these marginal overestimations occur between a confidence level range of 40 80 within the picp plots 4 3 2 future research given that this is the first implementation of the qr approach for dsm there are several areas for future research first the generalizability of the approach should be further investigated for other study areas soil properties spatial scales and machine learners although this study specifically tested the integration of qr and ml ml may be substituted with other types of predictive models such as a purely geostatistical model e g ordinary kriging or a hybrid model e g regression kriging given that the main inputs for qr are the model residuals such a comparison would be useful especially since kriging approaches provide an alternative for uncertainty estimation using the kriging variance in terms of existing ml based approaches for uncertainty estimation a comparison between the qr and bootstrapping approaches would be warranted for example szatmári and pásztor 2019 compared the uncertainty estimates produced from the kriging variance from universal kriging and rf regression kriging with sequential gaussian simulation sgs qrf and bootstrapping of rf regression kriging there it was shown that qrf and sgs were more optimal in estimating uncertainty however they also indicated that qrf and sgs were computationally demanding it is also necessary to note that our integration of rf and qr is fundamentally different from the qrf approach in qrf all predictions made at the terminal node across all individual trees are retained following this the residual distribution produced by the individual trees for each terminal node is then used to calculate and predict the quantiles meinshausen 2006 although qrf is the primary example of a ml technique that generates uncertainty estimates in the dsm literature e g rudiyanto et al 2018 szatmári and pásztor 2019 vaysse and lagacherie 2017 other types of ml techniques such as quantile regression neural networks cannon 2011 could be tested and compared in future work a potential issue with the bootstrapping approach malone et al 2017 is that increasing the number of bootstrap predictions would lead to a decrease in the prediction variance and thereby cause the narrowing of the prediction interval maps through a comparative study if qr yields similar or better results than bootstrapping this framework could overcome the major computational demands of bootstrapping which requires a spatial prediction for each model iteration although we did not carry out a direct comparison with bootstrapping with respect to computational time our personal experience was that the computationally demanding part of the bootstrapping process was in generating multiple e g 100 realizations of a soil property map i e applying a model to a covariate stack and having to store each realization in this proposed framework only one realization of a soil property map is generated as well as one realization of each quantile map e g 95 and 5 the uncertainty maps are generated by applying the qr function fitted using only the residual distribution from the observed sites and applied to the predicted soil map and hence not requiring multiple map realizations to be generated this solution would be particularly valuable when applied over large spatial extents e g national and global mapping initiatives when using ultra high resolution datasets e g lidar and when there are other big data challenges to overcome for example the most recent global scale mapping effort was the soilgrids250m hengl et al 2017 whereby the authors avoided the modelling of uncertainty for continuous soil variables and have indicated that qrf was a computationally intensive process especially with increasing data volumes by may 4 2020 soilgrids250m was updated to include uncertainty estimations using qrf 5 conclusions this study provides a generic framework for integrating ml techniques with qr for the purposes of producing uncertainty estimates for dsm thus far bootstrapping approaches have been the most common method for estimating uncertainty when using ml approaches however bootstrapping is a computationally demanding process using qr we proposed a computationally efficient alternative to uncertainty estimation that may overcome future challenges associated with big data although this study only tested some commonly used machine learners the overall approach may be applied to less commonly used learners as a well as geostatistical approaches by testing the approach on different study areas with different covariates sample sizes and soil attributes the picp and mpi plots show that qr can produce consistent and stable results although we recognize that further testing of the framework is required these results show considerable promise and the preliminary findings provide a novel contribution well beyond dsm and may be used for a variety of different spatial modelling applications declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments for the kamloops study area the authors acknowledge the financial support that was provided by the forest enhancement society of british columbia the forest innovation program of the canadian wood fibre centre natural resources canada british columbia ministry of forests lands natural resource operations and rural development contributors to field and lab activities include jin zhang doug terpsma heather richardson marc levesque joseph desmarais lauren foote natasha romalo yesheng chen emily koopmans justin walch and adrienne arbor for the ottawa study area the authors acknowledge the financial support that was provided by ontario ministry of agriculture food and rural affairs through the growing forward 2 program contributors to field and lab activities include jim warren adam gillespie stephanie vickers mackenzie clarke and veronika wright appendix a additional tables table a1 soil environmental covariates for the kamloops study area table a1 representation environmental covariate resolution map scale topography hillshade illumination 3 m diurnal anisotropic heating 3 m digital elevation model 3 m standardized height 3 m multiresolution valley bottom flatness index gallant and dowling 2003 3 m planform curvature 3 m profile curvature 3 m topographic openness yokoyama et al 2002 3 m slope 3 m slope height 3 m topographic wetness index beven and kirkby 1979 3 m valley depth 3 m vegetation biogeoclimatic ecosystem classification subzone 1 20 000 geology bedrock geology nutrient cui et al 2017 1 50 000 1 250 000 bedrock geology texture cui et al 2017 1 50 000 1 250 000 table a2 soil environmental covariates for the ottawa study area table a2 representation environmental covariate resolution map scale topography elevation 10 m deviation from maximum elevation 550m 10 m deviation from maximum elevation 1300m 10 m deviation from maximum elevation 4200m 10 m deviation from mean elevation 100m 10 m deviation from mean elevation 550m 10 m deviation from mean elevation 1300m 10 m deviation from mean elevation 4200m 10 m convergence index kothe and lehmeier 1996 10 m general curvature 10 m hillshade 10 m slope length factor moore et al 1993 10 m multi resolution ridge top flatness gallant and dowling 2003 10 m multi resolution valley bottom flatness gallant and dowling 2003 10 m normalized height 10 m profile curvature 10 m relative slope position 10 m slope height 10 m slope percent 10 m stream power index moore et al 1991 10 m saga wetness index 10 m topographic position index guisan et al 1999 10 m topographic ruggedness index riley et al 1999 10 m valley depth 10 m vegetation crop 2016 agriculture and agri food canada 2018 30 m crop 2017 agriculture and agri food canada 2018 30 m crop 2018 agriculture and agri food canada 2018 30 m geology bedrock geology ontario geological survey 2017 1 250 000 bedrock geology ontario geological survey 2017 1 100 000 radiometric k natural resources canada 2019 80 1000 m radiometric th natural resources canada 2019 80 1000 m radiometric u natural resources canada 2019 80 1000 m radiometric u k ratio natural resources canada 2019 80 1000 m radiometric u th ratio natural resources canada 2019 80 1000 m radiometric th k ratio natural resources canada 2019 80 1000 m spatial position distance from mid behrens et al 2018 10 m distance from ne behrens et al 2018 10 m distance from nw behrens et al 2018 10 m distance from se behrens et al 2018 10 m distance from sw behrens et al 2018 10 m distance from x max behrens et al 2018 10 m distance from y max behrens et al 2018 10 m soil soil order omafra 2019 1 50 000 table a3 summary statistics of accuracy metrics from 20 repeats of the nested cross validation procedure table a3 study area soil property descriptive statistic random forest quantile regression forest cubist k nearest neighbor support vector machine r2 ccc rmse r2 ccc rmse r2 ccc rmse r2 ccc rmse kamloops soil thickness cm minimum 0 32 0 46 101 32 0 29 0 47 104 81 0 25 0 36 107 64 0 27 0 45 103 74 mean 0 35 0 47 103 19 0 32 0 50 106 15 0 27 0 37 108 44 0 31 0 48 105 39 median 0 35 0 47 103 09 0 32 0 50 106 07 0 27 0 37 108 46 0 32 0 48 105 14 maximum 0 39 0 49 104 68 0 34 0 51 108 54 0 28 0 38 109 38 0 34 0 49 108 57 standard deviation 0 01 0 01 0 85 0 01 0 01 1 00 0 01 0 00 0 40 0 01 0 01 1 05 depth to carbonates cm minimum 0 03 0 10 29 73 0 00 0 12 29 63 0 09 0 13 29 15 0 05 0 12 29 33 mean 0 07 0 13 30 04 0 05 0 16 30 57 0 10 0 15 29 42 0 08 0 15 29 70 median 0 07 0 14 29 97 0 05 0 16 30 51 0 10 0 15 29 43 0 09 0 15 29 64 maximum 0 13 0 15 30 68 0 11 0 19 31 57 0 12 0 16 29 58 0 11 0 17 30 07 standard deviation 0 03 0 01 0 26 0 03 0 02 0 49 0 01 0 01 0 09 0 02 0 01 0 21 ph 0 30 cm minimum 0 23 0 34 0 66 0 14 0 32 0 69 0 09 0 14 0 73 0 12 0 29 0 67 mean 0 26 0 37 0 68 0 20 0 36 0 72 0 11 0 16 0 73 0 18 0 33 0 70 median 0 26 0 37 0 68 0 20 0 35 0 72 0 12 0 16 0 73 0 18 0 33 0 70 maximum 0 32 0 40 0 69 0 24 0 41 0 74 0 14 0 18 0 74 0 24 0 38 0 72 standard deviation 0 02 0 01 0 01 0 03 0 03 0 02 0 01 0 01 0 00 0 03 0 02 0 01 ottawa ph 0 5 cm minimum 0 37 0 51 0 70 0 35 0 56 0 70 0 33 0 49 0 72 0 36 0 55 0 70 mean 0 37 0 51 0 70 0 36 0 56 0 71 0 34 0 50 0 72 0 37 0 55 0 70 median 0 37 0 51 0 70 0 36 0 56 0 71 0 34 0 50 0 72 0 37 0 55 0 70 maximum 0 38 0 52 0 70 0 37 0 57 0 71 0 35 0 50 0 72 0 38 0 56 0 71 standard deviation 0 00 0 00 0 00 0 01 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 ph 30 60 cm minimum 0 43 0 57 0 61 0 41 0 61 0 62 0 40 0 59 0 62 0 42 0 59 0 61 mean 0 44 0 58 0 61 0 42 0 62 0 62 0 41 0 60 0 63 0 43 0 60 0 62 median 0 44 0 58 0 61 0 42 0 62 0 62 0 41 0 60 0 63 0 43 0 60 0 62 maximum 0 44 0 58 0 62 0 43 0 62 0 63 0 42 0 60 0 63 0 44 0 61 0 62 standard deviation 0 00 0 00 0 00 0 01 0 00 0 00 0 01 0 00 0 00 0 00 0 00 0 00 soc 0 5 cm minimum 0 44 0 60 6 89 0 40 0 62 6 89 0 35 0 56 7 36 0 41 0 58 7 10 mean 0 46 0 61 6 94 0 44 0 64 7 00 0 37 0 57 7 46 0 42 0 59 7 15 median 0 46 0 61 6 94 0 45 0 64 6 98 0 37 0 57 7 47 0 42 0 59 7 14 maximum 0 47 0 62 6 99 0 46 0 65 7 22 0 39 0 59 7 55 0 44 0 60 7 22 standard deviation 0 01 0 00 0 03 0 02 0 01 0 08 0 01 0 01 0 07 0 01 0 01 0 03 soc 30 60 cm minimum 0 83 0 47 3 90 0 66 0 55 3 89 2 56 0 42 4 10 1 78 0 47 4 08 mean 0 29 0 53 4 01 0 13 0 59 4 06 0 01 0 51 4 23 0 06 0 53 4 25 median 0 36 0 53 4 02 0 27 0 59 4 07 0 23 0 50 4 22 0 24 0 53 4 26 maximum 0 45 0 57 4 11 0 40 0 62 4 26 0 38 0 56 4 47 0 39 0 58 4 50 standard deviation 0 27 0 02 0 06 0 33 0 02 0 09 0 67 0 04 0 09 0 49 0 04 0 11 appendix b kamloops study area fig b1 soil thickness maps generated using random forest cubist k nearest neighbors and support vector machines at a 3 m spatial resolution for the kamloops study area fig b1 fig b2 depth to carbonates maps generated using random forest cubist k nearest neighbors and support vector machines at a 3 m spatial resolution for the kamloops study area fig b2 fig b3 soil ph maps generated using random forest cubist k nearest neighbors and support vector machines at a 3 m spatial resolution for the kamloops study area fig b3 fig b4 uncertainty maps for soil thickness predictions showing the 90 prediction interval generated using quantile regression forest random forest with quantile regression cubist k nearest neighbors and support vector for the kamloops study area fig b4 fig b5 uncertainty maps for depth to carbonates predictions showing the 90 prediction interval generated using quantile regression forest random forest with quantile regression cubist k nearest neighbors and support vector for the kamloops study area fig b5 fig b6 uncertainty maps for soil ph predictions showing the 90 prediction interval generated using quantile regression forest random forest with quantile regression cubist k nearest neighbors and support vector for the kamloops study area fig b6 fig b7 prediction interval coverage probability picp plots for uncertainty estimates of soil thickness predictions using quantile regression forest qrf random forest with quantile regression rf cubist k nearest neighbors knn and support vector machines svm for the kamloops study area fig b7 fig b8 prediction interval coverage probability picp plots for uncertainty estimates of depth to carbonates predictions using quantile regression forest qrf random forest with quantile regression rf cubist k nearest neighbors knn and support vector machines svm for the kamloops study area fig b8 fig b9 prediction interval coverage probability picp plots for uncertainty estimates of soil ph predictions using quantile regression forest qrf random forest with quantile regression rf cubist k nearest neighbors knn and support vector machines svm for the kamloops study area fig b9 appendix c ottawa study area fig c1 soil ph 0 5 cm maps generated using random forest cubist k nearest neighbors and support vector machines at a 10 m spatial resolution for the ottawa study area fig c1 fig c2 soil ph 30 60 cm maps generated using random forest cubist k nearest neighbors and support vector machines at a 10 m spatial resolution for the ottawa study area fig c2 fig c3 soil soc 0 5 cm maps generated using random forest cubist k nearest neighbors and support vector machines at a 10 m spatial resolution for the ottawa study area fig c3 fig c4 soil soc 30 60 cm maps generated using random forest cubist k nearest neighbors and support vector machines at a 10 m spatial resolution for the ottawa study area fig c4 fig c5 uncertainty maps for soil ph 0 5 cm predictions showing the 90 prediction interval generated using quantile regression forest random forest with quantile regression cubist k nearest neighbors and support vector for the ottawa study area fig c5 fig c6 uncertainty maps for soil ph 30 60 cm predictions showing the 90 prediction interval generated using quantile regression forest random forest with quantile regression cubist k nearest neighbors and support vector for the ottawa study area fig c6 fig c7 uncertainty maps for soil soc 0 5 cm predictions showing the 90 prediction interval generated using quantile regression forest random forest with quantile regression cubist k nearest neighbors and support vector for the ottawa study area fig c7 fig c8 uncertainty maps for soil soc 30 60 cm predictions showing the 90 prediction interval generated using quantile regression forest random forest with quantile regression cubist k nearest neighbors and support vector for the ottawa study area fig c8 fig c9 prediction interval coverage probability picp plots for uncertainty estimates of soil ph 0 5 cm predictions using quantile regression forest qrf random forest with quantile regression rf cubist k nearest neighbors knn and support vector machines svm for the ottawa study area fig c9 fig c10 prediction interval coverage probability picp plots for uncertainty estimates of soil ph 30 60 cm predictions using quantile regression forest qrf random forest with quantile regression rf cubist k nearest neighbors knn and support vector machines svm for the ottawa study area fig c10 fig c11 prediction interval coverage probability picp plots for uncertainty estimates of soc 0 5 cm predictions using quantile regression forest qrf random forest with quantile regression rf cubist k nearest neighbors knn and support vector machines svm for the ottawa study area fig c11 fig c12 prediction interval coverage probability picp plots for uncertainty estimates of soc 30 60 cm predictions using quantile regression forest qrf random forest with quantile regression rf cubist k nearest neighbors knn and support vector machines svm for the ottawa study area fig c12 
25758,digital soil mapping dsm techniques have provided soil information that has revolutionized soil management across multiple spatial extents and scales dsm practitioners have been increasingly reliant on machine learning ml techniques yet methods to generate uncertainty maps from ml predictions are limited to address this issue this study integrates ml based dsm with quantile regression qr in a methodological framework for estimating uncertainty we test the proposed framework on two case study areas in canada 1 a dry forest ecosystem in the kamloops region of british columbia canada and 2 an agricultural system in the ottawa region of ontario canada four ml techniques random forest cubist decision tree k nearest neighbors and support vector machine were compared using repeated cross validation maps showing the 90 prediction interval pi were produced regardless of the case study ml approach and predicted soil variable the uncertainty estimates were reliable and stable according to the pi coverage probability analysis keywords digital soil mapping machine learning uncertainty estimation quantile regression 1 introduction over the past decades the soil science community has witnessed the emergence of digital soil mapping dsm techniques in transforming our understanding of the spatial patterns of soils these maps are designed to provide decision makers with accurate and precise information which is necessary to practice sustainable soil management mitigate soil degradation and improve food security sanchez et al 2009 while the uses of digital soil maps are cross sectorial and its benefits are multi dimensional it is also well recognized that the maps produced using dsm techniques are not error free not only is it necessary to evaluate the accuracy of predictions using validation data within a dsm framework it is also necessary to quantitatively estimate and spatially represent the uncertainty so modellers and decision makers can assess the usefulness of a map folberth et al 2016 heuvelink 1998 mcbratney et al 2003 common forms of error may arise from measurements digitisation data entry interpretation classification generalisation and interpolation arrouays et al 2014b uncertainty in digital soil maps may arise from modeling bias parameterization or even measurement errors associated with the input data minasny and mcbratney 2002 to provide a better understanding of uncertainty nelson et al 2011 suggests performing an error budget to examine the contribution of each error by using a combination of geostatistical and monte carlo simulations it is also important to consider the difference between model error and spatially explicit uncertainty model error often measured as the mean square error mse is the average squared difference between the estimated value and the actual value malone et al 2011 wang and bovik 2009 however spatially explicit uncertainty sometimes referred to as local error refers to the quantification of prediction intervals for a model output feizizadeh et al 2014 malone et al 2011 vaysse and lagacherie 2017 based on the importance of spatially explicit uncertainty quantification in dsm the inclusion of uncertainty layers is often specified in international standards examples of these standards may require the inclusion of the 90 prediction interval pi according to the globalsoilmap net specifications arrouays et al 2014a or the inclusion of prediction standard deviation maps according to global scale digital soil maps produced by the food and agriculture of the united nations fao and gsp 2017 despite these specifications measuring uncertainty in dsm remains challenging and is generally uncommon minasny and mcbratney 2002 vaysse and lagacherie 2017 within the dsm literature there are many methods for producing spatially explicit estimates of uncertainty geostatistical modelling approaches for example can produce uncertainty estimates using the kriging prediction variance where spatial representations of the prediction intervals may be calculated however the uncertainty estimates are specific to the modelling approach itself and not applicable for machine learning techniques which are more commonly used now fouedjio and klump 2019 malone et al 2017 alternatively studies such as karunaratne et al 2014 have used conditional sequential gaussian simulation an approach that has been applied in geostatistical techniques to generate a suite of equally probable map realizations the bootstrapping technique provides another approach for uncertainty estimation ackerson et al 2015 padarian et al 2017 stumpf et al 2017 thomas et al 2015 where a model is trained on a random subset of the full training data and multiple realizations of a soil map are produced through the prediction process by generating these realizations uncertainty estimates are produced by calculating the average mse from the realizations and summing it with the bootstrap prediction variance that is estimated for each pixel malone et al 2017 however computational capabilities may limit the use of this approach when applied to large datasets since the full extent of each map realization needs to be predicted to estimate the prediction variance other uncertainty assessment methods also have limitations for example the bayesian method only measures uncertainty associated with input data while the monte carlo method specifically measures uncertainty in parameters solomatine and shrestha 2009 although the literature review primarily focusses on techniques for evaluating uncertainty when predicting continuous response variables spatial estimates of uncertainty may also be produced for categorical response variables using fuzzy inference qi and zhu 2011 zhu 1997 and bootstrapping chaney et al 2016 heung et al 2017 when concerning categorical variables uncertainty is estimated using metrics such as ignorance uncertainty goodchild et al 1994 leung et al 1992 exaggeration uncertainty zhu 1997 or confidence index burrough et al 1997 the use of machine learning ml techniques for regression purposes has become increasingly common in dsm and this is often tied to the increasing power of computational systems rossiter 2018 some of the ml techniques include random forest rf e g grimm et al 2008 mancini et al 2019 cubist decision tree e g adhikari et al 2019 k nearest neighbors knn merchant et al 2018 and support vector machine svm e g mancini et al 2019 merchant et al 2018 despite the popularity of ml techniques many ml techniques are not capable of generating localized uncertainty maps szatmári and pásztor 2019 vaysse and lagacherie 2017 veronesi and schillaci 2019 among all the ml methods used in dsm quantile regression forest qrf is one of the few that has a built in mechanism for uncertainty quantification qrf meinshausen 2006 is an extension of rf breiman 2001 and is used to calculate the full conditional distribution of a prediction rather than only the conditional mean fouedjio and klump 2019 meinshausen 2006 given some of the limitations of existing approaches a knowledge gap is apparent with regards to uncertainty estimations when using ml techniques hence the objectives of this study are as follows 1 to propose a generic framework using a quantile regression qr approach for estimating the uncertainty of digital soil maps produced from ml 2 to test the framework using common ml techniques for two case studies in contrasting landscapes from the kamloops british columbia and the ottawa ontario regions of canada 3 to assess the uncertainty estimates using metrics such as mean prediction intervals mpi and prediction interval coverage probability picp analysis and 4 to compare the proposed qr framework with an established technique such as qrf 2 theoretical background this section provides an overview of the qr approach methods for evaluating the uncertainty estimates and proposes how qr may be integrated into a generic dsm framework 2 1 on quantile regression quantile regression qr was first introduced by koenker and bassett 1978 and originally appeared in the field of quantitative economics however its use has since been extended to other applications within the soil science literature outside of dsm qr has been used for estimating the uncertainty of models used for assessing nitrate contamination of groundwater using different ml techniques rahmati et al 2019 in terms of the dsm literature however relatively few studies have explored this approach with the exception of lombardo et al 2018 who demonstrated the coupling of a generalized linear model approach with qr for predicting soil organic carbon soc however such couplings may be potentially extended to other ml techniques as part of a generic framework when applying qr for uncertainty estimation we assume that there is a linear relationship between a target variable s observed value and its model predicted value qr consists of a set of linear regression models where the response variable is the selected quantile of the variable s conditional distribution within the hydrological modelling literature dogulu et al 2015 lopez et al 2014 rahmati et al 2019 qr is applied as a post processing technique whereby the prediction of the response variable is dissociated from the uncertainty estimation process because of this dissociation there is the added flexibility in the choice of the predictive model because the uncertainty estimates are generated using the model residuals qr estimates the value for a target variable for any quantile that is needed koenker and hallock 2001 which may then be used to calculate prediction intervals e g 90 using the upper 95 and lower 5 quantile maps here a brief description of qr is provided however detailed descriptions may be found in lópez lópez et al 2014 this framework applies the same qr framework to that of lópez lópez et al 2014 dogulu et al 2015 and rahmati et al 2019 for each quantile τ we assumes a linear relationship between observed values y and predicted values y ˆ 1 y a τ y ˆ b τ where a τ is the slope and b τ is the intercept of the linear regression here a τ and b τ are both determined by minimizing the sum of residuals in the following loss function 2 m i n j 1 j ρ τ y j a τ y ˆ j b τ where y j and y ˆ j are the jth paired samples i e soil measurements with a total of j samples and ρ τ is the qr function for the τ th quantile 3 ρ τ j τ 1 j j 0 τ j j 0 where the model residuals j are the difference between the observed and predicted values acquired from eq 1 for the τ th quantile the qr function is applied to the residual j in eq 3 for the desired quantile τ lópez lópez et al 2014 dogulu et al 2015 rahmati et al 2019 2 2 evaluation of uncertainty estimates the uncertainty estimates may be evaluated using a combination of prediction interval coverage probability picp and mean prediction interval mpi graphs picp graphs sometimes referred to as accuracy plots are used to assess the performance of quantile regression in terms of uncertainty quantification by evaluating the encapsulation of observation values into an associated prediction interval goovaerts 2001 at each particular confidence level we should expect that the same percentage of observations equal to the associated confidence level is encapsulated by the pi for example it is expected that about 90 of the observations used for model validation will fall within the 90 pi here picp is calculated as follows 4 p i c p 1 n τ 1 n c c 1 p τ l o w e r l i m i t y τ p τ u p p e r l i m i t 0 where y τ is the observed value and p τ l o w e r l i m i t and p τ u p p e r l i m i t are the predicted lower and upper limit of the soil variable rahmati et al 2019 therefore to assess the sensitivity of qr uncertainty estimates pis for a series of confidence levels are defined and picp is then calculated for each confidence level ideally the picp should correspond to the confidence level and show a 1 1 relationship malone et al 2017 if picp is less than the confidence level the uncertainty has been underestimated and if picp is greater than the confidence level the uncertainty has been overestimated szatmári and pásztor 2019 mpi is calculated as the average width of the prediction intervals and may be used to quantify overall map uncertainty a wider prediction interval represents higher uncertainty while a narrower prediction interval represents lower uncertainty ding et al 2018 rahmati et al 2019 mpi is calculated as follows 5 m p i 1 n τ 1 n p τ u p p e r l i m i t p τ l o w e r l i m i t when comparing models with similar picp the model with the lower mpi is regarded as the better model muthusamy et al 2016 3 methodology based on the theoretical background we developed a framework that evaluates the performance of the uncertainty estimates using picp and mpi section 3 1 1 and generates maps of predictions and their corresponding uncertainty estimates section 3 1 2 to test the performance of the proposed framework it was applied to two case studies from contrasting environmental systems dry inland forests of kamloops british columbia section 3 2 and the agriculturally dominated landscapes of ottawa ontario section 3 3 in both study areas different sets of environmental predictors were used to predict different soil attributes to evaluate the performance of qr as a post processing technique in machine learning various machine learners were compared section 3 4 furthermore the performance of the framework which couples qr to the outputs of machine learning models was compared to a reference approach using quantile regression forest section 3 5 lastly the accuracy of model predictions and the performance of the uncertainty estimates were evaluated using independent test data section 3 6 3 1 integration of quantile regression for predictive mapping the proposed framework consists of two phases phase 1 testing the predictive model and uncertainty estimates and phase 2 generating predictive maps and uncertainty maps fig 1 all modelling activities were carried out using the r statistical language and the relevant packages are listed in table 1 3 1 1 testing the predictive model and uncertainty estimations in phase 1 the objective is to ascertain the accuracy of the ml model using goodness of fit metrics and to quantitatively evaluate the uncertainty estimates using picp eq 4 and mpi eq 5 graphs here the process consists of seven steps where the only input is a matrix that consists of the observed soil attribute value and the corresponding covariate values for each sample location this is acquired by spatially intersecting the geographical position of each sample point with a suite of environmental covariates representing the scorpan factors mcbratney et al 2003 to generate estimates of model accuracy picp and mpi a nested cross validation procedure is applied which consists of an outer loop and an inner loop the inner loop steps 2 5 calibrates and selects the predictive model with the optimal combination of model hyperparameters using the validation data the outer loop steps 1 7 assesses the ml model s accuracy and estimates the uncertainty in step 1 the matrix with the soil environmental data is randomly partitioned into k outer folds whereby k outer 1 folds are used as the input data to the inner loop the remaining fold is reserved for testing the predictive model and evaluating the uncertainty estimates in step 2 the outer training fold is further partitioned into k inner folds whereby k inner 1 folds are used to build the predictive model here the predictive models are fitted step 3 using different combinations of hyperparameters e g m try for random forest and sigma and cost for support vector machine and the model is predicted on the validation data which then allows the calculation of the model s accuracy the accuracy values are used to select the optimal hyperparameter values step 4 this process is reiterated k inner times so that each fold is used to validate the model once and the optimized model is selected in step 5 it is important to note that the optimized model is calibrated using all the data in the inner loop i e non partitioned lastly the residual distribution i e observed vs predicted values are retained from each validation fold and combined steps 2 5 may be carried out entirely using the caret package table 1 kuhn 2018 in the r statistical language which includes the model parameterization and selection functions step 5 produces a matrix of the residual distribution for the validation data inner loop which is compiled from each validation fold this matrix is then used to fit the qr function using the quantreg package table 1 koenker 2019 in step 6 because an independent test fold was retained in step 1 it is now possible in step 7 to produce an estimate of model accuracy by predicting the optimized model on the test data and generating the residual distribution for the test fold outer loop secondly the fitted qr model developed from the compiled model residuals is applied to the test fold at the desired percentiles from which the picp and mpi are later calculated to complete the outer loop the model testing process is also reiterated k outer times so that each test fold is used to test the model once and generate the percentiles for each sample location it is important to recall that the observed values from the k outer test folds are not used when generating the percentiles for the independent test data only the predicted values from the machine learner are used and therefore the test data remains completely independent in the validation process i e a nested cross validation procedure in step 7 a matrix consisting of the residual distribution values and their percentile values are retained from each test fold and combined this final matrix is used to calculate goodness of fit statistics e g lin s concordance correlation coefficient root mean square error calculate picp for a range of confidence levels using eq 4 and generate the corresponding picp plot and calculate the mpi using eq 5 to assess the reliability of the accuracy picp and mpi metrics phase 1 may be repeated over multiple iterations so the mean of the metrics may be reported as well as their standard deviations to generate the final uncertainty estimate maps using qr the residual distribution values for the test data are all compiled including the repeats and used as the input to step 9 in phase 2 3 1 2 generating predictive maps and uncertainty maps in phase 2 the final digital soil maps and uncertainty maps are produced using all soil observations in step 8 the soil environmental matrix is used to calibrate the predictive model and hyperparameter values are selected using cross validation in the caret package once the optimal hyperparameter values are determined a final model is produced using the entire training dataset and spatial predictions are made using the covariate stack to generate the final maps in step 9 using the raster package hijmans 2019 to generate the uncertainty maps the residual distribution values for the test data from step 7 are used to fit the qr function step 10 where the fitted qr function is then applied to the final predicted maps produced in step 9 to generate the upper and lower limit percentile maps and the pi map is then calculated as the difference between upper and lower limit percentile maps 3 2 kamloops study area this study area is 50 km west of kamloops british columbia where the eagle hill forest is located fig 2 the study area has a spatial extent of approximately 95 km2 and is located between 50 46 50 to 50 55 05 n and 120 46 37 to 120 57 03 w and has an elevation range of 545 m to 1455 m above mean sea level the climate is semi arid due to the rain shadow effect of the coast mountains with an average annual precipitation of 380 mm the annual temperature ranges between 10 c in the winter and 30 c in the summer the eagle hill forest is in the interior douglas fir zone according to british columbia s biogeoclimatic ecosystem classification system hope et al 1991 and is largely covered by mature douglas fir pseudotsuga menziesii stands with areas of grasslands at lower elevations the soils are primarily described as eutric brunisols and gray luvisols according to the canadian system of soil classification soil classification working group 1998 3 2 1 development of training data three sampling strategies were combined for the kamloops region conditioned latin hypercube sampling clhs minasny and mcbratney 2006 randomized road cut sampling and opportunistic sampling os for the clhs approach the environmental covariates were first generated and used to inform the sampling scheme due to limited accessibility to potential sampling locations as a result of the mountainous terrain and forested cover we restricted the clhs locations to areas that were within 200 m of a road road cuts are the locations where soil or rock are removed along a route to establish a road and are therefore suitable locations from which soil thickness can be estimated here randomly generated sampling locations were placed along the forest service roads finally os locations were selected in the field at the discretion of field pedologists this was done to supplement point data collection and to replace clhs points that fell in areas where access was not possible the soil variables included soil thickness depth to carbonates and soil ph at the 0 30 cm depth increment which were largely selected for their relevance in forest soil management soil thickness and depth to carbonates were both measured in the field and soil ph was measured using a 1 1 soil 0 01m cacl2 solution in the lab there were 410 observations for soil thickness 172 observations for depth to carbonates and 206 observations for soil ph only sample points where carbonates were measured via effervescence after dropping 10 hcl on the soil profile were included in the modeling the soil thickness ranged from 0 cm to 800 cm depth to carbonates ranged from 0 cm to 145 cm and soil ph ranged from 4 35 to 8 65 the mean values for soil thickness depth to carbonates and soil ph were 180 cm 63 cm and 5 9 respectively table 2 a total of 15 environmental covariates were selected from various sources to represent the scorpan model factors mcbratney et al 2003 a suite of 12 topographic variables were derived from a 3 m resolution digital elevation model dem which was produced from light detection and ranging lidar data all continuous covariates were centered and scaled in addition two categorical variables were included the biogeoclimatic subzone and bedrock geology for non tree based models categorical variables needed to be transformed into a series of binary variables for each category of the variable using an encoding process kuhn and johnson 2019 the encoding process was carried out using the onehot package graves 2017 in r where the resulting variables were used only for the knn and svm models non encoded categorical variables were used for rf and cubist models a list of environmental covariates is provided in appendix a table a1 3 3 ottawa study area the city of ottawa was formed from the amalgamation of the city of ottawa and the regional municipality of ottawa carleton and is bounded to the north by the ottawa river fig 2 the study area has a spatial extent of 2824 km2 and is located between 44 57 43 to 45 32 02 n and 75 14 45 to 76 21 20 w and had elevational range of 55 m 171 m above mean sea level the study area has an average annual precipitation of 919 mm and an annual average temperature of 6 6 c with january as the coldest month 10 2 c and july as the hottest month 21 2 c environment canada 2020 ottawa is within the lake simcoe rideau ecoregion of the mixedwood plains ecozone and has a mild and moist climate crins et al 2009 forested areas are dominated by deciduous species such as sugar maple acer saccharum var saccharum american beech fagus grandifolia and white ash fraxinus americana in upland sites and black spruce picea mariana and tamarack larix laricina in the low lying peatlands crins et al 2009 the most common agricultural land uses include annual crops corn soybeans winter wheat and forages and pasture for livestock or dairy operations soils are dominantly classified as orthic humic gleysols and orthic melanic brunisols of the canadian system of soil classification soil classification working group 1998 3 3 1 development of training data two sampling strategies were combined for the ottawa region clhs and os os points were used to account for clhs points that fell in areas with restricted access in total 1633 soil profiles were described and sampled according to pedological horizons as per the canadian system of soil classification soil classification working group 1998 soil samples were submitted for full laboratory analysis however only two soil properties were selected for this study soil ph and soc soil ph was determined using a 1 1 soil 0 01m cacl2 solution soc was determined using the combustion method kalembasa and jenkinson 1973 where total c and inorganic c were determined using a leco analyzer total c was determined from combustion of the soil sample while inorganic c was determined by first ashing the soil sample in a muffle furnace at 475 c for 3 h to remove the organic c fraction from the soil samples organic carbon was calculated as the difference between total carbon and inorganic carbon the two soil properties were modeled at two depth increments 0 5 cm and 30 60 cm soil profile data were harmonized from soil profile descriptions with analytical data to the standard depths using an equal area quadratic spline bishop et al 1999 as implemented in the ithir package malone 2017 some portions of the ottawa study area have shallow bedrock and truncated soil profiles for this reason for the 0 5 cm depth increment 1633 records were obtained for the training data however only 1531 records were obtained at the 30 60 cm depth increment for the 0 5 cm depth increment soil ph ranged from 3 1 to 7 7 and soc ranged from 0 01 to 48 1 table 1 the low ph and high soc values are associated with organic deposits in the study area the mean ph and soc for this shallow depth were 5 8 and 6 51 for the 30 60 cm depth increment soil ph ranged from 3 2 to 7 7 and soc ranged from 0 01 to 47 94 table 1 the mean values were 6 2 and 1 53 for soil ph and soc respectively the higher mean ph for the 30 60 cm depth resulted from the predominantly neutral to alkaline parent materials of the study area the lower mean soc for this depth indicated fewer organic deposits present at this depth and overall low soc in the parent materials in total 43 covariates were selected for the ottawa study and listed in appendix a table a2 six of the covariates were categorical represent bedrock geology quaternary surficial deposits soil order and crop or vegetation classification for the years 2016 2018 radiometric data was acquired and six covariates were created which included potassium thorium uranium and their ratios euclidean distance fields behrens et al 2018 were calculated to provide spatial context to the models the remaining covariates were derivatives of the lidar dem and were created in saga gis and whiteboxtools lindsay 2019 all covariates were prepared at a 10 m spatial resolution all continuous variables were centered and scaled and given the large number of predictors principal component analysis was applied for the training data used by the tree based learners qrf rf and cubist a principal component analysis pca was applied to the continuous data where the first 17 principal components pcs that consisted of 95 of the cumulative variance were retained in addition six categorical variables that represented soil type bedrock and surficial geology and satellite derived crop data were included for knn and svm the categorical variables were transformed through the encoding process therefore for the qrf rf and cubist models a total of 23 covariates were used 17 pcs 6 categorical whereas for the knn and svm models due to the encoding of categorical covariates a total of 53 covariates were used 17 pcs 36 encoded 3 4 machine learning models to demonstrate how the qr framework may be easily implemented for any machine learner four learners found in the dsm literature were used rf cubist knn and svm we recognize that there are many other learners that could be tested however a comprehensive comparison amongst ml techniques is beyond the scope of this study descriptions of these algorithms as applied to dsm may be found in publications such as heung et al 2016 and lamichhane et al 2019 as well as from the original sources related to rf breiman 2001 cubist quinlan 1992 1993 and svm cortes and vapnik 1995 in addition qrf was used to provide a comparison for the qr framework uncertainty maps it is important to note that qrf is an extension of rf and as such generates the same predictions within the proposed qr framework the hyperparameters that were optimized included the number of randomly selected predictors m try for rf and qrf the number of committees and neighbors for cubist the number of neighbors k for knn and the sigma and cost parameters for svm with radial basis function for each ml a range of hyperparameter values were tested and the optimal combination of values was selected based on the 10 fold cross validation process for the inner loop of the framework although the optimization of the hyperparameters could be computationally demanding the process may be made more efficient with bayesian optimization using the rbayesianoptimization package in r yan 2016 3 5 comparison to quantile regression forest a commonly used method for estimating prediction uncertainty in dsm studies is qrf for example vaysse and lagacherie 2017 compared regression kriging with qrf to test the ability of estimating the uncertainties of globalsoilmap net soil property grids their results showed that qrf provided more accurate predictions and also the spatial patterns of the uncertainty estimates were more easily interpretable than regression kriging subsequently szatmári and pásztor 2019 further tested the application of qrf for uncertainty estimation in dsm by comparing it to sequential gaussian simulation universal kriging variance rf regression kriging variance and bootstrapping of rf regression kriging variance where it was determined that methods using qrf and sequential gaussian simulation were superior when applying a qrf approach meinshausen 2006 explains that the weighted observations used for estimating the conditional mean in rf could also be used to approximate the full conditional distribution readers interested in the theoretical background of qrf should refer to meinshausen 2006 for additional details most importantly the key difference between rf and qrf is that for each terminal node of each tree rf retains only the mean of the predicted values that fall into each node whereas qrf also retains all predicted values within the terminal node and assesses the conditional distribution to estimate the prediction interval for the node meinshausen 2006 it is also important to reemphasize that qrf is fundamentally different than a coupling of rf and qr despite the similar terminology because the latter does not use information from the terminal nodes of rf to generate uncertainty estimates the qrf approach was implemented using the quantregforest package for the r statistical language 3 6 evaluation of accuracy uncertainty estimations for each outer loops of the qr framework the test data is used to evaluate the predictive model developed in the inner loop the outer loop is iterated 10 times once for each of the k outer folds thus validation statistics were compiled and averaged as described in section 2 3 1 the entire qr framework process can be repeated over multiple iterations for this study the qr framework was repeated 20 times validation statistics were therefore compiled averaged across all iterations and reported in the results the accuracy metrics used for this study included root mean square error rmse coefficient of determination r2 and lin s concordance correlation coefficient ccc the primary metric used for model selection and accuracy assessment was the ccc as it measures how closely the observed and predicted values fall along a 1 1 line lawrence and lin 1989 to summarize each model and its hyperparameter combination was iterated 2000 times whereby 10 fold cross validation was applied for each hyperparameter combination within a model inner loop 10 fold cross validation was applied to assess the uncertainty estimates using independent test data outer loop and 20 repeats of the nested cross validation procedure was applied to ensure the stability of the accuracy metrics through the 20 repeats the residual distribution for the test folds for each repeat was retained here mpi and picp plots were generated where the average mpi and picp values and their standard deviations were calculated over the range of confidence levels that were tested in addition this study also compared the proposed qr framework against a reference qrf approach for estimating uncertainty whereby mpi and picp plots were generated using a completely independent dataset i e test data for each of the k outer folds it is worth noting that the structure of the inner and outer loops may be adjusted according to the needs of the user by adjusting the number of folds or using an alternative cross validation approach such as leave one out cross validation or random holdback cross validation however the comparison of the different configurations of the nested cross validation procedure was beyond the scope of this study hence we selected k 10 fold which is routinely used 4 results discussion 4 1 kamloops study area the cubist model generally produced the best predictions and these are therefore shown in fig 3 however maps for each soil property and machine learner are provided in appendix b figs b1 b3 as expected soil thickness was predicted to be shallower at high elevations e g northeast and southwest of the study area and deeper soils were more common at lower elevations fig 3 soil ph maps for all four models showed a similar pattern of neutral to alkaline ph at lower elevations with a gradual transition to acidic ph values at higher elevations in the northeast and southwest fig 3 the high ph area aligned well with the central creek and flood plain areas in the study area there was a general increase in the depth to carbonates as elevation increased based on a visual assessment the resulting digital soil maps were consistent with our expert understanding of the soil patterns for this area 4 1 1 accuracy assessment overall the accuracy metrics for the soil thickness and soil ph results were promising the rf svm and cubist models had similar accuracies for soil thickness with ccc values ranging from 0 47 to 0 50 table 3 the rmse values were also similar for all models ranging from 103 to 105 cm the soil ph models also performed well with ccc ranging from 0 33 to 0 37 and rmse values ranging from 0 68 to 0 73 although the cubist model has the potential to predict values beyond the range of values found in the training data the accuracy metrics were still highest for all predicted soil properties across all models knn consistently performed the worst overall the depth to carbonates maps had the lowest accuracies regardless of the modelling approach used here it is important to recognize that given qrf is an extension of rf the same predicted maps are generated and hence the accuracy metrics are therefore the same for rf and qrf in table 3 because the nested cross validation process was repeated 20 times the summary statistics for all accuracy metrics across the repetitions are presented in appendix a table a3 when predicting soil thickness using svm negative values were predicted table 4 since the svm learner extrapolated values beyond the range of the training data similarly the cubist model extrapolated large positive numbers for soil thickness and depth to carbonates in both cases extreme values were very limited in distribution furthermore we chose not to restrict the predicted values to the range of values found in the training data to ensure that qr estimated the quantile values from the actual predicted values 4 1 2 uncertainty estimation maps of uncertainty estimates were produced for the 90 prediction intervals fig 4 and in addition picp plots fig 5 and mpi plots fig 6 were generated using the qr framework for the rf learner and the reference qrf approach for comparison all other maps and picp plots are provided in appendix b figs b4 b9 in general all models showed a similar spatial pattern of uncertainty however there were differences in the magnitude of the uncertainty although the accuracy metrics were similar amongst the rf cubist and svm model predictions the uncertainty maps were drastically different except for the soil ph predictions using the cubist model the mean uncertainty values between the learners were similar however the range and variability in uncertainty values differed considerably table 4 which led to the differences in spatial patterns the soil thickness uncertainty maps showed highest uncertainty at the lower elevations and in some pockets at higher elevations in the southwestern portion of the study area higher uncertainty was expected at these lower elevations because as the soil thickness increased at these elevations the more difficult it became to accurately measure the soil thickness in the field uncertainty was highest at high elevations for the depth to carbonates maps when using the rf knn and svm models while in contrast the cubist model showed the highest uncertainty at low elevations fig b5 the lower accuracy of the depth to carbonates maps at high elevations was expected because there were few opportunities to collect observations in these thinner soils coupled with the trend towards deeper carbonate depths there for the soil ph predictions uncertainty was lowest at high elevations found within the northeastern and southwestern regions of the study area the likely reason for the better model performance at higher elevation was the lack of soil carbonates in these shallow soils moreover a comparison was conducted between the two uncertainty estimation methods using the qr framework and qrf the uncertainty estimates for soil thickness using qrf showed a larger range of values when compared to qr coupled with the 4 ml models despite the larger width of the 90 pi with qrf the spatial patterns of uncertainty were quite comparable between qrf and the maps generated from the four ml techniques coupled with the qr framework the largest 90 pi width in the uncertainty maps for depth to carbonates was for the svm model which was similar to the uncertainty estimates generated using qrf fig 4 table 4 the qrf uncertainty map differed from the rf with qr framework uncertainty map with the latter having higher uncertainty over most of the study area the range of the 90 prediction interval values for qrf were again larger than those derived from using qr along with the four ml models fig 4 and table 4 however the qrf uncertainty map had the lowest overall uncertainty spatial trends in uncertainty for soil ph were similar across all maps the picp plots for the qr results showed optimal results across all soil properties and models where the picp values corresponded well to their respective confidence levels by repeating the nested cross validation procedure 20 times it was possible to assess the stability of the picp values overall the greatest variability in picp values occurred at confidence levels less than 50 however the extent of the variability was not particularly large at the 90 confidence level the picp values were optimal and showed little variability when compared to the reference approach using qrf the picp for qrf differed from the picp for the four ml models and show an overestimation of the pi width for soil thickness as indicated by the points above the bisector lines in fig 6 in comparison the overestimation of the pi width when modeling depth to carbonates and soil ph was less apparent the mpis for all models were also similar for soil thickness depth to carbonates and soil ph fig 6 in general the qrf and rf models had slightly lower mpi than cubist knn and svm however the differences were generally marginal e g depth to carbonates at the 95 confidence level overall these results indicate that in this landscape the qr approach provided uncertainty estimates that were reliable and stable regardless of the different ml techniques and soil properties furthermore the qr approach also yielded results that were comparable to or slightly better than the reference qrf approach 4 2 ottawa study area the cubist model generally produced the best predictions and the maps generated using this model are therefore shown in fig 7 however maps for each soil property and machine learner are provided in appendix c figs c1 c4 the soil ph maps for all four models at both depths showed a similar pattern of neutral to alkaline ph in the south and west portions of the study area with a gradual transition to acidic ph values in the northeast and northwest the low ph areas aligned well with known landscape features the carp ridge in the northwest which is of precambrian origin with acidic reaction and the mer bleue organic deposits in the northeast which are a combination of deep organic deposits and coarse sandy deposits located in glacial spillway channels in terms of soc the four models produced similar maps of soc distribution except for the knn model which failed to map organic deposits at the southern region of the study area other than this the soc maps were consistent with observations from the field work especially with regards to the large organic deposits 4 2 1 accuracy assessment the cubist model outputs for both depth intervals showed the largest range in soil ph and soc as indicated from the kamloops study cubist predictions were expected to extend outside the range of the input data similar to results from the kamloops study the cubist model was consistently the best performing model when considering ccc ranging from 0 56 to 0 64 across the two soil properties at the two modeled depths table 3 the rf knn and svm models had similar performance except for the knn model for soc 30 60 cm which performed less effectively ccc 0 50 rmse did not provide much insight into model performance with very little difference between the models for soil ph prediction both depths whereas for soc the knn model had the highest rmse for the 0 5 cm depth and for the 30 60 cm depth similar to the results from the kamloops study area svm predicted negative values for soc however the distribution of those negative values was also limited table 4 summary statistics for the accuracy metrics are presented in appendix a table a3 4 2 2 uncertainty estimation uncertainty estimates were assessed by deriving 90 prediction intervals fig 8 and generating picp plots fig 9 and mpi plots fig 10 using the qr framework for the cubist learner and the reference qrf approach all other maps and picp plots are provided in appendix c figs c5 c12 in general all models showed the same general spatial pattern of uncertainty however there were differences in the magnitude of the uncertainty for the soil ph maps at both depths the uncertainty was lowest in the southern and western regions of the maps and increased primarily in the northeast rf and cubist predictions had lower uncertainty than the knn and svm models especially when comparing the northeast sections of the maps this was most apparent in the soil ph maps for the 30 60 cm depth despite the similar range of the uncertainty between the upper and lower depths the differences in the magnitude of the uncertainty was more apparent across the four ml models at both prediction depths the knn model showed the largest areas of lowest and highest uncertainty the soil ph uncertainty maps generated from the qrf as compared to those from the four ml models using the qr framework showed some drastic differences fig 9 in particular the qrf uncertainty estimates at both depths showed the largest range in the prediction intervals furthermore the qrf uncertainty estimate maps were much more variable across the study area in comparison to the four qr uncertainty maps which showed gradual transitions in the uncertainty aligning well with the predictions fig 7 the soc prediction uncertainty maps showed the highest uncertainty in small pockets scattered throughout the maps which aligned well with areas of organic deposits whereby the organic carbon ranged from 17 to 58 from a visual inspection of the maps cubist seems to have had the highest uncertainty along the large organic deposits in the southwest and northeast sections of the map the soc prediction uncertainty for svm showed a consistently higher uncertainty across the entire study area at both prediction depths suggesting that svm had difficulty making accurate predictions at low concentrations of soc in contrast to the soil ph uncertainty maps from qrf the soc uncertainty maps generated from the qrf were more consistent with those generated using the qr framework however the qrf uncertainty map for the 0 5 cm depth showed consistently higher uncertainty in the southwestern and western regions of the study area fig 8 like the kamloops study area the picp plots for the ottawa study area showed a near 1 1 relationship between picp and confidence level when using qr fig 9 figs c9 c12 furthermore there was far less variability in terms of the picp values across the 20 replicates a result that may be partially due to the ottawa study area having a far larger number of sample observations once again these results indicated that the qr approach to uncertainty estimation was reliable and was not dependent on the modelling approach used when comparing the picp plots generated from the qrf model to plots generated using qr for soil ph there was a marginal overestimation of the pi width when qrf was used similar to the kamloops study area the mpi increased as the confidence level increased the mpis values were similar between all the models for soil ph at both depths fig 10 in general the cubist qrf and rf models had slightly lower mpi than knn and svm the mpi data for the soc predictions showed larger differences between models and more variability for the soc 0 5 cm depth the models all had similar mpi up to the 80 confidence level at which point the cubist qrf and rf models started to differ from the knn and svm models the knn and svm models consistently had larger mpis suggesting that as the level of confidence in the predictions increases these models did not perform as well as the tree based learners at the 30 60 cm depth mpis for the soc predictions showed similar trends as the confidence level increased the cubist qrf and rf models generated lower mpis this was only true for the 98 and 99 confidence levels and cubist had the lowest mpi starting at the 80 confidence level in some cases the differences between the models were significant however the magnitudes were negligible by generating a scatterplot of the observed versus predicted values and superimposing the qr functions at different confidence levels fig 11 for soil ph 0 5 cm and soc 0 5 cm using the cubist model we observed crossing qr lines for soc but not for soil ph the crossing qr lines also explain the occurrence of negative uncertainty estimates that are shown in table 4 this phenomenon has been observed in the qr literature bondell et al 2010 and is largely related to the distribution of the data the distribution of ph values was much narrower and more normally distributed than for soc and thus the fitted qr lines were closer together and more importantly parallel the parallel qr lines indicate that the prediction interval remained fairly constant in comparison it was previously established that the soc data had a high positive skewness because of the inclusion of organic deposits this resulted in a funnel shaped scatterplot and the resulting qr functions derived from these observations showed continuous divergence with increasing confidence levels here the prediction interval width increased as the soc increased furthermore a consequence of this effect was the estimation of negative pi values in areas with very low soc values transformation of the dependent variable to normalize the distribution of the observed values prior to modelling could be a reasonable approach to address this issue attempts to address this potential issue were beyond the scope of this study however studies such as bondell et al 2010 have proposed a constrained qr approach for linear and nonparametric quantile curves 4 3 general discussion future work given that the intention of this study was to demonstrate the novel integration of ml and qr within a dsm framework it was necessary to provide this demonstration using two unique case studies to assess the generalizability of the approach in both case studies the resulting picp plots were optimal where the confidence level and picp showed a 1 1 relationship regardless of study area soil attribute and machine learner although we followed the standard approach for assessing uncertainty estimates in the ml qr and dsm literature ding et al 2018 malone et al 2011 muthusamy et al 2016 shrestha and solomatine 2006 by using picp and mpi recent studies such as szatmári and pásztor 2019 have suggested the use of the g statistic deutsch 1997 which quantifies how close the picp values are to their associated confidence levels although there were differences in the choice of environmental covariates and the application of pca for the ottawa study area we do not believe this to impact our findings furthermore though it is recognized that the use of opportunistic sampling may not seem like an optimal approach by some we do not anticipate this to be an issue when the core objective was to test an approach for estimating uncertainty future research may investigate the use of different sampling approaches and combinations of environmental covariates to minimize the uncertainty estimates while maximizing accuracy related to the sampling approaches this study did not seek to identify the minimum number of sample locations required for a reliable application of the proposed method hence future work investigating how the qr approach as well as other uncertainty estimation approaches respond by systematically reducing the sampling density would be informative from the ottawa study area we raised a potential issue related to qr generating negative estimates of uncertainty and we had attributed it to crossing qr lines bondell et al 2010 which may be related to the distribution of soil attribute values and the model residuals furthermore we also recognize that the assumption of a linear relationship between the observed and predicted values may be a weakness of this approach however nonparametric quantile curves could be a potential solution bondell et al 2010 these issues warrant further investigation as applied to a dsm context 4 3 1 quantile regression forest we also demonstrated that the qr framework generates uncertainty estimates that are comparable to those generated from qrf this was true across both study areas and all soil properties generally the spatial patterns of uncertainty were quite similar between the qr framework and qrf outputs however the qrf uncertainty typically had the largest range this demonstrates that the qr framework generates valid estimates of uncertainty however the advantage is that the qr framework allows the user to integrate any ml model not just rf although the focus of this study was to propose a novel qr framework the evaluation of qrf also yielded some interesting observations throughout both the kamloops and ottawa study areas we observed the tendency of qrf to marginally overestimate the pi width which was most apparent when predicting ph when comparing this study with vaysse and lagacherie 2017 where ph organic carbon and clay were mapped the picp plots for qrf showed the same tendency to marginally overestimate the pi width whereby the overestimation was also the most apparent for soil ph we do not have an explanation for this observation and we are also unclear if this is more than a coincidence with respect to estimating the uncertainty of soil ph predictions in szatmári and pásztor 2019 the qrf approach was similarly applied for estimating the uncertainty of soc predictions and again there appeared to be a marginal tendency to overestimate the uncertainty lastly fouedjio and klump 2019 evaluated the qrf approach using a suite of synthetic and real world spatial data and the tendency to overestimate the uncertainty was more apparent than those observed in this study and vaysse and lagacherie 2017 and szatmári and pásztor 2019 when collectively evaluating the results from this study with vaysse and lagacherie 2017 szatmári and pásztor 2019 and fouedjio and klump 2019 it appears as though these marginal overestimations occur between a confidence level range of 40 80 within the picp plots 4 3 2 future research given that this is the first implementation of the qr approach for dsm there are several areas for future research first the generalizability of the approach should be further investigated for other study areas soil properties spatial scales and machine learners although this study specifically tested the integration of qr and ml ml may be substituted with other types of predictive models such as a purely geostatistical model e g ordinary kriging or a hybrid model e g regression kriging given that the main inputs for qr are the model residuals such a comparison would be useful especially since kriging approaches provide an alternative for uncertainty estimation using the kriging variance in terms of existing ml based approaches for uncertainty estimation a comparison between the qr and bootstrapping approaches would be warranted for example szatmári and pásztor 2019 compared the uncertainty estimates produced from the kriging variance from universal kriging and rf regression kriging with sequential gaussian simulation sgs qrf and bootstrapping of rf regression kriging there it was shown that qrf and sgs were more optimal in estimating uncertainty however they also indicated that qrf and sgs were computationally demanding it is also necessary to note that our integration of rf and qr is fundamentally different from the qrf approach in qrf all predictions made at the terminal node across all individual trees are retained following this the residual distribution produced by the individual trees for each terminal node is then used to calculate and predict the quantiles meinshausen 2006 although qrf is the primary example of a ml technique that generates uncertainty estimates in the dsm literature e g rudiyanto et al 2018 szatmári and pásztor 2019 vaysse and lagacherie 2017 other types of ml techniques such as quantile regression neural networks cannon 2011 could be tested and compared in future work a potential issue with the bootstrapping approach malone et al 2017 is that increasing the number of bootstrap predictions would lead to a decrease in the prediction variance and thereby cause the narrowing of the prediction interval maps through a comparative study if qr yields similar or better results than bootstrapping this framework could overcome the major computational demands of bootstrapping which requires a spatial prediction for each model iteration although we did not carry out a direct comparison with bootstrapping with respect to computational time our personal experience was that the computationally demanding part of the bootstrapping process was in generating multiple e g 100 realizations of a soil property map i e applying a model to a covariate stack and having to store each realization in this proposed framework only one realization of a soil property map is generated as well as one realization of each quantile map e g 95 and 5 the uncertainty maps are generated by applying the qr function fitted using only the residual distribution from the observed sites and applied to the predicted soil map and hence not requiring multiple map realizations to be generated this solution would be particularly valuable when applied over large spatial extents e g national and global mapping initiatives when using ultra high resolution datasets e g lidar and when there are other big data challenges to overcome for example the most recent global scale mapping effort was the soilgrids250m hengl et al 2017 whereby the authors avoided the modelling of uncertainty for continuous soil variables and have indicated that qrf was a computationally intensive process especially with increasing data volumes by may 4 2020 soilgrids250m was updated to include uncertainty estimations using qrf 5 conclusions this study provides a generic framework for integrating ml techniques with qr for the purposes of producing uncertainty estimates for dsm thus far bootstrapping approaches have been the most common method for estimating uncertainty when using ml approaches however bootstrapping is a computationally demanding process using qr we proposed a computationally efficient alternative to uncertainty estimation that may overcome future challenges associated with big data although this study only tested some commonly used machine learners the overall approach may be applied to less commonly used learners as a well as geostatistical approaches by testing the approach on different study areas with different covariates sample sizes and soil attributes the picp and mpi plots show that qr can produce consistent and stable results although we recognize that further testing of the framework is required these results show considerable promise and the preliminary findings provide a novel contribution well beyond dsm and may be used for a variety of different spatial modelling applications declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments for the kamloops study area the authors acknowledge the financial support that was provided by the forest enhancement society of british columbia the forest innovation program of the canadian wood fibre centre natural resources canada british columbia ministry of forests lands natural resource operations and rural development contributors to field and lab activities include jin zhang doug terpsma heather richardson marc levesque joseph desmarais lauren foote natasha romalo yesheng chen emily koopmans justin walch and adrienne arbor for the ottawa study area the authors acknowledge the financial support that was provided by ontario ministry of agriculture food and rural affairs through the growing forward 2 program contributors to field and lab activities include jim warren adam gillespie stephanie vickers mackenzie clarke and veronika wright appendix a additional tables table a1 soil environmental covariates for the kamloops study area table a1 representation environmental covariate resolution map scale topography hillshade illumination 3 m diurnal anisotropic heating 3 m digital elevation model 3 m standardized height 3 m multiresolution valley bottom flatness index gallant and dowling 2003 3 m planform curvature 3 m profile curvature 3 m topographic openness yokoyama et al 2002 3 m slope 3 m slope height 3 m topographic wetness index beven and kirkby 1979 3 m valley depth 3 m vegetation biogeoclimatic ecosystem classification subzone 1 20 000 geology bedrock geology nutrient cui et al 2017 1 50 000 1 250 000 bedrock geology texture cui et al 2017 1 50 000 1 250 000 table a2 soil environmental covariates for the ottawa study area table a2 representation environmental covariate resolution map scale topography elevation 10 m deviation from maximum elevation 550m 10 m deviation from maximum elevation 1300m 10 m deviation from maximum elevation 4200m 10 m deviation from mean elevation 100m 10 m deviation from mean elevation 550m 10 m deviation from mean elevation 1300m 10 m deviation from mean elevation 4200m 10 m convergence index kothe and lehmeier 1996 10 m general curvature 10 m hillshade 10 m slope length factor moore et al 1993 10 m multi resolution ridge top flatness gallant and dowling 2003 10 m multi resolution valley bottom flatness gallant and dowling 2003 10 m normalized height 10 m profile curvature 10 m relative slope position 10 m slope height 10 m slope percent 10 m stream power index moore et al 1991 10 m saga wetness index 10 m topographic position index guisan et al 1999 10 m topographic ruggedness index riley et al 1999 10 m valley depth 10 m vegetation crop 2016 agriculture and agri food canada 2018 30 m crop 2017 agriculture and agri food canada 2018 30 m crop 2018 agriculture and agri food canada 2018 30 m geology bedrock geology ontario geological survey 2017 1 250 000 bedrock geology ontario geological survey 2017 1 100 000 radiometric k natural resources canada 2019 80 1000 m radiometric th natural resources canada 2019 80 1000 m radiometric u natural resources canada 2019 80 1000 m radiometric u k ratio natural resources canada 2019 80 1000 m radiometric u th ratio natural resources canada 2019 80 1000 m radiometric th k ratio natural resources canada 2019 80 1000 m spatial position distance from mid behrens et al 2018 10 m distance from ne behrens et al 2018 10 m distance from nw behrens et al 2018 10 m distance from se behrens et al 2018 10 m distance from sw behrens et al 2018 10 m distance from x max behrens et al 2018 10 m distance from y max behrens et al 2018 10 m soil soil order omafra 2019 1 50 000 table a3 summary statistics of accuracy metrics from 20 repeats of the nested cross validation procedure table a3 study area soil property descriptive statistic random forest quantile regression forest cubist k nearest neighbor support vector machine r2 ccc rmse r2 ccc rmse r2 ccc rmse r2 ccc rmse kamloops soil thickness cm minimum 0 32 0 46 101 32 0 29 0 47 104 81 0 25 0 36 107 64 0 27 0 45 103 74 mean 0 35 0 47 103 19 0 32 0 50 106 15 0 27 0 37 108 44 0 31 0 48 105 39 median 0 35 0 47 103 09 0 32 0 50 106 07 0 27 0 37 108 46 0 32 0 48 105 14 maximum 0 39 0 49 104 68 0 34 0 51 108 54 0 28 0 38 109 38 0 34 0 49 108 57 standard deviation 0 01 0 01 0 85 0 01 0 01 1 00 0 01 0 00 0 40 0 01 0 01 1 05 depth to carbonates cm minimum 0 03 0 10 29 73 0 00 0 12 29 63 0 09 0 13 29 15 0 05 0 12 29 33 mean 0 07 0 13 30 04 0 05 0 16 30 57 0 10 0 15 29 42 0 08 0 15 29 70 median 0 07 0 14 29 97 0 05 0 16 30 51 0 10 0 15 29 43 0 09 0 15 29 64 maximum 0 13 0 15 30 68 0 11 0 19 31 57 0 12 0 16 29 58 0 11 0 17 30 07 standard deviation 0 03 0 01 0 26 0 03 0 02 0 49 0 01 0 01 0 09 0 02 0 01 0 21 ph 0 30 cm minimum 0 23 0 34 0 66 0 14 0 32 0 69 0 09 0 14 0 73 0 12 0 29 0 67 mean 0 26 0 37 0 68 0 20 0 36 0 72 0 11 0 16 0 73 0 18 0 33 0 70 median 0 26 0 37 0 68 0 20 0 35 0 72 0 12 0 16 0 73 0 18 0 33 0 70 maximum 0 32 0 40 0 69 0 24 0 41 0 74 0 14 0 18 0 74 0 24 0 38 0 72 standard deviation 0 02 0 01 0 01 0 03 0 03 0 02 0 01 0 01 0 00 0 03 0 02 0 01 ottawa ph 0 5 cm minimum 0 37 0 51 0 70 0 35 0 56 0 70 0 33 0 49 0 72 0 36 0 55 0 70 mean 0 37 0 51 0 70 0 36 0 56 0 71 0 34 0 50 0 72 0 37 0 55 0 70 median 0 37 0 51 0 70 0 36 0 56 0 71 0 34 0 50 0 72 0 37 0 55 0 70 maximum 0 38 0 52 0 70 0 37 0 57 0 71 0 35 0 50 0 72 0 38 0 56 0 71 standard deviation 0 00 0 00 0 00 0 01 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 ph 30 60 cm minimum 0 43 0 57 0 61 0 41 0 61 0 62 0 40 0 59 0 62 0 42 0 59 0 61 mean 0 44 0 58 0 61 0 42 0 62 0 62 0 41 0 60 0 63 0 43 0 60 0 62 median 0 44 0 58 0 61 0 42 0 62 0 62 0 41 0 60 0 63 0 43 0 60 0 62 maximum 0 44 0 58 0 62 0 43 0 62 0 63 0 42 0 60 0 63 0 44 0 61 0 62 standard deviation 0 00 0 00 0 00 0 01 0 00 0 00 0 01 0 00 0 00 0 00 0 00 0 00 soc 0 5 cm minimum 0 44 0 60 6 89 0 40 0 62 6 89 0 35 0 56 7 36 0 41 0 58 7 10 mean 0 46 0 61 6 94 0 44 0 64 7 00 0 37 0 57 7 46 0 42 0 59 7 15 median 0 46 0 61 6 94 0 45 0 64 6 98 0 37 0 57 7 47 0 42 0 59 7 14 maximum 0 47 0 62 6 99 0 46 0 65 7 22 0 39 0 59 7 55 0 44 0 60 7 22 standard deviation 0 01 0 00 0 03 0 02 0 01 0 08 0 01 0 01 0 07 0 01 0 01 0 03 soc 30 60 cm minimum 0 83 0 47 3 90 0 66 0 55 3 89 2 56 0 42 4 10 1 78 0 47 4 08 mean 0 29 0 53 4 01 0 13 0 59 4 06 0 01 0 51 4 23 0 06 0 53 4 25 median 0 36 0 53 4 02 0 27 0 59 4 07 0 23 0 50 4 22 0 24 0 53 4 26 maximum 0 45 0 57 4 11 0 40 0 62 4 26 0 38 0 56 4 47 0 39 0 58 4 50 standard deviation 0 27 0 02 0 06 0 33 0 02 0 09 0 67 0 04 0 09 0 49 0 04 0 11 appendix b kamloops study area fig b1 soil thickness maps generated using random forest cubist k nearest neighbors and support vector machines at a 3 m spatial resolution for the kamloops study area fig b1 fig b2 depth to carbonates maps generated using random forest cubist k nearest neighbors and support vector machines at a 3 m spatial resolution for the kamloops study area fig b2 fig b3 soil ph maps generated using random forest cubist k nearest neighbors and support vector machines at a 3 m spatial resolution for the kamloops study area fig b3 fig b4 uncertainty maps for soil thickness predictions showing the 90 prediction interval generated using quantile regression forest random forest with quantile regression cubist k nearest neighbors and support vector for the kamloops study area fig b4 fig b5 uncertainty maps for depth to carbonates predictions showing the 90 prediction interval generated using quantile regression forest random forest with quantile regression cubist k nearest neighbors and support vector for the kamloops study area fig b5 fig b6 uncertainty maps for soil ph predictions showing the 90 prediction interval generated using quantile regression forest random forest with quantile regression cubist k nearest neighbors and support vector for the kamloops study area fig b6 fig b7 prediction interval coverage probability picp plots for uncertainty estimates of soil thickness predictions using quantile regression forest qrf random forest with quantile regression rf cubist k nearest neighbors knn and support vector machines svm for the kamloops study area fig b7 fig b8 prediction interval coverage probability picp plots for uncertainty estimates of depth to carbonates predictions using quantile regression forest qrf random forest with quantile regression rf cubist k nearest neighbors knn and support vector machines svm for the kamloops study area fig b8 fig b9 prediction interval coverage probability picp plots for uncertainty estimates of soil ph predictions using quantile regression forest qrf random forest with quantile regression rf cubist k nearest neighbors knn and support vector machines svm for the kamloops study area fig b9 appendix c ottawa study area fig c1 soil ph 0 5 cm maps generated using random forest cubist k nearest neighbors and support vector machines at a 10 m spatial resolution for the ottawa study area fig c1 fig c2 soil ph 30 60 cm maps generated using random forest cubist k nearest neighbors and support vector machines at a 10 m spatial resolution for the ottawa study area fig c2 fig c3 soil soc 0 5 cm maps generated using random forest cubist k nearest neighbors and support vector machines at a 10 m spatial resolution for the ottawa study area fig c3 fig c4 soil soc 30 60 cm maps generated using random forest cubist k nearest neighbors and support vector machines at a 10 m spatial resolution for the ottawa study area fig c4 fig c5 uncertainty maps for soil ph 0 5 cm predictions showing the 90 prediction interval generated using quantile regression forest random forest with quantile regression cubist k nearest neighbors and support vector for the ottawa study area fig c5 fig c6 uncertainty maps for soil ph 30 60 cm predictions showing the 90 prediction interval generated using quantile regression forest random forest with quantile regression cubist k nearest neighbors and support vector for the ottawa study area fig c6 fig c7 uncertainty maps for soil soc 0 5 cm predictions showing the 90 prediction interval generated using quantile regression forest random forest with quantile regression cubist k nearest neighbors and support vector for the ottawa study area fig c7 fig c8 uncertainty maps for soil soc 30 60 cm predictions showing the 90 prediction interval generated using quantile regression forest random forest with quantile regression cubist k nearest neighbors and support vector for the ottawa study area fig c8 fig c9 prediction interval coverage probability picp plots for uncertainty estimates of soil ph 0 5 cm predictions using quantile regression forest qrf random forest with quantile regression rf cubist k nearest neighbors knn and support vector machines svm for the ottawa study area fig c9 fig c10 prediction interval coverage probability picp plots for uncertainty estimates of soil ph 30 60 cm predictions using quantile regression forest qrf random forest with quantile regression rf cubist k nearest neighbors knn and support vector machines svm for the ottawa study area fig c10 fig c11 prediction interval coverage probability picp plots for uncertainty estimates of soc 0 5 cm predictions using quantile regression forest qrf random forest with quantile regression rf cubist k nearest neighbors knn and support vector machines svm for the ottawa study area fig c11 fig c12 prediction interval coverage probability picp plots for uncertainty estimates of soc 30 60 cm predictions using quantile regression forest qrf random forest with quantile regression rf cubist k nearest neighbors knn and support vector machines svm for the ottawa study area fig c12 
25759,hydrological modelling can be a relevant tool to support sustainable planning decisions however urban hydrological models usually rely on simplified underground representation that may limit their applicability in shallow groundwater environments this paper introduces a set of modules designed to describe the numerous interactions between subsurface processes and surface hydrology that may occur in such contexts the modules provide a rather straightforward approach to simulate the influence of the groundwater on the surface as well as the influence of several underground structures sewer systems building basements on subsurface storage and the volumes to be managed by the sewerage network the modules are evaluated by comparison with reference models over a set of theoretical test cases and integrated within the urbs hydrological model an application to a realistic hypothetical watershed is carried out to illustrate the benefits and the feasibility of these focussed development in existing urban hydrological models keywords hydrological modelling urban hydrology urban hydrogeology hydrological processes groundwater unsaturated zone list of notations a i area of cell i m2 a m 1 b m 1 parameters of the green ampt model m b k shape parameter of the brooks and corey law in reservoir k ca infiltration capacity ms 1 d k thickness of reservoir k m d ij distance between the centre of gravity of cell i and the centre of the interface between cells i and j m dr k root density in reservoir k ed evaporative demand ms 1 f k k 1 exchange flux between reservoirs k and k 1 ms 1 f rm upper flux for the reservoir model computed by the green ampt model ms 1 f uz sz exchange flux between the lowest reservoir and the saturated zone ms 1 g sew i coefficient characterizing the conductivity of the sewer pipe and the surrounding medium in cell i ms 1 h i hydraulic head in cell i m i hydraulic gradient i cumulative infiltration m i p cumulative infiltration at surface ponding m i sup maximal upper flux ms 1 i surf surface infiltration flux ms 1 k eff effective hydraulic conductivity ms 1 k k k 1 hydraulic conductivity at the interface between reservoirs k and k 1 ms 1 k k hydraulic conductivity in reservoir k ms 1 ks k saturated hydraulic conductivity in reservoir k ms 1 ks dr i saturated hydraulic conductivity of soil below the drainage system in cell i ms 1 l ij length of the interface between cells i and j m l sew i sewer pipe length within cell i m p rainfall volume over a time step m s cf storage in the capillary fringe m se k saturation level of reservoir k se k k 1 saturation level at the interface between reservoirs k and k 1 s k storage in the reservoir k m s surf storage in the surface reservoir m sy specific yield sw m capillary pressure at the infiltration front lower limit m t current time s t p time between the beginning of the time step and the beginning of the surface ponding s t ij transmissivity at the interface between cells i and j m2s 1 tr k root water uptake in reservoir k ms 1 v ij volume exchanged between cells i and j m3 v dr i volume drained by a drainage system within a cell i m3 v sew i volume drained by a sewer pipe within a cell i m3 v well i volume pumped injected by a well within a cell i m3 q max maximum flow rate per surface unit ms 1 x y z coordinates m z dr i drainage system elevation in cell i m z sew i sewer pipe elevation in cell i m δt time step t ψ int capillary pressure at the interface between reservoirs m ψ k capillary pressure in reservoir k m ψ e k capillary pressure at air entry value in reservoir k m ψ fc k capillary pressure at field capacity in reservoir k m ψ wp k capillary pressure at wilting point in reservoir k m θ k water content in reservoir k θs k saturated water content in reservoir k θr k residual water content in reservoir k acronym bc boundary condition cf capillary fringe gw groundwater sz saturated zone uz unsaturated zone 1 introduction about half of the world s population currently lives in urban areas by 2050 these areas will be home to an additional 2 5 billion people bringing together two thirds of the world s population united nations 2014 urban expansion associated with such development implies significant changes in the hydrology of the affected watersheds the sealing of natural surfaces leads to an increase in runoff volumes and to a decrease in stormwater volumes infiltrating into the soil returning to the atmosphere trough evapotranspiration and contributing to groundwater gw recharge fletcher et al 2013 combined with the rapid transfer of runoff volumes across urban surfaces and drainage network it results in increased peak flows and volumes and shorter times of concentration fletcher et al 2013 to mitigate these effects stormwater management increasingly relies on small infiltration devices disseminated throughout urban catchments however the spatial concentration of runoff in such systems is likely to promote deep infiltration rather than evapotranspiration and thus to over recharge gw göbel et al 2004 stormwater infiltration may also be associated with highly heterogeneous gw elevation with localized mounding beneath infiltration devices following rain events bhaskar et al 2018 overall consequences of urbanization on gw levels and base flow depend on a complex water budget the balance involves a variety of sources that not only include rainfall infiltration but also other anthropogenic sources such as network leakage or irrigation and sinks such as drainage or pumping bhaskar et al 2016 besides underground water pathways local gw levels and gw flow systems are largely affected by the various soil modifications e g backfill compaction and underground structures e g sewer pipes underground car parks tunnels equipped or not with drainage system attard et al 2016a these numerous disturbances make urbanized areas highly heterogeneous environments concentrating a wide variety of natural and anthropogenic processes interacting with each other at various spatio temporal scales salvadore et al 2015 these interactions are often challenging to quantify and greatly increase the complexity of the hydrological functioning of these systems this is notably the case for interactions linking surface and underground processes for instance mounding forming beneath infiltration devices may increase transpiration in surrounding areas bonneau et al 2018 or affect in turn the infiltration capacity within the devices bouwer 2002 the rise in the water table resulting from the infiltration may also increase gw volumes drained by underground structures e g gw seepage into sewer pipes kidmose et al 2015 which affects both water volumes to be managed by sewerage systems and subsurface water storage the influence of such interactions on the hydrological functioning of the watershed depends strongly on the characteristics of the urban hydrogeological and climatic contexts in particular shallow gw contexts less than a few meters deep have a high potential for significant interactions between surface and subsurface understanding and managing the water cycle in urbanized areas in shallow gw contexts therefore requires hydrological models capable of taking these various processes and interactions into account hamel and fletcher 2014 such models should be suited to the morphology of urban environments and to their specific components they should simulate natural and urban processes involved e g runoff generation over natural and impervious surfaces evapotranspiration unsaturated flows gw interactions with underground structures they should also take into account the various interactions between these processes salvadore et al 2015 and the complex water paths in these environments e g centralised decentralised runoff management fate of drained groundwater they should allow long term continuous simulations with rather high temporal resolution a few minutes or less salvadore et al 2015 finally the computation times should not preclude the use of optimization procedures sensitivity studies or multi model scenarios to take into account the strong uncertainties regarding the underground composition only a few hydrological models designed for urban environments integrate a representation of both the surface and underground compartments in their source code e g swmm rossman 2016 wep jia et al 2001 urbs rodriguez et al 2008 these models usually incorporate a relatively fine representation of the urban environment and hydrological processes on the surface and a simplified description of the underground compartment in usual modelling contexts such descriptions provide satisfactory estimates of the influence of the underground compartment on the surface hydrology and of the impacts on subsurface storages and base flows however they neglect various processes and interactions that may significantly affect the water cycle in presence of a shallow water table and therefore limits the applicability of these models in such contexts for instance oversimplified representations of the unsaturated zone regarding soil heterogeneity water content profile and vertical fluxes e g through single reservoir approaches in urbs and swmm and non consideration of capillary rise from the water table e g in wep swmm limit the ability to represent the gw influence on evapotranspiration and surface infiltration and the gw recharge furthermore conceptual gw flow calculations e g in swmm or urbs are not appropriate to represent local fluctuations e g mounding beneath infiltration structures and their influence on surface besides the use of square mesh based gw flow computations wep doesn t suit well to urban object geometry finally interactions between gw and underground structures are usually neglected except for gw seepage into sewer pipes in urbs and pumping in wep which hinders the consideration of their influence on subsurface water storage and on water volumes to be managed by sewer systems the use of these models in shallow gw contexts therefore requires adaptations of the description of the subsurface compartment to better take into account the specificities of such environments a possible approach to overcome the limitations of these urban hydrology models is to couple them with gw models e g feflow modflow or integrated hydrogeological models developed for natural environments e g mike she gsflow if such couplings are commonly described in the literature many of them have not been designed to improve the representation of surface subsurface interactions and rather focus on gw flows as a consequence only a few of them actually consist in two ways couplings where feedbacks between surface and subsurface compartments are taken into account two way couplings e g swmm modflow zhang and chui 2020 mike urban mike she kidmose et al 2015 locatelli et al 2017 have the potential to simulate the urban water cycle in shallow gw contexts their use or implementation is however not necessarily straightforward two way couplings may first be associated with a significant increase of computation time limiting their applicability for optimization or sensitivity analysis procedures kidmose et al 2015 they may also be difficult to set up as they involve dynamic exchanges between numerous calculation variables in addition the models used to describe the subsurface compartment may not provide pragmatic alternatives to overcome the limitations regarding the description of the unsaturated zone in urban hydrology models the resolution of richards equation available in some models as the highest accuracy option can become cumbersome in many applications zha et al 2019 while simpler approaches such as the kinematic wave solutions included in modflow or gsflow are also provided they do not take into account diffusive fluxes or vertical soil heterogeneity besides except mike she mike urban existing couplings do not consider interactions between gw and underground structures although their representation is possible with most of the models used to depict the subsurface compartment their integration in couplings would require additional developments finally if several groundwater models allow the use of irregular meshes e g modflow 6 modflow usg feflow many others are based on rectangular meshes e g other versions of modflow gsflow mike she less adapted to the geometry of urban objects and the use of local refinements the features of existing software or couplings are therefore not fully in line with the objectives commonly associated with urban hydrology models in shallow gw contexts instead the introduction of selective developments in these models may also be a relevant approach to extend their applicability while taking advantage of their existing functionalities and retaining their relative simplicity in this study a set of modules is introduced to describe the role of the urban underground compartment in shallow gw environments and integrated within the urbs model the modules rely on a physically based but relatively simple depiction of this compartment and its interactions with the surface given the simplifications involved particular attention is paid to the evaluation of the modules ability to reproduce the processes and interactions of interest their integration into the urbs model aims to exploit the capabilities of an existing tool to develop a flexible modelling framework for the evaluation of the impacts of stormwater management strategies or urban developments on the water cycle in a variety of contexts including shallow gw environments this tool i is suited to the complexity of urban landscapes and the geometry of their components ii is able to simulate the evolution of the various terms of the water balance the surface and subsurface storage and the complex water pathways in these environments decentralised centralized management of runoff subsurface fate of infiltrated water fate of drained groundwater or groundwater resurgence etc and iii can be used to perform continuous simulations over long periods with computation times enabling optimization or sensitivity analysis procedures to be implemented the first section of the paper details the theoretical basis of the subsurface modelling approach it is based on 1 a 2d gw flow computation module adapted to irregular meshes and allowing interactions with several underground structures to be taken into account sewer systems wells underground constructions with or without draining systems and 2 a reservoir model for the computation of vertical uz flows capable of taking into account layered soils vegetation transpiration and upward capillary rise from the water table section 2 of the paper details the diagnosis conducted to better understand the functioning and limitations of these modules regarding the processes and interactions of interest it is based on comparisons with reference models hydrus 1d šimůnek et al 2005 and feflow diersch 2014 over a set of hypothetical test cases section 3 introduces the integration of these modules into the urbs model it also presents an application to a hypothetical yet realistic watershed designed to illustrate the model s applicability and potential benefits of considering such processes and interactions in shallow gw environments with stormwater infiltration devices finally section 4 discusses the benefits and limits of the approach 2 theoretical basis of the new modules this section details the theoretical basis of the modules developed to represent the urban underground compartment the first sub section describes the computation of gw flows and interactions with underground structures the second sub section focuses on the uz flows computation finally the last sub section details the computation procedure 2 1 saturated zone 2 1 1 lateral groundwater flows the modelling of gw flow is based on a 2d application of darcy s law at the interfaces between computation cells eq 1 fig 1 the computation is done by sequentially estimating the flow at each interface on the basis of hydraulic heads at the previous time step 1 v i j h n t i j l i j δ t where v ij m3 is the volume exchanged between cells i and j during a time step δt h the hydraulic gradient n the interface normal vector t ij m2s 1 the transmissivity at the interface and l ij m is the interface length t ij is computed at each time step by integrating the hydraulic conductivity along the interface according to z between the bed rock and the water level linearly interpolated between cells i and j in the case of layered soils the wet thickness of each soil layers is considered for the calculation of transmissivity if cells i and j have different geological properties t ij is taken equal to the harmonic mean of the transmissivity computed for each cell a simple estimation of the hydraulic gradient at the interface as the head difference between cells i and j divided by the distance between the centroids of the cells would produce inaccurate results for non rectangular cells the use of irregular meshes can however be relevant to better fit the geometry of urban objects to ensure the validity of the computation on such meshes the approach relies on the use of a third cell a to estimate the components of the hydraulic gradient along the interface de marsily 1981 fig 1 the third cell is selected by limiting its distance from the interface and excluding cells whose centre of gravity would be close to an alignment with those of cells i and j based on these three cells a taylor series expansion at the first order for any point m on the interface gives 2 h i h m x i x m δ h δ x m y i y m δ h δ y m h j h m x j x m δ h δ x m y j y m δ h δ y m h a h m x a x m δ h δ x m y a y m δ h δ y m where h i h j and h m are the hydraulic head within cell i j and on the interface respectively and x i y i x j y j x m and y m are the coordinates of the centre of gravity of cell i j and of the point m respectively the resolution of 2 provides the components of the hydraulic gradient at the interface 3 δ h δ x m h j h i y a y i h a h i y j y i x j x i y a y i x a x i y j y i δ h δ y m h j h i x a x i h a h i x j x i x j x i y a y i x a x i y j y i h in eq 1 is then be obtained by projecting these components onto the normal at the interface 4 h δ h δ x m n x δ h δ y m n y 2 1 2 interactions with underground structures the modelling of interactions between gw and underground structures is inspired by urban hydrogeology methods it relies on the use of either specific hydrodynamic parameters e g for underground structure or boundary conditions bcs e g for drainage systems dirichlet bcs flow constraint for wells neuman bcs or for sewer systems cauchy bcs attard et al 2016b the modelling of underground constructions buildings car parks etc is carried out by assigning to cells included in the structure perimeter a hydraulic conductivity of 10 9 ms 1 over the structure depth if the structure is equipped with a drainage system the volumes v dr i m3 drained at each time step are computed according to eq 5 it aims to mimic a dirichlet bc while limiting the flow by the surrounding medium and by a maximum permissible flow rate per unit area pump limit 5 v d r i m i n h i t δ t z d r i s y k s d r i δ t q m a x δ t 0 a i with h i m the gw level in cell i z dr i m the drainage system level sy the specific yield i e the difference between the total porosity and the specific retention considered here equal to the field capacity ks dr i ms 1 the saturated hydraulic conductivity of soil below the basement a i m2 the area of cell i and q max ms 1 the maximum flow rate per unit surface the volume v sew i m3 drained by a sewer networks at each time step within a cell i is determined by eq 6 it aims to mimic a cauchy bc 6 v s e w i m i n g s e w i h i t δ t z s e w i 0 l s e w i δ t with g sew i ms 1 a coefficient characterizing the conductivity of the pipe and the surrounding medium z sew i m the pipe elevation and l sew i m the pipe length within cell i pumping and injection wells are modelled using a sink source term v well i m3 corresponding to a given pumping injection rate multiplied by the time step 2 1 3 groundwater level update at the end of each time step the gw level is updated according to eq 7 7 h i t h i t δ t j 1 n v j i t v d r i t v s e w i t v w e l l i t f u z s z t δ t a i s y i l with v ji m3 the volume exchanged with the adjacent cell j by lateral gw flow f uz sz ms 1 the exchange between uz and sz section 2 2 and sy i l the specific yield of the geological layers where the water table movement occurs each groundwater level variation is calculated using the specific yield of the reservoir in which the variation occurs if the water table changes reservoir the calculation takes into account successively the specific yield of each reservoir crossed 2 2 unsaturated zone the uz is conceptualized as an overlay of reservoirs fig 2 reservoirs may have identical hydrodynamic properties and thus constitute discretization elements or conversely they may have different properties in order to represent layered soils the number and thickness of reservoirs are not fixed and are selected for each case based on material properties and modelling objectives the brooks and corey model brooks and corey 1964 is used to represent the relation between pressure water content and hydraulic conductivity eq 8 8 s e k θ k θ r k θ r k θ s k ψ k ψ e k i f ψ k ψ e k 1 i f ψ k ψ e k k k s e k k s k s e k 2 b k 3 with se k the saturation rate ψ k m the capillary pressure ψ e k m the capillary pressure at air entry value b k a shape parameters and ks k ms 1 the saturated hydraulic conductivity of reservoir k θ k θs k and θr k are respectively the water content the saturated water content and the residual water content of reservoir k following wep s approach jia et al 2001 vertical uz fluxes computation is based on a coupling between a reservoir model and a green ampt model the following sections detail the calculation of 1 the surface infiltration flux 2 the vertical fluxes outside the infiltration front and 3 the conceptualization of the capillary fringe 2 2 1 surface infiltration and green ampt model the surface infiltration flux i surf ms 1 depends on the soil s infiltration capacity first of all infiltration is assumed to be zero when the water table is at land surface apart from this case i surf computation method depends on the ratio between the surface flux to be infiltrated i sup p t s surf t δt δt ms 1 and the infiltration capacity ca min k s 0 d 0 θs 0 θ 0 t δt δt ms 1 with p ms 1 the rainfall volume during the time step s surf m the surface storage and k s 0 ms 1 d 0 m θs 0 and θ 0 the saturated hydraulic conductivity the thickness the water content at saturation and the current water content of the upper reservoir respectively if i sup ca the incident volumes can be fully infiltrated and i surf i sup if i sup ca i surf is computed using a green ampt model adapted to layered soils jia and tamai 1997 in order to more accurately model the infiltration dynamics while taking into account layered soils and heterogeneous initial water content profiles the green ampt model calculation only occurs when ca is exceeded which makes it possible to improve the estimation of infiltrated flow without significantly increasing the calculation time this mode of activation using ca allows the use of the green ampt model when a less permeable underlying reservoir limits the infiltration capacity indeed if the infiltration rate is higher than the hydraulic conductivity of the less permeable reservoir the water content within the upper reservoirs gradually increases calculated by the reservoir model as described in section 2 2 2 until i sup ca when the green ampt model is activated the cumulative infiltration since the beginning of the activation i m is computed as follows if the surface remains unsaturated 9 i t i t δ t i s u p t δ t the surface ponding occurs if the surface volume to be infiltrated is greater than the volume estimated by the green ampt model under saturated conditions 10 i t a m 1 i s u p k s m 1 b m 1 i s u p δ t i t δ t if surface ponding occurs at the present time step 11 i p i t δ t i s u p t p t p i p i t δ t i s u p i t i p k s m t t p a m 1 l n a m 1 b m 1 i t a m 1 b m 1 i p if surface ponding occurred during a previous time step and continues since 12 i t i t δ t m i n i s u p t δ t k s m δ t a m 1 l n a m 1 b m 1 i t a m 1 b m 1 i t δ t with m the index of the reservoir where the infiltration front is located t p s the duration between the beginning of the time step and the beginning of the surface ponding and i p m the cumulative infiltration from the activation of green ampt model to the surface ponding other variables are described below 13 a m 1 k 0 m 1 d k k 0 m 1 d k k s m k s k s w m s s u r f δ θ m b m 1 k 0 m 1 d k k s m k s k δ θ m k 0 m 1 d k δ θ k with d k m the thickness of reservoir k sw m the capillary pressure at the infiltration front lower limit computed according to eq 14 using brooks and corey law δθ k the difference between the saturated water content and the initial water content i e at the time of activation of green ampt model in reservoir k 14 s w m 0 ψ m k m s e ψ k s m d ψ for more details regarding the derivation of eqs 9 14 the reader may refer to jia and tamai 1997 if the saturation front reaches a reservoir with a higher hydraulic conductivity the inflow is not sufficient to saturate the new material and green ampt model s applicability assumptions are no longer met the green ampt model is then applied only within the upper reservoirs according to eq 15 the downflow at the reservoir bottom f rm ms 1 computed with the green ampt model is used as the upper limit condition for the reservoir model within the underlying reservoir in any other case f rm 0 ms 1 15 i s u r f f r m k e f f 1 s w m s s u r f k 0 m d k with k eff ms 1 the effective hydraulic conductivity of the medium above the saturation front obtained by a harmonic mean of the saturated hydraulic conductivity of reservoir weighted by their thickness if the saturation front reaches the substratum or the gw table infiltration stops and the gw level is fixed at soil surface this amounts to considering that the saturated column belongs to the saturated zone and contributes to increase the hydraulic head 2 2 2 unsaturated fluxes computation using the reservoir model vertical fluxes between reservoirs and vegetation transpiration are computed using a reservoir model when the green ampt model is not activated the fluxes are sequentially computed from the capillary fringe upper limit reservoir n to the ground surface reservoir 0 on the basis of capillary pressures at the previous time step fig 2 when the green ampt model is activated exchanges between reservoirs are still computed up to the reservoir located under the one including the infiltration front in order to take into account fluxes under the front in particular exchanges with the sz when the infiltration front computed by the green ampt model enters a reservoir the latter is excluded from the reservoir model computation the water content within the reservoir is then used as initial water content in the green ampt model for computing the progression of the infiltration front conversely when the green ampt model is disabled the water content in the various reservoirs is updated by adding the additional volume of the infiltration front contained therein this procedure ensures the water balance in uz the balance equation for a given reservoir k between 0 and n is given by eq 16 16 s k t s k t δ t f k 1 k t f k k 1 t t r k t δ t with s k m the storage in reservoir k f k k 1 ms 1 the exchange flux gravity and diffusion between reservoir k and k 1 and tr k ms 1 the root water uptake flux in reservoir k the computation of the various flux is subsequently described fluxes are limited by the storage of the incoming and outgoing reservoirs so that the water content remains between its residual and saturation level the root water uptake flux tr k within reservoir k is computed using the feddes model feddes et al 1978 17 t r k t min ψ k t δ t ψ w p k ψ f c k ψ w p k 1 d r k e d t with ψ k m the capillary pressure in reservoir k computed according to brooks and corey s law ψ wp k m and ψ fc k m the capillary pressure at wilting point i e 150 m and at field capacity i e 3 3 m respectively dr k the root density in the reservoir k k d r k 1 and ed ms 1 the evaporative demand exchange flux between two reservoirs f k k 1 is computed according to eq 18 it takes into account gravity flux first term in the bracket and diffusion flux second term in the bracket positive exchange fluxes are oriented downwards 18 f k k 1 t k k k 1 t 1 ψ k 1 t δ t ψ k t δ t d k d k 1 2 with k k k 1 ms 1 the average hydraulic conductivity at interface between reservoir k and k 1 and d k the thickness of reservoir k the calculation method of k k k 1 depends on whether the two reservoirs have identical or different hydrodynamic characteristics if the reservoirs have identical hydrodynamic characteristics k k k 1 is computed according to brooks and corey s law eq 8 considering an average saturation se k k 1 the latter is computed according to eq 19 it assumes that when the saturation rate of the upper reservoir is higher i e in the case of marked downward flux the saturation level of the upper reservoir governs the advance of the infiltration front 19 s e k k 1 t m a x s e k t s e k t s e k 1 t 2 if the reservoirs have different hydrodynamic characteristics k k k 1 is equal to the average hydraulic conductivity of the two reservoirs eq 20 computed using the brooks and corey law eq 8 20 k k k 1 t k k s k t k k 1 s k 1 t 2 the limits of the chosen parameterizations as well as those associated with the use of other types of averages are discussed in supplementary material section s1 and s3 for the upper reservoir f k k 1 is the surface infiltration i surf ms 1 section 2 2 1 if green ampt model is deactivated or f rm otherwise for the lowest reservoir n f k k 1 is the exchange flux with the saturated zone f uz sz ms 1 the latter is computed between the middle of the lower reservoir and the top of the capillary fringe cf according to eq 21 which is a direct application of eq 18 within these conditions the capillary pressure value at the top of the cf ψ cf is by default equal to the air entry pressure the special case of a material transition is detailed in the following section 2 2 3 note that the thickness of this reservoir d n varies with the depth of the gw table see section 2 3 21 f u z s z t k n s e n t δ t 1 2 1 ψ c f ψ n t δ t d n 2 2 2 3 capillary fringe conceptualization as the brooks and corey model cannot model variably saturated soil within the capillary fringe se k 1 if ψ k ψ e k the model assumes a conceptualization of the cf the cf is assumed to behave like a saturated zone uz sz exchanges through the cf are assumed to be instantaneous the cf belongs to uz and the corresponding water storage s cf m is then assigned to the uz storage by default the cf thickness and the capillary pressure at its upper boundary ψ cf are equal to the air entry pressure of the material ψ e k for stratified soils the model conceptualizes the influence of material transitions on the capillary fringe fig 3 consider two superimposed geological layers k and k 1 with k on top one made of coarse material and the other of fine material with the top of the water table located in the lower layer k 1 consider ψ int the capillary pressure at the interface at hydrostatic equilibrium condition i e equal to the distance between the interface and the top of the water table fig 3 to facilitate interpretation the various pressures are expressed in absolute values in the remainder of this paragraph if the upper material is coarser than the lower one then ψ e k ψ e k 1 and ψ e k ψ int ψ e k 1 and θ k ψ int θs k the cf upper boundary is thus fixed at the interface its storage is s cf ψ int θs k and the pressure value used for f uz sz computation is ψ cf ψ int fig 3a if the lower material is coarser than the upper then ψ e k ψ e k 1 and ψ e k 1 ψ int ψ e k and the cf reaches the upper reservoir while the lower reservoir remains unsaturated over a ψ int ψ e k 1 thickness fig 3b ψ cf ψ e k and the cf storage is thus computed as follows s cf ψ e k ψ int θs k ψ int ψ e k 1 θ m ψ e k 1 θs k 1 considering an estimate of the mean water content within this thickness θ m θ ψ m with ψ m ψ int ψ e k 1 2 fig 3b computed according to brooks and corey law eq 8 2 3 computation procedure from the beginning of the time step the computation follows the subsequent steps 1 computation of the various fluxes and storage related to the uz for each cell section 2 2 2 computation of the interactions between gw and underground structures for each cell section 2 1 3 computation of gw flows for each interface between cells section 2 1 4 update of the gw level according to eq 7 for each cell considering the specific yield of each reservoir intersected by the water table movement 5 update of the cf level for each cell section 2 2 3 6 update of the thickness of the lower reservoir for each cell accordingly to the new cf level in order to avoid numerical oscillations the thickness cannot fall below a threshold d min m typically about a few centimetres informed by the user and depending on the material properties below this threshold the reservoir is merged with the upper reservoir 7 update of the water content in the uz for each cell indeed as gw movements and consequently cf movements are determined on the basis of fixed specific yields maintaining the balance requires a reallocation of the uz volume located in the variation thickness of gw additionally in the case of layered soils the cf storage may vary within a time step as presented in section 2 2 3 the sum of these two differences positive or negative is added to the overlying reservoirs and its water content is updated if it leads to exceeding the water content at saturation the excess is added to the overlying reservoir and so on until the surface if all the reservoirs are saturated conversely if it leads to fall below the residual water content the corresponding deficit is spread over the overlying reservoir if it cannot be distributed over the soil column the water table level is lowered accordingly 3 evaluation of the subsurface modelling approach the modules are evaluated over a set of hypothetical test cases the evaluation is based on a comparison between the results provided by the modules and those obtained by reference models hydrus 1d šimůnek et al 2005 for the 1d fluxes in unsaturated zone and feflow diersch 2014 for the gw flows and the coupling between uz and sz modules results are compared over a set of variables distributed either in time e g infiltration gw level fluctuations at one point or in space e g gw levels at steady state comparison variables are specified in the corresponding test case subsections the error related to spatial distributions of variables is quantified focusing on the mean absolute error e xy mean eq 22 and the maximum absolute error e xy max eq 23 due to the difference in spatial discretization the results are interpolated for comparison on a regular g grid composed of 250 000 points spaced 1 m apart 22 e x y m e a n x y g x x y x r e f x y 250000 23 e x y m a x max x y g x x y x r e f x y with x and x ref the variable simulated by the modules presented in this study and the reference model respectively the error related to temporal distributions of variables is quantified focusing on the mean absolute error e t mean eq 24 and the determination criteria r 2 eq 25 24 e t m e a n t 0 n x t x r e f t n 25 r ² t 0 n x t x x t x r e f ² t 0 n x t x ² t 0 n x r e f t x r e f ² the test cases are divided into three categories depending on whether they focus on the sz subsection 3 1 the uz subsection 3 2 or the coupled sz uz system subsection 3 3 table 1 lists the characteristics of the various soils used chosen to cover a wide range of soils encountered in the literature morris and johnson 1967 table 2 summarizes the main characteristics of the various test cases 3 1 evaluation of the saturated zone modelling approach three test cases are dedicated to the evaluation of the sz modelling approach presented in section 2 1 for this evaluation the sz module is decoupled from the other modules gw flows are computed with a fixed time step of 6 min time step usually used in the urbs model and corresponding to rainfall data provided by the french national meteorological service the various meshes are made up of irregular elements with and refinement in the vicinity of sharp variations in flow conditions all the meshes used are shown in section s2 of the supplementary material the three test cases are based on a hypothetical 500 m 500 m aquifer with a thickness of 10 m the slope of the surface and bedrock is 0 5 except for test case sz1 the medium is homogeneous and made of coarse sand the gw level is set at a 2 m depth at the upstream and downstream boundaries so that the surface of the gw table is parallel to the model s top and bottom at natural steady state for a homogeneous medium 3 1 1 test case sz1 ability to simulate groundwater flows in a heterogeneous aquifer test case sz1 aims to evaluate the model s ability to reproduce steady state gw flow in the case of abrupt lateral variations in the surrounding environment such as changes in geological material or local anthropogenic modifications of the environment the aquifer is therefore divided into three zones of varying hydraulic conductivity and specific yield as described in fig 4 a the assessment is based on a comparison with results provided by the feflow model solving the darcy s law in 3d the domain is discretized horizontally into triangular elements with a refinement near to material transitions and vertically into 1 m thick layers the comparison focuses on the steady state distributed gw levels h xy considering the mean and maximum value of the absolute error respectively e xy mean eq 22 and e xy max eq 23 fig 7a shows the comparison between the steady state groundwater levels simulated by feflow and the sz module over the whole domain the maximum error is about 12 cm and the mean error is about 2 5 cm table 3 which is reasonable considering the uncertainties related to the composition of the underground compartment or to measurements the model slightly underestimates the water level in the most permeable part lower part in fig 5 a the maximal error is located at the transition between materials near the upstream boundary condition and gradually decreases downstream the mesh refinement here maintains a satisfactory level of error without excessively increasing the number of elements as shown in section s3 1 of the supplementary material the estimation tends towards the reference result when the mesh is more refined 3 1 2 test case sz2 ability to simulate groundwater fluctuations around sinks sources test case sz2 is designed to assess the module s ability to simulate sharp transitory disturbances of gw level generated by a source sink term such as injection pumping well or concentrated water inflows from infiltration devices a well is therefore added in the domain centre its operation is described in fig 4d the assessment is based on a comparison with results provided by the feflow model solving the darcy s law in 3d the domain is discretized horizontally into triangular elements with a refinement near to the well and vertically into 1 m thick layers the well is modelled in the feflow model using a multilayer well bcs the evaluation focuses on the spatial distribution of gw levels h xy after at t 50 d end of the well s operation considering the mean absolute error e xy mean eq 22 and the maximal absolute error e xy max eq 23 the temporal evolution of gw levels at the well location h t center and 50 m upstream h t 50m up considering the mean absolute error e t mean eq 24 and the determination criteria r 2 eq 25 results related to this test case fig 5b and c and table 4 show that the module satisfactorily reproduces the influence of a sink source term on gw levels the maximal error 13 cm is located in the vicinity of the well it is directly related to the homogenization of the injected pumped volumes over the cell surface here about 400 m2 greater accuracy would therefore be achieved by reducing the size of the well cell see section s3 1 of the supplementary material however the low average error and the good matching of head isolines show that the error is quickly smoothed on adjacent cells these various observations are confirmed by the comparison of gw level evolution at the domain centre and 50 m upstream fig 5c 3 1 3 test case sz3 ability to simulate interactions between groundwater and underground structures test case sz3 aims to evaluate the module s ability to estimate interactions between gw and underground structures through three sub cases focusing on a sewer pipe sz3a an impervious building basement sz3b and a building basement equipped with a draining system sz3c in test case sz3a a 300 m sewer pipe g sew 10 5 ms 1 is added in the central part of the domain perpendicular to the direction of flow fig 4b and at a depth of 4 m in test case sz3b and sz3c a building extending 100 m 20 m and 6 m deep two underground floors is added in the central part of the domain fig 4c in case sz3c the building is equipped with a draining system to lower the gw level to a depth of 6 m the assessment is based on a comparison with results provided by the feflow model solving the darcy s law in 3d the domain is discretized horizontally into triangular elements with a refinement near to the underground structures and vertically into 1 m thick layers the sewer pipe is modelled by assigning a cauchy boundary condition over a 0 5 m 300 m horizontal strip the building s basement is modelled by assigning a hydraulic conductivity of 10 9 ms 1 to cells within the building s perimeter the building s drainage system is modelled by applying a dirichlet boundary condition on the horizontal elements located at the basement bottom the evaluation focuses on the spatial distribution of steady state gw levels h xy considering the mean absolute error e xy mean eq 22 and the maximal absolute error e xy max eq 23 the temporal evolution of gw volumes drained by the sewer pipe and the draining system v t considering the mean absolute error e t mean eq 24 and the determination criteria r 2 eq 25 results related to test case sz3a to c are shown in table 5 and fig 6 the influence of the sewer pipe test case sz3a on steady state gw levels is satisfactorily reproduced despite a slight overestimation in the vicinity of the structure table 5 fig 6a as for the well it is linked to the distribution of infiltrated flows over a large area the module also well estimates the dynamics of gw drainage through the pipe table 5 fig 6b the influence of the impervious basements test case sz3b is satisfactorily estimated however results are significantly less good for the basement equipped with a draining system test case sz3c table 5 fig 6c if the flow shape is overall well reproduced the error is significantly greater than in the previous cases up to 57 cm in the vicinity of the building and fig 6c shows slight deformations of the head isolines in the vicinity of the basement e g at x 300 y 150 additionally the model strongly underestimates the gw drainage by the draining system at the beginning of the simulation although the steady state drainage flows are well estimated table 5 fig 7 these two observations reflect the insufficient refinement of the discretization near the building regarding the strong hydraulic gradients it generates see section s3 1 of the supplementary material 3 2 evaluation of the unsaturated zone modelling approach three test cases are dedicated to the evaluation of the uz modelling approach presented in section 2 2 for this evaluation the uz module is decoupled from the other modules the various processes are computed with a constant time step of 6 min soil columns are discretized into 0 5 m reservoirs in order to maintain a constant level of the water table level at the column bottom imposed pressure head equal to 0 m the computation of i surf and f uz sz when the ga infiltration front reaches the water table is done according to eq 15 considering a zero suction the test cases focus on single soil columns on which various rainfall and root water uptake conditions are applied to generate usual vertical water movements within common soils or infiltration devices columns are delimited in their lower boundary by a constant gw depth and are initially at a moisture content profile equilibrium state i e dψ dz 3 2 1 test case uz1 ability to simulate upward capillary rise from the water table the uz1 test case is designed to evaluate the model s ability to simulate the upward water movement from the gw to supply the upper soil horizons during periods of low precipitation and high evaporation demand it is based on a 2 m column of silt with a homogeneous root distribution over the first 50 cm fig 8 a surface forcing consists of two 30 days periods of high evaporative demand without precipitation fig 8c the assessment is based on a comparison with results provided by the hydrus model solving the 1d richards equations with the brooks and corey s model the column is discretized into 0 5 cm thick elements an atmospheric bc with a maximal pressure head of 1 m is assigned at the surface an imposed pressure at h 0 m is assigned at the column bottom the root water extraction is modelled using the feddes model without compensation šimůnek and hopmans 2009 the evaluation focuses on the temporal evolution of the root water uptake tr the exchanges between uz and sz f uz sz the mean water content within the root zone θ rz and within the whole column θ uz considering the mean absolute error e t moy eq 24 and the determination criteria r 2 eq 25 results related to this test case are shown in table 6 and fig 9 it shows that the reservoir model slightly overestimates the root water uptake tr this higher extraction of soil water directly results in lower average water content throughout the soil column θ r z and θ u z and as a consequence in a higher capillary rise from the gw f uz sz the tr overestimation is reinforced by the hydrus configuration feddes without compensation which implies that the model does not seek to compensate for a decrease in root water uptake in stressed areas of the root profile by an increase in less stressed areas conversely in the reservoir model the root zone is entirely included in the first reservoir 50 cm this implies an average value of the water content over the whole root zone a reduction in the reservoir thickness then leads to a significant improvement in results see section s3 2 of the supplementary material 3 2 2 test case uz2 ability to simulate the evolution of storages and fluxes within homogeneous and layered unsaturated soils under various rainfall intensities test case uz2 aims to evaluate the model s accuracy in simulating the evolution of vertical fluxes and storage in homogeneous and layered soils subjected to periods of rainfall of various intensities and to redistribution periods with root extraction the objective is to consider a set of soil and surface input combinations representative of common permeable areas and stormwater infiltration structures three sub cases differing by soil characteristics are therefore considered test case uz2a is based on a 3 m homogeneous column of silt test cases uz2b and uz2c cases rely on 3 m columns of layered soils designed to assess the module s ability to represent the influence of a different underlying soil layer they are composed of a 0 5 m surface layer of silt on top of a 2 5 m layer of a material either more permeable uz2b sand or less permeable uz2c clay fig 8b for the three sub cases roots are homogeneously distributed over the first 25 cm the rainfall inputs fig 8d identical within the three sub cases are set to meet various possible cases of exceeding or not exceeding the infiltration capacity of the materials they are interspersed with periods of moderate evaporative demand favouring the redistribution of water fig 8d the assessment is based on a comparison with results provided by the hydrus model configured identically to case uz1 except for the column extension and soil characteristics the evaluation focuses on the temporal evolution of the surface infiltration i surf the root water uptake tr the exchanges between uz and sz f uz sz the surface storage s surf and the mean water content within the root zone θ rz and within the whole column θ uz considering the mean absolute error e t mean eq 24 and the determination criteria r 2 eq 25 results related to the test case uz2a show that the model properly estimates rainfall infiltration i surf in a homogeneous soil table 6 fig 10 using both the reservoir model first rainfall pulse with i sup ks and the green ampt model second and third pulses with i sup ks the latter provides better results when the inflow more distinctly exceeds the hydraulic saturation conductivity of the surface soil the third pulse the good matching of mean water contents θ r z and θ u z variations during and between rainfall inputs validates the coupling between the green ampt model and the reservoir model the root water extraction tr is overall satisfactorily reproduced by hypothesis tr is not computed in the reservoirs where the green ampt computation occurs this leads to the error visible at t 80 h exchanges with the sz f uz sz are overall less accurately reproduced for t 120 h they are computed by the reservoir model the homogenization of incoming volumes over the lower reservoir thus leads to a smoother and earlier recharge 90 h t 120 h fig 10 for t 120 h the infiltration front computed by the green ampt model enters the lowest reservoir the reservoir model computation within this reservoir therefore stops and f uz sz suddenly drops to zero until the infiltration front reaches the gw table t 120 h fig 13 at t 135 h the green ampt model computation stops f uz sz therefore drops during this time step before being calculated with the reservoir model at the next time step a decrease in the reservoir thickness near the water table leads to a significant improvement in the estimation of these exchanges see section s3 2 of the supplementary material the model satisfactorily reproduces the impact of a transition to a more permeable environment on the various uz variables case uz2b table 6 fig 11 a and b the green ampt model slightly underestimates i surf when the infiltration front reaches the boundary between media 65 h t 80 h and 105 h t 130 h fig 11a it results in a slight overestimation of the surface ponding s surf table 6 fig 11b not computing tr within reservoirs where the green ampt computation occurs results here in a significant underestimation of this flux t 80 h and t 120 h fig 11a the gw recharge t 90 h is here estimated by the reservoir model which lead to similar conclusions as for the homogeneous soil earlier and smoother recharge the model also properly reproduces the influence of an underlying low permeable layer on the various fluxes and storages case uz2b table 6 fig 11 c and d i surf and s surf are accurately reproduced however the green ampt model slightly underestimates the time for the front to reach the water table t 130 h fig 11c in contrast to the previous cases a refinement of discretization does not correct this error see section 3 2 of the supplementary material 3 3 evaluation of the coupling three test cases are dedicated to the evaluation of the coupling between uz and sz they are simulated using the whole subsurface modelling approach described in section 2 using a constant time step of 6 min 3 3 1 test case co1 ability to simulate interactions between water table variation and unsaturated zone test case co1 is intended to assess interactions between gw and uz during gw fluctuations it focuses on a single 4 m column of soil without lateral exchanges subject to rainfall inputs and a sz sink term e g pumping well table 7 and fig 12 the column is initially at hydrostatic equilibrium i e dh dz with a bottom boundary condition fixed at h 1 m i e the gw table is at a 3 m depth from the surface the column is discretized into 0 5 m reservoirs the minimum thickness of the lower reservoir is set at d min 0 1 m test case co1 is divided into three sub cases test case co1a focuses on a homogeneous column of silt test cases co1b and co1c focuses on layered columns to assess the model s ability to reproduce the influence of material change including the influence of the cf conceptualization presented in section 2 2 3 on groundwater level variations two configurations are considered a more permeable soil above co1b 2 5 m of silt over 1 5 m of sand or a more permeable soil below co1c 2 5 m of sand over 1 5 m of silt table 1 and fig 12a the assessment is based on a comparison with results provided by the hydrus model solving the 1d richards equations with the brooks and corey s model the column is discretized into 0 5 cm thick elements an atmospheric bc with a maximal pressure head of 1 m is assigned at the surface a variable flow bc is assigned at the column bottom the evaluation focuses on the temporal evolution of the water table elevation h t and the storage in the unsaturated zone s uz considering the mean absolute error e t mean eq 24 and the determination criteria r 2 eq 25 results related to test cases co1a to c table 7 and fig 13 show the model s ability to estimate the water table h fluctuations and their influence on the unsaturated zone storage s uz the system properly returns to its initial condition at the end of the simulation the sum of the inflows outflows being zero results also validate the conceptualization of the capillary fringe in test cases co1b and co1c the water table intersects the interface between mediums both during rise and fall and the two specific cases presented in section 2 2 3 are encountered in both cases the impact on both h and s uz is satisfactorily reproduced however the coarseness of the discretization leads to sharp variation in the water content when the sz upper limit moves to a different reservoir during following time steps this results in adjustments of the water content by uz sz exchanges to restore the equilibrium fig 13 a refinement of the discretization attenuates this effect but does not completely prevent it see section s3 3 of the supplementary material 3 3 2 test case co2 ability to simulate the water table evolution under spatially homogeneous or heterogeneous upper input test cases co2a and co3b are designed to evaluate the coupling s accuracy in simulating the evolution of a water table in an unconfined aquifer subjected to a time varying surface input the latter is either spatially homogeneous co2a or heterogeneous co2b in order to reproduce a recharge similar to that occurring in a heterogeneous urban environment with infiltration devices the domain is a homogeneous medium composed of silt with an extension of 500 m 500 m and a thickness of 20 m the slope of the top and bottom is 0 5 the gw level is fixed on the upstream and downstream boundaries at a depth of 3 m so that the surface of the gw table is parallel to the model s top and bottom at equilibrium state in its initial state the uz sz system is in its moisture content profile equilibrium state the reference rainfall pattern used as forcing is shown in fig 12c in test case co2a it is applied as is over the entire surface of the domain in test case co2b the surface is divided into 625 areas associated with three types of surface boundary conditions fig 12d constructed by applying a multiplying factor 0 1 or 10 to the reference rainfall intensity fig 12c the spatial discretization used for the simulation of both test cases corresponds to this division into 625 square cells each cell is then vertically discretized into 0 5 m reservoirs within the first 4 m at top of a reservoir of 16 m the assessment is based on a comparison with results provided by the feflow model solving the 3d richards equations with the brooks and corey s model the domain is horizontally discretized into elements of about 6 m2 vertically it is divided into layers of 0 05 m thick in the first meters to 1 m in the saturated part the rainfall input is modelled by an inflow at the top of the domain for the initial condition a preliminary simulation without rainfall is performed until a steady state is reached the evaluation focuses on the spatial distribution of gw levels after 50 days h xy 50d 100 days h xy 100d and 300 days h xy 300d considering the mean absolute error e xy mean eq 22 and the maximal absolute error e xy max eq 23 the temporal evolution of gw levels at two points of the domain per test case considering the mean absolute error e t mean eq 24 and the determination criteria r 2 eq 25 for test case co2a the domain centre h t center and 120 m downstream h t 120m for test case co2b beneath an area of zero surface infiltration domain center h t center and beneath an area of concentrated surface infiltration coordinates x 310 y 370 h t bc10 results related to the test case co2 table 8 and fig 14 a and c show that the fluctuations of a water table subject to a spatially homogeneous surface input are overall well reproduced the model also satisfactorily reproduces gw level fluctuations with a heterogeneous surface forcing test case co2b table 8 and fig 14 b and d below areas where the inflow is concentrated the maximum elevation is slightly overestimated and the decrease in water table recession is sharper fig 14d as noticed in test case co1 the water tale level may drop when the sz upper limit moves to a different reservoir this is especially noticeable during the lowering phase t 90 d fig 14d a refinement of the vertical discretization leads to smoothing the latter effect but only marginally influences overestimation of the peak elevation and of the recession rate see section 3 3 of the supplementary material 4 integration within the urbs model and preliminary application this section presents the integration of the developed modules within the urbs model sub section 4 1 and a preliminary application of the model to a hypothetical yet realistic watershed sub section 4 2 4 1 brief description of the urbs model and integration of the new modules the urbs model provides continuous simulations of the hydrological functioning of urbanized watersheds rodriguez et al 2008 it is based on a watershed discretization into urban hydrological elements uhe s each composed of a cadastral parcel and half of the associated street section this results in a highly irregular grid adapted to the structuring elements of the urban environment on each uhe fluxes and storages are computed by land use profile natural street building natural and street profiles can be partially covered by trees to take into account their role in rain interception and evapotranspiration runoff volumes generated over the various profiles are routed either to the outlet via the hydrographic network composed by street segments and sewer pipes or to stormwater management devices green roofs reservoir pavements infiltration devices the initial modelling of the underground compartment assumes a soil where the water flows are controlled by an exponential decrease in hydraulic conductivity with depth the soil characteristics are homogeneous over the simulation domain the unsaturated zone is modelled by a single reservoir fed by infiltration from the surface storage at constant rate emptied by evapotranspiration the exchanges between the unsaturated zone and the saturated zone take into account both gravitational and diffusion fluxes gw flows are computed at each interface between uhes using a 2d darcy computation without correction of the gradient computation method for irregular meshes gw seepage into sewer pipes is considered by using an analytical formula derived from agricultural drainage for more details the reader may refer to rodriguez et al 2008 morena 2004 and li 2015 new modules introduced in section 2 are integrated into the urbs model as alternative modelling options however urbs specificities require coupling assumptions and intermediate calculation steps first of all the uz flow computation by land use profile implies that each sz computation cell could be associated with of up to three uz soil columns each characterized by its own water content profile and uz sz exchange flow uz computation on each land use profile is then done using an identical gw level and the exchange flows with the sz are averaged over the computation cell the water content update at the end of the time step section 2 3 is then carried out by land use profile fig 15 presents a conceptual representation of the partitioning of sub uhe by land use profiles and of the flows calculated on each profile surface discretization into uhes according to cadastral parcels is not optimal for the computation of underground flows 1 regarding the geometry of urban objects e g infiltration devices building basements 2 regarding the characteristic scale of processes to be modelled e g mounding effect or 3 to ensure an accurate computation consequently the possibility of using a refined underground mesh is added in such case each sub uhe must be fully included in a single uhe to ensure proper code operation each sub uhe can intersect several land use profiles on each sub uhe subsurface flow computation is performed as described in section 2 surface storage and runoff remain computed at the uhe scale the infiltrated volume over each land use profile is then computed by averaging the contribution of each sub uhe weighted by the intersecting area of each profile this sub level of discretization is defined in pre processing and can be obtained for example using gis tools to divide polygons here the uhes into triangles the application case presented in the following section provides an example of such a division of the uhes into triangular sub uhes fig 16 4 2 application to a hypothetical urban watershed this part presents an application of the resulting coupled model to a hypothetical yet realistic urban watershed concentrating various interactions between surface and sub surface hydrology it does not aim to validate the model but to highlight potential benefits of taking into account underground interactions when modelling urban water cycle in shallow gw environments 4 2 1 description of the application case the watershed has an extension of 500 m 500 m a constant thickness of 10 m and a uniform slope of 0 5 it is composed of 70 uhes varying from 1100 m2 to 40 000 m2 and spread over various areas representing either a dense urban environment a commercial area residential area and a natural area fig 16 from the bedrock to the surface the soil is composed of a 9 3 m layer of loamy sand representing an aquifer formation a 0 4 m layer of sand representing a reworked geological formation and a 0 3 m layer of sandy clay loam table 1 representing the topsoil three buildings have basements set at 3 m one underground level with a hydraulic conductivity of 10 9 ms 1 and one of them is equipped with a drainage system fig 16 the sewer pipes combined sewer systems are located under the roads fig 16 at a depth of 2 m their conductivity is considered homogeneous and equal to g sew 10 6 ms 1 stormwater is managed by several infiltration devices fig 16 with a 20 cm depth variable surfaces and where the soil is considered to be reworked the two upper geological layers are replaced by a single layer of sand in this simplified application the runoff is assumed to be transferred immediately to the infiltration structures and or the basin outlet without calculation of network flows the gw level is fixed on the upstream and downstream boundaries respectively left and right boundaries in fig 16 at a 2 m depth so that the surface of the gw table is parallel to the model s top and bottom at equilibrium state without underground structures a preliminary simulation without rainfall is done until steady state is reached in order to prevent artificial overestimation of the drained volumes in its initial state the uz sz system is in its moisture content profile equilibrium state five scenarios are built by successively adding components to the underground representation the first scenario does not consider any interaction with the gw the bedrock and the gw table are lowered down 20 m the second scenario focuses on gw interactions with surface without any underground structures the third scenario includes sewer pipes but no building basements with or without draining systems the fourth scenario includes all the underground elements as detailed in the previous paragraph the fifth scenario considers all underground elements but without infiltration devices in order to evaluate potential influence of these devices on the underground structures the various scenarios are simulated over a continuous 1 year period using real rainfall records 6 min and pet records hourly disaggregated from daily from the trappes station 48 77 n 2 01 e for the year 2013 fig 17 the model is used with a constant 6 min time step the subsurface mesh fig 16 is made of 1238 triangular cells with a refinement in the vicinity of the areas where sharp hydraulic gradients are expected fig 16 4 2 2 simulation results the simulation results related to the various scenarios are presented in fig 18 and table 9 and subsequently described results related to the first scenario show that when interactions with gw are neglected scenario 1 almost all stormwater volumes conveyed to infiltration devices infiltrates the comparison with the second scenario highlights the significant influence of considering interactions between gw and infiltration devices on simulations results this directly results in a significant drop in infiltration within the structures and consequently in an increase in runoff in addition taking into account the influence of capillary rise from the shallow gw leads here to a higher estimation of the transpiration the inclusion of sewer pipes scenario 3 leads to a slight decrease in the simulated rise of the water table both overall and in the mounding beneath infiltration devices table 9 fig 18 the interactions between gw and surface are consequently reduced and the simulation results present 1 an increase of the infiltration volumes in the infiltration devices and 2 a decrease of the evapotranspiration compared to the second scenario gw only the inclusion of all underground structures scenario 4 leads to similar but amplified conclusions this is mainly due to the high gw level reduction resulting from the draining system equipping the building basement which further reduces interactions between gw and surface compartment comparison of gw levels at the end of the simulation fig 18a shows that this effect is locally more noticeable especially for the infiltration device located upstream of the drainage system this figure also shows that the draining system is a major outlet for the area and largely captures the water infiltrated into the upstream infiltration device it strongly modifies local flow directions which leads among others to disruption of local exchanges between the water table and the downstream limit which could represent a watercourse for example neglecting such structures is therefore likely to bias the results when assessing the impact of infiltration strategies on runoff volumes gw levels and base flow of neighbouring watercourses taking into account pipes and underground structures equipped with draining systems also makes it possible to estimate the extent to which the infiltration devices will increase the infiltration of gw returning to the sewer system by voluntary draining system and involuntary pipes drainage for example comparison between scenario 4 and 5 without infiltration devices shows that the implementation of infiltration devices leads to a 3 5 fold increase in the gw volumes returning to the sewer system by drainage difference which represents 29 of the volumes infiltrated into the infiltration devices 5 discussion 5 1 positioning and novelty of the modelling approach the modules developed in this study satisfactorily reproduce various processes and interactions commonly neglected in urban hydrological models e g swmm urbs wep although likely to significantly influence the urban water cycle in shallow groundwater environments for instance the unsaturated zone flow calculation module takes into account the influence of vertical soil heterogeneity and of capillary upwelling from the water table on the various related fluxes e g infiltration and runoff formation evapotranspiration gw recharge or gw drainage to satisfy evaporative demand in dry periods the gw flow calculation module allows the simulation of local gw fluctuations for instance beneath stormwater infiltration devices while taking into account lateral heterogeneities of the medium and various underground structures it allows the estimation of groundwater volumes drained intentionally pumping drainage systems equipping underground structures or not e g seepage into sewer pipes it is therefore possible to estimate the influence of these structures on the underground water storage and additional volumes to be managed by the sewerage system using irregular computation cells allows mesh adjustments to fit to urban objects geometry and potential refinements for a more precise estimation of local gw fluctuations and of the related interactions with the surface or underground structures furthermore the coupling procedure between these modules takes into account the dynamic feedbacks between surface and underground compartments although the modelling approach is relatively straightforward the evaluation of the modules shows that they provide results of satisfactory accuracy regarding 1 the objectives assigned to the inclusion of the underground compartment in urban hydrological models and 2 the high uncertainties regarding the underground compartment composition they also ensure the water balance the maximum water budget error for all simulations is reached in scenario 3 of the urbs model application case the total error over one year of simulation is then below 0 001 m3 about 5 10 7 of rain volumes the integration of the developed modules within the urbs model and its application to a hypothetical urban area illustrate the relevance of such developments to extend the applicability of an existing urban hydrological model in shallow gw environments this allows the exploitation of existing model capabilities regarding the modelling of the surface hydrology of urbanized environments e g runoff generation on impermeable and permeable surfaces decentralised centralised runoff management following complex water paths transpiration by vegetation while taking into account the specificities of these contexts the final model can for example handle i the influence of groundwater fluctuations on the infiltration capacity within permeable areas and stormwater infiltration devices or on transpiration by vegetation ii localized groundwater resurgences and their fate in the urban environment or iii the influence of underground structures on groundwater storage and the fate of drained groundwater in the urban drainage system the inclusion of these various components and dynamic feedbacks in an urban hydrological model is relatively novel the level of detail in the description of the urban environment and hydrological processes makes this model suitable for studies at intermediate scales typically from the city district to small urbanized catchment a few km2 the integration of these new features as optional modules has the advantage of providing a single tool that can be easily adapted to the modelling context and objectives furthermore the relative simplicity of the additional modules maintains fairly low computation times that remain compatible with sensitivity studies or multi scenario modelling it also allows a rather straightforward implementation in existing code the open source modules developed in this study may hence be used or adapted to extend in a similar way the applicability of other hydrological models for shallow gw contexts the modules developed in this study are not intended to be a substitute for a groundwater flow calculation system as soon as the objectives imply a more accurate gw flow modelling it is necessary to use a coupling with a hydrogeological model e g modflow feflow or an integrated model dedicated to the natural environment e g mike she gsflow their implementation in the urbs model nevertheless provides a complementary tool allowing for example 1 a rapid assessment of the relative influence of the various processes and interactions involved 2 to identify where physically based groundwater models may be needed and 3 to perform multi scenario modelling to handle the high uncertainties regarding the composition of the subsurface compartment clark et al 2011 5 2 limitations and needs for further developments 5 2 1 spatio temporal discretization and computation method the developed modules assume a single fixed time step i e identical for each module and during the whole simulation which highly facilitates the management of dynamic interactions between the various model components however the time step required for surface or uz flow calculations is often unnecessarily fine for gw flow computation the use of a module specific time step does however introduce further questions on the maintenance of balances and communications between modules beegum et al 2018 branger et al 2010 both sz and uz modules rely on a sequential computation of flow between the computing units on the basis of hydraulic heads sz or capillary pressures uz at the previous time step it distinguishes them from true hydrogeological models based on a numeric resolution at each time step of the matrix system linking all the calculation units this method combined with a fixed time step limits the use of fine mesh the minimum size of the elements depending on the soil characteristics below this value the calculation may be unstable see section 3 of the supplementary material however urban hydrological modelling usually requires few minutes time step to describe surface hydrological processes such time steps alleviate these restrictions and allows a relatively stable resolution as shown by the various test cases special attention must nevertheless be paid to discretization if larger time steps are used or if highly permeable soils are considered the use of a variable time step for these modules allowing for example a division of the time step for particular units would make it possible to at least partially solve this issue however further development of the model should focus on the introduction of matrix resolutions for both uz and sz equations to extend the range of applicability of the model 5 2 2 modelled processes although the approach satisfactorily simulates subsurface flows it suffers from various limitations that could restrict its scope of application but may be partially addressed in the future first of all the uz model only considers single porosity media and further development should be carried out to allow for macropore flows which can significantly influence uz functioning and infiltration processes beven 1982 simple approaches such as those used in the mike she model dhi 2017 seem to be adapted to the level of complexity of the module and the level of knowledge of the macropore distribution similarly the gw flow calculation module cannot model the influence of double porosity matrix and cracks or horizontal anisotropy of the hydraulic conductivity in the aquifer for the latter case a minor modification of the calculation presented in part 2 1 could be considered following for example the approach presented in de marsily 1981 anyway the approach remains limited by the 2d assumption of a negligible vertical component of the hydraulic gradient it may thus provide erroneous results when this assumption is not valid for example in case of vertical anisotropy of permeability similarly the uz modelling method is simpler but remain less reliable and accurate than models solving the 1d richards equations however the latter remain associated with a high computation time and potential convergence issues ross approach ross 2003 seems from this point of view to be an interesting alternative to the usual methods of solving the richards equation the modelling of gw infiltration in sewer pipes is quite elementary first of all it assumes a homogeneous infiltration over the pipe length in a cell whereas this infiltration is actually due to punctual defects cracks joints however due to the low level of knowledge of the spatial distribution of these defects this homogeneous infiltration assumption is reasonable furthermore such description would allow the use of distributed value for the parameters controlling gw sewer interactions if the conditions of sewer pipes were known a second limitation of the module is that it does not consider the head in the pipes for the computation further developments should introduce this functionality to represent more realistically interactions between gw and pipes finally the vegetation depiction is relatively simplified it would in particular be relevant to introduce the possibility to vary vegetation characteristics throughout the year and to take into account compensation processes in the root zone 6 conclusion this paper details the development the evaluation and the integration within the urban hydrological model urbs of a set of modules designed to depict the urban underground compartment in shallow groundwater contexts the unsaturated flow computation is based on a coupling between a reservoir model and a green ampt model it allows taking into account the influence of a shallow water table on infiltration and evapotranspiration processes in homogeneous or stratified soils the modelling of the saturated zone is based on a 2d groundwater flow computation allowing 1 the use of irregular meshes adapted to the geometry of urban components and 2 the simulation of interactions between groundwater and various underground structures sewer pipes wells underground constructions basements car parks etc with or without draining systems the evaluation of the modules by comparison with reference models hydrus and feflow shows their ability to efficiently reproduce the processes and interactions of interest the new modules are integrated within the urbs model through an approach allowing dynamic feedbacks between the various components it provides an original framework for the assessment of the impacts of urban development or stormwater management strategies in shallow groundwater contexts its application to a hypothetical yet realistic urban area illustrates the benefits of such integration to extend the applicability of an existing urban hydrogeological model to such contexts the approach presented in this paper could be adapted to complement existing modelling frameworks to provide a more realistic description of the hydrological functioning of urbanized watersheds in shallow groundwater contexts software availability software name urbs development team floriane morena fabrice rodriguez marie laure mosini yinghao li jérémie sage william pophillat availability free download at https nam11 safelinks protection outlook com url http 3a 2f 2fdoi org 2f10 5281 2fzenodo 5115863 amp data 04 7c01 7ck sivakolundu 40elsevier com 7c98de0e20a77241be03c308d95894a063 7c9274ee3f94254109a27f9fb15c10675d 7c0 7c0 7c637638218747206460 7cunknown 7ctwfpbgzsb3d8eyjwijoimc4wljawmdailcjqijoiv2lumziilcjbtii6ik1hawwilcjxvci6mn0 3d 7c1000 amp sdata zknsrg8ugb4tqroroximq1evoilxsygqduxqdi1bd04 3d amp reserved 0 program language c program size 600 ko the model diagnosis was carried out using the following software packages feflow v7 2 hydrus 1d v4 17 author contributions w p j s f r and i b designed the research w p coded the model and performed the simulations j s f r and i b contributed to the definition of the model and the methods w p wrote a first version of the paper that was revised by j s f r and i b declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was carried out under the opur program and supported by the french ministry for the ecological and inclusive transition and the french national institute for agriculture food and environment the study also contributes to the research conducted within othu field observatory in urban hydrology and the onevu nantes urban environment observatory the authors would like to thank marie laure mosini for her useful contribution to the development of the urbs software the authors acknowledge dhi for the sponsored feflow license appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105144 
25759,hydrological modelling can be a relevant tool to support sustainable planning decisions however urban hydrological models usually rely on simplified underground representation that may limit their applicability in shallow groundwater environments this paper introduces a set of modules designed to describe the numerous interactions between subsurface processes and surface hydrology that may occur in such contexts the modules provide a rather straightforward approach to simulate the influence of the groundwater on the surface as well as the influence of several underground structures sewer systems building basements on subsurface storage and the volumes to be managed by the sewerage network the modules are evaluated by comparison with reference models over a set of theoretical test cases and integrated within the urbs hydrological model an application to a realistic hypothetical watershed is carried out to illustrate the benefits and the feasibility of these focussed development in existing urban hydrological models keywords hydrological modelling urban hydrology urban hydrogeology hydrological processes groundwater unsaturated zone list of notations a i area of cell i m2 a m 1 b m 1 parameters of the green ampt model m b k shape parameter of the brooks and corey law in reservoir k ca infiltration capacity ms 1 d k thickness of reservoir k m d ij distance between the centre of gravity of cell i and the centre of the interface between cells i and j m dr k root density in reservoir k ed evaporative demand ms 1 f k k 1 exchange flux between reservoirs k and k 1 ms 1 f rm upper flux for the reservoir model computed by the green ampt model ms 1 f uz sz exchange flux between the lowest reservoir and the saturated zone ms 1 g sew i coefficient characterizing the conductivity of the sewer pipe and the surrounding medium in cell i ms 1 h i hydraulic head in cell i m i hydraulic gradient i cumulative infiltration m i p cumulative infiltration at surface ponding m i sup maximal upper flux ms 1 i surf surface infiltration flux ms 1 k eff effective hydraulic conductivity ms 1 k k k 1 hydraulic conductivity at the interface between reservoirs k and k 1 ms 1 k k hydraulic conductivity in reservoir k ms 1 ks k saturated hydraulic conductivity in reservoir k ms 1 ks dr i saturated hydraulic conductivity of soil below the drainage system in cell i ms 1 l ij length of the interface between cells i and j m l sew i sewer pipe length within cell i m p rainfall volume over a time step m s cf storage in the capillary fringe m se k saturation level of reservoir k se k k 1 saturation level at the interface between reservoirs k and k 1 s k storage in the reservoir k m s surf storage in the surface reservoir m sy specific yield sw m capillary pressure at the infiltration front lower limit m t current time s t p time between the beginning of the time step and the beginning of the surface ponding s t ij transmissivity at the interface between cells i and j m2s 1 tr k root water uptake in reservoir k ms 1 v ij volume exchanged between cells i and j m3 v dr i volume drained by a drainage system within a cell i m3 v sew i volume drained by a sewer pipe within a cell i m3 v well i volume pumped injected by a well within a cell i m3 q max maximum flow rate per surface unit ms 1 x y z coordinates m z dr i drainage system elevation in cell i m z sew i sewer pipe elevation in cell i m δt time step t ψ int capillary pressure at the interface between reservoirs m ψ k capillary pressure in reservoir k m ψ e k capillary pressure at air entry value in reservoir k m ψ fc k capillary pressure at field capacity in reservoir k m ψ wp k capillary pressure at wilting point in reservoir k m θ k water content in reservoir k θs k saturated water content in reservoir k θr k residual water content in reservoir k acronym bc boundary condition cf capillary fringe gw groundwater sz saturated zone uz unsaturated zone 1 introduction about half of the world s population currently lives in urban areas by 2050 these areas will be home to an additional 2 5 billion people bringing together two thirds of the world s population united nations 2014 urban expansion associated with such development implies significant changes in the hydrology of the affected watersheds the sealing of natural surfaces leads to an increase in runoff volumes and to a decrease in stormwater volumes infiltrating into the soil returning to the atmosphere trough evapotranspiration and contributing to groundwater gw recharge fletcher et al 2013 combined with the rapid transfer of runoff volumes across urban surfaces and drainage network it results in increased peak flows and volumes and shorter times of concentration fletcher et al 2013 to mitigate these effects stormwater management increasingly relies on small infiltration devices disseminated throughout urban catchments however the spatial concentration of runoff in such systems is likely to promote deep infiltration rather than evapotranspiration and thus to over recharge gw göbel et al 2004 stormwater infiltration may also be associated with highly heterogeneous gw elevation with localized mounding beneath infiltration devices following rain events bhaskar et al 2018 overall consequences of urbanization on gw levels and base flow depend on a complex water budget the balance involves a variety of sources that not only include rainfall infiltration but also other anthropogenic sources such as network leakage or irrigation and sinks such as drainage or pumping bhaskar et al 2016 besides underground water pathways local gw levels and gw flow systems are largely affected by the various soil modifications e g backfill compaction and underground structures e g sewer pipes underground car parks tunnels equipped or not with drainage system attard et al 2016a these numerous disturbances make urbanized areas highly heterogeneous environments concentrating a wide variety of natural and anthropogenic processes interacting with each other at various spatio temporal scales salvadore et al 2015 these interactions are often challenging to quantify and greatly increase the complexity of the hydrological functioning of these systems this is notably the case for interactions linking surface and underground processes for instance mounding forming beneath infiltration devices may increase transpiration in surrounding areas bonneau et al 2018 or affect in turn the infiltration capacity within the devices bouwer 2002 the rise in the water table resulting from the infiltration may also increase gw volumes drained by underground structures e g gw seepage into sewer pipes kidmose et al 2015 which affects both water volumes to be managed by sewerage systems and subsurface water storage the influence of such interactions on the hydrological functioning of the watershed depends strongly on the characteristics of the urban hydrogeological and climatic contexts in particular shallow gw contexts less than a few meters deep have a high potential for significant interactions between surface and subsurface understanding and managing the water cycle in urbanized areas in shallow gw contexts therefore requires hydrological models capable of taking these various processes and interactions into account hamel and fletcher 2014 such models should be suited to the morphology of urban environments and to their specific components they should simulate natural and urban processes involved e g runoff generation over natural and impervious surfaces evapotranspiration unsaturated flows gw interactions with underground structures they should also take into account the various interactions between these processes salvadore et al 2015 and the complex water paths in these environments e g centralised decentralised runoff management fate of drained groundwater they should allow long term continuous simulations with rather high temporal resolution a few minutes or less salvadore et al 2015 finally the computation times should not preclude the use of optimization procedures sensitivity studies or multi model scenarios to take into account the strong uncertainties regarding the underground composition only a few hydrological models designed for urban environments integrate a representation of both the surface and underground compartments in their source code e g swmm rossman 2016 wep jia et al 2001 urbs rodriguez et al 2008 these models usually incorporate a relatively fine representation of the urban environment and hydrological processes on the surface and a simplified description of the underground compartment in usual modelling contexts such descriptions provide satisfactory estimates of the influence of the underground compartment on the surface hydrology and of the impacts on subsurface storages and base flows however they neglect various processes and interactions that may significantly affect the water cycle in presence of a shallow water table and therefore limits the applicability of these models in such contexts for instance oversimplified representations of the unsaturated zone regarding soil heterogeneity water content profile and vertical fluxes e g through single reservoir approaches in urbs and swmm and non consideration of capillary rise from the water table e g in wep swmm limit the ability to represent the gw influence on evapotranspiration and surface infiltration and the gw recharge furthermore conceptual gw flow calculations e g in swmm or urbs are not appropriate to represent local fluctuations e g mounding beneath infiltration structures and their influence on surface besides the use of square mesh based gw flow computations wep doesn t suit well to urban object geometry finally interactions between gw and underground structures are usually neglected except for gw seepage into sewer pipes in urbs and pumping in wep which hinders the consideration of their influence on subsurface water storage and on water volumes to be managed by sewer systems the use of these models in shallow gw contexts therefore requires adaptations of the description of the subsurface compartment to better take into account the specificities of such environments a possible approach to overcome the limitations of these urban hydrology models is to couple them with gw models e g feflow modflow or integrated hydrogeological models developed for natural environments e g mike she gsflow if such couplings are commonly described in the literature many of them have not been designed to improve the representation of surface subsurface interactions and rather focus on gw flows as a consequence only a few of them actually consist in two ways couplings where feedbacks between surface and subsurface compartments are taken into account two way couplings e g swmm modflow zhang and chui 2020 mike urban mike she kidmose et al 2015 locatelli et al 2017 have the potential to simulate the urban water cycle in shallow gw contexts their use or implementation is however not necessarily straightforward two way couplings may first be associated with a significant increase of computation time limiting their applicability for optimization or sensitivity analysis procedures kidmose et al 2015 they may also be difficult to set up as they involve dynamic exchanges between numerous calculation variables in addition the models used to describe the subsurface compartment may not provide pragmatic alternatives to overcome the limitations regarding the description of the unsaturated zone in urban hydrology models the resolution of richards equation available in some models as the highest accuracy option can become cumbersome in many applications zha et al 2019 while simpler approaches such as the kinematic wave solutions included in modflow or gsflow are also provided they do not take into account diffusive fluxes or vertical soil heterogeneity besides except mike she mike urban existing couplings do not consider interactions between gw and underground structures although their representation is possible with most of the models used to depict the subsurface compartment their integration in couplings would require additional developments finally if several groundwater models allow the use of irregular meshes e g modflow 6 modflow usg feflow many others are based on rectangular meshes e g other versions of modflow gsflow mike she less adapted to the geometry of urban objects and the use of local refinements the features of existing software or couplings are therefore not fully in line with the objectives commonly associated with urban hydrology models in shallow gw contexts instead the introduction of selective developments in these models may also be a relevant approach to extend their applicability while taking advantage of their existing functionalities and retaining their relative simplicity in this study a set of modules is introduced to describe the role of the urban underground compartment in shallow gw environments and integrated within the urbs model the modules rely on a physically based but relatively simple depiction of this compartment and its interactions with the surface given the simplifications involved particular attention is paid to the evaluation of the modules ability to reproduce the processes and interactions of interest their integration into the urbs model aims to exploit the capabilities of an existing tool to develop a flexible modelling framework for the evaluation of the impacts of stormwater management strategies or urban developments on the water cycle in a variety of contexts including shallow gw environments this tool i is suited to the complexity of urban landscapes and the geometry of their components ii is able to simulate the evolution of the various terms of the water balance the surface and subsurface storage and the complex water pathways in these environments decentralised centralized management of runoff subsurface fate of infiltrated water fate of drained groundwater or groundwater resurgence etc and iii can be used to perform continuous simulations over long periods with computation times enabling optimization or sensitivity analysis procedures to be implemented the first section of the paper details the theoretical basis of the subsurface modelling approach it is based on 1 a 2d gw flow computation module adapted to irregular meshes and allowing interactions with several underground structures to be taken into account sewer systems wells underground constructions with or without draining systems and 2 a reservoir model for the computation of vertical uz flows capable of taking into account layered soils vegetation transpiration and upward capillary rise from the water table section 2 of the paper details the diagnosis conducted to better understand the functioning and limitations of these modules regarding the processes and interactions of interest it is based on comparisons with reference models hydrus 1d šimůnek et al 2005 and feflow diersch 2014 over a set of hypothetical test cases section 3 introduces the integration of these modules into the urbs model it also presents an application to a hypothetical yet realistic watershed designed to illustrate the model s applicability and potential benefits of considering such processes and interactions in shallow gw environments with stormwater infiltration devices finally section 4 discusses the benefits and limits of the approach 2 theoretical basis of the new modules this section details the theoretical basis of the modules developed to represent the urban underground compartment the first sub section describes the computation of gw flows and interactions with underground structures the second sub section focuses on the uz flows computation finally the last sub section details the computation procedure 2 1 saturated zone 2 1 1 lateral groundwater flows the modelling of gw flow is based on a 2d application of darcy s law at the interfaces between computation cells eq 1 fig 1 the computation is done by sequentially estimating the flow at each interface on the basis of hydraulic heads at the previous time step 1 v i j h n t i j l i j δ t where v ij m3 is the volume exchanged between cells i and j during a time step δt h the hydraulic gradient n the interface normal vector t ij m2s 1 the transmissivity at the interface and l ij m is the interface length t ij is computed at each time step by integrating the hydraulic conductivity along the interface according to z between the bed rock and the water level linearly interpolated between cells i and j in the case of layered soils the wet thickness of each soil layers is considered for the calculation of transmissivity if cells i and j have different geological properties t ij is taken equal to the harmonic mean of the transmissivity computed for each cell a simple estimation of the hydraulic gradient at the interface as the head difference between cells i and j divided by the distance between the centroids of the cells would produce inaccurate results for non rectangular cells the use of irregular meshes can however be relevant to better fit the geometry of urban objects to ensure the validity of the computation on such meshes the approach relies on the use of a third cell a to estimate the components of the hydraulic gradient along the interface de marsily 1981 fig 1 the third cell is selected by limiting its distance from the interface and excluding cells whose centre of gravity would be close to an alignment with those of cells i and j based on these three cells a taylor series expansion at the first order for any point m on the interface gives 2 h i h m x i x m δ h δ x m y i y m δ h δ y m h j h m x j x m δ h δ x m y j y m δ h δ y m h a h m x a x m δ h δ x m y a y m δ h δ y m where h i h j and h m are the hydraulic head within cell i j and on the interface respectively and x i y i x j y j x m and y m are the coordinates of the centre of gravity of cell i j and of the point m respectively the resolution of 2 provides the components of the hydraulic gradient at the interface 3 δ h δ x m h j h i y a y i h a h i y j y i x j x i y a y i x a x i y j y i δ h δ y m h j h i x a x i h a h i x j x i x j x i y a y i x a x i y j y i h in eq 1 is then be obtained by projecting these components onto the normal at the interface 4 h δ h δ x m n x δ h δ y m n y 2 1 2 interactions with underground structures the modelling of interactions between gw and underground structures is inspired by urban hydrogeology methods it relies on the use of either specific hydrodynamic parameters e g for underground structure or boundary conditions bcs e g for drainage systems dirichlet bcs flow constraint for wells neuman bcs or for sewer systems cauchy bcs attard et al 2016b the modelling of underground constructions buildings car parks etc is carried out by assigning to cells included in the structure perimeter a hydraulic conductivity of 10 9 ms 1 over the structure depth if the structure is equipped with a drainage system the volumes v dr i m3 drained at each time step are computed according to eq 5 it aims to mimic a dirichlet bc while limiting the flow by the surrounding medium and by a maximum permissible flow rate per unit area pump limit 5 v d r i m i n h i t δ t z d r i s y k s d r i δ t q m a x δ t 0 a i with h i m the gw level in cell i z dr i m the drainage system level sy the specific yield i e the difference between the total porosity and the specific retention considered here equal to the field capacity ks dr i ms 1 the saturated hydraulic conductivity of soil below the basement a i m2 the area of cell i and q max ms 1 the maximum flow rate per unit surface the volume v sew i m3 drained by a sewer networks at each time step within a cell i is determined by eq 6 it aims to mimic a cauchy bc 6 v s e w i m i n g s e w i h i t δ t z s e w i 0 l s e w i δ t with g sew i ms 1 a coefficient characterizing the conductivity of the pipe and the surrounding medium z sew i m the pipe elevation and l sew i m the pipe length within cell i pumping and injection wells are modelled using a sink source term v well i m3 corresponding to a given pumping injection rate multiplied by the time step 2 1 3 groundwater level update at the end of each time step the gw level is updated according to eq 7 7 h i t h i t δ t j 1 n v j i t v d r i t v s e w i t v w e l l i t f u z s z t δ t a i s y i l with v ji m3 the volume exchanged with the adjacent cell j by lateral gw flow f uz sz ms 1 the exchange between uz and sz section 2 2 and sy i l the specific yield of the geological layers where the water table movement occurs each groundwater level variation is calculated using the specific yield of the reservoir in which the variation occurs if the water table changes reservoir the calculation takes into account successively the specific yield of each reservoir crossed 2 2 unsaturated zone the uz is conceptualized as an overlay of reservoirs fig 2 reservoirs may have identical hydrodynamic properties and thus constitute discretization elements or conversely they may have different properties in order to represent layered soils the number and thickness of reservoirs are not fixed and are selected for each case based on material properties and modelling objectives the brooks and corey model brooks and corey 1964 is used to represent the relation between pressure water content and hydraulic conductivity eq 8 8 s e k θ k θ r k θ r k θ s k ψ k ψ e k i f ψ k ψ e k 1 i f ψ k ψ e k k k s e k k s k s e k 2 b k 3 with se k the saturation rate ψ k m the capillary pressure ψ e k m the capillary pressure at air entry value b k a shape parameters and ks k ms 1 the saturated hydraulic conductivity of reservoir k θ k θs k and θr k are respectively the water content the saturated water content and the residual water content of reservoir k following wep s approach jia et al 2001 vertical uz fluxes computation is based on a coupling between a reservoir model and a green ampt model the following sections detail the calculation of 1 the surface infiltration flux 2 the vertical fluxes outside the infiltration front and 3 the conceptualization of the capillary fringe 2 2 1 surface infiltration and green ampt model the surface infiltration flux i surf ms 1 depends on the soil s infiltration capacity first of all infiltration is assumed to be zero when the water table is at land surface apart from this case i surf computation method depends on the ratio between the surface flux to be infiltrated i sup p t s surf t δt δt ms 1 and the infiltration capacity ca min k s 0 d 0 θs 0 θ 0 t δt δt ms 1 with p ms 1 the rainfall volume during the time step s surf m the surface storage and k s 0 ms 1 d 0 m θs 0 and θ 0 the saturated hydraulic conductivity the thickness the water content at saturation and the current water content of the upper reservoir respectively if i sup ca the incident volumes can be fully infiltrated and i surf i sup if i sup ca i surf is computed using a green ampt model adapted to layered soils jia and tamai 1997 in order to more accurately model the infiltration dynamics while taking into account layered soils and heterogeneous initial water content profiles the green ampt model calculation only occurs when ca is exceeded which makes it possible to improve the estimation of infiltrated flow without significantly increasing the calculation time this mode of activation using ca allows the use of the green ampt model when a less permeable underlying reservoir limits the infiltration capacity indeed if the infiltration rate is higher than the hydraulic conductivity of the less permeable reservoir the water content within the upper reservoirs gradually increases calculated by the reservoir model as described in section 2 2 2 until i sup ca when the green ampt model is activated the cumulative infiltration since the beginning of the activation i m is computed as follows if the surface remains unsaturated 9 i t i t δ t i s u p t δ t the surface ponding occurs if the surface volume to be infiltrated is greater than the volume estimated by the green ampt model under saturated conditions 10 i t a m 1 i s u p k s m 1 b m 1 i s u p δ t i t δ t if surface ponding occurs at the present time step 11 i p i t δ t i s u p t p t p i p i t δ t i s u p i t i p k s m t t p a m 1 l n a m 1 b m 1 i t a m 1 b m 1 i p if surface ponding occurred during a previous time step and continues since 12 i t i t δ t m i n i s u p t δ t k s m δ t a m 1 l n a m 1 b m 1 i t a m 1 b m 1 i t δ t with m the index of the reservoir where the infiltration front is located t p s the duration between the beginning of the time step and the beginning of the surface ponding and i p m the cumulative infiltration from the activation of green ampt model to the surface ponding other variables are described below 13 a m 1 k 0 m 1 d k k 0 m 1 d k k s m k s k s w m s s u r f δ θ m b m 1 k 0 m 1 d k k s m k s k δ θ m k 0 m 1 d k δ θ k with d k m the thickness of reservoir k sw m the capillary pressure at the infiltration front lower limit computed according to eq 14 using brooks and corey law δθ k the difference between the saturated water content and the initial water content i e at the time of activation of green ampt model in reservoir k 14 s w m 0 ψ m k m s e ψ k s m d ψ for more details regarding the derivation of eqs 9 14 the reader may refer to jia and tamai 1997 if the saturation front reaches a reservoir with a higher hydraulic conductivity the inflow is not sufficient to saturate the new material and green ampt model s applicability assumptions are no longer met the green ampt model is then applied only within the upper reservoirs according to eq 15 the downflow at the reservoir bottom f rm ms 1 computed with the green ampt model is used as the upper limit condition for the reservoir model within the underlying reservoir in any other case f rm 0 ms 1 15 i s u r f f r m k e f f 1 s w m s s u r f k 0 m d k with k eff ms 1 the effective hydraulic conductivity of the medium above the saturation front obtained by a harmonic mean of the saturated hydraulic conductivity of reservoir weighted by their thickness if the saturation front reaches the substratum or the gw table infiltration stops and the gw level is fixed at soil surface this amounts to considering that the saturated column belongs to the saturated zone and contributes to increase the hydraulic head 2 2 2 unsaturated fluxes computation using the reservoir model vertical fluxes between reservoirs and vegetation transpiration are computed using a reservoir model when the green ampt model is not activated the fluxes are sequentially computed from the capillary fringe upper limit reservoir n to the ground surface reservoir 0 on the basis of capillary pressures at the previous time step fig 2 when the green ampt model is activated exchanges between reservoirs are still computed up to the reservoir located under the one including the infiltration front in order to take into account fluxes under the front in particular exchanges with the sz when the infiltration front computed by the green ampt model enters a reservoir the latter is excluded from the reservoir model computation the water content within the reservoir is then used as initial water content in the green ampt model for computing the progression of the infiltration front conversely when the green ampt model is disabled the water content in the various reservoirs is updated by adding the additional volume of the infiltration front contained therein this procedure ensures the water balance in uz the balance equation for a given reservoir k between 0 and n is given by eq 16 16 s k t s k t δ t f k 1 k t f k k 1 t t r k t δ t with s k m the storage in reservoir k f k k 1 ms 1 the exchange flux gravity and diffusion between reservoir k and k 1 and tr k ms 1 the root water uptake flux in reservoir k the computation of the various flux is subsequently described fluxes are limited by the storage of the incoming and outgoing reservoirs so that the water content remains between its residual and saturation level the root water uptake flux tr k within reservoir k is computed using the feddes model feddes et al 1978 17 t r k t min ψ k t δ t ψ w p k ψ f c k ψ w p k 1 d r k e d t with ψ k m the capillary pressure in reservoir k computed according to brooks and corey s law ψ wp k m and ψ fc k m the capillary pressure at wilting point i e 150 m and at field capacity i e 3 3 m respectively dr k the root density in the reservoir k k d r k 1 and ed ms 1 the evaporative demand exchange flux between two reservoirs f k k 1 is computed according to eq 18 it takes into account gravity flux first term in the bracket and diffusion flux second term in the bracket positive exchange fluxes are oriented downwards 18 f k k 1 t k k k 1 t 1 ψ k 1 t δ t ψ k t δ t d k d k 1 2 with k k k 1 ms 1 the average hydraulic conductivity at interface between reservoir k and k 1 and d k the thickness of reservoir k the calculation method of k k k 1 depends on whether the two reservoirs have identical or different hydrodynamic characteristics if the reservoirs have identical hydrodynamic characteristics k k k 1 is computed according to brooks and corey s law eq 8 considering an average saturation se k k 1 the latter is computed according to eq 19 it assumes that when the saturation rate of the upper reservoir is higher i e in the case of marked downward flux the saturation level of the upper reservoir governs the advance of the infiltration front 19 s e k k 1 t m a x s e k t s e k t s e k 1 t 2 if the reservoirs have different hydrodynamic characteristics k k k 1 is equal to the average hydraulic conductivity of the two reservoirs eq 20 computed using the brooks and corey law eq 8 20 k k k 1 t k k s k t k k 1 s k 1 t 2 the limits of the chosen parameterizations as well as those associated with the use of other types of averages are discussed in supplementary material section s1 and s3 for the upper reservoir f k k 1 is the surface infiltration i surf ms 1 section 2 2 1 if green ampt model is deactivated or f rm otherwise for the lowest reservoir n f k k 1 is the exchange flux with the saturated zone f uz sz ms 1 the latter is computed between the middle of the lower reservoir and the top of the capillary fringe cf according to eq 21 which is a direct application of eq 18 within these conditions the capillary pressure value at the top of the cf ψ cf is by default equal to the air entry pressure the special case of a material transition is detailed in the following section 2 2 3 note that the thickness of this reservoir d n varies with the depth of the gw table see section 2 3 21 f u z s z t k n s e n t δ t 1 2 1 ψ c f ψ n t δ t d n 2 2 2 3 capillary fringe conceptualization as the brooks and corey model cannot model variably saturated soil within the capillary fringe se k 1 if ψ k ψ e k the model assumes a conceptualization of the cf the cf is assumed to behave like a saturated zone uz sz exchanges through the cf are assumed to be instantaneous the cf belongs to uz and the corresponding water storage s cf m is then assigned to the uz storage by default the cf thickness and the capillary pressure at its upper boundary ψ cf are equal to the air entry pressure of the material ψ e k for stratified soils the model conceptualizes the influence of material transitions on the capillary fringe fig 3 consider two superimposed geological layers k and k 1 with k on top one made of coarse material and the other of fine material with the top of the water table located in the lower layer k 1 consider ψ int the capillary pressure at the interface at hydrostatic equilibrium condition i e equal to the distance between the interface and the top of the water table fig 3 to facilitate interpretation the various pressures are expressed in absolute values in the remainder of this paragraph if the upper material is coarser than the lower one then ψ e k ψ e k 1 and ψ e k ψ int ψ e k 1 and θ k ψ int θs k the cf upper boundary is thus fixed at the interface its storage is s cf ψ int θs k and the pressure value used for f uz sz computation is ψ cf ψ int fig 3a if the lower material is coarser than the upper then ψ e k ψ e k 1 and ψ e k 1 ψ int ψ e k and the cf reaches the upper reservoir while the lower reservoir remains unsaturated over a ψ int ψ e k 1 thickness fig 3b ψ cf ψ e k and the cf storage is thus computed as follows s cf ψ e k ψ int θs k ψ int ψ e k 1 θ m ψ e k 1 θs k 1 considering an estimate of the mean water content within this thickness θ m θ ψ m with ψ m ψ int ψ e k 1 2 fig 3b computed according to brooks and corey law eq 8 2 3 computation procedure from the beginning of the time step the computation follows the subsequent steps 1 computation of the various fluxes and storage related to the uz for each cell section 2 2 2 computation of the interactions between gw and underground structures for each cell section 2 1 3 computation of gw flows for each interface between cells section 2 1 4 update of the gw level according to eq 7 for each cell considering the specific yield of each reservoir intersected by the water table movement 5 update of the cf level for each cell section 2 2 3 6 update of the thickness of the lower reservoir for each cell accordingly to the new cf level in order to avoid numerical oscillations the thickness cannot fall below a threshold d min m typically about a few centimetres informed by the user and depending on the material properties below this threshold the reservoir is merged with the upper reservoir 7 update of the water content in the uz for each cell indeed as gw movements and consequently cf movements are determined on the basis of fixed specific yields maintaining the balance requires a reallocation of the uz volume located in the variation thickness of gw additionally in the case of layered soils the cf storage may vary within a time step as presented in section 2 2 3 the sum of these two differences positive or negative is added to the overlying reservoirs and its water content is updated if it leads to exceeding the water content at saturation the excess is added to the overlying reservoir and so on until the surface if all the reservoirs are saturated conversely if it leads to fall below the residual water content the corresponding deficit is spread over the overlying reservoir if it cannot be distributed over the soil column the water table level is lowered accordingly 3 evaluation of the subsurface modelling approach the modules are evaluated over a set of hypothetical test cases the evaluation is based on a comparison between the results provided by the modules and those obtained by reference models hydrus 1d šimůnek et al 2005 for the 1d fluxes in unsaturated zone and feflow diersch 2014 for the gw flows and the coupling between uz and sz modules results are compared over a set of variables distributed either in time e g infiltration gw level fluctuations at one point or in space e g gw levels at steady state comparison variables are specified in the corresponding test case subsections the error related to spatial distributions of variables is quantified focusing on the mean absolute error e xy mean eq 22 and the maximum absolute error e xy max eq 23 due to the difference in spatial discretization the results are interpolated for comparison on a regular g grid composed of 250 000 points spaced 1 m apart 22 e x y m e a n x y g x x y x r e f x y 250000 23 e x y m a x max x y g x x y x r e f x y with x and x ref the variable simulated by the modules presented in this study and the reference model respectively the error related to temporal distributions of variables is quantified focusing on the mean absolute error e t mean eq 24 and the determination criteria r 2 eq 25 24 e t m e a n t 0 n x t x r e f t n 25 r ² t 0 n x t x x t x r e f ² t 0 n x t x ² t 0 n x r e f t x r e f ² the test cases are divided into three categories depending on whether they focus on the sz subsection 3 1 the uz subsection 3 2 or the coupled sz uz system subsection 3 3 table 1 lists the characteristics of the various soils used chosen to cover a wide range of soils encountered in the literature morris and johnson 1967 table 2 summarizes the main characteristics of the various test cases 3 1 evaluation of the saturated zone modelling approach three test cases are dedicated to the evaluation of the sz modelling approach presented in section 2 1 for this evaluation the sz module is decoupled from the other modules gw flows are computed with a fixed time step of 6 min time step usually used in the urbs model and corresponding to rainfall data provided by the french national meteorological service the various meshes are made up of irregular elements with and refinement in the vicinity of sharp variations in flow conditions all the meshes used are shown in section s2 of the supplementary material the three test cases are based on a hypothetical 500 m 500 m aquifer with a thickness of 10 m the slope of the surface and bedrock is 0 5 except for test case sz1 the medium is homogeneous and made of coarse sand the gw level is set at a 2 m depth at the upstream and downstream boundaries so that the surface of the gw table is parallel to the model s top and bottom at natural steady state for a homogeneous medium 3 1 1 test case sz1 ability to simulate groundwater flows in a heterogeneous aquifer test case sz1 aims to evaluate the model s ability to reproduce steady state gw flow in the case of abrupt lateral variations in the surrounding environment such as changes in geological material or local anthropogenic modifications of the environment the aquifer is therefore divided into three zones of varying hydraulic conductivity and specific yield as described in fig 4 a the assessment is based on a comparison with results provided by the feflow model solving the darcy s law in 3d the domain is discretized horizontally into triangular elements with a refinement near to material transitions and vertically into 1 m thick layers the comparison focuses on the steady state distributed gw levels h xy considering the mean and maximum value of the absolute error respectively e xy mean eq 22 and e xy max eq 23 fig 7a shows the comparison between the steady state groundwater levels simulated by feflow and the sz module over the whole domain the maximum error is about 12 cm and the mean error is about 2 5 cm table 3 which is reasonable considering the uncertainties related to the composition of the underground compartment or to measurements the model slightly underestimates the water level in the most permeable part lower part in fig 5 a the maximal error is located at the transition between materials near the upstream boundary condition and gradually decreases downstream the mesh refinement here maintains a satisfactory level of error without excessively increasing the number of elements as shown in section s3 1 of the supplementary material the estimation tends towards the reference result when the mesh is more refined 3 1 2 test case sz2 ability to simulate groundwater fluctuations around sinks sources test case sz2 is designed to assess the module s ability to simulate sharp transitory disturbances of gw level generated by a source sink term such as injection pumping well or concentrated water inflows from infiltration devices a well is therefore added in the domain centre its operation is described in fig 4d the assessment is based on a comparison with results provided by the feflow model solving the darcy s law in 3d the domain is discretized horizontally into triangular elements with a refinement near to the well and vertically into 1 m thick layers the well is modelled in the feflow model using a multilayer well bcs the evaluation focuses on the spatial distribution of gw levels h xy after at t 50 d end of the well s operation considering the mean absolute error e xy mean eq 22 and the maximal absolute error e xy max eq 23 the temporal evolution of gw levels at the well location h t center and 50 m upstream h t 50m up considering the mean absolute error e t mean eq 24 and the determination criteria r 2 eq 25 results related to this test case fig 5b and c and table 4 show that the module satisfactorily reproduces the influence of a sink source term on gw levels the maximal error 13 cm is located in the vicinity of the well it is directly related to the homogenization of the injected pumped volumes over the cell surface here about 400 m2 greater accuracy would therefore be achieved by reducing the size of the well cell see section s3 1 of the supplementary material however the low average error and the good matching of head isolines show that the error is quickly smoothed on adjacent cells these various observations are confirmed by the comparison of gw level evolution at the domain centre and 50 m upstream fig 5c 3 1 3 test case sz3 ability to simulate interactions between groundwater and underground structures test case sz3 aims to evaluate the module s ability to estimate interactions between gw and underground structures through three sub cases focusing on a sewer pipe sz3a an impervious building basement sz3b and a building basement equipped with a draining system sz3c in test case sz3a a 300 m sewer pipe g sew 10 5 ms 1 is added in the central part of the domain perpendicular to the direction of flow fig 4b and at a depth of 4 m in test case sz3b and sz3c a building extending 100 m 20 m and 6 m deep two underground floors is added in the central part of the domain fig 4c in case sz3c the building is equipped with a draining system to lower the gw level to a depth of 6 m the assessment is based on a comparison with results provided by the feflow model solving the darcy s law in 3d the domain is discretized horizontally into triangular elements with a refinement near to the underground structures and vertically into 1 m thick layers the sewer pipe is modelled by assigning a cauchy boundary condition over a 0 5 m 300 m horizontal strip the building s basement is modelled by assigning a hydraulic conductivity of 10 9 ms 1 to cells within the building s perimeter the building s drainage system is modelled by applying a dirichlet boundary condition on the horizontal elements located at the basement bottom the evaluation focuses on the spatial distribution of steady state gw levels h xy considering the mean absolute error e xy mean eq 22 and the maximal absolute error e xy max eq 23 the temporal evolution of gw volumes drained by the sewer pipe and the draining system v t considering the mean absolute error e t mean eq 24 and the determination criteria r 2 eq 25 results related to test case sz3a to c are shown in table 5 and fig 6 the influence of the sewer pipe test case sz3a on steady state gw levels is satisfactorily reproduced despite a slight overestimation in the vicinity of the structure table 5 fig 6a as for the well it is linked to the distribution of infiltrated flows over a large area the module also well estimates the dynamics of gw drainage through the pipe table 5 fig 6b the influence of the impervious basements test case sz3b is satisfactorily estimated however results are significantly less good for the basement equipped with a draining system test case sz3c table 5 fig 6c if the flow shape is overall well reproduced the error is significantly greater than in the previous cases up to 57 cm in the vicinity of the building and fig 6c shows slight deformations of the head isolines in the vicinity of the basement e g at x 300 y 150 additionally the model strongly underestimates the gw drainage by the draining system at the beginning of the simulation although the steady state drainage flows are well estimated table 5 fig 7 these two observations reflect the insufficient refinement of the discretization near the building regarding the strong hydraulic gradients it generates see section s3 1 of the supplementary material 3 2 evaluation of the unsaturated zone modelling approach three test cases are dedicated to the evaluation of the uz modelling approach presented in section 2 2 for this evaluation the uz module is decoupled from the other modules the various processes are computed with a constant time step of 6 min soil columns are discretized into 0 5 m reservoirs in order to maintain a constant level of the water table level at the column bottom imposed pressure head equal to 0 m the computation of i surf and f uz sz when the ga infiltration front reaches the water table is done according to eq 15 considering a zero suction the test cases focus on single soil columns on which various rainfall and root water uptake conditions are applied to generate usual vertical water movements within common soils or infiltration devices columns are delimited in their lower boundary by a constant gw depth and are initially at a moisture content profile equilibrium state i e dψ dz 3 2 1 test case uz1 ability to simulate upward capillary rise from the water table the uz1 test case is designed to evaluate the model s ability to simulate the upward water movement from the gw to supply the upper soil horizons during periods of low precipitation and high evaporation demand it is based on a 2 m column of silt with a homogeneous root distribution over the first 50 cm fig 8 a surface forcing consists of two 30 days periods of high evaporative demand without precipitation fig 8c the assessment is based on a comparison with results provided by the hydrus model solving the 1d richards equations with the brooks and corey s model the column is discretized into 0 5 cm thick elements an atmospheric bc with a maximal pressure head of 1 m is assigned at the surface an imposed pressure at h 0 m is assigned at the column bottom the root water extraction is modelled using the feddes model without compensation šimůnek and hopmans 2009 the evaluation focuses on the temporal evolution of the root water uptake tr the exchanges between uz and sz f uz sz the mean water content within the root zone θ rz and within the whole column θ uz considering the mean absolute error e t moy eq 24 and the determination criteria r 2 eq 25 results related to this test case are shown in table 6 and fig 9 it shows that the reservoir model slightly overestimates the root water uptake tr this higher extraction of soil water directly results in lower average water content throughout the soil column θ r z and θ u z and as a consequence in a higher capillary rise from the gw f uz sz the tr overestimation is reinforced by the hydrus configuration feddes without compensation which implies that the model does not seek to compensate for a decrease in root water uptake in stressed areas of the root profile by an increase in less stressed areas conversely in the reservoir model the root zone is entirely included in the first reservoir 50 cm this implies an average value of the water content over the whole root zone a reduction in the reservoir thickness then leads to a significant improvement in results see section s3 2 of the supplementary material 3 2 2 test case uz2 ability to simulate the evolution of storages and fluxes within homogeneous and layered unsaturated soils under various rainfall intensities test case uz2 aims to evaluate the model s accuracy in simulating the evolution of vertical fluxes and storage in homogeneous and layered soils subjected to periods of rainfall of various intensities and to redistribution periods with root extraction the objective is to consider a set of soil and surface input combinations representative of common permeable areas and stormwater infiltration structures three sub cases differing by soil characteristics are therefore considered test case uz2a is based on a 3 m homogeneous column of silt test cases uz2b and uz2c cases rely on 3 m columns of layered soils designed to assess the module s ability to represent the influence of a different underlying soil layer they are composed of a 0 5 m surface layer of silt on top of a 2 5 m layer of a material either more permeable uz2b sand or less permeable uz2c clay fig 8b for the three sub cases roots are homogeneously distributed over the first 25 cm the rainfall inputs fig 8d identical within the three sub cases are set to meet various possible cases of exceeding or not exceeding the infiltration capacity of the materials they are interspersed with periods of moderate evaporative demand favouring the redistribution of water fig 8d the assessment is based on a comparison with results provided by the hydrus model configured identically to case uz1 except for the column extension and soil characteristics the evaluation focuses on the temporal evolution of the surface infiltration i surf the root water uptake tr the exchanges between uz and sz f uz sz the surface storage s surf and the mean water content within the root zone θ rz and within the whole column θ uz considering the mean absolute error e t mean eq 24 and the determination criteria r 2 eq 25 results related to the test case uz2a show that the model properly estimates rainfall infiltration i surf in a homogeneous soil table 6 fig 10 using both the reservoir model first rainfall pulse with i sup ks and the green ampt model second and third pulses with i sup ks the latter provides better results when the inflow more distinctly exceeds the hydraulic saturation conductivity of the surface soil the third pulse the good matching of mean water contents θ r z and θ u z variations during and between rainfall inputs validates the coupling between the green ampt model and the reservoir model the root water extraction tr is overall satisfactorily reproduced by hypothesis tr is not computed in the reservoirs where the green ampt computation occurs this leads to the error visible at t 80 h exchanges with the sz f uz sz are overall less accurately reproduced for t 120 h they are computed by the reservoir model the homogenization of incoming volumes over the lower reservoir thus leads to a smoother and earlier recharge 90 h t 120 h fig 10 for t 120 h the infiltration front computed by the green ampt model enters the lowest reservoir the reservoir model computation within this reservoir therefore stops and f uz sz suddenly drops to zero until the infiltration front reaches the gw table t 120 h fig 13 at t 135 h the green ampt model computation stops f uz sz therefore drops during this time step before being calculated with the reservoir model at the next time step a decrease in the reservoir thickness near the water table leads to a significant improvement in the estimation of these exchanges see section s3 2 of the supplementary material the model satisfactorily reproduces the impact of a transition to a more permeable environment on the various uz variables case uz2b table 6 fig 11 a and b the green ampt model slightly underestimates i surf when the infiltration front reaches the boundary between media 65 h t 80 h and 105 h t 130 h fig 11a it results in a slight overestimation of the surface ponding s surf table 6 fig 11b not computing tr within reservoirs where the green ampt computation occurs results here in a significant underestimation of this flux t 80 h and t 120 h fig 11a the gw recharge t 90 h is here estimated by the reservoir model which lead to similar conclusions as for the homogeneous soil earlier and smoother recharge the model also properly reproduces the influence of an underlying low permeable layer on the various fluxes and storages case uz2b table 6 fig 11 c and d i surf and s surf are accurately reproduced however the green ampt model slightly underestimates the time for the front to reach the water table t 130 h fig 11c in contrast to the previous cases a refinement of discretization does not correct this error see section 3 2 of the supplementary material 3 3 evaluation of the coupling three test cases are dedicated to the evaluation of the coupling between uz and sz they are simulated using the whole subsurface modelling approach described in section 2 using a constant time step of 6 min 3 3 1 test case co1 ability to simulate interactions between water table variation and unsaturated zone test case co1 is intended to assess interactions between gw and uz during gw fluctuations it focuses on a single 4 m column of soil without lateral exchanges subject to rainfall inputs and a sz sink term e g pumping well table 7 and fig 12 the column is initially at hydrostatic equilibrium i e dh dz with a bottom boundary condition fixed at h 1 m i e the gw table is at a 3 m depth from the surface the column is discretized into 0 5 m reservoirs the minimum thickness of the lower reservoir is set at d min 0 1 m test case co1 is divided into three sub cases test case co1a focuses on a homogeneous column of silt test cases co1b and co1c focuses on layered columns to assess the model s ability to reproduce the influence of material change including the influence of the cf conceptualization presented in section 2 2 3 on groundwater level variations two configurations are considered a more permeable soil above co1b 2 5 m of silt over 1 5 m of sand or a more permeable soil below co1c 2 5 m of sand over 1 5 m of silt table 1 and fig 12a the assessment is based on a comparison with results provided by the hydrus model solving the 1d richards equations with the brooks and corey s model the column is discretized into 0 5 cm thick elements an atmospheric bc with a maximal pressure head of 1 m is assigned at the surface a variable flow bc is assigned at the column bottom the evaluation focuses on the temporal evolution of the water table elevation h t and the storage in the unsaturated zone s uz considering the mean absolute error e t mean eq 24 and the determination criteria r 2 eq 25 results related to test cases co1a to c table 7 and fig 13 show the model s ability to estimate the water table h fluctuations and their influence on the unsaturated zone storage s uz the system properly returns to its initial condition at the end of the simulation the sum of the inflows outflows being zero results also validate the conceptualization of the capillary fringe in test cases co1b and co1c the water table intersects the interface between mediums both during rise and fall and the two specific cases presented in section 2 2 3 are encountered in both cases the impact on both h and s uz is satisfactorily reproduced however the coarseness of the discretization leads to sharp variation in the water content when the sz upper limit moves to a different reservoir during following time steps this results in adjustments of the water content by uz sz exchanges to restore the equilibrium fig 13 a refinement of the discretization attenuates this effect but does not completely prevent it see section s3 3 of the supplementary material 3 3 2 test case co2 ability to simulate the water table evolution under spatially homogeneous or heterogeneous upper input test cases co2a and co3b are designed to evaluate the coupling s accuracy in simulating the evolution of a water table in an unconfined aquifer subjected to a time varying surface input the latter is either spatially homogeneous co2a or heterogeneous co2b in order to reproduce a recharge similar to that occurring in a heterogeneous urban environment with infiltration devices the domain is a homogeneous medium composed of silt with an extension of 500 m 500 m and a thickness of 20 m the slope of the top and bottom is 0 5 the gw level is fixed on the upstream and downstream boundaries at a depth of 3 m so that the surface of the gw table is parallel to the model s top and bottom at equilibrium state in its initial state the uz sz system is in its moisture content profile equilibrium state the reference rainfall pattern used as forcing is shown in fig 12c in test case co2a it is applied as is over the entire surface of the domain in test case co2b the surface is divided into 625 areas associated with three types of surface boundary conditions fig 12d constructed by applying a multiplying factor 0 1 or 10 to the reference rainfall intensity fig 12c the spatial discretization used for the simulation of both test cases corresponds to this division into 625 square cells each cell is then vertically discretized into 0 5 m reservoirs within the first 4 m at top of a reservoir of 16 m the assessment is based on a comparison with results provided by the feflow model solving the 3d richards equations with the brooks and corey s model the domain is horizontally discretized into elements of about 6 m2 vertically it is divided into layers of 0 05 m thick in the first meters to 1 m in the saturated part the rainfall input is modelled by an inflow at the top of the domain for the initial condition a preliminary simulation without rainfall is performed until a steady state is reached the evaluation focuses on the spatial distribution of gw levels after 50 days h xy 50d 100 days h xy 100d and 300 days h xy 300d considering the mean absolute error e xy mean eq 22 and the maximal absolute error e xy max eq 23 the temporal evolution of gw levels at two points of the domain per test case considering the mean absolute error e t mean eq 24 and the determination criteria r 2 eq 25 for test case co2a the domain centre h t center and 120 m downstream h t 120m for test case co2b beneath an area of zero surface infiltration domain center h t center and beneath an area of concentrated surface infiltration coordinates x 310 y 370 h t bc10 results related to the test case co2 table 8 and fig 14 a and c show that the fluctuations of a water table subject to a spatially homogeneous surface input are overall well reproduced the model also satisfactorily reproduces gw level fluctuations with a heterogeneous surface forcing test case co2b table 8 and fig 14 b and d below areas where the inflow is concentrated the maximum elevation is slightly overestimated and the decrease in water table recession is sharper fig 14d as noticed in test case co1 the water tale level may drop when the sz upper limit moves to a different reservoir this is especially noticeable during the lowering phase t 90 d fig 14d a refinement of the vertical discretization leads to smoothing the latter effect but only marginally influences overestimation of the peak elevation and of the recession rate see section 3 3 of the supplementary material 4 integration within the urbs model and preliminary application this section presents the integration of the developed modules within the urbs model sub section 4 1 and a preliminary application of the model to a hypothetical yet realistic watershed sub section 4 2 4 1 brief description of the urbs model and integration of the new modules the urbs model provides continuous simulations of the hydrological functioning of urbanized watersheds rodriguez et al 2008 it is based on a watershed discretization into urban hydrological elements uhe s each composed of a cadastral parcel and half of the associated street section this results in a highly irregular grid adapted to the structuring elements of the urban environment on each uhe fluxes and storages are computed by land use profile natural street building natural and street profiles can be partially covered by trees to take into account their role in rain interception and evapotranspiration runoff volumes generated over the various profiles are routed either to the outlet via the hydrographic network composed by street segments and sewer pipes or to stormwater management devices green roofs reservoir pavements infiltration devices the initial modelling of the underground compartment assumes a soil where the water flows are controlled by an exponential decrease in hydraulic conductivity with depth the soil characteristics are homogeneous over the simulation domain the unsaturated zone is modelled by a single reservoir fed by infiltration from the surface storage at constant rate emptied by evapotranspiration the exchanges between the unsaturated zone and the saturated zone take into account both gravitational and diffusion fluxes gw flows are computed at each interface between uhes using a 2d darcy computation without correction of the gradient computation method for irregular meshes gw seepage into sewer pipes is considered by using an analytical formula derived from agricultural drainage for more details the reader may refer to rodriguez et al 2008 morena 2004 and li 2015 new modules introduced in section 2 are integrated into the urbs model as alternative modelling options however urbs specificities require coupling assumptions and intermediate calculation steps first of all the uz flow computation by land use profile implies that each sz computation cell could be associated with of up to three uz soil columns each characterized by its own water content profile and uz sz exchange flow uz computation on each land use profile is then done using an identical gw level and the exchange flows with the sz are averaged over the computation cell the water content update at the end of the time step section 2 3 is then carried out by land use profile fig 15 presents a conceptual representation of the partitioning of sub uhe by land use profiles and of the flows calculated on each profile surface discretization into uhes according to cadastral parcels is not optimal for the computation of underground flows 1 regarding the geometry of urban objects e g infiltration devices building basements 2 regarding the characteristic scale of processes to be modelled e g mounding effect or 3 to ensure an accurate computation consequently the possibility of using a refined underground mesh is added in such case each sub uhe must be fully included in a single uhe to ensure proper code operation each sub uhe can intersect several land use profiles on each sub uhe subsurface flow computation is performed as described in section 2 surface storage and runoff remain computed at the uhe scale the infiltrated volume over each land use profile is then computed by averaging the contribution of each sub uhe weighted by the intersecting area of each profile this sub level of discretization is defined in pre processing and can be obtained for example using gis tools to divide polygons here the uhes into triangles the application case presented in the following section provides an example of such a division of the uhes into triangular sub uhes fig 16 4 2 application to a hypothetical urban watershed this part presents an application of the resulting coupled model to a hypothetical yet realistic urban watershed concentrating various interactions between surface and sub surface hydrology it does not aim to validate the model but to highlight potential benefits of taking into account underground interactions when modelling urban water cycle in shallow gw environments 4 2 1 description of the application case the watershed has an extension of 500 m 500 m a constant thickness of 10 m and a uniform slope of 0 5 it is composed of 70 uhes varying from 1100 m2 to 40 000 m2 and spread over various areas representing either a dense urban environment a commercial area residential area and a natural area fig 16 from the bedrock to the surface the soil is composed of a 9 3 m layer of loamy sand representing an aquifer formation a 0 4 m layer of sand representing a reworked geological formation and a 0 3 m layer of sandy clay loam table 1 representing the topsoil three buildings have basements set at 3 m one underground level with a hydraulic conductivity of 10 9 ms 1 and one of them is equipped with a drainage system fig 16 the sewer pipes combined sewer systems are located under the roads fig 16 at a depth of 2 m their conductivity is considered homogeneous and equal to g sew 10 6 ms 1 stormwater is managed by several infiltration devices fig 16 with a 20 cm depth variable surfaces and where the soil is considered to be reworked the two upper geological layers are replaced by a single layer of sand in this simplified application the runoff is assumed to be transferred immediately to the infiltration structures and or the basin outlet without calculation of network flows the gw level is fixed on the upstream and downstream boundaries respectively left and right boundaries in fig 16 at a 2 m depth so that the surface of the gw table is parallel to the model s top and bottom at equilibrium state without underground structures a preliminary simulation without rainfall is done until steady state is reached in order to prevent artificial overestimation of the drained volumes in its initial state the uz sz system is in its moisture content profile equilibrium state five scenarios are built by successively adding components to the underground representation the first scenario does not consider any interaction with the gw the bedrock and the gw table are lowered down 20 m the second scenario focuses on gw interactions with surface without any underground structures the third scenario includes sewer pipes but no building basements with or without draining systems the fourth scenario includes all the underground elements as detailed in the previous paragraph the fifth scenario considers all underground elements but without infiltration devices in order to evaluate potential influence of these devices on the underground structures the various scenarios are simulated over a continuous 1 year period using real rainfall records 6 min and pet records hourly disaggregated from daily from the trappes station 48 77 n 2 01 e for the year 2013 fig 17 the model is used with a constant 6 min time step the subsurface mesh fig 16 is made of 1238 triangular cells with a refinement in the vicinity of the areas where sharp hydraulic gradients are expected fig 16 4 2 2 simulation results the simulation results related to the various scenarios are presented in fig 18 and table 9 and subsequently described results related to the first scenario show that when interactions with gw are neglected scenario 1 almost all stormwater volumes conveyed to infiltration devices infiltrates the comparison with the second scenario highlights the significant influence of considering interactions between gw and infiltration devices on simulations results this directly results in a significant drop in infiltration within the structures and consequently in an increase in runoff in addition taking into account the influence of capillary rise from the shallow gw leads here to a higher estimation of the transpiration the inclusion of sewer pipes scenario 3 leads to a slight decrease in the simulated rise of the water table both overall and in the mounding beneath infiltration devices table 9 fig 18 the interactions between gw and surface are consequently reduced and the simulation results present 1 an increase of the infiltration volumes in the infiltration devices and 2 a decrease of the evapotranspiration compared to the second scenario gw only the inclusion of all underground structures scenario 4 leads to similar but amplified conclusions this is mainly due to the high gw level reduction resulting from the draining system equipping the building basement which further reduces interactions between gw and surface compartment comparison of gw levels at the end of the simulation fig 18a shows that this effect is locally more noticeable especially for the infiltration device located upstream of the drainage system this figure also shows that the draining system is a major outlet for the area and largely captures the water infiltrated into the upstream infiltration device it strongly modifies local flow directions which leads among others to disruption of local exchanges between the water table and the downstream limit which could represent a watercourse for example neglecting such structures is therefore likely to bias the results when assessing the impact of infiltration strategies on runoff volumes gw levels and base flow of neighbouring watercourses taking into account pipes and underground structures equipped with draining systems also makes it possible to estimate the extent to which the infiltration devices will increase the infiltration of gw returning to the sewer system by voluntary draining system and involuntary pipes drainage for example comparison between scenario 4 and 5 without infiltration devices shows that the implementation of infiltration devices leads to a 3 5 fold increase in the gw volumes returning to the sewer system by drainage difference which represents 29 of the volumes infiltrated into the infiltration devices 5 discussion 5 1 positioning and novelty of the modelling approach the modules developed in this study satisfactorily reproduce various processes and interactions commonly neglected in urban hydrological models e g swmm urbs wep although likely to significantly influence the urban water cycle in shallow groundwater environments for instance the unsaturated zone flow calculation module takes into account the influence of vertical soil heterogeneity and of capillary upwelling from the water table on the various related fluxes e g infiltration and runoff formation evapotranspiration gw recharge or gw drainage to satisfy evaporative demand in dry periods the gw flow calculation module allows the simulation of local gw fluctuations for instance beneath stormwater infiltration devices while taking into account lateral heterogeneities of the medium and various underground structures it allows the estimation of groundwater volumes drained intentionally pumping drainage systems equipping underground structures or not e g seepage into sewer pipes it is therefore possible to estimate the influence of these structures on the underground water storage and additional volumes to be managed by the sewerage system using irregular computation cells allows mesh adjustments to fit to urban objects geometry and potential refinements for a more precise estimation of local gw fluctuations and of the related interactions with the surface or underground structures furthermore the coupling procedure between these modules takes into account the dynamic feedbacks between surface and underground compartments although the modelling approach is relatively straightforward the evaluation of the modules shows that they provide results of satisfactory accuracy regarding 1 the objectives assigned to the inclusion of the underground compartment in urban hydrological models and 2 the high uncertainties regarding the underground compartment composition they also ensure the water balance the maximum water budget error for all simulations is reached in scenario 3 of the urbs model application case the total error over one year of simulation is then below 0 001 m3 about 5 10 7 of rain volumes the integration of the developed modules within the urbs model and its application to a hypothetical urban area illustrate the relevance of such developments to extend the applicability of an existing urban hydrological model in shallow gw environments this allows the exploitation of existing model capabilities regarding the modelling of the surface hydrology of urbanized environments e g runoff generation on impermeable and permeable surfaces decentralised centralised runoff management following complex water paths transpiration by vegetation while taking into account the specificities of these contexts the final model can for example handle i the influence of groundwater fluctuations on the infiltration capacity within permeable areas and stormwater infiltration devices or on transpiration by vegetation ii localized groundwater resurgences and their fate in the urban environment or iii the influence of underground structures on groundwater storage and the fate of drained groundwater in the urban drainage system the inclusion of these various components and dynamic feedbacks in an urban hydrological model is relatively novel the level of detail in the description of the urban environment and hydrological processes makes this model suitable for studies at intermediate scales typically from the city district to small urbanized catchment a few km2 the integration of these new features as optional modules has the advantage of providing a single tool that can be easily adapted to the modelling context and objectives furthermore the relative simplicity of the additional modules maintains fairly low computation times that remain compatible with sensitivity studies or multi scenario modelling it also allows a rather straightforward implementation in existing code the open source modules developed in this study may hence be used or adapted to extend in a similar way the applicability of other hydrological models for shallow gw contexts the modules developed in this study are not intended to be a substitute for a groundwater flow calculation system as soon as the objectives imply a more accurate gw flow modelling it is necessary to use a coupling with a hydrogeological model e g modflow feflow or an integrated model dedicated to the natural environment e g mike she gsflow their implementation in the urbs model nevertheless provides a complementary tool allowing for example 1 a rapid assessment of the relative influence of the various processes and interactions involved 2 to identify where physically based groundwater models may be needed and 3 to perform multi scenario modelling to handle the high uncertainties regarding the composition of the subsurface compartment clark et al 2011 5 2 limitations and needs for further developments 5 2 1 spatio temporal discretization and computation method the developed modules assume a single fixed time step i e identical for each module and during the whole simulation which highly facilitates the management of dynamic interactions between the various model components however the time step required for surface or uz flow calculations is often unnecessarily fine for gw flow computation the use of a module specific time step does however introduce further questions on the maintenance of balances and communications between modules beegum et al 2018 branger et al 2010 both sz and uz modules rely on a sequential computation of flow between the computing units on the basis of hydraulic heads sz or capillary pressures uz at the previous time step it distinguishes them from true hydrogeological models based on a numeric resolution at each time step of the matrix system linking all the calculation units this method combined with a fixed time step limits the use of fine mesh the minimum size of the elements depending on the soil characteristics below this value the calculation may be unstable see section 3 of the supplementary material however urban hydrological modelling usually requires few minutes time step to describe surface hydrological processes such time steps alleviate these restrictions and allows a relatively stable resolution as shown by the various test cases special attention must nevertheless be paid to discretization if larger time steps are used or if highly permeable soils are considered the use of a variable time step for these modules allowing for example a division of the time step for particular units would make it possible to at least partially solve this issue however further development of the model should focus on the introduction of matrix resolutions for both uz and sz equations to extend the range of applicability of the model 5 2 2 modelled processes although the approach satisfactorily simulates subsurface flows it suffers from various limitations that could restrict its scope of application but may be partially addressed in the future first of all the uz model only considers single porosity media and further development should be carried out to allow for macropore flows which can significantly influence uz functioning and infiltration processes beven 1982 simple approaches such as those used in the mike she model dhi 2017 seem to be adapted to the level of complexity of the module and the level of knowledge of the macropore distribution similarly the gw flow calculation module cannot model the influence of double porosity matrix and cracks or horizontal anisotropy of the hydraulic conductivity in the aquifer for the latter case a minor modification of the calculation presented in part 2 1 could be considered following for example the approach presented in de marsily 1981 anyway the approach remains limited by the 2d assumption of a negligible vertical component of the hydraulic gradient it may thus provide erroneous results when this assumption is not valid for example in case of vertical anisotropy of permeability similarly the uz modelling method is simpler but remain less reliable and accurate than models solving the 1d richards equations however the latter remain associated with a high computation time and potential convergence issues ross approach ross 2003 seems from this point of view to be an interesting alternative to the usual methods of solving the richards equation the modelling of gw infiltration in sewer pipes is quite elementary first of all it assumes a homogeneous infiltration over the pipe length in a cell whereas this infiltration is actually due to punctual defects cracks joints however due to the low level of knowledge of the spatial distribution of these defects this homogeneous infiltration assumption is reasonable furthermore such description would allow the use of distributed value for the parameters controlling gw sewer interactions if the conditions of sewer pipes were known a second limitation of the module is that it does not consider the head in the pipes for the computation further developments should introduce this functionality to represent more realistically interactions between gw and pipes finally the vegetation depiction is relatively simplified it would in particular be relevant to introduce the possibility to vary vegetation characteristics throughout the year and to take into account compensation processes in the root zone 6 conclusion this paper details the development the evaluation and the integration within the urban hydrological model urbs of a set of modules designed to depict the urban underground compartment in shallow groundwater contexts the unsaturated flow computation is based on a coupling between a reservoir model and a green ampt model it allows taking into account the influence of a shallow water table on infiltration and evapotranspiration processes in homogeneous or stratified soils the modelling of the saturated zone is based on a 2d groundwater flow computation allowing 1 the use of irregular meshes adapted to the geometry of urban components and 2 the simulation of interactions between groundwater and various underground structures sewer pipes wells underground constructions basements car parks etc with or without draining systems the evaluation of the modules by comparison with reference models hydrus and feflow shows their ability to efficiently reproduce the processes and interactions of interest the new modules are integrated within the urbs model through an approach allowing dynamic feedbacks between the various components it provides an original framework for the assessment of the impacts of urban development or stormwater management strategies in shallow groundwater contexts its application to a hypothetical yet realistic urban area illustrates the benefits of such integration to extend the applicability of an existing urban hydrogeological model to such contexts the approach presented in this paper could be adapted to complement existing modelling frameworks to provide a more realistic description of the hydrological functioning of urbanized watersheds in shallow groundwater contexts software availability software name urbs development team floriane morena fabrice rodriguez marie laure mosini yinghao li jérémie sage william pophillat availability free download at https nam11 safelinks protection outlook com url http 3a 2f 2fdoi org 2f10 5281 2fzenodo 5115863 amp data 04 7c01 7ck sivakolundu 40elsevier com 7c98de0e20a77241be03c308d95894a063 7c9274ee3f94254109a27f9fb15c10675d 7c0 7c0 7c637638218747206460 7cunknown 7ctwfpbgzsb3d8eyjwijoimc4wljawmdailcjqijoiv2lumziilcjbtii6ik1hawwilcjxvci6mn0 3d 7c1000 amp sdata zknsrg8ugb4tqroroximq1evoilxsygqduxqdi1bd04 3d amp reserved 0 program language c program size 600 ko the model diagnosis was carried out using the following software packages feflow v7 2 hydrus 1d v4 17 author contributions w p j s f r and i b designed the research w p coded the model and performed the simulations j s f r and i b contributed to the definition of the model and the methods w p wrote a first version of the paper that was revised by j s f r and i b declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this study was carried out under the opur program and supported by the french ministry for the ecological and inclusive transition and the french national institute for agriculture food and environment the study also contributes to the research conducted within othu field observatory in urban hydrology and the onevu nantes urban environment observatory the authors would like to thank marie laure mosini for her useful contribution to the development of the urbs software the authors acknowledge dhi for the sponsored feflow license appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105144 
