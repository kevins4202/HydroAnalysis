index,text
25690,watershed models are robust tools that inform management and policy in a variety of sectors but these models are often neglected through time due to economic or technical constraints additionally they are not readily accessible tools for key decision makers conversely machine learning models are robust alternatives to common hydrologic modeling frameworks the random forest algorithm specifically is an interpretable predictive tool we couple annualized agricultural non point source annagnps model output an abstract anthropogenic flood risk metric and develop a random forest model to provide an empirical tool that benefits decision makers in the des moines lobe of the prairie pothole region in north central iowa the developed model has the capacity to predict our flood risk metric calibration r2 0 9 validation r2 0 7 for individual farmed prairie potholes across a variety of morphologic and management conditions and can be used iteratively to assess alternative actions keywords prairie potholes random forest machine learning decision support tools software and data availability program title annualized agricultural non point source annagnps model annagnps v5 50 a 013 2018 08 21 was used for this study contact address ron bingner usda gov software access https www nrcs usda gov wps portal nrcs detailfull national water manage hydrology cid stelprdb1043535 software requirements annagnps is available for windows 7 8 and 10 and for windows xp 7 8 and 10 or on other platforms that have a compiler for ansi standard fortran 2015 a minimum of 2 gb of memory is recommended annagnps includes a memory manager and thus has no limits additional computer storage may be necessary for input and output data program language fortran availability and cost open source research data and model development repository ppmst model contact address kaleita iastate edu software access https github com bnahkala ppmst model software requirements r v3 6 2 or later program language r availability and cost open source 1 introduction complex watershed models are frequently used to support policy and land management decisions across the world and are considered robust decision support tools dst for scientific hydrological alternative assessment horn et al 2004 usepa 2001 however the resource intense models with high learning curves are generally unavailable to key stakeholders that are the primary decision makers even often including technical staff ranjan et al 2020 these models frequently become outdated or are left to collect dust as funding or primary users fizzle out bridging the gap between these challenging models and resilient and accessible decision support tools is thus a worthy endeavor empirical tools are an alternative to complex deterministic or physically based models in particular machine learning methods have been increasingly utilized in hydrologic analysis and decision making to reduce the computational or data requirements as a replacement for data intense hydrologic modeling rahman et al 2020 rasoili et al 2012 lange and sippell 2020 random forest machine learning contains an ensemble of categorization and regression trees cart that are grown independently and randomly lending a stronger validity and known uncertainty than traditional cart models random forest has emerged as a common machine learning algorithm within hydrologic studies wang et al 2015 singh et al 2017 naghibi et al 2017 as with other machine learning models random forest models can provide a less data intensive input parameter set than distributed watershed scale hydrologic models complex and local decisions are the daily considerations of landowners and renters in the agricultural landscape of the des moines lobe dml of iowa which is replete with farmed prairie potholes farmed prairie potholes are small isolated depressions frequently classified as semi permanent wetlands that make up a significant portion of land area in intensively farmed dml which is part of the larger prairie pothole region ppr flooding greater than 2 days during any portion of the growing season april september almost certainly leads to crop loss in these closed depressions naturally disconnected from any significant surface drainage network prairie potholes are only moderately connected to the surrounding geography via shallow subsurface and groundwater interactions and intermittently during overflow after large precipitation events roth and capel 2012 evenson et al 2016 alternative management options may be of interest to landowners who consistently lose crops and profits to these areas the near annual flooding despite artificial drainage within many prairie potholes results in a net economic loss from flooded regions of the field fey et al 2016 potholes are of interest hydrologically for their ambiguous connection to the surrounding landscape with implications affecting their status as a significant nexus in waters of the us golden et al 2016 jones et al 2019 studies further emphasize the ability of prairie potholes to filter nutrients schilling et al 2019 badiou et al 2018 and provide habitat and biodiversity janke et al 2017 in their natural state however those ecosystem benefits can be significantly reduced or eliminated under farmed conditions martin et al 2019b schilling and dinsmore 2018 badiou et al 2018 farmed potholes can be managed for profit or conservation more deliberately than current prevent plant support programs and conservation guidance generally encourages which largely focus on the intent of a single landowner or specific portion of the field ameli et al 2019 bauer et al 2017 however dsts are needed that bridge the gap between highly technical watershed models and farmers making decisions on a daily basis in order to bring perspective to flood expectations and conservation opportunities on larger scales farmed prairie potholes in the dml have been modeled to assess how field management can affect flood dynamics upadhyay et al 2018 2019 these studies utilize the usda annualized agricultural nonpoint source annagnps model a lumped continuous simulation surface runoff model bingner et al 2018 this model has successfully characterized changes in land use and drainage within potholes allowing for comparative assessments of viable management decisions these scenarios represent changes in the physical nature of pothole watersheds but these models do not provide accessible data in a decision making framework because they are specific to individual potholes and require relatively extensive parameterization in order to use effectively importantly and ignored by watershed models one of the biggest factors identified in agricultural decision making is managing risk specifically with regards to environmental risk wauters and mathijs 2012 this in part can be attributed to the volatility of agricultural markets and a frequent inability to return a profit in row crop agriculture this can involve the biophysical risk of farming certain crops or can involve the financial and economic implications of farming and conservation alternatives risk in the context of farmed prairie potholes involves the ability for a farmer to make a profit from the pothole land area the perceived risk can be explained by the frequency and extent of pothole flooding or flood risk which follows common frameworks for flood risk assessment merz and thieken 2009 koks et al 2015 this goal of this study was to develop and contextualize a flood risk identification method and tool from the lens of a farmer for prairie pothole management that is simple quantitative and utilitarian we assess the use of a machine learning model random forest for predicting flood risk of prairie potholes due to land use changes the model utilizes output from the hydrologic model annagnps integrates a method of risk interpretation in prairie pothole flooding and trained to predict flood risk based on a reduced easily attainable set of input parameters 2 methods 2 1 study potholes six prairie potholes named bunny walnut lettuce cardinal hen and mouth have been extensively monitored and modeled martin et al 2019a upadhyay et al 2018 2019 and are utilized for this study the 5 of these potholes reside in a 1 square mile section southeast of ames ia in the dml of the ppr mouth lies approximately 3 miles northeast of this site on a separate research farm all potholes but mouth have been conventionally managed through the study period monitored and modeling framework described in previous studies the western half of mouth has been enrolled in the conservation reserve program and its watershed is the most diverse with portions planted to plot scale studies of conventional and perennial biomass crops characteristics of each pothole are outlined in table 1 including a snapshot of their watershed and maximum flood extents morphology column maximum flood capacity overflow depth maximum concentrated flow path and maximum watershed relief these values were determined from gis analysis of the digital elevation model and flow path was sourced from the annagnps model interpretation of flow paths described in the following sections 2 2 annagnps simulations individual watershed models of the 6 potholes shown in table 1 were built using the annualized agricultural non point source pollutant loading model annagnps and were previously calibrated to monitored data as described in nahkala 2021a alternative management scenarios that assess drainage tillage and land management practices were assessed further nahkala et al 2021b and a synthesis of that output is provided in section 3 4 of greatest interest in this study were common practices applied by key decision makers in the dml drainage investments and land retirement are prevalent but to varying formalities and spatial extents our modeled scenarios reflect varying investment in drainage infrastructure and varying levels of conservation or lazy land retirement additionally modeled tillage practices reflect conservation and no till options that are increasingly adopted to reduce input costs or to provide environmental benefits busari et al 2015 a full matrix of these options was modeled to allow for direct comparison between factor levels models were run for a 25 year simulation period from 1994 to 2018 to capture variation in climate parameters rainfall depths were acquired from the parameter elevation regressions on independent slopes model prism and were merged with weather station parameters wind velocity wind direction maximum temperature minimum temperature dew point temperature solar radiation from a nearby station as part of the sustaining the earth s watersheds agricultural research data system stewards network naming of the scenarios is explicitly described in nahkala et al 2021b table 1 we used 4 levels for drainage none low medium high 4 levels for tillage field land use conventional conservation no till and retired and two levels for pothole land use farmed retired we coded each scenario based on these parameters following the convention dx lu tl where dx is the drainage represented by d0 d3 lu is the pothole land use represented by cs corn soybean rotation or r retired and tl is the tillage or field land use represented by cv conventional ct conservation nt no till and r retired the simulations provided 168 25 year simulations with 28 being assigned to each unique pothole but representing the same range of climatic and management conditions we summarized raw water level output of the pothole in terms of ponded water volume 2 3 flood risk quantification risk quantification can be conceptualized by the multiplicative interaction between the frequency of a damaging event occurring and the spatial or temporal extent at which that event occurs gloser et al 2015 we created three summary metrics from the annagnps simulations that reflect both frequency and spatial extent of pothole flooding fig 1 shows the conceptual data manipulation described in this section the first summary metric f averages the number of days in each month as a percent in which there is standing water in the pothole across all years of simulation an example is shown for may via equation 1 eq 1 f m a y i 1 25 f i 25 where f is the fraction of the month that is flooded and i is the year of simulation the second metric v is an annual average of the maximum volume of water that ponded in each month computed as a percent of the total storage volume of the pothole before overflow this was calculated using the same averaging as shown for percent of the month flooded risk parameters for these metrics were computed monthly to enable the incorporation of weighted risk ratings for each month reflecting an anthropogenic and biophysical concern for flooding during primary growing season months the scenarios weighted risk factor for each variable was computed by equation 2 eq 2a f s c e n a r i o j 1 12 f j w j eq 2b ϑ s c e n a r i o j 1 12 v j w j where f scenario is the scenario s overall risk factor for duration of ponding ϑ scenario is the scenario s overall risk factor for volume of ponding j is the month in the simulation and w is the weight assigned to each month finally the third metric averages the percent of annual ponding events that occur for varying durations measured in days which acknowledges that longer periods of inundation are more conducive of crop loss than short term events muth and bryden 2012 events were binned based on the number of days of consecutive flooding up to 10 days where all values greater than or equal to 10 were binned a weighting scheme was applied to these data where the weight increased linearly from 0 to 4 days and then remained constant this reflects that events of 1 4 days are increasingly risky to crop survival but that any flooding longer than 4 days results in a high probability of crop death rhine et al 2010 zaidi et al 2004 this follows the same calculation structure presented for the first two metrics as shown in equation 3 eq 3 ρ s c e n a r i o k 1 10 p k w k where ρ s c e n a r i o is the event based risk metric p k is the percent of flood events of a certain duration w k is the weight applied to those events and k increments the specific event bin the three parameters were scaled from 0 to 1 multiplied and rescaled to a maximum risk of 10 this completes the conceptualization of risk as a multiplicative interaction of spatial and temporal variables as shown in equation 4 eq 4 r i s k f ϑ ρ these values represent the risk rating for a single pothole under a set of management conditions to be used as the predictive output of a machine learning model described in section 3 4 the random forest model can predict numerical values regression and categorical labels classification for assessment of the model s classification option categories were created from the scaled ranking system described above jenks natural breaks were used to determine potential categorical risk bins jenks 1967 we assessed both a 3 category and 6 category classification system which approximate a high medium low risk classification scheme 2 4 training data the annagnps output from the 168 simulations described in section 3 2 were processed using the risk quantification method described in section 3 3 this dataset of 168 samples including the assigned risk calculated from section 3 3 and associated random forest input variables section 3 5 provides the training data for the random forest model described in section 3 5 fig 2 summarizes the distribution of training data averages for the percent of each month that has standing water on a daily basis each point represents the simulation average for each month may and august had the highest average temporal inundation with over 30 of the days each month seeing standing water in some of the potholes on average across a simulation scenarios with less drainage had higher averages while the converse was also true lettuce hen and cardinal generally flooded the longest with low drainage scenarios notably in all scenarios mouth remained less than 10 flooded by month across the 25 year average however mouth frequently reaches a significant portion of its volume seen in fig 2 showing that it s flood dynamics tend to be flashier despite the lack of artificial drainage this could be due to increased soil health from land retirement in general most scenarios only averaged 1 3 days of flooding by month not reflected in the averages are months that were flooded for significant portions in the natural drainage scenarios these results also show that in the growing season months some flooding is guaranteed across a range of climate conditions as there were no nonzero values in may june august and september fig 3 depicts the distribution of the 28 scenarios with regards to the 25 year average maximum flooded volume by month each data point represents one scenario s 25 year average of the maximum flood water volume compared to the maximum volume mouth is the only pothole that frequently approaches its maximum volume in the early growing season however the inclusion of large extreme values the lettuce and cardinal data suggest more of their surface area may be more susceptible in higher precipitation years compared to other potholes including all potholes and scenarios in fig 3 average iqr for the growing season months is 14 24 21 32 11 22 6 13 22 30 and 15 25 apr sept of the true maximum volume these data represent the middle 50 of monthly average maximum surface areas in all scenarios for all potholes interpreted this could be seen as the middle ground of flooding expected for moderate to minimally drained potholes in an average year 2 5 random forest machine learning 2 5 1 background random forest rf is a machine learning algorithm that generates an ensemble of classification trees from a subsampled matrix of ranked or classified data that includes labeled predictor variables breiman 2001 the algorithm randomly samples predictor variables to test at each tree node and finds the minimal residual sum of squares rss for regression or gini categorization to create a split in the data representing a branch in the tree samples are returned to the training set to be available for reuse known as bagging trees grow any number of splits necessary to separate data into either predicted values regression or identified categories once the model is trained a sample is processed by each tree within the forest and the average regression or majority categorization vote is used as the model prediction value for that sample the model is validated by testing out of bag samples from the remainder of the dataset random forest has been using frequently for hydrologic applications and decision making frameworks because of its interpretability chu et al 2020 gibert et al 2018 koks et al 2015 sahoo et al 2017 wang et al 2015 the algorithm creates a distinct understandable binary structure similar to commonly used flow charts this gives it an advantage in agricultural decision making over other machine learning methods such as deep neural networks or support vector machine which operate as black boxes and are more reliant on high level statistical methods adoption of online decision tools remains a challenge in agricultural settings ranjan et al 2020 and the identification of a tool with an interpretable structure and explainable algorithm is perceived as highly beneficial liu et al 2008 rose et al 2016 2 5 2 random forest model we used the open source software r r core team 2020 to implement the random forest model via the package randomforest liaw and wiener 2018 training samples represent a single simulation with the predicted output being the assigned flood risk from section 3 3 both regression and categorical models were evaluated based on their r2 and error rates respectively that is the risk eq 4 was treated as a continuous variable in the regression model and binned into categories in the categorical model a total of 8 predictor variables were used to train the rf model table 2 this includes 4 qualitative variables including drainage d tillage t land use land cover of the pothole lulc p and lulc of the field lulc f additionally 3 quantitative variables were identified using arcmap 10 4 including the maximum pothole depth md catchment area to pothole area capa ratio and maximum watershed relief wr highest lowest elevation maximum concentrated flow path mfp was calculated as the longest continuous set of reaches within the watershed model in annagnps three parameters in the random forest algorithm were assessed to optimize the model mtry ntree and maxnode these represent the number of input variables assessed for data splitting at the generation of each node mtry the total number of trees grown in the forest ntree and the maximum number of terminal nodes unique predictions grown by a tree maxnode we attempted to limit the terminal nodes in order to enforce simplicity on tree structure while maintaining predictive accuracy a desired goal for decision making tools the model was developed using bagging or sample with replacement to compensate for a small dataset 2 5 3 assessment of the forest and tool development categorical and regression models were calibrated and validated based on their error rate and r2 respectively seventy percent of the samples were used for training and 30 were reserved for cross validation all combinations of mtry constrained between 2 and 7 ntree constrained between 500 and 2100 at intervals of 200 and a set seed when subsampling the training set seed 1 to 100 were assessed the optimal set of input parameters were selected with uncertainty represented by the differences in statistical performance due to different training sets once a parameter set was selected terminal nodes in each tree were limited using a range from 5 to 50 and predictive performance was assessed the r package randomforestexplainer was used in addition to randomforest to assess the relative importance of input variables paluszynska et al 2019 liaw and wiener 2018 this package can extract the number of times a predictor variable is the root or first split in individual trees can calculate the mean depth of the first appearance of a variable in a forest can calculate the increase in mse caused by individual variables if removed among other variables after assessing input variable importance predictor variables were removed from the model and statistical performance was assessed the top three most important variables based on mse increase remained in all models while the removal of other variables was assessed further validation of the model was conducted using monte carlo mc and sensitivity analyses the monte carlo analysis was conducted by randomly sampling all input parameters including factors and numeric values provided as a discrete list within the range of training data samples were filtered to reflect reasonable land use decisions for example generated scenarios that contained a farmed pothole but retired field were removed sensitivity of the model to the 4 quantitative input variables was assessed by calculating the change in risk level divided by the change in parameter value compared to a baseline holding all other input parameters constant sensitivity was calculated across the range of values observed in the training set plus or minus 10 finally representative trees were extracted from the forest following the methods of banerjee et al 2012 which are available in the github package reprtree the method for identifying representative trees assesses both the architecture of the tree and the terminal node predictions the package implements one of three distance evaluation metrics used in the study for any two trees distance metrics are similarity in the input parameters used to split the trees similarity in clustering of outputs in the terminal nodes of the trees and similarity in the predictions of the trees the tree with the minimum distance to the all the other trees in the ensemble is considered representative trees were assessed to validate the model and identify patterns in the data splitting as a supplement to the randomforestexplainer package the versatility of employing a single tree from the random forest model was also assessed to identify opportunities to implement a simplified version of the model under potential software limitations or educational contexts a single tree provides versatility by having a singular structure that can be easily visualized or coded into simple decision making tools yet still provides realistic predictions 3 results and discussion 3 1 scenario risk levels the range of flood risk values derived from the ranking system ranged from approximately 0 1 to 10 the average and median risk levels are 2 1 and 1 1 respectively in thinking about the multiplicative effects of risk in our model which multiplies spatial extent by temporal frequency our risk ratings capture variation in risk with up to two orders of magnitude difference on this scale however this scale may not represent the full range of flooding observed in prairie potholes the potholes in iowa have been significantly drained and are considered semi permanent however potholes in other regions of the ppr can experience infrequent to permanent flooding for our purposes we are concerned with farmable potholes for which a working definition may be pinned by qualifications for prevent plant and crop insurance for prevent plant insurance a pothole would have to produce crop 1 in 4 years more frequent flooding while most prevent plant payments in iowa correlate to crop flood losses in 1 out of 4 years less flooding in this context risk in this model may show the distribution of pothole conditions that are farmable but may only produce crop 25 75 of the years 3 2 random forest performance and sensitivity 3 2 1 categorical vs regression model and model parameters categorical random forest models were tested using 3 classifications and 6 classifications that is whether risk is assessed using three levels or six levels models with less bins resulted in smaller error rates which correlates to an increased prediction capacity that is falsely elevated by weak bin resolution even using naturally occurring breaks in the data however the out of bag error rates for categorical models were poor table 3 show variations on oob error and gives tables for average oob error rate based on the random forest mtry average performance based on the number of trees did not vary the average oob error for the 3 category model was 13 with minor sensitivity to mtry table 3 the oob error for the 6 category model was 33 with some sensitivity to mtry the regression model performed better than the categorical models with average r2 values of 0 88 across all calibrated models validation frequently performed slightly higher with an average r2 of 0 91 fig 4 displays performance metrics error and r2 for calibration and validation variation was induced by setting the seed before taking a random sample of data to train the model resulting in 100 data points per boxplot we conclude the random forest model is more useable and interpretable when trained as a regression model than as a categorical model due to the strong performance with respect to r2 the categorical model s performance was visually deemed more contingent on the number of bins suggesting the prediction itself is less reliable than a numerical regression additionally an average classification error of 13 33 percent can be viewed as extremely poor in the context of decision making frameworks mowrer 2000 the regression model was chosen over classification for further application in this study a manual review of individual regression models was conducted and the highest performing parameter set was selected for analysis based on r2 we chose model parameters of mtry 6 and ntree 500 preferentially we found a seed that produced the highest r2 during calibration seed 22 the r2 for calibration and validation of these model inputs were 0 95 and 0 78 respectively while the mean squared error is 0 55 trees in random forest training naturally grew more than 20 terminal nodes in all cases from 5 terminal nodes to 50 the model s predictive capacity remained strong and acceptable with respect to r2 we observed that below 15 terminal nodes the explained variation significantly decreased with the reduction in terminal nodes r2 remained nearly constant when terminal nodes were expanded to 25 or greater between the 25 node and 15 node limitation a decrease in the amount of variance explained was 2 5 percent we selected 15 as the maximum number of terminal nodes for future use of the model to balance both usability reflected in the size of a decision tree if a person were to use one manually and predictive strength r2 3 2 2 model input variable analysis the physical location of input variables on regression tree nodes lends insight into the influence of each variable on model performance we first plotted the minimum depth or first occurrence of an input variable in each tree the distribution of this minimum across all 500 trees is shown in fig 5 the graph reveals that the drainage and tillage inputs were most frequently used as the first split in the forest trees with drainage being used the majority of the time these inputs thus are factors with the largest difference in risk observed between differences in factor levels as determined by the minimization algorithm that the model employs during calibration the catchment area to pothole area ratio and maximum watershed relief were used at a similar frequency to tillage the land cover of the field maximum flow path maximum pothole depth and pothole land cover were used less frequently within the trees and only to create higher resolution splits in the flood risk prediction the gray portion of each bar represents the number of trees in which a variable was not used most notably pothole land cover was not used in over half of the decision trees in the model suggesting it is either not important or is correlated to another input variable in this case the tillage level na was used instead of no till to represent differences in true no till conditions and the lack of tillage due to perennial vegetation this may suggest the removal of lulc of the field is appropriate because higher resolution can be provided by the tillage parameter the percent increase in model mse when a variable is removed is given in fig 6 and further shows that drainage is the most influential categorical input however maximum watershed relief and capa ratio incur the second and third largest increase in mse respectively followed by maximum flow path and maximum depth these do no perfectly align with the minimum node depth in fig 6 because while fig 6 shows the minimum split each variable may be used multiple times within a single tree while a variable like tillage can occur frequently within the upper reaches of a decision tree it does not guarantee a finer resolution of the data at the terminal nodes considering mse increase lulc of the pothole lulc of the field and tillage appear extraneous however this must be considered in tandem with other metrics like minimum node depth which reveal the importance of tillage we further tested the model by removing individual or groups of the five least important predictive variables based on mse increase fig 6 the results of which are shown in table 4 removing any one variable other than drainage did not significantly affect the r2 performance of the random forest model removing any two or any three of the least significant variables only moderately affected the model removing the lulc parameters marginally increased the performance of the model suggesting these are strong candidates for removal from the model inputs or suggesting that the model is overfitted these models were run with the prescribed maximum 15 terminal nodes the trees in the forest continued to grow 15 terminal nodes despite reductions in the number of input variables the tree simply grew more branches with nodes that recycled the same variables finding a higher resolution of divisions within each predictor however the flexibility and interpretation of the forest remains questionable with reduced inputs due to this limited dataset we decided on a model that utilized all variables to provide more opportunities for flexibility when more modeling data is generated 3 2 3 validation of the model the predicted flood risk from the monte carlo simulation are shown in fig 7 most of the synthetic data were predicted to have a risk less than 2 consistent with the frequency of data that should have had medium and high drainage the largest concern after training the model was the ability to predict values at higher risk levels due to the small sample size in the training dataset this was both due to the multiplicative nature of the risk ranking system creating larger risk differences for high values and the limits of the annagnps simulations we pursued however we see that there is a relatively balanced spread in the middle of the predictive range 2 6 currently the model as low resolution above a risk level of 6 because the training data includes a limited number of samples in which the model predicts high risk however the model still predicts nearly the full scale of risk values within the interpolation range of each input parameter panel b of fig 7 is most informative in describing the natural risk range of potholes the largest range of risk predicted occurs with no drainage which illustrates that some potholes are inherently riskier than others the reduced spread of data in the medium and high drainage scenarios shows that there are decreasing returns in drainage investment and that some level of flood risk is ultimately guaranteed based on current drainage practices these observations lend to the idea that potholes can be preferentially identified for management or conservation activities based on this risk scale the input parameters for this model that directly characterize specific potholes and thus those that are most helpful in comparing multiple potholes directly are the 4 quantitative variables capa ratio maximum flow path maximum depth maximum watershed relief 3 2 4 review of representative trees and their structure the structure of 1 of the top 10 representative trees in our model is shown in fig 8 here we can see the risk prediction at terminal nodes most trees had a higher density of a low risk predictions reflecting the distribution of the training dataset this results in reduced resolution when assigning risk to medium and high risk potholes it might be best to consider these higher risk levels with a band of uncertainty in the context of this model the mse being 0 55 additionally the model takes the average prediction of all trees and because there are less training samples in the higher risk ranges the average tends to underestimate some high risk conditions visual examination of the structure of a tree can provide insights into influential parameters parameter level relationships and by extension the behavior of the system which can be as powerful as interpreting the numerical prediction for example many of the terminal nodes group both high and medium drainage together without a node split suggesting the risk is similar between those two levels not only does this enable numerical predictions it has direct consequences on management recommendations while the model does not recommend action it might suggest that an investment in increased drainage does not reduce risk for many potholes the reason for using a random forest over a simpler cart model is to add robustness to a prediction it is counterintuitive to reduce a more robust tool to a simple classification and regression tree that generally has less predictive power however in the context of future applications as a decision tool users need an easy to use method that has versatility or can be easily explained the representative tree approach as explained by banerjee et al 2012 allows for easy decision making a single tree while maintaining results that are similar to those from the entire forest the r2 from the representative trees often equal or exceed that of the ensemble and thus can still be utilitarian for predictive and demonstrative purposes 3 2 5 interpreting output context and limitations of the model the data in these modeling methods reflect the intensity of flooding in farmed prairie pothole systems it is not intended to be used directly for flood mapping regulatory permitting or other professional testimonials this model is meant to be used iteratively and comparatively to help landowners assess the relative effectiveness of different actions however the larger context of decision making involves socioeconomic financial and personal values interpreting the output of this tool as presented therefor is neglecting to examine the context of its use additions to this model would include an economic assessment of key land management changes representing the cost associated with changes in field management flood risk data can be utilized to show options relative to a baseline by applying theoretical scenarios to the model this provides a basic level of information without prescribing a solution within a framework that cannot capture broader personal and societal context model interpretation is improved upon by evaluating both the output and the input equally once the model has been employed for example retiring the entire field or pothole microwatershed results in significantly lower predicted risk at face value all fields with potholes should be retired to crp however this ignores the need society has for agricultural production among many other reasons conversely many farmers add drainage to their potholes periodically evaluating a change from subsurface drainage to subsurface drainage with inlets shows that risk decreases however this risk may only be incremental and the economic burden for minimal gain is not reflected in the model this is where the model allows for flexible personal integration of values perceptions of risk and broader contextual goals of the user finally this model is limited by the small number of potholes and thus the small number of combinations of input variables in our original dataset and the resulting imbalance in outcomes with more low risk scenarios in our training dataset than high risk scenarios 4 conclusions a method has been devised to aggregate time series watershed modeling output of individual prairie potholes into a singular flood risk ranking this ranking represents the flood risk of a pothole on a relative scale based on the range of observed hydrologic conditions attained from 28 modeling scenarios and 6 potholes this risk is placed on an arbitrary scale and the risk does not represent an explicit parameter such as return period of a storm the prediction reflects a ranked value based on the anthropogenic lens with which potholes incur risk to farmers this incorporates both the extent of flooding and duration of flooding and considers how these apply to the biophysical risk to row crop agriculture though it does not include parameters that assess crop survival explicitly a machine learning method has been employed that predicts the flood risk of individual potholes based on the ranking system devised in this study the random forest algorithm can be trained to predict this risk in iowa based on easily identifiable management and morphological characteristics this method reduces the need for data intense spatially and temporally explicit watershed models that are time and resource dependent this alternative helps land managers assess the relative effectiveness of different land use practices on their field without having to have access to a fully developed process based model this method minimizes the need for highly technical knowledge of distributed integrated watershed modeling software in preliminary decision making contexts landowners may efficiently use knowledge offhand about their field and do a cursory review of quantitative parameters using a web based gis platform to inform this flood risk model however the risk level output by our empirical model does not fully consider the anthropogenic lens not all scenarios with high flood risk represent economic or agricultural risk and so the analysis must be carefully considered the flood risk presented in the model must be coupled with an analysis of cropping systems to assess potential losses or improvements incurred representative trees can be extracted from the forest and provide both predictive strength and educational opportunities reducing the forest to a classification and regression tree enables simplistic implementation of a tool for landowners if desired compared to the technical implementation of the entire random forest model the random forest r2 can be interpreted as the average across all trees in the forest suggesting that some individuals within the forest predict risk more accurately the representative trees presented here performed better than the ensemble with respect to r2 in the training period but performed poorly during the validation period the limitation of individual trees in practical application is that the number of terminal nodes was reduced to 15 within the model this limits any assessment of new unique output to 15 possible levels of classification many of which are clustered in the low risk spectrum this limits the resolution of classifying medium to high risk potholes using singular decision trees the model currently is limited by the training data despite remaining a valuable tool any physical misrepresentations in watershed models would propagate through this model suggesting the need for continued validation of hydrologic parameters in watershed modeling via field in situ field trials the sensitivity to inherent pothole characteristics shows that a more robust training set of potholes n 20 would be desired but the full assessment of the model reveals its educational capacity and ability to be easily developed further we have shown that the model retains the capacity to predict nearly the entire range of risk defined in our ranking system and its only limitation results from a lack of simulations in the high risk range future work should incorporate specific anthropogenic risks in modeling such as empirically predicting crop loss risk or economic risk taking the flood risk prediction one step further a larger set of pothole data across the des moines lobe is necessary to continue validating the random forest model and reduce any expected extrapolation required by the current model funding sources this work was supported by the us environmental protection agency assistance agreement cd97753901 it has not been subjected to the agency s product and administrative review and therefore might not necessarily reflect the views of the agency no official endorsement should be inferred declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors thank vitor martins for providing guidance in the construction of the random forest model to andrew vanloocke for providing additional insight on the stakeholder perspectives of agricultural models and to the anonymous reviewers who helped improve the quality of this paper author contributions b n a k and m s designed the research b n performed the research b n analyzed the data and b n wrote the paper a k and m s edited the paper 
25690,watershed models are robust tools that inform management and policy in a variety of sectors but these models are often neglected through time due to economic or technical constraints additionally they are not readily accessible tools for key decision makers conversely machine learning models are robust alternatives to common hydrologic modeling frameworks the random forest algorithm specifically is an interpretable predictive tool we couple annualized agricultural non point source annagnps model output an abstract anthropogenic flood risk metric and develop a random forest model to provide an empirical tool that benefits decision makers in the des moines lobe of the prairie pothole region in north central iowa the developed model has the capacity to predict our flood risk metric calibration r2 0 9 validation r2 0 7 for individual farmed prairie potholes across a variety of morphologic and management conditions and can be used iteratively to assess alternative actions keywords prairie potholes random forest machine learning decision support tools software and data availability program title annualized agricultural non point source annagnps model annagnps v5 50 a 013 2018 08 21 was used for this study contact address ron bingner usda gov software access https www nrcs usda gov wps portal nrcs detailfull national water manage hydrology cid stelprdb1043535 software requirements annagnps is available for windows 7 8 and 10 and for windows xp 7 8 and 10 or on other platforms that have a compiler for ansi standard fortran 2015 a minimum of 2 gb of memory is recommended annagnps includes a memory manager and thus has no limits additional computer storage may be necessary for input and output data program language fortran availability and cost open source research data and model development repository ppmst model contact address kaleita iastate edu software access https github com bnahkala ppmst model software requirements r v3 6 2 or later program language r availability and cost open source 1 introduction complex watershed models are frequently used to support policy and land management decisions across the world and are considered robust decision support tools dst for scientific hydrological alternative assessment horn et al 2004 usepa 2001 however the resource intense models with high learning curves are generally unavailable to key stakeholders that are the primary decision makers even often including technical staff ranjan et al 2020 these models frequently become outdated or are left to collect dust as funding or primary users fizzle out bridging the gap between these challenging models and resilient and accessible decision support tools is thus a worthy endeavor empirical tools are an alternative to complex deterministic or physically based models in particular machine learning methods have been increasingly utilized in hydrologic analysis and decision making to reduce the computational or data requirements as a replacement for data intense hydrologic modeling rahman et al 2020 rasoili et al 2012 lange and sippell 2020 random forest machine learning contains an ensemble of categorization and regression trees cart that are grown independently and randomly lending a stronger validity and known uncertainty than traditional cart models random forest has emerged as a common machine learning algorithm within hydrologic studies wang et al 2015 singh et al 2017 naghibi et al 2017 as with other machine learning models random forest models can provide a less data intensive input parameter set than distributed watershed scale hydrologic models complex and local decisions are the daily considerations of landowners and renters in the agricultural landscape of the des moines lobe dml of iowa which is replete with farmed prairie potholes farmed prairie potholes are small isolated depressions frequently classified as semi permanent wetlands that make up a significant portion of land area in intensively farmed dml which is part of the larger prairie pothole region ppr flooding greater than 2 days during any portion of the growing season april september almost certainly leads to crop loss in these closed depressions naturally disconnected from any significant surface drainage network prairie potholes are only moderately connected to the surrounding geography via shallow subsurface and groundwater interactions and intermittently during overflow after large precipitation events roth and capel 2012 evenson et al 2016 alternative management options may be of interest to landowners who consistently lose crops and profits to these areas the near annual flooding despite artificial drainage within many prairie potholes results in a net economic loss from flooded regions of the field fey et al 2016 potholes are of interest hydrologically for their ambiguous connection to the surrounding landscape with implications affecting their status as a significant nexus in waters of the us golden et al 2016 jones et al 2019 studies further emphasize the ability of prairie potholes to filter nutrients schilling et al 2019 badiou et al 2018 and provide habitat and biodiversity janke et al 2017 in their natural state however those ecosystem benefits can be significantly reduced or eliminated under farmed conditions martin et al 2019b schilling and dinsmore 2018 badiou et al 2018 farmed potholes can be managed for profit or conservation more deliberately than current prevent plant support programs and conservation guidance generally encourages which largely focus on the intent of a single landowner or specific portion of the field ameli et al 2019 bauer et al 2017 however dsts are needed that bridge the gap between highly technical watershed models and farmers making decisions on a daily basis in order to bring perspective to flood expectations and conservation opportunities on larger scales farmed prairie potholes in the dml have been modeled to assess how field management can affect flood dynamics upadhyay et al 2018 2019 these studies utilize the usda annualized agricultural nonpoint source annagnps model a lumped continuous simulation surface runoff model bingner et al 2018 this model has successfully characterized changes in land use and drainage within potholes allowing for comparative assessments of viable management decisions these scenarios represent changes in the physical nature of pothole watersheds but these models do not provide accessible data in a decision making framework because they are specific to individual potholes and require relatively extensive parameterization in order to use effectively importantly and ignored by watershed models one of the biggest factors identified in agricultural decision making is managing risk specifically with regards to environmental risk wauters and mathijs 2012 this in part can be attributed to the volatility of agricultural markets and a frequent inability to return a profit in row crop agriculture this can involve the biophysical risk of farming certain crops or can involve the financial and economic implications of farming and conservation alternatives risk in the context of farmed prairie potholes involves the ability for a farmer to make a profit from the pothole land area the perceived risk can be explained by the frequency and extent of pothole flooding or flood risk which follows common frameworks for flood risk assessment merz and thieken 2009 koks et al 2015 this goal of this study was to develop and contextualize a flood risk identification method and tool from the lens of a farmer for prairie pothole management that is simple quantitative and utilitarian we assess the use of a machine learning model random forest for predicting flood risk of prairie potholes due to land use changes the model utilizes output from the hydrologic model annagnps integrates a method of risk interpretation in prairie pothole flooding and trained to predict flood risk based on a reduced easily attainable set of input parameters 2 methods 2 1 study potholes six prairie potholes named bunny walnut lettuce cardinal hen and mouth have been extensively monitored and modeled martin et al 2019a upadhyay et al 2018 2019 and are utilized for this study the 5 of these potholes reside in a 1 square mile section southeast of ames ia in the dml of the ppr mouth lies approximately 3 miles northeast of this site on a separate research farm all potholes but mouth have been conventionally managed through the study period monitored and modeling framework described in previous studies the western half of mouth has been enrolled in the conservation reserve program and its watershed is the most diverse with portions planted to plot scale studies of conventional and perennial biomass crops characteristics of each pothole are outlined in table 1 including a snapshot of their watershed and maximum flood extents morphology column maximum flood capacity overflow depth maximum concentrated flow path and maximum watershed relief these values were determined from gis analysis of the digital elevation model and flow path was sourced from the annagnps model interpretation of flow paths described in the following sections 2 2 annagnps simulations individual watershed models of the 6 potholes shown in table 1 were built using the annualized agricultural non point source pollutant loading model annagnps and were previously calibrated to monitored data as described in nahkala 2021a alternative management scenarios that assess drainage tillage and land management practices were assessed further nahkala et al 2021b and a synthesis of that output is provided in section 3 4 of greatest interest in this study were common practices applied by key decision makers in the dml drainage investments and land retirement are prevalent but to varying formalities and spatial extents our modeled scenarios reflect varying investment in drainage infrastructure and varying levels of conservation or lazy land retirement additionally modeled tillage practices reflect conservation and no till options that are increasingly adopted to reduce input costs or to provide environmental benefits busari et al 2015 a full matrix of these options was modeled to allow for direct comparison between factor levels models were run for a 25 year simulation period from 1994 to 2018 to capture variation in climate parameters rainfall depths were acquired from the parameter elevation regressions on independent slopes model prism and were merged with weather station parameters wind velocity wind direction maximum temperature minimum temperature dew point temperature solar radiation from a nearby station as part of the sustaining the earth s watersheds agricultural research data system stewards network naming of the scenarios is explicitly described in nahkala et al 2021b table 1 we used 4 levels for drainage none low medium high 4 levels for tillage field land use conventional conservation no till and retired and two levels for pothole land use farmed retired we coded each scenario based on these parameters following the convention dx lu tl where dx is the drainage represented by d0 d3 lu is the pothole land use represented by cs corn soybean rotation or r retired and tl is the tillage or field land use represented by cv conventional ct conservation nt no till and r retired the simulations provided 168 25 year simulations with 28 being assigned to each unique pothole but representing the same range of climatic and management conditions we summarized raw water level output of the pothole in terms of ponded water volume 2 3 flood risk quantification risk quantification can be conceptualized by the multiplicative interaction between the frequency of a damaging event occurring and the spatial or temporal extent at which that event occurs gloser et al 2015 we created three summary metrics from the annagnps simulations that reflect both frequency and spatial extent of pothole flooding fig 1 shows the conceptual data manipulation described in this section the first summary metric f averages the number of days in each month as a percent in which there is standing water in the pothole across all years of simulation an example is shown for may via equation 1 eq 1 f m a y i 1 25 f i 25 where f is the fraction of the month that is flooded and i is the year of simulation the second metric v is an annual average of the maximum volume of water that ponded in each month computed as a percent of the total storage volume of the pothole before overflow this was calculated using the same averaging as shown for percent of the month flooded risk parameters for these metrics were computed monthly to enable the incorporation of weighted risk ratings for each month reflecting an anthropogenic and biophysical concern for flooding during primary growing season months the scenarios weighted risk factor for each variable was computed by equation 2 eq 2a f s c e n a r i o j 1 12 f j w j eq 2b ϑ s c e n a r i o j 1 12 v j w j where f scenario is the scenario s overall risk factor for duration of ponding ϑ scenario is the scenario s overall risk factor for volume of ponding j is the month in the simulation and w is the weight assigned to each month finally the third metric averages the percent of annual ponding events that occur for varying durations measured in days which acknowledges that longer periods of inundation are more conducive of crop loss than short term events muth and bryden 2012 events were binned based on the number of days of consecutive flooding up to 10 days where all values greater than or equal to 10 were binned a weighting scheme was applied to these data where the weight increased linearly from 0 to 4 days and then remained constant this reflects that events of 1 4 days are increasingly risky to crop survival but that any flooding longer than 4 days results in a high probability of crop death rhine et al 2010 zaidi et al 2004 this follows the same calculation structure presented for the first two metrics as shown in equation 3 eq 3 ρ s c e n a r i o k 1 10 p k w k where ρ s c e n a r i o is the event based risk metric p k is the percent of flood events of a certain duration w k is the weight applied to those events and k increments the specific event bin the three parameters were scaled from 0 to 1 multiplied and rescaled to a maximum risk of 10 this completes the conceptualization of risk as a multiplicative interaction of spatial and temporal variables as shown in equation 4 eq 4 r i s k f ϑ ρ these values represent the risk rating for a single pothole under a set of management conditions to be used as the predictive output of a machine learning model described in section 3 4 the random forest model can predict numerical values regression and categorical labels classification for assessment of the model s classification option categories were created from the scaled ranking system described above jenks natural breaks were used to determine potential categorical risk bins jenks 1967 we assessed both a 3 category and 6 category classification system which approximate a high medium low risk classification scheme 2 4 training data the annagnps output from the 168 simulations described in section 3 2 were processed using the risk quantification method described in section 3 3 this dataset of 168 samples including the assigned risk calculated from section 3 3 and associated random forest input variables section 3 5 provides the training data for the random forest model described in section 3 5 fig 2 summarizes the distribution of training data averages for the percent of each month that has standing water on a daily basis each point represents the simulation average for each month may and august had the highest average temporal inundation with over 30 of the days each month seeing standing water in some of the potholes on average across a simulation scenarios with less drainage had higher averages while the converse was also true lettuce hen and cardinal generally flooded the longest with low drainage scenarios notably in all scenarios mouth remained less than 10 flooded by month across the 25 year average however mouth frequently reaches a significant portion of its volume seen in fig 2 showing that it s flood dynamics tend to be flashier despite the lack of artificial drainage this could be due to increased soil health from land retirement in general most scenarios only averaged 1 3 days of flooding by month not reflected in the averages are months that were flooded for significant portions in the natural drainage scenarios these results also show that in the growing season months some flooding is guaranteed across a range of climate conditions as there were no nonzero values in may june august and september fig 3 depicts the distribution of the 28 scenarios with regards to the 25 year average maximum flooded volume by month each data point represents one scenario s 25 year average of the maximum flood water volume compared to the maximum volume mouth is the only pothole that frequently approaches its maximum volume in the early growing season however the inclusion of large extreme values the lettuce and cardinal data suggest more of their surface area may be more susceptible in higher precipitation years compared to other potholes including all potholes and scenarios in fig 3 average iqr for the growing season months is 14 24 21 32 11 22 6 13 22 30 and 15 25 apr sept of the true maximum volume these data represent the middle 50 of monthly average maximum surface areas in all scenarios for all potholes interpreted this could be seen as the middle ground of flooding expected for moderate to minimally drained potholes in an average year 2 5 random forest machine learning 2 5 1 background random forest rf is a machine learning algorithm that generates an ensemble of classification trees from a subsampled matrix of ranked or classified data that includes labeled predictor variables breiman 2001 the algorithm randomly samples predictor variables to test at each tree node and finds the minimal residual sum of squares rss for regression or gini categorization to create a split in the data representing a branch in the tree samples are returned to the training set to be available for reuse known as bagging trees grow any number of splits necessary to separate data into either predicted values regression or identified categories once the model is trained a sample is processed by each tree within the forest and the average regression or majority categorization vote is used as the model prediction value for that sample the model is validated by testing out of bag samples from the remainder of the dataset random forest has been using frequently for hydrologic applications and decision making frameworks because of its interpretability chu et al 2020 gibert et al 2018 koks et al 2015 sahoo et al 2017 wang et al 2015 the algorithm creates a distinct understandable binary structure similar to commonly used flow charts this gives it an advantage in agricultural decision making over other machine learning methods such as deep neural networks or support vector machine which operate as black boxes and are more reliant on high level statistical methods adoption of online decision tools remains a challenge in agricultural settings ranjan et al 2020 and the identification of a tool with an interpretable structure and explainable algorithm is perceived as highly beneficial liu et al 2008 rose et al 2016 2 5 2 random forest model we used the open source software r r core team 2020 to implement the random forest model via the package randomforest liaw and wiener 2018 training samples represent a single simulation with the predicted output being the assigned flood risk from section 3 3 both regression and categorical models were evaluated based on their r2 and error rates respectively that is the risk eq 4 was treated as a continuous variable in the regression model and binned into categories in the categorical model a total of 8 predictor variables were used to train the rf model table 2 this includes 4 qualitative variables including drainage d tillage t land use land cover of the pothole lulc p and lulc of the field lulc f additionally 3 quantitative variables were identified using arcmap 10 4 including the maximum pothole depth md catchment area to pothole area capa ratio and maximum watershed relief wr highest lowest elevation maximum concentrated flow path mfp was calculated as the longest continuous set of reaches within the watershed model in annagnps three parameters in the random forest algorithm were assessed to optimize the model mtry ntree and maxnode these represent the number of input variables assessed for data splitting at the generation of each node mtry the total number of trees grown in the forest ntree and the maximum number of terminal nodes unique predictions grown by a tree maxnode we attempted to limit the terminal nodes in order to enforce simplicity on tree structure while maintaining predictive accuracy a desired goal for decision making tools the model was developed using bagging or sample with replacement to compensate for a small dataset 2 5 3 assessment of the forest and tool development categorical and regression models were calibrated and validated based on their error rate and r2 respectively seventy percent of the samples were used for training and 30 were reserved for cross validation all combinations of mtry constrained between 2 and 7 ntree constrained between 500 and 2100 at intervals of 200 and a set seed when subsampling the training set seed 1 to 100 were assessed the optimal set of input parameters were selected with uncertainty represented by the differences in statistical performance due to different training sets once a parameter set was selected terminal nodes in each tree were limited using a range from 5 to 50 and predictive performance was assessed the r package randomforestexplainer was used in addition to randomforest to assess the relative importance of input variables paluszynska et al 2019 liaw and wiener 2018 this package can extract the number of times a predictor variable is the root or first split in individual trees can calculate the mean depth of the first appearance of a variable in a forest can calculate the increase in mse caused by individual variables if removed among other variables after assessing input variable importance predictor variables were removed from the model and statistical performance was assessed the top three most important variables based on mse increase remained in all models while the removal of other variables was assessed further validation of the model was conducted using monte carlo mc and sensitivity analyses the monte carlo analysis was conducted by randomly sampling all input parameters including factors and numeric values provided as a discrete list within the range of training data samples were filtered to reflect reasonable land use decisions for example generated scenarios that contained a farmed pothole but retired field were removed sensitivity of the model to the 4 quantitative input variables was assessed by calculating the change in risk level divided by the change in parameter value compared to a baseline holding all other input parameters constant sensitivity was calculated across the range of values observed in the training set plus or minus 10 finally representative trees were extracted from the forest following the methods of banerjee et al 2012 which are available in the github package reprtree the method for identifying representative trees assesses both the architecture of the tree and the terminal node predictions the package implements one of three distance evaluation metrics used in the study for any two trees distance metrics are similarity in the input parameters used to split the trees similarity in clustering of outputs in the terminal nodes of the trees and similarity in the predictions of the trees the tree with the minimum distance to the all the other trees in the ensemble is considered representative trees were assessed to validate the model and identify patterns in the data splitting as a supplement to the randomforestexplainer package the versatility of employing a single tree from the random forest model was also assessed to identify opportunities to implement a simplified version of the model under potential software limitations or educational contexts a single tree provides versatility by having a singular structure that can be easily visualized or coded into simple decision making tools yet still provides realistic predictions 3 results and discussion 3 1 scenario risk levels the range of flood risk values derived from the ranking system ranged from approximately 0 1 to 10 the average and median risk levels are 2 1 and 1 1 respectively in thinking about the multiplicative effects of risk in our model which multiplies spatial extent by temporal frequency our risk ratings capture variation in risk with up to two orders of magnitude difference on this scale however this scale may not represent the full range of flooding observed in prairie potholes the potholes in iowa have been significantly drained and are considered semi permanent however potholes in other regions of the ppr can experience infrequent to permanent flooding for our purposes we are concerned with farmable potholes for which a working definition may be pinned by qualifications for prevent plant and crop insurance for prevent plant insurance a pothole would have to produce crop 1 in 4 years more frequent flooding while most prevent plant payments in iowa correlate to crop flood losses in 1 out of 4 years less flooding in this context risk in this model may show the distribution of pothole conditions that are farmable but may only produce crop 25 75 of the years 3 2 random forest performance and sensitivity 3 2 1 categorical vs regression model and model parameters categorical random forest models were tested using 3 classifications and 6 classifications that is whether risk is assessed using three levels or six levels models with less bins resulted in smaller error rates which correlates to an increased prediction capacity that is falsely elevated by weak bin resolution even using naturally occurring breaks in the data however the out of bag error rates for categorical models were poor table 3 show variations on oob error and gives tables for average oob error rate based on the random forest mtry average performance based on the number of trees did not vary the average oob error for the 3 category model was 13 with minor sensitivity to mtry table 3 the oob error for the 6 category model was 33 with some sensitivity to mtry the regression model performed better than the categorical models with average r2 values of 0 88 across all calibrated models validation frequently performed slightly higher with an average r2 of 0 91 fig 4 displays performance metrics error and r2 for calibration and validation variation was induced by setting the seed before taking a random sample of data to train the model resulting in 100 data points per boxplot we conclude the random forest model is more useable and interpretable when trained as a regression model than as a categorical model due to the strong performance with respect to r2 the categorical model s performance was visually deemed more contingent on the number of bins suggesting the prediction itself is less reliable than a numerical regression additionally an average classification error of 13 33 percent can be viewed as extremely poor in the context of decision making frameworks mowrer 2000 the regression model was chosen over classification for further application in this study a manual review of individual regression models was conducted and the highest performing parameter set was selected for analysis based on r2 we chose model parameters of mtry 6 and ntree 500 preferentially we found a seed that produced the highest r2 during calibration seed 22 the r2 for calibration and validation of these model inputs were 0 95 and 0 78 respectively while the mean squared error is 0 55 trees in random forest training naturally grew more than 20 terminal nodes in all cases from 5 terminal nodes to 50 the model s predictive capacity remained strong and acceptable with respect to r2 we observed that below 15 terminal nodes the explained variation significantly decreased with the reduction in terminal nodes r2 remained nearly constant when terminal nodes were expanded to 25 or greater between the 25 node and 15 node limitation a decrease in the amount of variance explained was 2 5 percent we selected 15 as the maximum number of terminal nodes for future use of the model to balance both usability reflected in the size of a decision tree if a person were to use one manually and predictive strength r2 3 2 2 model input variable analysis the physical location of input variables on regression tree nodes lends insight into the influence of each variable on model performance we first plotted the minimum depth or first occurrence of an input variable in each tree the distribution of this minimum across all 500 trees is shown in fig 5 the graph reveals that the drainage and tillage inputs were most frequently used as the first split in the forest trees with drainage being used the majority of the time these inputs thus are factors with the largest difference in risk observed between differences in factor levels as determined by the minimization algorithm that the model employs during calibration the catchment area to pothole area ratio and maximum watershed relief were used at a similar frequency to tillage the land cover of the field maximum flow path maximum pothole depth and pothole land cover were used less frequently within the trees and only to create higher resolution splits in the flood risk prediction the gray portion of each bar represents the number of trees in which a variable was not used most notably pothole land cover was not used in over half of the decision trees in the model suggesting it is either not important or is correlated to another input variable in this case the tillage level na was used instead of no till to represent differences in true no till conditions and the lack of tillage due to perennial vegetation this may suggest the removal of lulc of the field is appropriate because higher resolution can be provided by the tillage parameter the percent increase in model mse when a variable is removed is given in fig 6 and further shows that drainage is the most influential categorical input however maximum watershed relief and capa ratio incur the second and third largest increase in mse respectively followed by maximum flow path and maximum depth these do no perfectly align with the minimum node depth in fig 6 because while fig 6 shows the minimum split each variable may be used multiple times within a single tree while a variable like tillage can occur frequently within the upper reaches of a decision tree it does not guarantee a finer resolution of the data at the terminal nodes considering mse increase lulc of the pothole lulc of the field and tillage appear extraneous however this must be considered in tandem with other metrics like minimum node depth which reveal the importance of tillage we further tested the model by removing individual or groups of the five least important predictive variables based on mse increase fig 6 the results of which are shown in table 4 removing any one variable other than drainage did not significantly affect the r2 performance of the random forest model removing any two or any three of the least significant variables only moderately affected the model removing the lulc parameters marginally increased the performance of the model suggesting these are strong candidates for removal from the model inputs or suggesting that the model is overfitted these models were run with the prescribed maximum 15 terminal nodes the trees in the forest continued to grow 15 terminal nodes despite reductions in the number of input variables the tree simply grew more branches with nodes that recycled the same variables finding a higher resolution of divisions within each predictor however the flexibility and interpretation of the forest remains questionable with reduced inputs due to this limited dataset we decided on a model that utilized all variables to provide more opportunities for flexibility when more modeling data is generated 3 2 3 validation of the model the predicted flood risk from the monte carlo simulation are shown in fig 7 most of the synthetic data were predicted to have a risk less than 2 consistent with the frequency of data that should have had medium and high drainage the largest concern after training the model was the ability to predict values at higher risk levels due to the small sample size in the training dataset this was both due to the multiplicative nature of the risk ranking system creating larger risk differences for high values and the limits of the annagnps simulations we pursued however we see that there is a relatively balanced spread in the middle of the predictive range 2 6 currently the model as low resolution above a risk level of 6 because the training data includes a limited number of samples in which the model predicts high risk however the model still predicts nearly the full scale of risk values within the interpolation range of each input parameter panel b of fig 7 is most informative in describing the natural risk range of potholes the largest range of risk predicted occurs with no drainage which illustrates that some potholes are inherently riskier than others the reduced spread of data in the medium and high drainage scenarios shows that there are decreasing returns in drainage investment and that some level of flood risk is ultimately guaranteed based on current drainage practices these observations lend to the idea that potholes can be preferentially identified for management or conservation activities based on this risk scale the input parameters for this model that directly characterize specific potholes and thus those that are most helpful in comparing multiple potholes directly are the 4 quantitative variables capa ratio maximum flow path maximum depth maximum watershed relief 3 2 4 review of representative trees and their structure the structure of 1 of the top 10 representative trees in our model is shown in fig 8 here we can see the risk prediction at terminal nodes most trees had a higher density of a low risk predictions reflecting the distribution of the training dataset this results in reduced resolution when assigning risk to medium and high risk potholes it might be best to consider these higher risk levels with a band of uncertainty in the context of this model the mse being 0 55 additionally the model takes the average prediction of all trees and because there are less training samples in the higher risk ranges the average tends to underestimate some high risk conditions visual examination of the structure of a tree can provide insights into influential parameters parameter level relationships and by extension the behavior of the system which can be as powerful as interpreting the numerical prediction for example many of the terminal nodes group both high and medium drainage together without a node split suggesting the risk is similar between those two levels not only does this enable numerical predictions it has direct consequences on management recommendations while the model does not recommend action it might suggest that an investment in increased drainage does not reduce risk for many potholes the reason for using a random forest over a simpler cart model is to add robustness to a prediction it is counterintuitive to reduce a more robust tool to a simple classification and regression tree that generally has less predictive power however in the context of future applications as a decision tool users need an easy to use method that has versatility or can be easily explained the representative tree approach as explained by banerjee et al 2012 allows for easy decision making a single tree while maintaining results that are similar to those from the entire forest the r2 from the representative trees often equal or exceed that of the ensemble and thus can still be utilitarian for predictive and demonstrative purposes 3 2 5 interpreting output context and limitations of the model the data in these modeling methods reflect the intensity of flooding in farmed prairie pothole systems it is not intended to be used directly for flood mapping regulatory permitting or other professional testimonials this model is meant to be used iteratively and comparatively to help landowners assess the relative effectiveness of different actions however the larger context of decision making involves socioeconomic financial and personal values interpreting the output of this tool as presented therefor is neglecting to examine the context of its use additions to this model would include an economic assessment of key land management changes representing the cost associated with changes in field management flood risk data can be utilized to show options relative to a baseline by applying theoretical scenarios to the model this provides a basic level of information without prescribing a solution within a framework that cannot capture broader personal and societal context model interpretation is improved upon by evaluating both the output and the input equally once the model has been employed for example retiring the entire field or pothole microwatershed results in significantly lower predicted risk at face value all fields with potholes should be retired to crp however this ignores the need society has for agricultural production among many other reasons conversely many farmers add drainage to their potholes periodically evaluating a change from subsurface drainage to subsurface drainage with inlets shows that risk decreases however this risk may only be incremental and the economic burden for minimal gain is not reflected in the model this is where the model allows for flexible personal integration of values perceptions of risk and broader contextual goals of the user finally this model is limited by the small number of potholes and thus the small number of combinations of input variables in our original dataset and the resulting imbalance in outcomes with more low risk scenarios in our training dataset than high risk scenarios 4 conclusions a method has been devised to aggregate time series watershed modeling output of individual prairie potholes into a singular flood risk ranking this ranking represents the flood risk of a pothole on a relative scale based on the range of observed hydrologic conditions attained from 28 modeling scenarios and 6 potholes this risk is placed on an arbitrary scale and the risk does not represent an explicit parameter such as return period of a storm the prediction reflects a ranked value based on the anthropogenic lens with which potholes incur risk to farmers this incorporates both the extent of flooding and duration of flooding and considers how these apply to the biophysical risk to row crop agriculture though it does not include parameters that assess crop survival explicitly a machine learning method has been employed that predicts the flood risk of individual potholes based on the ranking system devised in this study the random forest algorithm can be trained to predict this risk in iowa based on easily identifiable management and morphological characteristics this method reduces the need for data intense spatially and temporally explicit watershed models that are time and resource dependent this alternative helps land managers assess the relative effectiveness of different land use practices on their field without having to have access to a fully developed process based model this method minimizes the need for highly technical knowledge of distributed integrated watershed modeling software in preliminary decision making contexts landowners may efficiently use knowledge offhand about their field and do a cursory review of quantitative parameters using a web based gis platform to inform this flood risk model however the risk level output by our empirical model does not fully consider the anthropogenic lens not all scenarios with high flood risk represent economic or agricultural risk and so the analysis must be carefully considered the flood risk presented in the model must be coupled with an analysis of cropping systems to assess potential losses or improvements incurred representative trees can be extracted from the forest and provide both predictive strength and educational opportunities reducing the forest to a classification and regression tree enables simplistic implementation of a tool for landowners if desired compared to the technical implementation of the entire random forest model the random forest r2 can be interpreted as the average across all trees in the forest suggesting that some individuals within the forest predict risk more accurately the representative trees presented here performed better than the ensemble with respect to r2 in the training period but performed poorly during the validation period the limitation of individual trees in practical application is that the number of terminal nodes was reduced to 15 within the model this limits any assessment of new unique output to 15 possible levels of classification many of which are clustered in the low risk spectrum this limits the resolution of classifying medium to high risk potholes using singular decision trees the model currently is limited by the training data despite remaining a valuable tool any physical misrepresentations in watershed models would propagate through this model suggesting the need for continued validation of hydrologic parameters in watershed modeling via field in situ field trials the sensitivity to inherent pothole characteristics shows that a more robust training set of potholes n 20 would be desired but the full assessment of the model reveals its educational capacity and ability to be easily developed further we have shown that the model retains the capacity to predict nearly the entire range of risk defined in our ranking system and its only limitation results from a lack of simulations in the high risk range future work should incorporate specific anthropogenic risks in modeling such as empirically predicting crop loss risk or economic risk taking the flood risk prediction one step further a larger set of pothole data across the des moines lobe is necessary to continue validating the random forest model and reduce any expected extrapolation required by the current model funding sources this work was supported by the us environmental protection agency assistance agreement cd97753901 it has not been subjected to the agency s product and administrative review and therefore might not necessarily reflect the views of the agency no official endorsement should be inferred declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors thank vitor martins for providing guidance in the construction of the random forest model to andrew vanloocke for providing additional insight on the stakeholder perspectives of agricultural models and to the anonymous reviewers who helped improve the quality of this paper author contributions b n a k and m s designed the research b n performed the research b n analyzed the data and b n wrote the paper a k and m s edited the paper 
25691,a framework that integrates bayesian optimization bo and high performance computing was developed to automate calibration of complex hydrological models it adopts a loosely coupled web architecture integrating tornado and springboot to facilitate bidirectional transfer of variables between bo and model evaluation extensive model evaluations were implemented on a hadoop cluster to wrap the model into the calculation flexibly and separate the calculation process from the algorithm execution effectively a case study calibrating a swat model in the meichuan basin jiangxi province china indicated that the framework provides an ideal environment for assessment of the capability of bo to quantify the efficient estimation of swat parameters compared with that of the built in swat cup tool the number of executions was reduced from 1500 to 150 while maintaining similar accuracy the framework also allows evaluation of the performance of different surrogate models and acquisition functions and provides instant visualization for searching for optimal parameters keywords bayesian optimization calibration sensitivity analysis swat 1 introduction hydrological systems exhibit considerable spatial heterogeneity because of the different physical and chemical processes occurring within such systems and this spatial variability requires careful modeling the advent of distributed hydrological modeling has made it possible to capture such spatial variations when simulating hydrological systems consequently in the field of water resources designers researchers and decision makers rely increasingly on such models to help elucidate the natural processes and investigate the effects of anthropogenic activities on watershed systems howarth et al 1996 wu and xu 2006 many hydrological models contain parameters that cannot be determined directly from field measurements and thus are poorly constrained such parameters must be estimated using indirect methods such as calibration model calibration refers to the adjustment of such model parameters within recommended ranges to optimize the agreement between observed data and simulated results tolson and shoemaker 2007 because calibrated values are affected by correlations between parameters and statistical features of model residuals this may lead to equifinality duan et al 1992 for large complex models such as the soil and water assessment tool swat uncertainty is a major concern ng et al 2010 zhang et al 2009 uncertainty may be associated with input data model structure and parameters each of which represents a considerable challenge in model calibration and may restrict the successful application of a hydrological model to improve model accuracy and reduce uncertainty many calibration methods have been developed to estimate hydrological parameters by matching modeled results with corresponding field measurements traditionally hydrologists adjust the values of model parameters using trial and error methods such that the model closely matches the behavior of the real system that it is intended to represent in its most elementary form this calibration is performed by manual adjustment of the parameters and visual inspection of the agreement between observations and model predictions janssen 1995 however distributed parameter hydrological models partition watersheds into spatially discrete computational units and solve model equations for each unit separately fitzhugh and mackay 2000 therefore when the number of spatial units increases so does the number of model parameters to be calibrated the large number of parameters involved in distributed hydrological models makes manual calibration a less feasible option zou et al 2004 this is because the conceptual relationship between the parameters of the model and the watershed characteristics becomes increasingly difficult to interpret visually a large number of interacting parameters can result in unpredictable effects when multiple parameters are adjusted gupta et al 1999 furthermore manual calibration is a tedious and daunting task that requires a trained hydrologist with comprehensive knowledge of the watershed in contrast automatic calibration is less subjective and it is capable of extensive searching of sets of model parameters within their acceptable ranges in a very short time increasing the likelihood of finding optimal parameters bekele et al 2007 because of the subjective and time consuming nature of manual trial and error calibration considerable research effort has been devoted to the development of methods for automatic calibration of hydrologic models gupta and sorooshian 1994 automatic methods seek to take advantage of the speed and power of digital computers while being objective and reasonably easy to implement zou et al 2014 wellen et al 2015 chaudhary et al 2017 yang et al 2018 research into the development of automatic calibration methods has become increasingly popular driven by the need for efficient and better calibration research has focused on the development of automatic parameter optimization algorithms to identify parameters duan et al 1992 sorooshian et al 1993 wang 1991 lee et al 2020 however many of these algorithms attempt to find an individual optimal parameter set ps which is inconsistent with the fact that multiple similar or equivalent solutions may exist as a consequence of uncertainties in model structure input data and parameters geza and mccray 2008 shen et al 2010 hence many uncertainty analysis methods and parameter estimation algorithms such as generalized likelihood uncertainty estimation beven and binley 1992 stedinger et al 2008 genetic algorithms zou et al 2007 simulated annealing algorithms borgomeo et al 2015 particle swarm optimization afshar et al 2011 markov chain monte carlo mcmc methods vrugt et al 2003 haario et al 2006 liang et al 2016 the parasol optimization and uncertainty analysis tool van griensven and meixner 2006 and sequential uncertainty fitting abbaspour 2011 mousavi et al 2012 rouholahnejad et al 2012 have been developed to establish more robust models and many have shown promising results calibration entails a large number of model simulations which can be computationally intensive or even prohibitively expensive to perform if the model simulation is extremely complex while enabling better accounting for uncertainties stemming from various sources such methods and algorithms used for calibration and uncertainty analysis do not dramatically reduce execution time in some instances they can actually increase it such costs are further exacerbated by automated calibration which can require thousands of model evaluations to address this problem a highly efficient calibration method is needed that requires fewer model runs while maintaining global optimal performance one such method is the bayesian optimization bo algorithm the bo technique uses bayes theorem to direct the search for the maximum or minimum of an objective function mockus et al 1978 hutter et al 2019 because model calibration can be formulated as an optimization problem that seeks to minimize the weighted sum of the squared residuals between the field measured and model computed observations by adjusting parameters with uncertain values it could potentially benefit from this optimization technique the parameter problems that exist in traditional population based algorithms such as genetic algorithms and particle swarm optimization do not affect bo which is particularly useful when the calculation of the objective function is expensive hutter et al 2019 these features make bo an important technique in computer based science and bo implementation has been reported to have achieved state of the art performance in many domains for example image processing and speech recognition ma et al 2017 hutter et al 2019 parameter estimation in artificial intelligence shahriari et al 2015 cornejo bueno et al 2018 soil respiration modeling toda et al 2020 optimization for intelligent environmental monitoring marchant et al 2012 and optimization of pump operations in water distribution systems candelieri et al 2018 although bo has been used to reduce the computational burden of advanced analyses using simulation models across several disciplines the lack of its application within hydrological modeling has impeded the uptake of bo methods that can support computationally demanding analyses such as the application of complex model calibration noting that bo based parameter estimation has rarely been employed in the context of hydrological model calibration and recognizing the importance of parameter optimization in the swat model the objective of this study was to develop a bo based calibration framework to investigate the applicability of bo to the automatic calibration of the swat model the framework adopts a loosely coupled web architecture that integrates tornado and springboot to facilitate the bidirectional transfer of variables between bo and model evaluation extensive model evaluations were implemented on a hadoop cluster where complex models like swat were wrapped and executed flexibly within mapreduce further separating the model calculation process from the algorithm execution process specifically a case study of bo based calibration for a swat model was performed wherein the developed framework took advantage of the global convergence of bo within a limited number of iterations the key concept is the use of bo to construct a probabilistic surrogate of the swat model to fit the real objective function and then to evaluate the cheap to run surrogate model in the optimization process wherein the optimal potential ps is suggested to avoid unnecessary sampling and improve overall computational efficiency the main contributions of our work lie in the development of a comprehensive framework that integrates complex models bo and high performance computing hpc to wrap the model into the calculation flexibly and separate the calculation process and algorithm execution effectively in addition within the framework we assessed the capability of bo to quantify the efficient estimation of swat parameters and evaluated the performance of three surrogate models and three acquisition functions of the bo algorithm the remainder of this paper is organized as follows in section 2 the bo based model calibration framework is illustrated and its major components which include the bo algorithm physical swat model and parameter sensitivity analysis sa are described section 3 describes both the application of the proposed approach to calibration of the swat model in a case study and the evaluation of the performance of the three surrogate models and three acquisition functions of bo to guide future algorithm configuration sections 4 and 5 present the optimization results and a discussion of future research respectively finally our conclusions are presented in section 6 2 methods 2 1 framework for bayesian optimization bo based model calibration because the swat model is a fortran based hydrological model its capabilities in output analysis parameter identifiability model calibration and visualization need to be enhanced in this paper we present a bo based calibration framework written mainly in python and java our motivation is to take advantage of the full feature set of the three programming languages fortran s efficient scientific computing python s facilities for fast prototyping numerous libraries for data processing data analysis and data visualization and java s hpc while obviating the need for extra programming effort to allow them to interact the whole framework can be divided logically into the following three parts with respect to the programming languages used the python based tornado web framework is used to accommodate our core algorithms involving sa and bo the java based springboot framework serves as a transformer it is responsible for receiving pss from the tornado framework parsing pss to generate hadoop mapreduce jobs and submitting these jobs to a hadoop cluster for further resource scheduling and model execution the fortran based swat model was wrapped as a mapreduce task within the hadoop cluster for model evaluation multiple swat models can be invoked and executed simultaneously and pss and simulation results can be transferred bidirectionally via http requests between python and java our proposed framework was implemented using python code in a linux environment by coupling the swat model with the salib sa library herman et al 2017 and the scikit optimize bo library tornado was chosen to host both the salib and scikit optimize libraries where the sampled pss in the sa procedure and the recommended pss in the bo procedure are all sent to the java based springboot framework in the form of http requests springboot is responsible for receiving and transforming the variables noted above the sampled and suggested pss into model evaluations and then writing a driver program to run these model evaluations on the hadoop cluster the model evaluations numbering several hundred are distributed to a hadoop cluster comprising seven nodes each equipped with an intel 8 core i7 3770 processor and 32 gb ram and four nodes each equipped with an intel 32 core xeon r silver 421 processor and 96 gb ram the procedure for running swat model evaluations on a hadoop cluster is to call the fortran based swat model from the java based mapreduce framework because the fortran code of swat is compiled to an exe file in the linux environment this model can easily be executed in the mapreduce framework finally http requests are used to communicate between programs in the two languages python and java to transfer variables bidirectionally between them in contrast to a previous calibration framework which used a standalone r package fme for automating calibration sensitivity and uncertainty wu et al 2012 our framework adopts a web based architecture instead of using standalone architecture to coordinate the relationship between complex model and optimization algorithm which is highly modular and facilitates maintenance this is beneficial for taking advantage of the full feature set of different programming languages while obviating the need for extra effort to allow them to interact in addition our framework extends a prior study on moving swat model calibration to the hadoop based cloud zhang et al 2016 by reducing the whole hadoop based cloud to a pure computing system without performing any algorithm analysis our approach can enable complex models like swat to be wrapped and executed more flexibly within mapreduce moreover objective values can be returned easily from the model evaluation environment to the algorithm environment thus effectively separating the model calculation process from the algorithm execution process the framework flowchart and components are elaborated in the following subsections the methodology of the framework involves six steps the development of the swat model design of a suite of representative simulations parameter sa evaluation of the objective function construction of bo based surrogate models and minimization of the objective function using the surrogate models and optimizer the core tasks are explained below and illustrated in fig 1 i model setup first development of the swat model is completed the swat model requires explicit information regarding weather topography soil properties vegetation and land management practices to simulate processes such as surface and subsurface flow sediment transport nutrient cycling and crop growth here arcswat is used for the swat model setup because it has a user friendly interface and presents the results as intuitive and informative maps during the setup process swat is fed by multiple databases including digital elevation models dems streamlines soil data and land use data as shown in fig 1 m hydrogeological parameters are initially chosen for estimation by the proposed method ii representative simulations second representative simulations for parameter sa are designed the m parameters are sampled n times following specified probability density functions from the m dimensional parameter space constrained by their ranges using parameter sampling methods such as latin hypercube sampling mckay et al 1979 the parameter ranges are carefully determined according to available field data each sample drawn from the m dimensional space is an m vector and the n samples represent complete possible combinations of the m parameters as the inputs to the n simulations of the swat model n swat models are then constructed from these n input samples iii sensitivity analysis next sa and parameter screening are performed the sensitivities of the m parameters are evaluated using specified sensitivity indices such as morris s mean elementary effect and sobol s total order indices sobol 1993 which can be calculated efficiently from the response surface of the streamflow to the input parameters constructed from the n pairs of the training dataset fig 1 based on the sa j j m sensitive parameters are identified and included in an iterative calibration process while insensitive parameters m j are screened out of the original m vector inputs in this stage according to the sa it is worth noting that multiple sa methods can be used in parallel to reduce the subjectivity commonly introduced when using a single evaluation approach iv construction of bo in the next step the bo objective function is defined the streamflow outputs of the n representative swat simulations are used to calculate the objective function which in this study is the nash sutcliffe efficiency nse coefficient to be minimized between the simulated and measured streamflow histories the objective function nse is used as the response variable of the surrogate model to the j input variables having retrieved n pairs of j vector inputs and the corresponding objective functions the bo object is constructed the l pairs of k vector inputs and the corresponding objective function are used to instantiate a bo object using random exploration from the above n pairs of j vector inputs this can help by diversifying the exploration space as shown in fig 1 v bo based calibration finally the key bo based calibration loop is entered having initialized the bo object four steps i e evaluate suggest run update are performed in a loop as depicted in fig 1 by the dashed rectangular box first bo creates a new suggestion point which is a j vector input ps via the acquisition function the suggestion point represents a potential j vector input balanced between exploration and exploitation for locating the next potential optima second the suggested j vector inputs are fed into the swat model to evaluate the objective function having retrieved the objective function value the combination of the j vector inputs and the corresponding objective function value is treated as a new observation for updating the bo object in this manner the bo object is continuously improved which is conducive to recommending a more promising ps the loop continues until a specified criterion is reached such as a maximum number of iterations or an objective function value finally the j optimal parameters are determined the above evaluate suggest run update loop forms the core of the bo based calibration framework as the loop continues bo balances its needs for exploration and exploitation by taking into account what it knows about the target function at each step a surrogate model e g gaussian process gp is fitted to the known observations i e the points previously explored and the posterior distribution combined with an exploration strategy e g probability of improvement pi upper confidence bound ucb or expected improvement ei is used to determine the next suggestion that should be explored the final loop results in the optimal values of the j input parameters 2 2 swat model the swat hydrological model used in this study is a semi distributed continuous time watershed model capable of running on daily and subdaily time steps gassman et al 2007 it was originally developed to better understand the impact of management scenarios and non point source pollution on water supplies at a watershed scale arnold et al 1998 it has been used in various watershed studies including simulations of both water quantity and water quality lee et al 2010 liu et al 2013 the watershed is first divided into a number of subbasins dependent on outlets which are usually defined by subdivision of a dem and manual adjustment each subbasin is divided into a number of hydrological response units hrus with specific attributes by setting different thresholds for soil land use and slope classes the swat model uses the concept of the hru to represent variability within the subbasins of a watershed the hrus which are unique representations of land cover soil and management characteristics within a single subbasin are used for water balance calculations within the model the hrus are not spatially contiguous and therefore are often composed of many disjointed parcels of land within a watershed the objective of model calibration can be stated generally as the estimation of model parameters such that the model simulation and the hydrological behavior of the watershed match closely watershed responses that characterize the hydrological behavior of a watershed e g streamflow hydrographs sediments or nutrient loadings can be used as calibration objectives the complexity of the calibration process depends on model type because the number of parameters to be estimated varies accordingly lumped models which treat an entire watershed as a homogenous unit often have a small number of adjustable parameters that can be easily calibrated manually however the number of parameters in semidistributed models such as swat can often be two or three orders of magnitude higher than that in lumped models this is because the swat model takes the spatial heterogeneity of the watershed into account by dividing the watershed into different subbasins that belong within the same range of hydrological factors although a finer level of discretization often results in better simulation of watershed responses it demands greater computational time and increases the number of model parameters substantially that is each parameter value must be estimated through a calibration process 2 3 bayesian optimization bo bo is an efficient framework for the global optimization of expensive objective functions and it is especially useful for optimizing black box functions whose exact form is unknown in particular bo is used in machine learning applications for automating the model deployment pipeline recently bo has become popular among the machine learning and data mining communities as an efficient technique for tuning the hyperparameters of machine learning models auer 2002 the core of bo employs a probabilistic model such as gp extra trees et or random forests rf to capture the unknown function of these gp is often the preferred choice of probabilistic model the gp probabilistic model is specified by its mean and covariance the subsequent stage of bo uses surrogate utility functions known as acquisition functions these functions are constructed to balance the need to explore regions in which epistemic uncertainty regarding the objective function is high with the need to exploit regions in which the predicted values are high these acquisition functions can be optimized to extract the next candidate point for evaluating the objective function 2 3 1 bayesian optimization of model parameters parameter estimation of a complex watershed hydrological model can be regarded as a nonlinear optimization process which aims to find the optimal ps x o p that makes the unknown objective function f reach an extreme maximum or minimum value 1 x o p argmax f x x r d where x is the set of model parameters and r d is the d dimensional value space of the parameters the bo process refers to bayes theorem 2 p f d t p d t f p f where p f and p f d i are the prior and posterior probabilities of f respectively p f d i is the likelihood function d i is the observed set and the following recursive relationships are satisfied 3 d 1 x 1 f 1 d t d t 1 x 1 f t where f t is the objective function value corresponding to x t as mentioned above two key components should be considered when performing bo the probabilistic surrogate model and the acquisition function snoek et al 2012 shahriari et al 2015 in comparison with other similar procedures bo is unique in that it avoids unnecessary sampling by constructing a probabilistic model for f x and the optimal potential minimum loss function evaluation point is selected for the next assessment according to the acquisition function while integrating out uncertainty thus bo can theoretically guarantee final convergence on the premise of optimization of the acquisition function because the optimal potential points are selected for the evaluation during each step of the iteration process provided that sufficiently many iterations are performed the algorithm will eventually converge to the global optimal solution the following subsections describe the probabilistic surrogate model and acquisition function 2 3 2 probabilistic surrogate model the probabilistic surrogate model consists of a prior distribution and an observational model which increases the information and modifies the prior knowledge through continuous iteration according to bayes theorem shahriari et al 2015 therefore a prior function should be selected that presents assumptions related to the function being optimized for this we choose the gp prior which is the most efficient probabilistic surrogate model in bo because of its flexibility tractability analyzability and good substitutability for linear and nonlinear relationships jones 2001 rasmussen et al 2006 snoek et al 2012 other surrogate models such as et and fr are not elaborated for gp the argument x of the stochastic function f x is used as an index set for each input x and there is an associated stochastic variable f x which is the value of the random function f at that location therefore gp is the normalization of the multidimensional gaussian probability distribution which is characterized by a mean function m x and a covariance function k x x 4 g x g p m x k x x in practice it is very difficult to specify the prior information for the mean function of the gp therefore for the sake of simplicity it is usually assumed that the prior mean function is 0 which has little effect on the accuracy of the posterior distribution hence the prior mean value is set as 0 covariance specifies the smoothing line and amplitude of the unknown objective function and represents the similarity between the two calculation points which has a strong influence on whether the ideal prediction effect can be obtained the matérn covariance function cluster which is a type of highly flexible covariance function that can create a second order differentiable sample function can be expressed as follows 5 k m r 2 1 v γ v 2 v r l v k v 2 v r l where v is the smoothing parameter with default value 2 5 l is the scale parameter with default value 1 and k v is the second type of deformed bessel function 2 3 3 objective function similar to other optimization algorithms the objective function should be selected first in bo this is the statistic that is required to be maximized or minimized in the optimization process the nse nash et al 1970 is one of the most commonly used model evaluation indicators bennett et al 2013 bae 2018 and it represents the ratio of model fitting variance to total variance therefore the nse was selected as the model evaluation index in this study 6 n s e 1 i 1 n o i s i 2 i 1 n o i o i 2 where o i is the observed value o i is the average of the observed values and s i is the simulated value when the value of the nse coefficient is equal to 1 it represents a perfect fit of the simulated value to the observed value 2 3 4 acquisition function a simple strategy adopted in bo is the use of a surrogate utility function which is easy to evaluate the surrogate utility function is called the acquisition function the acquisition function is constructed according to the posterior probability distribution using the sampling function to determine the next point to be evaluated whereby the number of iterations and the evaluation cost can be reduced shahriari et al 2015 generally the selection of sampling points depends on exploration and exploitation ghahramani et al 2015 the former searches around the current optimal solution to find the global optimal solution whereas the latter tries to exploit the sample points that have not been evaluated to avoid developing a locally optimal solution the acquisition function can be based on one of four strategies promotion based confidence bound information based and combination strategies shahriar et al 2015 when gps are used as probabilistic surrogate models the commonly used collection functions include pi kushner 1963 ei and ucb snoek et al 2012 the pi approach quantifies the probability that each point may improve the current optimal objective function value and selects the point with the highest probability of increasing the current target value as the new evaluation 7 α p i x t d t p f x t ω φ m x t ω σ t where φ refers to the cumulative distribution function of the standard normal distribution ω is the threshold value with default value 0 01 of the evaluation promotion i e if f exceeds this value it is considered that the optimization effect is improved and σ t is the variance of f the pi approach selects the evaluation point with the largest promotion probability but pi also regards the promotion of all points as equal which means that it can reflect only the probability of promotion not its magnitude in contrast the ei approach incorporates both the promotion probability and the promotion amount 8 α e i x t d t m x ω φ m x t ω x t σ t φ m x t ω x t a 2 b 2 where φ is the probability density function of the standard normal distribution the upper bound of the confidence interval of each point is compared directly in ucb which can be regarded as the linear weighting of the mean exploration and variance exploitation 9 u c b m x k σ in eq 9 k is the weighting factor used to weight the exploration and exploitation of detection whereby the greater the value of k the greater the weight of the detection breadth in this study the above three functions were selected for optimization when ucb was selected as the acquisition function the value of k was set as 2 5 however to solve for the minimum value of the objective function the lower confidence bound lcb strategy was used instead 10 l c b m x k σ 2 4 sensitivity analysis library to discern the parameters that have the greatest influence on model performance and to identify the most appropriate parameter values it is necessary to screen out sensitive parameters and quantitatively evaluate the influence of each parameter on model performance for this specific purpose many previous related studies have used sa van griensven et al 2006 borgonovo et al 2012 because it can identify parameters for which reduction in the uncertainty specification has the greatest impact on improving model performance thus if some noninfluential parameters can be identified and reasonably fixed at given values within their ranges the computational costs may decrease without a reduction in model performance for this purpose salib was used in this study to conduct the sa for the swat model salib is a python implementation of commonly used sa methods including sobol fast the delta moment independent measure and the derivative based global sensitivity measure dgsm salib is useful in simulation optimization and system modeling for calculating the influence of model inputs or exogenous factors on outputs of interest herman and usher 2017 salib provides a range of global sa techniques that can be easily implemented in typical modeling workflows the library facilitates the generation of samples associated with model inputs and provides functions to analyze and visualize model outputs 3 case study 3 1 study area the selected study area was the meichuan river basin 26 17ʹ34ʺn 27 08ʹ54ʺn 115 44ʹ28ʺe 116 16ʹ06ʺe which is one of the subbasins of the poyang lake basin with a catchment area of 3304 km2 fig 2 the basin is dominated by low medium elevation terrain elevation 170 1440 m average 379 1 m the source of the meichuan river is wangpizhang at the northern edge of the village of xiaotian in ningdu county jiangxi province china and the river runs through the entire basin the regional climate is mild and humid featuring a distinct monsoon and abundant rainfall the annual average temperature is 17 3 c with an extreme maximum minimum temperature of 37 9 c 6 2 c the annual precipitation is in the range of 1500 1700 mm of which more than 55 occurs in the rainy season may august the annual average relative humidity is 80 the soil in the meichuan river basin is mostly deep and fertile red soil and yellow red soil developed from metamorphic rocks and granites comprising loam and sandy loam soil types the primary land use types are forest and grassland which account for more than 60 of the total area the cropping system is biannual with rice as the dominant crop 3 2 input data a variety of geospatial and temporal data are used in swat simulations the basic data required by the swat model include a dem a river network land use and land cover soil type and its physicochemical properties plant and plant growth fertilizer hydrological and water quality meteorology livestock husbandry and pollution emissions data neitsch et al 2004 among these hydrological and water quality data are used mainly for modeling determination of model parameters and calibration in the case study the required spatial data included topographic data soil data and land use data temperature precipitation humidity wind speed and solar radiation data from eight meteorological stations were obtained from the china meteorological assimilation driving datasets for the swat model cmads meng et al 2018 records of monthly streamflow and sediment observed at the watershed outlets i e ningdu fenkeng and shicheng were acquired from chinese hydrological data yearbooks from 2008 to 2018 details of the data used in the present study are listed in table 1 3 3 model parameters the selection and range of model parameters were determined through a review of literature related to swat model calibration arabi et al 2008 overall the 27 parameters recommended in the literature ghaith and li 2020 were selected the names upper and lower limits and descriptions of the parameters are listed in appendix a the parameters were classified into three groups the first group describes channel properties which can be measured directly through stream and watershed surveys and thus are not considered to be uncertain parameters most of the names of the parameters in this group begin with ch for example ch k2 is the effective hydraulic conductivity in a stream channel unit mm h the second group contains soil moisture and density parameters which are the most sensitive because of their considerable impact on infiltration runoff and evapotranspiration processes for example the soil evaporation compensation factor esco and available water content sol awc are often selected to represent the evaporation process and the amount of infiltration respectively the third group of parameters is also sensitive and related to baseflow which determines the amount of infiltrated water that contributes to streamflow as well as the travel time required to reach the watershed outlet for example the baseflow alpha factor alpha bf and groundwater delay gw delay were selected to determine the ratio of groundwater contributing to streamflow and the time required for it to reach the stream further details of these parameters can be found in theoretical documentation relating to the model neitsch et al 2005 notably parameters such as ch k2 and ch n2 in the first group are measurable parameters although they were not measured in this case study they were still used as calibration parameters 3 4 implementation of bayesian optimization for the swat model information regarding the software and hardware of the hadoop cluster is available in table 1 of the literature zhang et al 2016 in accordance with the flowchart of the bo based sa and calibration framework shown in fig 1 the implementation process for the bayesian method is as follows 1 initialize the parameter space implement various sampling iterations and use these to create batch model configuration files 2 distribute the batch files to the hadoop cluster which then performs the model calculation on the node where the file is located and returns the nse objective function value when the model run is finished 3 determine whether the calibration requirement is satisfied according to the nse value when the calibration requirement is not satisfied the bo s built in acquisition function automatically recommends the next set of parameters to continue the iterative calculation otherwise the cluster operation is terminated 4 return the calibration result when the calibration criterion is satisfied because the bo algorithm is a probabilistic optimization algorithm we used 20 repetitions to ensure the reasonableness of the optimization results overall 560 iterations of the calculations were performed in a single optimization process therefore there were 11 200 20 560 bo model executions in total similarly the rs algorithm was also repeated 20 times for comparison purposes thus 40 optimization results were saved for further analysis 4 results 4 1 parameter identifiability to reduce the subjectivity commonly introduced by using a single sa approach six methods were used to concurrently analyze sort and filter the selected important parameters and to identify the sensitive ones the names measured metrics sampling technique and sample size of the six methods are presented in table 2 notably it was observed that as many as 24 000 parameter combinations were required by morris sampling after batch calculation via the cluster the same number of nse results were obtained thus forming 24 000 pairs of 27 vector inputs and corresponding objective values the 24 000 pairs were fed into the morris algorithm to calculate the sensitivity index the other five methods shared the same analysis process as shown in fig 3 the morris sa approach suggested strong recognizability in comparison with the other approaches therefore we used the morris method to select eight parameters shown in appendix b with a sensitivity index of 0 08 as calibration parameters among the eight parameters cn2 i e the curve number used to estimate surface runoff was established as the most sensitive according to the six sa results arnold et al 1998 neitsch et al 2005 the other seven sensitive parameters were alpha bnk sol k ch k2 canmx sol awc sol bd and ch n2 alpha bnk is the baseflow alpha factor for bank storage sol k is the saturated hydraulic conductivity ch k2 is the effective hydraulic conductivity in a stream channel unit mm h canmx is the maximum canopy storage sol awc is the available water capacity of the soil layer which represents the amount of infiltration and ch n2 is manning s n coefficient for the main channel 4 2 model calibration evaluation was conducted to determine the surrogate model that was most efficient and effective within bo there are several choices for the type of surrogate model to use within bo for approximating the expensive swat in this study the performance of three surrogate models i e gp et and rf was evaluated for comparison purposes and a purely random search rs optimization strategy was used as a baseline as shown in fig 4 gp et and rf all exhibited similar convergence performance which was higher than that of rs unsurprisingly in the early stages 100 iterations the convergence performance of et was slightly higher than that of both gp and rf for a given number of iterations e g 150 the nse obtained by gp 0 89 was slightly higher than that achieved by both et and rf thereafter gp maintained its small advantage until convergence was reached in summary this comparison clearly suggests that gp et and rf as surrogate models were superior to rs with respect to the effectiveness and efficiency of swat parameter determination and optimization and gp slightly outperformed both et and rf however this finding does not mean that gp is definitely superior to the others the pi ei and lcb functions were compared to determine the acquisition function that was most efficient and effective within bo the numbers of iterations required for convergence of the different acquisition functions are shown in fig 5 all three acquisition functions showed good convergence within 300 iterations however the number of iterations required for the convergence of ei was similar to that of lcb and less than that of pi but the value of the convergent nse of ei was greater than that of both lcb and pi thus ei had the highest convergence efficiency the convergence efficiency of lcb was slightly higher than that of pi overall ei is considered preferable to be the best function in the bo based model calibration process to compare bo based automatic calibration with manual calibration the commonly used swat cup 2012 manual calibration and uncertainty analysis program was used the built in sequential uncertainty fitting algorithm of swat cup 2012 was used to conduct manual calibration which usually necessitates a few thousand simulations zamani et al 2021 three hundred swat executions swat cup s number of simulations with five reconciliation epochs i e at least 300 5 simulations in total were needed to obtain a value of nse 0 87 notably the range of each parameter needed to be reduced manually to guide locate optima depending on the degree of fitness between the observed values and the simulated values therefore the ranges of eight parameters identified from sa above were adjusted manually five times in total moreover attention needed to be paid to carefully reconcile the range of each parameter which determines whether the calibration result is feasible this supports the finding that the bo algorithm based on gp was characterized by rapid convergence and fewer optimization iterations i e an acceptable number of simulations 150 while maintaining similar accuracy nse 0 89 therefore the approach is considered promising for solving the automatic rate parameter problem of environmental models with high evaluation costs traditionally most evolutionary algorithms for parameter valuation in complex models require thousands of iterations to determine better solutions in this case the proposed bo method was shown capable of quickly finding a better solution using fewer iterations and the selection of an appropriate surrogate model and acquisition function further promote rapid convergence and high computational efficiency to quantify the calibration effect of the model fig 6 a shows a comparison of the simulated and measured streamflow at the three hydrological stations an average nse value as high as 0 89 indicates that the calibrated parameters captured the monthly runoff variation accurately and thus can be used for future monthly runoff simulations the probability statistics of the 20 560 simulated runoff values generated during the bo process are shown in fig 6 b d the probability histograms of the runoff simulation results at the three hydrological stations indicate that during the calibration process the simulated runoff values converged to a near normal distribution and that the flow at the maximum value in the probability distribution was similar to the observed value this shows that the uncertainty of the parameters in the calibration process propagated to the simulation results influencing the uncertainty of the simulation results in this sense the convergent trend of the distribution of the simulated results was more valuable in practice than taking a particular value further highlighting the importance of uncertainty analysis during calibration to help understand why the optimization process proceeded as it did it is useful to plot the location and order of the points at which the objective was evaluated fig 7 visualizes the order in which the points were sampled during optimization it can be observed that the early samples are spread throughout the entire parameter space whereas the later samples are clustered around the minimum in fig 7 the diagonal shows histograms for each of the dimensions and the remainder of the figure shows eight dimensional scatter plots of all the points the order in which points were evaluated is encoded in the color of each point darker purple colors correspond to earlier samples lighter yellow colors correspond to later samples and a red point indicates the location of the minimum found by the optimization process it can also be observed that the points begin to cluster around the location of the true minimum the histograms show that the objective is evaluated more often at locations near minima 5 discussion the goal of parameter estimation is to efficiently and robustly locate the global minimum of an objective function concerning model parameters with complex models such as swat this goal is not easy to achieve because the objective function can contain multiple local minima and the evaluation of the objective function can be computationally expensive and may not result in an optimal solution within an acceptable time thus a better optimization algorithm should yield a smaller objective function value and a better model fit to the calibration data using less computational time for this reason the proposed bo framework represents a superior approach to solving parameter optimization problems it facilitates bidirectional variable transfer between the optimization algorithm and model evaluation in addition it moves extensive model evaluations to the hadoop cluster to wrap the model into the calculation flexibly and separate the calculation process from the algorithm execution effectively such a design shows the advantage that the number of executions was drastically reduced from 1500 to 150 compared with that of the manual swat cup tool while similar accuracy was maintained nse coefficient 0 89 and 0 87 respectively in our case study wrapping complex models such as swat into an hpc hadoop cluster is beneficial for speeding up model evaluations and separating model evaluations from optimization algorithms endows the proposed framework with greater flexibility and scalability allowing it to be adapted to other complex model applications that may benefit from automated calibration instead of selecting from a grid uninformed by past objective function evaluations as in a random grid search or a manual search bo methods consider previous results to try more promising values they work by constructing a probability model of the objective function called a surrogate function which can be optimized more easily than the actual objective function after each evaluation of the objective function the algorithm updates the probability model by incorporating the new results the algorithm forms an initial idea of the objective function and updates it with each new piece of evidence the subsequent values tested in the objective function are selected by the algorithm optimizing the probability model surrogate function usually with a criterion such as ei pi or ucb finding the values that yield the greatest ei pi or ucb in the surrogate function is much cheaper than evaluating the objective function itself by choosing subsequent values according to a model rather than at random the intention is that the algorithm will converge more rapidly to the true optimal values the overall goal is to evaluate the objective function fewer times by spending a little more time choosing subsequent values several approaches for addressing parameter uncertainty estimation have been proposed in recent decades including contour plot methods such as uniform grid sampling uniform random sampling and the sequential uncertainty fitting algorithm abbaspour et al 1999 uhlenbrook et al 1999 these approaches are robust but require massive computational resources for a high dimensional parameter space li et al 2010 in contrast monte carlo based methods including importance sampling and mcmc are more popular than traditional methods because their strengths lie in handling the nonlinearity and interdependency of parameters in complex hydrological models li et al 2010 although mcmc which is one of the most important numerical techniques for creating a sample from the posterior distribution has been used widely in hydrological modeling to quantify parameter uncertainties it still demands a considerable number of model iterations in the mcmc method the next point i e ps to try is dependent only on the current status therefore it can be approximated by an improved rs optimization approach overall the objective function converges to a lower score with bo than with rs and bo requires far less time to find the optimum of the objective function garrido merchán et al 2019 in our study as shown in fig 4 we obtained faster optimization and a better result confirming that the bo algorithm is a promising solution that is suitable for application to swat model calibration for complex model applications including swat model calibration has been a longstanding challenge because bo has become a popular and effective approach for the black box optimization of nonconvex expensive functions in robotics machine learning computer vision and many other areas of science and engineering lizotte et al 2007 brochu et al 2009 snoek et al 2012 bagnara et al 2015 ghahramani et al 2015 shahriari et al 2016 wu et al 2019 the proposed bo based automatic calibration method can easily be adapted to other complex model calibration applications in bo a prior is posed on the unknown objective function and the uncertainty given by the associated posterior is the basis for an acquisition function that guides the selection of the next point to query the function in practice the key to this method can be simplified to construct objective functions for various complex models where the objective function measures the relationship between input parameters and model simulation results the real complex model can be treated as a black box function in this manner the bo based calibration method is readily adaptable to other complex model applications requiring automated calibration in reality the optimized parameters obtained from a formal optimization process are not guaranteed to be optimal because the optimized parameters may not truly represent the underlying physical system any simulations based on these parameters may not produce performance as good as in the calibration the reason is that optimization tunes model parameters concerning an objective function which is defined according to the model and calibration data xi et al 2017 in this study the objective function nse was defined solely using the calibration data meaning that a good gp based surrogate optimization method produces good models that can fit only the calibration data when we calibrate parameters against a dataset the parameters are tuned to fit the calibration data therefore when the prediction data differ from the calibration data the simulation results based on the calibrated parameters may also deviate from the prediction data this may result in inability to tune an ideal model that perfectly represents the system and can capture all its changes under different conditions moreover it may be a problem in relation to an inadequate model because the optimization may reflect overfitting to compensate for model uncertainties xi et al 2017 furthermore the optimized parameters obtained from a formal optimization are a probability distribution each item value of the optimized parameters is a probability distribution rather than a specific value because bo commonly uses a typical probability model such as gp as a surrogate model after each evaluation of the objective function the parameters converge to a certain distribution the convergence process of eight parameters is illustrated in fig 7 by plotting the locations and the order in which the samples were evaluated for objectives certainly the design of the objective function is destined to affect the parameter convergence process taken together the optimized parameters based on this definition of the objective function may not be physically optimal formulation of such an advanced objective function is required in the future and it is expected that the new calibration approach can be implemented efficiently with the proposed surrogate optimization method because it uses a surrogate of the model itself rather than of the objective function 6 conclusions this paper presented a bo based global optimization framework for the automatic calibration of the parameters of the swat semidistributed hydrological model the framework provides an ideal environment in which to assess the capability of bo to quantify the efficient estimation of swat parameters it also offers the advantage of evaluating the performance of different surrogate models and acquisition functions and provides instant visualization for searching for optimal parameters because the execution of the model is computationally intensive and because conventional manual calibration methods cannot find an optimal result within a reasonable time our goal was to explore the use of a surrogate model based optimization method for model calibration in a computationally efficient manner by maximizing the nse between the observations and model results within the bo method a surrogate model is used to construct a system for approximation of the actual swat and the surrogate model is then calibrated in the optimization process by resorting to acquisition functions in this manner bo efficiently balances the exploration and exploitation of the parameter space to quickly guide the user to the configuration that best optimizes a certain overall evaluation criterion nse in our study comparison of various surrogate models i e gp et rf and rs indicated that gp et and rf exhibited similar convergence performance which was higher than that of rs and gp slightly outperformed et and rf this helped in the selection of the appropriate surrogate strategy comparison of three common acquisition functions i e ei pi and lcb suggested that the performance of ei was better than that of the others which helped in the selection of the appropriate acquisition strategy a bo based global optimization approach is critical to the practical application of global optimization for complex hydrological models the nonintrusive nature of the method makes it useable with many other models and optimization schemes used in hydrology and other research fields declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was funded by the national key r d program of china 2019yfd0901105 the authors thank the reviewers for their helpful comments we thank liwen bianji edanz https www liwenbianji cn for editing the english text of a draft of this manuscript appendix c supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105235 appendices appendix a table a 1 selected parameters and their ranges in relation to the meichuan river used for swat model calibration table a 1 parameter id min max parameter description rcn2 mgt 35 98 scs runoff curve number ch w2 rte 0 1000 average width of main channel ch l2 rte 0 05 500 length of main channel ch k2 rte 0 05 500 hydraulic conductivity in the main channel sol awc sol 0 1 soil available water content sol bd sol 0 9 2 5 soil moist bulk density alpha bf gw 0 1 baseflow alpha factor days esco hru 0 1 soil evaporation compensation factor gw delay gw 0 500 groundwater delay days sol k sol 0 2000 saturated hydraulic conductivity ch d rte 0 30 the average depth of the main channel tlaps sub 10 10 temperature lapse rate timp bsn 0 05 0 9 snowpack temperature lag factor gwqmn gw 0 2 threshold depth of water in the shallow aquifer required for return flow to occur mm smfmx bsn 5 5 maximum melt rate for snow during the year occurs in summer gw revap gw 0 0 2 groundwater revap coefficient revapmn gw 0 500 threshold depth of water in the shallow aquifer for revap to occur mm ch s2 rte 0 001 10 the average slope of the main channel smfmn bsn 5 5 minimum melt rate for snow during the year occurs in winter sno50cov bsn 0 0 9 snow water equivalent that corresponds to 50 snow cover canmx hru 0 100 maximum canopy storage snocovmx bsn 0 500 minimum snow water content that corresponds to 100 snow cover ch n2 rte 0 0 3 manning s n value for the main channel sol alb sol 0 0 25 moist soil albedo surlag bsn 0 1 20 surface runoff lag time sftmp bsn 5 5 snowfall temperature epco bsn 0 0 9 plant uptake compensation factor appendix b table b 1 identified parameters of the meichuan river used for swat model calibration table b 1 parameter id min max parameter description cn2 35 98 scs runoff curve number alpha bnk 0 1 baseflow alpha factor for bank storage sol k 0 2000 saturated hydraulic conductivity ch k2 0 01 500 effective hydraulic conductivity in main channel alluvium canmx 0 100 maximum canopy storage sol awc 0 1 available water capacity of the soil layer sol bd 0 9 2 5 moist bulk density ch n2 0 0 3 manning s n value for the main channel 
25691,a framework that integrates bayesian optimization bo and high performance computing was developed to automate calibration of complex hydrological models it adopts a loosely coupled web architecture integrating tornado and springboot to facilitate bidirectional transfer of variables between bo and model evaluation extensive model evaluations were implemented on a hadoop cluster to wrap the model into the calculation flexibly and separate the calculation process from the algorithm execution effectively a case study calibrating a swat model in the meichuan basin jiangxi province china indicated that the framework provides an ideal environment for assessment of the capability of bo to quantify the efficient estimation of swat parameters compared with that of the built in swat cup tool the number of executions was reduced from 1500 to 150 while maintaining similar accuracy the framework also allows evaluation of the performance of different surrogate models and acquisition functions and provides instant visualization for searching for optimal parameters keywords bayesian optimization calibration sensitivity analysis swat 1 introduction hydrological systems exhibit considerable spatial heterogeneity because of the different physical and chemical processes occurring within such systems and this spatial variability requires careful modeling the advent of distributed hydrological modeling has made it possible to capture such spatial variations when simulating hydrological systems consequently in the field of water resources designers researchers and decision makers rely increasingly on such models to help elucidate the natural processes and investigate the effects of anthropogenic activities on watershed systems howarth et al 1996 wu and xu 2006 many hydrological models contain parameters that cannot be determined directly from field measurements and thus are poorly constrained such parameters must be estimated using indirect methods such as calibration model calibration refers to the adjustment of such model parameters within recommended ranges to optimize the agreement between observed data and simulated results tolson and shoemaker 2007 because calibrated values are affected by correlations between parameters and statistical features of model residuals this may lead to equifinality duan et al 1992 for large complex models such as the soil and water assessment tool swat uncertainty is a major concern ng et al 2010 zhang et al 2009 uncertainty may be associated with input data model structure and parameters each of which represents a considerable challenge in model calibration and may restrict the successful application of a hydrological model to improve model accuracy and reduce uncertainty many calibration methods have been developed to estimate hydrological parameters by matching modeled results with corresponding field measurements traditionally hydrologists adjust the values of model parameters using trial and error methods such that the model closely matches the behavior of the real system that it is intended to represent in its most elementary form this calibration is performed by manual adjustment of the parameters and visual inspection of the agreement between observations and model predictions janssen 1995 however distributed parameter hydrological models partition watersheds into spatially discrete computational units and solve model equations for each unit separately fitzhugh and mackay 2000 therefore when the number of spatial units increases so does the number of model parameters to be calibrated the large number of parameters involved in distributed hydrological models makes manual calibration a less feasible option zou et al 2004 this is because the conceptual relationship between the parameters of the model and the watershed characteristics becomes increasingly difficult to interpret visually a large number of interacting parameters can result in unpredictable effects when multiple parameters are adjusted gupta et al 1999 furthermore manual calibration is a tedious and daunting task that requires a trained hydrologist with comprehensive knowledge of the watershed in contrast automatic calibration is less subjective and it is capable of extensive searching of sets of model parameters within their acceptable ranges in a very short time increasing the likelihood of finding optimal parameters bekele et al 2007 because of the subjective and time consuming nature of manual trial and error calibration considerable research effort has been devoted to the development of methods for automatic calibration of hydrologic models gupta and sorooshian 1994 automatic methods seek to take advantage of the speed and power of digital computers while being objective and reasonably easy to implement zou et al 2014 wellen et al 2015 chaudhary et al 2017 yang et al 2018 research into the development of automatic calibration methods has become increasingly popular driven by the need for efficient and better calibration research has focused on the development of automatic parameter optimization algorithms to identify parameters duan et al 1992 sorooshian et al 1993 wang 1991 lee et al 2020 however many of these algorithms attempt to find an individual optimal parameter set ps which is inconsistent with the fact that multiple similar or equivalent solutions may exist as a consequence of uncertainties in model structure input data and parameters geza and mccray 2008 shen et al 2010 hence many uncertainty analysis methods and parameter estimation algorithms such as generalized likelihood uncertainty estimation beven and binley 1992 stedinger et al 2008 genetic algorithms zou et al 2007 simulated annealing algorithms borgomeo et al 2015 particle swarm optimization afshar et al 2011 markov chain monte carlo mcmc methods vrugt et al 2003 haario et al 2006 liang et al 2016 the parasol optimization and uncertainty analysis tool van griensven and meixner 2006 and sequential uncertainty fitting abbaspour 2011 mousavi et al 2012 rouholahnejad et al 2012 have been developed to establish more robust models and many have shown promising results calibration entails a large number of model simulations which can be computationally intensive or even prohibitively expensive to perform if the model simulation is extremely complex while enabling better accounting for uncertainties stemming from various sources such methods and algorithms used for calibration and uncertainty analysis do not dramatically reduce execution time in some instances they can actually increase it such costs are further exacerbated by automated calibration which can require thousands of model evaluations to address this problem a highly efficient calibration method is needed that requires fewer model runs while maintaining global optimal performance one such method is the bayesian optimization bo algorithm the bo technique uses bayes theorem to direct the search for the maximum or minimum of an objective function mockus et al 1978 hutter et al 2019 because model calibration can be formulated as an optimization problem that seeks to minimize the weighted sum of the squared residuals between the field measured and model computed observations by adjusting parameters with uncertain values it could potentially benefit from this optimization technique the parameter problems that exist in traditional population based algorithms such as genetic algorithms and particle swarm optimization do not affect bo which is particularly useful when the calculation of the objective function is expensive hutter et al 2019 these features make bo an important technique in computer based science and bo implementation has been reported to have achieved state of the art performance in many domains for example image processing and speech recognition ma et al 2017 hutter et al 2019 parameter estimation in artificial intelligence shahriari et al 2015 cornejo bueno et al 2018 soil respiration modeling toda et al 2020 optimization for intelligent environmental monitoring marchant et al 2012 and optimization of pump operations in water distribution systems candelieri et al 2018 although bo has been used to reduce the computational burden of advanced analyses using simulation models across several disciplines the lack of its application within hydrological modeling has impeded the uptake of bo methods that can support computationally demanding analyses such as the application of complex model calibration noting that bo based parameter estimation has rarely been employed in the context of hydrological model calibration and recognizing the importance of parameter optimization in the swat model the objective of this study was to develop a bo based calibration framework to investigate the applicability of bo to the automatic calibration of the swat model the framework adopts a loosely coupled web architecture that integrates tornado and springboot to facilitate the bidirectional transfer of variables between bo and model evaluation extensive model evaluations were implemented on a hadoop cluster where complex models like swat were wrapped and executed flexibly within mapreduce further separating the model calculation process from the algorithm execution process specifically a case study of bo based calibration for a swat model was performed wherein the developed framework took advantage of the global convergence of bo within a limited number of iterations the key concept is the use of bo to construct a probabilistic surrogate of the swat model to fit the real objective function and then to evaluate the cheap to run surrogate model in the optimization process wherein the optimal potential ps is suggested to avoid unnecessary sampling and improve overall computational efficiency the main contributions of our work lie in the development of a comprehensive framework that integrates complex models bo and high performance computing hpc to wrap the model into the calculation flexibly and separate the calculation process and algorithm execution effectively in addition within the framework we assessed the capability of bo to quantify the efficient estimation of swat parameters and evaluated the performance of three surrogate models and three acquisition functions of the bo algorithm the remainder of this paper is organized as follows in section 2 the bo based model calibration framework is illustrated and its major components which include the bo algorithm physical swat model and parameter sensitivity analysis sa are described section 3 describes both the application of the proposed approach to calibration of the swat model in a case study and the evaluation of the performance of the three surrogate models and three acquisition functions of bo to guide future algorithm configuration sections 4 and 5 present the optimization results and a discussion of future research respectively finally our conclusions are presented in section 6 2 methods 2 1 framework for bayesian optimization bo based model calibration because the swat model is a fortran based hydrological model its capabilities in output analysis parameter identifiability model calibration and visualization need to be enhanced in this paper we present a bo based calibration framework written mainly in python and java our motivation is to take advantage of the full feature set of the three programming languages fortran s efficient scientific computing python s facilities for fast prototyping numerous libraries for data processing data analysis and data visualization and java s hpc while obviating the need for extra programming effort to allow them to interact the whole framework can be divided logically into the following three parts with respect to the programming languages used the python based tornado web framework is used to accommodate our core algorithms involving sa and bo the java based springboot framework serves as a transformer it is responsible for receiving pss from the tornado framework parsing pss to generate hadoop mapreduce jobs and submitting these jobs to a hadoop cluster for further resource scheduling and model execution the fortran based swat model was wrapped as a mapreduce task within the hadoop cluster for model evaluation multiple swat models can be invoked and executed simultaneously and pss and simulation results can be transferred bidirectionally via http requests between python and java our proposed framework was implemented using python code in a linux environment by coupling the swat model with the salib sa library herman et al 2017 and the scikit optimize bo library tornado was chosen to host both the salib and scikit optimize libraries where the sampled pss in the sa procedure and the recommended pss in the bo procedure are all sent to the java based springboot framework in the form of http requests springboot is responsible for receiving and transforming the variables noted above the sampled and suggested pss into model evaluations and then writing a driver program to run these model evaluations on the hadoop cluster the model evaluations numbering several hundred are distributed to a hadoop cluster comprising seven nodes each equipped with an intel 8 core i7 3770 processor and 32 gb ram and four nodes each equipped with an intel 32 core xeon r silver 421 processor and 96 gb ram the procedure for running swat model evaluations on a hadoop cluster is to call the fortran based swat model from the java based mapreduce framework because the fortran code of swat is compiled to an exe file in the linux environment this model can easily be executed in the mapreduce framework finally http requests are used to communicate between programs in the two languages python and java to transfer variables bidirectionally between them in contrast to a previous calibration framework which used a standalone r package fme for automating calibration sensitivity and uncertainty wu et al 2012 our framework adopts a web based architecture instead of using standalone architecture to coordinate the relationship between complex model and optimization algorithm which is highly modular and facilitates maintenance this is beneficial for taking advantage of the full feature set of different programming languages while obviating the need for extra effort to allow them to interact in addition our framework extends a prior study on moving swat model calibration to the hadoop based cloud zhang et al 2016 by reducing the whole hadoop based cloud to a pure computing system without performing any algorithm analysis our approach can enable complex models like swat to be wrapped and executed more flexibly within mapreduce moreover objective values can be returned easily from the model evaluation environment to the algorithm environment thus effectively separating the model calculation process from the algorithm execution process the framework flowchart and components are elaborated in the following subsections the methodology of the framework involves six steps the development of the swat model design of a suite of representative simulations parameter sa evaluation of the objective function construction of bo based surrogate models and minimization of the objective function using the surrogate models and optimizer the core tasks are explained below and illustrated in fig 1 i model setup first development of the swat model is completed the swat model requires explicit information regarding weather topography soil properties vegetation and land management practices to simulate processes such as surface and subsurface flow sediment transport nutrient cycling and crop growth here arcswat is used for the swat model setup because it has a user friendly interface and presents the results as intuitive and informative maps during the setup process swat is fed by multiple databases including digital elevation models dems streamlines soil data and land use data as shown in fig 1 m hydrogeological parameters are initially chosen for estimation by the proposed method ii representative simulations second representative simulations for parameter sa are designed the m parameters are sampled n times following specified probability density functions from the m dimensional parameter space constrained by their ranges using parameter sampling methods such as latin hypercube sampling mckay et al 1979 the parameter ranges are carefully determined according to available field data each sample drawn from the m dimensional space is an m vector and the n samples represent complete possible combinations of the m parameters as the inputs to the n simulations of the swat model n swat models are then constructed from these n input samples iii sensitivity analysis next sa and parameter screening are performed the sensitivities of the m parameters are evaluated using specified sensitivity indices such as morris s mean elementary effect and sobol s total order indices sobol 1993 which can be calculated efficiently from the response surface of the streamflow to the input parameters constructed from the n pairs of the training dataset fig 1 based on the sa j j m sensitive parameters are identified and included in an iterative calibration process while insensitive parameters m j are screened out of the original m vector inputs in this stage according to the sa it is worth noting that multiple sa methods can be used in parallel to reduce the subjectivity commonly introduced when using a single evaluation approach iv construction of bo in the next step the bo objective function is defined the streamflow outputs of the n representative swat simulations are used to calculate the objective function which in this study is the nash sutcliffe efficiency nse coefficient to be minimized between the simulated and measured streamflow histories the objective function nse is used as the response variable of the surrogate model to the j input variables having retrieved n pairs of j vector inputs and the corresponding objective functions the bo object is constructed the l pairs of k vector inputs and the corresponding objective function are used to instantiate a bo object using random exploration from the above n pairs of j vector inputs this can help by diversifying the exploration space as shown in fig 1 v bo based calibration finally the key bo based calibration loop is entered having initialized the bo object four steps i e evaluate suggest run update are performed in a loop as depicted in fig 1 by the dashed rectangular box first bo creates a new suggestion point which is a j vector input ps via the acquisition function the suggestion point represents a potential j vector input balanced between exploration and exploitation for locating the next potential optima second the suggested j vector inputs are fed into the swat model to evaluate the objective function having retrieved the objective function value the combination of the j vector inputs and the corresponding objective function value is treated as a new observation for updating the bo object in this manner the bo object is continuously improved which is conducive to recommending a more promising ps the loop continues until a specified criterion is reached such as a maximum number of iterations or an objective function value finally the j optimal parameters are determined the above evaluate suggest run update loop forms the core of the bo based calibration framework as the loop continues bo balances its needs for exploration and exploitation by taking into account what it knows about the target function at each step a surrogate model e g gaussian process gp is fitted to the known observations i e the points previously explored and the posterior distribution combined with an exploration strategy e g probability of improvement pi upper confidence bound ucb or expected improvement ei is used to determine the next suggestion that should be explored the final loop results in the optimal values of the j input parameters 2 2 swat model the swat hydrological model used in this study is a semi distributed continuous time watershed model capable of running on daily and subdaily time steps gassman et al 2007 it was originally developed to better understand the impact of management scenarios and non point source pollution on water supplies at a watershed scale arnold et al 1998 it has been used in various watershed studies including simulations of both water quantity and water quality lee et al 2010 liu et al 2013 the watershed is first divided into a number of subbasins dependent on outlets which are usually defined by subdivision of a dem and manual adjustment each subbasin is divided into a number of hydrological response units hrus with specific attributes by setting different thresholds for soil land use and slope classes the swat model uses the concept of the hru to represent variability within the subbasins of a watershed the hrus which are unique representations of land cover soil and management characteristics within a single subbasin are used for water balance calculations within the model the hrus are not spatially contiguous and therefore are often composed of many disjointed parcels of land within a watershed the objective of model calibration can be stated generally as the estimation of model parameters such that the model simulation and the hydrological behavior of the watershed match closely watershed responses that characterize the hydrological behavior of a watershed e g streamflow hydrographs sediments or nutrient loadings can be used as calibration objectives the complexity of the calibration process depends on model type because the number of parameters to be estimated varies accordingly lumped models which treat an entire watershed as a homogenous unit often have a small number of adjustable parameters that can be easily calibrated manually however the number of parameters in semidistributed models such as swat can often be two or three orders of magnitude higher than that in lumped models this is because the swat model takes the spatial heterogeneity of the watershed into account by dividing the watershed into different subbasins that belong within the same range of hydrological factors although a finer level of discretization often results in better simulation of watershed responses it demands greater computational time and increases the number of model parameters substantially that is each parameter value must be estimated through a calibration process 2 3 bayesian optimization bo bo is an efficient framework for the global optimization of expensive objective functions and it is especially useful for optimizing black box functions whose exact form is unknown in particular bo is used in machine learning applications for automating the model deployment pipeline recently bo has become popular among the machine learning and data mining communities as an efficient technique for tuning the hyperparameters of machine learning models auer 2002 the core of bo employs a probabilistic model such as gp extra trees et or random forests rf to capture the unknown function of these gp is often the preferred choice of probabilistic model the gp probabilistic model is specified by its mean and covariance the subsequent stage of bo uses surrogate utility functions known as acquisition functions these functions are constructed to balance the need to explore regions in which epistemic uncertainty regarding the objective function is high with the need to exploit regions in which the predicted values are high these acquisition functions can be optimized to extract the next candidate point for evaluating the objective function 2 3 1 bayesian optimization of model parameters parameter estimation of a complex watershed hydrological model can be regarded as a nonlinear optimization process which aims to find the optimal ps x o p that makes the unknown objective function f reach an extreme maximum or minimum value 1 x o p argmax f x x r d where x is the set of model parameters and r d is the d dimensional value space of the parameters the bo process refers to bayes theorem 2 p f d t p d t f p f where p f and p f d i are the prior and posterior probabilities of f respectively p f d i is the likelihood function d i is the observed set and the following recursive relationships are satisfied 3 d 1 x 1 f 1 d t d t 1 x 1 f t where f t is the objective function value corresponding to x t as mentioned above two key components should be considered when performing bo the probabilistic surrogate model and the acquisition function snoek et al 2012 shahriari et al 2015 in comparison with other similar procedures bo is unique in that it avoids unnecessary sampling by constructing a probabilistic model for f x and the optimal potential minimum loss function evaluation point is selected for the next assessment according to the acquisition function while integrating out uncertainty thus bo can theoretically guarantee final convergence on the premise of optimization of the acquisition function because the optimal potential points are selected for the evaluation during each step of the iteration process provided that sufficiently many iterations are performed the algorithm will eventually converge to the global optimal solution the following subsections describe the probabilistic surrogate model and acquisition function 2 3 2 probabilistic surrogate model the probabilistic surrogate model consists of a prior distribution and an observational model which increases the information and modifies the prior knowledge through continuous iteration according to bayes theorem shahriari et al 2015 therefore a prior function should be selected that presents assumptions related to the function being optimized for this we choose the gp prior which is the most efficient probabilistic surrogate model in bo because of its flexibility tractability analyzability and good substitutability for linear and nonlinear relationships jones 2001 rasmussen et al 2006 snoek et al 2012 other surrogate models such as et and fr are not elaborated for gp the argument x of the stochastic function f x is used as an index set for each input x and there is an associated stochastic variable f x which is the value of the random function f at that location therefore gp is the normalization of the multidimensional gaussian probability distribution which is characterized by a mean function m x and a covariance function k x x 4 g x g p m x k x x in practice it is very difficult to specify the prior information for the mean function of the gp therefore for the sake of simplicity it is usually assumed that the prior mean function is 0 which has little effect on the accuracy of the posterior distribution hence the prior mean value is set as 0 covariance specifies the smoothing line and amplitude of the unknown objective function and represents the similarity between the two calculation points which has a strong influence on whether the ideal prediction effect can be obtained the matérn covariance function cluster which is a type of highly flexible covariance function that can create a second order differentiable sample function can be expressed as follows 5 k m r 2 1 v γ v 2 v r l v k v 2 v r l where v is the smoothing parameter with default value 2 5 l is the scale parameter with default value 1 and k v is the second type of deformed bessel function 2 3 3 objective function similar to other optimization algorithms the objective function should be selected first in bo this is the statistic that is required to be maximized or minimized in the optimization process the nse nash et al 1970 is one of the most commonly used model evaluation indicators bennett et al 2013 bae 2018 and it represents the ratio of model fitting variance to total variance therefore the nse was selected as the model evaluation index in this study 6 n s e 1 i 1 n o i s i 2 i 1 n o i o i 2 where o i is the observed value o i is the average of the observed values and s i is the simulated value when the value of the nse coefficient is equal to 1 it represents a perfect fit of the simulated value to the observed value 2 3 4 acquisition function a simple strategy adopted in bo is the use of a surrogate utility function which is easy to evaluate the surrogate utility function is called the acquisition function the acquisition function is constructed according to the posterior probability distribution using the sampling function to determine the next point to be evaluated whereby the number of iterations and the evaluation cost can be reduced shahriari et al 2015 generally the selection of sampling points depends on exploration and exploitation ghahramani et al 2015 the former searches around the current optimal solution to find the global optimal solution whereas the latter tries to exploit the sample points that have not been evaluated to avoid developing a locally optimal solution the acquisition function can be based on one of four strategies promotion based confidence bound information based and combination strategies shahriar et al 2015 when gps are used as probabilistic surrogate models the commonly used collection functions include pi kushner 1963 ei and ucb snoek et al 2012 the pi approach quantifies the probability that each point may improve the current optimal objective function value and selects the point with the highest probability of increasing the current target value as the new evaluation 7 α p i x t d t p f x t ω φ m x t ω σ t where φ refers to the cumulative distribution function of the standard normal distribution ω is the threshold value with default value 0 01 of the evaluation promotion i e if f exceeds this value it is considered that the optimization effect is improved and σ t is the variance of f the pi approach selects the evaluation point with the largest promotion probability but pi also regards the promotion of all points as equal which means that it can reflect only the probability of promotion not its magnitude in contrast the ei approach incorporates both the promotion probability and the promotion amount 8 α e i x t d t m x ω φ m x t ω x t σ t φ m x t ω x t a 2 b 2 where φ is the probability density function of the standard normal distribution the upper bound of the confidence interval of each point is compared directly in ucb which can be regarded as the linear weighting of the mean exploration and variance exploitation 9 u c b m x k σ in eq 9 k is the weighting factor used to weight the exploration and exploitation of detection whereby the greater the value of k the greater the weight of the detection breadth in this study the above three functions were selected for optimization when ucb was selected as the acquisition function the value of k was set as 2 5 however to solve for the minimum value of the objective function the lower confidence bound lcb strategy was used instead 10 l c b m x k σ 2 4 sensitivity analysis library to discern the parameters that have the greatest influence on model performance and to identify the most appropriate parameter values it is necessary to screen out sensitive parameters and quantitatively evaluate the influence of each parameter on model performance for this specific purpose many previous related studies have used sa van griensven et al 2006 borgonovo et al 2012 because it can identify parameters for which reduction in the uncertainty specification has the greatest impact on improving model performance thus if some noninfluential parameters can be identified and reasonably fixed at given values within their ranges the computational costs may decrease without a reduction in model performance for this purpose salib was used in this study to conduct the sa for the swat model salib is a python implementation of commonly used sa methods including sobol fast the delta moment independent measure and the derivative based global sensitivity measure dgsm salib is useful in simulation optimization and system modeling for calculating the influence of model inputs or exogenous factors on outputs of interest herman and usher 2017 salib provides a range of global sa techniques that can be easily implemented in typical modeling workflows the library facilitates the generation of samples associated with model inputs and provides functions to analyze and visualize model outputs 3 case study 3 1 study area the selected study area was the meichuan river basin 26 17ʹ34ʺn 27 08ʹ54ʺn 115 44ʹ28ʺe 116 16ʹ06ʺe which is one of the subbasins of the poyang lake basin with a catchment area of 3304 km2 fig 2 the basin is dominated by low medium elevation terrain elevation 170 1440 m average 379 1 m the source of the meichuan river is wangpizhang at the northern edge of the village of xiaotian in ningdu county jiangxi province china and the river runs through the entire basin the regional climate is mild and humid featuring a distinct monsoon and abundant rainfall the annual average temperature is 17 3 c with an extreme maximum minimum temperature of 37 9 c 6 2 c the annual precipitation is in the range of 1500 1700 mm of which more than 55 occurs in the rainy season may august the annual average relative humidity is 80 the soil in the meichuan river basin is mostly deep and fertile red soil and yellow red soil developed from metamorphic rocks and granites comprising loam and sandy loam soil types the primary land use types are forest and grassland which account for more than 60 of the total area the cropping system is biannual with rice as the dominant crop 3 2 input data a variety of geospatial and temporal data are used in swat simulations the basic data required by the swat model include a dem a river network land use and land cover soil type and its physicochemical properties plant and plant growth fertilizer hydrological and water quality meteorology livestock husbandry and pollution emissions data neitsch et al 2004 among these hydrological and water quality data are used mainly for modeling determination of model parameters and calibration in the case study the required spatial data included topographic data soil data and land use data temperature precipitation humidity wind speed and solar radiation data from eight meteorological stations were obtained from the china meteorological assimilation driving datasets for the swat model cmads meng et al 2018 records of monthly streamflow and sediment observed at the watershed outlets i e ningdu fenkeng and shicheng were acquired from chinese hydrological data yearbooks from 2008 to 2018 details of the data used in the present study are listed in table 1 3 3 model parameters the selection and range of model parameters were determined through a review of literature related to swat model calibration arabi et al 2008 overall the 27 parameters recommended in the literature ghaith and li 2020 were selected the names upper and lower limits and descriptions of the parameters are listed in appendix a the parameters were classified into three groups the first group describes channel properties which can be measured directly through stream and watershed surveys and thus are not considered to be uncertain parameters most of the names of the parameters in this group begin with ch for example ch k2 is the effective hydraulic conductivity in a stream channel unit mm h the second group contains soil moisture and density parameters which are the most sensitive because of their considerable impact on infiltration runoff and evapotranspiration processes for example the soil evaporation compensation factor esco and available water content sol awc are often selected to represent the evaporation process and the amount of infiltration respectively the third group of parameters is also sensitive and related to baseflow which determines the amount of infiltrated water that contributes to streamflow as well as the travel time required to reach the watershed outlet for example the baseflow alpha factor alpha bf and groundwater delay gw delay were selected to determine the ratio of groundwater contributing to streamflow and the time required for it to reach the stream further details of these parameters can be found in theoretical documentation relating to the model neitsch et al 2005 notably parameters such as ch k2 and ch n2 in the first group are measurable parameters although they were not measured in this case study they were still used as calibration parameters 3 4 implementation of bayesian optimization for the swat model information regarding the software and hardware of the hadoop cluster is available in table 1 of the literature zhang et al 2016 in accordance with the flowchart of the bo based sa and calibration framework shown in fig 1 the implementation process for the bayesian method is as follows 1 initialize the parameter space implement various sampling iterations and use these to create batch model configuration files 2 distribute the batch files to the hadoop cluster which then performs the model calculation on the node where the file is located and returns the nse objective function value when the model run is finished 3 determine whether the calibration requirement is satisfied according to the nse value when the calibration requirement is not satisfied the bo s built in acquisition function automatically recommends the next set of parameters to continue the iterative calculation otherwise the cluster operation is terminated 4 return the calibration result when the calibration criterion is satisfied because the bo algorithm is a probabilistic optimization algorithm we used 20 repetitions to ensure the reasonableness of the optimization results overall 560 iterations of the calculations were performed in a single optimization process therefore there were 11 200 20 560 bo model executions in total similarly the rs algorithm was also repeated 20 times for comparison purposes thus 40 optimization results were saved for further analysis 4 results 4 1 parameter identifiability to reduce the subjectivity commonly introduced by using a single sa approach six methods were used to concurrently analyze sort and filter the selected important parameters and to identify the sensitive ones the names measured metrics sampling technique and sample size of the six methods are presented in table 2 notably it was observed that as many as 24 000 parameter combinations were required by morris sampling after batch calculation via the cluster the same number of nse results were obtained thus forming 24 000 pairs of 27 vector inputs and corresponding objective values the 24 000 pairs were fed into the morris algorithm to calculate the sensitivity index the other five methods shared the same analysis process as shown in fig 3 the morris sa approach suggested strong recognizability in comparison with the other approaches therefore we used the morris method to select eight parameters shown in appendix b with a sensitivity index of 0 08 as calibration parameters among the eight parameters cn2 i e the curve number used to estimate surface runoff was established as the most sensitive according to the six sa results arnold et al 1998 neitsch et al 2005 the other seven sensitive parameters were alpha bnk sol k ch k2 canmx sol awc sol bd and ch n2 alpha bnk is the baseflow alpha factor for bank storage sol k is the saturated hydraulic conductivity ch k2 is the effective hydraulic conductivity in a stream channel unit mm h canmx is the maximum canopy storage sol awc is the available water capacity of the soil layer which represents the amount of infiltration and ch n2 is manning s n coefficient for the main channel 4 2 model calibration evaluation was conducted to determine the surrogate model that was most efficient and effective within bo there are several choices for the type of surrogate model to use within bo for approximating the expensive swat in this study the performance of three surrogate models i e gp et and rf was evaluated for comparison purposes and a purely random search rs optimization strategy was used as a baseline as shown in fig 4 gp et and rf all exhibited similar convergence performance which was higher than that of rs unsurprisingly in the early stages 100 iterations the convergence performance of et was slightly higher than that of both gp and rf for a given number of iterations e g 150 the nse obtained by gp 0 89 was slightly higher than that achieved by both et and rf thereafter gp maintained its small advantage until convergence was reached in summary this comparison clearly suggests that gp et and rf as surrogate models were superior to rs with respect to the effectiveness and efficiency of swat parameter determination and optimization and gp slightly outperformed both et and rf however this finding does not mean that gp is definitely superior to the others the pi ei and lcb functions were compared to determine the acquisition function that was most efficient and effective within bo the numbers of iterations required for convergence of the different acquisition functions are shown in fig 5 all three acquisition functions showed good convergence within 300 iterations however the number of iterations required for the convergence of ei was similar to that of lcb and less than that of pi but the value of the convergent nse of ei was greater than that of both lcb and pi thus ei had the highest convergence efficiency the convergence efficiency of lcb was slightly higher than that of pi overall ei is considered preferable to be the best function in the bo based model calibration process to compare bo based automatic calibration with manual calibration the commonly used swat cup 2012 manual calibration and uncertainty analysis program was used the built in sequential uncertainty fitting algorithm of swat cup 2012 was used to conduct manual calibration which usually necessitates a few thousand simulations zamani et al 2021 three hundred swat executions swat cup s number of simulations with five reconciliation epochs i e at least 300 5 simulations in total were needed to obtain a value of nse 0 87 notably the range of each parameter needed to be reduced manually to guide locate optima depending on the degree of fitness between the observed values and the simulated values therefore the ranges of eight parameters identified from sa above were adjusted manually five times in total moreover attention needed to be paid to carefully reconcile the range of each parameter which determines whether the calibration result is feasible this supports the finding that the bo algorithm based on gp was characterized by rapid convergence and fewer optimization iterations i e an acceptable number of simulations 150 while maintaining similar accuracy nse 0 89 therefore the approach is considered promising for solving the automatic rate parameter problem of environmental models with high evaluation costs traditionally most evolutionary algorithms for parameter valuation in complex models require thousands of iterations to determine better solutions in this case the proposed bo method was shown capable of quickly finding a better solution using fewer iterations and the selection of an appropriate surrogate model and acquisition function further promote rapid convergence and high computational efficiency to quantify the calibration effect of the model fig 6 a shows a comparison of the simulated and measured streamflow at the three hydrological stations an average nse value as high as 0 89 indicates that the calibrated parameters captured the monthly runoff variation accurately and thus can be used for future monthly runoff simulations the probability statistics of the 20 560 simulated runoff values generated during the bo process are shown in fig 6 b d the probability histograms of the runoff simulation results at the three hydrological stations indicate that during the calibration process the simulated runoff values converged to a near normal distribution and that the flow at the maximum value in the probability distribution was similar to the observed value this shows that the uncertainty of the parameters in the calibration process propagated to the simulation results influencing the uncertainty of the simulation results in this sense the convergent trend of the distribution of the simulated results was more valuable in practice than taking a particular value further highlighting the importance of uncertainty analysis during calibration to help understand why the optimization process proceeded as it did it is useful to plot the location and order of the points at which the objective was evaluated fig 7 visualizes the order in which the points were sampled during optimization it can be observed that the early samples are spread throughout the entire parameter space whereas the later samples are clustered around the minimum in fig 7 the diagonal shows histograms for each of the dimensions and the remainder of the figure shows eight dimensional scatter plots of all the points the order in which points were evaluated is encoded in the color of each point darker purple colors correspond to earlier samples lighter yellow colors correspond to later samples and a red point indicates the location of the minimum found by the optimization process it can also be observed that the points begin to cluster around the location of the true minimum the histograms show that the objective is evaluated more often at locations near minima 5 discussion the goal of parameter estimation is to efficiently and robustly locate the global minimum of an objective function concerning model parameters with complex models such as swat this goal is not easy to achieve because the objective function can contain multiple local minima and the evaluation of the objective function can be computationally expensive and may not result in an optimal solution within an acceptable time thus a better optimization algorithm should yield a smaller objective function value and a better model fit to the calibration data using less computational time for this reason the proposed bo framework represents a superior approach to solving parameter optimization problems it facilitates bidirectional variable transfer between the optimization algorithm and model evaluation in addition it moves extensive model evaluations to the hadoop cluster to wrap the model into the calculation flexibly and separate the calculation process from the algorithm execution effectively such a design shows the advantage that the number of executions was drastically reduced from 1500 to 150 compared with that of the manual swat cup tool while similar accuracy was maintained nse coefficient 0 89 and 0 87 respectively in our case study wrapping complex models such as swat into an hpc hadoop cluster is beneficial for speeding up model evaluations and separating model evaluations from optimization algorithms endows the proposed framework with greater flexibility and scalability allowing it to be adapted to other complex model applications that may benefit from automated calibration instead of selecting from a grid uninformed by past objective function evaluations as in a random grid search or a manual search bo methods consider previous results to try more promising values they work by constructing a probability model of the objective function called a surrogate function which can be optimized more easily than the actual objective function after each evaluation of the objective function the algorithm updates the probability model by incorporating the new results the algorithm forms an initial idea of the objective function and updates it with each new piece of evidence the subsequent values tested in the objective function are selected by the algorithm optimizing the probability model surrogate function usually with a criterion such as ei pi or ucb finding the values that yield the greatest ei pi or ucb in the surrogate function is much cheaper than evaluating the objective function itself by choosing subsequent values according to a model rather than at random the intention is that the algorithm will converge more rapidly to the true optimal values the overall goal is to evaluate the objective function fewer times by spending a little more time choosing subsequent values several approaches for addressing parameter uncertainty estimation have been proposed in recent decades including contour plot methods such as uniform grid sampling uniform random sampling and the sequential uncertainty fitting algorithm abbaspour et al 1999 uhlenbrook et al 1999 these approaches are robust but require massive computational resources for a high dimensional parameter space li et al 2010 in contrast monte carlo based methods including importance sampling and mcmc are more popular than traditional methods because their strengths lie in handling the nonlinearity and interdependency of parameters in complex hydrological models li et al 2010 although mcmc which is one of the most important numerical techniques for creating a sample from the posterior distribution has been used widely in hydrological modeling to quantify parameter uncertainties it still demands a considerable number of model iterations in the mcmc method the next point i e ps to try is dependent only on the current status therefore it can be approximated by an improved rs optimization approach overall the objective function converges to a lower score with bo than with rs and bo requires far less time to find the optimum of the objective function garrido merchán et al 2019 in our study as shown in fig 4 we obtained faster optimization and a better result confirming that the bo algorithm is a promising solution that is suitable for application to swat model calibration for complex model applications including swat model calibration has been a longstanding challenge because bo has become a popular and effective approach for the black box optimization of nonconvex expensive functions in robotics machine learning computer vision and many other areas of science and engineering lizotte et al 2007 brochu et al 2009 snoek et al 2012 bagnara et al 2015 ghahramani et al 2015 shahriari et al 2016 wu et al 2019 the proposed bo based automatic calibration method can easily be adapted to other complex model calibration applications in bo a prior is posed on the unknown objective function and the uncertainty given by the associated posterior is the basis for an acquisition function that guides the selection of the next point to query the function in practice the key to this method can be simplified to construct objective functions for various complex models where the objective function measures the relationship between input parameters and model simulation results the real complex model can be treated as a black box function in this manner the bo based calibration method is readily adaptable to other complex model applications requiring automated calibration in reality the optimized parameters obtained from a formal optimization process are not guaranteed to be optimal because the optimized parameters may not truly represent the underlying physical system any simulations based on these parameters may not produce performance as good as in the calibration the reason is that optimization tunes model parameters concerning an objective function which is defined according to the model and calibration data xi et al 2017 in this study the objective function nse was defined solely using the calibration data meaning that a good gp based surrogate optimization method produces good models that can fit only the calibration data when we calibrate parameters against a dataset the parameters are tuned to fit the calibration data therefore when the prediction data differ from the calibration data the simulation results based on the calibrated parameters may also deviate from the prediction data this may result in inability to tune an ideal model that perfectly represents the system and can capture all its changes under different conditions moreover it may be a problem in relation to an inadequate model because the optimization may reflect overfitting to compensate for model uncertainties xi et al 2017 furthermore the optimized parameters obtained from a formal optimization are a probability distribution each item value of the optimized parameters is a probability distribution rather than a specific value because bo commonly uses a typical probability model such as gp as a surrogate model after each evaluation of the objective function the parameters converge to a certain distribution the convergence process of eight parameters is illustrated in fig 7 by plotting the locations and the order in which the samples were evaluated for objectives certainly the design of the objective function is destined to affect the parameter convergence process taken together the optimized parameters based on this definition of the objective function may not be physically optimal formulation of such an advanced objective function is required in the future and it is expected that the new calibration approach can be implemented efficiently with the proposed surrogate optimization method because it uses a surrogate of the model itself rather than of the objective function 6 conclusions this paper presented a bo based global optimization framework for the automatic calibration of the parameters of the swat semidistributed hydrological model the framework provides an ideal environment in which to assess the capability of bo to quantify the efficient estimation of swat parameters it also offers the advantage of evaluating the performance of different surrogate models and acquisition functions and provides instant visualization for searching for optimal parameters because the execution of the model is computationally intensive and because conventional manual calibration methods cannot find an optimal result within a reasonable time our goal was to explore the use of a surrogate model based optimization method for model calibration in a computationally efficient manner by maximizing the nse between the observations and model results within the bo method a surrogate model is used to construct a system for approximation of the actual swat and the surrogate model is then calibrated in the optimization process by resorting to acquisition functions in this manner bo efficiently balances the exploration and exploitation of the parameter space to quickly guide the user to the configuration that best optimizes a certain overall evaluation criterion nse in our study comparison of various surrogate models i e gp et rf and rs indicated that gp et and rf exhibited similar convergence performance which was higher than that of rs and gp slightly outperformed et and rf this helped in the selection of the appropriate surrogate strategy comparison of three common acquisition functions i e ei pi and lcb suggested that the performance of ei was better than that of the others which helped in the selection of the appropriate acquisition strategy a bo based global optimization approach is critical to the practical application of global optimization for complex hydrological models the nonintrusive nature of the method makes it useable with many other models and optimization schemes used in hydrology and other research fields declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was funded by the national key r d program of china 2019yfd0901105 the authors thank the reviewers for their helpful comments we thank liwen bianji edanz https www liwenbianji cn for editing the english text of a draft of this manuscript appendix c supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix c supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105235 appendices appendix a table a 1 selected parameters and their ranges in relation to the meichuan river used for swat model calibration table a 1 parameter id min max parameter description rcn2 mgt 35 98 scs runoff curve number ch w2 rte 0 1000 average width of main channel ch l2 rte 0 05 500 length of main channel ch k2 rte 0 05 500 hydraulic conductivity in the main channel sol awc sol 0 1 soil available water content sol bd sol 0 9 2 5 soil moist bulk density alpha bf gw 0 1 baseflow alpha factor days esco hru 0 1 soil evaporation compensation factor gw delay gw 0 500 groundwater delay days sol k sol 0 2000 saturated hydraulic conductivity ch d rte 0 30 the average depth of the main channel tlaps sub 10 10 temperature lapse rate timp bsn 0 05 0 9 snowpack temperature lag factor gwqmn gw 0 2 threshold depth of water in the shallow aquifer required for return flow to occur mm smfmx bsn 5 5 maximum melt rate for snow during the year occurs in summer gw revap gw 0 0 2 groundwater revap coefficient revapmn gw 0 500 threshold depth of water in the shallow aquifer for revap to occur mm ch s2 rte 0 001 10 the average slope of the main channel smfmn bsn 5 5 minimum melt rate for snow during the year occurs in winter sno50cov bsn 0 0 9 snow water equivalent that corresponds to 50 snow cover canmx hru 0 100 maximum canopy storage snocovmx bsn 0 500 minimum snow water content that corresponds to 100 snow cover ch n2 rte 0 0 3 manning s n value for the main channel sol alb sol 0 0 25 moist soil albedo surlag bsn 0 1 20 surface runoff lag time sftmp bsn 5 5 snowfall temperature epco bsn 0 0 9 plant uptake compensation factor appendix b table b 1 identified parameters of the meichuan river used for swat model calibration table b 1 parameter id min max parameter description cn2 35 98 scs runoff curve number alpha bnk 0 1 baseflow alpha factor for bank storage sol k 0 2000 saturated hydraulic conductivity ch k2 0 01 500 effective hydraulic conductivity in main channel alluvium canmx 0 100 maximum canopy storage sol awc 0 1 available water capacity of the soil layer sol bd 0 9 2 5 moist bulk density ch n2 0 0 3 manning s n value for the main channel 
25692,meta analysis power analysis and sensitivity analysis are widespread statistical techniques which can be correctly performed only if variability statistics such as standard deviation are available however standard deviations are often missing in published articles this work illustrates the functionality and the versatility of a newly developed excel tool for the standard deviation extraction from anova and multiple comparison test mct results the tool implements four methods which can be alternatively applied according to the available statistics usually reported in anova and or mct tables and graphs 1 least significant difference lsd 2 significance level p f 3 letters for means separation assigned by mct 4 a range of significance level indicated by stars the tool can be applied in one two and three way factorial experiments arranged in complete randomization randomized block split plot or split block the performances of the different methods were tested in a case study about meta analysis database preparation keywords data extraction standard deviation meta analysis excel tool anova 1 introduction in the scientific community the information sharing is increasingly becoming important for enhancing knowledge integration open access journals and several international peer review scientific publishers offer full access to scientific articles so that there is full availability of data and statistical analysis results which can be retrieved from such publications this information can be employed for meta analysis or for other data elaborations such as global sensitivity analysis and the estimation of the number of replications that are needed to obtain a desired power in a new experiment i e power analysis when data are retrieved from anova experiments the metric which is fundamental for performing the aforementioned types of analysis is the pooled standard deviation of the estimated means as it indicates the variability associated to the means koricheva et al 2013 note that this metric differs from the standard error associated to each mean because it is a single value that describes the pooled variability of all the means in a one way anova experiment the pooled standard deviation coincides with the residual standard deviation quinn and keough 2002 meta analysis is a powerful statistical methodology for synthesizing research evidence across independent studies and for this reason it has increasingly gained importance in environmental and agricultural research philibert et al 2012 valkama et al 2019 a key aspect of modern approaches to meta analysis is weighting each study s effect size by the inverse of its variance borenstein 2009 weir et al 2018 to avoid the bias produced by unweighted analyses koricheva et al 2013 however many published experiments failed to report sample sizes and variances related statistics and this aspect often hampers to include these studies in a meta analysis with this regard valkama et al 2019 reported that for a meta analysis on nitrogen retention by buffer zones about 200 out of 246 analysed articles were not suitable for inclusion in the meta analysis due to the lack of information about sample variances in another meta analysis on soil organic carbon changes due to cover cropping 80 out of 131 articles did not report standard deviations of means jian et al 2020 environmental and agricultural meta analyses often attempt to include the possible largest number of articles by using alternative methods to overcome the lacking information about standard deviation these alternative methods are often based on 1 performing unweighted meta analysis mcdaniel et al 2014 jian et al 2020 2 replacing the missing standard deviation with a given percentage of the mean frequently set to 10 luo et al 2010 gattinger et al 2012 garcía palacios et al 2018 or 3 weighting the effect size by the sample size maillard and angers 2014 morugán coronado et al 2020 when available information about data dispersion such the percentage of the range of values or interquartile range can be used to estimate standard deviation foscarini et al 2010 weir et al 2019 the application of the first two methods is to be discouraged as the resulting meta analysis is strongly biased due to the unvarying weight assignment similarly the third method may be suitable only in the case of equal variances across the studies however variances are rarely equal across studies so that this method can potentially introduce serious bias koricheva et al 2013 gurevitch et al 2018 in the meta analysis it was demonstrated by hungate et al 2009 that weighting by sample size is not a better solution than the unweighted analysis and both cause large distortion in the statistical tests besides meta analysis global sensitivity analysis of models also requires the value of the standard deviation and the mean of the input factors as input information saltelli et al 2004 confalonieri et al 2010 quillet et al 2013 similarly the value of the means standard deviation is fundamental for computing the replications number needed to set the power i e the probability to not committing a type ii error in an analysis of variance anova or regression based experiments ahrens and pearson 1974 lenth 2001 dagnelie 2013 when variability metrics are reported in published articles they are displayed in tables or graphs as error bars or written in the text these metrics are usually referred to a specific treatment and they are reported in form of standard deviation standard error of means or confidence intervals these metrics are equivalent because all allow to obtain the standard deviation of a treatment through the use of the following identity 1 s s x n 2 s u c l x t α 2 n 1 n where s standard deviation s x standard error of mean n sample size ucl upper confidence limit x mean value t α 2 student t value for a prefixed α level when these metrics are not reported a way to retrieve or estimate variability information is to use the data reported in the anova tables this analysis is the most frequently used statistical analysis in the environmental and agricultural research acutis et al 2012 although international journals e g nature publishing group https www nature com documents nr reporting summary flat pdf ask authors to report the standard deviations of the means or other numerically equivalent statistics and authors can rely on the use of guides for the anova result presentation mcintosh 2015 many publications still fail in reporting variability metrics as can be deduced from published meta analyses mondal et al 2020 jian et al 2020 valkama et al 2019 haddaway et al 2017 for instance haddaway et al 2017 additional file n 6 reported that the percentage of missing standard deviations was 40 and 58 in the two meta analyses about soil organic concentration and soil organic carbon stock respectively this missing information can be estimated as the pooled error standard deviation s w which can be considered as the standard deviation of the treatments means under the assumption of the homogeneity of variances in anova and in multiple comparison tests mct the main outcomes are computed as a function of s w however only a few authors proposed a method to retrieve s w from these outcomes thiessen philbrook et al 2007 koricheva et al 2013 as summarized by weir et al 2018 previous studies dealt with the extraction of standard deviation when it is missing some authors e g abrams et al 2005 sung et al 2006 made available the code for potentially implementing the extraction of standard deviations but a self standing tool has not been developed yet in addition none of the listed studies offers the options to retrieve standard deviations from experiments analysed with anova especially in complex experimental designs this latter point is of high importance in the environmental and agricultural fields of research as anova is the most utilized test in the statistical elaboration there is the opportunity to valorise the hidden information in many published articles none of the published approaches allow for extracting the standard deviations from the mct the objective of this study is to provide an easy to use excel worksheet to compute s w and or its upper and lower limits using feasible methods to obtain s w from the anova and mct outcomes the software tool allows the user to obtain s w from one two and three way anova factorial experiments in completely randomized design crd completely randomized block design crbd split plot split block or split split plot design the study also provides the theoretical background of each of the four implemented methods 2 material and methods 2 1 methods for estimation of s w and its uncertainty the ex tract tool offers four methods to estimate s w according to the available information of anova or mct results reported in the published articles the four methods were identified because they are the statistics most often employed and reported in published articles the first three methods were chosen as they are based on all the possible statistics that are computed in experiments analysed with anova and from which it is possible to extract s w the fourth method was chosen because mcts results are also frequently reported in the published studies and it has never used before to estimate s w the four methods use the following input information i the lsd least significant difference values ii the p f i e the type i probability error values iii the letters assignment indicating differences among means based on the result of commonly used mct such as lsd tukey 1953 duncan 1955 snk keuls 1952 and regwq ryan 1960 einot and gabriel 1975 welsch 1977 tests iv stars used as a shorthand to indicate significance levels all these methods estimate an uncertainty range of s w i e l l s w and u l s w being the lower and upper limits of s w respectively which includes its true value the first two methods have an uncertainty that is only due to the rounding errors in the input data the uncertainty of the other two methods is due to the fact that they do not rely on values such as the lsd or the p f value but on the classification of letter labelled means and ranges of probabilities respectively the four methods were developed for balanced experiments i e equal number of replications for each treatment under the anova assumption of homogeneity of variances the application of these methods requires to indicate the experimental design the number of levels for each factor which is included in an experiment and the number of replications the type of experimental design is needed to retrieve the degrees of freedom of the error i e the degrees of freedom within the groups dfw and the number of replications used to compute mean values moreover all the methods except the one using the lsd value also require the values of the means of the source of variation to perform the computation of s w 2 1 1 from lsd method i this method relies on the availability of the lsd value which is commonly reported in tables and graphs lsd is used as a threshold to define whether two means significantly differ or not for balanced experiments lsd is defined as 3 l s d t α 2 d f w 2 s w 2 n where t α 2 dfw is the student t value for a selected significance level α and n is the number of replications used to compute the means fisher 1935 consequently 4 s w l s d 2 n 2 t a 2 d f w 2 the uncertainty range of s w depends only on the number of the significant digits which are used in reporting the lsd value or derived from extracting the lsd value from a graph details about the uncertainty range calculation are given in appendix a 2 1 2 from p f method ii the p f value defines the probability level of a false rejection of a null hypothesis for a specific effect being the source of variation under examination the method to compute s w from the p f value is because the f statistic is the between groups mean square s b 2 over the within groups mean square s w 2 ratio in a one way balanced anova s b 2 is computed as 5 s b 2 n i 1 n t x i x 2 where n t is the number of treatments x i is the i th treatment mean and x is the grand mean then s w is computed as 6 s w s b 2 i n v f α d f b d f w where invf is the inverse of the f cumulative distribution function dfb is the between groups degrees of freedom and α is the available value of p f formulas for calculating s b 2 for the main effects and interactions in multi way experiments can be found in basic statistics textbooks e g sokal and rohlf 2012 the uncertainty in the estimate of s w is due to the number of the significant digits used to report the means of treatments and the p f value the procedure to assess the uncertainty of s w estimation is reported in appendix b 2 1 3 from letters method iii mct results are commonly presented by labelling means in graphs and tables with the same letter if they are not significantly different quinn and keough 2002 this method can be applied if letter labelled means are available in a published article it computes two finite boundaries including the true value of s w when at least two means are labelled with different letters and two means are labelled with the same letter in the case of no significant differences between means all the means share the same letter only a minimum value of s w can be defined being the upper boundary equal to when all the means are classified as different i e all the means are labelled with different letters only an upper boundary could be determined being the lower boundary equal to zero this method implements five mcts lsd tukey duncan student newman keuls snk ryan einot gabriel welsh studentized range q regwq it works in three steps for all the mcts in case of lsd the procedure is as follows 1 to seek for the minimum value of a difference between means that is declared significantly different i e to choose the minimum difference between two means that do not have any letter in common assuming this value as the lsd s w is calculated according to method i and represents the upper limit u l s w for s w estimation this is the highest value of s w that is possible to assume 2 to seek for the maximum difference that is declared not significantly different i e the maximum difference between two means that have at least one letter in common and to use it as lsd value to compute s w according to the method i this is the lowest value that is possible to assume so it represents the lower limit l l s w for s w 3 to decide what s w outcome is the most suitable when the user intends to make a conservative choice the user has to take the u l s w as the s w value conversely the mean between u l s w and l l s w or the l l s w value can be taken for a more liberal choice when tukey test is used the value of lsd is replaced by the so called honest lsd i e hsd this value is computed as 7 h s d q α k d f e s w 2 n where q is the studentized range distribution and k is the number of all the means included in the experiment consequently 8 s w n h s d 2 q α k d f e 2 for the snk duncan and regwq tests being step down tests s w is computed from 9 s w n c v a l 2 q α k d f e 2 where 10 c v a l q α p d f e s w 2 n with p being the number of means whose range is to be sequentially tested and a the adjusted significance level for a test of the equality of p means day and quinn 1989 the a is differently defined for each test for the snk test α α for the duncan test α 1 1 α p 1 for the regwq test α 1 1 α p k to calculate the q value for the a probability for each mct we used the theoretical method proposed by gleason 1998 1999 and implemented in an excel code snippet proposed by klasson 2018 an exact value for s w could not be obtained with this method only the limits of the s w range are computed with this method according to the difference between the means labelled with different and common letters in particular the uncertainty is reduced when the values of means are close enough to detect significant and non significant differences within a narrow range 2 1 4 from stars method iv frequently in scientific articles simple indication like or are reported instead of the p f value these symbols can have different correspondence to the significance level in different disciplines journals and statistical packages therefore this method requires to know what level of significance corresponds to the symbol used three conventional significance codes associating p f and stars are proposed in the tool table 1 being the most common cases in scientific and technical literature it is possible to obtain a bounded estimate of s w only when the reported stars indicate a range of p f otherwise one of the limits is zero or table 1 2 2 tool applicability the ex tract tool allows to operate with the most common experimental designs one two and three way anova in complete randomized design crd complete randomized block design crbd split plot split split plot or split block design table 2 the tool allows to work with a maximum of 16 means being the levels of the source of variation with an unlimited number of replications in factorial experiments i e where all possible main effects and interactions in multi way experiments to obtain the exact value or range of s w the ex tract tool also requires the selection of the desired available source of variation the methods from p f and from stars method ii and iv can be used for all experimental main factors and interactions while methods from lsd and from letters method i and iii can be used only in cases reported in table 2 2 3 ex tract tool implementation we chose to code the ex tract tool in the excel environment following the user centered design ucd paradigm barnum 2011 which is an iso standard 9241 210 2019 and it is based on a participatory approach according to this a panel of users i e phd students junior and senior researchers were involved in order to provide feedbacks in an iterative design process the most reported requirement was the need of simple tool with this regard the excel software is a well known environment and therefore users of different backgrounds can use the tool functionalities in an efficient way without the need of knowing dedicated statistical packages such packages are more complex than a spreadsheet based solution as they require coding skills and in depth knowledge about the statistics theory behind each test moreover the ex tract tool allows for the visualization of input and output within the same page and for an automatic saving of the extraction results the user interface of the ex tract tool has a clear arrangement of the input and output data and includes a contextual help system a detailed user manual is available in the supplemental material s1 finally we have also provided a set of video tutorials to present the main features of the tool ex tract results of the s w extraction can be automatically saved in a sheet which is structured as a database 2 4 evaluation of the tool utility a case study to evaluate the ex tract tool capability in extracting s w we applied it under the hypothesis of increasing the number of articles which can be used for carrying out a meta analysis therefore we created a database which can be potentially used in a meta analysis for detecting the effect of conservation agriculture on soil organic carbon stock according to koricheva et al 2013 a four step procedure was adopted 1 to perform a systematic search in scientific bibliography databases i e scopus and wos 2 to select the studies that are potentially suitable for a meta analysis in which measurements were available from clearly defined controls and treatment groups 3 to detect the standard deviation of control and treatment from a given study note that an article may report more than one study 4 to identify articles that were selected in the second step but did not report explicitly the standard deviation of control and treatment in this case it may possible to overcome this missing information by estimating s w on the basis of the available anova model features using the ex tract tool for the evaluation of methods performance we used the two coefficients of variation of the s w upper and lower limits c v l o w e r and c v u p p e r which were calculated as follows 11 c v l o w e r l l s w x c t r l x t r e a t 2 12 c v u p p e r u l s w x c t r l x t r e a t 2 where l l s w an u l s w are the lower and upper limits of s w while x c t r l and x t r e a t are the observed means of the control and the treatment respectively to assess the standard deviation uncertainty when data are directly retrieved we used the rounding error while we used the digitizer error along with the rounding error when data were extracted from a graph 3 illustrative results and discussion the main result of the present work is a tool that estimates s w from anova experiments and mct this tool implements and extends the s w estimation approaches already proposed by thiessen philbrook et al 2007 and koricheva et al 2013 to a wide range of experimental designs including 3 way anova which are typically used in the environmental sciences moreover the tool implements new and original methods to estimate s w on the basis on mct results the following paragraphs describe the interface the code validation the performance evaluation of the four implemented methods and the results of a case study 3 1 tool interface as a result of the ucd approach the developed excel spreadsheet shows an easy to use interface in the ex tract tool interface input and output are displayed together in one page here the user is guided in the data entry phase and supported by a help system drop down lists offer the available choices for each input feature and an automatic check ensures the input data consistency the tool consists of six sheets an introductory screen with general information and method selection fig 1 four sheets for the extraction of s w value and or limits i e from lsd from p f from letters and from stars and the database sheet in which the article or experiment reference are automatically stored along with the associated output 3 1 1 input the four s w extraction methods require the following data as common input fig 2 experiment type source of variation number of levels of each factor of the anova experiment and the number of replications note that the orange colour of the cell indicates a drop down list of available options besides the common input each method requires specific inputs the from lsd method i requires the value of the lsd and the probability level used to compute it the from p f method ii requires the p f value the mean values of the levels of the source of variation mls under analysis and the means value approximation rounding the from letters method iii requires mls the associated letters the type of mct and the significance level used to perform the mct the from stars method iv requires mls and the p f value or range which corresponds to the star significance codes adopted in the article details about the specific input required for each method are reported in the user manual supplemental material s1 and in the video tutorials available in the ex tract youtube channel https www youtube com channel ucky xnopyhs0pcr5exhsv a 3 1 2 output along with the estimated s w and its range the tool returns the degrees of freedom and the number of replications used to compute the means the s w is differently reported in the output section according to the different approaches described in paragraph 2 1 for lsd and p f methods fig 3 a the output shows the estimated values of s w with the l l s w and u l s w in the interface named min and max respectively while for letters and star only l l s w and u l s w are reported fig 3b 3 2 tool validation and debugging the correct coding of the algorithms in the tool was validated according to the following procedure 1 generation of a set of synthetic experiments with s w known according to the different experimental schemes handled by the tool e g rcdb split plot etc varying the number of factors up to three and levels 2 performing an anova analysis and different mct for each experiment using the ibm spss statistical package version 26 1 3 applying the ex tract tool to compute the pooled s w based on the data of the same experiments in the debugging phase we performed a comparison between the tool output and the known value of s w which proved that the tool implementation was correct more than 200 simulated experiments were analysed before the tool application to real cases 3 3 methods performances estimation the s w estimation from lsd method i is the most reliable method because of it is an exact computation of the standard deviations the only uncertainty comes from the precision in terms of the number of significant digits used to express the lsd value most of the time the lsd value is reported with two or more digits in this case the uncertainty is for sure acceptable see appendix 1 for practical determination of standard deviation for meta analysis sample size determination or sensitivity analysis as for the first method the estimation of s w computed from the p f method ii value follows an exact procedure see appendix 2 for the computation of uncertainties of the procedure the p f and means values expressed with only one significant digit produce large uncertainty in s w estimation while when they are expressed with two significant digits uncertainty is low in most of the cases as already stated by thiessen philbrook et al 2007 in medical field obtaining the estimation of s w from letters method iii is an interesting perspective because mct comparisons and the consequent letters assignment are widespread in scientific publications even if the uncertainty of estimation is unpredictable depending mainly on the distribution of the treatments means the user is then free to alternatively keep the central value of the s w range or the upper limit in the case a conservative approach is required two cases can also arise while using this method a all the means are declared as different so that the lower limit of s w results equal to zero and b all the means are declared not different so that the upper limit of s w results equal to in the former case the conservative approach suggests using the upper limit of s w in the latter case the user can follow a liberal approach and keep the lower limit of s w to our knowledge this is the first time that a method to estimate s w from the multiple comparison letters attribution is proposed the method iv stars is the option that yields the highest uncertainty for s w estimation due to the limited input information i e p f below a threshold or included between two values as for this method the greatest input information is given when stars indicate a p f included within two boundaries e g when indicates 0 01 p f 0 05 which allows the method to compute two defined limits both different from 0 and in the other cases this method can compute only one s w boundary similarly to the letters method the uncertainty of the estimation is unpredictable being dependent on the values of the means and significance levels available from the anova analysis 3 4 tool utility a case study the case study concerned the database creation for a meta analysis regarding the effects of conservation agriculture on soil fertility table 3 to populate the database standard deviations of the means were directly retrieved from published articles or estimated by the ex tract tool the final database consisted of 43 studies published in literature that meet the meta analytic requirements listed by koricheva et al 2013 studies from 1 to 18 reported the standard deviations of treatments in the text in tables or as graphical bars studies from 19 to 43 required ex tract to compute the s w in the latter case the method column defines the specific tool method utilized table 3 means of control and treatment s w data the exact value or the ll s w and ul s w and coefficient of variation cv of the two boundaries are also displayed in table 3 this example showed that ex tract tool allowed to obtain the minimum number of studies i e 30 40 required to perform a meta analysis valentine et al 2010 the global influence of a single data in a meta analysis depends on the ratio of s w over mean i e the coefficient of variation cv the power analysis also relies on the use of cv as shown in table 3 cv had a considerable variation across the 43 experiments from 0 5 to 34 9 the lowest is the cv range cv u l s w cv l l s w the most reliable the s w estimate the extent of the cv range depends on the method of extraction direct and lsd methods allow to obtain narrow cv ranges for which the maximum value of range was 0 04 and 0 1 respectively when standard deviation is estimated from a graph the uncertainty is still low with a maximum range value of 0 33 as for the p f method only one experiment was found however the result of p f indicates a low cv range 0 26 and this agrees with the fact that is an exact procedure leading to a s w value with limited uncertainty with the s w extraction from letters it was possible to estimate the uncertainty i e the cv range in seven out of ten studies since in three cases one of the limits was zero or this method performed worse than in the lsd and p f methods having a mean cv range of 4 21 with the s w extraction from star it was possible to estimate the uncertainty in three out of six studies since in three cases one of the limits was zero or due to the limited input information used with the star method the mean cv range 9 41 was much higher than the other methods given that the uncertainty obtained using letters and star was larger the conservative approach suggests using the upper limit of the estimated s w although this conservative approach reduces the weight of a specific study in the meta analysis it allows to include the study in the analysis this approach is recommended by weir et al 2018 who studied the effect of different approach of standard deviation on the reliability of meta analysis results the same authors recommended to use several methods which led to standard deviation values comparable to those estimated in our study with letter and stars methods as a final consideration we encourage the user to apply the best method being lsd and p f methods when the required input information is available 4 conclusions standard deviation is a strictly required statistic for proper data processing such as meta analysis sensitivity analysis and power analysis the present work illustrates the functionality of an excel tool ex tract which allows to estimate the pooled standard deviation of existing experimental datasets when this value is not directly reported this tool is capable to extract standard deviation from anova and post anova multiple comparison tests being the most common test in the environmental research by applying four methods which can be chosen according to the availability of one of the following information lsd values p f values letters used to classify means five multiple comparison tests are managed by the tool and stars attribution to indicate significance level the flexibility of the ex tract tool is also ensured by the wide range of experiment types one two three ways and design schemes crd crdb split plot strip block in which standard deviation can be extracted the lsd and p f methods are capable to estimate standard deviation with a defined level of uncertainty which is comparable to the one directly reported in tables or graphs although the other two methods letters and stars are associated with a higher degree of uncertainty they offer the possibility to derive standard deviation from those datasets which would be otherwise neglected the application to a real case demonstrates that the tool allows to double the number of studies that can be included in a meta analysis therefore the ex tract tool offers an operational facilitation in meta analytic research valorising the hidden information in published articles increasing the number of suitable studies and avoiding questionable procedure of study weighting software availability name of software ex tract developers andrea di guardo andrea diguardo unimib it marco acutis marco acutis unimi it year first available 2021 language vba for excel version 2016 2019 and 365 availability https doi org 10 6084 m9 figshare 14987130 supported systems microsoft windows macos licence cc by 4 0 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is a part of the project sommit sustainable management of soil organic matter to mitigate trade offs between c sequestration and nitrous oxide methane and nitrate losses that received funding from the european joint programme soil grant agreement id 862695 this study is also funded by the european union s horizon 2020 framework programme for research and innovation h2020 rur 2017 2 as part of the landsupport project grant agreement no 774234 which aims at developing a decision support system for optimizing soil management in europe the work is also financially supported by the doctoral school of agriculture environment and bioenergy and of the university of milan appendix a supplementary data the following is are the supplementary data to this article s2 video tutorials at https drive google com drive folders 1pr bcjbfco1dgndxjnfsyglwhzvikp5x multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105236 appendix a uncertainty calculation in the estimation of s w when lsd value is available method i a let lsd apx be the approximation value of lsd that is automatically computed by the software based on the number of significant digits of lsd e g 0 5 b let min lsd lsd lsd apx such as lsd round lsd apx c let max lsd lsd lsd apx such as lsd round lsd apx the results of b and c are then plugged into the equation 4 to obtain the lower and the upper limits of s w note that in this case the ratio r l l s w l l s w s w 100 depends only on the number of significant digits used to report lsd or from reading the length of lsd if the value is obtained from a graph and from the lsd value itself defining lsd as the significant digits of the lsd written in the significant normalized scientific notation fleisch and kregenow 2014 without the decimal point the r value is given by 100 lsd the tool considers the final zeroes of a number integer or decimal as significant for instance if the value of the lsd is 1 the normalized scientific notation is 1 100 and so lsd 1 the r is 100 if the lsd is 2 2 the normalized scientific notation is 2 2 100 and lsd 22 the r is 4 5 if lsd is 99 9 the normalized scientific notation is 9 99 101 and lsd 999 the r is 0 001 appendix b uncertainty calculation in the estimation of s w when p f value is available let apx be the approximation of mean values set by user e g 0 005 for each x i a let min x i be the smallest value such as the rounding of x i is obtained using the approximation value apx i e round min x i apx x i b let max x i be the greatest value such as the rounding of x i is obtained using the approximation value apx i e round max x i apx x i and let a pfapx be the approximation value of p f that is automatically computed by the software based on the number of significant digits of p f b min p f be the smallest values such as the rounding of p f is obtained using the approximation value pfapx i e round p f pfapx p f c max p f be the greatest values such as the rounding of p f is obtained using the approximation value pfapx i e round p f pfapx p f then the minimum value of s w can be estimated by replacing each x i x with min x i and each x i x with max x i in formula 5 and using p f min as p f value in the formula 6 similarly the maximum value of s w can be estimated replacing each x i x with max x i and each x i x with min x i in formula 5 and using p f min as p f value in formula 6 
25692,meta analysis power analysis and sensitivity analysis are widespread statistical techniques which can be correctly performed only if variability statistics such as standard deviation are available however standard deviations are often missing in published articles this work illustrates the functionality and the versatility of a newly developed excel tool for the standard deviation extraction from anova and multiple comparison test mct results the tool implements four methods which can be alternatively applied according to the available statistics usually reported in anova and or mct tables and graphs 1 least significant difference lsd 2 significance level p f 3 letters for means separation assigned by mct 4 a range of significance level indicated by stars the tool can be applied in one two and three way factorial experiments arranged in complete randomization randomized block split plot or split block the performances of the different methods were tested in a case study about meta analysis database preparation keywords data extraction standard deviation meta analysis excel tool anova 1 introduction in the scientific community the information sharing is increasingly becoming important for enhancing knowledge integration open access journals and several international peer review scientific publishers offer full access to scientific articles so that there is full availability of data and statistical analysis results which can be retrieved from such publications this information can be employed for meta analysis or for other data elaborations such as global sensitivity analysis and the estimation of the number of replications that are needed to obtain a desired power in a new experiment i e power analysis when data are retrieved from anova experiments the metric which is fundamental for performing the aforementioned types of analysis is the pooled standard deviation of the estimated means as it indicates the variability associated to the means koricheva et al 2013 note that this metric differs from the standard error associated to each mean because it is a single value that describes the pooled variability of all the means in a one way anova experiment the pooled standard deviation coincides with the residual standard deviation quinn and keough 2002 meta analysis is a powerful statistical methodology for synthesizing research evidence across independent studies and for this reason it has increasingly gained importance in environmental and agricultural research philibert et al 2012 valkama et al 2019 a key aspect of modern approaches to meta analysis is weighting each study s effect size by the inverse of its variance borenstein 2009 weir et al 2018 to avoid the bias produced by unweighted analyses koricheva et al 2013 however many published experiments failed to report sample sizes and variances related statistics and this aspect often hampers to include these studies in a meta analysis with this regard valkama et al 2019 reported that for a meta analysis on nitrogen retention by buffer zones about 200 out of 246 analysed articles were not suitable for inclusion in the meta analysis due to the lack of information about sample variances in another meta analysis on soil organic carbon changes due to cover cropping 80 out of 131 articles did not report standard deviations of means jian et al 2020 environmental and agricultural meta analyses often attempt to include the possible largest number of articles by using alternative methods to overcome the lacking information about standard deviation these alternative methods are often based on 1 performing unweighted meta analysis mcdaniel et al 2014 jian et al 2020 2 replacing the missing standard deviation with a given percentage of the mean frequently set to 10 luo et al 2010 gattinger et al 2012 garcía palacios et al 2018 or 3 weighting the effect size by the sample size maillard and angers 2014 morugán coronado et al 2020 when available information about data dispersion such the percentage of the range of values or interquartile range can be used to estimate standard deviation foscarini et al 2010 weir et al 2019 the application of the first two methods is to be discouraged as the resulting meta analysis is strongly biased due to the unvarying weight assignment similarly the third method may be suitable only in the case of equal variances across the studies however variances are rarely equal across studies so that this method can potentially introduce serious bias koricheva et al 2013 gurevitch et al 2018 in the meta analysis it was demonstrated by hungate et al 2009 that weighting by sample size is not a better solution than the unweighted analysis and both cause large distortion in the statistical tests besides meta analysis global sensitivity analysis of models also requires the value of the standard deviation and the mean of the input factors as input information saltelli et al 2004 confalonieri et al 2010 quillet et al 2013 similarly the value of the means standard deviation is fundamental for computing the replications number needed to set the power i e the probability to not committing a type ii error in an analysis of variance anova or regression based experiments ahrens and pearson 1974 lenth 2001 dagnelie 2013 when variability metrics are reported in published articles they are displayed in tables or graphs as error bars or written in the text these metrics are usually referred to a specific treatment and they are reported in form of standard deviation standard error of means or confidence intervals these metrics are equivalent because all allow to obtain the standard deviation of a treatment through the use of the following identity 1 s s x n 2 s u c l x t α 2 n 1 n where s standard deviation s x standard error of mean n sample size ucl upper confidence limit x mean value t α 2 student t value for a prefixed α level when these metrics are not reported a way to retrieve or estimate variability information is to use the data reported in the anova tables this analysis is the most frequently used statistical analysis in the environmental and agricultural research acutis et al 2012 although international journals e g nature publishing group https www nature com documents nr reporting summary flat pdf ask authors to report the standard deviations of the means or other numerically equivalent statistics and authors can rely on the use of guides for the anova result presentation mcintosh 2015 many publications still fail in reporting variability metrics as can be deduced from published meta analyses mondal et al 2020 jian et al 2020 valkama et al 2019 haddaway et al 2017 for instance haddaway et al 2017 additional file n 6 reported that the percentage of missing standard deviations was 40 and 58 in the two meta analyses about soil organic concentration and soil organic carbon stock respectively this missing information can be estimated as the pooled error standard deviation s w which can be considered as the standard deviation of the treatments means under the assumption of the homogeneity of variances in anova and in multiple comparison tests mct the main outcomes are computed as a function of s w however only a few authors proposed a method to retrieve s w from these outcomes thiessen philbrook et al 2007 koricheva et al 2013 as summarized by weir et al 2018 previous studies dealt with the extraction of standard deviation when it is missing some authors e g abrams et al 2005 sung et al 2006 made available the code for potentially implementing the extraction of standard deviations but a self standing tool has not been developed yet in addition none of the listed studies offers the options to retrieve standard deviations from experiments analysed with anova especially in complex experimental designs this latter point is of high importance in the environmental and agricultural fields of research as anova is the most utilized test in the statistical elaboration there is the opportunity to valorise the hidden information in many published articles none of the published approaches allow for extracting the standard deviations from the mct the objective of this study is to provide an easy to use excel worksheet to compute s w and or its upper and lower limits using feasible methods to obtain s w from the anova and mct outcomes the software tool allows the user to obtain s w from one two and three way anova factorial experiments in completely randomized design crd completely randomized block design crbd split plot split block or split split plot design the study also provides the theoretical background of each of the four implemented methods 2 material and methods 2 1 methods for estimation of s w and its uncertainty the ex tract tool offers four methods to estimate s w according to the available information of anova or mct results reported in the published articles the four methods were identified because they are the statistics most often employed and reported in published articles the first three methods were chosen as they are based on all the possible statistics that are computed in experiments analysed with anova and from which it is possible to extract s w the fourth method was chosen because mcts results are also frequently reported in the published studies and it has never used before to estimate s w the four methods use the following input information i the lsd least significant difference values ii the p f i e the type i probability error values iii the letters assignment indicating differences among means based on the result of commonly used mct such as lsd tukey 1953 duncan 1955 snk keuls 1952 and regwq ryan 1960 einot and gabriel 1975 welsch 1977 tests iv stars used as a shorthand to indicate significance levels all these methods estimate an uncertainty range of s w i e l l s w and u l s w being the lower and upper limits of s w respectively which includes its true value the first two methods have an uncertainty that is only due to the rounding errors in the input data the uncertainty of the other two methods is due to the fact that they do not rely on values such as the lsd or the p f value but on the classification of letter labelled means and ranges of probabilities respectively the four methods were developed for balanced experiments i e equal number of replications for each treatment under the anova assumption of homogeneity of variances the application of these methods requires to indicate the experimental design the number of levels for each factor which is included in an experiment and the number of replications the type of experimental design is needed to retrieve the degrees of freedom of the error i e the degrees of freedom within the groups dfw and the number of replications used to compute mean values moreover all the methods except the one using the lsd value also require the values of the means of the source of variation to perform the computation of s w 2 1 1 from lsd method i this method relies on the availability of the lsd value which is commonly reported in tables and graphs lsd is used as a threshold to define whether two means significantly differ or not for balanced experiments lsd is defined as 3 l s d t α 2 d f w 2 s w 2 n where t α 2 dfw is the student t value for a selected significance level α and n is the number of replications used to compute the means fisher 1935 consequently 4 s w l s d 2 n 2 t a 2 d f w 2 the uncertainty range of s w depends only on the number of the significant digits which are used in reporting the lsd value or derived from extracting the lsd value from a graph details about the uncertainty range calculation are given in appendix a 2 1 2 from p f method ii the p f value defines the probability level of a false rejection of a null hypothesis for a specific effect being the source of variation under examination the method to compute s w from the p f value is because the f statistic is the between groups mean square s b 2 over the within groups mean square s w 2 ratio in a one way balanced anova s b 2 is computed as 5 s b 2 n i 1 n t x i x 2 where n t is the number of treatments x i is the i th treatment mean and x is the grand mean then s w is computed as 6 s w s b 2 i n v f α d f b d f w where invf is the inverse of the f cumulative distribution function dfb is the between groups degrees of freedom and α is the available value of p f formulas for calculating s b 2 for the main effects and interactions in multi way experiments can be found in basic statistics textbooks e g sokal and rohlf 2012 the uncertainty in the estimate of s w is due to the number of the significant digits used to report the means of treatments and the p f value the procedure to assess the uncertainty of s w estimation is reported in appendix b 2 1 3 from letters method iii mct results are commonly presented by labelling means in graphs and tables with the same letter if they are not significantly different quinn and keough 2002 this method can be applied if letter labelled means are available in a published article it computes two finite boundaries including the true value of s w when at least two means are labelled with different letters and two means are labelled with the same letter in the case of no significant differences between means all the means share the same letter only a minimum value of s w can be defined being the upper boundary equal to when all the means are classified as different i e all the means are labelled with different letters only an upper boundary could be determined being the lower boundary equal to zero this method implements five mcts lsd tukey duncan student newman keuls snk ryan einot gabriel welsh studentized range q regwq it works in three steps for all the mcts in case of lsd the procedure is as follows 1 to seek for the minimum value of a difference between means that is declared significantly different i e to choose the minimum difference between two means that do not have any letter in common assuming this value as the lsd s w is calculated according to method i and represents the upper limit u l s w for s w estimation this is the highest value of s w that is possible to assume 2 to seek for the maximum difference that is declared not significantly different i e the maximum difference between two means that have at least one letter in common and to use it as lsd value to compute s w according to the method i this is the lowest value that is possible to assume so it represents the lower limit l l s w for s w 3 to decide what s w outcome is the most suitable when the user intends to make a conservative choice the user has to take the u l s w as the s w value conversely the mean between u l s w and l l s w or the l l s w value can be taken for a more liberal choice when tukey test is used the value of lsd is replaced by the so called honest lsd i e hsd this value is computed as 7 h s d q α k d f e s w 2 n where q is the studentized range distribution and k is the number of all the means included in the experiment consequently 8 s w n h s d 2 q α k d f e 2 for the snk duncan and regwq tests being step down tests s w is computed from 9 s w n c v a l 2 q α k d f e 2 where 10 c v a l q α p d f e s w 2 n with p being the number of means whose range is to be sequentially tested and a the adjusted significance level for a test of the equality of p means day and quinn 1989 the a is differently defined for each test for the snk test α α for the duncan test α 1 1 α p 1 for the regwq test α 1 1 α p k to calculate the q value for the a probability for each mct we used the theoretical method proposed by gleason 1998 1999 and implemented in an excel code snippet proposed by klasson 2018 an exact value for s w could not be obtained with this method only the limits of the s w range are computed with this method according to the difference between the means labelled with different and common letters in particular the uncertainty is reduced when the values of means are close enough to detect significant and non significant differences within a narrow range 2 1 4 from stars method iv frequently in scientific articles simple indication like or are reported instead of the p f value these symbols can have different correspondence to the significance level in different disciplines journals and statistical packages therefore this method requires to know what level of significance corresponds to the symbol used three conventional significance codes associating p f and stars are proposed in the tool table 1 being the most common cases in scientific and technical literature it is possible to obtain a bounded estimate of s w only when the reported stars indicate a range of p f otherwise one of the limits is zero or table 1 2 2 tool applicability the ex tract tool allows to operate with the most common experimental designs one two and three way anova in complete randomized design crd complete randomized block design crbd split plot split split plot or split block design table 2 the tool allows to work with a maximum of 16 means being the levels of the source of variation with an unlimited number of replications in factorial experiments i e where all possible main effects and interactions in multi way experiments to obtain the exact value or range of s w the ex tract tool also requires the selection of the desired available source of variation the methods from p f and from stars method ii and iv can be used for all experimental main factors and interactions while methods from lsd and from letters method i and iii can be used only in cases reported in table 2 2 3 ex tract tool implementation we chose to code the ex tract tool in the excel environment following the user centered design ucd paradigm barnum 2011 which is an iso standard 9241 210 2019 and it is based on a participatory approach according to this a panel of users i e phd students junior and senior researchers were involved in order to provide feedbacks in an iterative design process the most reported requirement was the need of simple tool with this regard the excel software is a well known environment and therefore users of different backgrounds can use the tool functionalities in an efficient way without the need of knowing dedicated statistical packages such packages are more complex than a spreadsheet based solution as they require coding skills and in depth knowledge about the statistics theory behind each test moreover the ex tract tool allows for the visualization of input and output within the same page and for an automatic saving of the extraction results the user interface of the ex tract tool has a clear arrangement of the input and output data and includes a contextual help system a detailed user manual is available in the supplemental material s1 finally we have also provided a set of video tutorials to present the main features of the tool ex tract results of the s w extraction can be automatically saved in a sheet which is structured as a database 2 4 evaluation of the tool utility a case study to evaluate the ex tract tool capability in extracting s w we applied it under the hypothesis of increasing the number of articles which can be used for carrying out a meta analysis therefore we created a database which can be potentially used in a meta analysis for detecting the effect of conservation agriculture on soil organic carbon stock according to koricheva et al 2013 a four step procedure was adopted 1 to perform a systematic search in scientific bibliography databases i e scopus and wos 2 to select the studies that are potentially suitable for a meta analysis in which measurements were available from clearly defined controls and treatment groups 3 to detect the standard deviation of control and treatment from a given study note that an article may report more than one study 4 to identify articles that were selected in the second step but did not report explicitly the standard deviation of control and treatment in this case it may possible to overcome this missing information by estimating s w on the basis of the available anova model features using the ex tract tool for the evaluation of methods performance we used the two coefficients of variation of the s w upper and lower limits c v l o w e r and c v u p p e r which were calculated as follows 11 c v l o w e r l l s w x c t r l x t r e a t 2 12 c v u p p e r u l s w x c t r l x t r e a t 2 where l l s w an u l s w are the lower and upper limits of s w while x c t r l and x t r e a t are the observed means of the control and the treatment respectively to assess the standard deviation uncertainty when data are directly retrieved we used the rounding error while we used the digitizer error along with the rounding error when data were extracted from a graph 3 illustrative results and discussion the main result of the present work is a tool that estimates s w from anova experiments and mct this tool implements and extends the s w estimation approaches already proposed by thiessen philbrook et al 2007 and koricheva et al 2013 to a wide range of experimental designs including 3 way anova which are typically used in the environmental sciences moreover the tool implements new and original methods to estimate s w on the basis on mct results the following paragraphs describe the interface the code validation the performance evaluation of the four implemented methods and the results of a case study 3 1 tool interface as a result of the ucd approach the developed excel spreadsheet shows an easy to use interface in the ex tract tool interface input and output are displayed together in one page here the user is guided in the data entry phase and supported by a help system drop down lists offer the available choices for each input feature and an automatic check ensures the input data consistency the tool consists of six sheets an introductory screen with general information and method selection fig 1 four sheets for the extraction of s w value and or limits i e from lsd from p f from letters and from stars and the database sheet in which the article or experiment reference are automatically stored along with the associated output 3 1 1 input the four s w extraction methods require the following data as common input fig 2 experiment type source of variation number of levels of each factor of the anova experiment and the number of replications note that the orange colour of the cell indicates a drop down list of available options besides the common input each method requires specific inputs the from lsd method i requires the value of the lsd and the probability level used to compute it the from p f method ii requires the p f value the mean values of the levels of the source of variation mls under analysis and the means value approximation rounding the from letters method iii requires mls the associated letters the type of mct and the significance level used to perform the mct the from stars method iv requires mls and the p f value or range which corresponds to the star significance codes adopted in the article details about the specific input required for each method are reported in the user manual supplemental material s1 and in the video tutorials available in the ex tract youtube channel https www youtube com channel ucky xnopyhs0pcr5exhsv a 3 1 2 output along with the estimated s w and its range the tool returns the degrees of freedom and the number of replications used to compute the means the s w is differently reported in the output section according to the different approaches described in paragraph 2 1 for lsd and p f methods fig 3 a the output shows the estimated values of s w with the l l s w and u l s w in the interface named min and max respectively while for letters and star only l l s w and u l s w are reported fig 3b 3 2 tool validation and debugging the correct coding of the algorithms in the tool was validated according to the following procedure 1 generation of a set of synthetic experiments with s w known according to the different experimental schemes handled by the tool e g rcdb split plot etc varying the number of factors up to three and levels 2 performing an anova analysis and different mct for each experiment using the ibm spss statistical package version 26 1 3 applying the ex tract tool to compute the pooled s w based on the data of the same experiments in the debugging phase we performed a comparison between the tool output and the known value of s w which proved that the tool implementation was correct more than 200 simulated experiments were analysed before the tool application to real cases 3 3 methods performances estimation the s w estimation from lsd method i is the most reliable method because of it is an exact computation of the standard deviations the only uncertainty comes from the precision in terms of the number of significant digits used to express the lsd value most of the time the lsd value is reported with two or more digits in this case the uncertainty is for sure acceptable see appendix 1 for practical determination of standard deviation for meta analysis sample size determination or sensitivity analysis as for the first method the estimation of s w computed from the p f method ii value follows an exact procedure see appendix 2 for the computation of uncertainties of the procedure the p f and means values expressed with only one significant digit produce large uncertainty in s w estimation while when they are expressed with two significant digits uncertainty is low in most of the cases as already stated by thiessen philbrook et al 2007 in medical field obtaining the estimation of s w from letters method iii is an interesting perspective because mct comparisons and the consequent letters assignment are widespread in scientific publications even if the uncertainty of estimation is unpredictable depending mainly on the distribution of the treatments means the user is then free to alternatively keep the central value of the s w range or the upper limit in the case a conservative approach is required two cases can also arise while using this method a all the means are declared as different so that the lower limit of s w results equal to zero and b all the means are declared not different so that the upper limit of s w results equal to in the former case the conservative approach suggests using the upper limit of s w in the latter case the user can follow a liberal approach and keep the lower limit of s w to our knowledge this is the first time that a method to estimate s w from the multiple comparison letters attribution is proposed the method iv stars is the option that yields the highest uncertainty for s w estimation due to the limited input information i e p f below a threshold or included between two values as for this method the greatest input information is given when stars indicate a p f included within two boundaries e g when indicates 0 01 p f 0 05 which allows the method to compute two defined limits both different from 0 and in the other cases this method can compute only one s w boundary similarly to the letters method the uncertainty of the estimation is unpredictable being dependent on the values of the means and significance levels available from the anova analysis 3 4 tool utility a case study the case study concerned the database creation for a meta analysis regarding the effects of conservation agriculture on soil fertility table 3 to populate the database standard deviations of the means were directly retrieved from published articles or estimated by the ex tract tool the final database consisted of 43 studies published in literature that meet the meta analytic requirements listed by koricheva et al 2013 studies from 1 to 18 reported the standard deviations of treatments in the text in tables or as graphical bars studies from 19 to 43 required ex tract to compute the s w in the latter case the method column defines the specific tool method utilized table 3 means of control and treatment s w data the exact value or the ll s w and ul s w and coefficient of variation cv of the two boundaries are also displayed in table 3 this example showed that ex tract tool allowed to obtain the minimum number of studies i e 30 40 required to perform a meta analysis valentine et al 2010 the global influence of a single data in a meta analysis depends on the ratio of s w over mean i e the coefficient of variation cv the power analysis also relies on the use of cv as shown in table 3 cv had a considerable variation across the 43 experiments from 0 5 to 34 9 the lowest is the cv range cv u l s w cv l l s w the most reliable the s w estimate the extent of the cv range depends on the method of extraction direct and lsd methods allow to obtain narrow cv ranges for which the maximum value of range was 0 04 and 0 1 respectively when standard deviation is estimated from a graph the uncertainty is still low with a maximum range value of 0 33 as for the p f method only one experiment was found however the result of p f indicates a low cv range 0 26 and this agrees with the fact that is an exact procedure leading to a s w value with limited uncertainty with the s w extraction from letters it was possible to estimate the uncertainty i e the cv range in seven out of ten studies since in three cases one of the limits was zero or this method performed worse than in the lsd and p f methods having a mean cv range of 4 21 with the s w extraction from star it was possible to estimate the uncertainty in three out of six studies since in three cases one of the limits was zero or due to the limited input information used with the star method the mean cv range 9 41 was much higher than the other methods given that the uncertainty obtained using letters and star was larger the conservative approach suggests using the upper limit of the estimated s w although this conservative approach reduces the weight of a specific study in the meta analysis it allows to include the study in the analysis this approach is recommended by weir et al 2018 who studied the effect of different approach of standard deviation on the reliability of meta analysis results the same authors recommended to use several methods which led to standard deviation values comparable to those estimated in our study with letter and stars methods as a final consideration we encourage the user to apply the best method being lsd and p f methods when the required input information is available 4 conclusions standard deviation is a strictly required statistic for proper data processing such as meta analysis sensitivity analysis and power analysis the present work illustrates the functionality of an excel tool ex tract which allows to estimate the pooled standard deviation of existing experimental datasets when this value is not directly reported this tool is capable to extract standard deviation from anova and post anova multiple comparison tests being the most common test in the environmental research by applying four methods which can be chosen according to the availability of one of the following information lsd values p f values letters used to classify means five multiple comparison tests are managed by the tool and stars attribution to indicate significance level the flexibility of the ex tract tool is also ensured by the wide range of experiment types one two three ways and design schemes crd crdb split plot strip block in which standard deviation can be extracted the lsd and p f methods are capable to estimate standard deviation with a defined level of uncertainty which is comparable to the one directly reported in tables or graphs although the other two methods letters and stars are associated with a higher degree of uncertainty they offer the possibility to derive standard deviation from those datasets which would be otherwise neglected the application to a real case demonstrates that the tool allows to double the number of studies that can be included in a meta analysis therefore the ex tract tool offers an operational facilitation in meta analytic research valorising the hidden information in published articles increasing the number of suitable studies and avoiding questionable procedure of study weighting software availability name of software ex tract developers andrea di guardo andrea diguardo unimib it marco acutis marco acutis unimi it year first available 2021 language vba for excel version 2016 2019 and 365 availability https doi org 10 6084 m9 figshare 14987130 supported systems microsoft windows macos licence cc by 4 0 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work is a part of the project sommit sustainable management of soil organic matter to mitigate trade offs between c sequestration and nitrous oxide methane and nitrate losses that received funding from the european joint programme soil grant agreement id 862695 this study is also funded by the european union s horizon 2020 framework programme for research and innovation h2020 rur 2017 2 as part of the landsupport project grant agreement no 774234 which aims at developing a decision support system for optimizing soil management in europe the work is also financially supported by the doctoral school of agriculture environment and bioenergy and of the university of milan appendix a supplementary data the following is are the supplementary data to this article s2 video tutorials at https drive google com drive folders 1pr bcjbfco1dgndxjnfsyglwhzvikp5x multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105236 appendix a uncertainty calculation in the estimation of s w when lsd value is available method i a let lsd apx be the approximation value of lsd that is automatically computed by the software based on the number of significant digits of lsd e g 0 5 b let min lsd lsd lsd apx such as lsd round lsd apx c let max lsd lsd lsd apx such as lsd round lsd apx the results of b and c are then plugged into the equation 4 to obtain the lower and the upper limits of s w note that in this case the ratio r l l s w l l s w s w 100 depends only on the number of significant digits used to report lsd or from reading the length of lsd if the value is obtained from a graph and from the lsd value itself defining lsd as the significant digits of the lsd written in the significant normalized scientific notation fleisch and kregenow 2014 without the decimal point the r value is given by 100 lsd the tool considers the final zeroes of a number integer or decimal as significant for instance if the value of the lsd is 1 the normalized scientific notation is 1 100 and so lsd 1 the r is 100 if the lsd is 2 2 the normalized scientific notation is 2 2 100 and lsd 22 the r is 4 5 if lsd is 99 9 the normalized scientific notation is 9 99 101 and lsd 999 the r is 0 001 appendix b uncertainty calculation in the estimation of s w when p f value is available let apx be the approximation of mean values set by user e g 0 005 for each x i a let min x i be the smallest value such as the rounding of x i is obtained using the approximation value apx i e round min x i apx x i b let max x i be the greatest value such as the rounding of x i is obtained using the approximation value apx i e round max x i apx x i and let a pfapx be the approximation value of p f that is automatically computed by the software based on the number of significant digits of p f b min p f be the smallest values such as the rounding of p f is obtained using the approximation value pfapx i e round p f pfapx p f c max p f be the greatest values such as the rounding of p f is obtained using the approximation value pfapx i e round p f pfapx p f then the minimum value of s w can be estimated by replacing each x i x with min x i and each x i x with max x i in formula 5 and using p f min as p f value in the formula 6 similarly the maximum value of s w can be estimated replacing each x i x with max x i and each x i x with min x i in formula 5 and using p f min as p f value in formula 6 
25693,lake kivu is a 485 m deep central east african rift lake with huge amounts of carbon dioxide and methane dissolved in its stably stratified deep waters in view of future large scale methane extraction one dimensional numerical modelling is an important and computationally inexpensive tool to analyze the evolution of stratification and the content of gases in lake kivu for this purpose we coupled the physical lake model simstrat to the biogeochemical library aed2 compared to an earlier modelling approach this coupled approach offers several key improvements most importantly the dynamic evaluation of mixing processes over the whole water column including a parameterization for double diffusive transport and the density dependent stratification of groundwater inflows the coupled model successfully reproduces today s near steady state of lake kivu and we demonstrate that a complete mixing event 2000 years ago is compatible with today s physical and biogeochemical state keywords lake kivu simstrat aed2 1d modelling gas accumulation hydrothermal groundwater software availability software name kivu simstrat developer fabian bärenbold fabian baerenbold eawag ch availability and cost free software repository https github com eawag appliedsystemanalysis kivu simstrat software documentation https github com eawag appliedsystemanalysis kivu simstrat tree master doc and this work program language fortran90 program size 2 3 mb depending on os hardware required basic cpu and ram software required windows or linux operating system with a fortran compiler e g gfortran 1 introduction lake kivu is a large 2386 km2 and deep 485 m tropical rift lake situated on the boundary between rwanda and the democratic republic of the congo drc directly south of the virunga volcano chain it is fed by numerous small streams and discharges via the ruzizi river into lake tanganyika fig 1 a in addition to surface streams 45 of the inflow into lake kivu is provided by subaqueous groundwater sources schmid and wüest 2012 some of which were identified in the northern part of the lake using temperature and salinity profiles ross et al 2015a and fig 1a part of this intruding groundwater is hydrothermal meaning that it is warm salty and rich in carbon dioxide co2 due to their high density these hydrothermal sources have a tendency to plunge before eventually stratifying ross et al 2015a in addition to the hydrothermal groundwater two cooler and less salty sources were proposed based on numerical modelling schmid et al 2005 and subsequently identified by conductivity and temperature profiling ross et al 2015a the model of schmid et al 2005 suggested that the discharge of the cooler sources is one magnitude larger than the hydrothermal discharge and that they explain the strong thermo and chemoclines at around 190 and 250 m see fig 1b for an overview the groundwater intrusion into lake kivu has two major consequences for the whole water column i a continuous upwelling of up to 1 m yr 1 pasche et al 2009 and ii a strong density stratification and therefore meromixis below 60 m below the main chemocline at 250 m the lake water probably has not been in contact with the atmosphere for up to 1000 years and hence the inflowing co2 accumulated to concentrations of up to almost 100 mmol l 1 tietze 1978 schmid et al 2005 bärenbold et al 2020a furthermore the decomposition of settling organic matter and the reduction of co2 pasche et al 2011 also led to the accumulation of methane ch4 in the deep water the current total content of co2 and ch4 is estimated to 285 and 62 km3 respectively bärenbold et al 2020a the ch4 reservoir in lake kivu is a valuable resource for the neighboring countries rwanda and drc but also a looming danger for the surrounding population in fact the smaller gas rich lakes nyos and monoun in cameroon have experienced limnic gas eruptions in the past kling et al 1987 sigurdsson et al 1987 and 1746 and 37 people were killed by asphyxiation respectively in 2002 the volcano nyiragongo erupted and lava flowed into lake kivu it was feared that the hot lava could sink to the deeper strata of lake kivu and heat up water to an extent that it could rise to a level where the hydrostatic pressure is no longer sufficient to keep the gases trapped however lorke et al 2004 showed that the influence of the lava on the lake was minimal and they concluded that subaerial volcano eruptions are not likely to trigger a limnic eruption of lake kivu in may 2021 nyiragongo erupted again but without apparently disturbing the lake s stratification although the observations indicate the formation of a new dyke beneath the lake jones 2021 even if these recent volcanic activities did not significantly affect the lake they highlight how geologically dynamic and potentially unstable this region is in order to prevent further gas eruptions lakes nyos and monoun as well as kabuno bay which is a small 150 m deep subbasin of lake kivu with only low ch4 content are degassed artificially today degassing is performed using long pipes which transfer gas rich deep water by self siphoning to the lake surface kusakabe 2017 halbwachs et al 2020 in the main basin of lake kivu an artificial decrease of gas concentrations is beneficial for two reasons i decreasing gas pressure increases the safety margin i e the distance that a water parcel can rise without outgassing during a mixing event and ii the extracted ch4 can provide energy for both rwanda and the drc in december 2015 the u s company contourglobal started the operation of the first large scale ch4 extraction facility kivuwatt with an electric capacity of 26 mw kivuwatt extracts water from a depth of 350 m removes most of the ch4 and reinjects the partially degassed water at 240 m right above the main chemocline the projected expansion of gas extraction at lake kivu expert working group on lake kivu 2009 would lead to a major redistribution of warm salty and nutrient rich deep water within the lake if not managed carefully such redistribution of water has the potential to change stratification gas distribution and nutrient availability in lake kivu during the next decades for example if the reinjected water ends up too close to the lake surface it could provoke harmful events like outgassing or algal blooms regular monitoring of stratification and gas concentrations combined with numerical modelling for forecasting are key to prevent such negative consequences see schmid et al 2019 for a detailed discussion on monitoring due to the almost perfect horizontal homogeneity in the stably stratified deep waters i e below 60 m see schmid and wüest 2012 for a detailed evaluation of horizontal homogeneity one dimensional modelling is an adequate tool to assess the changes discussed above note however that such modelling is not capable to evaluate inherently three dimensional processes like surface layer dynamics across basins or the dynamics in the vicinity of groundwater sources or water extraction and reinjection by power plants 1d modelling of lake kivu s water and gas dynamics was previously performed by schmid et al 2005 using aquasim a tool for simulating diverse aquatic systems reichert 1994 this model was later improved and used to predict changes in the lake s stratification gas content and nutrient concentrations for different methane extraction scenarios wüest et al 2009 schmid et al 2019 however the model by schmid et al 2005 included several simplifications and assumptions that could lead to erroneous conclusions if applied to long term transient simulations most importantly the model could not reproduce feedbacks between changes in lake stratification and the intrusion depths of the subaquatic springs and turbulent diffusive transport was assumed to be constant in the top 120 m both of these simplifications are not adequate when doing transient simulations here we propose a new modelling approach using the physical one dimensional lake model simstrat goudsmit et al 2002 which we coupled to the biogeochemical library aed2 an uncalibrated version of simstrat had already been used to model lake kivu s surface waters in a lake model intercomparison study and performed reasonably well thiery et al 2014 in short the new modelling approach has the following advantages i diffusive transport between 0 and 120 m is dynamically computed using a k ε turbulence model ii the transport through double diffusive staircases below 120 m is parameterized based on recent field observations and iii the groundwater inflows are free to plunge and rise as a function of their density compared to the density of the ambient lake water due to these advantages our new model can replace the model developed by schmid et al 2005 for longer transient simulations of lake kivu i e for example to model the evolution of gas concentrations and stratification due to large scale gas extraction projects although our model could be readily used for such simulations we do not focus on the changes induced by gas extraction in this work instead we use the model to address three specific scientific questions firstly ross et al 2015a suggested that the cool and relatively fresh sources at 190 m and 250 m might mainly consist of recently infiltrated rainwater at the northern shore of the lake there rainwater probably quickly percolates through the porous volcanic rock we can test this hypothesis using our model and recently acquired tritium 3h measurements secondly based on observations in the sedimentary record several authors ross et al 2015b votava et al 2017 uveges et al 2020 suggest that lake kivu experienced a large scale mixing event 1000 years ago with nutrient and gas rich water reaching the surface layers and creating algae blooms to assess whether a complete mixing event could have happened at that time we use our model to determine whether and within which time period today s stratification and gas concentrations could have developed from a homogeneous i e completely mixed and degassed lake finally we previously observed large depletions of atmospheric noble gases in the deep water of lake kivu bärenbold et al 2020b using our model we test our hypothesis that the inflow of depleted groundwater is the most probable cause of these depletions 2 methods 2 1 basic model description the lake model simstrat was originally developed at eawag by goudsmit et al 2002 and it is available on github https github com eawag appliedsystemanalysis simstrat simstrat solves the one dimensional hydrodynamic equations on a staggered grid using an implicit euler numerical scheme turbulent diffusion is resolved by a k ε closure which means that the turbulent kinetic energy k and its dissipation ε are explicitly modelled as state variables the main sources of turbulent kinetic energy are direct wind forcing and buoyant instability of the water column however simstrat also allows for turbulent energy production by indirect wind forcing i e the dissipation of internal waves goudsmit et al 2002 a certain fraction of the wind energy represented by the seiche energy parameter αs is redistributed to internal seiche energy which is subsequently dissipated through friction at the boundaries and supplies mixing energy distributed over the entire depth of the lake as discussed by goudsmit et al 2002 this process is a major source of energy for turbulent mixing in the hypolimnia of lakes and neglecting it leads to systematic underestimation of the vertical turbulent mixing the original version of simstrat has been updated several times in the past the most important upgrades were the use of a different heat flux parameterization schmid and köster 2016 the implementation of density driven plunging of inflows gaudard et al 2017 and the possibility to simulate lake ice cover gaudard et al 2019 in order to explicitly simulate the formation of ch4 in lake kivu we coupled simstrat v2 2 with the biogeochemical model library aed2 v1 3 5 https github com aquaticecodynamics libaed2 which is developed by the university of western australia aed2 consists of a set of biogeochemical modules carbon nitrogen phosphorus phytoplankton etc which each can be switched on and off or adjusted according to specific needs in this work we use the carbon and oxygen modules of aed2 to simulate the carbon cycle including the formation and consumption of ch4 the calculation of the radiation budget in simstrat requires some basic physical parameters water albedo is determined as a function of latitude and seasonality by using a linear interpolation of the modified values of grishchenko in cogley 1979 which are implemented in simstrat v2 2 we use a standard value of 0 35 for the fraction of shortwave radiation absorbed as heat in the surface boundary layer the remaining shortwave radiation is absorbed in each water layer with an extinction coefficient of 0 27 m 1 thiery et al 2014 in addition we need to define the geothermal heat flux which is not well constrained in the lake kivu region degens et al 1973 recorded a few measurements in the sediment which indicate a range of 0 016 0 18 w m 2 for our simulations we chose a value of 0 13 w m 2 which is well within this range and leads to the best agreement between observed and simulated deep water temperatures all simstrat settings and parameters are summarized in the supporting information tables s1 and s2 in the aed2 carbon module the ch4 sediment flux p c h 4 in mmol m 2 d 1 is modelled taking into account oxygen inhibition and temperature dependence 1 p c h 4 f s e d c h 4 k s e d c h 4 k s e d c h 4 o 2 θ s e d c h 4 t 20 where f s e d c h 4 is the maximum sediment ch4 flux at 20 c k s e d c h 4 is the half saturation constant for the inhibition of ch4 production by oxygen o 2 is the oxygen concentration and θ s e d c h 4 is the arrhenius temperature multiplier for ch4 production similarly ch4 oxidation by methanotrophs o x c h 4 in mmol m 3 d 1 is modelled as 2 o x c h 4 r o x c h 4 o 2 k o x c h 4 o 2 v t o x c h 4 t 20 where r o x c h 4 is the maximum oxidation rate at 20 c k o x c h 4 is the half saturation constant for ch4 oxidation and v t o x c h 4 is the arrhenius temperature multiplier for ch4 oxidation due to the narrow range of temperature in lake kivu 23 26 c we neglect the temperature dependence in both equations and equation 1 then reduces to a constant production rate f s e d c h 4 in the anoxic waters below 60 m see table s3 for numerical values ch4 oxidation by methanotrophs equation 2 on the other hand happens right at the oxycline and therefore is a function of oxycline depth note that for computational efficiency we did not include the simulation of nutrients and phytoplankton so oxygen consumption is underestimated and the oxycline depth is probably overestimated which slightly affects the depth at which ch4 disappears 2 2 modifications of the model for lake kivu due to exceptionally high ch4 and co2 concentrations and the presence of double diffusive staircases the simulation of lake kivu requires some changes to the original simstrat aed2 this adjusted version is called kivu simstrat see software availability section and it is used throughout this work the respective changes are described in this section and depicted in the modelling flow chart in fig 2 2 2 1 parameterization of double diffusive transport one of the unique features of lake kivu are the double diffusive staircases which exist below a depth of 120 m and all the way down to the lake bottom at 485 m newman 1976 sommer et al 2013 these staircases consist of 300 thin mixed layers with an average thickness of 70 cm and sharp gradients in between sommer et al 2013 double diffusion is caused by the coexistence of two substances with reciprocal effect on density and very different molecular diffusion coefficients in lake kivu the temperature increase with depth destabilizes the water column whereas the increase with depth of salinity and co2 somewhat weakened by the ch4 increase stabilizes it consequently the strong stratification suppresses turbulent diffusion below 120 m but molecular transport is enhanced by the step wise profile compared to a smooth profile by measuring the temperature and salinity gradients between double diffusive layers sommer et al 2019 concluded that the apparent temperature diffusivity calculated as the heat flux divided by the smoothed temperature gradient is usually between 10 6 and 10 5 m2 s 1 therefore double diffusion in lake kivu enhances the heat flux by a factor of 10 100 compared to a smooth profile with only molecular heat diffusion 1 4 10 7 m2 s 1 conversely the salinity flux which has a molecular diffusion coefficient of 1 2 10 9 m2 s 1 is increased by a factor of 30 300 due to the steeper interface gradients sommer et al 2013 double diffusive convection is an inherently three dimensional process and therefore not readily replicable in one dimensional models we initially chose a very high depth resolution grid cells 1 cm in order to correctly represent the thin interfaces however the simulated interfaces were too steep and the mixed layers too large and thus the resulting double diffusive heat flux was on the order of 0 3 w m 2 instead of 0 1 0 15 w m 2 determined by sommer et al 2013 because of this disagreement with data and because such a high spatial resolution is computationally very expensive we decided to resort to a coarser resolution which produces a smoothed profile between 120 and 485 m without double diffusive staircases we found that a resolution of 2 m is sufficient to correctly reproduce the smoothed profile and that it allows simulations over thousands of years within a few days time the double diffusive transport of heat and salinity through this smoothed profile is expressed as apparent diffusion and we parameterize it as a function of stability n 2 s 2 which is given by 3 n 2 g ρ 0 ρ z where g is the gravitational acceleration ρ is water density ρ 0 is the density of freshwater 1 kg l 1 and z is the vertical axis n 2 is thus proportional to the density gradient and positive values indicate a stably stratified water column while negative values represent unstable conditions and thus convective instability fig 3 a and b shows that the relationships between the apparent diffusivities of temperature k t app and salinity k s app data from sommer et al 2019 and n 2 correlate well in the log log domain the diffusivities can therefore be expressed as power laws according to 4 k t a p p 2 12 10 8 n 0 95 f 5 k s a p p 1 44 10 10 n 1 26 f where f 0 74 is a dimensionless correction factor this correction factor accounts for the fact that the parameterization tends to lead to steeper temperature and salinity gradients in the model compared to observations of sommer et al 2019 and thus slightly overestimates vertical transport at the logarithmic scale this correction factor consists of an offset of the y axis but preserves the slope of the regression line at high values of n 2 the lower limit for apparent diffusion is the respective molecular diffusion fig 3 conversely when n 2 is very close to zero i e 10 6 s 2 our parameterization simulates convective instability and can produce very high diffusivities to avoid unphysically high mixing rates we use a threshold of 10 10 s 2 for n 2 which yields upper bounds of 8 10 4 and 2 10 4 m2 s 1 for the diffusivities of temperature and salinity respectively the molecular diffusion coefficients of co2 ch4 sommer et al 2013 noble gases wise and houghton 1968 and water isotopes 3h and 2h mills 1973 are of similar order of magnitude compared to salinity and therefore the double diffusive transport of each of these variables is approximated as 6 k i a p p k s a p p d i m o l d s m o l where d i m o l and d s m o l are the molecular diffusion coefficients of variable i and salinity respectively 2 2 2 influence of dissolved gases on density due to their high concentrations salinity and dissolved gases need to be taken into account in water density calculations for lake kivu in our model we compute water density ρ as a function of temperature t according to chen and millero 1986 and we add the effects of salinity s co2 and ch4 similarly to schmid et al 2004 7 ρ t s c o 2 c h 4 ρ t 1 β s s β c o 2 c o 2 β c h 4 c h 4 where β s β c o 2 and β c h 4 are the contraction coefficients of salinity co2 and ch4 respectively see schmid et al 2004 for numerical values in equation 7 and throughout this work s is computed from conductivity measurements by using ionic composition in lake kivu according to wüest et al 1996 2 2 3 total alkalinity in the aed2 carbon module total alkalinity ta is derived as a regression of dissolved inorganic carbon dic and s with different built in regression functions to choose from for lake kivu wüest et al 2009 showed that ta in mmol m 3 is a linear function of s in g kg 1 and suggested the following relation 8 t a 11960 s we implemented this option in aed2 and used it throughout our simulations 2 2 4 lake atmosphere interactions of water isotopes 3h and 2h and noble gases in order to model the water isotopes 3h and 2h we take into account their interaction with the atmosphere using the craig gordon model see for example aeschbach hertig 1994 the net flux f c of the isotope concentration c out of the water column is given by the combined effect of evaporation and precipitation 9 f c e α c p h w c l 1 h δ ε r c p where c p is the concentration in precipitation c l the surface concentration in the lake e the evaporation rate r the precipitation rate h w the relative humidity with respect to surface water temperature α the equilibrium fractionation coefficient and δε the kinetic enrichment for 3h we assume an equilibrium fractionation factor α of 0 89 and negligible kinetic fractionation aeschbach hertig 1994 for 2h α is computed according to majoube 1971 and the kinetic enrichment factor δ ε is derived from relative humidity gonfiantini 1986 finally we need to determine the precipitation isotope signals for 3h we use the database of the global network of isotopes in precipitation gnip because data is scarce in africa and there is no measurement station close to lake kivu we use the average of surrounding stations to derive a time series with yearly data from 1953 to 2018 iaea wmo 2020 note that there is no data available after 2000 and thus we calculate the values from 2000 to 2018 using exponential extrapolation of the trend from 1990 to 2000 see figs s1 and s2 in the supporting information we express 2h in δ notation i e as excess with regard to its ratio with the vienna standard mean ocean water vsmow and we use a constant precipitation value of 4 5 because i the value is in the range of observed values iaea wmo 2020 ii data is too scarce for deriving seasonal variation and iii the value leads to a good agreement between modelled δ2h signals and observations at the lake surface note that we also use these 3h and δ2h precipitation values for the surface inflows tributaries into lake kivu thus neglecting evaporation during transport to the lake the assumption of neglecting evaporation effects on the isotopic composition of the tributaries is motivated by the fact that the watershed is small and thus rainwater ends up in the lake quickly in reality there is substantial evaporation despite the small watershed and this in turn would lead to higher δ2h values in tributaries compared to precipitation we neglected this evaporation effect in our study i e assumed the same δ2h in both tributaries and precipitation because we do not simulate 2h dynamics in the surface waters the exchange of noble gases across the lake atmosphere interface was approximated by prescribing air saturated water at 25 c at the lake surface due to virtually constant concentrations of noble gases in the atmosphere and because the surface water temperature of lake kivu is usually close to 25 c 2 3 meteorological forcing simstrat requires air temperature t a horizontal wind speed at 10 m above the water surface uv 10 wind direction φ atmospheric vapor pressure e a incoming short wave radiation sw in and incoming long wave radiation lw in these meteorological variables are monitored continuously every 30 min by an automatic weather station on a platform on lake kivu see rooney et al 2018 for details w thiery from vrije universiteit brussel kindly provided continuous data from october 2012 to september 2019 from this weather station to allow for the simulation of longer timescales we use the output of a regional climate model which is used for scenario simulations within the isimip project inter sectoral impact model intercomparison project see frieler et al 2017 for scenario descriptions the climate model output consists of daily modelled values of all important meteorological variables from the year 1660 2300 at lake kivu grid cell 1 75 s 29 2 e we chose miroc5 from the four available regional models but the choice of climate model is not important as long as interannual variability is well represented the air pressure and air temperature of the climate model are corrected by adjusting their average to the measured average by the weather station fig 2 while wind speed and incoming radiation are adjusted using simstrat s built in correction factors table 1 note that the difference in air pressure and temperature table 1 is caused by the higher elevation of the climate model grid cell compared to lake kivu we also noticed that wind directions differ between observations and the regional climate model probably due to local effects below the grid scale of the climate model however simulation results are only weakly sensitive to wind direction in the lake model because wind direction mostly influences the timing and less the magnitude of mixing events in addition the magnitude of mixing events is scaled by the wind and seiche energy fit parameters of the model to match the observed temperature and salinity profiles finally the difference in wind speed could be shown to be caused by the different time resolutions of the weather station half hourly and the climate model daily averages using the corrected climate modelling output of miroc5 we created two forcing time series i the control dataset picontrol from 1660 to 2300 which represents climate forcing with constant pre industrial greenhouse gas concentrations and ii a combination of a historical dataset from 1860 to 2005 and a projection from 2006 to 2100 with a net radiative forcing of 6 0 w m 2 in 2100 which will be called rcp6 representative concentration pathway 6 0 the goal of the picontrol time series is to provide a realistic but stochastic distribution of seasonal mixing depth for the steady state and long term simulations see results section while rcp6 is only used for the transient simulation of 3h from 1953 to 2018 2 4 water balance the water balance of lake kivu can be written as 10 p i g e o with precipitation p surface inflow i groundwater inflow g evaporation e and outflow o precipitation and evaporation are almost equal in lake kivu schmid and wüest 2012 muvundja et al 2014 meaning that the discharge of inflowing tributaries and groundwater is mostly balanced by the outflowing ruzizi river in our model we use the values by schmid and wüest 2012 who suggest average values of 63 m3 s 1 for surface inflow 41 m3 s 1 for groundwater inflow and 95 m3 s 1 for the outflow we add the net difference between p and e to the outflow which then amounts to 108 m3 s 1 2 5 tritium 3h measurements 3h or t is a radioactive hydrogen isotope which is transported in the environment as tritiated water hto it is extremely rare in nature due to low natural production and a relatively fast decay with a half life of 12 3 years however the atomic bomb tests in the 1950s and 60s released large amounts of 3h into the atmosphere up to several hundred tritium units tu where 1 tu is 1 3h atom per 1018 water atoms since then 3h has been widely used in lake and groundwater environments as a physical tracer kipfer et al 2002 we took water samples using the pressure proof sampling equipment described in bärenbold et al 2020b and analyzed them for 3h content by measuring its decay product 3he according to beyerle et al 2000 in this procedure samples are degassed under vacuum in order to evacuate the naturally present 3he then left on the shelf to allow for sufficient decay of 3h to 3he typically 6 12 months and subsequently analyzed for newly produced 3he knowing the amount of 3he produced in the sample and the elapsed time one can back calculate the original amount of 3h present in the sample due to the high helium content in lake kivu bärenbold et al 2020b some of our samples had to be degassed twice to remove all naturally present 3he the estimated standard deviation of our measurements due to calibration uncertainty and possible contamination with atmospheric air is in the range of 0 03 0 10 tu in addition to this data we added two unpublished 3h measurements taken by our own group during an expedition in 2004 3 results and discussion 3 1 seasonal mixing dynamics 0 60 m seasonal mixing in lake kivu usually occurs during the dry season and affects the top 30 60 m it is strongest during july and august due to high wind speeds and lower air temperatures which lead to convective instability and enhanced mixing schmid and wüest 2012 interannual variability of seasonal mixing depth is therefore mainly driven by meteorological conditions the extent of the surface layer where conductivity is nearly constant gives an impression of varying mixing depth in different years fig 4 a the typical range of surface temperatures in lake kivu is 23 26 c thiery et al 2014 morana et al 2015 with warmer temperatures in the stratified rainy season and cooler temperatures in the windier dry season in the model average mixing depth and surface temperature are mainly determined by the fit parameters of wind speed short wave radiation and long wave radiation by manually tuning these three parameters table s2 for numerical values we can adequately reproduce mixing depth which varies between 20 and 60 m fig 4b modelled water temperatures exhibit a range of 23 27 c fig 4c with peak water temperatures during stratified periods occasionally 1 2 c higher than typical observations thiery et al 2014 morana et al 2015 however as long as the temporal distribution of the mixed layer extent is realistic these higher surface temperatures will not influence our model results 3 2 turbulent mixing at intermediate depths 60 120 m diffusive transport between 60 and 120 m is governed neither by seasonal mixing nor by double diffusive convection this depth range is characterized by a strong increase in salinity with depth and therefore a strong density gradient it was therefore put forward by schmid et al 2005 that turbulent mixing in this depth range is weak in contrast the stable isotope measurements by ross et al 2015a indicate that mixing might be stronger than previously thought in fact their measurements seem to show that the evaporative isotope signal at the lake surface is mixed down against the groundwater induced upwelling our simulation results suggest that direct wind induced turbulent mixing is indeed very weak in lake kivu below 60 m the wind induced currents are not able to penetrate below the surface layer and without considering the energy transfer via internal seiches the modelled salinity gradient below the surface mixed layer becomes much steeper than observed fig 5 a if the seiche energy parameter α s is tuned to 0 0027 i e 0 27 of wind energy is transformed to seiche energy our model can reproduce the observed salinity gradient very well fig 5b except for a small jump at 120 m this jump is an artefact caused by the transition from the k ε mixing scheme to the parameterization of double diffusion we were able to reduce the size of the jump by tuning the calibration parameter q nn but not to eliminate it the parameter q nn determines how the mixing energy from internal seiching is distributed vertically as a function of stability n 2 we conclude that internal seiching can explain why there is still a certain amount of turbulent mixing 10 7 10 6 m2 s 1 in a depth region where the density gradient is very strong 3 3 steady state simulation while diffusive transport is computed using the k ε closure 0 120 m and a parameterization of double diffusive convection 120 485 m vertical advection is a function of the discharge inflow depth and properties t s co2 ch4 of the different groundwater springs present in lake kivu unfortunately the properties of the groundwater sources are inherently difficult to measure in the field however despite the lack of data we can use indirect evidence to shed light on the influence of the groundwater sources on lake kivu using the water balance of lake kivu schmid and wüest 2012 suggested that the total groundwater input into lake kivu is between 32 and 48 m3 s 1 in addition ross et al 2015a were able to identify several warm and saline signals below 250 m as well as a strong cool and less saline signal right at the chemocline 250 255 m and a more diffusive cool signal between 90 and 195 m based on this information and the current vertical structure of the lake we assume that i four hydrothermal point sources enter the lake below 250 m and plunge due to their high density ii two cooler point sources enter the lake at 253 and 190 m and iii one diffusive groundwater source which has the same properties as the point source at 190 m enters the lake between 90 and 195 m depth fig 2 in ross et al 2015a we summarized this information in table 2 throughout this work we refer to the ground water sources at 190 and 253 m as cool sources 1 and 2 respectively and the sources below 250 m as hydrothermal sources the water introduced as point sources is free to plunge or rise according to its density in simstrat thereby entraining ambient lake water the final stratification depth of these groundwater inflows is not only influenced by their temperature salinity and gas content but also by the current lake density profile which leads to a highly complex and interdependent calibration problem with many unknown variables in order to calibrate inflow depth and discharge but also temperature salinity and co2 content of the groundwater sources we chose to tune our model with the goal of producing a steady state with temperature salinity and gas profiles close to today s observations fig 2 this approach is motivated by the findings of bärenbold et al 2020a who conclude that ch4 and co2 concentrations did not show a clear temporal trend during the last 45 years conversely it has been shown katsev et al 2014 sommer et al 2019 that lake kivu experienced a warming trend on the order of 0 01 c yr between 2004 and 2015 throughout the stratified deep water below 60 m we chose to neglect this warming rate in our simulations for two reasons i the observed warming trend seems variable in time i e no warming observed below 250 m from 1974 to 2004 and is comparably slow and ii we focus on the present state and the past evolution of lake kivu in this work not on its future evolution the causes for the recent lake warming have not been clearly identified the warming near the surface is likely due to climate change and thus probably a recent phenomenon katsev et al 2014 in contrast to this recent trend the near surface lake temperatures could have varied to some extent in both directions during the simulated period of the past 2000 years depending on past climate conditions the causes for the recent warming of the deep water are unknown and it needs to be further evaluated in the future whether they represent a persistent change in the thermal dynamics of the lake that requires adaptation of the model for future projections note that a warming trend could be introduced into the model by slightly modifying the temperature of the hydrothermal groundwater inflows or the geothermal heat flux while most of the co2 stems from groundwater inflow this is not true for ch4 which was shown to contain a mixture of modern and old 14c dead carbon schoell et al 1988 pasche et al 2011 using 14c analysis pasche et al 2011 showed that the degradation of organic matter can only provide a maximum of 50 of ch4 below 250 m the remaining 50 stem either from reduction of old geogenic co2 or from the direct inflow of geogenic ch4 the absolute ch4 fluxes derived in pasche et al 2011 lead to an increase in ch4 concentrations of around 7 8 within 30 years however such an increase is clearly not validated by the most recent measurements bärenbold et al 2020a which indicate approximately constant concentrations since the measurements of tietze in 1974 tietze 1978 therefore we assume that the actual ch4 production below 250 m is close to a steady state production of 28 gc m 2 y 1 according to pasche et al 2011 50 of this production is supplied by degradation of organic matter and therefore we set f sed ch4 in aed2 to 3 2 mmol m 2 d 1 ch4 14 0 gc m 2 y 1 for the whole lake depth for the remaining 50 below 250 m we will explore two scenarios i the reduction of geogenic co2 to ch4 within the lake implemented analogously to ch4 production from organic matter and ii the direct groundwater mediated inflow of geogenic ch4 in both cases an additional 3 2 mmol m 2 d 1 of ch4 is added below 250 m thus yielding a total ch4 production of 6 4 mmol m 2 d 1 28 gc m 2 y 1 below 250 m fig 6 shows simulation results for temperature salinity co2 and ch4 for a simulation duration of 500 years exact groundwater properties used in the simulation are provided in table s4 the initial conditions used in this simulation are the temperature and salinity background profile of ross et al 2015a and the ch4 and co2 concentrations measured in 2018 bärenbold et al 2020a our model assumptions and tuned inflows lead to a steady state after 400 500 years for temperature salinity and co2 concentrations with co2 concentrations agreeing with observations well within measurement uncertainty the agreement between data and simulation is also excellent for salinity with a root mean square error rmse of 0 014 but not for temperature while the general structure of the temperature profile is generally well reproduced rmse of 0 024 c modelled temperature is consistently 0 3 c too low between 250 and 350 m and 0 1 c too high below 400 m despite a large calibration effort on the groundwater properties these differences could not be eliminated the main difference between temperature and the other main agents in lake kivu is that the transport of salinity and gases is dominated by vertical advection i e the upwelling caused by groundwater inflow while for temperature both advection and diffusive transport are important sommer et al 2019 as a consequence it seems that the properties of the groundwater sources are well calibrated while the parameterization of heat transport through the double diffusive staircases only works satisfactorily indeed fig 3a shows that the proposed linear regression reflects well the general relationship between stability and apparent temperature diffusivity however the variability is rather large thus indicating that the prediction capacity of stability to deduce double diffusive transport in lake kivu is limited for ch4 we explored two scenarios with different origins for the 14c dead part of ch4 scenario 1 implies that the remaining 50 of ch4 are produced by co2 reduction within the lake fig 6d while in scenario 2 the remaining 50 stem from groundwater fig 6e scenario 1 indicates that ch4 concentrations should reach a clear maximum between 300 and 400 m and decrease towards the lake bottom due to the inflow of ch4 free groundwater there this decrease is supported by the ch4 data of tietze in 1974 tietze 1978 and the eawag dataset in bärenbold et al 2020a but not by the data of halbwachs and tochon in 2003 schmid et al 2005 and the ufz dataset in bärenbold et al 2020a scenario 2 also shows slightly decreasing concentrations at the lake bottom but this decrease is small and could probably be eliminated by further tuning the ch4 concentrations in the groundwater sources see table s4 for values used in scenario 2 scenario 2 agrees better with observed ch4 concentrations and thus our results suggest that at least part of the remaining 50 of ch4 are introduced by the hydrothermal groundwater sources in both scenarios ch4 production from the decomposition of organic matter is assumed constant with depth in reality sediment focusing or the reduction of degradable organic matter at greater depths due to longer settling times could potentially lead to varying ch4 production with depth 3 4 water tracer simulations in this section we use the calibrated steady state model to reproduce the 3h and δ2h measurements from this work and by ross et al 2015a respectively 3 4 1 3h simulations water age and origin there are no direct 3h measurements available from the groundwater sources in general 3h in groundwater can have two origins namely a low rather constant background signal due to interaction with rocks mamyrin and tolstikhin 1984 and a variable signal due to gas exchange with the atmosphere therefore we make the following assumptions for our model runs i the old hydrothermal groundwater in the lake kivu basin has a constant background 3h signal of 0 5 tu due to interaction with rocks ii young groundwater stems from the recent infiltration of precipitation and has an atmospheric 3h signal and iii all groundwater sources in lake kivu can be a mixture of old and young groundwater note that we assume that the time from infiltration of precipitation to groundwater recharge is short i e there is no lag that would decrease the 3h concentration due to radioactive decay this assumption is justified by the fact that the northern shore where the groundwater sources were found ross et al 2015a consists of permeable volcanic rock which allows fast infiltration of rainwater fig 7 a shows simulation results under the assumption that every groundwater source is entirely composed of old hydrothermal groundwater compared to field observations from 2004 to 2018 see table s5 for numerical values the observations show a decrease in time in the seasonal mixed layer from 2 2 tu in 2004 to 1 6 tu in 2018 and a decrease with depth from 1 6 tu to close to 0 tu at 200 m the decrease in time can be explained by decreasing 3h concentrations in the atmosphere due to radioactive decay of the remaining bomb 3h while the decrease with depth shows that the water below 60 m is predominantly old i e no recent interaction with the atmosphere and that vertical mixing is weak the assumption that all groundwater sources are entirely composed of old groundwater agrees very well with the observations in the mixed layer as well as with the observation below 300 m however between 100 and 200 m the measured 3h concentrations are higher than predicted by the simulation thus indicating that the cool groundwater sources do also contain atmospheric 3h indeed if we adjust the fraction of young groundwater in the cool sources 1 and 2 to 45 and 20 respectively we get a very good agreement with the observations in the corresponding depth range fig 7b the finding that the cool groundwater sources are partially young is supported by the absence of surface tributaries in the north of lake kivu where ross et al 2015a identified the cool sources this northern part of the catchment has a surface area of 686 km2 which accounts for 14 of the whole lake kivu catchment area of 4940 km2 we can estimate a lower bound for the amount of water which infiltrates and feeds the groundwater sources by using the surface inflow of 63 m3 s 1 schmid and wüest 2012 as 86 of the catchment produce 63 m3 s 1 of run off the remaining 14 would yield a potential groundwater recharge of 10 m3 s 1 however precipitation is expected to infiltrate quickly through the porous volcanic rock in the north and thus there is less water lost to evaporation than in the remaining watershed an upper bound of the infiltrated water can be derived by using the average precipitation of 1470 mm yr 1 muvundja et al 2014 which would give 32 m3 s 1 over volcanic part of the catchment the amount of young water required by our calibrated model to fit the observed 3h profile is 15 5 m3 s 1 with a range of 11 5 18 m3 s 1 range estimated from uncertainty of 3h and is therefore well within the range of 10 32 m3 s 1 given by run off and precipitation data we conclude that the cooler groundwater sources are probably a mixture of old and more recent groundwater with 45 and 20 of recently infiltrated water in cool sources 1 and 2 respectively conversely our only observation below 300 m suggests that the deep groundwater sources predominantly consist of old hydrothermal water 3 4 2 δ2h simulations contradict field observations in contrast to 3h there are field observations available for δ2h for at least some of the groundwater sources ross et al 2015a below 200 m ross et al 2015a found very variable δ2h groundwater signals ranging from 2 8 to 17 4 in our model we cannot use these values because our implemented hydrothermal sources do not directly correspond to the hydrothermal sources found by ross et al 2015a therefore the δ2h concentrations of the hydrothermal sources are adjusted to reach the best agreement between simulation and observed background observations by ross et al 2015a in contrast to the warm sources our cool sources 1 and 2 were clearly identified by ross et al 2015a and thus fig 7c shows a simulation where the δ2h concentrations of cool groundwater sources 1 and 2 correspond to the values found by ross et al 2015a while in 7d we adjust all sources so that the model fits the observed background profile of ross et al 2015a in both figures we also show the δ2h observations of tietze 1978 and katsev et al 2014 comparing the three datasets illustrates the seasonal variability of the δ2h values in the surface layer as well as the generally good agreement below 100 m due to better data consistency i e less scattering of measurements at similar depths we use the data of ross et al 2015a for the calibration of δ2h values in the inflows fig 7d to reproduce the observed δ2h values in the surface layer the δ2h concentration in precipitation was fixed to 4 5 meaning that the model cannot reproduce seasonal or interannual variations at the surface when using the δ2h observations of ross et al 2015a as groundwater input the observed δ2h values above 250 m are considerably higher compared to simulated δ2h fig 7c the δ2h values in groundwater necessary to provide a good agreement of observations and simulation are around 15 and 30 higher for cool sources 1 and 2 respectively fig 7d see table s6 for numerical values these differences are well beyond measurement uncertainty and in addition both δ2h groundwater observations consist of several measurements with rather low variability pers comm k a ross the results of the δ2h simulations are further corroborated by δ18o simulations not shown in this work which exhibit the same systematic deviation from observations we conclude that our model is able to reproduce the observed δ2h profiles within the uncertainty of almost all data points of ross et al 2015a if the inflow concentrations are freely adjustable and allowed to have an evaporative δ2h signal in this case the observed stable isotope profiles are not in contradiction with low turbulent mixing in lake kivu however the observed δ2h concentrations close to cool sources 1 and 2 are not high enough to explain the observed lake profiles potentially a certain discrepancy could be because δ2h in the groundwater inflows or the groundwater inflows themselves are variable in time however it is unlikely that this could explain a difference of 15 and 30 in inflow concentrations and we are unable to fully understand the reason for this disagreement between model and observations 3 5 origin and maintenance of lake kivu s density stratification it is generally assumed that lake kivu has experienced several partial degassing events in the past haberyan and hecky 1987 zhang et al 2014 ross et al 2015b votava et al 2017 uveges et al 2020 this may have happened by spontaneous ebullition due to gas oversaturation in the deep waters another possibility is the creation of instability within the water column due to either subaquatic volcanism ross et al 2015b or hyperpycnal flows due to exceptional rain events zhang et al 2014 ross et al 2015b and uveges et al 2020 both estimate that the last large mixing event may have occurred 800 1000 years ago with several smaller events between 800 years bp and the present in this context we can use our model to evaluate how long it takes lake kivu to reach its current state because we don t know when and to what extent lake kivu has mixed in the past we assume that i the lake is completely mixed and gas free at the start of the simulation ii the groundwater discharge and properties i e temperature salinity and gas concentrations are constant in time and the same as today and iii the average climatic conditions are constant and the same as today our simulated salinity and gas concentrations agree well with today s state after roughly 1500 years fig 8 thus confirming that the density driven inflow of several groundwater sources can explain today s vertical lake structure this result is in contrast to arguments by hirslund 2020 who suggested that the inflow of groundwater sources cannot be responsible for lake kivu s density stratification for temperature it takes more time to reach a steady state the reason is that double diffusive transport is much faster for temperature compared to salinity and gases due to the much higher molecular diffusion coefficient while the vertical transport of salinity and gas concentrations is dominated by advection double diffusion contributes around half of the vertical heat flux the double diffusive heat flux is parameterized as a function of n 2 and therefore a stable temperature profile will only develop once the density stratification mainly determined by salinity and co2 is fairly stable after 1500 years at this point the overshooting temperature below 350 m decreases again and converges to 26 c our model results suggest that we should see slightly decreasing deep water temperatures if the lake is close to a steady state in contrast recent temperature observations indicate an increase of 0 01 c yr 1 sommer et al 2019 there are several possible answers to this problem i the lake is not at a steady state and thus temperature salinity and gas concentrations are still slightly increasing ii the lake was close to a steady state but there was an increased discharge of hydrothermal water during the last decades which counteracts the temperature decrease and iii our parameterization of double diffusive heat flux is not suitable for the simulation of a near homogeneous lake at the start of the simulation and thus the simulated overshooting is a model artefact note that answer ii would also imply a slight steepening of the main chemocline at 250 m due to increased upwelling as it was observed recently hirslund 2012 however this steepening stopped again after 2012 pers comm m schmid assuming constant groundwater inflows throughout the simulation duration our simulations show that it is improbable that today s lake structure has developed within only 1000 years from an initially homogeneous and degassed lake the time which is necessary to reach a steady state close to today s observed temperature salinity and gas concentrations rather seems to be around 2000 years therefore a potential mixing event 1000 years ago as suggested by ross et al 2015b and uveges et al 2020 could not have mixed the entire lake however a partial mixing event 1000 years ago or even smaller mixing events during the last few centuries are well compatible with our simulations 3 6 atmospheric noble gas depletions in the deep water in an earlier publication we described that the atmospheric noble gases i e the noble gases whose main source is the atmosphere ne 36ar and kr are depleted by 55 70 in the deep water of lake kivu bärenbold et al 2020b therein we discussed three different mechanisms to explain the observed noble gas depletions namely i continuous outgassing ii the inflow of noble gas depleted groundwater and iii a relic of a past large outgassing event mechanism i was subsequently excluded because the isotope ratios 20ne 22ne and 40ar 36ar did not show the expected depletion patterns bärenbold et al 2020b we further argued that mechanism ii the inflow of depleted groundwater is the most probable reason for the observed profiles in this section we use the model setup of the previous section to test and evaluate hypotheses ii and iii for hypothesis ii we adjusted the concentrations in the groundwater sources to fit the observed noble gas profiles in bärenbold et al 2020a conversely in hypothesis iii the noble gas concentrations in the groundwater are fixed to those of air saturated water asw at 25 c the resulting ne profiles indicate that we can exclude that the currently observed profiles are a result of a past outgassing event only fig 9 b in contrast the observed concentrations can be perfectly reproduced by the inflow of noble gas depleted groundwater fig 9a the best agreement with observations is reached if the hydrothermal sources are depleted by 40 50 table s6 and the cool sources 1 and 2 by 5 and 10 respectively the depth profiles of the other atmospheric noble gases measured in bärenbold et al 2020b can be successfully reproduced as well but are not shown here 3 7 model limitations compared to past one dimensional modelling attempts on lake kivu schmid et al 2005 one of the main advantages of our model is the fact that turbulent transport is dynamically calculated throughout the whole water column from 0 to 120 m we use simstrat s built in k ε turbulence closure and below we apply a newly derived parameterization which is based on measurements by sommer et al 2013 this situation creates an artificial transition point at 120 m however while both the transition at 120 m and the parameterization are valid for today s lake structure back to at least 1976 newman 1976 we have no information about the presence and extent of double diffusive staircases during the past centuries particularly during periods with a weaker density stratification we do not know how well our parameterization performs nevertheless it can be assumed that an initially weak density stratification quickly becomes stronger due to the constant deep inflow of salts and co2 as a consequence although turbulent transport might initially be underestimated by our parameterization this probably does not significantly alter the simulations results another important simplifying assumption of our modelling exercise is the fact that climatic conditions as well as the discharges and properties of all groundwater sources are assumed to be constant in time part of the cooler groundwater sources seems to have an atmospheric 3h signature and thus we expect the discharge and properties of these sources to vary with climate conditions i e higher discharge in a wet climate lower discharge during droughts furthermore volcanic activity can greatly influence hydrological parameters like evaporation and rock permeability and thus lead to variable infiltration and groundwater properties over time for example votava et al 2017 suggest that the intermittent deposition of caco3 found in lake sediments indicate a corresponding intermittency of deep groundwater inflow into lake kivu however for our long term simulations it is not crucial to know whether climate or inflow do vary over shorter timescales due to the long water residence times in the stably stratified deep waters finally it is clear that our model is overparameterized which is unavoidable given the lack of direct observations of the properties of the groundwater sources besides the calibration parameters of simstrat aed2 including meteorological and mixing fit parameters the properties of every groundwater source temperature salinity gas content stable isotopes etc as well as their inflow depths and discharges can be freely adjusted thus even if the simulations are constrained by observed vertical profiles and the water balance likely several parameter sets exist which yield comparable results for example the discharge of the hydrothermal groundwater sources could be lower but its temperature salinity and gas content could be higher to keep equal the heat salt and gas input by the sources however such changes to groundwater properties also influence density and thus the depth where the source stratifies which makes the search for alternative solutions difficult our suggested set of parameters and inflow properties might not be unique but it strongly suggests that to form and maintain the current state several warm and dense groundwater sources below 260 m and two cooler sources at 190 and 250 m depth are required 4 conclusions we developed and calibrated a one dimensional model for lake kivu exploiting the fact that the lake is well mixed horizontally below 60 m in order to simulate the effect of dissolved gases co2 and ch4 on lake physics we first coupled the physical lake model simstrat with the biogeochemical model library aed2 we then developed a parameterization of vertical transport of heat salts and gases across the double diffusive staircases of lake kivu below 120 m as a function of lake stability n 2 after manually calibrating temperature salinity and gas content of the inflowing groundwater sources we were able to reproduce today s lake profile by a steady state simulation using this steady state model and our 3h dataset allowed to show that the cooler groundwater sources which enter the lake between 100 and 250 m are probably a mixture of modern and old hydrothermal water most importantly we showed that the evolution of lake kivu from a completely homogeneous and gas free state to today s density stratification within 2000 years is successfully reproduced using a one dimensional lake model with constant groundwater inflows our results demonstrate that the stratification depth of groundwater inflows markedly shifts as a function of the evolving density structure of the lake as the duration of 2000 years is an upper bound which is valid for an initially completely mixed lake our simulations indicate that a partial mixing event 1000 years bp zhang et al 2014 ross et al 2015b uveges et al 2020 is in good agreement with the currently observed state of the lake furthermore our simulation results suggest that the depletion of atmospheric noble gases in the deep water of lake kivu bärenbold et al 2020b are not a relic from a past outgassing event but have to be sustained by an active process i e inflow of noble gas depleted groundwater in this work we used our model to focus on finding a steady state solution for today s lake structure and on reproducing the past evolution of lake kivu in the future our model can be a valuable tool to simulate the effect of a changing climate on lake temperature stratification and nutrient availability in addition it can facilitate the decision making concerning the construction and operation of ongoing and future methane harvesting projects in lake kivu in order to better constrain the discharge and properties of the groundwater inflows in future model applications more research on the hydrogeological setting in the lake kivu region would be beneficial moreover the sampling of inflowing groundwater sources using a remotely operated underwater vehicle rov could allow the determination of temperature salinity and gas concentrations of groundwater sources without contamination with ambient lake water the kivu simstrat model is derived from the coupling of the physical lake model simstrat and the biogeochemical library aed2 this coupled model without the specific changes for lake kivu represents a computationally efficient and physically sound tool to simulate geochemical tracers and biochemical processes in lakes and reservoirs worldwide in the future funding this work was supported by the swiss national science foundation grant number 200021 160114 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to thank augusta umutoni and ange mugisha from lkmp lake kivu monitoring programme for help with the organization of the field work and reto britt michael plüss maximilian schmidt and the whole team from lkmp for help with tritium sampling in lake kivu we also want to thank alexandra lightfoot and matthias brennwald from the noble gas laboratory at eth zurich for help with tritium sample analysis adrien gaudard and davide vanzo provided helpful insights into modelling and the implementation of new simstrat features particularly adrien s manual was of great help we also would like to acknowledge wim thiery from vrije universiteit brussel who made the data of the lake kivu weather station available to us appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105251 
25693,lake kivu is a 485 m deep central east african rift lake with huge amounts of carbon dioxide and methane dissolved in its stably stratified deep waters in view of future large scale methane extraction one dimensional numerical modelling is an important and computationally inexpensive tool to analyze the evolution of stratification and the content of gases in lake kivu for this purpose we coupled the physical lake model simstrat to the biogeochemical library aed2 compared to an earlier modelling approach this coupled approach offers several key improvements most importantly the dynamic evaluation of mixing processes over the whole water column including a parameterization for double diffusive transport and the density dependent stratification of groundwater inflows the coupled model successfully reproduces today s near steady state of lake kivu and we demonstrate that a complete mixing event 2000 years ago is compatible with today s physical and biogeochemical state keywords lake kivu simstrat aed2 1d modelling gas accumulation hydrothermal groundwater software availability software name kivu simstrat developer fabian bärenbold fabian baerenbold eawag ch availability and cost free software repository https github com eawag appliedsystemanalysis kivu simstrat software documentation https github com eawag appliedsystemanalysis kivu simstrat tree master doc and this work program language fortran90 program size 2 3 mb depending on os hardware required basic cpu and ram software required windows or linux operating system with a fortran compiler e g gfortran 1 introduction lake kivu is a large 2386 km2 and deep 485 m tropical rift lake situated on the boundary between rwanda and the democratic republic of the congo drc directly south of the virunga volcano chain it is fed by numerous small streams and discharges via the ruzizi river into lake tanganyika fig 1 a in addition to surface streams 45 of the inflow into lake kivu is provided by subaqueous groundwater sources schmid and wüest 2012 some of which were identified in the northern part of the lake using temperature and salinity profiles ross et al 2015a and fig 1a part of this intruding groundwater is hydrothermal meaning that it is warm salty and rich in carbon dioxide co2 due to their high density these hydrothermal sources have a tendency to plunge before eventually stratifying ross et al 2015a in addition to the hydrothermal groundwater two cooler and less salty sources were proposed based on numerical modelling schmid et al 2005 and subsequently identified by conductivity and temperature profiling ross et al 2015a the model of schmid et al 2005 suggested that the discharge of the cooler sources is one magnitude larger than the hydrothermal discharge and that they explain the strong thermo and chemoclines at around 190 and 250 m see fig 1b for an overview the groundwater intrusion into lake kivu has two major consequences for the whole water column i a continuous upwelling of up to 1 m yr 1 pasche et al 2009 and ii a strong density stratification and therefore meromixis below 60 m below the main chemocline at 250 m the lake water probably has not been in contact with the atmosphere for up to 1000 years and hence the inflowing co2 accumulated to concentrations of up to almost 100 mmol l 1 tietze 1978 schmid et al 2005 bärenbold et al 2020a furthermore the decomposition of settling organic matter and the reduction of co2 pasche et al 2011 also led to the accumulation of methane ch4 in the deep water the current total content of co2 and ch4 is estimated to 285 and 62 km3 respectively bärenbold et al 2020a the ch4 reservoir in lake kivu is a valuable resource for the neighboring countries rwanda and drc but also a looming danger for the surrounding population in fact the smaller gas rich lakes nyos and monoun in cameroon have experienced limnic gas eruptions in the past kling et al 1987 sigurdsson et al 1987 and 1746 and 37 people were killed by asphyxiation respectively in 2002 the volcano nyiragongo erupted and lava flowed into lake kivu it was feared that the hot lava could sink to the deeper strata of lake kivu and heat up water to an extent that it could rise to a level where the hydrostatic pressure is no longer sufficient to keep the gases trapped however lorke et al 2004 showed that the influence of the lava on the lake was minimal and they concluded that subaerial volcano eruptions are not likely to trigger a limnic eruption of lake kivu in may 2021 nyiragongo erupted again but without apparently disturbing the lake s stratification although the observations indicate the formation of a new dyke beneath the lake jones 2021 even if these recent volcanic activities did not significantly affect the lake they highlight how geologically dynamic and potentially unstable this region is in order to prevent further gas eruptions lakes nyos and monoun as well as kabuno bay which is a small 150 m deep subbasin of lake kivu with only low ch4 content are degassed artificially today degassing is performed using long pipes which transfer gas rich deep water by self siphoning to the lake surface kusakabe 2017 halbwachs et al 2020 in the main basin of lake kivu an artificial decrease of gas concentrations is beneficial for two reasons i decreasing gas pressure increases the safety margin i e the distance that a water parcel can rise without outgassing during a mixing event and ii the extracted ch4 can provide energy for both rwanda and the drc in december 2015 the u s company contourglobal started the operation of the first large scale ch4 extraction facility kivuwatt with an electric capacity of 26 mw kivuwatt extracts water from a depth of 350 m removes most of the ch4 and reinjects the partially degassed water at 240 m right above the main chemocline the projected expansion of gas extraction at lake kivu expert working group on lake kivu 2009 would lead to a major redistribution of warm salty and nutrient rich deep water within the lake if not managed carefully such redistribution of water has the potential to change stratification gas distribution and nutrient availability in lake kivu during the next decades for example if the reinjected water ends up too close to the lake surface it could provoke harmful events like outgassing or algal blooms regular monitoring of stratification and gas concentrations combined with numerical modelling for forecasting are key to prevent such negative consequences see schmid et al 2019 for a detailed discussion on monitoring due to the almost perfect horizontal homogeneity in the stably stratified deep waters i e below 60 m see schmid and wüest 2012 for a detailed evaluation of horizontal homogeneity one dimensional modelling is an adequate tool to assess the changes discussed above note however that such modelling is not capable to evaluate inherently three dimensional processes like surface layer dynamics across basins or the dynamics in the vicinity of groundwater sources or water extraction and reinjection by power plants 1d modelling of lake kivu s water and gas dynamics was previously performed by schmid et al 2005 using aquasim a tool for simulating diverse aquatic systems reichert 1994 this model was later improved and used to predict changes in the lake s stratification gas content and nutrient concentrations for different methane extraction scenarios wüest et al 2009 schmid et al 2019 however the model by schmid et al 2005 included several simplifications and assumptions that could lead to erroneous conclusions if applied to long term transient simulations most importantly the model could not reproduce feedbacks between changes in lake stratification and the intrusion depths of the subaquatic springs and turbulent diffusive transport was assumed to be constant in the top 120 m both of these simplifications are not adequate when doing transient simulations here we propose a new modelling approach using the physical one dimensional lake model simstrat goudsmit et al 2002 which we coupled to the biogeochemical library aed2 an uncalibrated version of simstrat had already been used to model lake kivu s surface waters in a lake model intercomparison study and performed reasonably well thiery et al 2014 in short the new modelling approach has the following advantages i diffusive transport between 0 and 120 m is dynamically computed using a k ε turbulence model ii the transport through double diffusive staircases below 120 m is parameterized based on recent field observations and iii the groundwater inflows are free to plunge and rise as a function of their density compared to the density of the ambient lake water due to these advantages our new model can replace the model developed by schmid et al 2005 for longer transient simulations of lake kivu i e for example to model the evolution of gas concentrations and stratification due to large scale gas extraction projects although our model could be readily used for such simulations we do not focus on the changes induced by gas extraction in this work instead we use the model to address three specific scientific questions firstly ross et al 2015a suggested that the cool and relatively fresh sources at 190 m and 250 m might mainly consist of recently infiltrated rainwater at the northern shore of the lake there rainwater probably quickly percolates through the porous volcanic rock we can test this hypothesis using our model and recently acquired tritium 3h measurements secondly based on observations in the sedimentary record several authors ross et al 2015b votava et al 2017 uveges et al 2020 suggest that lake kivu experienced a large scale mixing event 1000 years ago with nutrient and gas rich water reaching the surface layers and creating algae blooms to assess whether a complete mixing event could have happened at that time we use our model to determine whether and within which time period today s stratification and gas concentrations could have developed from a homogeneous i e completely mixed and degassed lake finally we previously observed large depletions of atmospheric noble gases in the deep water of lake kivu bärenbold et al 2020b using our model we test our hypothesis that the inflow of depleted groundwater is the most probable cause of these depletions 2 methods 2 1 basic model description the lake model simstrat was originally developed at eawag by goudsmit et al 2002 and it is available on github https github com eawag appliedsystemanalysis simstrat simstrat solves the one dimensional hydrodynamic equations on a staggered grid using an implicit euler numerical scheme turbulent diffusion is resolved by a k ε closure which means that the turbulent kinetic energy k and its dissipation ε are explicitly modelled as state variables the main sources of turbulent kinetic energy are direct wind forcing and buoyant instability of the water column however simstrat also allows for turbulent energy production by indirect wind forcing i e the dissipation of internal waves goudsmit et al 2002 a certain fraction of the wind energy represented by the seiche energy parameter αs is redistributed to internal seiche energy which is subsequently dissipated through friction at the boundaries and supplies mixing energy distributed over the entire depth of the lake as discussed by goudsmit et al 2002 this process is a major source of energy for turbulent mixing in the hypolimnia of lakes and neglecting it leads to systematic underestimation of the vertical turbulent mixing the original version of simstrat has been updated several times in the past the most important upgrades were the use of a different heat flux parameterization schmid and köster 2016 the implementation of density driven plunging of inflows gaudard et al 2017 and the possibility to simulate lake ice cover gaudard et al 2019 in order to explicitly simulate the formation of ch4 in lake kivu we coupled simstrat v2 2 with the biogeochemical model library aed2 v1 3 5 https github com aquaticecodynamics libaed2 which is developed by the university of western australia aed2 consists of a set of biogeochemical modules carbon nitrogen phosphorus phytoplankton etc which each can be switched on and off or adjusted according to specific needs in this work we use the carbon and oxygen modules of aed2 to simulate the carbon cycle including the formation and consumption of ch4 the calculation of the radiation budget in simstrat requires some basic physical parameters water albedo is determined as a function of latitude and seasonality by using a linear interpolation of the modified values of grishchenko in cogley 1979 which are implemented in simstrat v2 2 we use a standard value of 0 35 for the fraction of shortwave radiation absorbed as heat in the surface boundary layer the remaining shortwave radiation is absorbed in each water layer with an extinction coefficient of 0 27 m 1 thiery et al 2014 in addition we need to define the geothermal heat flux which is not well constrained in the lake kivu region degens et al 1973 recorded a few measurements in the sediment which indicate a range of 0 016 0 18 w m 2 for our simulations we chose a value of 0 13 w m 2 which is well within this range and leads to the best agreement between observed and simulated deep water temperatures all simstrat settings and parameters are summarized in the supporting information tables s1 and s2 in the aed2 carbon module the ch4 sediment flux p c h 4 in mmol m 2 d 1 is modelled taking into account oxygen inhibition and temperature dependence 1 p c h 4 f s e d c h 4 k s e d c h 4 k s e d c h 4 o 2 θ s e d c h 4 t 20 where f s e d c h 4 is the maximum sediment ch4 flux at 20 c k s e d c h 4 is the half saturation constant for the inhibition of ch4 production by oxygen o 2 is the oxygen concentration and θ s e d c h 4 is the arrhenius temperature multiplier for ch4 production similarly ch4 oxidation by methanotrophs o x c h 4 in mmol m 3 d 1 is modelled as 2 o x c h 4 r o x c h 4 o 2 k o x c h 4 o 2 v t o x c h 4 t 20 where r o x c h 4 is the maximum oxidation rate at 20 c k o x c h 4 is the half saturation constant for ch4 oxidation and v t o x c h 4 is the arrhenius temperature multiplier for ch4 oxidation due to the narrow range of temperature in lake kivu 23 26 c we neglect the temperature dependence in both equations and equation 1 then reduces to a constant production rate f s e d c h 4 in the anoxic waters below 60 m see table s3 for numerical values ch4 oxidation by methanotrophs equation 2 on the other hand happens right at the oxycline and therefore is a function of oxycline depth note that for computational efficiency we did not include the simulation of nutrients and phytoplankton so oxygen consumption is underestimated and the oxycline depth is probably overestimated which slightly affects the depth at which ch4 disappears 2 2 modifications of the model for lake kivu due to exceptionally high ch4 and co2 concentrations and the presence of double diffusive staircases the simulation of lake kivu requires some changes to the original simstrat aed2 this adjusted version is called kivu simstrat see software availability section and it is used throughout this work the respective changes are described in this section and depicted in the modelling flow chart in fig 2 2 2 1 parameterization of double diffusive transport one of the unique features of lake kivu are the double diffusive staircases which exist below a depth of 120 m and all the way down to the lake bottom at 485 m newman 1976 sommer et al 2013 these staircases consist of 300 thin mixed layers with an average thickness of 70 cm and sharp gradients in between sommer et al 2013 double diffusion is caused by the coexistence of two substances with reciprocal effect on density and very different molecular diffusion coefficients in lake kivu the temperature increase with depth destabilizes the water column whereas the increase with depth of salinity and co2 somewhat weakened by the ch4 increase stabilizes it consequently the strong stratification suppresses turbulent diffusion below 120 m but molecular transport is enhanced by the step wise profile compared to a smooth profile by measuring the temperature and salinity gradients between double diffusive layers sommer et al 2019 concluded that the apparent temperature diffusivity calculated as the heat flux divided by the smoothed temperature gradient is usually between 10 6 and 10 5 m2 s 1 therefore double diffusion in lake kivu enhances the heat flux by a factor of 10 100 compared to a smooth profile with only molecular heat diffusion 1 4 10 7 m2 s 1 conversely the salinity flux which has a molecular diffusion coefficient of 1 2 10 9 m2 s 1 is increased by a factor of 30 300 due to the steeper interface gradients sommer et al 2013 double diffusive convection is an inherently three dimensional process and therefore not readily replicable in one dimensional models we initially chose a very high depth resolution grid cells 1 cm in order to correctly represent the thin interfaces however the simulated interfaces were too steep and the mixed layers too large and thus the resulting double diffusive heat flux was on the order of 0 3 w m 2 instead of 0 1 0 15 w m 2 determined by sommer et al 2013 because of this disagreement with data and because such a high spatial resolution is computationally very expensive we decided to resort to a coarser resolution which produces a smoothed profile between 120 and 485 m without double diffusive staircases we found that a resolution of 2 m is sufficient to correctly reproduce the smoothed profile and that it allows simulations over thousands of years within a few days time the double diffusive transport of heat and salinity through this smoothed profile is expressed as apparent diffusion and we parameterize it as a function of stability n 2 s 2 which is given by 3 n 2 g ρ 0 ρ z where g is the gravitational acceleration ρ is water density ρ 0 is the density of freshwater 1 kg l 1 and z is the vertical axis n 2 is thus proportional to the density gradient and positive values indicate a stably stratified water column while negative values represent unstable conditions and thus convective instability fig 3 a and b shows that the relationships between the apparent diffusivities of temperature k t app and salinity k s app data from sommer et al 2019 and n 2 correlate well in the log log domain the diffusivities can therefore be expressed as power laws according to 4 k t a p p 2 12 10 8 n 0 95 f 5 k s a p p 1 44 10 10 n 1 26 f where f 0 74 is a dimensionless correction factor this correction factor accounts for the fact that the parameterization tends to lead to steeper temperature and salinity gradients in the model compared to observations of sommer et al 2019 and thus slightly overestimates vertical transport at the logarithmic scale this correction factor consists of an offset of the y axis but preserves the slope of the regression line at high values of n 2 the lower limit for apparent diffusion is the respective molecular diffusion fig 3 conversely when n 2 is very close to zero i e 10 6 s 2 our parameterization simulates convective instability and can produce very high diffusivities to avoid unphysically high mixing rates we use a threshold of 10 10 s 2 for n 2 which yields upper bounds of 8 10 4 and 2 10 4 m2 s 1 for the diffusivities of temperature and salinity respectively the molecular diffusion coefficients of co2 ch4 sommer et al 2013 noble gases wise and houghton 1968 and water isotopes 3h and 2h mills 1973 are of similar order of magnitude compared to salinity and therefore the double diffusive transport of each of these variables is approximated as 6 k i a p p k s a p p d i m o l d s m o l where d i m o l and d s m o l are the molecular diffusion coefficients of variable i and salinity respectively 2 2 2 influence of dissolved gases on density due to their high concentrations salinity and dissolved gases need to be taken into account in water density calculations for lake kivu in our model we compute water density ρ as a function of temperature t according to chen and millero 1986 and we add the effects of salinity s co2 and ch4 similarly to schmid et al 2004 7 ρ t s c o 2 c h 4 ρ t 1 β s s β c o 2 c o 2 β c h 4 c h 4 where β s β c o 2 and β c h 4 are the contraction coefficients of salinity co2 and ch4 respectively see schmid et al 2004 for numerical values in equation 7 and throughout this work s is computed from conductivity measurements by using ionic composition in lake kivu according to wüest et al 1996 2 2 3 total alkalinity in the aed2 carbon module total alkalinity ta is derived as a regression of dissolved inorganic carbon dic and s with different built in regression functions to choose from for lake kivu wüest et al 2009 showed that ta in mmol m 3 is a linear function of s in g kg 1 and suggested the following relation 8 t a 11960 s we implemented this option in aed2 and used it throughout our simulations 2 2 4 lake atmosphere interactions of water isotopes 3h and 2h and noble gases in order to model the water isotopes 3h and 2h we take into account their interaction with the atmosphere using the craig gordon model see for example aeschbach hertig 1994 the net flux f c of the isotope concentration c out of the water column is given by the combined effect of evaporation and precipitation 9 f c e α c p h w c l 1 h δ ε r c p where c p is the concentration in precipitation c l the surface concentration in the lake e the evaporation rate r the precipitation rate h w the relative humidity with respect to surface water temperature α the equilibrium fractionation coefficient and δε the kinetic enrichment for 3h we assume an equilibrium fractionation factor α of 0 89 and negligible kinetic fractionation aeschbach hertig 1994 for 2h α is computed according to majoube 1971 and the kinetic enrichment factor δ ε is derived from relative humidity gonfiantini 1986 finally we need to determine the precipitation isotope signals for 3h we use the database of the global network of isotopes in precipitation gnip because data is scarce in africa and there is no measurement station close to lake kivu we use the average of surrounding stations to derive a time series with yearly data from 1953 to 2018 iaea wmo 2020 note that there is no data available after 2000 and thus we calculate the values from 2000 to 2018 using exponential extrapolation of the trend from 1990 to 2000 see figs s1 and s2 in the supporting information we express 2h in δ notation i e as excess with regard to its ratio with the vienna standard mean ocean water vsmow and we use a constant precipitation value of 4 5 because i the value is in the range of observed values iaea wmo 2020 ii data is too scarce for deriving seasonal variation and iii the value leads to a good agreement between modelled δ2h signals and observations at the lake surface note that we also use these 3h and δ2h precipitation values for the surface inflows tributaries into lake kivu thus neglecting evaporation during transport to the lake the assumption of neglecting evaporation effects on the isotopic composition of the tributaries is motivated by the fact that the watershed is small and thus rainwater ends up in the lake quickly in reality there is substantial evaporation despite the small watershed and this in turn would lead to higher δ2h values in tributaries compared to precipitation we neglected this evaporation effect in our study i e assumed the same δ2h in both tributaries and precipitation because we do not simulate 2h dynamics in the surface waters the exchange of noble gases across the lake atmosphere interface was approximated by prescribing air saturated water at 25 c at the lake surface due to virtually constant concentrations of noble gases in the atmosphere and because the surface water temperature of lake kivu is usually close to 25 c 2 3 meteorological forcing simstrat requires air temperature t a horizontal wind speed at 10 m above the water surface uv 10 wind direction φ atmospheric vapor pressure e a incoming short wave radiation sw in and incoming long wave radiation lw in these meteorological variables are monitored continuously every 30 min by an automatic weather station on a platform on lake kivu see rooney et al 2018 for details w thiery from vrije universiteit brussel kindly provided continuous data from october 2012 to september 2019 from this weather station to allow for the simulation of longer timescales we use the output of a regional climate model which is used for scenario simulations within the isimip project inter sectoral impact model intercomparison project see frieler et al 2017 for scenario descriptions the climate model output consists of daily modelled values of all important meteorological variables from the year 1660 2300 at lake kivu grid cell 1 75 s 29 2 e we chose miroc5 from the four available regional models but the choice of climate model is not important as long as interannual variability is well represented the air pressure and air temperature of the climate model are corrected by adjusting their average to the measured average by the weather station fig 2 while wind speed and incoming radiation are adjusted using simstrat s built in correction factors table 1 note that the difference in air pressure and temperature table 1 is caused by the higher elevation of the climate model grid cell compared to lake kivu we also noticed that wind directions differ between observations and the regional climate model probably due to local effects below the grid scale of the climate model however simulation results are only weakly sensitive to wind direction in the lake model because wind direction mostly influences the timing and less the magnitude of mixing events in addition the magnitude of mixing events is scaled by the wind and seiche energy fit parameters of the model to match the observed temperature and salinity profiles finally the difference in wind speed could be shown to be caused by the different time resolutions of the weather station half hourly and the climate model daily averages using the corrected climate modelling output of miroc5 we created two forcing time series i the control dataset picontrol from 1660 to 2300 which represents climate forcing with constant pre industrial greenhouse gas concentrations and ii a combination of a historical dataset from 1860 to 2005 and a projection from 2006 to 2100 with a net radiative forcing of 6 0 w m 2 in 2100 which will be called rcp6 representative concentration pathway 6 0 the goal of the picontrol time series is to provide a realistic but stochastic distribution of seasonal mixing depth for the steady state and long term simulations see results section while rcp6 is only used for the transient simulation of 3h from 1953 to 2018 2 4 water balance the water balance of lake kivu can be written as 10 p i g e o with precipitation p surface inflow i groundwater inflow g evaporation e and outflow o precipitation and evaporation are almost equal in lake kivu schmid and wüest 2012 muvundja et al 2014 meaning that the discharge of inflowing tributaries and groundwater is mostly balanced by the outflowing ruzizi river in our model we use the values by schmid and wüest 2012 who suggest average values of 63 m3 s 1 for surface inflow 41 m3 s 1 for groundwater inflow and 95 m3 s 1 for the outflow we add the net difference between p and e to the outflow which then amounts to 108 m3 s 1 2 5 tritium 3h measurements 3h or t is a radioactive hydrogen isotope which is transported in the environment as tritiated water hto it is extremely rare in nature due to low natural production and a relatively fast decay with a half life of 12 3 years however the atomic bomb tests in the 1950s and 60s released large amounts of 3h into the atmosphere up to several hundred tritium units tu where 1 tu is 1 3h atom per 1018 water atoms since then 3h has been widely used in lake and groundwater environments as a physical tracer kipfer et al 2002 we took water samples using the pressure proof sampling equipment described in bärenbold et al 2020b and analyzed them for 3h content by measuring its decay product 3he according to beyerle et al 2000 in this procedure samples are degassed under vacuum in order to evacuate the naturally present 3he then left on the shelf to allow for sufficient decay of 3h to 3he typically 6 12 months and subsequently analyzed for newly produced 3he knowing the amount of 3he produced in the sample and the elapsed time one can back calculate the original amount of 3h present in the sample due to the high helium content in lake kivu bärenbold et al 2020b some of our samples had to be degassed twice to remove all naturally present 3he the estimated standard deviation of our measurements due to calibration uncertainty and possible contamination with atmospheric air is in the range of 0 03 0 10 tu in addition to this data we added two unpublished 3h measurements taken by our own group during an expedition in 2004 3 results and discussion 3 1 seasonal mixing dynamics 0 60 m seasonal mixing in lake kivu usually occurs during the dry season and affects the top 30 60 m it is strongest during july and august due to high wind speeds and lower air temperatures which lead to convective instability and enhanced mixing schmid and wüest 2012 interannual variability of seasonal mixing depth is therefore mainly driven by meteorological conditions the extent of the surface layer where conductivity is nearly constant gives an impression of varying mixing depth in different years fig 4 a the typical range of surface temperatures in lake kivu is 23 26 c thiery et al 2014 morana et al 2015 with warmer temperatures in the stratified rainy season and cooler temperatures in the windier dry season in the model average mixing depth and surface temperature are mainly determined by the fit parameters of wind speed short wave radiation and long wave radiation by manually tuning these three parameters table s2 for numerical values we can adequately reproduce mixing depth which varies between 20 and 60 m fig 4b modelled water temperatures exhibit a range of 23 27 c fig 4c with peak water temperatures during stratified periods occasionally 1 2 c higher than typical observations thiery et al 2014 morana et al 2015 however as long as the temporal distribution of the mixed layer extent is realistic these higher surface temperatures will not influence our model results 3 2 turbulent mixing at intermediate depths 60 120 m diffusive transport between 60 and 120 m is governed neither by seasonal mixing nor by double diffusive convection this depth range is characterized by a strong increase in salinity with depth and therefore a strong density gradient it was therefore put forward by schmid et al 2005 that turbulent mixing in this depth range is weak in contrast the stable isotope measurements by ross et al 2015a indicate that mixing might be stronger than previously thought in fact their measurements seem to show that the evaporative isotope signal at the lake surface is mixed down against the groundwater induced upwelling our simulation results suggest that direct wind induced turbulent mixing is indeed very weak in lake kivu below 60 m the wind induced currents are not able to penetrate below the surface layer and without considering the energy transfer via internal seiches the modelled salinity gradient below the surface mixed layer becomes much steeper than observed fig 5 a if the seiche energy parameter α s is tuned to 0 0027 i e 0 27 of wind energy is transformed to seiche energy our model can reproduce the observed salinity gradient very well fig 5b except for a small jump at 120 m this jump is an artefact caused by the transition from the k ε mixing scheme to the parameterization of double diffusion we were able to reduce the size of the jump by tuning the calibration parameter q nn but not to eliminate it the parameter q nn determines how the mixing energy from internal seiching is distributed vertically as a function of stability n 2 we conclude that internal seiching can explain why there is still a certain amount of turbulent mixing 10 7 10 6 m2 s 1 in a depth region where the density gradient is very strong 3 3 steady state simulation while diffusive transport is computed using the k ε closure 0 120 m and a parameterization of double diffusive convection 120 485 m vertical advection is a function of the discharge inflow depth and properties t s co2 ch4 of the different groundwater springs present in lake kivu unfortunately the properties of the groundwater sources are inherently difficult to measure in the field however despite the lack of data we can use indirect evidence to shed light on the influence of the groundwater sources on lake kivu using the water balance of lake kivu schmid and wüest 2012 suggested that the total groundwater input into lake kivu is between 32 and 48 m3 s 1 in addition ross et al 2015a were able to identify several warm and saline signals below 250 m as well as a strong cool and less saline signal right at the chemocline 250 255 m and a more diffusive cool signal between 90 and 195 m based on this information and the current vertical structure of the lake we assume that i four hydrothermal point sources enter the lake below 250 m and plunge due to their high density ii two cooler point sources enter the lake at 253 and 190 m and iii one diffusive groundwater source which has the same properties as the point source at 190 m enters the lake between 90 and 195 m depth fig 2 in ross et al 2015a we summarized this information in table 2 throughout this work we refer to the ground water sources at 190 and 253 m as cool sources 1 and 2 respectively and the sources below 250 m as hydrothermal sources the water introduced as point sources is free to plunge or rise according to its density in simstrat thereby entraining ambient lake water the final stratification depth of these groundwater inflows is not only influenced by their temperature salinity and gas content but also by the current lake density profile which leads to a highly complex and interdependent calibration problem with many unknown variables in order to calibrate inflow depth and discharge but also temperature salinity and co2 content of the groundwater sources we chose to tune our model with the goal of producing a steady state with temperature salinity and gas profiles close to today s observations fig 2 this approach is motivated by the findings of bärenbold et al 2020a who conclude that ch4 and co2 concentrations did not show a clear temporal trend during the last 45 years conversely it has been shown katsev et al 2014 sommer et al 2019 that lake kivu experienced a warming trend on the order of 0 01 c yr between 2004 and 2015 throughout the stratified deep water below 60 m we chose to neglect this warming rate in our simulations for two reasons i the observed warming trend seems variable in time i e no warming observed below 250 m from 1974 to 2004 and is comparably slow and ii we focus on the present state and the past evolution of lake kivu in this work not on its future evolution the causes for the recent lake warming have not been clearly identified the warming near the surface is likely due to climate change and thus probably a recent phenomenon katsev et al 2014 in contrast to this recent trend the near surface lake temperatures could have varied to some extent in both directions during the simulated period of the past 2000 years depending on past climate conditions the causes for the recent warming of the deep water are unknown and it needs to be further evaluated in the future whether they represent a persistent change in the thermal dynamics of the lake that requires adaptation of the model for future projections note that a warming trend could be introduced into the model by slightly modifying the temperature of the hydrothermal groundwater inflows or the geothermal heat flux while most of the co2 stems from groundwater inflow this is not true for ch4 which was shown to contain a mixture of modern and old 14c dead carbon schoell et al 1988 pasche et al 2011 using 14c analysis pasche et al 2011 showed that the degradation of organic matter can only provide a maximum of 50 of ch4 below 250 m the remaining 50 stem either from reduction of old geogenic co2 or from the direct inflow of geogenic ch4 the absolute ch4 fluxes derived in pasche et al 2011 lead to an increase in ch4 concentrations of around 7 8 within 30 years however such an increase is clearly not validated by the most recent measurements bärenbold et al 2020a which indicate approximately constant concentrations since the measurements of tietze in 1974 tietze 1978 therefore we assume that the actual ch4 production below 250 m is close to a steady state production of 28 gc m 2 y 1 according to pasche et al 2011 50 of this production is supplied by degradation of organic matter and therefore we set f sed ch4 in aed2 to 3 2 mmol m 2 d 1 ch4 14 0 gc m 2 y 1 for the whole lake depth for the remaining 50 below 250 m we will explore two scenarios i the reduction of geogenic co2 to ch4 within the lake implemented analogously to ch4 production from organic matter and ii the direct groundwater mediated inflow of geogenic ch4 in both cases an additional 3 2 mmol m 2 d 1 of ch4 is added below 250 m thus yielding a total ch4 production of 6 4 mmol m 2 d 1 28 gc m 2 y 1 below 250 m fig 6 shows simulation results for temperature salinity co2 and ch4 for a simulation duration of 500 years exact groundwater properties used in the simulation are provided in table s4 the initial conditions used in this simulation are the temperature and salinity background profile of ross et al 2015a and the ch4 and co2 concentrations measured in 2018 bärenbold et al 2020a our model assumptions and tuned inflows lead to a steady state after 400 500 years for temperature salinity and co2 concentrations with co2 concentrations agreeing with observations well within measurement uncertainty the agreement between data and simulation is also excellent for salinity with a root mean square error rmse of 0 014 but not for temperature while the general structure of the temperature profile is generally well reproduced rmse of 0 024 c modelled temperature is consistently 0 3 c too low between 250 and 350 m and 0 1 c too high below 400 m despite a large calibration effort on the groundwater properties these differences could not be eliminated the main difference between temperature and the other main agents in lake kivu is that the transport of salinity and gases is dominated by vertical advection i e the upwelling caused by groundwater inflow while for temperature both advection and diffusive transport are important sommer et al 2019 as a consequence it seems that the properties of the groundwater sources are well calibrated while the parameterization of heat transport through the double diffusive staircases only works satisfactorily indeed fig 3a shows that the proposed linear regression reflects well the general relationship between stability and apparent temperature diffusivity however the variability is rather large thus indicating that the prediction capacity of stability to deduce double diffusive transport in lake kivu is limited for ch4 we explored two scenarios with different origins for the 14c dead part of ch4 scenario 1 implies that the remaining 50 of ch4 are produced by co2 reduction within the lake fig 6d while in scenario 2 the remaining 50 stem from groundwater fig 6e scenario 1 indicates that ch4 concentrations should reach a clear maximum between 300 and 400 m and decrease towards the lake bottom due to the inflow of ch4 free groundwater there this decrease is supported by the ch4 data of tietze in 1974 tietze 1978 and the eawag dataset in bärenbold et al 2020a but not by the data of halbwachs and tochon in 2003 schmid et al 2005 and the ufz dataset in bärenbold et al 2020a scenario 2 also shows slightly decreasing concentrations at the lake bottom but this decrease is small and could probably be eliminated by further tuning the ch4 concentrations in the groundwater sources see table s4 for values used in scenario 2 scenario 2 agrees better with observed ch4 concentrations and thus our results suggest that at least part of the remaining 50 of ch4 are introduced by the hydrothermal groundwater sources in both scenarios ch4 production from the decomposition of organic matter is assumed constant with depth in reality sediment focusing or the reduction of degradable organic matter at greater depths due to longer settling times could potentially lead to varying ch4 production with depth 3 4 water tracer simulations in this section we use the calibrated steady state model to reproduce the 3h and δ2h measurements from this work and by ross et al 2015a respectively 3 4 1 3h simulations water age and origin there are no direct 3h measurements available from the groundwater sources in general 3h in groundwater can have two origins namely a low rather constant background signal due to interaction with rocks mamyrin and tolstikhin 1984 and a variable signal due to gas exchange with the atmosphere therefore we make the following assumptions for our model runs i the old hydrothermal groundwater in the lake kivu basin has a constant background 3h signal of 0 5 tu due to interaction with rocks ii young groundwater stems from the recent infiltration of precipitation and has an atmospheric 3h signal and iii all groundwater sources in lake kivu can be a mixture of old and young groundwater note that we assume that the time from infiltration of precipitation to groundwater recharge is short i e there is no lag that would decrease the 3h concentration due to radioactive decay this assumption is justified by the fact that the northern shore where the groundwater sources were found ross et al 2015a consists of permeable volcanic rock which allows fast infiltration of rainwater fig 7 a shows simulation results under the assumption that every groundwater source is entirely composed of old hydrothermal groundwater compared to field observations from 2004 to 2018 see table s5 for numerical values the observations show a decrease in time in the seasonal mixed layer from 2 2 tu in 2004 to 1 6 tu in 2018 and a decrease with depth from 1 6 tu to close to 0 tu at 200 m the decrease in time can be explained by decreasing 3h concentrations in the atmosphere due to radioactive decay of the remaining bomb 3h while the decrease with depth shows that the water below 60 m is predominantly old i e no recent interaction with the atmosphere and that vertical mixing is weak the assumption that all groundwater sources are entirely composed of old groundwater agrees very well with the observations in the mixed layer as well as with the observation below 300 m however between 100 and 200 m the measured 3h concentrations are higher than predicted by the simulation thus indicating that the cool groundwater sources do also contain atmospheric 3h indeed if we adjust the fraction of young groundwater in the cool sources 1 and 2 to 45 and 20 respectively we get a very good agreement with the observations in the corresponding depth range fig 7b the finding that the cool groundwater sources are partially young is supported by the absence of surface tributaries in the north of lake kivu where ross et al 2015a identified the cool sources this northern part of the catchment has a surface area of 686 km2 which accounts for 14 of the whole lake kivu catchment area of 4940 km2 we can estimate a lower bound for the amount of water which infiltrates and feeds the groundwater sources by using the surface inflow of 63 m3 s 1 schmid and wüest 2012 as 86 of the catchment produce 63 m3 s 1 of run off the remaining 14 would yield a potential groundwater recharge of 10 m3 s 1 however precipitation is expected to infiltrate quickly through the porous volcanic rock in the north and thus there is less water lost to evaporation than in the remaining watershed an upper bound of the infiltrated water can be derived by using the average precipitation of 1470 mm yr 1 muvundja et al 2014 which would give 32 m3 s 1 over volcanic part of the catchment the amount of young water required by our calibrated model to fit the observed 3h profile is 15 5 m3 s 1 with a range of 11 5 18 m3 s 1 range estimated from uncertainty of 3h and is therefore well within the range of 10 32 m3 s 1 given by run off and precipitation data we conclude that the cooler groundwater sources are probably a mixture of old and more recent groundwater with 45 and 20 of recently infiltrated water in cool sources 1 and 2 respectively conversely our only observation below 300 m suggests that the deep groundwater sources predominantly consist of old hydrothermal water 3 4 2 δ2h simulations contradict field observations in contrast to 3h there are field observations available for δ2h for at least some of the groundwater sources ross et al 2015a below 200 m ross et al 2015a found very variable δ2h groundwater signals ranging from 2 8 to 17 4 in our model we cannot use these values because our implemented hydrothermal sources do not directly correspond to the hydrothermal sources found by ross et al 2015a therefore the δ2h concentrations of the hydrothermal sources are adjusted to reach the best agreement between simulation and observed background observations by ross et al 2015a in contrast to the warm sources our cool sources 1 and 2 were clearly identified by ross et al 2015a and thus fig 7c shows a simulation where the δ2h concentrations of cool groundwater sources 1 and 2 correspond to the values found by ross et al 2015a while in 7d we adjust all sources so that the model fits the observed background profile of ross et al 2015a in both figures we also show the δ2h observations of tietze 1978 and katsev et al 2014 comparing the three datasets illustrates the seasonal variability of the δ2h values in the surface layer as well as the generally good agreement below 100 m due to better data consistency i e less scattering of measurements at similar depths we use the data of ross et al 2015a for the calibration of δ2h values in the inflows fig 7d to reproduce the observed δ2h values in the surface layer the δ2h concentration in precipitation was fixed to 4 5 meaning that the model cannot reproduce seasonal or interannual variations at the surface when using the δ2h observations of ross et al 2015a as groundwater input the observed δ2h values above 250 m are considerably higher compared to simulated δ2h fig 7c the δ2h values in groundwater necessary to provide a good agreement of observations and simulation are around 15 and 30 higher for cool sources 1 and 2 respectively fig 7d see table s6 for numerical values these differences are well beyond measurement uncertainty and in addition both δ2h groundwater observations consist of several measurements with rather low variability pers comm k a ross the results of the δ2h simulations are further corroborated by δ18o simulations not shown in this work which exhibit the same systematic deviation from observations we conclude that our model is able to reproduce the observed δ2h profiles within the uncertainty of almost all data points of ross et al 2015a if the inflow concentrations are freely adjustable and allowed to have an evaporative δ2h signal in this case the observed stable isotope profiles are not in contradiction with low turbulent mixing in lake kivu however the observed δ2h concentrations close to cool sources 1 and 2 are not high enough to explain the observed lake profiles potentially a certain discrepancy could be because δ2h in the groundwater inflows or the groundwater inflows themselves are variable in time however it is unlikely that this could explain a difference of 15 and 30 in inflow concentrations and we are unable to fully understand the reason for this disagreement between model and observations 3 5 origin and maintenance of lake kivu s density stratification it is generally assumed that lake kivu has experienced several partial degassing events in the past haberyan and hecky 1987 zhang et al 2014 ross et al 2015b votava et al 2017 uveges et al 2020 this may have happened by spontaneous ebullition due to gas oversaturation in the deep waters another possibility is the creation of instability within the water column due to either subaquatic volcanism ross et al 2015b or hyperpycnal flows due to exceptional rain events zhang et al 2014 ross et al 2015b and uveges et al 2020 both estimate that the last large mixing event may have occurred 800 1000 years ago with several smaller events between 800 years bp and the present in this context we can use our model to evaluate how long it takes lake kivu to reach its current state because we don t know when and to what extent lake kivu has mixed in the past we assume that i the lake is completely mixed and gas free at the start of the simulation ii the groundwater discharge and properties i e temperature salinity and gas concentrations are constant in time and the same as today and iii the average climatic conditions are constant and the same as today our simulated salinity and gas concentrations agree well with today s state after roughly 1500 years fig 8 thus confirming that the density driven inflow of several groundwater sources can explain today s vertical lake structure this result is in contrast to arguments by hirslund 2020 who suggested that the inflow of groundwater sources cannot be responsible for lake kivu s density stratification for temperature it takes more time to reach a steady state the reason is that double diffusive transport is much faster for temperature compared to salinity and gases due to the much higher molecular diffusion coefficient while the vertical transport of salinity and gas concentrations is dominated by advection double diffusion contributes around half of the vertical heat flux the double diffusive heat flux is parameterized as a function of n 2 and therefore a stable temperature profile will only develop once the density stratification mainly determined by salinity and co2 is fairly stable after 1500 years at this point the overshooting temperature below 350 m decreases again and converges to 26 c our model results suggest that we should see slightly decreasing deep water temperatures if the lake is close to a steady state in contrast recent temperature observations indicate an increase of 0 01 c yr 1 sommer et al 2019 there are several possible answers to this problem i the lake is not at a steady state and thus temperature salinity and gas concentrations are still slightly increasing ii the lake was close to a steady state but there was an increased discharge of hydrothermal water during the last decades which counteracts the temperature decrease and iii our parameterization of double diffusive heat flux is not suitable for the simulation of a near homogeneous lake at the start of the simulation and thus the simulated overshooting is a model artefact note that answer ii would also imply a slight steepening of the main chemocline at 250 m due to increased upwelling as it was observed recently hirslund 2012 however this steepening stopped again after 2012 pers comm m schmid assuming constant groundwater inflows throughout the simulation duration our simulations show that it is improbable that today s lake structure has developed within only 1000 years from an initially homogeneous and degassed lake the time which is necessary to reach a steady state close to today s observed temperature salinity and gas concentrations rather seems to be around 2000 years therefore a potential mixing event 1000 years ago as suggested by ross et al 2015b and uveges et al 2020 could not have mixed the entire lake however a partial mixing event 1000 years ago or even smaller mixing events during the last few centuries are well compatible with our simulations 3 6 atmospheric noble gas depletions in the deep water in an earlier publication we described that the atmospheric noble gases i e the noble gases whose main source is the atmosphere ne 36ar and kr are depleted by 55 70 in the deep water of lake kivu bärenbold et al 2020b therein we discussed three different mechanisms to explain the observed noble gas depletions namely i continuous outgassing ii the inflow of noble gas depleted groundwater and iii a relic of a past large outgassing event mechanism i was subsequently excluded because the isotope ratios 20ne 22ne and 40ar 36ar did not show the expected depletion patterns bärenbold et al 2020b we further argued that mechanism ii the inflow of depleted groundwater is the most probable reason for the observed profiles in this section we use the model setup of the previous section to test and evaluate hypotheses ii and iii for hypothesis ii we adjusted the concentrations in the groundwater sources to fit the observed noble gas profiles in bärenbold et al 2020a conversely in hypothesis iii the noble gas concentrations in the groundwater are fixed to those of air saturated water asw at 25 c the resulting ne profiles indicate that we can exclude that the currently observed profiles are a result of a past outgassing event only fig 9 b in contrast the observed concentrations can be perfectly reproduced by the inflow of noble gas depleted groundwater fig 9a the best agreement with observations is reached if the hydrothermal sources are depleted by 40 50 table s6 and the cool sources 1 and 2 by 5 and 10 respectively the depth profiles of the other atmospheric noble gases measured in bärenbold et al 2020b can be successfully reproduced as well but are not shown here 3 7 model limitations compared to past one dimensional modelling attempts on lake kivu schmid et al 2005 one of the main advantages of our model is the fact that turbulent transport is dynamically calculated throughout the whole water column from 0 to 120 m we use simstrat s built in k ε turbulence closure and below we apply a newly derived parameterization which is based on measurements by sommer et al 2013 this situation creates an artificial transition point at 120 m however while both the transition at 120 m and the parameterization are valid for today s lake structure back to at least 1976 newman 1976 we have no information about the presence and extent of double diffusive staircases during the past centuries particularly during periods with a weaker density stratification we do not know how well our parameterization performs nevertheless it can be assumed that an initially weak density stratification quickly becomes stronger due to the constant deep inflow of salts and co2 as a consequence although turbulent transport might initially be underestimated by our parameterization this probably does not significantly alter the simulations results another important simplifying assumption of our modelling exercise is the fact that climatic conditions as well as the discharges and properties of all groundwater sources are assumed to be constant in time part of the cooler groundwater sources seems to have an atmospheric 3h signature and thus we expect the discharge and properties of these sources to vary with climate conditions i e higher discharge in a wet climate lower discharge during droughts furthermore volcanic activity can greatly influence hydrological parameters like evaporation and rock permeability and thus lead to variable infiltration and groundwater properties over time for example votava et al 2017 suggest that the intermittent deposition of caco3 found in lake sediments indicate a corresponding intermittency of deep groundwater inflow into lake kivu however for our long term simulations it is not crucial to know whether climate or inflow do vary over shorter timescales due to the long water residence times in the stably stratified deep waters finally it is clear that our model is overparameterized which is unavoidable given the lack of direct observations of the properties of the groundwater sources besides the calibration parameters of simstrat aed2 including meteorological and mixing fit parameters the properties of every groundwater source temperature salinity gas content stable isotopes etc as well as their inflow depths and discharges can be freely adjusted thus even if the simulations are constrained by observed vertical profiles and the water balance likely several parameter sets exist which yield comparable results for example the discharge of the hydrothermal groundwater sources could be lower but its temperature salinity and gas content could be higher to keep equal the heat salt and gas input by the sources however such changes to groundwater properties also influence density and thus the depth where the source stratifies which makes the search for alternative solutions difficult our suggested set of parameters and inflow properties might not be unique but it strongly suggests that to form and maintain the current state several warm and dense groundwater sources below 260 m and two cooler sources at 190 and 250 m depth are required 4 conclusions we developed and calibrated a one dimensional model for lake kivu exploiting the fact that the lake is well mixed horizontally below 60 m in order to simulate the effect of dissolved gases co2 and ch4 on lake physics we first coupled the physical lake model simstrat with the biogeochemical model library aed2 we then developed a parameterization of vertical transport of heat salts and gases across the double diffusive staircases of lake kivu below 120 m as a function of lake stability n 2 after manually calibrating temperature salinity and gas content of the inflowing groundwater sources we were able to reproduce today s lake profile by a steady state simulation using this steady state model and our 3h dataset allowed to show that the cooler groundwater sources which enter the lake between 100 and 250 m are probably a mixture of modern and old hydrothermal water most importantly we showed that the evolution of lake kivu from a completely homogeneous and gas free state to today s density stratification within 2000 years is successfully reproduced using a one dimensional lake model with constant groundwater inflows our results demonstrate that the stratification depth of groundwater inflows markedly shifts as a function of the evolving density structure of the lake as the duration of 2000 years is an upper bound which is valid for an initially completely mixed lake our simulations indicate that a partial mixing event 1000 years bp zhang et al 2014 ross et al 2015b uveges et al 2020 is in good agreement with the currently observed state of the lake furthermore our simulation results suggest that the depletion of atmospheric noble gases in the deep water of lake kivu bärenbold et al 2020b are not a relic from a past outgassing event but have to be sustained by an active process i e inflow of noble gas depleted groundwater in this work we used our model to focus on finding a steady state solution for today s lake structure and on reproducing the past evolution of lake kivu in the future our model can be a valuable tool to simulate the effect of a changing climate on lake temperature stratification and nutrient availability in addition it can facilitate the decision making concerning the construction and operation of ongoing and future methane harvesting projects in lake kivu in order to better constrain the discharge and properties of the groundwater inflows in future model applications more research on the hydrogeological setting in the lake kivu region would be beneficial moreover the sampling of inflowing groundwater sources using a remotely operated underwater vehicle rov could allow the determination of temperature salinity and gas concentrations of groundwater sources without contamination with ambient lake water the kivu simstrat model is derived from the coupling of the physical lake model simstrat and the biogeochemical library aed2 this coupled model without the specific changes for lake kivu represents a computationally efficient and physically sound tool to simulate geochemical tracers and biochemical processes in lakes and reservoirs worldwide in the future funding this work was supported by the swiss national science foundation grant number 200021 160114 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we would like to thank augusta umutoni and ange mugisha from lkmp lake kivu monitoring programme for help with the organization of the field work and reto britt michael plüss maximilian schmidt and the whole team from lkmp for help with tritium sampling in lake kivu we also want to thank alexandra lightfoot and matthias brennwald from the noble gas laboratory at eth zurich for help with tritium sample analysis adrien gaudard and davide vanzo provided helpful insights into modelling and the implementation of new simstrat features particularly adrien s manual was of great help we also would like to acknowledge wim thiery from vrije universiteit brussel who made the data of the lake kivu weather station available to us appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105251 
25694,under ongoing global warming due to climate change heat waves in australia are expected to become more frequent and severe extreme heat waves have devastating impacts on both terrestrial and marine ecosystems a multi characteristic heat wave framework is used to estimate historical and future projected heat waves across australia a google earth engine based toolkit named heat wave tracker hwt is developed which can be used for dynamic visualization extraction and processing of complex heat wave events the toolkit exploits the public long term high resolution climate datasets to developed nine heat wave datasets across australia for extreme heat wave value analysis to examine climate change on heat waves and how they vary in time and space we also explore the probability and return periods of extreme heat waves over a period of 100 years the datasets toolkit and findings we developed contribute to global studies on heat waves under accelerated global warming keywords extreme heat wave google earth engine climate datasets risk analysis gcm australia 1 introduction under ongoing global warming due to climate change heat waves are expected to become more frequent and severe in the future ipcc 2019 extreme heat waves during the last two decades have been recorded across many regions in the world such as those in europe in 2003 schär et al 2004 moscow region in russia in 2010 rahmstorf and coumou 2011 and australia in 2013 lewis and karoly 2013 heat waves in australia incur significant hazard for both humans and ecosystems and cause more deaths than other natural hazards including floods storms and bushfires in terms of heat wave impacts on ecosystems extreme heat waves increase the probability of bushfire risk affect crops and food security for terrestrial systems luo 2011 and cause catastrophic damage to marine ecosystems hobday et al 2016 moreover extreme temperatures contribute to widespread unfavorable health outcomes and even the death of vulnerable people although heat wave is commonly known as a period of exceptional hot weather event there is currently no universal informative measurement in climate science community alexander and perkins 2013 to overcome these issues a set of climate indices developed by the expert team on climate change detection and indices etccdi have been widely applied to observational and modeled climate data to understand previous and future changes in extreme heat wave events zhang and yang 2004 alexander et al 2006 the work by etccdi is extensively recognized as pioneering however the indices only measure one feature of extreme events such as frequency intensity or duration perkins 2015 a comprehensive and consistent analysis of heat waves is required which should consider multi characteristics of heat wave events namely i frequency ii intensity iii duration and iv spatial extent raei et al 2018 the multi characteristic heat wave definition method used in this study is from a well known heat wave framework constructed by alexander and perkins 2013 and includes a minimum temperature approach a maximum temperature approach and an excess heat factor ehf approach this framework has proven to be successful in measuring historical and future projected heat waves however useful public software or tools that identify all the required characteristics of heat waves frequency intensity duration spatial extent are still rare most studies with their own tools cannot fully reflect the four characteristics of complex heat wave events feron et al 2019 lyon et al 2019 li 2020 by summarizing the classical heat wave definition an r package called heatwaver was developed which provides a comprehensive analysis to detect and visualize ocean heat waves schlegel and smit 2017 however it is inefficient when applied to large gridded data products global heat wave and warm spell data record and analysis toolbox ghwr which is a matlab toolbox allows processing and extracting heat wave records for any location efficiently it not only contains multiple definitions but also detects the required multi characteristics raei et al 2018 however desktop applications like ghwr still have a bottleneck when encountering the challenges related to accessibility of long term gridded climate data data storage and computational requirements in the current era of big spatial and earth observation eo data users need to deal with a vast amount of different spectral temporal and spatial resolutions data gomes et al 2020 to meet these demands there is need for novel technologies based on cloud computing to properly extract heat wave information in the server side without having to download vast amounts of climate data and provide dynamic visualization extraction and processing of complex heat wave events google earth engine gee a powerful cloud computing geospatial analysis platform has given researchers the opportunity to use big data for petabyte scale environmental data analysis gorelick et al 2017 with the gridded global reanalysed datasets e g hadley centre global historical climatology network hadghcnd climate prediction centre cpc and regional reanalysis datasets e g the coordinated regional downscaling experiment cordex australian water availability project awap being freely available many studies have investigated heat waves at various scales perkins et al 2012 ma et al 2020 christidis et al 2014 the atmospheric reanalysis datasets are quite useful for gaining understanding in how the heat wave will change reanalysis datasets are created by data assimilation and numeric models to represent a synthesized estimate of the atmospheric state and provide global scale dataset over several decades or longer one benefit of using reanalysis data is that it extends the study to locations without observation records another important advantage is that the spatially contiguous heat wave regions derived from the reanalysis data have crucial implications for heat related impacts such as exposure of the community to extreme heat wave events and high energy demands lyon et al 2019 li 2020 however some heat wave assessments are mostly based on climate datasets with relatively coarse resolution which would affect the representation of heat waves resulting in biased conclusions furthermore key processes that occur on regional scales may not be adequately simulated benefiting from those newly reanalysed climate datasets and high spatio temporal gridded regional climate datasets our analysis will explore how these climate datasets differ in representing heat waves and how the methods differ in identifying and characterizing heat waves increasingly researchers are becoming less interested in data in the normal range and more concerned with the abnormal and extreme events that are recurrent and unpredictable extreme value theory evt is the statistical framework that estimates the probability of an extreme event occurring in the future coles et al 2001 because of its importance many public packages and toolboxes over the last decade have been developed to implement various methods from evt ribatet et al 2011 cheng et al 2014 gilleland and katz 2016 heffernan et al 2016 it is clear from much of the literature using gridded observed data and projected climate model data at regional and global scales that the probability of extreme heat waves will change over time alexander and perkins 2013 purich et al 2014 recently several studies of the risks of heat wave by means of the evt have been published ma et al 2020 tanarhte et al 2015 shen et al 2016 however the precise probabilities of intensity frequency and duration of extreme heat wave at a continent scale like australia over the time are still unknown meanwhile the potential impact of climate change on heat wave varies in space and time in this context we explore the risk of heat waves in australia by performing non stationary analysis of extreme heat waves for the past 100 years in this study we will develop a multi method global heat wave data record and analysis toolbox namely heat wave tracker to process and extract heat wave records from multi source climate datasets with our toolbox s computational power in handling long term high resolution climate datasets we developed nine extreme heat wave datasets in australia for extreme heat wave value analysis in addition we first use non stationary generalized extreme values method to analysis the characteristics of extreme heat wave events in australia over the past 100 years to help adjust policies for climate change adaptation finally we also explore how the characteristics of heat waves are projected to change across australia under future climates 2 data and methods 2 1 earth observation datasets silo is a database of australian climate data from 1889 to the present hosted by the queensland department of environment and science des it provides daily climate variables on a 0 05 grid across australia for research modelling and climate applications the datasets are constructed from observational data obtained from the australian bureau of meteorology bom silo uses a thin plate smoothing spline to interpolate daily climate variables there is some evidence that the data quality of maximum and minimum temperatures corresponds strongly to station density with the largest errors tending to occur where the network of observed stations is sparse jones et al 2009 currently silo data are uploaded into the gee data catalog and maintained by earth observation data science earth observation data science in addition to using high resolution interpolated climate data there have been many studies using reanalysed temperature data for heat wave studies such as the latest fifth generation ecmwf european centre for medium range weather forecasts reanalysed climate data era5 and cpc global daily temperature dataset dating back to 1979 physical sciences laboratory era5 combines physical modeling and data assimilation into a complete hourly based and consistent dataset for example minimum and maximum daily air temperature at 2 m from ear5 daily are calculated based on the hourly 2 m air temperature data the era5 daily used in this study were obtained within the gee data catalog copernicus climate change service cpc global daily temperature dataset includes both daily tmax and tmin on a 0 5 0 5 grid from 1979 to the present this product is constructed by a combination of two weather station datasets around the world namely climate anomaly monitoring system cams and global historical climatology network version 2 gchn these two datasets together have about 10978 stations around the global the temperatures from which are gridded using inverse distance weighting idw interpolation algorithm in addition the temperature lapse rate estimated from observation based global reanalysis temperatures are used to make topographical adjustments note that observations from cams and gchn have less coverage over central australia and good coverage over usa europe and china the lack of accuracy from the sparse density of observation stations would impact the identification of heat wave events in this study cpc dataset netcdf4 files have been transformed into geotiffs format using r scripts and uploaded into the gee catalogue for further analysis for projection periods 2006 2100 coupled model intercomparison project phase 5 cmip5 that have daily maximum and minimum temperature from the historical experiment and two representative concentration pathway rcp experiments rcp4 5 and rcp8 5 are analyzed in this study within the gee data catalog the nasa nex dataset contains daily downscaled projections of 21 gcms under the cmip5 across two greenhouse gas emissions scenarios thrasher et al 2012 cmip5 reference periods 1975 2005 and projection periods 2006 2100 which contain daily maximum and minimum temperature are used to construct multi model mean composites for summer heat wave under two rcp emission scenarios 2 2 heat wave indices the core algorithms behind the toolbox are based on a general heat wave framework which employs three separate heat wave identification methods daily minimum and maximum temperature and the excess heat factor and use the fixed and dynamic thresholds as the baseline to determine a heat wave event which has at least three days in a row where the threshold is exceeded from a climatological perspective heat wave indices with absolute thresholds such as etccdi may only be suitable when studying heat waves in a small region where a single climate regime exists however for large regional or continental studies like australia where a broad range of climates exist three separate heat wave identification methods used in this study can be readily calculated from climatological data is more applicable for representing heat wave occurrence across multiple climates of which ehf is not only more sensitive than other heat wave indices in measuring heat waves but is also the official definition used australia wide alexander and perkins 2013 nairn and fawcett 2015 for each grid point three heat wave indices were calculated for the australian warm season from november to march these indices include 1 tx90pct the 90th percentile of tmax in calendar day based on a centered 15 day window i e 7 days after and before a calendar day the thresholds are calculated for each time period and grid point separately 2 tn90pct the 90th percentile of tmin in calendar day same time period and unit as tmax 3 excess heat factor ehf ehf is a product of two metrics based on tmean ehisig and ehiaccl the first index is denoted as significance ehisig and determines how extreme the temperature conditions are by comparing the previous 3 day mean with climatology the 95th percentile of the daily mean temperature calculated over the period of reference equation 1 the second index is a measure of acclimatization ehiaccl and the difference of the 3 day mean to the previous 30 day mean equation 2 with this second index heat stress is only likely to occur in summer from fig 1 the threshold 0 means the unusual 3 day mean temperature is above the 95th percentile of the average temperature over a fixed climatological period ehf can also be defined as ehf ehiaccl ehisig which means ehiaccl acts as an amplification term on ehisig thus ehf can be negative 1 ehi sig τ i τ i 1 τ i 2 3 τ 95 2 ehi accl τ i τ i 1 τ i 2 3 τ i 3 τ i 32 3 eq 3 ehf ehisig max 1 ehiaccl for heat wave identification method based on daily mean temperature heat wave represented as excess heat factor ehf is a product of two metrics ehisig and ehiaccl so the unit of heat wave is given in c2 however for heat wave identification method based on daily minimum and maximum temperature heat wave is defined as a spell of at least three consecutive days with daily minimum and maximum temperature exceeding the local 90th percentile of a centered 15 day of window therefore the unit of heat wave is given in c further to these three indices we used a multi aspect framework to represent heat wave characteristics including 1 heat wave number hwn the total number of discrete heat wave events 2 heat wave duration hwd the length of the longest heat wave event 3 heat wave frequency hwf the sum of days satisfying positive heat wave values 4 heat wave amplitude hwa the peak magnitudes the highest value of the heat wave in a season 5 heat wave magnitude hwm the mean magnitudes average magnitude across all heat waves among them hwm and hwa are measures of heat wave intensity while hwd hwf and hwn are measures of heat wave longevity 2 3 non stationary generalized extreme value analysis extreme value theory has a rigorous framework for analysis of climate extremes and their return levels coles et al 2001 generalized extreme value gev distribution is a combination of three limiting distributions gumbel fréchet or weibull comes from the limit theorems for block maxima minima or annual maxima minima katz 2010 a variety of studies apply the gev to analyze climatic extremes this technique is often referred to as the block maxima approach another form of the evt is known as the peak over threshold pot approach in which extreme values above a high threshold are analyzed using a generalized pareto distribution both block maxima approach and pot are widely applied in studying climatic extreme events the cumulative distribution function of the gev can be expressed as 4 ψ x 1 ξ x μ σ 1 ξ 1 ξ x μ σ 0 the gev distribution has three distribution parameters θ μ σ ξ 1 the location parameter μ determines the center of the distribution 2 the scale parameter σ specifies deviations around μ and 3 the shape parameter ξ governs the tail behavior of the gev distribution for ξ 0 ξ 0 and ξ 0 leads to frechet distributions gumbel distribution and weibull distribution respectively the extreme value theory for stationary random sequences has been extensively studied in this study a stationarity process assumes no change to extreme s properties while a non stationary process is time dependent and the properties of the distribution would change in the future the location parameter is assumed to be a linear function of time to account for non stationarity while keeping the other two parameters constant 5 μ t μ 1 t μ 0 where t is the time in years and β μ 1 μ 0 σ ξ are the parameters in this study a practical package named non stationary extreme value analysis neva matlab package was introduced for assessing extremes in a changing climate neva offers a framework for performing non stationary analysis of extremes and provides non stationary effective return levels with t year return period and risks of climatic extremes using bayesian inference and also includes simulated ensembles with upper bound and lower bound cheng et al 2014 this study estimated extremes heat wave metrics based on non stationary maximum likelihood estimators here from the long term 1920 2019 time series of heat wave magnitudes non stationary gev was fitted together with the standard error using r package introduction to statistical modeling of extreme values ismev we kept the scale and shape parameters constant while the location parameters were calculated from the regression parameters μ 1 μ 0 of equation 5 at the median of the corresponding time period for example the median of the corresponding time was 1970 over the period 1920 2019 for the sub time periods 1980 2019 the estimation for the non stationary gev distribution is similar 2 4 online heat wave measurement under a framework the heat wave tracker is to facilitate the exploitation of the up to date climate data described in table 1 by providing users a multi characteristic and multi source heat wave measurement toolkit the entire process of heat wave measurement at a continental scale is shown in fig 2 the required inputs for our online system include the historical climate data and their future projection with long time series of climate data two separate methods were used to calculate fixed and dynamic thresholds the fixed thresholds are calculated by the 95th percentile of a fix reference period the dynamic thresholds are based on the 90th percentile of a temporal moving window three separate heat wave indices were then used to determine the heat wave characteristics the core algorithm contains five iterations three band math operations and two spatial operations to retrieve five heat wave characteristics at each grid the first iteration is to do an accumulation of the number of positive values of heat wave indices the second and third iteration are combined to detect heat wave events defined as a spell of at least three consecutive days with values of heat wave indices exceeding the threshold the fourth iteration is used to find the end point of each heat wave events the fifth iteration is used to accumulate the positive values of heat wave indices based on those extreme value analyses and heat wave characteristics database we created an online heat wave tracker app for public users 3 results 3 1 heat wave tracker heat wave tracker is a user friendly web tool we developed in google earth engine gee the temperature datasets and heat wave definition outlined above are integrated into this online software tool to study heat waves in australia the first step is to pre define the temperature above a certain threshold and pre process the corresponding five month long heat wave records more precisely thresholds from the reference period of silo data 1960 1990 and the reference of era5 data 1979 1999 were calculated beforehand then the multi source heat wave record datasets e g heat wave records between 1990 and 2019 are from silo 2000 2019 are from era5 2000 2019 are from cpc 2030 2099 are from cimp5 using multi method are generated and stored in gee cloud data catalog for further visualization analysis to decrease processing times subsequent steps are performed in the graphical user interface gui the users can define the point of interest and select the year data type heat wave identification method and run the program then the tool will plot several figures displaying the time series of heat wave records and five heat wave metrics maps hwn hwd hwf hwm hwa the information can also be exported e g csv files for further analysis in such a case analysis ready heat wave records prove to be a practical and economical way for real time and human interactive visualization heat wave tracker is freely available from the authors for educational and academic purposes at https github com geogismx heatwavetracker the online tool is publicly available at https tensorflow users earthengine app view heat wave tracker while we have focused on the heat waves of australia users can also define their own research area and produce their heat wave outcomes for example users can even use the tool to evaluate the global heat wave with era5 datasets 3 2 how do the datasets differ in representing heat waves despite the use of the same heat wave definition ehf different temperature datasets may provide different heat wave metric maps it relates to the issues of spatial resolution instrumentation and data quality an example of the spatial variation from different climate datasets for heat wave metrics identification is given in fig 3 which shows the heat waves across australia in 2018 2019 over the period of november december january february march from silo gridded datasets era5 reanalysis datasets and cpc australia daily temperature datasets generally climate datasets with a high spatial resolution are much smoother than those with lower spatial resolution seen in fig 3 each heat wave metric between the three datasets shows similar data range on the color scale however the contiguous spatial distribution clearly differs between the three datasets specifically the extreme hwa for each dataset all occur over southern australia while northern australia does not experience extreme heat waves hwa can increase up to 80 c2 in the northwest of nsw in era5 larger hwa values are more confined to lower elevations of southern australia whilst hwa in silo and cpc also appear in the central areas similar to hwa the spatial pattern of hwm is mainly centered around the south coast and northwest of nsw however the anomalous red spots of hwm in cpc may be caused by the coarse resolution it is interesting to note that the hwf and the hwn are similar but do not always overlap from these three datasets we can see that the hwf and hwn are located in north western and southeast australia we also find that hwn from cpc can reach up to 12 times per year and is about two times larger than that from silo and era5 implying that caution should exert when using cpc the hwf has some influences on hwd which means the extent of hwd almost falls in the regions of hwf since local scale differences can not be detected by simple visualization or in cell by cell comparison we used a map comparison method named the structural similarity index ssi to identify local differences in terms of mean variance and covariance between two maps jones et al 2016 wiederholt et al 2019 islam et al 2020 based on the global average value of the ssi metric we try to provide a quantitative analysis of which climate data set is more reliable with respect to five aspects hwa hwd hwf hwm hwn from table 2 we can see that the similarities between three gridded datasets in terms of five aspects are quite different there is strong similarity between era5 and silo 0 78 in hwa the ssi between cpc and era5 in hwa is similar 0 68 but weaker for silo 0 67 the strong level of ssi between era5 and silo 0 77 is also found in hwd while era5 and cpc has a similarity of 0 67 the weakest similarity of 0 66 is from silo and cpc the occurrence based aspects like hwf and hwd lead to reduced similarity the weaker similarity in hwn exists between three climate datasets but the ssi between cpc and era5 is better 0 56 than cpc and silo 0 55 overall it suggests that era5 is the most reliable climate dataset 3 3 how do the methods differ in identifying and characterising heat waves five heat wave metrics for each method here are defined by era5 seen in fig 4 hwa measured by ehf c2 tends to be higher than hwa tmax c and hwa tmin c due to the different units regions that display the higher values in hwa tmax and hwa tmin are very similar mostly located in the southeast and central australia while the ehf based hwa not only shows higher values in the southeast but also along the coastal regions of south australia and victoria the extreme hwa by ehf all exists in the southward of 20 s in contrast hwa is not as large as expected in the northern tropical area as hwm and hwa are related to heat wave intensity their spatial patterns are largely similar for those heat wave aspects hwd hwf related to longevity in different ways hwd and hwf defined by tmax and tmin are similar in spatial structure which are centered in northwestern australia and in eastern australia however the lengths of hwd and hwf from tmax and tmin are about two times higher than hwd and hwf from ehf compared to northwestern australia hwf ehf is shorter at 60 days conversely to hwd and hwf hwn produces different results in northwestern and eastern australia where there are larger hwn variations from the ehf method fig 5 shows that the ehf based method identifies four distinct heat wave events while tx90 based method detects nine heat wave events and tn90 based method finds three heat wave events the ehf method can combine the characteristics of both tx90 and tn90 3 4 how does the heat wave risk change in recent climates to explore the heat wave risk in recent climates the average values of hwa the highest value of the heat wave in a season over australia for the past 100 years were used non stationary return levels based on hwa versus the time covariate across the whole continent were generated by neva as shown in fig 6 a the effective return levels vary over time indicating return level should be chosen for years to have the same probability of occurrence for example the effective return level hwa corresponding to a 25 year event during 1920 1944 is 37 c2 the effective return level for a once in 50 year event 1920 1969 should be 45 c2 and the effective return level for a 100 year period 1920 2019 is 60 c2 in fig 6a we also observe that there is a strong upward trend p 0 005 for hwa over australia during the 1920 2019 period this suggests that heat wave amplitude was increasing under climate change fig 6b compares the probability density functions pdf of the hwa under two different time intervals 1920 2020 1980 2020 we find that there is an obvious warming shift of pdfs of the hwa during 1920 2020 compared with that during 1980 2020 this is consistent with the observed increasing trend in fig 6a in addition the warm tail of the pdfs for the period of 1980 2019 is greater than that of 1920 2019 implying that extreme heat events have much higher probability with effects of climate change we also find that the 2019 heat wave event is not rare over 10 year effective return levels fig 6a with the pdf observed in 2019 for the 1980 2020 higher than that for the 1920 2020 as shown in fig 6b from the long term 1920 2019 and the short term 1980 2019 time series of hwa gev fits were estimated together with the corresponding 1 96 standard error for a 95 confidence interval in fig 6c it denotes that the 2019 heat wave hwa is 45 6 c2 has a lower probability of occurrence over 1920 2020 climate and a higher probability over 1980 2020 climate over 10 year return periods for gev fit 1980 2020 fig 6c 3 5 how does the heat wave risk change under future climate conditions fig 7 shows the near future 2030 2060 and far future 2069 2099 projected hwa using cmip5 gcm datasets under two emissions scenarios compared with the 1976 2006 climate overall hwa is projected to increase significantly during the two future periods and a larger fraction of southern australia is projected to experience more extreme heat wave events we also see that the average hwa derived from cmip5 multi gcm ensemble mean ranges from 0 to 10 c2 and hwa decreases equatorward to 3 c2 in the northern australia under the two future periods of rcp4 5 the spatial extent of hwa mainly aggregates in the southern australia compared with hwa in the near future hwa in the far future expands from southeast to western and central australia under the two future periods of rcp8 5 hwa not only increases its intensity but also expands from south to north as expected the change in hwa from rcp8 5 is more extreme than that from rcp4 5 indicating that greenhouse warming strongly amplifies the amplitude of heat wave events fig 8 shows the characteristic of hwd changes in the two future periods with different emission scenarios the patterns of change for hwd are opposite to the change for hwa northern australia shows significant increases and southern australia experience a moderate increase in the far future period of rcp4 5 we also note that hwd shows a stronger increase in western coastal areas and in northern tropical australia with hwd across northern tropical area reaching 120 days again in the far future period of rcp8 5 hwd represents an amplification of the rcp4 5 pattern that is the duration of heat waves is much stronger than for rcp4 5 this indicates that the duration of southern australia heat waves is not as sensitive to warming as those in northern australia largely due to the southern regions being associated with anticyclones and cold fronts 4 discussion 4 1 model comparison to evaluate the performance of our model we made a comparison with ghwr toolbox of mojtaba sadegh 2018 for the comparison the cpc datasets during a period of 1979 2019 were used to model heat wave metrics both software toolboxes apply ehf based method to measure the heat wave metrics note that the definition of ehf is composed of the previous three day mean and the previous thirty day mean the threshold of the 95th percentile of tmean was calculated based on the 20 years period 1979 2009 two 2018 heat wave indices were obtained from two different software packages we can see that the spatial pattern of hwd from our model is consistent with that of ghwr seen in fig 9 however the comparison of hwm shows large difference in spatial patterns based on the hwm results of alexander and perkins 2013 the hwm of the northern australia are no more than 12 as the tropical climate imposes less diurnal and seasonal variation in temperature than that in southern australia in contrast the higher hwm values tends to occur in southern australia and experience higher average peak values argüeso et al 2015 reported higher hwm values towards the south west of nsw and lower hwm values to the north coast of nsw is consistent with the spatial pattern from our model we also note that the hwm from ghwr 3 day average has a similar spatial pattern similar to that of hwm from argüeso et al 2015 i e the highest values of hwm are found in the north west corner and the lowest values in the mountains of the south it means that the heat wave metrics from our model are consistent with the original definition of alexander and perkins 2013 4 2 heat wave threshold the cmip5 multi gcm ensemble mean projects that longer summer heat waves will occur in northern australia and hotter heat wave events will increase for southern australia in the late twenty first century with more extreme change in the higher emission scenario rcp8 5 than for the lower emission scenario rcp4 5 the results reveal that the hottest heat waves will increase in southern australia which may account for the increasing trend of severe summer bushfires occurring in southeast australia despite the different heat wave definitions and 25 member ensemble mean our model results are consistent with the results from purich et al 2014 however possibly due to the coarse resolution of the hwd from purich et al 2014 trends over tasmania an island state are opposite to the overall pattern of change while the patterns of change in tasmania are consistent with the changes in other continental states it means that our hwd results show promise in simulating fine scale projections without using downscaling techniques future extreme heat waves in our study are defined relative to a historical reference period we find a substantial increase in amplitude duration and extent in both near future and far future periods seen in figs 7 and 8 for example the duration of heat waves can even last over the entire warm season in some areas which amounts to 152 days the amplitude of heat waves significantly increases over southern australia such results are not surprising and are in line with other findings perkins kirkpatrick and gibson 2017 lyon et al 2019 however the sensitivity of heat waves to different heat wave thresholds was not explored vogel et al 2020 identified future heat waves with different heat wave thresholds fixed seasonally moving and fully moving where fixed thresholds are based on hot days relative to a historical baseline seasonal and fully moving thresholds are defined by hot days relative to future conditions they find that using fixed thresholds might overestimate future heat waves while using seasonal and fully moving threshold results in little or no changes in future heat wave metrics to better estimate heat wave characteristics and risk in a warming world it would be useful to adopt varying heat wave thresholds for future spatiotemporal heat wave studies 4 3 future needs for this study we use the 5 km silo gridded climate data reanalysed data 25 km 50 km to estimate the heat wave at a large scale however those climate datasets do not take into account the smaller scale temperature variations that is the weather stations used to produce the gridded climate data were too sparse to record fine scale variations in extreme temperatures for example we find that the gridded climate data have relatively coarse spatial resolutions and cannot meet the need of monitoring heat wave variances in complex settings and the heat wave maps are generally distributed evenly over urban heat islands furthermore the location of most weather stations is away from building areas and the associated heat islands where extreme heat waves pose the greatest risk to human health this issue can be at least partially resolved by using satellite thermal infrared sensing method to monitor and analyze heat waves at a local scale the proliferation of land surface temperature lst products offers an opportunity to study the characteristics of extreme heat waves at the community scale and give insight into urban heat wave planning and the prevention of heat related mortality for example modis lsts have higher spatial resolution 1 km and temporal resolution four passes per day modis lsts provide the maximum and minimum products for the 20 years back to march 5th 2000 which could be a valuable resource to capture extreme heat waves and for regional and local scale heat wave research however it is difficult to map lst accurately as the temperature are very variable and could be affected by climate factors like clouds and wind venter et al 2020 compared to daily satellite data from modis four passes a day himwari 8 data provides real time data at 10 min intervals but the spatial resolution is 2 km which is also suitable to conduct regional studies the high temporal resolution of himawari 8 can show the diurnal characteristics of extreme heat waves on urban heat waves despite the limitations of the relatively short time period from 2015 to present of the historical data archive of himawari 8 a combination of modis lst and himawari 8 lst offers a better solution for obtaining a higher spatial resolution while maintaining a higher temporal resolution which is extremely useful for characterizing the heat wave characteristics and investigating the relationship between heat waves land cover and population the health or agriculture impacts of heat waves are not only related with temperature measurements but also affected by some additional factors for example health effects are associated with factors including perceived temperature solar radiation relative humidity wind while for agriculture the parallel occurrence of droughts is highly relevant due to the problems of short time spans inconsistency and biases these additional measurements have limitations on precisely capturing spatio temporal pattern of heat wave impacts the reason why we choose temperature based heat wave definition is because it can be calculated from readily available climatological data and provides information on various aspects of heat waves in other words the choice of temperature rather than other measures is based on their feasibility across varying climates on long term scales further the availability of long term temperature datasets at finer spatial scales can greatly improve our understanding of heat wave we concur that the heat wave definitions directly rely on the critical temperature thresholds however there is no universal temperature threshold for health impacts because of regional variability of health status socio economic factors and demographic factors alexander and perkins 2013 this impact also exists in agriculture due to varying regional patterns of plant species and physiology therefore a given threshold suitable in a small region may not be applicable to a continental study like ours fischer and schär 2010 explored health related heat wave indices in three health factors heat wave duration minimum temperature and relative humidity our study also quantified the heat wave duration minimum temperature based heat wave indices a combined calculation of temperature and humidity will be considered in our future study 5 conclusion we have developed a heat wave toolbox that has the ability to estimate past current and future changes in heat waves at a continental scale it uses a well known heat wave framework constructed by alexander and perkins 2013 and considers intensity frequency magnitude duration and areal extent to explore the spatio temporal evolution of heat wave severity and coverage this study is the first attempt to estimate heat wave events across australia using high spatio temporal climate datasets with these heat wave aspects from multi source data and different methods we were able to investigate the effects of scales data quality and definition we find that era5 datasets are the best in characterizing the heat wave events in exploring the role of different methods on the identification of heat waves we find that heat wave characteristics based on the excess heat factor index integrate the features of both tx90 based and tn90 based methods with the past 100 years of heat wave datasets the hwa average mean values were calculated and used to estimate non stationary return levels and return periods we find that extreme heat wave events have much higher probability due to the effects of climate change the heat wave event in 2019 may be more frequent in the coming decades for the climate by the end of century using heat wave metrics derived from a multi model ensemble mean we predict hwa to increase significantly during the two future periods and a larger fraction of southern australia is projected to experience more extreme heat wave events furthermore the patterns of change for hwd are opposite to those for hwa northern australia shows significant increases and southern australia experience a moderate increase the methodology and the cloud computing based toolbox hwt is useful for dynamic visualization extraction and processing of complex heat wave events and applicable anywhere in the world declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the first author acknowledges the chinese scholarship council and university of technology sydney for scholarships and nsw department of planning industry and environment for providing office facilities to conduct this work the first author also acknowledges dr jonathan gray for article revision help 
25694,under ongoing global warming due to climate change heat waves in australia are expected to become more frequent and severe extreme heat waves have devastating impacts on both terrestrial and marine ecosystems a multi characteristic heat wave framework is used to estimate historical and future projected heat waves across australia a google earth engine based toolkit named heat wave tracker hwt is developed which can be used for dynamic visualization extraction and processing of complex heat wave events the toolkit exploits the public long term high resolution climate datasets to developed nine heat wave datasets across australia for extreme heat wave value analysis to examine climate change on heat waves and how they vary in time and space we also explore the probability and return periods of extreme heat waves over a period of 100 years the datasets toolkit and findings we developed contribute to global studies on heat waves under accelerated global warming keywords extreme heat wave google earth engine climate datasets risk analysis gcm australia 1 introduction under ongoing global warming due to climate change heat waves are expected to become more frequent and severe in the future ipcc 2019 extreme heat waves during the last two decades have been recorded across many regions in the world such as those in europe in 2003 schär et al 2004 moscow region in russia in 2010 rahmstorf and coumou 2011 and australia in 2013 lewis and karoly 2013 heat waves in australia incur significant hazard for both humans and ecosystems and cause more deaths than other natural hazards including floods storms and bushfires in terms of heat wave impacts on ecosystems extreme heat waves increase the probability of bushfire risk affect crops and food security for terrestrial systems luo 2011 and cause catastrophic damage to marine ecosystems hobday et al 2016 moreover extreme temperatures contribute to widespread unfavorable health outcomes and even the death of vulnerable people although heat wave is commonly known as a period of exceptional hot weather event there is currently no universal informative measurement in climate science community alexander and perkins 2013 to overcome these issues a set of climate indices developed by the expert team on climate change detection and indices etccdi have been widely applied to observational and modeled climate data to understand previous and future changes in extreme heat wave events zhang and yang 2004 alexander et al 2006 the work by etccdi is extensively recognized as pioneering however the indices only measure one feature of extreme events such as frequency intensity or duration perkins 2015 a comprehensive and consistent analysis of heat waves is required which should consider multi characteristics of heat wave events namely i frequency ii intensity iii duration and iv spatial extent raei et al 2018 the multi characteristic heat wave definition method used in this study is from a well known heat wave framework constructed by alexander and perkins 2013 and includes a minimum temperature approach a maximum temperature approach and an excess heat factor ehf approach this framework has proven to be successful in measuring historical and future projected heat waves however useful public software or tools that identify all the required characteristics of heat waves frequency intensity duration spatial extent are still rare most studies with their own tools cannot fully reflect the four characteristics of complex heat wave events feron et al 2019 lyon et al 2019 li 2020 by summarizing the classical heat wave definition an r package called heatwaver was developed which provides a comprehensive analysis to detect and visualize ocean heat waves schlegel and smit 2017 however it is inefficient when applied to large gridded data products global heat wave and warm spell data record and analysis toolbox ghwr which is a matlab toolbox allows processing and extracting heat wave records for any location efficiently it not only contains multiple definitions but also detects the required multi characteristics raei et al 2018 however desktop applications like ghwr still have a bottleneck when encountering the challenges related to accessibility of long term gridded climate data data storage and computational requirements in the current era of big spatial and earth observation eo data users need to deal with a vast amount of different spectral temporal and spatial resolutions data gomes et al 2020 to meet these demands there is need for novel technologies based on cloud computing to properly extract heat wave information in the server side without having to download vast amounts of climate data and provide dynamic visualization extraction and processing of complex heat wave events google earth engine gee a powerful cloud computing geospatial analysis platform has given researchers the opportunity to use big data for petabyte scale environmental data analysis gorelick et al 2017 with the gridded global reanalysed datasets e g hadley centre global historical climatology network hadghcnd climate prediction centre cpc and regional reanalysis datasets e g the coordinated regional downscaling experiment cordex australian water availability project awap being freely available many studies have investigated heat waves at various scales perkins et al 2012 ma et al 2020 christidis et al 2014 the atmospheric reanalysis datasets are quite useful for gaining understanding in how the heat wave will change reanalysis datasets are created by data assimilation and numeric models to represent a synthesized estimate of the atmospheric state and provide global scale dataset over several decades or longer one benefit of using reanalysis data is that it extends the study to locations without observation records another important advantage is that the spatially contiguous heat wave regions derived from the reanalysis data have crucial implications for heat related impacts such as exposure of the community to extreme heat wave events and high energy demands lyon et al 2019 li 2020 however some heat wave assessments are mostly based on climate datasets with relatively coarse resolution which would affect the representation of heat waves resulting in biased conclusions furthermore key processes that occur on regional scales may not be adequately simulated benefiting from those newly reanalysed climate datasets and high spatio temporal gridded regional climate datasets our analysis will explore how these climate datasets differ in representing heat waves and how the methods differ in identifying and characterizing heat waves increasingly researchers are becoming less interested in data in the normal range and more concerned with the abnormal and extreme events that are recurrent and unpredictable extreme value theory evt is the statistical framework that estimates the probability of an extreme event occurring in the future coles et al 2001 because of its importance many public packages and toolboxes over the last decade have been developed to implement various methods from evt ribatet et al 2011 cheng et al 2014 gilleland and katz 2016 heffernan et al 2016 it is clear from much of the literature using gridded observed data and projected climate model data at regional and global scales that the probability of extreme heat waves will change over time alexander and perkins 2013 purich et al 2014 recently several studies of the risks of heat wave by means of the evt have been published ma et al 2020 tanarhte et al 2015 shen et al 2016 however the precise probabilities of intensity frequency and duration of extreme heat wave at a continent scale like australia over the time are still unknown meanwhile the potential impact of climate change on heat wave varies in space and time in this context we explore the risk of heat waves in australia by performing non stationary analysis of extreme heat waves for the past 100 years in this study we will develop a multi method global heat wave data record and analysis toolbox namely heat wave tracker to process and extract heat wave records from multi source climate datasets with our toolbox s computational power in handling long term high resolution climate datasets we developed nine extreme heat wave datasets in australia for extreme heat wave value analysis in addition we first use non stationary generalized extreme values method to analysis the characteristics of extreme heat wave events in australia over the past 100 years to help adjust policies for climate change adaptation finally we also explore how the characteristics of heat waves are projected to change across australia under future climates 2 data and methods 2 1 earth observation datasets silo is a database of australian climate data from 1889 to the present hosted by the queensland department of environment and science des it provides daily climate variables on a 0 05 grid across australia for research modelling and climate applications the datasets are constructed from observational data obtained from the australian bureau of meteorology bom silo uses a thin plate smoothing spline to interpolate daily climate variables there is some evidence that the data quality of maximum and minimum temperatures corresponds strongly to station density with the largest errors tending to occur where the network of observed stations is sparse jones et al 2009 currently silo data are uploaded into the gee data catalog and maintained by earth observation data science earth observation data science in addition to using high resolution interpolated climate data there have been many studies using reanalysed temperature data for heat wave studies such as the latest fifth generation ecmwf european centre for medium range weather forecasts reanalysed climate data era5 and cpc global daily temperature dataset dating back to 1979 physical sciences laboratory era5 combines physical modeling and data assimilation into a complete hourly based and consistent dataset for example minimum and maximum daily air temperature at 2 m from ear5 daily are calculated based on the hourly 2 m air temperature data the era5 daily used in this study were obtained within the gee data catalog copernicus climate change service cpc global daily temperature dataset includes both daily tmax and tmin on a 0 5 0 5 grid from 1979 to the present this product is constructed by a combination of two weather station datasets around the world namely climate anomaly monitoring system cams and global historical climatology network version 2 gchn these two datasets together have about 10978 stations around the global the temperatures from which are gridded using inverse distance weighting idw interpolation algorithm in addition the temperature lapse rate estimated from observation based global reanalysis temperatures are used to make topographical adjustments note that observations from cams and gchn have less coverage over central australia and good coverage over usa europe and china the lack of accuracy from the sparse density of observation stations would impact the identification of heat wave events in this study cpc dataset netcdf4 files have been transformed into geotiffs format using r scripts and uploaded into the gee catalogue for further analysis for projection periods 2006 2100 coupled model intercomparison project phase 5 cmip5 that have daily maximum and minimum temperature from the historical experiment and two representative concentration pathway rcp experiments rcp4 5 and rcp8 5 are analyzed in this study within the gee data catalog the nasa nex dataset contains daily downscaled projections of 21 gcms under the cmip5 across two greenhouse gas emissions scenarios thrasher et al 2012 cmip5 reference periods 1975 2005 and projection periods 2006 2100 which contain daily maximum and minimum temperature are used to construct multi model mean composites for summer heat wave under two rcp emission scenarios 2 2 heat wave indices the core algorithms behind the toolbox are based on a general heat wave framework which employs three separate heat wave identification methods daily minimum and maximum temperature and the excess heat factor and use the fixed and dynamic thresholds as the baseline to determine a heat wave event which has at least three days in a row where the threshold is exceeded from a climatological perspective heat wave indices with absolute thresholds such as etccdi may only be suitable when studying heat waves in a small region where a single climate regime exists however for large regional or continental studies like australia where a broad range of climates exist three separate heat wave identification methods used in this study can be readily calculated from climatological data is more applicable for representing heat wave occurrence across multiple climates of which ehf is not only more sensitive than other heat wave indices in measuring heat waves but is also the official definition used australia wide alexander and perkins 2013 nairn and fawcett 2015 for each grid point three heat wave indices were calculated for the australian warm season from november to march these indices include 1 tx90pct the 90th percentile of tmax in calendar day based on a centered 15 day window i e 7 days after and before a calendar day the thresholds are calculated for each time period and grid point separately 2 tn90pct the 90th percentile of tmin in calendar day same time period and unit as tmax 3 excess heat factor ehf ehf is a product of two metrics based on tmean ehisig and ehiaccl the first index is denoted as significance ehisig and determines how extreme the temperature conditions are by comparing the previous 3 day mean with climatology the 95th percentile of the daily mean temperature calculated over the period of reference equation 1 the second index is a measure of acclimatization ehiaccl and the difference of the 3 day mean to the previous 30 day mean equation 2 with this second index heat stress is only likely to occur in summer from fig 1 the threshold 0 means the unusual 3 day mean temperature is above the 95th percentile of the average temperature over a fixed climatological period ehf can also be defined as ehf ehiaccl ehisig which means ehiaccl acts as an amplification term on ehisig thus ehf can be negative 1 ehi sig τ i τ i 1 τ i 2 3 τ 95 2 ehi accl τ i τ i 1 τ i 2 3 τ i 3 τ i 32 3 eq 3 ehf ehisig max 1 ehiaccl for heat wave identification method based on daily mean temperature heat wave represented as excess heat factor ehf is a product of two metrics ehisig and ehiaccl so the unit of heat wave is given in c2 however for heat wave identification method based on daily minimum and maximum temperature heat wave is defined as a spell of at least three consecutive days with daily minimum and maximum temperature exceeding the local 90th percentile of a centered 15 day of window therefore the unit of heat wave is given in c further to these three indices we used a multi aspect framework to represent heat wave characteristics including 1 heat wave number hwn the total number of discrete heat wave events 2 heat wave duration hwd the length of the longest heat wave event 3 heat wave frequency hwf the sum of days satisfying positive heat wave values 4 heat wave amplitude hwa the peak magnitudes the highest value of the heat wave in a season 5 heat wave magnitude hwm the mean magnitudes average magnitude across all heat waves among them hwm and hwa are measures of heat wave intensity while hwd hwf and hwn are measures of heat wave longevity 2 3 non stationary generalized extreme value analysis extreme value theory has a rigorous framework for analysis of climate extremes and their return levels coles et al 2001 generalized extreme value gev distribution is a combination of three limiting distributions gumbel fréchet or weibull comes from the limit theorems for block maxima minima or annual maxima minima katz 2010 a variety of studies apply the gev to analyze climatic extremes this technique is often referred to as the block maxima approach another form of the evt is known as the peak over threshold pot approach in which extreme values above a high threshold are analyzed using a generalized pareto distribution both block maxima approach and pot are widely applied in studying climatic extreme events the cumulative distribution function of the gev can be expressed as 4 ψ x 1 ξ x μ σ 1 ξ 1 ξ x μ σ 0 the gev distribution has three distribution parameters θ μ σ ξ 1 the location parameter μ determines the center of the distribution 2 the scale parameter σ specifies deviations around μ and 3 the shape parameter ξ governs the tail behavior of the gev distribution for ξ 0 ξ 0 and ξ 0 leads to frechet distributions gumbel distribution and weibull distribution respectively the extreme value theory for stationary random sequences has been extensively studied in this study a stationarity process assumes no change to extreme s properties while a non stationary process is time dependent and the properties of the distribution would change in the future the location parameter is assumed to be a linear function of time to account for non stationarity while keeping the other two parameters constant 5 μ t μ 1 t μ 0 where t is the time in years and β μ 1 μ 0 σ ξ are the parameters in this study a practical package named non stationary extreme value analysis neva matlab package was introduced for assessing extremes in a changing climate neva offers a framework for performing non stationary analysis of extremes and provides non stationary effective return levels with t year return period and risks of climatic extremes using bayesian inference and also includes simulated ensembles with upper bound and lower bound cheng et al 2014 this study estimated extremes heat wave metrics based on non stationary maximum likelihood estimators here from the long term 1920 2019 time series of heat wave magnitudes non stationary gev was fitted together with the standard error using r package introduction to statistical modeling of extreme values ismev we kept the scale and shape parameters constant while the location parameters were calculated from the regression parameters μ 1 μ 0 of equation 5 at the median of the corresponding time period for example the median of the corresponding time was 1970 over the period 1920 2019 for the sub time periods 1980 2019 the estimation for the non stationary gev distribution is similar 2 4 online heat wave measurement under a framework the heat wave tracker is to facilitate the exploitation of the up to date climate data described in table 1 by providing users a multi characteristic and multi source heat wave measurement toolkit the entire process of heat wave measurement at a continental scale is shown in fig 2 the required inputs for our online system include the historical climate data and their future projection with long time series of climate data two separate methods were used to calculate fixed and dynamic thresholds the fixed thresholds are calculated by the 95th percentile of a fix reference period the dynamic thresholds are based on the 90th percentile of a temporal moving window three separate heat wave indices were then used to determine the heat wave characteristics the core algorithm contains five iterations three band math operations and two spatial operations to retrieve five heat wave characteristics at each grid the first iteration is to do an accumulation of the number of positive values of heat wave indices the second and third iteration are combined to detect heat wave events defined as a spell of at least three consecutive days with values of heat wave indices exceeding the threshold the fourth iteration is used to find the end point of each heat wave events the fifth iteration is used to accumulate the positive values of heat wave indices based on those extreme value analyses and heat wave characteristics database we created an online heat wave tracker app for public users 3 results 3 1 heat wave tracker heat wave tracker is a user friendly web tool we developed in google earth engine gee the temperature datasets and heat wave definition outlined above are integrated into this online software tool to study heat waves in australia the first step is to pre define the temperature above a certain threshold and pre process the corresponding five month long heat wave records more precisely thresholds from the reference period of silo data 1960 1990 and the reference of era5 data 1979 1999 were calculated beforehand then the multi source heat wave record datasets e g heat wave records between 1990 and 2019 are from silo 2000 2019 are from era5 2000 2019 are from cpc 2030 2099 are from cimp5 using multi method are generated and stored in gee cloud data catalog for further visualization analysis to decrease processing times subsequent steps are performed in the graphical user interface gui the users can define the point of interest and select the year data type heat wave identification method and run the program then the tool will plot several figures displaying the time series of heat wave records and five heat wave metrics maps hwn hwd hwf hwm hwa the information can also be exported e g csv files for further analysis in such a case analysis ready heat wave records prove to be a practical and economical way for real time and human interactive visualization heat wave tracker is freely available from the authors for educational and academic purposes at https github com geogismx heatwavetracker the online tool is publicly available at https tensorflow users earthengine app view heat wave tracker while we have focused on the heat waves of australia users can also define their own research area and produce their heat wave outcomes for example users can even use the tool to evaluate the global heat wave with era5 datasets 3 2 how do the datasets differ in representing heat waves despite the use of the same heat wave definition ehf different temperature datasets may provide different heat wave metric maps it relates to the issues of spatial resolution instrumentation and data quality an example of the spatial variation from different climate datasets for heat wave metrics identification is given in fig 3 which shows the heat waves across australia in 2018 2019 over the period of november december january february march from silo gridded datasets era5 reanalysis datasets and cpc australia daily temperature datasets generally climate datasets with a high spatial resolution are much smoother than those with lower spatial resolution seen in fig 3 each heat wave metric between the three datasets shows similar data range on the color scale however the contiguous spatial distribution clearly differs between the three datasets specifically the extreme hwa for each dataset all occur over southern australia while northern australia does not experience extreme heat waves hwa can increase up to 80 c2 in the northwest of nsw in era5 larger hwa values are more confined to lower elevations of southern australia whilst hwa in silo and cpc also appear in the central areas similar to hwa the spatial pattern of hwm is mainly centered around the south coast and northwest of nsw however the anomalous red spots of hwm in cpc may be caused by the coarse resolution it is interesting to note that the hwf and the hwn are similar but do not always overlap from these three datasets we can see that the hwf and hwn are located in north western and southeast australia we also find that hwn from cpc can reach up to 12 times per year and is about two times larger than that from silo and era5 implying that caution should exert when using cpc the hwf has some influences on hwd which means the extent of hwd almost falls in the regions of hwf since local scale differences can not be detected by simple visualization or in cell by cell comparison we used a map comparison method named the structural similarity index ssi to identify local differences in terms of mean variance and covariance between two maps jones et al 2016 wiederholt et al 2019 islam et al 2020 based on the global average value of the ssi metric we try to provide a quantitative analysis of which climate data set is more reliable with respect to five aspects hwa hwd hwf hwm hwn from table 2 we can see that the similarities between three gridded datasets in terms of five aspects are quite different there is strong similarity between era5 and silo 0 78 in hwa the ssi between cpc and era5 in hwa is similar 0 68 but weaker for silo 0 67 the strong level of ssi between era5 and silo 0 77 is also found in hwd while era5 and cpc has a similarity of 0 67 the weakest similarity of 0 66 is from silo and cpc the occurrence based aspects like hwf and hwd lead to reduced similarity the weaker similarity in hwn exists between three climate datasets but the ssi between cpc and era5 is better 0 56 than cpc and silo 0 55 overall it suggests that era5 is the most reliable climate dataset 3 3 how do the methods differ in identifying and characterising heat waves five heat wave metrics for each method here are defined by era5 seen in fig 4 hwa measured by ehf c2 tends to be higher than hwa tmax c and hwa tmin c due to the different units regions that display the higher values in hwa tmax and hwa tmin are very similar mostly located in the southeast and central australia while the ehf based hwa not only shows higher values in the southeast but also along the coastal regions of south australia and victoria the extreme hwa by ehf all exists in the southward of 20 s in contrast hwa is not as large as expected in the northern tropical area as hwm and hwa are related to heat wave intensity their spatial patterns are largely similar for those heat wave aspects hwd hwf related to longevity in different ways hwd and hwf defined by tmax and tmin are similar in spatial structure which are centered in northwestern australia and in eastern australia however the lengths of hwd and hwf from tmax and tmin are about two times higher than hwd and hwf from ehf compared to northwestern australia hwf ehf is shorter at 60 days conversely to hwd and hwf hwn produces different results in northwestern and eastern australia where there are larger hwn variations from the ehf method fig 5 shows that the ehf based method identifies four distinct heat wave events while tx90 based method detects nine heat wave events and tn90 based method finds three heat wave events the ehf method can combine the characteristics of both tx90 and tn90 3 4 how does the heat wave risk change in recent climates to explore the heat wave risk in recent climates the average values of hwa the highest value of the heat wave in a season over australia for the past 100 years were used non stationary return levels based on hwa versus the time covariate across the whole continent were generated by neva as shown in fig 6 a the effective return levels vary over time indicating return level should be chosen for years to have the same probability of occurrence for example the effective return level hwa corresponding to a 25 year event during 1920 1944 is 37 c2 the effective return level for a once in 50 year event 1920 1969 should be 45 c2 and the effective return level for a 100 year period 1920 2019 is 60 c2 in fig 6a we also observe that there is a strong upward trend p 0 005 for hwa over australia during the 1920 2019 period this suggests that heat wave amplitude was increasing under climate change fig 6b compares the probability density functions pdf of the hwa under two different time intervals 1920 2020 1980 2020 we find that there is an obvious warming shift of pdfs of the hwa during 1920 2020 compared with that during 1980 2020 this is consistent with the observed increasing trend in fig 6a in addition the warm tail of the pdfs for the period of 1980 2019 is greater than that of 1920 2019 implying that extreme heat events have much higher probability with effects of climate change we also find that the 2019 heat wave event is not rare over 10 year effective return levels fig 6a with the pdf observed in 2019 for the 1980 2020 higher than that for the 1920 2020 as shown in fig 6b from the long term 1920 2019 and the short term 1980 2019 time series of hwa gev fits were estimated together with the corresponding 1 96 standard error for a 95 confidence interval in fig 6c it denotes that the 2019 heat wave hwa is 45 6 c2 has a lower probability of occurrence over 1920 2020 climate and a higher probability over 1980 2020 climate over 10 year return periods for gev fit 1980 2020 fig 6c 3 5 how does the heat wave risk change under future climate conditions fig 7 shows the near future 2030 2060 and far future 2069 2099 projected hwa using cmip5 gcm datasets under two emissions scenarios compared with the 1976 2006 climate overall hwa is projected to increase significantly during the two future periods and a larger fraction of southern australia is projected to experience more extreme heat wave events we also see that the average hwa derived from cmip5 multi gcm ensemble mean ranges from 0 to 10 c2 and hwa decreases equatorward to 3 c2 in the northern australia under the two future periods of rcp4 5 the spatial extent of hwa mainly aggregates in the southern australia compared with hwa in the near future hwa in the far future expands from southeast to western and central australia under the two future periods of rcp8 5 hwa not only increases its intensity but also expands from south to north as expected the change in hwa from rcp8 5 is more extreme than that from rcp4 5 indicating that greenhouse warming strongly amplifies the amplitude of heat wave events fig 8 shows the characteristic of hwd changes in the two future periods with different emission scenarios the patterns of change for hwd are opposite to the change for hwa northern australia shows significant increases and southern australia experience a moderate increase in the far future period of rcp4 5 we also note that hwd shows a stronger increase in western coastal areas and in northern tropical australia with hwd across northern tropical area reaching 120 days again in the far future period of rcp8 5 hwd represents an amplification of the rcp4 5 pattern that is the duration of heat waves is much stronger than for rcp4 5 this indicates that the duration of southern australia heat waves is not as sensitive to warming as those in northern australia largely due to the southern regions being associated with anticyclones and cold fronts 4 discussion 4 1 model comparison to evaluate the performance of our model we made a comparison with ghwr toolbox of mojtaba sadegh 2018 for the comparison the cpc datasets during a period of 1979 2019 were used to model heat wave metrics both software toolboxes apply ehf based method to measure the heat wave metrics note that the definition of ehf is composed of the previous three day mean and the previous thirty day mean the threshold of the 95th percentile of tmean was calculated based on the 20 years period 1979 2009 two 2018 heat wave indices were obtained from two different software packages we can see that the spatial pattern of hwd from our model is consistent with that of ghwr seen in fig 9 however the comparison of hwm shows large difference in spatial patterns based on the hwm results of alexander and perkins 2013 the hwm of the northern australia are no more than 12 as the tropical climate imposes less diurnal and seasonal variation in temperature than that in southern australia in contrast the higher hwm values tends to occur in southern australia and experience higher average peak values argüeso et al 2015 reported higher hwm values towards the south west of nsw and lower hwm values to the north coast of nsw is consistent with the spatial pattern from our model we also note that the hwm from ghwr 3 day average has a similar spatial pattern similar to that of hwm from argüeso et al 2015 i e the highest values of hwm are found in the north west corner and the lowest values in the mountains of the south it means that the heat wave metrics from our model are consistent with the original definition of alexander and perkins 2013 4 2 heat wave threshold the cmip5 multi gcm ensemble mean projects that longer summer heat waves will occur in northern australia and hotter heat wave events will increase for southern australia in the late twenty first century with more extreme change in the higher emission scenario rcp8 5 than for the lower emission scenario rcp4 5 the results reveal that the hottest heat waves will increase in southern australia which may account for the increasing trend of severe summer bushfires occurring in southeast australia despite the different heat wave definitions and 25 member ensemble mean our model results are consistent with the results from purich et al 2014 however possibly due to the coarse resolution of the hwd from purich et al 2014 trends over tasmania an island state are opposite to the overall pattern of change while the patterns of change in tasmania are consistent with the changes in other continental states it means that our hwd results show promise in simulating fine scale projections without using downscaling techniques future extreme heat waves in our study are defined relative to a historical reference period we find a substantial increase in amplitude duration and extent in both near future and far future periods seen in figs 7 and 8 for example the duration of heat waves can even last over the entire warm season in some areas which amounts to 152 days the amplitude of heat waves significantly increases over southern australia such results are not surprising and are in line with other findings perkins kirkpatrick and gibson 2017 lyon et al 2019 however the sensitivity of heat waves to different heat wave thresholds was not explored vogel et al 2020 identified future heat waves with different heat wave thresholds fixed seasonally moving and fully moving where fixed thresholds are based on hot days relative to a historical baseline seasonal and fully moving thresholds are defined by hot days relative to future conditions they find that using fixed thresholds might overestimate future heat waves while using seasonal and fully moving threshold results in little or no changes in future heat wave metrics to better estimate heat wave characteristics and risk in a warming world it would be useful to adopt varying heat wave thresholds for future spatiotemporal heat wave studies 4 3 future needs for this study we use the 5 km silo gridded climate data reanalysed data 25 km 50 km to estimate the heat wave at a large scale however those climate datasets do not take into account the smaller scale temperature variations that is the weather stations used to produce the gridded climate data were too sparse to record fine scale variations in extreme temperatures for example we find that the gridded climate data have relatively coarse spatial resolutions and cannot meet the need of monitoring heat wave variances in complex settings and the heat wave maps are generally distributed evenly over urban heat islands furthermore the location of most weather stations is away from building areas and the associated heat islands where extreme heat waves pose the greatest risk to human health this issue can be at least partially resolved by using satellite thermal infrared sensing method to monitor and analyze heat waves at a local scale the proliferation of land surface temperature lst products offers an opportunity to study the characteristics of extreme heat waves at the community scale and give insight into urban heat wave planning and the prevention of heat related mortality for example modis lsts have higher spatial resolution 1 km and temporal resolution four passes per day modis lsts provide the maximum and minimum products for the 20 years back to march 5th 2000 which could be a valuable resource to capture extreme heat waves and for regional and local scale heat wave research however it is difficult to map lst accurately as the temperature are very variable and could be affected by climate factors like clouds and wind venter et al 2020 compared to daily satellite data from modis four passes a day himwari 8 data provides real time data at 10 min intervals but the spatial resolution is 2 km which is also suitable to conduct regional studies the high temporal resolution of himawari 8 can show the diurnal characteristics of extreme heat waves on urban heat waves despite the limitations of the relatively short time period from 2015 to present of the historical data archive of himawari 8 a combination of modis lst and himawari 8 lst offers a better solution for obtaining a higher spatial resolution while maintaining a higher temporal resolution which is extremely useful for characterizing the heat wave characteristics and investigating the relationship between heat waves land cover and population the health or agriculture impacts of heat waves are not only related with temperature measurements but also affected by some additional factors for example health effects are associated with factors including perceived temperature solar radiation relative humidity wind while for agriculture the parallel occurrence of droughts is highly relevant due to the problems of short time spans inconsistency and biases these additional measurements have limitations on precisely capturing spatio temporal pattern of heat wave impacts the reason why we choose temperature based heat wave definition is because it can be calculated from readily available climatological data and provides information on various aspects of heat waves in other words the choice of temperature rather than other measures is based on their feasibility across varying climates on long term scales further the availability of long term temperature datasets at finer spatial scales can greatly improve our understanding of heat wave we concur that the heat wave definitions directly rely on the critical temperature thresholds however there is no universal temperature threshold for health impacts because of regional variability of health status socio economic factors and demographic factors alexander and perkins 2013 this impact also exists in agriculture due to varying regional patterns of plant species and physiology therefore a given threshold suitable in a small region may not be applicable to a continental study like ours fischer and schär 2010 explored health related heat wave indices in three health factors heat wave duration minimum temperature and relative humidity our study also quantified the heat wave duration minimum temperature based heat wave indices a combined calculation of temperature and humidity will be considered in our future study 5 conclusion we have developed a heat wave toolbox that has the ability to estimate past current and future changes in heat waves at a continental scale it uses a well known heat wave framework constructed by alexander and perkins 2013 and considers intensity frequency magnitude duration and areal extent to explore the spatio temporal evolution of heat wave severity and coverage this study is the first attempt to estimate heat wave events across australia using high spatio temporal climate datasets with these heat wave aspects from multi source data and different methods we were able to investigate the effects of scales data quality and definition we find that era5 datasets are the best in characterizing the heat wave events in exploring the role of different methods on the identification of heat waves we find that heat wave characteristics based on the excess heat factor index integrate the features of both tx90 based and tn90 based methods with the past 100 years of heat wave datasets the hwa average mean values were calculated and used to estimate non stationary return levels and return periods we find that extreme heat wave events have much higher probability due to the effects of climate change the heat wave event in 2019 may be more frequent in the coming decades for the climate by the end of century using heat wave metrics derived from a multi model ensemble mean we predict hwa to increase significantly during the two future periods and a larger fraction of southern australia is projected to experience more extreme heat wave events furthermore the patterns of change for hwd are opposite to those for hwa northern australia shows significant increases and southern australia experience a moderate increase the methodology and the cloud computing based toolbox hwt is useful for dynamic visualization extraction and processing of complex heat wave events and applicable anywhere in the world declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the first author acknowledges the chinese scholarship council and university of technology sydney for scholarships and nsw department of planning industry and environment for providing office facilities to conduct this work the first author also acknowledges dr jonathan gray for article revision help 
