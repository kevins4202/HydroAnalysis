index,text
5730,expected increases in intensity and frequency of rainfall extremes due to climate change and increased paving and loss of water storage space in urban areas is making cities more vulnerable to pluvial flooding it is a crucial step towards risk mitigation and adaptation planning to evaluate and predict flood disaster risk in this study a model coupling ontology and bayesian network that can capture the potential relationships between different factors influencing flood disaster and has capable of quantifying uncertainty and utilizing both data and observations was proposed based on the proposed model the flood disaster risk in zhengzhou city was assessed which was validated by comparing with actual historical flood records and the sensitivity analysis was carried out based on this model to determine that which factors would affect the disaster most the results show that 72 3 of the study area was characterized by very high to moderate flood disaster risk which was mainly located in the central and eastern regions the sensitivity analysis show that the rainfall duration is the most impactful factor and the factors related to disaster formative environment have low effect the results revealed quite good agreement between the predicted flood disaster risk with actual flood records which can be useful to assist flood mitigation and management the model proposed in this study offers the zhengzhou city and flood researches around the world an effective way for evaluating flood disaster risk to manage uncertainty in water availability under climate change keywords urban flood disaster risk evaluation ontology bayesian network sensitivity analysis 1 introduction over the past few decades prevalent flooding in urban areas is increasing having become a challenging issue urban flood disaster a product of extreme rainfall and the vulnerability and resiliency of the affected area threatens the security of society and normal development of economy in cities darabi et al 2019 toosi et al 2019 for example between 2003 and 2009 26 major events caused market valued damages amounting to about eur 17 billion with 320 human fatalities eea 2010 in 2018 typhoon rumbia and ensuing flood threatened 8 provinces in china costing 1400 million dollars accordingly urban flood disaster risk evaluation and management programs were often established to control flood damage tehrany et al 2014 chen et al 2015 for instance a study applied empirical models using parameters that directly or indirectly affect flood occurrence to estimate the flood susceptibility of an area was proposed by marconi et al 2016 orencio and fujii 2013 implemented analytic hierarchy process ahp and weighted linear average to prepare a disaster resilience index and similar methods were used in studies of other catchments wu et al 2019a because of this wide range the difficulty of assessing flood disaster risk and complex uncertainties urban flood disaster risk evaluation has also become a focus in the field of flood disaster control and management internationally how to quantitatively assess the flood disaster risk to avoid or manage the level of hazard change and to minimize potential damages is key to implement flooding management practices methods for estimating urban flood disaster risk include historical flood data based method sado inamura and fukushi 2019 index system method mahmoud and yew 2018 and scenario simulation based on hydrologic hydraulic model li et al 2019 the index system method is preferred by many researchers for example chen et al 2015 developed spatial multi criteria decision making prototype to assess flood disaster risk in australia but the analysis results were restricted by the data availability and the selection of criteria the ahp was adopted by lin et al 2017 to evaluate flood disaster vulnerability in zhengzhou city louise et al 2019 proposed a multi criteria index called spatialized urban flood resilience index to measure and visualize the changes in flood resilience attained by different scenarios however the method for flood disaster assessment was usually based on experts knowledge weighted sum or weighted mean index aggregation method were often used to determine the influence of various factors on flood disaster which assumed the parameters considered to be independent aroca jimenez et al 2018 the index system method fails to deal with subjective effects resulting from too much reliance on empirical knowledge and do not take advantage of objective data indicating a relatively strong uncertainty and low credibility in assessment furthermore this method has defects in describing mutual relationships between risk factors in fact according to the characteristics of formation of urban flood disaster the flood disaster risk was formed by disaster drivers disaster formative environment and disaster bearers including many parameters and data ozdemir 2011 wu et al 2019b these decision variables and their interrelations are often interdependent in a complex and uncertain way disaster drivers such as extreme rainfall are key indicators of flood disaster while disaster formative environment plays a vital role in the redistribution of rainfall meanwhile the consequence caused by disaster drivers depends on the situation of disaster bearers such as the number of population and impacted infrastructure the vulnerability and resiliency of the affected area the underlying question is how to establish a model to quantify the relationship between various hazardous factors and evaluate flood disaster risk objectively a methodology that can capture the potential relationships between factors influencing flood disaster and quantify uncertainty based on ontology and bayesian network bn was presented in this study firstly on the basis of factors influencing urban flood disasters bn structure graph was established based on ontology to capture the potential relationships among various hazardous factors and then the probability distribution table of bn was calculated based on historical disaster data finally the likelihood and probability of flood disaster risk in zhengzhou city was quantified and the sensitivity analysis was carried out to determine the most impactful factors 2 materials and methods 2 1 case study and available data 2 1 1 case study zhengzhou a city in north central henan province china is located between 112 42 and 114 14 eastern longitude and between 34 16 and 34 58 northern latitude which comprises an area of approximately 1010 km2 fig 1 zhengzhou is in a temperate continental climate with a mean annual precipitation of 625 9 mm the flood season a period of frequent rainstorm and flood disasters spans from july to september every year during which the rainfall accounts for 60 70 of the total annual rainfall prolonged heavy rainfall during the flood season caused a significantly high flow discharge which severely affected the development of society zhengzhou city is characterized as lowland with an average altitude of 50 m and the average slope in most of the wards 70 of the total area is below 5 in addition there are few underground drainage network in this city and most of them have been constructed for years which are aging and have low design capacity because of this the water flow cannot be discharged in time when heavy rainfall occurs which resulted in serious water accumulation fig 2 according to the statistics zhengzhou has suffered heavy rainstorm more than 15 times per year since 2006 and each time a flood disaster has caused more than 30 million dollars in economic losses 2 1 2 identification of the criteria making any decision requires employing the measurable and comparable decision criteria according to the questionnaire survey experts opinion and a review of literature combined with the characteristics of flood disaster in the study area 11 effective flood criteria were selected to assess flood disaster risk in this study the factors influencing flood disaster were divided into three classes disaster drivers disaster formative environment and disaster bearers and each of them was composed of a series of sub factors as shown in fig 3 more information about determining the criteria can be introduced by wu et al 2019b in fig 3 disaster drivers are defined as extreme severe and prolonged weather events that adversely affect human life property and security the flood disaster is closely related to the size of rainfall the number of rainstorm days and the rainfall intensity since the rainfall intensity was calculated through the accumulated rainfall and rainfall duration the accumulated rainfall and rainfall duration were used to characterize disaster drivers disaster formative environment refers to conditions and surroundings where flood disaster occurred determined by factors that mainly result from the combination of both climate variables and underlying surfaces according to the characteristics of study topography river network and land cover were considered as the most important factors for flood disaster risk assessment these factors interactively influence and control the dynamic processes of flooding disaster bearers refers to the human being and social properties affected by the disaster and its damage the flood disaster is not only related to rainfall and urban conditions but also to resilience and adaptability of a region such as the distribution of population the condition of road and underground pipe the development of economy and so on therefore four indices were considered as most critical criteria for the resilience and adaptability of case study as shown in fig 3 2 1 3 data sources major data collected from different sources can be classified into four categories 1 times series observations including accumulated rainfall from 2010 to 2018 2 raster data including dem land cover and the images of historical flood disaster risk 3 vector data including the study area boundary the river network road distribution drainage pipe and 4 statistical data including the population and gdp data was processed using arcgis 10 3 the data collection and processing are discussed below and some of the data were presented in table s1 in supplementary materials 1 rainfall rainfall data was obtained from the china meteorological science data sharing network http data cma cn and the rainfall monitoring stations in zhengzhou city two indices were derived from this rainfall duration that is the number of days from the onset of a rainstorm to the end of a flood and accumulated rainfall that is the total amount of rainfall during a flood there are 13 rainfall stations in and around the case study fig 4 a based on these stations rainfall for all part of the city was calculated using kriging interpolation in arcgis as shown in fig 4a 2 topographic characteristics topography has a significant effect on flood formation and redistribution elevation and slope are regarded as the most impactful factors of terrain in flood disaster elevation is commonly represented by the vertical distance from certain surface to the reference basement while slope is a measure of the average rate of change of elevation in a given domain both elevation and slope in each study grid cells were generated from the digital elevation model dem at a 30 m resolution which was downloaded from the geospatial data cloud service platform and then these two terrain factors were reclassified using arcgis 10 3 which were divided into 4 levels by block statistics and grid algebra calculation as shown in fig 4b and c respectively 3 river network the distribution of river was collected from national bureau of surveying and mapping geographic information river density and river proximity were designed to describe the impact of rivers on flood disasters river density refers to the length of rivers per unit area which was calculated from the line density function using 200 m radius while river proximity shows the distance to the closest river which was obtained using the multiple buffer operator as shown in fig 4d and e respectively 4 land cover land cover is one of the most important underlying factors in disaster formative environment which can also be downloaded from national bureau of surveying and mapping geographic information of particular interest for this study is the relative proportion of paved surfaces and impervious area therefore impervious area was estimated by summing up buildings roads and other paved surfaces in this study impervious area ratio paved surfaces divided by the total area was considered as one of the indicator of impacting flood disaster as shown in fig 4f 5 social and economic factors the flood disaster is not only related to the intensity of the disaster drivers and disaster formative environment but also to the resilience and adaptability of a region in densely populated areas the damage caused by floods is more serious than others which could be at high risk the regions with prosperous economy and convenient transportation show higher adaptability to flood disaster furthermore the development situation of drainage pipe is other important factor that affects the likelihood of flood disaster water flow can be discharged quickly in areas with dense drainage pipes therefore population density per unit gdp road density and pipe density were considered as measuring elements of resilience and adaptability which were provided by zhengzhou statistical yearbook and exhibited using arcgis as shown in fig 4g j 2 2 bn description bn is probabilistic graphical model which explicitly includes dependence between multiple random variables balbi et al 2016 abebe et al 2018 it generally contains a qualitative component and a quantitative one the qualitative component of bn is a directed acyclic graph where nodes and directed links signify system variables and their mutual relationships the structure determines which nodes are identified as parents of given node i e the predecessors or as children i e the successors the quantitative one is a collection of numerical parameters representing probability distributions that specify the dependence relations encoded in the structure graph the bn can be denoted as follows 1 n g p where g is the bn structure graph g v e v stands for the set of nodes i e v 1 v 2 v n denoting variables in flood domain e represents the set of directed edges that indicate the dependence between nodes which generally points from the parent node to the child node p expresses the parameters set of the bn including the prior probability and the conditional probability distribution table cpt of nodes denoting the strength of dependencies between nodes the prior probabilities are defined based on prior knowledge alternatively it may be learned from data or even a combination of the two while the conditional probability distribution for each variable x i is given conditioned on its parent set p x i x p a i where x p a i is the parent set of x i a bn was shown in fig 5 where x y and z represent random variables and the arcs with arrow specify the dependencies of the variables for example x points to z indicating that x is the parent node of z while z is the child node of x x and y have no parent node which is also called root node root node has the prior probability while the child node has no prior probability but conditional probability the bn is explained using the genie development tool a graphical interface to smile structural modeling inference and learning engine balbi et al 2016 genie provides a user interface for introducing data where data can be directly imported from excel or database besides it not only has a variety of algorithms that is helpful for structure and parameter learning which facilitates logical reasoning of the problems being studied but performs sensitivity analysis on simple graphs to calculate their impact on the results therefore the flood disaster risk evaluation model was created using genie 2 3 ontology description there are many factors influencing flood disaster which can be obtained from various sources such as internet government experts literature and so on they maybe have different descriptions of the same influencing factors making it challenging to understand and share ontology has many advantages such as standardizing the definition of factors influencing flood disaster to avoid conflicts capturing and signifying mutual dependencies between various factors using unified terms and standards based on their properties and so on garrido et al 2012 elag and goodall 2013 aragao and el diraby 2019 ontology is a formal explicit specification of a shared conceptualization to communicate a consensual and general understanding kontopoulos et al 2016 it is supported by a graphical network representing potential relationships between different factors considered in a study aragao and el diraby 2019 which enables the description of super sub cause effect and attribute relationships wu et al 2019c ontology has been utilized as a tool in urban flood disaster studies such as disaster monitoring alirezaie et al 2017 risk assessment giupponi et al 2015 disaster management jung and chung 2015 ontology consists of concepts properties and relationships between the concepts where concepts are usually represented by classes of factors properties refer to features that the factors have and relationships between the concepts are typically represented by properties kontopoulos et al 2016 as shown in eq 2 2 ontology c p r where c refers to the concepts related to factors influencing flood disaster which is the kernel element of ontology p is the property restriction which denotes the features that factors have the nature and inherent features of concepts are indicated by property restrictions relating to a class or a set of classes to a data type for example category describes the conceptual classification like observed data and history data while format denotes specific file pattern such as geotiff img and shpfile r is the relationships between influencing factors two main relationships cause and effect parent and child were defined to characterize the connections between concepts which will be introduced in the following a set of connections composes a directed graph that specifies the relationship between the concepts of flood disaster the technological process of building ontology was shown in fig 6 which was divided into three steps 1 identify and determine the properties of the factors 2 classify factors influencing flood disaster and 3 define the relationship between influencing factors in ontology the influencing factors were redefined and classified into different classes according to their properties while the relationships between various factors were established based on this classification and their properties different from traditional methods where the relationships between different influencing factors were established based on experience and experts knowledge ontology develops the mutual relationship between various influencing factors by mining their properties which can effectively minimize the impact of subjectivity 2 4 development of flood disaster risk evaluation model based on ontology and bn the construction of flood disaster risk evaluation model based on ontology and bn was divided into two steps structural learning and parameter learning so called training which will be detailed in the following sections 2 4 1 structural learning the structural learning was carried out based on ontology bn structure graph was established based on ontology to clearly demonstrate the system variables and their mutual relationships which is one of the highlights to establish flood disaster risk evaluation model first of all the selected factors influencing flood disaster such as rainfall elevation slope population road and so on were considered as the nodes of bn structure graph according to their properties the influencing factors were generalized and classified into several classes climate rainfall and duration topography elevation and slope hydrology river density and river proximity land cover impervious area ratio and socioeconomic population density road density drainage pipe and per unit gdp the relationships between various factors influencing flood disaster were then determined and defined based on classification and their properties there are many relationships in ontology but according to the characteristic of probabilistic inferences of bn model and the aim of flood disaster risk evaluation the parent and child cause and effect are the most important relationships to be considered as follows 1 parent and child which can be represented using is a in ontology classes of factors would be organized in a specific hierarchy using this relationship for example the disaster formative environment consists of topography hydrology and land cover which can be further classed into different factors as shown in fig 7 a where initial node of the arrow expresses the class of parent while the pointing node expresses the class of child 2 cause and effect which also called causal relationship it is expressed that the occurrence of one thing leads to the occurrence of another for example heavy rainfall causes flood disaster this relationship can be shown in fig 7b where the initial node of the arrow is the cause while the pointing node is the effect 2 4 2 parameter learning since bn generally deals with discrete probabilities each node was classified into a finite set of state values accompanying with a probability han and coulibaly 2017 for this reason it is necessary to discretize all relevant data which is the first and important procedure in parameter learning the factors affecting flood disaster were grouped into 4 grades as summarized in table 1 where the accumulated rainfall rainfall duration elevation slope river density and proximity were classified according to recent works chen et al 2015 yoon et al 2015 lin et al 2017 while the impervious area ratio road density population density pipe density and per unit gdp were divided through the statistics of land population and economy and historical disasters in case study given the developed bn structure graph certain learning algorithms such as bayesian estimation and maximum likelihood estimation mle can be used for parameter learning through training sample data to determine the conditional probability distribution among related variables vogel et al 2014 the mle is generally used for large data size and the estimated parameters coincide well with the actual state in this research many data related to rainstorms and floods could be gathered from various sources so the mle is more suitable for parameter learning this method aims to find the parameters that maximize the likelihood function and the working principle is as follows it is assumed that the observed data set d y 1 y 2 y n is independent and uniformly distributed and the likelihood function of d is functional to the model parameters which can be computed as 3 p d θ s s h i 1 n p y i θ s s h where θ s represents an unknown parameter s h refers to the bn structure graph since the observed data set d has been determined the parameters can be obtained by maximizing the likelihood function in parameter learning the updated probability for the m number of mutually exclusive variables or parameters v i i 1 2 m and given evidence or data d were determined by the following relationship 4 p v j d p d v j p v j i 1 m p d v i p v i where p v i d is the conditional probability of v based on the data or evidence d p v j refers to the prior probability p d v j denotes the conditional probability 2 5 effect evaluation the ontology and bn based flood disaster risk evaluation model was developed using the historical case data during the period 2010 2018 and the flood disaster risk in zhengzhou city from august 16 to 20 2019 was predicted and evaluated in this model the state of one or more parent nodes is changed and changes of probability distributions in child nodes are examined for instance in one scenario if it was low land the biggest amount of rainfall and duration a lot of impervious area and residents it was expected that the extent of damage to the physical environment with regard to the variable used in the proposed model was increased significantly and the probability value was given by this model the probability of flood disaster in other scenarios were also obtained through the same way based on the predicted probability of flood disaster the flood disaster risk was divided into 5 categories very low low moderate high and very high and their corresponding probabilities were presented in table 2 respectively the spatial buffer technique of arcgis was employed to analyze the spatial distribution characteristics of flood disaster risk the number of grid cells per risk class for predicted and actual conditions were counted using arcgis 10 3 the performance of model was quantified by analyzing relative error of prediction which is defined as follows 5 re y pre y act y act 100 where re refers to the relative error of prediction y pre and y act are respectively the number of cell grids for predicted and actual state in this paper zhengzhou city has been classified into 23 844 grid cells except for zones near the city s boundary all grid cells have an area of 0 04 square kilometers 3 results and discussions 3 1 construction of flood disaster risk evaluation model based on ontology and bn flood disaster risk evaluation model in case study was established based on the methodologies proposed above as shown in fig 8 where the nodes the relationships between them and their corresponding states were clearly presented from this figure the model for assessing flood disaster were divided into four groups of factors which were expressed by different colors respectively the white node expressed flood disaster where the level of flood disaster risk was classified according to the probability of flood occurrence the factors related to disaster drivers were expressed by blue while the influencing factors related to disaster formative environment were represented by red and yellow nodes represented disaster bearers which included pipe density population density road density and per unit gdp the states of these nodes were given in the squares e g the node elevation has different altitudes such as low moderate high or serious in each square the bars represent the probabilities of the state expressed as percentage combined with the classification of different factors table 1 the meaning of each probability was indicated taking the node rainfall as an example 2 of the accumulated rainfall are below 50 mm that is represented using the state of low moderate indicates that the rainfall is between 51 and 100 mm with a probability of 49 high means that accumulated rainfall ranges from 101 to 150 mm and its probability is 41 and serious says that the probability of being above 150 mm is 8 the state of the rest of the nodes could also be defined in this way it was shown that the remaining 11 factors directly or indirectly affected the occurrence of flood disaster the connected nodes have causal relationship among which the initial node of the arrow is the cause while the pointing node is the effect this relationship was expressed by the conditional probability calculated based on historical data eq 4 which can be queried by attributes of each node the conditional probabilities distribution of node pipe density was shown in table s2 in supplementary materials which allowed several queries of conditional probabilities in different states 3 2 prediction of urban flood disaster risk using the developed flood disaster risk evaluation model under the specific 2010 2018 conditions the expected values of probability of flood disaster risk for the entire 23 844 grid cells were predicted and summarized in table 3 for instance 2763 grid cells have 0 8 probability of having flood disaster risk of very high the summarized results of table 3 emphasized the changes between risk classes aggregated per cell which can be a good quantitative measure of the performance of the proposed model in this table the number of cells corresponding to different levels of risk for actual flood disasters conditions was obtained from historical case information while the number of grids for prediction was obtained from the proposed model it can be seen from this table that the most grids recorded a moderate to high risk to flood disaster while few cells were at very low risk further analysis was shown in fig 9 in fig 9 a flood disaster risk map was delineated as an output from the proposed risk evaluation model combined with the relevant data in table 3 the percentage of each risk class in the study area was calculated about 6 5 of the total study area was classified as very low risk and 21 2 was low risk suggesting that these regions were less likely to suffer from floods whereas 26 8 grids were found to have high risk plus another 11 6 were very high risk this means that 72 3 of the study area was characterized by very high to moderate flood disaster risk which explained reasons behind the recurrent flood events in the zhengzhou city from fig 9 these areas were located in the eastern and central region of zhengzhou these zones have elevation ranging from 0 to 100 m high population density with more than 4000 people per kilometers high intensity rainfall and consist mainly of built up areas such as residential district business district therefore when an intense rain event occurs the highest surface runoff is observed in this areas due to the increase in impervious surface which decreases the soil infiltration loss factors such as prolonged rainfall density and topography of these areas can contribute to increase flood disaster risk it is suggested that the importance of increased urbanization is evident in recurrent flood disaster events southwest areas belong to very low and low risk zones partly because they have high elevation and less rainfall the northwest parts of zhengzhou are new city zones with low density of property and population which also records low flood disaster risk overall it can be recognized in fig 9 that the flood disaster risk of zhengzhou city is decreasing from the middle to the surroundings which was basically consistent with the previous studies in this area lin et al 2017 wu et al 2019a according to this results strategies to cope with and prevent flood disaster should be proposed to reduce losses for different regions in very low to low risk areas risk reduction should be targeted towards individual protection especially the elderly and children while for the high and very high risks the measures are not restricted to the reduction of the flood disaster by forecasting the occurrence of rainfall but also include the reduction of risk by changing the receptors e g new land use or improved management allowing the application of multi disciplinary approaches for example the impervious areas should be decreased by promoting green development like green roof constructions encouraging the use of pours pavement materials that allow water infiltration and increasing proportion of green areas 3 3 model performance the result of flood disaster risk evaluation was validated by comparing with the actual historical flood records based on the statistical results eq 5 was used to calculate the relative error of this method and the calculation results were presented at line 3 of table 3 the results revealed quite good agreement between the predicted flood disaster risk with actual flood disaster according to the comparison of relative errors the model has the smallest error in the prediction of very low risk whose relative error is about 4 4 followed by low and moderate risks the relative error of very high risk is the highest at 12 2 it can be concluded that compared with low and very low risks the prediction of high and very high risks is poor the formation of flood disaster is very complicated and the flood disaster risk was affected by many factors but the influencing factors considered in this study were not comprehensive enough which may result in poor prediction in areas with very high and high risks moreover the map of actual flood disaster risk obtained from historical case was compared with the distribution of flood disaster risk predicted using the proposed model the actual risk map obtained from historical disaster case was shown in fig 10 the modeling results conform basically and reasonably to the actual disaster with almost the same risk distribution within the area delineated as low and very low risks in the proposed model southwestern and northwestern parts of the city with low urbanization rate and population density are at low and very low risk in historical flood records 10 mismatched pixels were found in very high and high risk areas which are mainly spread in the middle of zhengzhou the scope of high to very high risk areas was underestimated it is indicated that the prediction accuracy of high risk needs to be further improved it can be observed that although there are still certain mismatches between predicted and actual conditions the developed flood disaster risk evaluation model can predict the general distributed trends of flood disaster risk which provides a new way for flood disaster risk assessment 3 4 sensitivity analysis because many factors are being produced with different sets of input variables the proposed model based on ontology and bn can learn and record the relative importance of the input variables in predicting the output sensitivity analysis was applied to find variables that strongly affect the behavior of a system and to determine variables that were not very sensitive to changes in the process of assessing flood disaster risk the results of sensitivity analysis were presented in fig 11 where the sensitivity was demonstrated using the depth of node color and the sensitivity value of each factor influencing flood disaster risk was exhibited in table 4 according to the sensitivity values of different factors rainfall and duration are the most impactful factors followed by population density road density and per unit gdp which affects the possibility of flood disasters to some degree it should be noted that although the sensitivity value of some factors is low it does not mean these are not vital elements in pluvial flood which is particularly evident for elevation slope pipe density and river network several reasons can be used to explain this zhengzhou city is characterized as lowland and there are little changes in elevation and slope the elevation and slope have direct effect on trend of most rivers therefore the factors related to disaster formative environment have lower sensitivity value partly due to their almost uniform across the case study which have an unnoticeable impact on the flood disaster it is also interesting to notice that the pipe density has low sensitivity values which indicates that its influence on flood disaster is not as visible as others there are few drainage pipes in zhengzhou city and the pipe density with in different regions has little difference which have less impact on the evaluation result according to the sensitivity analysis it is necessary to strengthen the rainstorm forecast in the future to prevent the occurrence of disasters in advance this can be achieved by employing more advanced methods for forecasting rainfall encouraging the application of new technology such as radar and remote sensing to improve forecast accuracy and lead time and so on in addition it is necessary to enhance business and public awareness of the possible damage they may suffer and of the total damage a flood can cause and improve their adaptability to flood disaster improving the urban land use situation and reducing the impervious area are also key measures to avoid major costs in case of flood disaster 3 5 advantages and limitations over the past few years we have been dedicated to flood disaster risk analysis in zhengzhou city for example lin et al 2017 proposed a flood disaster vulnerability evaluation method based on ahp in zhengzhou however the ahp was characterized by subjectivity because the index weight was determined based on experts knowledge besides only six indices were selected to assess flood disaster vulnerability in this study and failing to make full use of objective data was another limitation of this research in order to takes advantage of objective data and reduce the uncertainty caused by the determination of index weight the bn model was proposed by wu et al 2019b to assess flood disaster risk the results show that the flood disaster risk in central and eastern area appears to be high which was in agreement with the distribution of risk obtained from the present study regarding risk some factors such as topography river network road density and so on were selected as indicators but they seem to be correlated with other indicators and mutual indicators however there were less relationships among various factors in the bn model developed by wu et al 2019b and some of potential relationships were also defined by experiences which had some subjectivity besides the development situation of drainage facilities is important for flooding but it has not been considered at all in wu et al 2019b in order to fully determine the relationships between various factors the ontology was introduced in this work combining ontology with bn the proposed model can not only objectively define the potential relationships among factors based on their properties in ontology but make full use of historical disaster data which reduces the subjectivity and uncertainty and as a result the results of flood disaster assessment are more accurate and reliable than previous methods although it is expected that the methodologies adopted in this paper would better predict such flood events based on ontology and bn there are still many problems in the research the data processing steps have reduced the quality of the analysis a lot of data were obtained using interpolation technology in arcgis such as population density per unit gdp besides the influencing factors considered in this research were not comprehensive due to the data availability therefore how to obtain high quality data and constantly improve the evaluation index system is the focus of future study 4 conclusions in this study a model for the assessment of flood disaster risk under extreme climates was developed based on the ontology and bn the potential relationships between factors influencing flood disaster were determined based on ontology by mining their properties and the probability distribution table was developed using bn based on a lot of historical disaster data that were collected and processed using gis the flood disaster risk in zhengzhou city was evaluated based on this model the case study revealed that the relative error of very low low moderate high and very high risks predicted by the proposed model were 4 4 10 1 9 3 11 3 and 12 2 respectively based on the spatial distribution of different risk levels it can be recognized that the flood disaster risk of zhengzhou city was decreasing from the middle to the surroundings and the sensitivity analysis showed that disaster drivers and disaster bearers were the most influential factors therefore the rainstorm forecast of zhengzhou should be strengthened to prevent the occurrence of disasters in advance and the resilience and adaptability of city should also be improved this can be achieved by increasing proportion of green areas laying underground pipeline improving the ability of residents to cope with disasters through propaganda and education and so on compared with previous methods where the weights of different factors were determined through experiences and knowledge to assess flood disaster risk deterministically the main innovation of the proposed model based on ontology and bn is that it can take advantage of many historical data to predict the probability of the flood disaster quantitatively and has the ability to quantify uncertainty in the decision making process in other urban areas based on its characteristic and data availability the proposed model can be revised and adopted which provides a new method for flood disaster risk assessment in other areas declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the study is funded by the key project of national natural science foundation of china no 51739009 the authors thank the anonymous reviewers for their valuable comments appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2020 124596 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
5730,expected increases in intensity and frequency of rainfall extremes due to climate change and increased paving and loss of water storage space in urban areas is making cities more vulnerable to pluvial flooding it is a crucial step towards risk mitigation and adaptation planning to evaluate and predict flood disaster risk in this study a model coupling ontology and bayesian network that can capture the potential relationships between different factors influencing flood disaster and has capable of quantifying uncertainty and utilizing both data and observations was proposed based on the proposed model the flood disaster risk in zhengzhou city was assessed which was validated by comparing with actual historical flood records and the sensitivity analysis was carried out based on this model to determine that which factors would affect the disaster most the results show that 72 3 of the study area was characterized by very high to moderate flood disaster risk which was mainly located in the central and eastern regions the sensitivity analysis show that the rainfall duration is the most impactful factor and the factors related to disaster formative environment have low effect the results revealed quite good agreement between the predicted flood disaster risk with actual flood records which can be useful to assist flood mitigation and management the model proposed in this study offers the zhengzhou city and flood researches around the world an effective way for evaluating flood disaster risk to manage uncertainty in water availability under climate change keywords urban flood disaster risk evaluation ontology bayesian network sensitivity analysis 1 introduction over the past few decades prevalent flooding in urban areas is increasing having become a challenging issue urban flood disaster a product of extreme rainfall and the vulnerability and resiliency of the affected area threatens the security of society and normal development of economy in cities darabi et al 2019 toosi et al 2019 for example between 2003 and 2009 26 major events caused market valued damages amounting to about eur 17 billion with 320 human fatalities eea 2010 in 2018 typhoon rumbia and ensuing flood threatened 8 provinces in china costing 1400 million dollars accordingly urban flood disaster risk evaluation and management programs were often established to control flood damage tehrany et al 2014 chen et al 2015 for instance a study applied empirical models using parameters that directly or indirectly affect flood occurrence to estimate the flood susceptibility of an area was proposed by marconi et al 2016 orencio and fujii 2013 implemented analytic hierarchy process ahp and weighted linear average to prepare a disaster resilience index and similar methods were used in studies of other catchments wu et al 2019a because of this wide range the difficulty of assessing flood disaster risk and complex uncertainties urban flood disaster risk evaluation has also become a focus in the field of flood disaster control and management internationally how to quantitatively assess the flood disaster risk to avoid or manage the level of hazard change and to minimize potential damages is key to implement flooding management practices methods for estimating urban flood disaster risk include historical flood data based method sado inamura and fukushi 2019 index system method mahmoud and yew 2018 and scenario simulation based on hydrologic hydraulic model li et al 2019 the index system method is preferred by many researchers for example chen et al 2015 developed spatial multi criteria decision making prototype to assess flood disaster risk in australia but the analysis results were restricted by the data availability and the selection of criteria the ahp was adopted by lin et al 2017 to evaluate flood disaster vulnerability in zhengzhou city louise et al 2019 proposed a multi criteria index called spatialized urban flood resilience index to measure and visualize the changes in flood resilience attained by different scenarios however the method for flood disaster assessment was usually based on experts knowledge weighted sum or weighted mean index aggregation method were often used to determine the influence of various factors on flood disaster which assumed the parameters considered to be independent aroca jimenez et al 2018 the index system method fails to deal with subjective effects resulting from too much reliance on empirical knowledge and do not take advantage of objective data indicating a relatively strong uncertainty and low credibility in assessment furthermore this method has defects in describing mutual relationships between risk factors in fact according to the characteristics of formation of urban flood disaster the flood disaster risk was formed by disaster drivers disaster formative environment and disaster bearers including many parameters and data ozdemir 2011 wu et al 2019b these decision variables and their interrelations are often interdependent in a complex and uncertain way disaster drivers such as extreme rainfall are key indicators of flood disaster while disaster formative environment plays a vital role in the redistribution of rainfall meanwhile the consequence caused by disaster drivers depends on the situation of disaster bearers such as the number of population and impacted infrastructure the vulnerability and resiliency of the affected area the underlying question is how to establish a model to quantify the relationship between various hazardous factors and evaluate flood disaster risk objectively a methodology that can capture the potential relationships between factors influencing flood disaster and quantify uncertainty based on ontology and bayesian network bn was presented in this study firstly on the basis of factors influencing urban flood disasters bn structure graph was established based on ontology to capture the potential relationships among various hazardous factors and then the probability distribution table of bn was calculated based on historical disaster data finally the likelihood and probability of flood disaster risk in zhengzhou city was quantified and the sensitivity analysis was carried out to determine the most impactful factors 2 materials and methods 2 1 case study and available data 2 1 1 case study zhengzhou a city in north central henan province china is located between 112 42 and 114 14 eastern longitude and between 34 16 and 34 58 northern latitude which comprises an area of approximately 1010 km2 fig 1 zhengzhou is in a temperate continental climate with a mean annual precipitation of 625 9 mm the flood season a period of frequent rainstorm and flood disasters spans from july to september every year during which the rainfall accounts for 60 70 of the total annual rainfall prolonged heavy rainfall during the flood season caused a significantly high flow discharge which severely affected the development of society zhengzhou city is characterized as lowland with an average altitude of 50 m and the average slope in most of the wards 70 of the total area is below 5 in addition there are few underground drainage network in this city and most of them have been constructed for years which are aging and have low design capacity because of this the water flow cannot be discharged in time when heavy rainfall occurs which resulted in serious water accumulation fig 2 according to the statistics zhengzhou has suffered heavy rainstorm more than 15 times per year since 2006 and each time a flood disaster has caused more than 30 million dollars in economic losses 2 1 2 identification of the criteria making any decision requires employing the measurable and comparable decision criteria according to the questionnaire survey experts opinion and a review of literature combined with the characteristics of flood disaster in the study area 11 effective flood criteria were selected to assess flood disaster risk in this study the factors influencing flood disaster were divided into three classes disaster drivers disaster formative environment and disaster bearers and each of them was composed of a series of sub factors as shown in fig 3 more information about determining the criteria can be introduced by wu et al 2019b in fig 3 disaster drivers are defined as extreme severe and prolonged weather events that adversely affect human life property and security the flood disaster is closely related to the size of rainfall the number of rainstorm days and the rainfall intensity since the rainfall intensity was calculated through the accumulated rainfall and rainfall duration the accumulated rainfall and rainfall duration were used to characterize disaster drivers disaster formative environment refers to conditions and surroundings where flood disaster occurred determined by factors that mainly result from the combination of both climate variables and underlying surfaces according to the characteristics of study topography river network and land cover were considered as the most important factors for flood disaster risk assessment these factors interactively influence and control the dynamic processes of flooding disaster bearers refers to the human being and social properties affected by the disaster and its damage the flood disaster is not only related to rainfall and urban conditions but also to resilience and adaptability of a region such as the distribution of population the condition of road and underground pipe the development of economy and so on therefore four indices were considered as most critical criteria for the resilience and adaptability of case study as shown in fig 3 2 1 3 data sources major data collected from different sources can be classified into four categories 1 times series observations including accumulated rainfall from 2010 to 2018 2 raster data including dem land cover and the images of historical flood disaster risk 3 vector data including the study area boundary the river network road distribution drainage pipe and 4 statistical data including the population and gdp data was processed using arcgis 10 3 the data collection and processing are discussed below and some of the data were presented in table s1 in supplementary materials 1 rainfall rainfall data was obtained from the china meteorological science data sharing network http data cma cn and the rainfall monitoring stations in zhengzhou city two indices were derived from this rainfall duration that is the number of days from the onset of a rainstorm to the end of a flood and accumulated rainfall that is the total amount of rainfall during a flood there are 13 rainfall stations in and around the case study fig 4 a based on these stations rainfall for all part of the city was calculated using kriging interpolation in arcgis as shown in fig 4a 2 topographic characteristics topography has a significant effect on flood formation and redistribution elevation and slope are regarded as the most impactful factors of terrain in flood disaster elevation is commonly represented by the vertical distance from certain surface to the reference basement while slope is a measure of the average rate of change of elevation in a given domain both elevation and slope in each study grid cells were generated from the digital elevation model dem at a 30 m resolution which was downloaded from the geospatial data cloud service platform and then these two terrain factors were reclassified using arcgis 10 3 which were divided into 4 levels by block statistics and grid algebra calculation as shown in fig 4b and c respectively 3 river network the distribution of river was collected from national bureau of surveying and mapping geographic information river density and river proximity were designed to describe the impact of rivers on flood disasters river density refers to the length of rivers per unit area which was calculated from the line density function using 200 m radius while river proximity shows the distance to the closest river which was obtained using the multiple buffer operator as shown in fig 4d and e respectively 4 land cover land cover is one of the most important underlying factors in disaster formative environment which can also be downloaded from national bureau of surveying and mapping geographic information of particular interest for this study is the relative proportion of paved surfaces and impervious area therefore impervious area was estimated by summing up buildings roads and other paved surfaces in this study impervious area ratio paved surfaces divided by the total area was considered as one of the indicator of impacting flood disaster as shown in fig 4f 5 social and economic factors the flood disaster is not only related to the intensity of the disaster drivers and disaster formative environment but also to the resilience and adaptability of a region in densely populated areas the damage caused by floods is more serious than others which could be at high risk the regions with prosperous economy and convenient transportation show higher adaptability to flood disaster furthermore the development situation of drainage pipe is other important factor that affects the likelihood of flood disaster water flow can be discharged quickly in areas with dense drainage pipes therefore population density per unit gdp road density and pipe density were considered as measuring elements of resilience and adaptability which were provided by zhengzhou statistical yearbook and exhibited using arcgis as shown in fig 4g j 2 2 bn description bn is probabilistic graphical model which explicitly includes dependence between multiple random variables balbi et al 2016 abebe et al 2018 it generally contains a qualitative component and a quantitative one the qualitative component of bn is a directed acyclic graph where nodes and directed links signify system variables and their mutual relationships the structure determines which nodes are identified as parents of given node i e the predecessors or as children i e the successors the quantitative one is a collection of numerical parameters representing probability distributions that specify the dependence relations encoded in the structure graph the bn can be denoted as follows 1 n g p where g is the bn structure graph g v e v stands for the set of nodes i e v 1 v 2 v n denoting variables in flood domain e represents the set of directed edges that indicate the dependence between nodes which generally points from the parent node to the child node p expresses the parameters set of the bn including the prior probability and the conditional probability distribution table cpt of nodes denoting the strength of dependencies between nodes the prior probabilities are defined based on prior knowledge alternatively it may be learned from data or even a combination of the two while the conditional probability distribution for each variable x i is given conditioned on its parent set p x i x p a i where x p a i is the parent set of x i a bn was shown in fig 5 where x y and z represent random variables and the arcs with arrow specify the dependencies of the variables for example x points to z indicating that x is the parent node of z while z is the child node of x x and y have no parent node which is also called root node root node has the prior probability while the child node has no prior probability but conditional probability the bn is explained using the genie development tool a graphical interface to smile structural modeling inference and learning engine balbi et al 2016 genie provides a user interface for introducing data where data can be directly imported from excel or database besides it not only has a variety of algorithms that is helpful for structure and parameter learning which facilitates logical reasoning of the problems being studied but performs sensitivity analysis on simple graphs to calculate their impact on the results therefore the flood disaster risk evaluation model was created using genie 2 3 ontology description there are many factors influencing flood disaster which can be obtained from various sources such as internet government experts literature and so on they maybe have different descriptions of the same influencing factors making it challenging to understand and share ontology has many advantages such as standardizing the definition of factors influencing flood disaster to avoid conflicts capturing and signifying mutual dependencies between various factors using unified terms and standards based on their properties and so on garrido et al 2012 elag and goodall 2013 aragao and el diraby 2019 ontology is a formal explicit specification of a shared conceptualization to communicate a consensual and general understanding kontopoulos et al 2016 it is supported by a graphical network representing potential relationships between different factors considered in a study aragao and el diraby 2019 which enables the description of super sub cause effect and attribute relationships wu et al 2019c ontology has been utilized as a tool in urban flood disaster studies such as disaster monitoring alirezaie et al 2017 risk assessment giupponi et al 2015 disaster management jung and chung 2015 ontology consists of concepts properties and relationships between the concepts where concepts are usually represented by classes of factors properties refer to features that the factors have and relationships between the concepts are typically represented by properties kontopoulos et al 2016 as shown in eq 2 2 ontology c p r where c refers to the concepts related to factors influencing flood disaster which is the kernel element of ontology p is the property restriction which denotes the features that factors have the nature and inherent features of concepts are indicated by property restrictions relating to a class or a set of classes to a data type for example category describes the conceptual classification like observed data and history data while format denotes specific file pattern such as geotiff img and shpfile r is the relationships between influencing factors two main relationships cause and effect parent and child were defined to characterize the connections between concepts which will be introduced in the following a set of connections composes a directed graph that specifies the relationship between the concepts of flood disaster the technological process of building ontology was shown in fig 6 which was divided into three steps 1 identify and determine the properties of the factors 2 classify factors influencing flood disaster and 3 define the relationship between influencing factors in ontology the influencing factors were redefined and classified into different classes according to their properties while the relationships between various factors were established based on this classification and their properties different from traditional methods where the relationships between different influencing factors were established based on experience and experts knowledge ontology develops the mutual relationship between various influencing factors by mining their properties which can effectively minimize the impact of subjectivity 2 4 development of flood disaster risk evaluation model based on ontology and bn the construction of flood disaster risk evaluation model based on ontology and bn was divided into two steps structural learning and parameter learning so called training which will be detailed in the following sections 2 4 1 structural learning the structural learning was carried out based on ontology bn structure graph was established based on ontology to clearly demonstrate the system variables and their mutual relationships which is one of the highlights to establish flood disaster risk evaluation model first of all the selected factors influencing flood disaster such as rainfall elevation slope population road and so on were considered as the nodes of bn structure graph according to their properties the influencing factors were generalized and classified into several classes climate rainfall and duration topography elevation and slope hydrology river density and river proximity land cover impervious area ratio and socioeconomic population density road density drainage pipe and per unit gdp the relationships between various factors influencing flood disaster were then determined and defined based on classification and their properties there are many relationships in ontology but according to the characteristic of probabilistic inferences of bn model and the aim of flood disaster risk evaluation the parent and child cause and effect are the most important relationships to be considered as follows 1 parent and child which can be represented using is a in ontology classes of factors would be organized in a specific hierarchy using this relationship for example the disaster formative environment consists of topography hydrology and land cover which can be further classed into different factors as shown in fig 7 a where initial node of the arrow expresses the class of parent while the pointing node expresses the class of child 2 cause and effect which also called causal relationship it is expressed that the occurrence of one thing leads to the occurrence of another for example heavy rainfall causes flood disaster this relationship can be shown in fig 7b where the initial node of the arrow is the cause while the pointing node is the effect 2 4 2 parameter learning since bn generally deals with discrete probabilities each node was classified into a finite set of state values accompanying with a probability han and coulibaly 2017 for this reason it is necessary to discretize all relevant data which is the first and important procedure in parameter learning the factors affecting flood disaster were grouped into 4 grades as summarized in table 1 where the accumulated rainfall rainfall duration elevation slope river density and proximity were classified according to recent works chen et al 2015 yoon et al 2015 lin et al 2017 while the impervious area ratio road density population density pipe density and per unit gdp were divided through the statistics of land population and economy and historical disasters in case study given the developed bn structure graph certain learning algorithms such as bayesian estimation and maximum likelihood estimation mle can be used for parameter learning through training sample data to determine the conditional probability distribution among related variables vogel et al 2014 the mle is generally used for large data size and the estimated parameters coincide well with the actual state in this research many data related to rainstorms and floods could be gathered from various sources so the mle is more suitable for parameter learning this method aims to find the parameters that maximize the likelihood function and the working principle is as follows it is assumed that the observed data set d y 1 y 2 y n is independent and uniformly distributed and the likelihood function of d is functional to the model parameters which can be computed as 3 p d θ s s h i 1 n p y i θ s s h where θ s represents an unknown parameter s h refers to the bn structure graph since the observed data set d has been determined the parameters can be obtained by maximizing the likelihood function in parameter learning the updated probability for the m number of mutually exclusive variables or parameters v i i 1 2 m and given evidence or data d were determined by the following relationship 4 p v j d p d v j p v j i 1 m p d v i p v i where p v i d is the conditional probability of v based on the data or evidence d p v j refers to the prior probability p d v j denotes the conditional probability 2 5 effect evaluation the ontology and bn based flood disaster risk evaluation model was developed using the historical case data during the period 2010 2018 and the flood disaster risk in zhengzhou city from august 16 to 20 2019 was predicted and evaluated in this model the state of one or more parent nodes is changed and changes of probability distributions in child nodes are examined for instance in one scenario if it was low land the biggest amount of rainfall and duration a lot of impervious area and residents it was expected that the extent of damage to the physical environment with regard to the variable used in the proposed model was increased significantly and the probability value was given by this model the probability of flood disaster in other scenarios were also obtained through the same way based on the predicted probability of flood disaster the flood disaster risk was divided into 5 categories very low low moderate high and very high and their corresponding probabilities were presented in table 2 respectively the spatial buffer technique of arcgis was employed to analyze the spatial distribution characteristics of flood disaster risk the number of grid cells per risk class for predicted and actual conditions were counted using arcgis 10 3 the performance of model was quantified by analyzing relative error of prediction which is defined as follows 5 re y pre y act y act 100 where re refers to the relative error of prediction y pre and y act are respectively the number of cell grids for predicted and actual state in this paper zhengzhou city has been classified into 23 844 grid cells except for zones near the city s boundary all grid cells have an area of 0 04 square kilometers 3 results and discussions 3 1 construction of flood disaster risk evaluation model based on ontology and bn flood disaster risk evaluation model in case study was established based on the methodologies proposed above as shown in fig 8 where the nodes the relationships between them and their corresponding states were clearly presented from this figure the model for assessing flood disaster were divided into four groups of factors which were expressed by different colors respectively the white node expressed flood disaster where the level of flood disaster risk was classified according to the probability of flood occurrence the factors related to disaster drivers were expressed by blue while the influencing factors related to disaster formative environment were represented by red and yellow nodes represented disaster bearers which included pipe density population density road density and per unit gdp the states of these nodes were given in the squares e g the node elevation has different altitudes such as low moderate high or serious in each square the bars represent the probabilities of the state expressed as percentage combined with the classification of different factors table 1 the meaning of each probability was indicated taking the node rainfall as an example 2 of the accumulated rainfall are below 50 mm that is represented using the state of low moderate indicates that the rainfall is between 51 and 100 mm with a probability of 49 high means that accumulated rainfall ranges from 101 to 150 mm and its probability is 41 and serious says that the probability of being above 150 mm is 8 the state of the rest of the nodes could also be defined in this way it was shown that the remaining 11 factors directly or indirectly affected the occurrence of flood disaster the connected nodes have causal relationship among which the initial node of the arrow is the cause while the pointing node is the effect this relationship was expressed by the conditional probability calculated based on historical data eq 4 which can be queried by attributes of each node the conditional probabilities distribution of node pipe density was shown in table s2 in supplementary materials which allowed several queries of conditional probabilities in different states 3 2 prediction of urban flood disaster risk using the developed flood disaster risk evaluation model under the specific 2010 2018 conditions the expected values of probability of flood disaster risk for the entire 23 844 grid cells were predicted and summarized in table 3 for instance 2763 grid cells have 0 8 probability of having flood disaster risk of very high the summarized results of table 3 emphasized the changes between risk classes aggregated per cell which can be a good quantitative measure of the performance of the proposed model in this table the number of cells corresponding to different levels of risk for actual flood disasters conditions was obtained from historical case information while the number of grids for prediction was obtained from the proposed model it can be seen from this table that the most grids recorded a moderate to high risk to flood disaster while few cells were at very low risk further analysis was shown in fig 9 in fig 9 a flood disaster risk map was delineated as an output from the proposed risk evaluation model combined with the relevant data in table 3 the percentage of each risk class in the study area was calculated about 6 5 of the total study area was classified as very low risk and 21 2 was low risk suggesting that these regions were less likely to suffer from floods whereas 26 8 grids were found to have high risk plus another 11 6 were very high risk this means that 72 3 of the study area was characterized by very high to moderate flood disaster risk which explained reasons behind the recurrent flood events in the zhengzhou city from fig 9 these areas were located in the eastern and central region of zhengzhou these zones have elevation ranging from 0 to 100 m high population density with more than 4000 people per kilometers high intensity rainfall and consist mainly of built up areas such as residential district business district therefore when an intense rain event occurs the highest surface runoff is observed in this areas due to the increase in impervious surface which decreases the soil infiltration loss factors such as prolonged rainfall density and topography of these areas can contribute to increase flood disaster risk it is suggested that the importance of increased urbanization is evident in recurrent flood disaster events southwest areas belong to very low and low risk zones partly because they have high elevation and less rainfall the northwest parts of zhengzhou are new city zones with low density of property and population which also records low flood disaster risk overall it can be recognized in fig 9 that the flood disaster risk of zhengzhou city is decreasing from the middle to the surroundings which was basically consistent with the previous studies in this area lin et al 2017 wu et al 2019a according to this results strategies to cope with and prevent flood disaster should be proposed to reduce losses for different regions in very low to low risk areas risk reduction should be targeted towards individual protection especially the elderly and children while for the high and very high risks the measures are not restricted to the reduction of the flood disaster by forecasting the occurrence of rainfall but also include the reduction of risk by changing the receptors e g new land use or improved management allowing the application of multi disciplinary approaches for example the impervious areas should be decreased by promoting green development like green roof constructions encouraging the use of pours pavement materials that allow water infiltration and increasing proportion of green areas 3 3 model performance the result of flood disaster risk evaluation was validated by comparing with the actual historical flood records based on the statistical results eq 5 was used to calculate the relative error of this method and the calculation results were presented at line 3 of table 3 the results revealed quite good agreement between the predicted flood disaster risk with actual flood disaster according to the comparison of relative errors the model has the smallest error in the prediction of very low risk whose relative error is about 4 4 followed by low and moderate risks the relative error of very high risk is the highest at 12 2 it can be concluded that compared with low and very low risks the prediction of high and very high risks is poor the formation of flood disaster is very complicated and the flood disaster risk was affected by many factors but the influencing factors considered in this study were not comprehensive enough which may result in poor prediction in areas with very high and high risks moreover the map of actual flood disaster risk obtained from historical case was compared with the distribution of flood disaster risk predicted using the proposed model the actual risk map obtained from historical disaster case was shown in fig 10 the modeling results conform basically and reasonably to the actual disaster with almost the same risk distribution within the area delineated as low and very low risks in the proposed model southwestern and northwestern parts of the city with low urbanization rate and population density are at low and very low risk in historical flood records 10 mismatched pixels were found in very high and high risk areas which are mainly spread in the middle of zhengzhou the scope of high to very high risk areas was underestimated it is indicated that the prediction accuracy of high risk needs to be further improved it can be observed that although there are still certain mismatches between predicted and actual conditions the developed flood disaster risk evaluation model can predict the general distributed trends of flood disaster risk which provides a new way for flood disaster risk assessment 3 4 sensitivity analysis because many factors are being produced with different sets of input variables the proposed model based on ontology and bn can learn and record the relative importance of the input variables in predicting the output sensitivity analysis was applied to find variables that strongly affect the behavior of a system and to determine variables that were not very sensitive to changes in the process of assessing flood disaster risk the results of sensitivity analysis were presented in fig 11 where the sensitivity was demonstrated using the depth of node color and the sensitivity value of each factor influencing flood disaster risk was exhibited in table 4 according to the sensitivity values of different factors rainfall and duration are the most impactful factors followed by population density road density and per unit gdp which affects the possibility of flood disasters to some degree it should be noted that although the sensitivity value of some factors is low it does not mean these are not vital elements in pluvial flood which is particularly evident for elevation slope pipe density and river network several reasons can be used to explain this zhengzhou city is characterized as lowland and there are little changes in elevation and slope the elevation and slope have direct effect on trend of most rivers therefore the factors related to disaster formative environment have lower sensitivity value partly due to their almost uniform across the case study which have an unnoticeable impact on the flood disaster it is also interesting to notice that the pipe density has low sensitivity values which indicates that its influence on flood disaster is not as visible as others there are few drainage pipes in zhengzhou city and the pipe density with in different regions has little difference which have less impact on the evaluation result according to the sensitivity analysis it is necessary to strengthen the rainstorm forecast in the future to prevent the occurrence of disasters in advance this can be achieved by employing more advanced methods for forecasting rainfall encouraging the application of new technology such as radar and remote sensing to improve forecast accuracy and lead time and so on in addition it is necessary to enhance business and public awareness of the possible damage they may suffer and of the total damage a flood can cause and improve their adaptability to flood disaster improving the urban land use situation and reducing the impervious area are also key measures to avoid major costs in case of flood disaster 3 5 advantages and limitations over the past few years we have been dedicated to flood disaster risk analysis in zhengzhou city for example lin et al 2017 proposed a flood disaster vulnerability evaluation method based on ahp in zhengzhou however the ahp was characterized by subjectivity because the index weight was determined based on experts knowledge besides only six indices were selected to assess flood disaster vulnerability in this study and failing to make full use of objective data was another limitation of this research in order to takes advantage of objective data and reduce the uncertainty caused by the determination of index weight the bn model was proposed by wu et al 2019b to assess flood disaster risk the results show that the flood disaster risk in central and eastern area appears to be high which was in agreement with the distribution of risk obtained from the present study regarding risk some factors such as topography river network road density and so on were selected as indicators but they seem to be correlated with other indicators and mutual indicators however there were less relationships among various factors in the bn model developed by wu et al 2019b and some of potential relationships were also defined by experiences which had some subjectivity besides the development situation of drainage facilities is important for flooding but it has not been considered at all in wu et al 2019b in order to fully determine the relationships between various factors the ontology was introduced in this work combining ontology with bn the proposed model can not only objectively define the potential relationships among factors based on their properties in ontology but make full use of historical disaster data which reduces the subjectivity and uncertainty and as a result the results of flood disaster assessment are more accurate and reliable than previous methods although it is expected that the methodologies adopted in this paper would better predict such flood events based on ontology and bn there are still many problems in the research the data processing steps have reduced the quality of the analysis a lot of data were obtained using interpolation technology in arcgis such as population density per unit gdp besides the influencing factors considered in this research were not comprehensive due to the data availability therefore how to obtain high quality data and constantly improve the evaluation index system is the focus of future study 4 conclusions in this study a model for the assessment of flood disaster risk under extreme climates was developed based on the ontology and bn the potential relationships between factors influencing flood disaster were determined based on ontology by mining their properties and the probability distribution table was developed using bn based on a lot of historical disaster data that were collected and processed using gis the flood disaster risk in zhengzhou city was evaluated based on this model the case study revealed that the relative error of very low low moderate high and very high risks predicted by the proposed model were 4 4 10 1 9 3 11 3 and 12 2 respectively based on the spatial distribution of different risk levels it can be recognized that the flood disaster risk of zhengzhou city was decreasing from the middle to the surroundings and the sensitivity analysis showed that disaster drivers and disaster bearers were the most influential factors therefore the rainstorm forecast of zhengzhou should be strengthened to prevent the occurrence of disasters in advance and the resilience and adaptability of city should also be improved this can be achieved by increasing proportion of green areas laying underground pipeline improving the ability of residents to cope with disasters through propaganda and education and so on compared with previous methods where the weights of different factors were determined through experiences and knowledge to assess flood disaster risk deterministically the main innovation of the proposed model based on ontology and bn is that it can take advantage of many historical data to predict the probability of the flood disaster quantitatively and has the ability to quantify uncertainty in the decision making process in other urban areas based on its characteristic and data availability the proposed model can be revised and adopted which provides a new method for flood disaster risk assessment in other areas declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the study is funded by the key project of national natural science foundation of china no 51739009 the authors thank the anonymous reviewers for their valuable comments appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2020 124596 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
5731,in flood forecasting and design for peak flows understanding and characterizing the hydrologic response to rainfall events is vitally important one of the key parameters utilized to characterize the catchment response time is the time to peak tp which represents the net rise time of a storm hydrograph or the time from when a precipitation event begins to contribute to stream discharge to the time that peak flow qp is reached previously influencing factors on tp have been static in nature with no consideration of the variability in tp due to size of the storm event and the antecedent moisture conditions of the watershed seasonal effects using 1400 storm event observations and the corresponding catchment characteristics of 153 stream gauges across the united kingdom uk the importance of different factors on estimating tp are evaluated these data points span three decades and this breadth of temporal data allowed meaningful annual trends to be observed and seasonal variations in soil moisture to be identified and applied a new wetness coefficient is applied herein to reflect the antecedent conditions within a catchment the qp is selected as a dynamic variable utilized to represent the magnitude of a given storm due to the demonstrated correlation with tp an explicit equation based on gene expression programming is designed which accounts for the dynamic nature of tp through qp and seasonal moisture effects the results of the proposed model are compared to the results of the existing equation for tp prediction in the uk outlined by the flood estimation handbook feh the proposed equation with nash sutcliffe coefficient r2 and rmse values equal to 0 60 0 66 and 3 64 respectively has improved characteristics compared with the traditional feh equation nash 0 42 r2 0 54 and rmse 4 37 a forensic analysis of the contributing factors for tp involves development of an empirical model with improved prediction accuracy by accounting for the dynamic inputs improving previous models both statistically as well as in the hydrologic understanding of the catchment response keywords catchment response time time to peak peak flow soil moisture gep nomenclature 95 cbi 95 confidence bound interval da contributing drainage area catchment area km2 dplbar mean of distances between each node on the uk institute of hydrology digital terrain model ihdtm grid and the catchment outlet in kilometres used to characterise catchment size and configuration https nrfa ceh ac uk feh catchment descriptors dpsbar this landform descriptor mean drainage path slope provides an index of overall catchment steepness it was developed for the flood estimation handbook and is calculated as the mean of all inter nodal slopes derived using the ihdtm for the catchment the index is expressed in metres per kilometre with values ranging from 300 in mountainous terrain to 25 in the flattest parts of the country https nrfa ceh ac uk feh catchment descriptors ei calculated error see eq 3 et expression tree feh uk flood estimation handbook fsr uk flood studies report 1975 ga genetic algorithm gep gene expression programming gp genetic programming l length dplbar mean drainage path length index km calculated using the mean drainage path length from each node of the uk institute of hydrology digital terrain model ihdtm https nrfa ceh ac uk feh catchment descriptors l s topographic ratio dplbar divided by the square root of dpsbar lc land cover classification or usage lcm land cover moisture factor ratio of mean smd of seasonal and lc group and the mean of the entire data set mpe mean prediction error see eq 4 nash nash sutcliffe coefficient nash and sutcliffe 1970 nrfa national river flow archive pdsa partial derivative sensitivity analysis propwet dry soils tend to inhibit flood formation whilst in contrast saturated soil conditions precede and contribute to many large flood events this catchment wetness index proportion of time soils are wet developed for the flood estimation handbook provides a measure of the proportion of time that catchment soils are defined as wet in this context when soil moisture deficits are less than 6 mm propwet values range from over 80 in the wettest catchments to less than 20 in the driest parts of the country proportion of time when the smd was less than or equal to 6 mm during the period 1961 to 1990 https nrfa ceh ac uk feh catchment descriptors qp peak flow of storm m3 s qp da normalized peak flow of storm by catchment size m3 s km2 r2 determination coefficient refh revitalised flood hydrograph s slope dpsbar mean drainage path slope index m km smd soil moisture deficit capacity of the soil to absorb water field capacity has zero smd i e saturated the model used in the feh allows a maximum value of 125 mm std standard deviation see eq 5 tc time of concentration hours tl lag time hours tp time to peak hours uk united kingdom urb 1 urbext1990 urbext1990 extent of urban and suburban land cover in 1990 vsa variable source area also referred to as variable contributing area wcb width of the confidence bound see eq 6 wsm wilson score method of uncertainty analysis 1 introduction in hydrologic and hydraulic modelling understanding and characterizing the response of a catchment to a rainfall event is of vital importance this knowledge is required for proper watershed management and to protect the public from flood hazards usda and nrcs 2010 the catchment response is characterized by time parameters which reflect the speed of response of a catchment area to a storm event pavlovic and moglen 2008 perdikaris et al 2018 sharifi and hosseini 2011 the most commonly used catchment response time parameters are time to peak tp time of concentration tc and lag time tl usda and nrcs 2010 tp is defined as the rise time of a storm hydrograph encompassing the time from the first stream contributions from a precipitation event to the arrival of the peak flow qp of the event granato 2012 kirpich 1940 mccuen et al 1984 usda and nrcs 2010 ramser 1927 kirpich 1940 gray 1961 wu 1963 bell and om kar 1969 fang et al 2005 walton et al 2019 this is consistent with multi peak flood events where tp can be defined as the total net rise of the hydrograph gericke and smithers 2017 tp can be directly measured from a storm event which makes it immediately applicable as well as beneficial for comparison purposes when developing a prediction method tc is another frequently used time parameter defined similarly to tp where the definitions are often mistakenly used interchangeably a regularly applied definition for tc is the time for water to travel from the most hydraulically distant point to the outlet of a catchment nrcs 1986 despite the various applications of tc in water management and infrastructure development kuichling 1889 nrcs 1986 gazendam et al 2016 the parameter lacks a commonly accepted definition and cannot be observed or measured explicitly therefore a true value is always unknown fang et al 2007 sharifi and hosseini 2011 perdikaris et al 2018 tl has been defined as the time a water parcel takes to travel from one location in a basin to another location downstream or the time delay between the peak precipitation and the peak discharge in an event seyam and othman 2014 hence tl corresponds with tc and often the definitions are similar which makes the differences between the two parameters confusing due to the wide variety of equations to calculate tl the value is highly dependent upon the formula used nrcs staff 2010 costache 2014 nagy et al 2016 these parameters are vital in characterizing catchment response time and are widely utilized a key component of sustainable water resource management and watershed planning is the ability to accurately predict stream flows vaze et al 2011 salimi et al 2017 catchment response parameters are directly applied in the design and operation of hydraulic structures pegram and parak 2004 atieh et al 2015 bonakdari et al 2019 and provide an understanding of potential flood and failure hazards through the temporal distribution of runoff gericke and smithers 2014 atieh et al 2017 specifically estimation of peak discharge water levels design storm duration and runoff are directly reliant upon the accuracy of predictions of catchment response time parameters kuichling 1889 gericke and smithers 2014 nagy et al 2016 salimi et al 2017 fang et al 2008 demonstrated that stream flow models are limited by timing errors and seyam and othman 2014 showed that model performance shows significant improvement with increased accuracy of catchment response timing in predicting design peak flows in ungauged catchments errors in time parameters have been reported to contribute up to 75 of the total prediction error bondelid et al 1982 fang et al 2008 gericke and smithers 2016 the most common application of time parameters is in the construction of the design hydrograph for a given catchment with an observed or design qp value and tp a unit hydrograph can be constructed for any location on a regularly shaped watershed usda and nrcs 2010 further modified hydrograph versions such as the revitalised flood hydrograph refh model proposed by the flood estimation handbook feh introduce more flexible shapes but are still dependent solely upon tp and qp kjeldsen 2007 numerous studies have evaluated and compared the various time parameter prediction equations and methods e g askew 1970 wong 2005 fang et al 2008 2007 mccuen 2009 grimaldi et al 2012 gericke and smithers 2014 2016 2017 nagy et al 2016 salimi et al 2017 perdikaris et al 2018 chen et al 2019 studies evaluating tc prediction methods are most common as it is the most frequently applied time parameter nagy et al 2016 however tp can be considered a more relevant response parameter as it can be directly measured from a storm event and has a commonly accepted definition fang et al 2007 concluded there is no single correct value of tc for a given watershed while mccuen 2009 demonstrated that tc calculated for the principal flow path does not accurately reflect temporal responses of the entire watershed due to the range of roughness flow depth slope and runoff timing across a catchment area in the uk the flood studies report fsr 1975 and the subsequent flood estimation handbook feh kjeldsen 2007 provide regression equations based on different input variables to calculate tp values in uk watersheds this empirical equation is currently the accepted method of predicting tp for streams in the uk kjeldsen 2007 it is of note that each of the input parameters for this and other available methods are static in nature and depend upon catchment characteristics e g drainage area da length l and slope s of a catchment and therefore will predict a single tp value for any given storm within a catchment at any time these methods do not account for variations between storm sizes or the antecedent conditions of the catchment the main objective of this study is to determine features which influence fluctuations in tp a forensic analysis of the hydrologic processes that influence catchment response time is developed to identify the controlling factors of tp the timing of peak flows is subject to dynamic variability this study aims to pinpoint key characteristics influencing the sources of the dynamic variability and determine a way to capture them in prediction accordingly this study proposes that tp is dynamic in nature and the fluctuations are dependent on the magnitude of the storm and the antecedent watershed conditions as well as the established static catchment characteristics in this paper an extensive data series is compiled where tp has been measured for 1412 storm events at 153 gauges in the uk and comparisons between predicted and observed values were made tp is chosen as it has been directly measured and therefore conclusions as to influencing variables can be directly drawn in prediction of this catchment response parameter two influential types of dynamic variables storm dependent variables and dynamic catchment variables are considered in addition to the previously applied static variables soil moisture deficit smd has been reported for each of the storms and gauges in the collected data set this study investigates alternative methods to incorporate the impact of antecedent soil moisture conditions without requiring a direct observation by using seasonal trends to describe the dynamic seasonal variability of tp this study builds upon the tp research presented in the fsr feh by providing a more comprehensive evaluation of the available research and tools with application to the large data set further the static prediction nature of current methods is described and improved upon subsequently a practical simple and robust equation based on gene expression programming for prediction of the tp parameter for the uk is introduced furthermore the feh equation as a conventional model in tp prediction is used to evaluate and compare with the proposed equation in this study with application to a large data set 2 methodology 2 1 data collection the uk has a large network of stream gauges maintained by the national river flow archive nrfa and the uk institute of hydrology approximately 1500 stream gauges are currently monitored in the uk through this program marsh and hannaford 2008 from these gauging stations a long term data set with a diverse set of measured and observed hydrologic parameters was obtained for use in this study the data set was originally developed for the uk winfap software which provides analyses based upon the feh leman et al 2009 these flood data and statistical procedures for estimating flood frequency are outlined in the third volume of the feh robinson and duncan 1999 the catchment characteristics from each stream gauge were combined with the set of storm event data to create one encompassing data set of the 1500 stations 153 stations have an entire set of 43 measured parameters corresponding to 1412 storms these data provided valuable information for developing a tp prediction method fig 1 demonstrates the location of each stream gauge used in this study further information on the collected data can be found in the nomenclature list the following sections detailing the influential variables considered in this study and the flood estimation handbook 2 2 flood estimation handbook the 1975 uk fsr and the subsequent feh kjeldsen 2007 are comprehensive guides to understanding flood prediction in the uk they provide the method to develop the previously mentioned refh hydrograph and corresponding regression equation for calculation of tp the regression equation is 1 tp 1 56 p r o p w e t 1 09 d p l b a r 0 60 1 u r b e x t 1990 3 34 d p s b a r 0 28 where propwet proportion of time when the soil moisture deficit smd was less than or equal to 6 mm during the period 1961 to 1990 bayliss 1999 dplbar feh mean drainage path length index km urbext1990 extent of urban and suburban land cover in 1990 and dpsbar feh mean drainage path slope index m km empirical eq 1 is currently the accepted method utilized for predicting tp for streams in the uk it is of note that each of these input parameters are static in nature and therefore will predict a single tp value for any given storm within a catchment at any time 2 3 influential variables in prediction of tp the first step in developing a suitable prediction method for tp was to identify a key set of variables to incorporate utilizing data and literature analyses the data set was narrowed from the available 43 variables to apply only the key inputs based on the developed hypothesis three classifications of variable groups were identified static catchment descriptors dynamic catchment descriptors and storm dependent variables in this research two dynamic characteristic classifications are used to predict tp in addition to static characteristics which are used in most previous studies askew 1970 fang et al 2008 2007 gericke and smithers 2016 2014 grimaldi et al 2012 haktanir and sezen 1990 mccuen 2009 nagy et al 2016 salimi et al 2017 wong 2005 these variables provide a comprehensive understanding of the nature of tp prediction and the catchment response time table 1 demonstrates the range of data compiled and analysed in this research 2 3 1 static catchment variables static catchment descriptors are variables that identify key characteristics of a given catchment and remain constant static catchment variables have been the primary basis of previous studies and prediction methods and have been demonstrated to have an influence on tp this group includes the flow path length l average flow path or catchment slope s catchment drainage area da and land cover classification lc including the percentage of urban area urbext in the feh propwet is utilized to identify the variations in general wetness between catchments by accounting for the frequency of low soil moisture deficit within a given catchment 2 3 1 1 length and slope the lengths of the catchment area or main channel have been shown by years of research to have a strong influence on catchment response time askew 1970 fang et al 2008 2007 gericke and smithers 2016 2014 grimaldi et al 2012 haktanir and sezen 1990 mccuen 2009 nagy et al 2016 salimi et al 2017 wong 2005 reviewers of temporal response parameters have identified length as the most prevalent variable in prediction the slope of the channel or catchment is the next most predominant prediction variable frequently l and s are used in conjunction when predicting tp or tc as l s costache 2014 fang et al 2007 kc and fang 2015 kirpich 1940 kjeldsen 2007 nagy et al 2016 simas and hawkins 1998 based on review of literature and previous empirical equations for prediction of catchment response time parameters a common tendency in prediction methods is to utilize the ratio l s rather than applying each parameter individually to the final model this ratio is utilized and provides a factor which accounts for these static catchment characteristics creating a ratio which encompasses the topography of the catchment area this key relationship provides new insights into the relationship between these parameters l s and the catchment response and is applied to the model in conjunction with da to represent the static catchment conditions the dpsbar and dplbar used in this analysis are not the slope or length themselves but indices created by the nrfa and the feh the values are the means of the entire catchment and characterize the catchment in relative terms 2 3 1 2 drainage area da or catchment area is a similarly static catchment descriptor of the watershed size while the inclusion of both l and da simultaneously may seem redundant as both variables represent the relative size of the catchment numerous studies have demonstrated the necessity for da in catchment response prediction williams 1922 simas and hawkins 1998 pegram and parak 2004 gericke and smithers 2017 pegram and parak 2004 determined catchment area as the most significant geomorphological catchment variable in flood prediction further salimi et al 2017 found that catchment response time parameter models that did not include da overestimated qp while those which include da had improved accuracy nester et al 2011 found that catchment size was the most important control when assessing regional flood simulations while fang et al 2007 found a direct positive correlation between da and tc tp has been demonstrated to increase with the da and as such catchment area has an influence on the magnitude of tp gericke and smithers 2014 kirpich 1940 mccuen et al 1984 pegram and parak 2004 schmidt and schulze 1984 simas 1996 2 3 1 3 propwet propwet is defined as the proportion of time when the soil moisture deficit smd was equal to or less than 6 mm during the period 1961 to 1990 bayliss 1999 this variable is intended to account for the historical wetness of a catchment by giving the percentage of time catchment soils are saturated this encompasses the tendency of the catchment to have wet soils and correspondingly the tendency to produce runoff quickly in a storm event ahmed et al 2013 jahanfar et al 2018 the feh equation utilizes propwet directly however due to the specificity of this variable this research is proposing grouping catchment as historically wet average or dry and where wet catchments have a propwet value of at least 0 6 and dry catchments have a propwet value of less than 0 4 2 3 2 storm dependent variables storm dependent variables are those which encompass the variability between storm events these parameters encompass the magnitude of a storm and provide dynamic variability within a catchment discharge and precipitation are representative of an individual storm event and variables characterizing them were considered to incorporate dynamic variability stemming from the storm size qp is identified in this study as the key variable to encompass the magnitude of the specific storm event incorporating the qp value in calculation of tp allows for the magnitude of the storm to be considered while reducing the need for extensive convolution processes or rainfall or runoff data for hydrograph creation gericke and smithers 2017 grimaldi et al 2012 found that a significant variability of time estimation parameter tc was attributed to qp variations and suggested that prediction methods could be significantly improved by utilizing the channel discharge utilizing the peak flow itself rather than precipitation data accounts for the amount of rain that directly turns to runoff and impacts the stream tp removing the variability in evapotranspiration and infiltration losses as such qp is a better representation of storm size and is a more applicable variable given its application in a number of models to route or forecast flood flows a strong negative correlation between tp and qp has been reported where increasing qp corresponded to a reduction in tp parallel to the results obtained by costache 2014 and nagy et al 2016 the peak flow can be normalized by the size of the catchment through division by da creating a ratio of qp da for the relative storm size this ratio is plotted against observed tp in fig 2 demonstrating the notable negative correlation between normalized qp relative storm size and tp which encapsulates a degree of predictive capabilities by applying this ratio almost all data points are centered on the fitted line which represents the acceptable and significant correlation of tp with qp grimaldi et al 2012 as such qp has been identified as having a correlation with the catchment response and is applied as the storm specific parameter to the final predictive model for tp estimation 2 3 3 dynamic catchment variables while descriptors which demonstrate the variances between catchments are important to understanding the correspondingly varying response time there can also be variances introduced within a catchment due to the antecedent conditions or season of occurrence dynamic catchment variables encompass these antecedent conditions which change between each event within a specific catchment soil moisture is a key descriptor of the antecedent catchment conditions and directly impacts the infiltration and runoff of a catchment soil moisture deficit represents the capacity of the soil to absorb water measured in millimetres of soil depth where smd of 0 mm indicates saturated conditions and above zero indicates an increased capacity to absorb water smd has been reported for each of the storms and gauges in the collected data set however in a review of past studies smd was not identified as a predictor variable this may be due to the nature of the variable the storm specific soil moisture can be measured directly but is tedious and time consuming this study investigates alternative methods to incorporate the impact of antecedent soil moisture without requiring a direct observation by using seasonal trends to describe the dynamic seasonal variability of tp in this research the influence of seasonal effects upon antecedent conditions was considered precipitation often varies between seasons this seasonal variation inflicts seasonal patterns on other hydrologic variables such as soil moisture magnitude of peak flows base flow etc indicating tp experiences similar seasonal trends to explore this trend fig 3 displays the average monthly variation in tp and smd at the uk gauging stations in general it can be seen that as tp increases smd declines while this is a generalization and there is complexity implicit in tp this adds strength to the idea that seasonal factors affect tp mean smd reaches a peak in july followed by a sharp decline until december the opposite trend is noted from january to july there is a notable opposite relationship between smd and tp hypothetically due to the contributing or variable source area vsa as the soil becomes increasingly saturated less precipitation is infiltrated thus creating runoff and a greater portion of the catchment contributes to the stream discharge dunne and black 1970 lim 2016 the wetness of a catchment directly affects the size and time of response in a precipitation event due to the dynamic contributing area the response is faster when only the river network contributes and slower when the entire drainage area contributes in saturated conditions beven and kirkby 1979 hewlett and hibbert 1967 raju and nandagiri 2018 correspondingly this increases the tp as the runoff from the farthest points of the catchment must connect with the stream before tp is reached rather than simply the contributions from the area immediately surrounding the stream accounting for contributions from the full da this is consistent with the average smd values peaking in spring using the smd trends demonstrated in fig 3 three seasons were identified wet average and dry these seasons are identified in fig 3 and summarized in supplemental files table 2 the wet season corresponds to the months of january february march and december where smd is on average less than four millimeters indicating saturated soil the average season corresponds to the months of april may october and november where smd is on average 4 and 14 mm indicating moderately saturated soils the dry season consists of june july august and september where smd is 14 mm indicating unsaturated soil conditions it has been established that the soil moisture conditions vary throughout the year allowing wet and dry seasons to be isolated nandintsetseg and shinoda 2011 bollman et al 2013 this allows for a dynamic catchment descriptor to be incorporated based upon the period of the storm s occurrence while maintaining relative ease of prediction with the variation between catchments and between storms within a catchment represented the final consideration to allow for dynamic prediction is encompassing the antecedent moisture conditions of the catchment at the time of the storm event fig 3 demonstrates seasonal trends in soil moisture and correspondingly tp using these trends in mean smd and the three soil moisture seasons wet dry and average identified this allows for the antecedent moisture conditions of the catchment to be accounted for without the necessity for soil moisture measurements this is the first time seasonal effects and antecedent conditions are applied to catchment response time parameter prediction 2 4 model development 2 4 1 gene expression programming gep gep methods provide a prediction expression for a desired output variable using corresponding input variables through the application of an evolutionary phenotype genotype genetic algorithm using genes organized head to tail to build chromosomes and was initially introduced by ferreira 2001 this method builds upon genetic algorithm ga and genetic programming gp methods optimally applying linear fixed length chromosomes as utilized in ga and expression trees ets of various shapes and sizes as applied in gp koza 1992 khozani et al 2017 power et al 2019 this coordination of methodologies provides the benefits from the original models without the corresponding disadvantages of the original models allowing gep to surpass the capabilities of the original techniques the basic components of gep algorithms encompass the control fitness function function set terminal set and terminal condition parameters gholami et al 2018 milukow et al 2019 a fixed length symbol set of terminal set and function set respectively as z l m n 1 or tan cos alavi et al 2013 comprise each gene in gep model alavi et al 2013 an initial population of chromosomes is generated and expressed as computer programs which are then assessed using a process of fitness functions to select the best performing program these chromosomes are then reproduced to create a new generation of chromosomes different genetic operators of gep model are used to modify the chosen chromosomes the adjustable parameters used in the gep model proposed in this paper are provided in the supplemental files supplemental file table 3 the gep model is evolutionary in nature increasing in accuracy with each generation the process is reiterated until the termination criterion is reached azimi et al 2017 gholami et al 2018 a representative flowchart of the gep model presented in this paper can be found in the supplemental files supplemental file fig 1 2 4 2 development of gep model while this paper is primarily a forensic analysis of the influencing factors behind catchment response time a simplified model incorporating this knowledge is prepared as an improvement to the original feh equation utilizing the previous literature review statistical analyses and machine learning key input variables can be identified and utilized to create an improved empirical equation for prediction of tp a cross correlation table of all 43 variables collected and compiled for this study was prepared demonstrating the relationships between variables supplemental files table 1 this allowed for those most influential to tp to be identified as well as grouped to remove redundancies in variable use similarly machine learning techniques were employed to assess the degree of impact of different input variables upon tp variability and prediction ability using these methods a set of variables was produced for use in gep to generate an empirical prediction model gep provides the opportunity to apply machine learning and evolutionary computation techniques while monitoring and controlling the complexity and length of the final prediction equation the data were divided into a 2 3 training 939 points 1 3 testing 473 points split then through the use of gep model development an empirical equation was created and compared to previous models identified in the technical literature utilizing these equations and regression analyses simplified empirical models are created as mentioned in section 2 3 1 the parameters of l and s are the most predominant static variables in tp prediction as identified by many researchers wong 2005 mccuen 2009 grimaldi et al 2012 gericke and smithers 2014 2016 nagy et al 2016 salimi et al 2017 furthermore as mentioned the da parameter is another influential parameter in tp estimation gericke and smithers 2017 pegram and parak 2004 simas and hawkins 1998 williams 1922 therefore in designing the gep model l s and da parameters are considered as representative of static variables and qp is provided as the storm dependent variable in order to simplify the equation provided by the gep model only the most effective and significant parameters are considered as input variables furthermore the effect of seasonality and dynamic variables are applied by adjusting the coefficients of gep model s equation by employing optimization methods to the training data set 3 results and discussion the key results of this study deal with identifying and applying key input parameters which can be utilized to provide an optimized model for prediction of tp using machine learning methods in the initial investigation statistical and hydrologic analyses are utilized to determine the correlation of different input parameters with tp and are compared to find the optimal inputs to predict tp in the uk in the second phase a gep empirical model and equation is proposed as an alternative method to the previous traditional feh model furthermore the performance of the traditional feh equation eq 1 with input parameters of l s propwet and urbext1990 is evaluated against the gep model 3 1 proposed gep model a gep model was developed to provide a simple empirical equation for comparable use to the feh equation while introducing the significance of dynamic prediction utilizing gep modelling development an empirical equation was derived and analysed for hydrologic significance and accuracy this model shows a notable improvement from the feh while maintaining ease of application this simplified gep equation is as follows 2 tp c 1 l s c 2 qp da 1 3 2 the final gep model utilizes only four direct input parameters l s applied in the ratio of l s qp and da in the ratio qp da previously referred to as the normalized peak flow however coefficients c1 and c2 encompass the wetness of the catchment at the start time of the storm as identified in fig 4 these c1 and c2 are optimized coefficients prepared by grouping catchments by propwet classification as outlined in section 2 3 1 3 and season as outlined in section 2 3 3 these coefficients consider the previously identified seasons based on smd trends wet in the months of december through march dry in june through september and average in april may october and november this accounts for the seasonal wetness based on the time of occurrence of the event similarly catchment propwet values were utilized to classify historical wetness where wet catchments have a propwet value of at least 0 6 and dry catchments have a propwet value of less than 0 4 this provides nine optimized wetness coefficients for use in the prediction equation the proposed gep model utilizes the previously identified topographic ratio of l s and the normalized storm size of qp da as input parameters reiterating the importance of these parameters this equation independently recognized the l s topographic ratio as statistically significant and identified the negative correlation between tp and the relative storm size as such the proposed gep model demonstrates both statistical and hydrologic insight however this model does not utilize smd measurements directly increasing the opportunities for application while maintaining the importance of seasonality and catchment wetness through the use of coefficients c1 and c2 further the variations in coefficients c1 and c2 demonstrate the significance of antecedent moisture conditions to the catchment response time when the catchment is saturated the first term c1 which represents the topography of the catchment using length and slope is of less importance than when the catchment is dry where the relative size of the storm represented by the ratio of qp da becomes more prevalent when the soil is saturated the relative contribution of each term shifts due to the wetness constants while evolutionary computation models such as gep are often efficient in hydrologic applications it is cautioned that deep understanding of the hydrologic relationships and significance must be enforced and maintained throughout use of gep modelling because the model evolves via statistical accuracy alone without understanding the relationships between parameters zorn and shamseldin 2015 this risk was accounted for through the use of hydrologic and literature analyses of the importance of each parameter and understanding the relationships between the hydrology and the mathematics of the models the proposed gep model includes static catchment descriptors including the previously identified l s ratio storm specific inputs by applying the relative storm size through the ratio of qp da and consideration for the antecedent moisture conditions through the optimized wetness coefficients of c1 and c2 3 2 final gep model the performance of models used in this paper is assessed using different statistical indexes such as root mean square error rmse determination coefficient r2 and nash sutcliffe coefficient nash nash and sutcliffe 1970 testing of the original feh equation on the data set demonstrated that while there is some success using this method there remains a notable degree of variability remaining to be accounted for the proposed gep equation statistically and hydrologically outperforms the existing feh equation where the r2 improves from 0 54 to 0 66 the nash increases from 0 42 to 0 60 and the rmse decreases from 4 37 to 3 64 fig 5 represents the scatter plot of tp values predicted by the feh equation and gep model in comparison with corresponding observed values as seen the gep model maintains simplicity but has an increased r2 value compared to the feh model and as such has a better correlation with the corresponding observed values furthermore the compaction of data points predicted by the gep model around the exact line y x is more than feh equation similarly the fitted line on gep model is closer to the exact line than the fitted line on feh equation this graph demonstrates the improved capability of the proposed gep model over the traditional feh equation further table 2 summarizes the r2 nash and rmse values for the feh equation and the gep model as an entire set as well as the training and testing data sets for better evaluation according to statistical indices in table 2 it can be seen that the proposed gep model shows improvement over the original feh equation overall the ease and breadth of application increased accuracy from the original feh equation and the inclusion of a storm specific input and a seasonal descriptor which encompasses the antecedent conditions at the time of the storm make this equation superior to the feh model fig 5 is the regression plot of tp estimated by feh and the final gep model in comparison with corresponding observed values as seen for the gep model the fitted line is in the vicinity of the exact line which represents the improved correlation of data points estimated by the gep model in comparison to the feh method further in the feh model stratification can be seen to occur where the same constant value is predicted for multiple storms within the same catchment this issue is nullified by the inclusion of dynamic variables in the presented model fig 5 and table 2 demonstrate that the gep model shows notable improvements in predictive capabilities from the original feh equation the final gep model was created using a 2 3 training 1 3 testing split of the collected 1412 storm events spanning more than three decades and the expanse of the uk further it encompasses the dynamic variability between storms and greatly improves the ability of prediction compared to the original feh equation the results of the literature review and understanding of hydrologic significance of each variable s importance as explored in previous sections were supported by statistical and graphical analyses of the models results the final model utilizes optimized wetness coefficients to quantify the antecedent conditions the qp to represent the magnitude of the storm and traditional static catchment descriptors including l s and da 3 3 uncertainty analysis the uncertainty of the original feh and the proposed ann and gep models were evaluated using the wilson score method wsm newcombe 1998 gholami et al 2019 through calculation of four indices of uncertainty mean prediction error mpe standard deviation std the width of the confidence bound wcb and the 95 confidence bound interval 95 cbi in calculating the mpe the basis of this method focuses upon calculating the error ei of each tp value estimated by the model and the corresponding observed values then finding the average of this value for all 1412 data points as according to eqs 3 and 4 3 e i t p i obs t p i model 4 mpe 1 n i 1 n t p i o b s t p i mod e l furthermore the standard deviation std of data points is calculated according to 5 s t d i 1 n e r r o r i error 2 n 1 1 2 subsequently the confidence bound cb is used in calculating the wcb and 95 cbi using eq 6 6 wcb t s t d n 1 2 where t is the left tailed inverse of the t distribution of error as calculated by the excel function t inv berry and armitage 1995 the smallest wcb represents the lowest uncertainty and hence greatest reliability of the proposed prediction model the 95 cbi represents the area that 95 of data points are located within therefore the 95 cb intervals 95 cbi represents the upper and lower limits of the cb table 3 represents the calculated uncertainty indices of mpe std wcb and 95 cbi for the traditional feh and proposed gep equations as evident the gep model has a marginally smaller wcb equal to 0 18 compared to the feh equation 0 19 and as such has a lower uncertainty furthermore the gep model has a lower mpe and std equal to 0 92 and 3 52 respectively and as such has the greatest ability to etimate tp values 3 4 case study as outlined the conceptual error behind current prediction methods lies in the singular constant tp value predicted for any storm within a given catchment to demonstrate this a case study of station 48 009 that has 17 storm events spanning a decade of collection is showcased as demonstrated in fig 6 observed tp values can range significantly strengthening the necessity for dynamic prediction over the current static methods the observed tp values range from 6 8 to 11 5 h while the feh equation predicts a constant value of 3 5 h while the gep equation predicts values from 6 4 to 13 9 h while the gep values do not exactly match those observed the proposed gep equation provides a considerably improved prediction method over the existing feh as discussed it can be found that the addition of the influential dynamic parameters such as qp and wetness coefficients increased the capability of prediction and accordingly decreased the uncertainty further the presentation of a simple and straightforward equation by the gep model which maintains consideration of the antecedent and storm dependent variables as input parameters justifies the application of the gep model over the traditional feh equation with acceptable security in all practical cases to estimate tp values 4 conclusions a data set containing more than 1400 storms spanning the uk was analysed using machine learning and literature review to identify and apply key input parameters to gep allowing evolutionary computation to provide a simplified empirical tp equation the resulting model provides dynamic prediction capabilities predicting storm specific catchment response this provides variability between storms rather than providing a constant single value for a given catchment three key variable types encompass the nature of tp storm specific encompassing the magnitude of the storm qp static catchment encompassing the variability between each catchment l s da propwet and dynamic catchment encompassing the variability within a catchment due to antecedent conditions c wetness coefficients the gep model utilizes various parameters from each of these three categories a key relationship between l and s was identified from the review of literature and previous empirical equations for prediction of catchment response time parameters this provided a ratio over l s which provided a measure of the catchment topography qp was identified as having a notable correlation with the tp and was therefore selected to encompass the magnitude of the storm in dynamic prediction as reinforced through a sensitivity analysis in the gep model the antecedent conditions are captured by the seasonal coefficients which change the relative importance of the other key inputs the research developed a robust dynamic prediction model for predicting tp and identifies seasonal trends in soil moisture to hydrologic modelling the final empirical model provides a method of prediction for tp within streams spanning the entirety of the uk and accounts for the variability between catchments antecedent conditions and storm size the gep equation employs dynamic prediction through the application of storm size and the seasonal wetness coefficient while allowing for an increased ease of application as it does not require smd tracking improvement in prediction is observed from the previous feh equation which this research builds upon this information can be applied to increase accuracy of stream flow models and correspondingly improve water resource management and the design and operation of hydraulic structures declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work was supported by the natural sciences and engineering research council of canada nserc discovery grant 400675 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2020 124630 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
5731,in flood forecasting and design for peak flows understanding and characterizing the hydrologic response to rainfall events is vitally important one of the key parameters utilized to characterize the catchment response time is the time to peak tp which represents the net rise time of a storm hydrograph or the time from when a precipitation event begins to contribute to stream discharge to the time that peak flow qp is reached previously influencing factors on tp have been static in nature with no consideration of the variability in tp due to size of the storm event and the antecedent moisture conditions of the watershed seasonal effects using 1400 storm event observations and the corresponding catchment characteristics of 153 stream gauges across the united kingdom uk the importance of different factors on estimating tp are evaluated these data points span three decades and this breadth of temporal data allowed meaningful annual trends to be observed and seasonal variations in soil moisture to be identified and applied a new wetness coefficient is applied herein to reflect the antecedent conditions within a catchment the qp is selected as a dynamic variable utilized to represent the magnitude of a given storm due to the demonstrated correlation with tp an explicit equation based on gene expression programming is designed which accounts for the dynamic nature of tp through qp and seasonal moisture effects the results of the proposed model are compared to the results of the existing equation for tp prediction in the uk outlined by the flood estimation handbook feh the proposed equation with nash sutcliffe coefficient r2 and rmse values equal to 0 60 0 66 and 3 64 respectively has improved characteristics compared with the traditional feh equation nash 0 42 r2 0 54 and rmse 4 37 a forensic analysis of the contributing factors for tp involves development of an empirical model with improved prediction accuracy by accounting for the dynamic inputs improving previous models both statistically as well as in the hydrologic understanding of the catchment response keywords catchment response time time to peak peak flow soil moisture gep nomenclature 95 cbi 95 confidence bound interval da contributing drainage area catchment area km2 dplbar mean of distances between each node on the uk institute of hydrology digital terrain model ihdtm grid and the catchment outlet in kilometres used to characterise catchment size and configuration https nrfa ceh ac uk feh catchment descriptors dpsbar this landform descriptor mean drainage path slope provides an index of overall catchment steepness it was developed for the flood estimation handbook and is calculated as the mean of all inter nodal slopes derived using the ihdtm for the catchment the index is expressed in metres per kilometre with values ranging from 300 in mountainous terrain to 25 in the flattest parts of the country https nrfa ceh ac uk feh catchment descriptors ei calculated error see eq 3 et expression tree feh uk flood estimation handbook fsr uk flood studies report 1975 ga genetic algorithm gep gene expression programming gp genetic programming l length dplbar mean drainage path length index km calculated using the mean drainage path length from each node of the uk institute of hydrology digital terrain model ihdtm https nrfa ceh ac uk feh catchment descriptors l s topographic ratio dplbar divided by the square root of dpsbar lc land cover classification or usage lcm land cover moisture factor ratio of mean smd of seasonal and lc group and the mean of the entire data set mpe mean prediction error see eq 4 nash nash sutcliffe coefficient nash and sutcliffe 1970 nrfa national river flow archive pdsa partial derivative sensitivity analysis propwet dry soils tend to inhibit flood formation whilst in contrast saturated soil conditions precede and contribute to many large flood events this catchment wetness index proportion of time soils are wet developed for the flood estimation handbook provides a measure of the proportion of time that catchment soils are defined as wet in this context when soil moisture deficits are less than 6 mm propwet values range from over 80 in the wettest catchments to less than 20 in the driest parts of the country proportion of time when the smd was less than or equal to 6 mm during the period 1961 to 1990 https nrfa ceh ac uk feh catchment descriptors qp peak flow of storm m3 s qp da normalized peak flow of storm by catchment size m3 s km2 r2 determination coefficient refh revitalised flood hydrograph s slope dpsbar mean drainage path slope index m km smd soil moisture deficit capacity of the soil to absorb water field capacity has zero smd i e saturated the model used in the feh allows a maximum value of 125 mm std standard deviation see eq 5 tc time of concentration hours tl lag time hours tp time to peak hours uk united kingdom urb 1 urbext1990 urbext1990 extent of urban and suburban land cover in 1990 vsa variable source area also referred to as variable contributing area wcb width of the confidence bound see eq 6 wsm wilson score method of uncertainty analysis 1 introduction in hydrologic and hydraulic modelling understanding and characterizing the response of a catchment to a rainfall event is of vital importance this knowledge is required for proper watershed management and to protect the public from flood hazards usda and nrcs 2010 the catchment response is characterized by time parameters which reflect the speed of response of a catchment area to a storm event pavlovic and moglen 2008 perdikaris et al 2018 sharifi and hosseini 2011 the most commonly used catchment response time parameters are time to peak tp time of concentration tc and lag time tl usda and nrcs 2010 tp is defined as the rise time of a storm hydrograph encompassing the time from the first stream contributions from a precipitation event to the arrival of the peak flow qp of the event granato 2012 kirpich 1940 mccuen et al 1984 usda and nrcs 2010 ramser 1927 kirpich 1940 gray 1961 wu 1963 bell and om kar 1969 fang et al 2005 walton et al 2019 this is consistent with multi peak flood events where tp can be defined as the total net rise of the hydrograph gericke and smithers 2017 tp can be directly measured from a storm event which makes it immediately applicable as well as beneficial for comparison purposes when developing a prediction method tc is another frequently used time parameter defined similarly to tp where the definitions are often mistakenly used interchangeably a regularly applied definition for tc is the time for water to travel from the most hydraulically distant point to the outlet of a catchment nrcs 1986 despite the various applications of tc in water management and infrastructure development kuichling 1889 nrcs 1986 gazendam et al 2016 the parameter lacks a commonly accepted definition and cannot be observed or measured explicitly therefore a true value is always unknown fang et al 2007 sharifi and hosseini 2011 perdikaris et al 2018 tl has been defined as the time a water parcel takes to travel from one location in a basin to another location downstream or the time delay between the peak precipitation and the peak discharge in an event seyam and othman 2014 hence tl corresponds with tc and often the definitions are similar which makes the differences between the two parameters confusing due to the wide variety of equations to calculate tl the value is highly dependent upon the formula used nrcs staff 2010 costache 2014 nagy et al 2016 these parameters are vital in characterizing catchment response time and are widely utilized a key component of sustainable water resource management and watershed planning is the ability to accurately predict stream flows vaze et al 2011 salimi et al 2017 catchment response parameters are directly applied in the design and operation of hydraulic structures pegram and parak 2004 atieh et al 2015 bonakdari et al 2019 and provide an understanding of potential flood and failure hazards through the temporal distribution of runoff gericke and smithers 2014 atieh et al 2017 specifically estimation of peak discharge water levels design storm duration and runoff are directly reliant upon the accuracy of predictions of catchment response time parameters kuichling 1889 gericke and smithers 2014 nagy et al 2016 salimi et al 2017 fang et al 2008 demonstrated that stream flow models are limited by timing errors and seyam and othman 2014 showed that model performance shows significant improvement with increased accuracy of catchment response timing in predicting design peak flows in ungauged catchments errors in time parameters have been reported to contribute up to 75 of the total prediction error bondelid et al 1982 fang et al 2008 gericke and smithers 2016 the most common application of time parameters is in the construction of the design hydrograph for a given catchment with an observed or design qp value and tp a unit hydrograph can be constructed for any location on a regularly shaped watershed usda and nrcs 2010 further modified hydrograph versions such as the revitalised flood hydrograph refh model proposed by the flood estimation handbook feh introduce more flexible shapes but are still dependent solely upon tp and qp kjeldsen 2007 numerous studies have evaluated and compared the various time parameter prediction equations and methods e g askew 1970 wong 2005 fang et al 2008 2007 mccuen 2009 grimaldi et al 2012 gericke and smithers 2014 2016 2017 nagy et al 2016 salimi et al 2017 perdikaris et al 2018 chen et al 2019 studies evaluating tc prediction methods are most common as it is the most frequently applied time parameter nagy et al 2016 however tp can be considered a more relevant response parameter as it can be directly measured from a storm event and has a commonly accepted definition fang et al 2007 concluded there is no single correct value of tc for a given watershed while mccuen 2009 demonstrated that tc calculated for the principal flow path does not accurately reflect temporal responses of the entire watershed due to the range of roughness flow depth slope and runoff timing across a catchment area in the uk the flood studies report fsr 1975 and the subsequent flood estimation handbook feh kjeldsen 2007 provide regression equations based on different input variables to calculate tp values in uk watersheds this empirical equation is currently the accepted method of predicting tp for streams in the uk kjeldsen 2007 it is of note that each of the input parameters for this and other available methods are static in nature and depend upon catchment characteristics e g drainage area da length l and slope s of a catchment and therefore will predict a single tp value for any given storm within a catchment at any time these methods do not account for variations between storm sizes or the antecedent conditions of the catchment the main objective of this study is to determine features which influence fluctuations in tp a forensic analysis of the hydrologic processes that influence catchment response time is developed to identify the controlling factors of tp the timing of peak flows is subject to dynamic variability this study aims to pinpoint key characteristics influencing the sources of the dynamic variability and determine a way to capture them in prediction accordingly this study proposes that tp is dynamic in nature and the fluctuations are dependent on the magnitude of the storm and the antecedent watershed conditions as well as the established static catchment characteristics in this paper an extensive data series is compiled where tp has been measured for 1412 storm events at 153 gauges in the uk and comparisons between predicted and observed values were made tp is chosen as it has been directly measured and therefore conclusions as to influencing variables can be directly drawn in prediction of this catchment response parameter two influential types of dynamic variables storm dependent variables and dynamic catchment variables are considered in addition to the previously applied static variables soil moisture deficit smd has been reported for each of the storms and gauges in the collected data set this study investigates alternative methods to incorporate the impact of antecedent soil moisture conditions without requiring a direct observation by using seasonal trends to describe the dynamic seasonal variability of tp this study builds upon the tp research presented in the fsr feh by providing a more comprehensive evaluation of the available research and tools with application to the large data set further the static prediction nature of current methods is described and improved upon subsequently a practical simple and robust equation based on gene expression programming for prediction of the tp parameter for the uk is introduced furthermore the feh equation as a conventional model in tp prediction is used to evaluate and compare with the proposed equation in this study with application to a large data set 2 methodology 2 1 data collection the uk has a large network of stream gauges maintained by the national river flow archive nrfa and the uk institute of hydrology approximately 1500 stream gauges are currently monitored in the uk through this program marsh and hannaford 2008 from these gauging stations a long term data set with a diverse set of measured and observed hydrologic parameters was obtained for use in this study the data set was originally developed for the uk winfap software which provides analyses based upon the feh leman et al 2009 these flood data and statistical procedures for estimating flood frequency are outlined in the third volume of the feh robinson and duncan 1999 the catchment characteristics from each stream gauge were combined with the set of storm event data to create one encompassing data set of the 1500 stations 153 stations have an entire set of 43 measured parameters corresponding to 1412 storms these data provided valuable information for developing a tp prediction method fig 1 demonstrates the location of each stream gauge used in this study further information on the collected data can be found in the nomenclature list the following sections detailing the influential variables considered in this study and the flood estimation handbook 2 2 flood estimation handbook the 1975 uk fsr and the subsequent feh kjeldsen 2007 are comprehensive guides to understanding flood prediction in the uk they provide the method to develop the previously mentioned refh hydrograph and corresponding regression equation for calculation of tp the regression equation is 1 tp 1 56 p r o p w e t 1 09 d p l b a r 0 60 1 u r b e x t 1990 3 34 d p s b a r 0 28 where propwet proportion of time when the soil moisture deficit smd was less than or equal to 6 mm during the period 1961 to 1990 bayliss 1999 dplbar feh mean drainage path length index km urbext1990 extent of urban and suburban land cover in 1990 and dpsbar feh mean drainage path slope index m km empirical eq 1 is currently the accepted method utilized for predicting tp for streams in the uk it is of note that each of these input parameters are static in nature and therefore will predict a single tp value for any given storm within a catchment at any time 2 3 influential variables in prediction of tp the first step in developing a suitable prediction method for tp was to identify a key set of variables to incorporate utilizing data and literature analyses the data set was narrowed from the available 43 variables to apply only the key inputs based on the developed hypothesis three classifications of variable groups were identified static catchment descriptors dynamic catchment descriptors and storm dependent variables in this research two dynamic characteristic classifications are used to predict tp in addition to static characteristics which are used in most previous studies askew 1970 fang et al 2008 2007 gericke and smithers 2016 2014 grimaldi et al 2012 haktanir and sezen 1990 mccuen 2009 nagy et al 2016 salimi et al 2017 wong 2005 these variables provide a comprehensive understanding of the nature of tp prediction and the catchment response time table 1 demonstrates the range of data compiled and analysed in this research 2 3 1 static catchment variables static catchment descriptors are variables that identify key characteristics of a given catchment and remain constant static catchment variables have been the primary basis of previous studies and prediction methods and have been demonstrated to have an influence on tp this group includes the flow path length l average flow path or catchment slope s catchment drainage area da and land cover classification lc including the percentage of urban area urbext in the feh propwet is utilized to identify the variations in general wetness between catchments by accounting for the frequency of low soil moisture deficit within a given catchment 2 3 1 1 length and slope the lengths of the catchment area or main channel have been shown by years of research to have a strong influence on catchment response time askew 1970 fang et al 2008 2007 gericke and smithers 2016 2014 grimaldi et al 2012 haktanir and sezen 1990 mccuen 2009 nagy et al 2016 salimi et al 2017 wong 2005 reviewers of temporal response parameters have identified length as the most prevalent variable in prediction the slope of the channel or catchment is the next most predominant prediction variable frequently l and s are used in conjunction when predicting tp or tc as l s costache 2014 fang et al 2007 kc and fang 2015 kirpich 1940 kjeldsen 2007 nagy et al 2016 simas and hawkins 1998 based on review of literature and previous empirical equations for prediction of catchment response time parameters a common tendency in prediction methods is to utilize the ratio l s rather than applying each parameter individually to the final model this ratio is utilized and provides a factor which accounts for these static catchment characteristics creating a ratio which encompasses the topography of the catchment area this key relationship provides new insights into the relationship between these parameters l s and the catchment response and is applied to the model in conjunction with da to represent the static catchment conditions the dpsbar and dplbar used in this analysis are not the slope or length themselves but indices created by the nrfa and the feh the values are the means of the entire catchment and characterize the catchment in relative terms 2 3 1 2 drainage area da or catchment area is a similarly static catchment descriptor of the watershed size while the inclusion of both l and da simultaneously may seem redundant as both variables represent the relative size of the catchment numerous studies have demonstrated the necessity for da in catchment response prediction williams 1922 simas and hawkins 1998 pegram and parak 2004 gericke and smithers 2017 pegram and parak 2004 determined catchment area as the most significant geomorphological catchment variable in flood prediction further salimi et al 2017 found that catchment response time parameter models that did not include da overestimated qp while those which include da had improved accuracy nester et al 2011 found that catchment size was the most important control when assessing regional flood simulations while fang et al 2007 found a direct positive correlation between da and tc tp has been demonstrated to increase with the da and as such catchment area has an influence on the magnitude of tp gericke and smithers 2014 kirpich 1940 mccuen et al 1984 pegram and parak 2004 schmidt and schulze 1984 simas 1996 2 3 1 3 propwet propwet is defined as the proportion of time when the soil moisture deficit smd was equal to or less than 6 mm during the period 1961 to 1990 bayliss 1999 this variable is intended to account for the historical wetness of a catchment by giving the percentage of time catchment soils are saturated this encompasses the tendency of the catchment to have wet soils and correspondingly the tendency to produce runoff quickly in a storm event ahmed et al 2013 jahanfar et al 2018 the feh equation utilizes propwet directly however due to the specificity of this variable this research is proposing grouping catchment as historically wet average or dry and where wet catchments have a propwet value of at least 0 6 and dry catchments have a propwet value of less than 0 4 2 3 2 storm dependent variables storm dependent variables are those which encompass the variability between storm events these parameters encompass the magnitude of a storm and provide dynamic variability within a catchment discharge and precipitation are representative of an individual storm event and variables characterizing them were considered to incorporate dynamic variability stemming from the storm size qp is identified in this study as the key variable to encompass the magnitude of the specific storm event incorporating the qp value in calculation of tp allows for the magnitude of the storm to be considered while reducing the need for extensive convolution processes or rainfall or runoff data for hydrograph creation gericke and smithers 2017 grimaldi et al 2012 found that a significant variability of time estimation parameter tc was attributed to qp variations and suggested that prediction methods could be significantly improved by utilizing the channel discharge utilizing the peak flow itself rather than precipitation data accounts for the amount of rain that directly turns to runoff and impacts the stream tp removing the variability in evapotranspiration and infiltration losses as such qp is a better representation of storm size and is a more applicable variable given its application in a number of models to route or forecast flood flows a strong negative correlation between tp and qp has been reported where increasing qp corresponded to a reduction in tp parallel to the results obtained by costache 2014 and nagy et al 2016 the peak flow can be normalized by the size of the catchment through division by da creating a ratio of qp da for the relative storm size this ratio is plotted against observed tp in fig 2 demonstrating the notable negative correlation between normalized qp relative storm size and tp which encapsulates a degree of predictive capabilities by applying this ratio almost all data points are centered on the fitted line which represents the acceptable and significant correlation of tp with qp grimaldi et al 2012 as such qp has been identified as having a correlation with the catchment response and is applied as the storm specific parameter to the final predictive model for tp estimation 2 3 3 dynamic catchment variables while descriptors which demonstrate the variances between catchments are important to understanding the correspondingly varying response time there can also be variances introduced within a catchment due to the antecedent conditions or season of occurrence dynamic catchment variables encompass these antecedent conditions which change between each event within a specific catchment soil moisture is a key descriptor of the antecedent catchment conditions and directly impacts the infiltration and runoff of a catchment soil moisture deficit represents the capacity of the soil to absorb water measured in millimetres of soil depth where smd of 0 mm indicates saturated conditions and above zero indicates an increased capacity to absorb water smd has been reported for each of the storms and gauges in the collected data set however in a review of past studies smd was not identified as a predictor variable this may be due to the nature of the variable the storm specific soil moisture can be measured directly but is tedious and time consuming this study investigates alternative methods to incorporate the impact of antecedent soil moisture without requiring a direct observation by using seasonal trends to describe the dynamic seasonal variability of tp in this research the influence of seasonal effects upon antecedent conditions was considered precipitation often varies between seasons this seasonal variation inflicts seasonal patterns on other hydrologic variables such as soil moisture magnitude of peak flows base flow etc indicating tp experiences similar seasonal trends to explore this trend fig 3 displays the average monthly variation in tp and smd at the uk gauging stations in general it can be seen that as tp increases smd declines while this is a generalization and there is complexity implicit in tp this adds strength to the idea that seasonal factors affect tp mean smd reaches a peak in july followed by a sharp decline until december the opposite trend is noted from january to july there is a notable opposite relationship between smd and tp hypothetically due to the contributing or variable source area vsa as the soil becomes increasingly saturated less precipitation is infiltrated thus creating runoff and a greater portion of the catchment contributes to the stream discharge dunne and black 1970 lim 2016 the wetness of a catchment directly affects the size and time of response in a precipitation event due to the dynamic contributing area the response is faster when only the river network contributes and slower when the entire drainage area contributes in saturated conditions beven and kirkby 1979 hewlett and hibbert 1967 raju and nandagiri 2018 correspondingly this increases the tp as the runoff from the farthest points of the catchment must connect with the stream before tp is reached rather than simply the contributions from the area immediately surrounding the stream accounting for contributions from the full da this is consistent with the average smd values peaking in spring using the smd trends demonstrated in fig 3 three seasons were identified wet average and dry these seasons are identified in fig 3 and summarized in supplemental files table 2 the wet season corresponds to the months of january february march and december where smd is on average less than four millimeters indicating saturated soil the average season corresponds to the months of april may october and november where smd is on average 4 and 14 mm indicating moderately saturated soils the dry season consists of june july august and september where smd is 14 mm indicating unsaturated soil conditions it has been established that the soil moisture conditions vary throughout the year allowing wet and dry seasons to be isolated nandintsetseg and shinoda 2011 bollman et al 2013 this allows for a dynamic catchment descriptor to be incorporated based upon the period of the storm s occurrence while maintaining relative ease of prediction with the variation between catchments and between storms within a catchment represented the final consideration to allow for dynamic prediction is encompassing the antecedent moisture conditions of the catchment at the time of the storm event fig 3 demonstrates seasonal trends in soil moisture and correspondingly tp using these trends in mean smd and the three soil moisture seasons wet dry and average identified this allows for the antecedent moisture conditions of the catchment to be accounted for without the necessity for soil moisture measurements this is the first time seasonal effects and antecedent conditions are applied to catchment response time parameter prediction 2 4 model development 2 4 1 gene expression programming gep gep methods provide a prediction expression for a desired output variable using corresponding input variables through the application of an evolutionary phenotype genotype genetic algorithm using genes organized head to tail to build chromosomes and was initially introduced by ferreira 2001 this method builds upon genetic algorithm ga and genetic programming gp methods optimally applying linear fixed length chromosomes as utilized in ga and expression trees ets of various shapes and sizes as applied in gp koza 1992 khozani et al 2017 power et al 2019 this coordination of methodologies provides the benefits from the original models without the corresponding disadvantages of the original models allowing gep to surpass the capabilities of the original techniques the basic components of gep algorithms encompass the control fitness function function set terminal set and terminal condition parameters gholami et al 2018 milukow et al 2019 a fixed length symbol set of terminal set and function set respectively as z l m n 1 or tan cos alavi et al 2013 comprise each gene in gep model alavi et al 2013 an initial population of chromosomes is generated and expressed as computer programs which are then assessed using a process of fitness functions to select the best performing program these chromosomes are then reproduced to create a new generation of chromosomes different genetic operators of gep model are used to modify the chosen chromosomes the adjustable parameters used in the gep model proposed in this paper are provided in the supplemental files supplemental file table 3 the gep model is evolutionary in nature increasing in accuracy with each generation the process is reiterated until the termination criterion is reached azimi et al 2017 gholami et al 2018 a representative flowchart of the gep model presented in this paper can be found in the supplemental files supplemental file fig 1 2 4 2 development of gep model while this paper is primarily a forensic analysis of the influencing factors behind catchment response time a simplified model incorporating this knowledge is prepared as an improvement to the original feh equation utilizing the previous literature review statistical analyses and machine learning key input variables can be identified and utilized to create an improved empirical equation for prediction of tp a cross correlation table of all 43 variables collected and compiled for this study was prepared demonstrating the relationships between variables supplemental files table 1 this allowed for those most influential to tp to be identified as well as grouped to remove redundancies in variable use similarly machine learning techniques were employed to assess the degree of impact of different input variables upon tp variability and prediction ability using these methods a set of variables was produced for use in gep to generate an empirical prediction model gep provides the opportunity to apply machine learning and evolutionary computation techniques while monitoring and controlling the complexity and length of the final prediction equation the data were divided into a 2 3 training 939 points 1 3 testing 473 points split then through the use of gep model development an empirical equation was created and compared to previous models identified in the technical literature utilizing these equations and regression analyses simplified empirical models are created as mentioned in section 2 3 1 the parameters of l and s are the most predominant static variables in tp prediction as identified by many researchers wong 2005 mccuen 2009 grimaldi et al 2012 gericke and smithers 2014 2016 nagy et al 2016 salimi et al 2017 furthermore as mentioned the da parameter is another influential parameter in tp estimation gericke and smithers 2017 pegram and parak 2004 simas and hawkins 1998 williams 1922 therefore in designing the gep model l s and da parameters are considered as representative of static variables and qp is provided as the storm dependent variable in order to simplify the equation provided by the gep model only the most effective and significant parameters are considered as input variables furthermore the effect of seasonality and dynamic variables are applied by adjusting the coefficients of gep model s equation by employing optimization methods to the training data set 3 results and discussion the key results of this study deal with identifying and applying key input parameters which can be utilized to provide an optimized model for prediction of tp using machine learning methods in the initial investigation statistical and hydrologic analyses are utilized to determine the correlation of different input parameters with tp and are compared to find the optimal inputs to predict tp in the uk in the second phase a gep empirical model and equation is proposed as an alternative method to the previous traditional feh model furthermore the performance of the traditional feh equation eq 1 with input parameters of l s propwet and urbext1990 is evaluated against the gep model 3 1 proposed gep model a gep model was developed to provide a simple empirical equation for comparable use to the feh equation while introducing the significance of dynamic prediction utilizing gep modelling development an empirical equation was derived and analysed for hydrologic significance and accuracy this model shows a notable improvement from the feh while maintaining ease of application this simplified gep equation is as follows 2 tp c 1 l s c 2 qp da 1 3 2 the final gep model utilizes only four direct input parameters l s applied in the ratio of l s qp and da in the ratio qp da previously referred to as the normalized peak flow however coefficients c1 and c2 encompass the wetness of the catchment at the start time of the storm as identified in fig 4 these c1 and c2 are optimized coefficients prepared by grouping catchments by propwet classification as outlined in section 2 3 1 3 and season as outlined in section 2 3 3 these coefficients consider the previously identified seasons based on smd trends wet in the months of december through march dry in june through september and average in april may october and november this accounts for the seasonal wetness based on the time of occurrence of the event similarly catchment propwet values were utilized to classify historical wetness where wet catchments have a propwet value of at least 0 6 and dry catchments have a propwet value of less than 0 4 this provides nine optimized wetness coefficients for use in the prediction equation the proposed gep model utilizes the previously identified topographic ratio of l s and the normalized storm size of qp da as input parameters reiterating the importance of these parameters this equation independently recognized the l s topographic ratio as statistically significant and identified the negative correlation between tp and the relative storm size as such the proposed gep model demonstrates both statistical and hydrologic insight however this model does not utilize smd measurements directly increasing the opportunities for application while maintaining the importance of seasonality and catchment wetness through the use of coefficients c1 and c2 further the variations in coefficients c1 and c2 demonstrate the significance of antecedent moisture conditions to the catchment response time when the catchment is saturated the first term c1 which represents the topography of the catchment using length and slope is of less importance than when the catchment is dry where the relative size of the storm represented by the ratio of qp da becomes more prevalent when the soil is saturated the relative contribution of each term shifts due to the wetness constants while evolutionary computation models such as gep are often efficient in hydrologic applications it is cautioned that deep understanding of the hydrologic relationships and significance must be enforced and maintained throughout use of gep modelling because the model evolves via statistical accuracy alone without understanding the relationships between parameters zorn and shamseldin 2015 this risk was accounted for through the use of hydrologic and literature analyses of the importance of each parameter and understanding the relationships between the hydrology and the mathematics of the models the proposed gep model includes static catchment descriptors including the previously identified l s ratio storm specific inputs by applying the relative storm size through the ratio of qp da and consideration for the antecedent moisture conditions through the optimized wetness coefficients of c1 and c2 3 2 final gep model the performance of models used in this paper is assessed using different statistical indexes such as root mean square error rmse determination coefficient r2 and nash sutcliffe coefficient nash nash and sutcliffe 1970 testing of the original feh equation on the data set demonstrated that while there is some success using this method there remains a notable degree of variability remaining to be accounted for the proposed gep equation statistically and hydrologically outperforms the existing feh equation where the r2 improves from 0 54 to 0 66 the nash increases from 0 42 to 0 60 and the rmse decreases from 4 37 to 3 64 fig 5 represents the scatter plot of tp values predicted by the feh equation and gep model in comparison with corresponding observed values as seen the gep model maintains simplicity but has an increased r2 value compared to the feh model and as such has a better correlation with the corresponding observed values furthermore the compaction of data points predicted by the gep model around the exact line y x is more than feh equation similarly the fitted line on gep model is closer to the exact line than the fitted line on feh equation this graph demonstrates the improved capability of the proposed gep model over the traditional feh equation further table 2 summarizes the r2 nash and rmse values for the feh equation and the gep model as an entire set as well as the training and testing data sets for better evaluation according to statistical indices in table 2 it can be seen that the proposed gep model shows improvement over the original feh equation overall the ease and breadth of application increased accuracy from the original feh equation and the inclusion of a storm specific input and a seasonal descriptor which encompasses the antecedent conditions at the time of the storm make this equation superior to the feh model fig 5 is the regression plot of tp estimated by feh and the final gep model in comparison with corresponding observed values as seen for the gep model the fitted line is in the vicinity of the exact line which represents the improved correlation of data points estimated by the gep model in comparison to the feh method further in the feh model stratification can be seen to occur where the same constant value is predicted for multiple storms within the same catchment this issue is nullified by the inclusion of dynamic variables in the presented model fig 5 and table 2 demonstrate that the gep model shows notable improvements in predictive capabilities from the original feh equation the final gep model was created using a 2 3 training 1 3 testing split of the collected 1412 storm events spanning more than three decades and the expanse of the uk further it encompasses the dynamic variability between storms and greatly improves the ability of prediction compared to the original feh equation the results of the literature review and understanding of hydrologic significance of each variable s importance as explored in previous sections were supported by statistical and graphical analyses of the models results the final model utilizes optimized wetness coefficients to quantify the antecedent conditions the qp to represent the magnitude of the storm and traditional static catchment descriptors including l s and da 3 3 uncertainty analysis the uncertainty of the original feh and the proposed ann and gep models were evaluated using the wilson score method wsm newcombe 1998 gholami et al 2019 through calculation of four indices of uncertainty mean prediction error mpe standard deviation std the width of the confidence bound wcb and the 95 confidence bound interval 95 cbi in calculating the mpe the basis of this method focuses upon calculating the error ei of each tp value estimated by the model and the corresponding observed values then finding the average of this value for all 1412 data points as according to eqs 3 and 4 3 e i t p i obs t p i model 4 mpe 1 n i 1 n t p i o b s t p i mod e l furthermore the standard deviation std of data points is calculated according to 5 s t d i 1 n e r r o r i error 2 n 1 1 2 subsequently the confidence bound cb is used in calculating the wcb and 95 cbi using eq 6 6 wcb t s t d n 1 2 where t is the left tailed inverse of the t distribution of error as calculated by the excel function t inv berry and armitage 1995 the smallest wcb represents the lowest uncertainty and hence greatest reliability of the proposed prediction model the 95 cbi represents the area that 95 of data points are located within therefore the 95 cb intervals 95 cbi represents the upper and lower limits of the cb table 3 represents the calculated uncertainty indices of mpe std wcb and 95 cbi for the traditional feh and proposed gep equations as evident the gep model has a marginally smaller wcb equal to 0 18 compared to the feh equation 0 19 and as such has a lower uncertainty furthermore the gep model has a lower mpe and std equal to 0 92 and 3 52 respectively and as such has the greatest ability to etimate tp values 3 4 case study as outlined the conceptual error behind current prediction methods lies in the singular constant tp value predicted for any storm within a given catchment to demonstrate this a case study of station 48 009 that has 17 storm events spanning a decade of collection is showcased as demonstrated in fig 6 observed tp values can range significantly strengthening the necessity for dynamic prediction over the current static methods the observed tp values range from 6 8 to 11 5 h while the feh equation predicts a constant value of 3 5 h while the gep equation predicts values from 6 4 to 13 9 h while the gep values do not exactly match those observed the proposed gep equation provides a considerably improved prediction method over the existing feh as discussed it can be found that the addition of the influential dynamic parameters such as qp and wetness coefficients increased the capability of prediction and accordingly decreased the uncertainty further the presentation of a simple and straightforward equation by the gep model which maintains consideration of the antecedent and storm dependent variables as input parameters justifies the application of the gep model over the traditional feh equation with acceptable security in all practical cases to estimate tp values 4 conclusions a data set containing more than 1400 storms spanning the uk was analysed using machine learning and literature review to identify and apply key input parameters to gep allowing evolutionary computation to provide a simplified empirical tp equation the resulting model provides dynamic prediction capabilities predicting storm specific catchment response this provides variability between storms rather than providing a constant single value for a given catchment three key variable types encompass the nature of tp storm specific encompassing the magnitude of the storm qp static catchment encompassing the variability between each catchment l s da propwet and dynamic catchment encompassing the variability within a catchment due to antecedent conditions c wetness coefficients the gep model utilizes various parameters from each of these three categories a key relationship between l and s was identified from the review of literature and previous empirical equations for prediction of catchment response time parameters this provided a ratio over l s which provided a measure of the catchment topography qp was identified as having a notable correlation with the tp and was therefore selected to encompass the magnitude of the storm in dynamic prediction as reinforced through a sensitivity analysis in the gep model the antecedent conditions are captured by the seasonal coefficients which change the relative importance of the other key inputs the research developed a robust dynamic prediction model for predicting tp and identifies seasonal trends in soil moisture to hydrologic modelling the final empirical model provides a method of prediction for tp within streams spanning the entirety of the uk and accounts for the variability between catchments antecedent conditions and storm size the gep equation employs dynamic prediction through the application of storm size and the seasonal wetness coefficient while allowing for an increased ease of application as it does not require smd tracking improvement in prediction is observed from the previous feh equation which this research builds upon this information can be applied to increase accuracy of stream flow models and correspondingly improve water resource management and the design and operation of hydraulic structures declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement this work was supported by the natural sciences and engineering research council of canada nserc discovery grant 400675 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2020 124630 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
5732,the interpolation algorithms and the satellite based precipitation products spps are two major approaches to estimating spatially distributed precipitation this would inevitably raise the questions whether the interpolation algorithms could outperform the spps or vice versa and what s the implications for hydrological modeling these questions however have received little attention in the literature this study compared the performances of two quasi physically based interpolation algorithms i e micromet and preclaps with the widely used spps i e tropical rainfall measuring mission multisatellite precipitation analysis tmpa and integrated multi satellite retrievals for global precipitation measurement imerg in and around the babao river basin brb a meso scale mountainous watershed on the qinghai tibet plateau meanwhile the hydrological utilities of the interpolated and the satellite based precipitation were evaluated by using the distributed hydrology soil vegetation model dhsvm results indicate that tmpa and imerg perform worse than the interpolation algorithms in estimating daily precipitation while they show comparable and even higher performance in reproducing monthly precipitation both the interpolation algorithms and the spps have an obvious lower performance in winter than the other seasons dhsvm with the tmpa based or imerg based precipitation when not subjected to additional calibration performs worse than those with the interpolated precipitation in simulating streamflow nevertheless interestingly the et simulations consistently match well with the independent remote sensing rs based et product as indicated by the higher coefficient of determination r 2 0 85 and nash sutcliffe efficiency nse 0 72 the additional calibration of dhsvm with the satellite based precipitation could enhance the streamflow simulation accuracy substantially with the nse increasing by 70 59 132 in the validation period this however would bring about larger discrepancies between the simulated and the rs based et in summer the study further discussed the implications of these findings for hydrological modeling over the data scarce mountainous watersheds and revealed the uncertainties associated with the rain gauge density keywords interpolation hydrological modeling mountainous watersheds tmpa imerg dhsvm nomenclature acronyms brb babao river basin preclaps precipitation lapse spps satellite based precipitation products dhsvm distributed hydrology soil vegetation model micromet meteorological distribution system for high resolution terrestrial modeling tmpa tropical rainfall measuring mission trmm multisatellite precipitation analysis imerg integrated multi satellite retrievals for global precipitation measurement 1 introduction precipitation is one of the key factors affecting watershed hydrology and water resource systems it varies significantly both in space and time bárdossy and pegram 2013 hence accurate and reliable information regarding the temporal and spatial variabilities of precipitation are of vital importance for a wide range of applications camera et al 2014 sun et al 2018 such as hydrological and ecological modeling lima et al 2018 water resource managements supit et al 2012 flood and drought monitoring hui mean et al 2018 yuan et al 2019 and terrestrial ecosystem studies gritti et al 2006 mo et al 2019 rain gauge is the traditional method used to measure precipitation at the point scale of the earth s surface li et al 2018 theoretically the spatial patterns of precipitation at the basin or regional scales can be well captured if the gauge networks are dense enough unfortunately in reality the rain gauges are usually sparsely and unevenly distributed particularly in some less developed regions as well as the remote mountainous areas duan et al 2019 consequently the interpolation algorithms are typically used together with the ground based measurements to derive spatially distributed precipitation tobin et al 2011 camera et al 2014 huang et al 2019 to date various interpolation schemes have been proposed and developed ranging from the simplest deterministic approach such as the thiessen polygon and inverse distance weighting idw methods to the intermediate complexity approaches such as the micromet liston and elder 2006 and precipitation lapse preclaps algorithms which take into account the effects of topography and to some other more complexed geostatistical methods such as the kriging the performance of the interpolation algorithms depends on many factors hwang et al 2012 such as the spatio temporal scales of the precipitation estimates the mechanism of the methods the gauge density as well as the topographic features in recent years many efforts have been devoted to compare the performance and the hydrological influences of different spatial interpolation techniques camera et al 2014 assessed the performance of 15 interpolation techniques in reproducing daily precipitation over topographically complex areas and reported that their relative ranks are closely associated with the station density and rainfall amounts xu et al 2015 compared three interpolation techniques for daily rainfall estimations in sichuan province of china and concluded that the ordinary cokriging ck is the optimal method zhang et al 2017a found that the physically based inverse distance and elevation weighted pbidew method is more suitable than the inverse distance weighted idw method in estimating precipitation for hydrological modelling in the topographically complex mountainous watersheds huang et al 2019 found that the interpolation approach based on the information diffusion principle and the idw and geostatistical interpolators provide similar spatial distributions for annual precipitation ossa moreno et al 2019 compared the interpolation approaches of different complexities in the upper aconcagua catchment in central chile and recommended the utilization of the method based on the residuals between observations and worldclim data or climate hazards group infrared precipitation with station chirps data besides the rain gauge weather radar is another ground based tool used to measure precipitation across large domains with fine spatiotemporal resolutions however it involves a lot of problems such as the ground clutter and beam blockage the calibration of the relations between reflectivity and rainfall intensity for different types of precipitation and the limited distributions around the globe delrieu et al 2009 sun et al 2018 in addition to the ground based observations the satellite based precipitation products spps emerge as an additional and promising approach to reproduce the spatial patterns of precipitation at the basin regional and global scales since the tropical rainfall measuring mission trmm a joint mission between the national aeronautics and space administration nasa of the united states and the japan aerospace exploration agency jaxa was launched in 1997 kummerow et al 1998 a series of spps have been developed and are freely available to the public among them tropical rainfall measuring mission trmm multi satellite precipitation analysis tmpa huffman et al 2007 and its successor integrated multi satellite retrievals for global precipitation measurement imerg huffman et al 2014 are perhaps the most widely used ones in a variety of researches and operational applications nevertheless they are inherently subjected to some systematic and random errors arising from the retrieval algorithms the shortcomings of the instruments the indirect measurements and the sampling uncertainties bharti and singh 2015 li et al 2015 ebrahimi et al 2017 hence a region specific assessment is an essential step before their applications currently numerous studies have evaluated the performances of the tmpa and imerg products over different regions of the world at various spatio temporal scales by directly comparing them against the ground based observations bharti and singh 2015 yong et al 2015 xu et al 2017 beck et al 2019 or by the indirect modeling based inference with a focus on hydrological utilities or operational flood modeling e g li et al 2015 camici et al 2018 belabid et al 2019 duan et al 2019 yuan et al 2019 in particular the researchers in the field of hydrometeorology have given much attention to the performance of tmpa and imerg over the mountainous areas where the spatial patterns of precipitation are typically not well captured by the ground based observations due to the paucity of rain gauges and the strong complexity of the climatic conditions meng et al 2014 evaluated the hydrological utilizes of the tmpa 3b42v6 product in the source region of yellow river and proved that it is not suitable for long term hydrological predictions el kenawy et al 2015 assessed the tmpa 3b42 product over north eastern iberia and highlighted the needs of further improvements of the precipitation retrieval algorithms over the areas of heterogeneous terrain bharti and singh 2015 validated the tmpa 3b42v7 product over the himalayan region and concluded that it exhibits a better performance for the lower altitude regions but exacerbates over the higher altitude ones kim et al 2016 reported that tmpa is applicable for hydrological modeling in the mountainous region of south korea ebrahimi et al 2017 evaluated the performance of the tmpa 3b42 product over the tibetan plateau and demonstrated that it has deficiency in capturing daily precipitation yuan et al 2017 reported the feasibility of the bias corrected imerg and tmpa 3b42v7 products in hydrological simulations in a data sparse mountainous watershed in myanmar khan et al 2018 suggested that the utilization of tmpa product for watershed hydrological simulation is a valuable alternative in data scarce regions like the upper indus basin wang et al 2019 evaluated and compared the imerg v05b and tmpa 3b42v7 products under over the hexi region with complex terrains in northwest china and concluded that they underestimate precipitation in the high altitude mountainous areas these studies could contribute valuable information to both the data users and producers nevertheless there are obviously conflicting results regarding the performance and the hydrological utilities of the tmpa and imerg products over the mountainous watersheds which calls for further investigations from the literature reviews it is very clear that in most cases the performance of the interpolation algorithm and the spps were assessed separately in the past however since both of them are the commonly used methods to deriving spatially distributed precipitation a key driving variable for distributed hydrological models it would inevitably raise two important questions i whether the interpolation algorithms could outperform the spps or vice versa especially over the data scarce mountainous areas and ii what s the implications for hydrological modeling to our best knowledge however there questions have been given rare attention in the literature hence the primary objectives and motivations of this study are i to compare the performances of two quasi physically based interpolation algorithms i e micromet and preclaps with the widely used spps i e tmpa 3b42v7 and imerg in and around the babao river basin brb a meso scale mountainous watershed on the northeast qinghai tibet plateau and meanwhile ii to assess the hydrological utilities of the interpolated and the satellite based precipitation through the modeling experiments by using the distributed hydrology soil vegetation model dhsvm the rest of the paper is organized as follows after the introduction the study area and data are briefly described in section 2 section 3 presents the methodology used in this research the results and discussion are arranged in section 4 and section 5 respectively and the conclusions are summarized in the last section 2 study area and data 2 1 study area the heihe river basin hrb is a typical endorheic river basin in the arid region of northwest china its upstream area lying between 98 0 e 101 3 e and 38 0 n 42 0 n is located on the northeastern qinghai tibetan plateau the babao river basin brb which is the eastern tributary of the upstream hrb was focused in this study fig 1 it serves as a good case study considering that it is a typical mountainous watershed within which only one national level meteorological station g03 exists the brb is a meso scale watershed with a drainage area of approximately 2 500 km2 it is characterized by complex terrains and has elevations ranging from 2686 m to 4916 m above sea level the basin has a continental alpine semi humid climate with an annual precipitation ranging 270 600 mm year and a mean annual temperature less than 1 c zhang et al 2017b snowfall is an important component of precipitation in the cold season winter and the spring snowmelt plays an important role in streamflow generations in the brb wang et al 2015 as shown in fig 1 there are seven vegetation types within the study area of which the alpine meadow alpine sparse vegetation aps and shrub account for approximately 90 of the total study area the soil textures include sandy loam loam and silt silt loam lu et al 2011 and the soil depth exhibits a strong spatial variability varying from 5 66 to 180 cm song et al 2016 yang et al 2016 2 2 data as listed in table 1 three categories of data were used in this study the first one mainly includes the data used for model setup and parametrization the meteorological forcing data of five national level stations fig 1 g1 5 including maximum and minimum temperature precipitation relative humidity and wind speed were obtained from the national meteorological information center nmic http data cma cn in addition the observations from another seven local rain gauges were further collected from the water resources bulletin of gansu province wrbgp http www gssl gov cn the 90 m local digital elevation model dem clipped from the aster global dem was used to define the topography and generate stream map and network the 1 100 000 vegetation map zhang et al 2018c the patterns of soil texture lu et al 2011 and soil depth and the data of soil hydrological properties song et al 2016 yang et al 2016 were used to define the vegetation and soil types and to parametrize the soil physical characteristics meanwhile the leaf area index lai data at the spatial resolution of 1 km which was reconstructed from the modis ndvi product through the harmonic analysis of time series hants algorithm jia et al 2011 was adopted to estimate the monthly lai all the data of the first category except for the meteorological data were collected from the science data center for cold and arid regions sdccar http westdc westgis ac cn the data of the second category were mainly used for model calibration and validation the daily streamflow observations at the outlet of the brb i e the qilian station were obtained from the wrbgp for the period 1998 2013 besides the remote sensing rs based product which was derived through an operational software system etwatch for regional evapotranspiration et monitoring wu et al 2012 was used to crosscheck the simulations of dhsvm the data of the last category i e the tmpa 3b42 and imerg products were used for the modeling experiments the tmpa 3b42 product covers the quasi global areas ranging 50 n 50 s with a 0 25 0 25 spatial resolution and a 3 hourly temporal resolution from january 1998 to present while its successor imerg covers the globe with a higher spatial and temporal resolutions 0 1 0 1 30 min from june 2000 to present the key advancement of imerg over tmpa is the extended capability to capture solid precipitation and light rain 0 5 mm h 1 the latest versions of the post real time tmpa 3b42 and imerg products for the period 2001 2014 were used in this study and would be simply referred to tmpa and imerg hereafter for conciseness these data are provided by the national aeronautics and space administration and can be freely downloaded from the goddard earth sciences data and information services center ges disc https disc gsfc nasa gov as we known the post real time tmpa and imerg products have been climatologically adjusted month by month using the global precipitation climatology center gpcc monthly full monitoring gauge product however there are only 194 international exchange stations across china within the gpcc network sheng et al 2013 wang et al 2018 none of rain gauges used in this research belong to the international exchange stations leading to an independent assessment of tmpa and imerg 3 methodology 3 1 quasi physically based precipitation interpolation algorithms 3 1 1 micromet the meteorological distribution system for high resolution terrestrial modeling micromet is an intermediate complexity and quasi physically based model that was designed to produce high resolution meteorological forcing distributions based on the relationships between climatic variables and the surrounding landscapes primarily topography liston and elder 2006 it has been widely used to perform meteorological interpolations over the mountainous areas with complex terrains e g mernild et al 2017 cao et al 2018 zhang et al 2018b zhao et al 2019 within the framework of micromet precipitation is distributed over a spatial domain through a two step process first the observations and elevations of the rain gauges are interpolated horizontally to the model grids by using a barnes objective analysis scheme which adopts a distance dependent weighting function koch et al 1983 the weight w assigned to a precipitation station is calculated use eq 1 1 w e x p r 2 f d n where r is the distance between the interpolated grid cell and the rain gauge and f d n is a parameter that defines the shapes of the filter response function and is determined by the data spacing and distribution liston and elder 2006 second the interpolated precipitations of the model grids are adjusted vertically through eq 2 2 p p 0 1 β z z 0 1 β z z 0 where p and p0 are the adjusted and interpolated precipitation respectively z and z0 are the actual and interpolated elevations respectively and β is the topography based adjustment factor that varies from month to month 3 1 2 preclaps the precipitation lapse preclaps scheme is a relatively simple approach that performs interpolation according to the precipitation lapse rate i e the decrement of precipitation with height and the elevation difference between the model grid and rain gauge specifically it estimates precipitation at the grid where rain gauge is absent by using eq 3 3 p i nsta w i p i 1 γ z z i where p and pi are the interpolated and measured precipitation respectively z and zi are the elevations of the model grid and rain gauge respectively wi is the weight assigned to the rain gauge i which is estimated through the idw method nsta is the number of gauges used for interpolation and γ is the precipitation lapse rate the preclaps algorithm was selected in this research mainly due to its simplicity and more importantly it is a built in scheme in dhsvm 3 2 performance of the interpolation algorithms and the spps the interpolation algorithms and the spps i e tmpa and imerg were evaluated by using the gauge based observations in and around the brb in terms of spps the precipitation estimates in the grid that contains the rain gauge were directly compared with the corresponding observations regarding the interpolation algorithms the assessment was conducted by using the leave one out cross validation method more specifically one of the rain gauges is left out for which precipitation was interpolated from the remaining ones and then verified against the observations the interpolations were carried out at the daily scale by using the micromet and preclaps algorithms respectively the grid size of the interpolation was set to 0 25 about 28 km in consistent with the tmpa product as shown in table 2 the topography based adjustment factor β of micromet was estimated for each month through the least square fit of eq 2 and the precipitation lapse rate γ of preclaps was derived based on the linear relationship between the mean annual precipitation and the elevations of the rain gauges the performance of the interpolation algorithms and the spps were quantitatively assessed through the typical statistical measures including i the correlation coefficient cc ii relative bias rbias iii root mean square error rmse and iv normalized rmse nrmse which are defined as in eqs 4 5 6 and 7 respectively a higher value of cc together with lower absolute values of rbias rmse and nrmse signify better performance of the interpolation algorithm or the spps and vice versa 4 cc i 1 n p i obs p obs mean p i p mean i 1 n p i obs p obs mean 2 i 1 n p i p mean 2 5 rbias i 1 n p i p i obs i 1 n p i obs 100 6 rmse 1 n i 1 n p i p i obs 2 7 nrmse rmse 1 n i 1 n p i obs where p i and p i obs are the precipitation estimates of the spps or the interpolation algorithms and the observations respectively at the time step i p mean and p obs mean are the mean values of the precipitation estimates and observations respectively and n is the number of time steps 3 3 distributed hydrology soil vegetation model 3 3 1 model description the distributed hydrology soil vegetation model dhsvm is a high resolution and process based hydrological model that describes the dynamics of snow cover soil moisture et and runoff at the catchment scale wigmosta et al 1994 the model was originally designed for mountainous regions with complex terrains and its source codes are freely available to the public the latest version of dhsvm i e version 3 1 2 was selected in this study dhsvm represents a watershed by a series of regular grids with the same size based on dem each of which is assigned with appropriate vegetation and soil parameters within the framework of dhsvm i the et is modeled using a two layer canopy model ii the snow accumulation and melt are simulated through a mass and energy balance model iii the unsaturated moisture movements through multiple rooting zone soil layers are described by darcy s law iv the lateral saturated subsurface flow is routed through a cell by cell approach while the overland flow is routed through either the cell by cell approach or the kinematic wave model zhang et al 2018a and v the channel flow is modeled using a robust linear storage routing algorithm more details about the model could be found in wigmosta et al 1994 storck et al 1998 and wigmosta and perkins 2001 dhsvm experienced a rapid development after its born in 1994 and has been widely used for a variety of applications such as the hydrological process modeling du et al 2007 cuo et al 2008 zhang et al 2016 the hydrological impact assessment alvarenga et al 2018 zhang et al 2018b yearsley et al 2019 and the sediment transport simulation lanini et al 2009 3 3 2 model setup and calibration dhsvm was setup with a 150 m spatial resolution to ensure an acceptable computational efficiency the variable infiltration capacity vic model liang et al 1994 was utilized as a meteorological forcing disaggregator to estimate sub daily 3 hour meteorological values from the daily ones in order to meet the general requirement of dhsvm zhang et al 2018b the built in preclaps algorithm was used to interpolate precipitation across the brb the stream map and network were generated using the aml script that is available at the official site of the model http dhsvm pnnl gov the dhsvm model was run for the period 2000 2013 at the sub daily time scale i e 3 h the first year 2000 was reserved for model warm up in order to mitigate the effects of inaccurate initial conditions the period from 2001 to 2004 was selected as the calibration period and the remaining one i e 2005 2013 as the validation period the calibration of dhsvm was performed through a trial and error process the sensitive parameters of dhsvm were first identified through the global sensitivity analysis algorithm i e the extended fourier amplitude sensitivity test efast saltelli et al 1999 we selected eight soil related parameters i e lateral conductivity lc exponential decrease ed maximum infiltration mi manning coefficient mc filed capacity fc and pore size distribution psd bubbling pressure bp depth threshold dt and three vegetation related parameters i e leaf area index lai leaf albedo la and minimum resistance mr to conduct the sensitive analysis as shown in fig 2 the identified sensitive parameters include fd lc ed lai bp and psd for which the global sensitivity indexes are greater than 0 10 these sensitive parameters were divided into two groups uncalibrated and calibrated ones the uncalibrated group consists of lai and fd which were parametrized according to the available rs based lai product jia et al 2011 and the data of soil properties song et al 2016 yang et al 2016 they were assumed to be true and were not subjected to further calibration the remaining ones belong to the calibrated group and each of them was adjusted at a time until the simulations of streamflow at the outlet of the brb are completely satisfactory 3 4 hydrological modeling experiments as shown in table 3 we designed six modeling experiments to evaluate the hydrological utilities of the interpolated and the satellite based precipitation the experiment dhsvm preclaps is in line with the simulations of the calibrated dhsvm and serves as the baseline scenario in dhsvm micromet the precipitation over the brb were estimated via the micromet algorithm while in the experiments dhsvm tmpa uncal and dhsvm imerg uncal they were estimated via the tmpa and imerg products respectively the parameters as well as the other model settings were kept constant for the four experiments i e dhsvm preclaps dhsvm micromet dhsvm tmpa uncal and dhsvm imerg uncal besides we further designed two another experiments i e dhsvm tmpa cal and dhsvm imerg cal in which dhsvm with the satellite based precipitation were subjected to additional calibration the grid size of dhsvm 150 m is smaller than that of the precipitation data 0 25 for tmpa and 0 1 for imerg in this study we did not further interpolate the coarse precipitation to the model grid 150 m and assumed that precipitation is uniform within the coarse grids the streamflow simulation accuracy of dhsvm was evaluated via the visual hydrograph inspection and the statistical indexes including the rmse coefficient of determination r 2 and nash sutcliffe efficiency nse nash and sutcliffe 1970 the rmse is defined in eq 5 the r 2 is equal to the square of cc and the nse is defined as eq 8 in addition the et simulations were crosschecked with the independent rs based et product wu et al 2012 8 nse 1 i 1 n q i obs q i sim 2 i 1 n q i obs q mean obs 2 where q i obs and q i sim are the observations and simulations respectively at time step i q mean obs is the mean value of the observations and n is the number of time steps 4 results 4 1 evaluations of the interpolation algorithms and the spps fig 3 presents the boxplots of the performance metrics cc rmse nrmse and rbias for the spps tmpa and imerg and the interpolation algorithms preclaps and micromet at multiple time scales we can see that the medium cc are 0 51 and 0 62 respectively at the daily scale for the preclaps and micromet schemes higher than those for tmpa 0 43 and imerg 0 50 meanwhile the medium rmse and nrmse for the two spps are greater than those for the interpolation algorithms the absolute medium rbias are 10 68 and 5 35 respectively for tmpa and imerg higher than those for the preclaps and micromet algorithms which are 1 20 and 1 98 respectively these results indicate the interpolation algorithms perform better than the spps in reproducing daily precipitation at the monthly scale however the preclaps algorithm performs slightly worse than the tmpa and imerg product as indicated by the higher rmse and nrmse the micromet algorithm which achieves a relatively high medium cc 0 93 outperforms tmpa and the preclaps algorithm while it shows a comparable performance with imerg at the annual scale the interpolation algorithms perform better than the spps in terms of cc whereas they exhibit lower performance in terms of nrmse comparing the results at different time scales we can see that the interpolation algorithms and the spps perform apparently better in estimating monthly precipitation than in reproducing daily and annual ones it should be mentioned that the preclaps algorithm tends to have a higher variability of the performance in comparison to the micromet algorithm and the spps as indicated by the larger ranges of the performance metrics fig 4 shows the boxplots of the performance metrics in different seasons including spring march to may summer jun to august autumn september to november and winter december to february similar patterns of the seasonal variations of the performance can be observed for the interpolation algorithms and the spps in terms of cc it shows an increasing trend from spring to summer and autumn and then drop rapidly in winter the medium rmse is highest in summer followed by autumn spring and winter due to the higher precipitation amounts than the other seasons the medium nrmse on the contrary shows the highest value in winter followed by spring autumn and summer the micromet algorithm consistently performs better than imerg preclaps and tmpa in all the seasons in terms of cc rmse and nrmse in winter the medium rbias is positive and large for tmpa and the interpolation algorithms indicating an overestimation of precipitation however it has a noticeable negative value for imerg implying an underestimation of precipitation overall the spps and the interpolation algorithms have an obvious lower performance in the cold winter than the other seasons in line with the findings of the previous studies conducted around our study region peng et al 2014 yang et al 2017 the seasonal alterations of the performance metrics at the monthly scale not presented here are very similar to those at the daily scale 4 2 hydrological simulations in different experiments figs 5 and 6 compare the daily and monthly streamflow observations against the simulations obtained from the modeling experiments at the outlet of the brb for the period 2001 2013 in the experiment dhsvm preclaps the simulated streamflow hydrograph exhibits a reasonably good agreement with the observed one as shown in table 4 the values of nse r 2 and rmse are 0 55 0 65 0 58 0 68 and 9 99 9 53 m3 s respectively for the daily monthly streamflow during the calibration period and they are 0 53 0 63 0 56 0 67 and 9 38 6 82 m3 s respectively during the validation period the discrepancies between the simulations and the observations mainly lie in the spring streamflow and peak flows the spring streamflow tends to be underestimated in most of the years this is possibly due to the absence of the representation of the soil freezing thawing processes in dhsvm which plays an important role in the streamflow generations in the brb owing to the freezing process the water could be preserved in the soil with the form of ice during the cold winter which would in turn lead to high soil moisture during the warm spring due to the thawing process zheng et al 2018 this could induce high streamflows when confronted with moderate or high precipitation in spring the peak flows also tend to be underestimated which might be partially due to the deficiency of dhsvm in representing the preferential flow and partially due to the exponential decay assumption of the soil lateral saturated hydraulic conductivity zhang et al 2018b in the experiment dhsvm micromet the r 2 and nse are slightly higher and meanwhile the rmse is lower than those in dhsvm preclaps during the calibration and validation periods the relatively better performance of dhsvm with the interpolated precipitation of micromet is possibly due to the higher precision of the precipitation estimates as shown in fig 3 in the experiment dhsvm tmpa uncal the hydrological model could overall reconstruct the variation patterns of streamflow as indicated by the high r 2 which are 0 53 and 0 71 respectively for the calibration and validation periods at the monthly scale nevertheless the values of nse are relatively low which are 0 17 0 21 and 0 21 0 25 respectively during the calibration and validation periods for daily monthly streamflow meanwhile the rmse is obviously higher than those in dhsvm preclaps and dhsvm micromet in the experiment dhsvm imerg uncal similarly the nse and r 2 or rmse is significantly lower or higher than those in dhsvm preclaps and dhsvm micromet the streamflow simulation accuracy is even lower than in dhsvm tmpa uncal during the calibration period but it becomes higher during the validation period the relatively poor performance of the hydrological model in dhsvm tmpa uncal can be explained by the overestimated summer precipitation of tmpa particularly in the years 2002 2007 as depicted in fig 6a in comparison to the interpolation algorithms the poor streamflow simulation precision in dhsvm imerg uncal however is mainly due to the underestimation of precipitation by imerg over the study region particularly in the years 2003 2008 and 2013 which has also been reported by wang et al 2019 the experiments dhsvm tmpa cal and dhsvm imerg cal are similar to dhsvm tmpa uncal and dhsvm imerg uncal respectively with the exception that dhsvm were subjected to additional calibrations we can clearly see that after the further calibration the streamflow simulation accuracy could be improved significantly more specifically the nse could increase from 0 17 0 21 to 0 32 0 40 at the daily monthly scale in dhsvm tmpa cal during the calibration period and from 0 21 0 25 to 0 44 0 58 during the validation period in the experiment dhsvm imerg cal the nse could increase from 0 04 0 06 to 0 40 0 48 at the daily monthly scale in the calibration period and from 0 34 0 38 to 0 58 0 71 in the validation period dhsvm with the tmpa based precipitation after the additional calibration still performs worse than those with the interpolated precipitation however dhsvm with the imerg based precipitation outperforms those with the interpolated precipitation in terms of streamflow simulation during the validation period as indicated by the highest nse and r2 and the lowest rmse fig 7 shows the comparisons and relations between the independent rs based et estimates and the simulations of different modeling experiments for the period 2000 2011 we can see that the variations of the et could be well captured in the experiments dhsvm preclaps dhsvm micromet and dhsvm tmpa uncal and dhsvm imerg uncal as indicated by the higher r 2 0 85 and nse 0 72 the discrepancies are primarily observed in spring this is possibly due to the underestimation of soil moisture by dhsvm as mentioned above which could limit the water available for et as plotted in fig 7a b and c the performance metrics are very close in the experiments dhsvm preclaps dhsvm micromet and dhsvm tmpa uncal implying comparable et simulations this is because on the one hand the discrepancies between the precipitation estimates of the interpolation algorithms and the tmpa product mainly occur in summer fig 6a during which however the et are unlikely subjected to water stress on the other hand the other climatic factors such as the air temperature and humidity and solar radiation which could also affect the et simulations profoundly were kept consistent for all the experiments in dhsvm tmpa uncal the streamflow was overestimated due to the higher precipitation estimates by tmpa to better match with the streamflow observations the additional model calibration in dhsvm tmpa cal reduced the simulated streamflow by increasing the et in summer consequently as marked by the red circle in fig 7e the modeled summer et tend to be overestimated and diverge more significantly from the rs based one in comparison to the experiment dhsvm tmpa uncal leading to a relatively low nse 0 65 in dhsvm imerg uncal the et simulations agree better with the rs based et product than all the other experiments as indicated by the highest nse 0 81 nevertheless the summer streamflow was apparently underestimated in this experiment due to the lower precipitation estimates by imerg the further model calibration in dhsvm imerg cal increased the summer streamflow by decreasing the et hence as marked by the blue circle in fig 7f the summer et simulations become more negatively biased in comparison to dhsvm imerg uncal resulting in a relatively low nse 0 64 5 discussion 5 1 interpolated or satellite based precipitation the interpolation algorithm and the spps are the two major approaches to estimating spatially distributed precipitation a fundamental input for distributed hydrological models this study compared and evaluated the interpolation algorithms and the spps in estimating precipitation at multiple time scales we found that the quasi physically based interpolation algorithms i e preclaps and micromet outperform the tmpa product in capturing daily precipitation particularly in the cold season winter the deficiency of tmpa in reproducing daily precipitation has also been reported in some other studies conducted in qinghai tibet platea e g ebrahimi et al 2017 hussain et al 2017 yan et al 2017 as well as those carried out over the mountainous areas in some other regions such as the upper indus basin khan et al 2018 and the chindwin river basin in myanmar yuan et al 2017 this might be explained by the following reasons to begin with the snow cover within and around the brb could induce strong scattering in winter which would lead to overestimations of precipitation by the passive microwave sensor i e trmm microwave imager tmi ebrahimi et al 2017 xu et al 2017 as indicated by the high positive value of median rbias 26 19 in fig 4 moreover the localized and short term orographic precipitation might not be captured by both the infrared ir and passive microwave instruments on board trmm bharti and singh 2015 xu et al 2017 additionally the other factors such as the time lag and spatial mismatches between the rain gauge and tmpa could also contribute to explain the low performance of tmpa at the daily scale hence the tmpa product cannot be substituted for ground measurements in terms of daily precipitation estimations as suggested by camici et al 2018 nevertheless the tmpa product shows a good performance in reproducing monthly precipitation which is slightly better than the preclaps interpolation algorithm this is partially due to that as mention in the section of 2 2 tmpa has been subjected to a month by month adjustment by using the ground based observations i e the gpcc data and partially due to the relatively lower complexities of the monthly precipitation patterns than the daily one hence the tmpa product still has a good potential in the field of hydrometeorology particularly in the remote mountainous regions without ground based observations the low performance of tmpa in capturing daily precipitation but higher performance in estimating monthly precipitation over the mountainous areas have also been reported in many other previous studies e g ebrahimi et al 2017 hussain et al 2017 yan et al 2017 yuan et al 2019 comparing the two spps imerg consistently outperforms its predecessor tmpa at the daily monthly and annual scales in line with many previous studies e g tang et al 2016 xu et al 2017 yuan et al 2018 as a result the advantage of the interpolation algorithms over imerg for daily precipitation estimations is smaller than that over tmpa at the monthly scale the imerg product even shows a better and comparable performance in comparison to the preclaps and micromet algorithms these results are encouraging and confirm that imerg should be a better alternative than tmpa in estimating precipitation over the data scarce regions nevertheless imerg doesn t exhibit a better performance than tmpa in winter as indicated by the relatively low cc and higher absolute rbias this indicates that more efforts are still needed to improve the imerg product in the cold season over the mountainous areas although the interpolation algorithms outperform the spps in estimating daily precipitation their performance are not very satisfactory as well mainly due to the deficiencies of the interpolation algorithms to begin with the precipitation elevation relationship which is derived from the monthly and annual observations may not be necessarily accurate at the daily scale garen et al 1994 second the precipitation frequency tends to be overestimated owing to that any of the station used for interpolation that have precipitation could lead to the occurrence of precipitation in the interpolated grid cell this drawback might be mitigated by introducing a precipitation occurrence probability within the framework of the interpolation algorithms as did by thornton et al 1997 in addition the other factors such as the topographic aspect and slope can also affect precipitation estimates considerably which however are not considered by the interpolation algorithms hence the interpolation algorithms could be further improved which might enhance their advantages over the spps 5 2 implications for hydrological modeling the interpolated and the satellite based spatially distributed precipitations were used to drive the fully distributed hydrological model dhsvm to assess their hydrological utilities we found that dhsvm with the tmpa based precipitation even after the additional calibration had overall a lower performance than that with the interpolated precipitation in modeling streamflow this indicates that the interpolated precipitation even from scarce rain gauges should be more preferred than the tmpa based one for streamflow simulations in the mountainous brb regarding the imerg based precipitation however we found that the streamflow simulations accuracy could be higher than those with the interpolated precipitation during the validation period if dhsvm was subjected to additional calibration this suggests that imerg provides a valuable alternative for driving hydrological models to simulate streamflow over the data scarce mountainous areas such as the brb in other words imerg could be used as a substitute to the gauge based interpolated precipitation in case that only the streamflow is focused the spps were proved to have a relatively low performance in capturing daily precipitation but a good performance in estimating monthly precipitation it could be inferred that they might be utilized to detect the trends of precipitation and streamflow both of which are of great interest to the managers and policy makers fig 8 depicts the variations and trends of the observed annual streamflows and the simulations generated in different experiments we can see a consistent increasing trend for the period 2001 2007 together with a concurrent decreasing trend for the period 2007 2013 in line with the observations this confirms the usefulness of the spps for determining the sign of streamflow trends although not for accurately estimating the magnitudes of the trends in other words both the tmpa and imerg products could be used as a substitute to the interpolated precipitation if the sign of streamflow trend is targeted the above results also imply that the utilities of the spps depend heavily on the goals of the hydrological modeling it should be noticed that dhsvm with the tmpa based or imerg based precipitation tends to exhibit different performance over different periods in simulating streamflow taking a closer look at the streamflow hydrography fig 5 we can observe that the modeled streamflows in dhsvm tmpa uncal are very close to dhsvm preclaps and dhsvm micromet for the years 2001 2009 and 2012 2013 due to the similar precipitation estimates but they were obviously overestimated in the years 2002 2007 the further model optimization in the experiment dhsvm tmpa cal could therefore not consistently improve the streamflow simulation accuracy for all the years moreover as listed in table 4 the streamflow simulation accuracy in the experiment dhsvm imerg uncal cal is obviously better in the validation period than the calibration period these results indicate that the hydrological utilities of the spps might vary for different time periods underlining the necessity of conducting a hydrological evaluation of the spps for a relatively long period in order to give a more holistic assessment the hydrological utilities of the spps were typically assessed by using only the streamflow observations in many previous studies e g kim et al 2016 yan et al 2017 yuan et al 2017 li et al 2018 duan et al 2019 jiang and bauer gottwein 2019 in this study we have also crosschecked the modeling results with the independent rs based et product interestingly the simulated et match well with the independent rs based et product nse 0 72 and r 2 0 85 in the experiments dhsvm preclaps dhsvm micromet dhsvm tmpa uncal and dhsv imerg uncal although the streamflow simulation accuracy are obviously lower in the latter two experiments than the former two the additional calibration of dhsvm with the tmpa based or imerg based precipitation in the experiments dhsvm tmpa cal or dhsvm imerg cal would enhance the streamflow simulation accuracy substantially as presented in section 4 2 however the simulated summer et would diverge more significantly from the rs based et estimates than those without additional calibration i e dhsvm tmpa uncal or dhsvm imerg uncal thus the improvement of streamflow estimates is actually achieved by sacrificing the et simulation accuracy highlighting that the streamflow observations alone may not be adequate for evaluating the hydrological utilities of the spps this also indicates that the hydrological model driven by the satellite based precipitation if calibrated solely against the streamflow observations might not be adequate for water balance analyses or water accounting since the precipitation estimation errors of the spps could be propagated to some other water balance components such as et as proven in this study 5 3 uncertainties in this study the observations of 12 rain gauges were used to assess the performance of two quasi physically interpolation algorithms in and around the brb through the leave one out cross validation technique the distances between two neighbor rain gauges are mostly less than 50 km which could contribute to explain the reason why the interpolation algorithms could have better performances than the spps in capturing daily precipitation however the distribution of the rain gauges might be very sparse in some other regions such as the northwest qinghai tibetan platea in this case the interpolation algorithms may not still outperform the spps particularly the imerg product considering that the nearest stations may not be able to represent the precipitation variations in the interpolated areas there is another case that the rain gauges are very unevenly distributed in some large watersheds in this case the interpolation algorithm may not be able to capture the spatial variability of precipitation at the watershed scale although it could have a higher performance than the spps when verified against the observations hence the findings of the study might not be applicable to the regions with very sparse or unevenly distributed rain gauges which necessitates further investigations in the future rain gauge density was recognized as an important source of uncertainties in the evaluations of the spps e g amitai et al 2012 tang et al 2018 tian et al 2018 in this study we further assessed the performance of the interpolation algorithm i e preclaps and the spps tmpa and imerg with the increasing numbers of rain gauges more specifically we used the five national level meteorological stations as the baseline and then randomly increased the number of rain gauges to the totally twelve because of the different combinations of the additional rain gauges there would be some variations of the performance metrics for instance there would be totally 15 random combinations when two additional rain gauges are selected from the remaining six ones the results are plotted in fig 9 we can see that at the daily scale the cc exhibits a consistent upward trend with the increasing number of rain gauges for the interpolation algorithm and the spps meanwhile the rmse shows an opposite downward trend the results indicate that the performance of the interpolation algorithm and the spps would increase with more dense rain gauge network thus if fewer rain gauges are utilized the performance of the spps would be underestimated as reported by tang et al 2018 and tian et al 2018 at the monthly scales we can observe that there are no obvious trends of the cc and rmse with the increasing number of rain gauges this implies that the rain gauge density has a relatively low influence on the performance of the interpolation algorithms and the spps at the monthly scale we can also see from fig 9 that the performance metrics of the interpolation algorithm show larger ranges for different combinations of the additional rain gauges compared with those of the spps this is due to the fact that the performance of the preclaps algorithm is not only intimately associated with the rain gauge density but also with their distributions 6 conclusions this study compared the performances of two quasi physically based interpolation algorithms i e micromet and preclaps with the widely used spps i e tmpa 3b42v7 and imerg at multiple time scales in and around the babao river basin brb a meso scale mountainous watershed on the northeast qinghai tibet plateau meanwhile the fully distributed hydrological model dhsvm was used to evaluate the hydrological utilities of the interpolated and the satellite based precipitation through different modeling experiments i e dhsvm preclaps dhsvm micromet and dhsvm tmpa uncal cal and dhsvm imerg uncal cal results indicate that the spps perform worse than the interpolation algorithms in reproducing daily precipitation while they show comparable and even higher performance in estimating monthly precipitation at all the time scales the imerg product outperforms tmpa and meanwhile the micromet algorithm outperforms the preclaps scheme both the interpolation algorithms and the spps exhibit an obvious seasonal variation regarding the performance increasing from spring to summer and autumn and then dropping rapidly in the cold winter dhsvm with the tmpa based or imerg based precipitation when not subjected to additional calibration in the experiments dhsvm tmpa uncal and dhsvm imerg uncal has an apparent lower performance in simulating streamflow compared to those with the interpolated precipitation in dhsvm preclaps and dhsvm micromet however interestingly the et simulations consistently agree well with the independent rs based et product in these experiments as indicated by the higher performance metrics nse 0 72 and r 2 0 85 the additional calibration of dhsvm with the tmpa based or imerg based precipitation in dhsvm tmpa cal and dhsvm imerg cal could enhance the streamflow simulation accuracy significantly the nse could increase by 70 59 109 52 and 86 84 132 respectively at the daily and monthly scales in the validation period nevertheless the simulated summer et would diverge more significantly from the rs based et product than those without additional calibration the improvement of streamflow estimates is actually achieved by sacrificing the et simulation accuracy highlighting that the streamflow observations alone may not be adequate for evaluating the hydrological utilities of the spps particularly if not only the streamflow but also some other water balance components are focused the further analyses revealed that the utilities of the spps depend heavily on the goals of the hydrological modeling moreover the hydrological model with the satellite based precipitation could have varying performance for different periods implying the necessity of conducting a hydrological evaluation of the spps for a relatively long period in addition the performance of the spps and the interpolation algorithms are closely associated with the rain gauge density one limitation of the study is that only one specific watershed was focused thus further work needs to be carried out to verifywhether our results can be generalized in some other mountainous watersheds with similar characteristics credit authorship contribution statement ling zhang conceptualization formal analysis investigation funding acquisition methodology resources visualization writing original draft writing review editing dong ren investigation formal analysis resources writing review editing zhuotong nan resources supervision writing review editing weizhen wang resources supervision writing review editing yi zhao investigation software resources yanbo zhao investigation validation writing review editing qimin ma investigation validation writing review editing xiaobo wu investigation validation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by the strategic priority research program of chinese academy of sciences xda19040504 and xda20100104 the national natural science foundation of china 41901045 and 41471448 and the cas light of west china program 29y929661 the authors would like to thank the uw hydro computational hydrology group at the university of washington for providing the source codes of dhsvm the authors also wish to express their gratitude to the anonymous reviewers for their valuable comments and suggestions on the paper 
5732,the interpolation algorithms and the satellite based precipitation products spps are two major approaches to estimating spatially distributed precipitation this would inevitably raise the questions whether the interpolation algorithms could outperform the spps or vice versa and what s the implications for hydrological modeling these questions however have received little attention in the literature this study compared the performances of two quasi physically based interpolation algorithms i e micromet and preclaps with the widely used spps i e tropical rainfall measuring mission multisatellite precipitation analysis tmpa and integrated multi satellite retrievals for global precipitation measurement imerg in and around the babao river basin brb a meso scale mountainous watershed on the qinghai tibet plateau meanwhile the hydrological utilities of the interpolated and the satellite based precipitation were evaluated by using the distributed hydrology soil vegetation model dhsvm results indicate that tmpa and imerg perform worse than the interpolation algorithms in estimating daily precipitation while they show comparable and even higher performance in reproducing monthly precipitation both the interpolation algorithms and the spps have an obvious lower performance in winter than the other seasons dhsvm with the tmpa based or imerg based precipitation when not subjected to additional calibration performs worse than those with the interpolated precipitation in simulating streamflow nevertheless interestingly the et simulations consistently match well with the independent remote sensing rs based et product as indicated by the higher coefficient of determination r 2 0 85 and nash sutcliffe efficiency nse 0 72 the additional calibration of dhsvm with the satellite based precipitation could enhance the streamflow simulation accuracy substantially with the nse increasing by 70 59 132 in the validation period this however would bring about larger discrepancies between the simulated and the rs based et in summer the study further discussed the implications of these findings for hydrological modeling over the data scarce mountainous watersheds and revealed the uncertainties associated with the rain gauge density keywords interpolation hydrological modeling mountainous watersheds tmpa imerg dhsvm nomenclature acronyms brb babao river basin preclaps precipitation lapse spps satellite based precipitation products dhsvm distributed hydrology soil vegetation model micromet meteorological distribution system for high resolution terrestrial modeling tmpa tropical rainfall measuring mission trmm multisatellite precipitation analysis imerg integrated multi satellite retrievals for global precipitation measurement 1 introduction precipitation is one of the key factors affecting watershed hydrology and water resource systems it varies significantly both in space and time bárdossy and pegram 2013 hence accurate and reliable information regarding the temporal and spatial variabilities of precipitation are of vital importance for a wide range of applications camera et al 2014 sun et al 2018 such as hydrological and ecological modeling lima et al 2018 water resource managements supit et al 2012 flood and drought monitoring hui mean et al 2018 yuan et al 2019 and terrestrial ecosystem studies gritti et al 2006 mo et al 2019 rain gauge is the traditional method used to measure precipitation at the point scale of the earth s surface li et al 2018 theoretically the spatial patterns of precipitation at the basin or regional scales can be well captured if the gauge networks are dense enough unfortunately in reality the rain gauges are usually sparsely and unevenly distributed particularly in some less developed regions as well as the remote mountainous areas duan et al 2019 consequently the interpolation algorithms are typically used together with the ground based measurements to derive spatially distributed precipitation tobin et al 2011 camera et al 2014 huang et al 2019 to date various interpolation schemes have been proposed and developed ranging from the simplest deterministic approach such as the thiessen polygon and inverse distance weighting idw methods to the intermediate complexity approaches such as the micromet liston and elder 2006 and precipitation lapse preclaps algorithms which take into account the effects of topography and to some other more complexed geostatistical methods such as the kriging the performance of the interpolation algorithms depends on many factors hwang et al 2012 such as the spatio temporal scales of the precipitation estimates the mechanism of the methods the gauge density as well as the topographic features in recent years many efforts have been devoted to compare the performance and the hydrological influences of different spatial interpolation techniques camera et al 2014 assessed the performance of 15 interpolation techniques in reproducing daily precipitation over topographically complex areas and reported that their relative ranks are closely associated with the station density and rainfall amounts xu et al 2015 compared three interpolation techniques for daily rainfall estimations in sichuan province of china and concluded that the ordinary cokriging ck is the optimal method zhang et al 2017a found that the physically based inverse distance and elevation weighted pbidew method is more suitable than the inverse distance weighted idw method in estimating precipitation for hydrological modelling in the topographically complex mountainous watersheds huang et al 2019 found that the interpolation approach based on the information diffusion principle and the idw and geostatistical interpolators provide similar spatial distributions for annual precipitation ossa moreno et al 2019 compared the interpolation approaches of different complexities in the upper aconcagua catchment in central chile and recommended the utilization of the method based on the residuals between observations and worldclim data or climate hazards group infrared precipitation with station chirps data besides the rain gauge weather radar is another ground based tool used to measure precipitation across large domains with fine spatiotemporal resolutions however it involves a lot of problems such as the ground clutter and beam blockage the calibration of the relations between reflectivity and rainfall intensity for different types of precipitation and the limited distributions around the globe delrieu et al 2009 sun et al 2018 in addition to the ground based observations the satellite based precipitation products spps emerge as an additional and promising approach to reproduce the spatial patterns of precipitation at the basin regional and global scales since the tropical rainfall measuring mission trmm a joint mission between the national aeronautics and space administration nasa of the united states and the japan aerospace exploration agency jaxa was launched in 1997 kummerow et al 1998 a series of spps have been developed and are freely available to the public among them tropical rainfall measuring mission trmm multi satellite precipitation analysis tmpa huffman et al 2007 and its successor integrated multi satellite retrievals for global precipitation measurement imerg huffman et al 2014 are perhaps the most widely used ones in a variety of researches and operational applications nevertheless they are inherently subjected to some systematic and random errors arising from the retrieval algorithms the shortcomings of the instruments the indirect measurements and the sampling uncertainties bharti and singh 2015 li et al 2015 ebrahimi et al 2017 hence a region specific assessment is an essential step before their applications currently numerous studies have evaluated the performances of the tmpa and imerg products over different regions of the world at various spatio temporal scales by directly comparing them against the ground based observations bharti and singh 2015 yong et al 2015 xu et al 2017 beck et al 2019 or by the indirect modeling based inference with a focus on hydrological utilities or operational flood modeling e g li et al 2015 camici et al 2018 belabid et al 2019 duan et al 2019 yuan et al 2019 in particular the researchers in the field of hydrometeorology have given much attention to the performance of tmpa and imerg over the mountainous areas where the spatial patterns of precipitation are typically not well captured by the ground based observations due to the paucity of rain gauges and the strong complexity of the climatic conditions meng et al 2014 evaluated the hydrological utilizes of the tmpa 3b42v6 product in the source region of yellow river and proved that it is not suitable for long term hydrological predictions el kenawy et al 2015 assessed the tmpa 3b42 product over north eastern iberia and highlighted the needs of further improvements of the precipitation retrieval algorithms over the areas of heterogeneous terrain bharti and singh 2015 validated the tmpa 3b42v7 product over the himalayan region and concluded that it exhibits a better performance for the lower altitude regions but exacerbates over the higher altitude ones kim et al 2016 reported that tmpa is applicable for hydrological modeling in the mountainous region of south korea ebrahimi et al 2017 evaluated the performance of the tmpa 3b42 product over the tibetan plateau and demonstrated that it has deficiency in capturing daily precipitation yuan et al 2017 reported the feasibility of the bias corrected imerg and tmpa 3b42v7 products in hydrological simulations in a data sparse mountainous watershed in myanmar khan et al 2018 suggested that the utilization of tmpa product for watershed hydrological simulation is a valuable alternative in data scarce regions like the upper indus basin wang et al 2019 evaluated and compared the imerg v05b and tmpa 3b42v7 products under over the hexi region with complex terrains in northwest china and concluded that they underestimate precipitation in the high altitude mountainous areas these studies could contribute valuable information to both the data users and producers nevertheless there are obviously conflicting results regarding the performance and the hydrological utilities of the tmpa and imerg products over the mountainous watersheds which calls for further investigations from the literature reviews it is very clear that in most cases the performance of the interpolation algorithm and the spps were assessed separately in the past however since both of them are the commonly used methods to deriving spatially distributed precipitation a key driving variable for distributed hydrological models it would inevitably raise two important questions i whether the interpolation algorithms could outperform the spps or vice versa especially over the data scarce mountainous areas and ii what s the implications for hydrological modeling to our best knowledge however there questions have been given rare attention in the literature hence the primary objectives and motivations of this study are i to compare the performances of two quasi physically based interpolation algorithms i e micromet and preclaps with the widely used spps i e tmpa 3b42v7 and imerg in and around the babao river basin brb a meso scale mountainous watershed on the northeast qinghai tibet plateau and meanwhile ii to assess the hydrological utilities of the interpolated and the satellite based precipitation through the modeling experiments by using the distributed hydrology soil vegetation model dhsvm the rest of the paper is organized as follows after the introduction the study area and data are briefly described in section 2 section 3 presents the methodology used in this research the results and discussion are arranged in section 4 and section 5 respectively and the conclusions are summarized in the last section 2 study area and data 2 1 study area the heihe river basin hrb is a typical endorheic river basin in the arid region of northwest china its upstream area lying between 98 0 e 101 3 e and 38 0 n 42 0 n is located on the northeastern qinghai tibetan plateau the babao river basin brb which is the eastern tributary of the upstream hrb was focused in this study fig 1 it serves as a good case study considering that it is a typical mountainous watershed within which only one national level meteorological station g03 exists the brb is a meso scale watershed with a drainage area of approximately 2 500 km2 it is characterized by complex terrains and has elevations ranging from 2686 m to 4916 m above sea level the basin has a continental alpine semi humid climate with an annual precipitation ranging 270 600 mm year and a mean annual temperature less than 1 c zhang et al 2017b snowfall is an important component of precipitation in the cold season winter and the spring snowmelt plays an important role in streamflow generations in the brb wang et al 2015 as shown in fig 1 there are seven vegetation types within the study area of which the alpine meadow alpine sparse vegetation aps and shrub account for approximately 90 of the total study area the soil textures include sandy loam loam and silt silt loam lu et al 2011 and the soil depth exhibits a strong spatial variability varying from 5 66 to 180 cm song et al 2016 yang et al 2016 2 2 data as listed in table 1 three categories of data were used in this study the first one mainly includes the data used for model setup and parametrization the meteorological forcing data of five national level stations fig 1 g1 5 including maximum and minimum temperature precipitation relative humidity and wind speed were obtained from the national meteorological information center nmic http data cma cn in addition the observations from another seven local rain gauges were further collected from the water resources bulletin of gansu province wrbgp http www gssl gov cn the 90 m local digital elevation model dem clipped from the aster global dem was used to define the topography and generate stream map and network the 1 100 000 vegetation map zhang et al 2018c the patterns of soil texture lu et al 2011 and soil depth and the data of soil hydrological properties song et al 2016 yang et al 2016 were used to define the vegetation and soil types and to parametrize the soil physical characteristics meanwhile the leaf area index lai data at the spatial resolution of 1 km which was reconstructed from the modis ndvi product through the harmonic analysis of time series hants algorithm jia et al 2011 was adopted to estimate the monthly lai all the data of the first category except for the meteorological data were collected from the science data center for cold and arid regions sdccar http westdc westgis ac cn the data of the second category were mainly used for model calibration and validation the daily streamflow observations at the outlet of the brb i e the qilian station were obtained from the wrbgp for the period 1998 2013 besides the remote sensing rs based product which was derived through an operational software system etwatch for regional evapotranspiration et monitoring wu et al 2012 was used to crosscheck the simulations of dhsvm the data of the last category i e the tmpa 3b42 and imerg products were used for the modeling experiments the tmpa 3b42 product covers the quasi global areas ranging 50 n 50 s with a 0 25 0 25 spatial resolution and a 3 hourly temporal resolution from january 1998 to present while its successor imerg covers the globe with a higher spatial and temporal resolutions 0 1 0 1 30 min from june 2000 to present the key advancement of imerg over tmpa is the extended capability to capture solid precipitation and light rain 0 5 mm h 1 the latest versions of the post real time tmpa 3b42 and imerg products for the period 2001 2014 were used in this study and would be simply referred to tmpa and imerg hereafter for conciseness these data are provided by the national aeronautics and space administration and can be freely downloaded from the goddard earth sciences data and information services center ges disc https disc gsfc nasa gov as we known the post real time tmpa and imerg products have been climatologically adjusted month by month using the global precipitation climatology center gpcc monthly full monitoring gauge product however there are only 194 international exchange stations across china within the gpcc network sheng et al 2013 wang et al 2018 none of rain gauges used in this research belong to the international exchange stations leading to an independent assessment of tmpa and imerg 3 methodology 3 1 quasi physically based precipitation interpolation algorithms 3 1 1 micromet the meteorological distribution system for high resolution terrestrial modeling micromet is an intermediate complexity and quasi physically based model that was designed to produce high resolution meteorological forcing distributions based on the relationships between climatic variables and the surrounding landscapes primarily topography liston and elder 2006 it has been widely used to perform meteorological interpolations over the mountainous areas with complex terrains e g mernild et al 2017 cao et al 2018 zhang et al 2018b zhao et al 2019 within the framework of micromet precipitation is distributed over a spatial domain through a two step process first the observations and elevations of the rain gauges are interpolated horizontally to the model grids by using a barnes objective analysis scheme which adopts a distance dependent weighting function koch et al 1983 the weight w assigned to a precipitation station is calculated use eq 1 1 w e x p r 2 f d n where r is the distance between the interpolated grid cell and the rain gauge and f d n is a parameter that defines the shapes of the filter response function and is determined by the data spacing and distribution liston and elder 2006 second the interpolated precipitations of the model grids are adjusted vertically through eq 2 2 p p 0 1 β z z 0 1 β z z 0 where p and p0 are the adjusted and interpolated precipitation respectively z and z0 are the actual and interpolated elevations respectively and β is the topography based adjustment factor that varies from month to month 3 1 2 preclaps the precipitation lapse preclaps scheme is a relatively simple approach that performs interpolation according to the precipitation lapse rate i e the decrement of precipitation with height and the elevation difference between the model grid and rain gauge specifically it estimates precipitation at the grid where rain gauge is absent by using eq 3 3 p i nsta w i p i 1 γ z z i where p and pi are the interpolated and measured precipitation respectively z and zi are the elevations of the model grid and rain gauge respectively wi is the weight assigned to the rain gauge i which is estimated through the idw method nsta is the number of gauges used for interpolation and γ is the precipitation lapse rate the preclaps algorithm was selected in this research mainly due to its simplicity and more importantly it is a built in scheme in dhsvm 3 2 performance of the interpolation algorithms and the spps the interpolation algorithms and the spps i e tmpa and imerg were evaluated by using the gauge based observations in and around the brb in terms of spps the precipitation estimates in the grid that contains the rain gauge were directly compared with the corresponding observations regarding the interpolation algorithms the assessment was conducted by using the leave one out cross validation method more specifically one of the rain gauges is left out for which precipitation was interpolated from the remaining ones and then verified against the observations the interpolations were carried out at the daily scale by using the micromet and preclaps algorithms respectively the grid size of the interpolation was set to 0 25 about 28 km in consistent with the tmpa product as shown in table 2 the topography based adjustment factor β of micromet was estimated for each month through the least square fit of eq 2 and the precipitation lapse rate γ of preclaps was derived based on the linear relationship between the mean annual precipitation and the elevations of the rain gauges the performance of the interpolation algorithms and the spps were quantitatively assessed through the typical statistical measures including i the correlation coefficient cc ii relative bias rbias iii root mean square error rmse and iv normalized rmse nrmse which are defined as in eqs 4 5 6 and 7 respectively a higher value of cc together with lower absolute values of rbias rmse and nrmse signify better performance of the interpolation algorithm or the spps and vice versa 4 cc i 1 n p i obs p obs mean p i p mean i 1 n p i obs p obs mean 2 i 1 n p i p mean 2 5 rbias i 1 n p i p i obs i 1 n p i obs 100 6 rmse 1 n i 1 n p i p i obs 2 7 nrmse rmse 1 n i 1 n p i obs where p i and p i obs are the precipitation estimates of the spps or the interpolation algorithms and the observations respectively at the time step i p mean and p obs mean are the mean values of the precipitation estimates and observations respectively and n is the number of time steps 3 3 distributed hydrology soil vegetation model 3 3 1 model description the distributed hydrology soil vegetation model dhsvm is a high resolution and process based hydrological model that describes the dynamics of snow cover soil moisture et and runoff at the catchment scale wigmosta et al 1994 the model was originally designed for mountainous regions with complex terrains and its source codes are freely available to the public the latest version of dhsvm i e version 3 1 2 was selected in this study dhsvm represents a watershed by a series of regular grids with the same size based on dem each of which is assigned with appropriate vegetation and soil parameters within the framework of dhsvm i the et is modeled using a two layer canopy model ii the snow accumulation and melt are simulated through a mass and energy balance model iii the unsaturated moisture movements through multiple rooting zone soil layers are described by darcy s law iv the lateral saturated subsurface flow is routed through a cell by cell approach while the overland flow is routed through either the cell by cell approach or the kinematic wave model zhang et al 2018a and v the channel flow is modeled using a robust linear storage routing algorithm more details about the model could be found in wigmosta et al 1994 storck et al 1998 and wigmosta and perkins 2001 dhsvm experienced a rapid development after its born in 1994 and has been widely used for a variety of applications such as the hydrological process modeling du et al 2007 cuo et al 2008 zhang et al 2016 the hydrological impact assessment alvarenga et al 2018 zhang et al 2018b yearsley et al 2019 and the sediment transport simulation lanini et al 2009 3 3 2 model setup and calibration dhsvm was setup with a 150 m spatial resolution to ensure an acceptable computational efficiency the variable infiltration capacity vic model liang et al 1994 was utilized as a meteorological forcing disaggregator to estimate sub daily 3 hour meteorological values from the daily ones in order to meet the general requirement of dhsvm zhang et al 2018b the built in preclaps algorithm was used to interpolate precipitation across the brb the stream map and network were generated using the aml script that is available at the official site of the model http dhsvm pnnl gov the dhsvm model was run for the period 2000 2013 at the sub daily time scale i e 3 h the first year 2000 was reserved for model warm up in order to mitigate the effects of inaccurate initial conditions the period from 2001 to 2004 was selected as the calibration period and the remaining one i e 2005 2013 as the validation period the calibration of dhsvm was performed through a trial and error process the sensitive parameters of dhsvm were first identified through the global sensitivity analysis algorithm i e the extended fourier amplitude sensitivity test efast saltelli et al 1999 we selected eight soil related parameters i e lateral conductivity lc exponential decrease ed maximum infiltration mi manning coefficient mc filed capacity fc and pore size distribution psd bubbling pressure bp depth threshold dt and three vegetation related parameters i e leaf area index lai leaf albedo la and minimum resistance mr to conduct the sensitive analysis as shown in fig 2 the identified sensitive parameters include fd lc ed lai bp and psd for which the global sensitivity indexes are greater than 0 10 these sensitive parameters were divided into two groups uncalibrated and calibrated ones the uncalibrated group consists of lai and fd which were parametrized according to the available rs based lai product jia et al 2011 and the data of soil properties song et al 2016 yang et al 2016 they were assumed to be true and were not subjected to further calibration the remaining ones belong to the calibrated group and each of them was adjusted at a time until the simulations of streamflow at the outlet of the brb are completely satisfactory 3 4 hydrological modeling experiments as shown in table 3 we designed six modeling experiments to evaluate the hydrological utilities of the interpolated and the satellite based precipitation the experiment dhsvm preclaps is in line with the simulations of the calibrated dhsvm and serves as the baseline scenario in dhsvm micromet the precipitation over the brb were estimated via the micromet algorithm while in the experiments dhsvm tmpa uncal and dhsvm imerg uncal they were estimated via the tmpa and imerg products respectively the parameters as well as the other model settings were kept constant for the four experiments i e dhsvm preclaps dhsvm micromet dhsvm tmpa uncal and dhsvm imerg uncal besides we further designed two another experiments i e dhsvm tmpa cal and dhsvm imerg cal in which dhsvm with the satellite based precipitation were subjected to additional calibration the grid size of dhsvm 150 m is smaller than that of the precipitation data 0 25 for tmpa and 0 1 for imerg in this study we did not further interpolate the coarse precipitation to the model grid 150 m and assumed that precipitation is uniform within the coarse grids the streamflow simulation accuracy of dhsvm was evaluated via the visual hydrograph inspection and the statistical indexes including the rmse coefficient of determination r 2 and nash sutcliffe efficiency nse nash and sutcliffe 1970 the rmse is defined in eq 5 the r 2 is equal to the square of cc and the nse is defined as eq 8 in addition the et simulations were crosschecked with the independent rs based et product wu et al 2012 8 nse 1 i 1 n q i obs q i sim 2 i 1 n q i obs q mean obs 2 where q i obs and q i sim are the observations and simulations respectively at time step i q mean obs is the mean value of the observations and n is the number of time steps 4 results 4 1 evaluations of the interpolation algorithms and the spps fig 3 presents the boxplots of the performance metrics cc rmse nrmse and rbias for the spps tmpa and imerg and the interpolation algorithms preclaps and micromet at multiple time scales we can see that the medium cc are 0 51 and 0 62 respectively at the daily scale for the preclaps and micromet schemes higher than those for tmpa 0 43 and imerg 0 50 meanwhile the medium rmse and nrmse for the two spps are greater than those for the interpolation algorithms the absolute medium rbias are 10 68 and 5 35 respectively for tmpa and imerg higher than those for the preclaps and micromet algorithms which are 1 20 and 1 98 respectively these results indicate the interpolation algorithms perform better than the spps in reproducing daily precipitation at the monthly scale however the preclaps algorithm performs slightly worse than the tmpa and imerg product as indicated by the higher rmse and nrmse the micromet algorithm which achieves a relatively high medium cc 0 93 outperforms tmpa and the preclaps algorithm while it shows a comparable performance with imerg at the annual scale the interpolation algorithms perform better than the spps in terms of cc whereas they exhibit lower performance in terms of nrmse comparing the results at different time scales we can see that the interpolation algorithms and the spps perform apparently better in estimating monthly precipitation than in reproducing daily and annual ones it should be mentioned that the preclaps algorithm tends to have a higher variability of the performance in comparison to the micromet algorithm and the spps as indicated by the larger ranges of the performance metrics fig 4 shows the boxplots of the performance metrics in different seasons including spring march to may summer jun to august autumn september to november and winter december to february similar patterns of the seasonal variations of the performance can be observed for the interpolation algorithms and the spps in terms of cc it shows an increasing trend from spring to summer and autumn and then drop rapidly in winter the medium rmse is highest in summer followed by autumn spring and winter due to the higher precipitation amounts than the other seasons the medium nrmse on the contrary shows the highest value in winter followed by spring autumn and summer the micromet algorithm consistently performs better than imerg preclaps and tmpa in all the seasons in terms of cc rmse and nrmse in winter the medium rbias is positive and large for tmpa and the interpolation algorithms indicating an overestimation of precipitation however it has a noticeable negative value for imerg implying an underestimation of precipitation overall the spps and the interpolation algorithms have an obvious lower performance in the cold winter than the other seasons in line with the findings of the previous studies conducted around our study region peng et al 2014 yang et al 2017 the seasonal alterations of the performance metrics at the monthly scale not presented here are very similar to those at the daily scale 4 2 hydrological simulations in different experiments figs 5 and 6 compare the daily and monthly streamflow observations against the simulations obtained from the modeling experiments at the outlet of the brb for the period 2001 2013 in the experiment dhsvm preclaps the simulated streamflow hydrograph exhibits a reasonably good agreement with the observed one as shown in table 4 the values of nse r 2 and rmse are 0 55 0 65 0 58 0 68 and 9 99 9 53 m3 s respectively for the daily monthly streamflow during the calibration period and they are 0 53 0 63 0 56 0 67 and 9 38 6 82 m3 s respectively during the validation period the discrepancies between the simulations and the observations mainly lie in the spring streamflow and peak flows the spring streamflow tends to be underestimated in most of the years this is possibly due to the absence of the representation of the soil freezing thawing processes in dhsvm which plays an important role in the streamflow generations in the brb owing to the freezing process the water could be preserved in the soil with the form of ice during the cold winter which would in turn lead to high soil moisture during the warm spring due to the thawing process zheng et al 2018 this could induce high streamflows when confronted with moderate or high precipitation in spring the peak flows also tend to be underestimated which might be partially due to the deficiency of dhsvm in representing the preferential flow and partially due to the exponential decay assumption of the soil lateral saturated hydraulic conductivity zhang et al 2018b in the experiment dhsvm micromet the r 2 and nse are slightly higher and meanwhile the rmse is lower than those in dhsvm preclaps during the calibration and validation periods the relatively better performance of dhsvm with the interpolated precipitation of micromet is possibly due to the higher precision of the precipitation estimates as shown in fig 3 in the experiment dhsvm tmpa uncal the hydrological model could overall reconstruct the variation patterns of streamflow as indicated by the high r 2 which are 0 53 and 0 71 respectively for the calibration and validation periods at the monthly scale nevertheless the values of nse are relatively low which are 0 17 0 21 and 0 21 0 25 respectively during the calibration and validation periods for daily monthly streamflow meanwhile the rmse is obviously higher than those in dhsvm preclaps and dhsvm micromet in the experiment dhsvm imerg uncal similarly the nse and r 2 or rmse is significantly lower or higher than those in dhsvm preclaps and dhsvm micromet the streamflow simulation accuracy is even lower than in dhsvm tmpa uncal during the calibration period but it becomes higher during the validation period the relatively poor performance of the hydrological model in dhsvm tmpa uncal can be explained by the overestimated summer precipitation of tmpa particularly in the years 2002 2007 as depicted in fig 6a in comparison to the interpolation algorithms the poor streamflow simulation precision in dhsvm imerg uncal however is mainly due to the underestimation of precipitation by imerg over the study region particularly in the years 2003 2008 and 2013 which has also been reported by wang et al 2019 the experiments dhsvm tmpa cal and dhsvm imerg cal are similar to dhsvm tmpa uncal and dhsvm imerg uncal respectively with the exception that dhsvm were subjected to additional calibrations we can clearly see that after the further calibration the streamflow simulation accuracy could be improved significantly more specifically the nse could increase from 0 17 0 21 to 0 32 0 40 at the daily monthly scale in dhsvm tmpa cal during the calibration period and from 0 21 0 25 to 0 44 0 58 during the validation period in the experiment dhsvm imerg cal the nse could increase from 0 04 0 06 to 0 40 0 48 at the daily monthly scale in the calibration period and from 0 34 0 38 to 0 58 0 71 in the validation period dhsvm with the tmpa based precipitation after the additional calibration still performs worse than those with the interpolated precipitation however dhsvm with the imerg based precipitation outperforms those with the interpolated precipitation in terms of streamflow simulation during the validation period as indicated by the highest nse and r2 and the lowest rmse fig 7 shows the comparisons and relations between the independent rs based et estimates and the simulations of different modeling experiments for the period 2000 2011 we can see that the variations of the et could be well captured in the experiments dhsvm preclaps dhsvm micromet and dhsvm tmpa uncal and dhsvm imerg uncal as indicated by the higher r 2 0 85 and nse 0 72 the discrepancies are primarily observed in spring this is possibly due to the underestimation of soil moisture by dhsvm as mentioned above which could limit the water available for et as plotted in fig 7a b and c the performance metrics are very close in the experiments dhsvm preclaps dhsvm micromet and dhsvm tmpa uncal implying comparable et simulations this is because on the one hand the discrepancies between the precipitation estimates of the interpolation algorithms and the tmpa product mainly occur in summer fig 6a during which however the et are unlikely subjected to water stress on the other hand the other climatic factors such as the air temperature and humidity and solar radiation which could also affect the et simulations profoundly were kept consistent for all the experiments in dhsvm tmpa uncal the streamflow was overestimated due to the higher precipitation estimates by tmpa to better match with the streamflow observations the additional model calibration in dhsvm tmpa cal reduced the simulated streamflow by increasing the et in summer consequently as marked by the red circle in fig 7e the modeled summer et tend to be overestimated and diverge more significantly from the rs based one in comparison to the experiment dhsvm tmpa uncal leading to a relatively low nse 0 65 in dhsvm imerg uncal the et simulations agree better with the rs based et product than all the other experiments as indicated by the highest nse 0 81 nevertheless the summer streamflow was apparently underestimated in this experiment due to the lower precipitation estimates by imerg the further model calibration in dhsvm imerg cal increased the summer streamflow by decreasing the et hence as marked by the blue circle in fig 7f the summer et simulations become more negatively biased in comparison to dhsvm imerg uncal resulting in a relatively low nse 0 64 5 discussion 5 1 interpolated or satellite based precipitation the interpolation algorithm and the spps are the two major approaches to estimating spatially distributed precipitation a fundamental input for distributed hydrological models this study compared and evaluated the interpolation algorithms and the spps in estimating precipitation at multiple time scales we found that the quasi physically based interpolation algorithms i e preclaps and micromet outperform the tmpa product in capturing daily precipitation particularly in the cold season winter the deficiency of tmpa in reproducing daily precipitation has also been reported in some other studies conducted in qinghai tibet platea e g ebrahimi et al 2017 hussain et al 2017 yan et al 2017 as well as those carried out over the mountainous areas in some other regions such as the upper indus basin khan et al 2018 and the chindwin river basin in myanmar yuan et al 2017 this might be explained by the following reasons to begin with the snow cover within and around the brb could induce strong scattering in winter which would lead to overestimations of precipitation by the passive microwave sensor i e trmm microwave imager tmi ebrahimi et al 2017 xu et al 2017 as indicated by the high positive value of median rbias 26 19 in fig 4 moreover the localized and short term orographic precipitation might not be captured by both the infrared ir and passive microwave instruments on board trmm bharti and singh 2015 xu et al 2017 additionally the other factors such as the time lag and spatial mismatches between the rain gauge and tmpa could also contribute to explain the low performance of tmpa at the daily scale hence the tmpa product cannot be substituted for ground measurements in terms of daily precipitation estimations as suggested by camici et al 2018 nevertheless the tmpa product shows a good performance in reproducing monthly precipitation which is slightly better than the preclaps interpolation algorithm this is partially due to that as mention in the section of 2 2 tmpa has been subjected to a month by month adjustment by using the ground based observations i e the gpcc data and partially due to the relatively lower complexities of the monthly precipitation patterns than the daily one hence the tmpa product still has a good potential in the field of hydrometeorology particularly in the remote mountainous regions without ground based observations the low performance of tmpa in capturing daily precipitation but higher performance in estimating monthly precipitation over the mountainous areas have also been reported in many other previous studies e g ebrahimi et al 2017 hussain et al 2017 yan et al 2017 yuan et al 2019 comparing the two spps imerg consistently outperforms its predecessor tmpa at the daily monthly and annual scales in line with many previous studies e g tang et al 2016 xu et al 2017 yuan et al 2018 as a result the advantage of the interpolation algorithms over imerg for daily precipitation estimations is smaller than that over tmpa at the monthly scale the imerg product even shows a better and comparable performance in comparison to the preclaps and micromet algorithms these results are encouraging and confirm that imerg should be a better alternative than tmpa in estimating precipitation over the data scarce regions nevertheless imerg doesn t exhibit a better performance than tmpa in winter as indicated by the relatively low cc and higher absolute rbias this indicates that more efforts are still needed to improve the imerg product in the cold season over the mountainous areas although the interpolation algorithms outperform the spps in estimating daily precipitation their performance are not very satisfactory as well mainly due to the deficiencies of the interpolation algorithms to begin with the precipitation elevation relationship which is derived from the monthly and annual observations may not be necessarily accurate at the daily scale garen et al 1994 second the precipitation frequency tends to be overestimated owing to that any of the station used for interpolation that have precipitation could lead to the occurrence of precipitation in the interpolated grid cell this drawback might be mitigated by introducing a precipitation occurrence probability within the framework of the interpolation algorithms as did by thornton et al 1997 in addition the other factors such as the topographic aspect and slope can also affect precipitation estimates considerably which however are not considered by the interpolation algorithms hence the interpolation algorithms could be further improved which might enhance their advantages over the spps 5 2 implications for hydrological modeling the interpolated and the satellite based spatially distributed precipitations were used to drive the fully distributed hydrological model dhsvm to assess their hydrological utilities we found that dhsvm with the tmpa based precipitation even after the additional calibration had overall a lower performance than that with the interpolated precipitation in modeling streamflow this indicates that the interpolated precipitation even from scarce rain gauges should be more preferred than the tmpa based one for streamflow simulations in the mountainous brb regarding the imerg based precipitation however we found that the streamflow simulations accuracy could be higher than those with the interpolated precipitation during the validation period if dhsvm was subjected to additional calibration this suggests that imerg provides a valuable alternative for driving hydrological models to simulate streamflow over the data scarce mountainous areas such as the brb in other words imerg could be used as a substitute to the gauge based interpolated precipitation in case that only the streamflow is focused the spps were proved to have a relatively low performance in capturing daily precipitation but a good performance in estimating monthly precipitation it could be inferred that they might be utilized to detect the trends of precipitation and streamflow both of which are of great interest to the managers and policy makers fig 8 depicts the variations and trends of the observed annual streamflows and the simulations generated in different experiments we can see a consistent increasing trend for the period 2001 2007 together with a concurrent decreasing trend for the period 2007 2013 in line with the observations this confirms the usefulness of the spps for determining the sign of streamflow trends although not for accurately estimating the magnitudes of the trends in other words both the tmpa and imerg products could be used as a substitute to the interpolated precipitation if the sign of streamflow trend is targeted the above results also imply that the utilities of the spps depend heavily on the goals of the hydrological modeling it should be noticed that dhsvm with the tmpa based or imerg based precipitation tends to exhibit different performance over different periods in simulating streamflow taking a closer look at the streamflow hydrography fig 5 we can observe that the modeled streamflows in dhsvm tmpa uncal are very close to dhsvm preclaps and dhsvm micromet for the years 2001 2009 and 2012 2013 due to the similar precipitation estimates but they were obviously overestimated in the years 2002 2007 the further model optimization in the experiment dhsvm tmpa cal could therefore not consistently improve the streamflow simulation accuracy for all the years moreover as listed in table 4 the streamflow simulation accuracy in the experiment dhsvm imerg uncal cal is obviously better in the validation period than the calibration period these results indicate that the hydrological utilities of the spps might vary for different time periods underlining the necessity of conducting a hydrological evaluation of the spps for a relatively long period in order to give a more holistic assessment the hydrological utilities of the spps were typically assessed by using only the streamflow observations in many previous studies e g kim et al 2016 yan et al 2017 yuan et al 2017 li et al 2018 duan et al 2019 jiang and bauer gottwein 2019 in this study we have also crosschecked the modeling results with the independent rs based et product interestingly the simulated et match well with the independent rs based et product nse 0 72 and r 2 0 85 in the experiments dhsvm preclaps dhsvm micromet dhsvm tmpa uncal and dhsv imerg uncal although the streamflow simulation accuracy are obviously lower in the latter two experiments than the former two the additional calibration of dhsvm with the tmpa based or imerg based precipitation in the experiments dhsvm tmpa cal or dhsvm imerg cal would enhance the streamflow simulation accuracy substantially as presented in section 4 2 however the simulated summer et would diverge more significantly from the rs based et estimates than those without additional calibration i e dhsvm tmpa uncal or dhsvm imerg uncal thus the improvement of streamflow estimates is actually achieved by sacrificing the et simulation accuracy highlighting that the streamflow observations alone may not be adequate for evaluating the hydrological utilities of the spps this also indicates that the hydrological model driven by the satellite based precipitation if calibrated solely against the streamflow observations might not be adequate for water balance analyses or water accounting since the precipitation estimation errors of the spps could be propagated to some other water balance components such as et as proven in this study 5 3 uncertainties in this study the observations of 12 rain gauges were used to assess the performance of two quasi physically interpolation algorithms in and around the brb through the leave one out cross validation technique the distances between two neighbor rain gauges are mostly less than 50 km which could contribute to explain the reason why the interpolation algorithms could have better performances than the spps in capturing daily precipitation however the distribution of the rain gauges might be very sparse in some other regions such as the northwest qinghai tibetan platea in this case the interpolation algorithms may not still outperform the spps particularly the imerg product considering that the nearest stations may not be able to represent the precipitation variations in the interpolated areas there is another case that the rain gauges are very unevenly distributed in some large watersheds in this case the interpolation algorithm may not be able to capture the spatial variability of precipitation at the watershed scale although it could have a higher performance than the spps when verified against the observations hence the findings of the study might not be applicable to the regions with very sparse or unevenly distributed rain gauges which necessitates further investigations in the future rain gauge density was recognized as an important source of uncertainties in the evaluations of the spps e g amitai et al 2012 tang et al 2018 tian et al 2018 in this study we further assessed the performance of the interpolation algorithm i e preclaps and the spps tmpa and imerg with the increasing numbers of rain gauges more specifically we used the five national level meteorological stations as the baseline and then randomly increased the number of rain gauges to the totally twelve because of the different combinations of the additional rain gauges there would be some variations of the performance metrics for instance there would be totally 15 random combinations when two additional rain gauges are selected from the remaining six ones the results are plotted in fig 9 we can see that at the daily scale the cc exhibits a consistent upward trend with the increasing number of rain gauges for the interpolation algorithm and the spps meanwhile the rmse shows an opposite downward trend the results indicate that the performance of the interpolation algorithm and the spps would increase with more dense rain gauge network thus if fewer rain gauges are utilized the performance of the spps would be underestimated as reported by tang et al 2018 and tian et al 2018 at the monthly scales we can observe that there are no obvious trends of the cc and rmse with the increasing number of rain gauges this implies that the rain gauge density has a relatively low influence on the performance of the interpolation algorithms and the spps at the monthly scale we can also see from fig 9 that the performance metrics of the interpolation algorithm show larger ranges for different combinations of the additional rain gauges compared with those of the spps this is due to the fact that the performance of the preclaps algorithm is not only intimately associated with the rain gauge density but also with their distributions 6 conclusions this study compared the performances of two quasi physically based interpolation algorithms i e micromet and preclaps with the widely used spps i e tmpa 3b42v7 and imerg at multiple time scales in and around the babao river basin brb a meso scale mountainous watershed on the northeast qinghai tibet plateau meanwhile the fully distributed hydrological model dhsvm was used to evaluate the hydrological utilities of the interpolated and the satellite based precipitation through different modeling experiments i e dhsvm preclaps dhsvm micromet and dhsvm tmpa uncal cal and dhsvm imerg uncal cal results indicate that the spps perform worse than the interpolation algorithms in reproducing daily precipitation while they show comparable and even higher performance in estimating monthly precipitation at all the time scales the imerg product outperforms tmpa and meanwhile the micromet algorithm outperforms the preclaps scheme both the interpolation algorithms and the spps exhibit an obvious seasonal variation regarding the performance increasing from spring to summer and autumn and then dropping rapidly in the cold winter dhsvm with the tmpa based or imerg based precipitation when not subjected to additional calibration in the experiments dhsvm tmpa uncal and dhsvm imerg uncal has an apparent lower performance in simulating streamflow compared to those with the interpolated precipitation in dhsvm preclaps and dhsvm micromet however interestingly the et simulations consistently agree well with the independent rs based et product in these experiments as indicated by the higher performance metrics nse 0 72 and r 2 0 85 the additional calibration of dhsvm with the tmpa based or imerg based precipitation in dhsvm tmpa cal and dhsvm imerg cal could enhance the streamflow simulation accuracy significantly the nse could increase by 70 59 109 52 and 86 84 132 respectively at the daily and monthly scales in the validation period nevertheless the simulated summer et would diverge more significantly from the rs based et product than those without additional calibration the improvement of streamflow estimates is actually achieved by sacrificing the et simulation accuracy highlighting that the streamflow observations alone may not be adequate for evaluating the hydrological utilities of the spps particularly if not only the streamflow but also some other water balance components are focused the further analyses revealed that the utilities of the spps depend heavily on the goals of the hydrological modeling moreover the hydrological model with the satellite based precipitation could have varying performance for different periods implying the necessity of conducting a hydrological evaluation of the spps for a relatively long period in addition the performance of the spps and the interpolation algorithms are closely associated with the rain gauge density one limitation of the study is that only one specific watershed was focused thus further work needs to be carried out to verifywhether our results can be generalized in some other mountainous watersheds with similar characteristics credit authorship contribution statement ling zhang conceptualization formal analysis investigation funding acquisition methodology resources visualization writing original draft writing review editing dong ren investigation formal analysis resources writing review editing zhuotong nan resources supervision writing review editing weizhen wang resources supervision writing review editing yi zhao investigation software resources yanbo zhao investigation validation writing review editing qimin ma investigation validation writing review editing xiaobo wu investigation validation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by the strategic priority research program of chinese academy of sciences xda19040504 and xda20100104 the national natural science foundation of china 41901045 and 41471448 and the cas light of west china program 29y929661 the authors would like to thank the uw hydro computational hydrology group at the university of washington for providing the source codes of dhsvm the authors also wish to express their gratitude to the anonymous reviewers for their valuable comments and suggestions on the paper 
5733,more than 25 of the world s population either lives on or obtains its water from karst aquifers which makes the understanding of these systems essential for water resources management however it is often difficult to observe and assess their internal dynamics precisely in this study we investigated 1 the ability of both conceptual model approach and signal decomposition methods to extract meaningful components from karst spring hydrographs and 2 how a modelled spring discharge acquires its spectral signature through a conceptual reservoirs model we used the conceptual modeling software karstmod to model the discharge of a small karst spring in normandy france we compared the model internal discharges with natural tracers of the system conductivity turbidity and piezometry the results suggested that the conceptual model was able to reproduce part of the internal hydrological behavior of the karst system especially the exchange dynamics between conduits and the surrounding aquifer we compared the internal flows of the model with wavelet details obtained from multiresolution analysis of the spring discharge these internal flows appeared strongly correlated to specific consecutive sums of wavelet details finally we studied how the modeled discharge at the spring was acquiring its spectral characteristics across the model using fourier spectra analysis cut off frequencies visible on the fourier spectrum of the spring were matching the frequencies obtained with the multiresolution analysis these results combined illustrate the ability of decomposition methods to reproduce the spectral variations of the different discharges within the model and likely within the observed karst system keywords karst aquifers conceptual reservoir modelling time series analysis signal decomposition methods 1 introduction karst aquifers account for around 25 of the world groundwater resources ford and williams 2007 forming in solute rocks like limestones dolomite and chalk they are characterized by their heterogeneity and a complex interplay between matrix fractures and conduits flow the estimation of this groundwater flow is essential in terms of management conservation and protection of these aquifers as a consequence different mathematical models have been used in order to better understand karst many conceptual models more specifically rainfall runoff reservoirs models are developed based on simple discharge equation between linked reservoirs routing the input signal the rainfall to an output signal the discharge hartmann et al 2014b their structure is generally based on a production function and a transfer function such models are widely used in river hydrology and have sometimes been adapted to karstic hydrology crec models cormary and guilobot 2010 bemer bezes 1976 gardenia thiéry 2003 hymke rimmer and salingar 2006 the reservoir model implemented in the vensim software fleury et al 2007 or one of its modified versions including thresholds based transfer functions pumping discharge or hysteretic behavior tritz et al 2011 geyer et al examined the interrelation of recharge function and spring hydrograph using a two reservoirs model geyer et al 2008 rrawflow long 2015 simulates the response to recharge by convolution and uses impulse response functions irfs some models conceptualize soil epikarst vadose and phreatic zone jukić and denić jukić 2009 reservoir models have also been coupled with conduits model with success chen and goldscheider 2014 five different lumped model structures have been used to assess climate change impact of a karst system over a 30 years period hartmann et al 2012 a modular structure including all those possibilities in order to be adjustable to various karstic systems has also been developed jourde et al 2015 some studies used non linear reservoir modelling to separate the baseflow eris and wittenberg 2015 single reservoirs model showed good performances using an infinite characteristic time transfer function guinot et al 2015 all those lumped models are widely use in hydrology and karst hydrology especially to simulate spring discharge time series to fill gaps or simulate different scenarii however initially they were not designed to finely study internal dynamics of the physical catchment itself some recent approaches have been developed to estimate realism of reservoirs models structures by evaluating performance identifiability and plausibility with respect to karst systems water quality and quantity signatures hartmann et al 2013c 2013b in a similar spirit exploring the links between internal time series of the reservoirs models and observed times series and quality data can be another tool to interpret modelling results explored in this study spectral and correlation methods have also been widely used in karst hydrology since the 1970s and have been powerful tools to achieve a better understanding of karst systems mangin 1984 padilla and pulido bosch 1995 panagopoulos and lambrakis 2006 larocque et al 1998 labat et al 2000a majone et al 2004 mathevet et al 2004 massei et al 2006 delbart et al 2014 li et al 2017 nevertheless they have some limits when it comes to taking into account temporal variability of statistical properties of hydrological time series intermittence for instance other methods have thus been tested out in hydrological context to address this problem especially wavelet analysis used initially in geosciences grossmann and morlet 1984 kumar and foufoula georgiou 1997 they provide a better understanding of hydrologic processes including karst hydrology from a temporal representation and scale labat 2005 wavelet transform continuous wavelet transform and multiresolution wavelet analysis or mra can help to interpret the temporal structures of the response of a karstic basin to rainfall by separating the fast response karstic network draining and a slower response diffuse infiltration labat et al 2000b among other applications transport properties and turbidity dynamics in karstic system have been studied with these methods massei et al 2006 as well as relationships between filtering properties and structural heterogeneity chinarro et al 2010 hao et al 2012 or catchment dynamics impacted by snowmelt mathevet et al 2004 they also enabled to highlight the effect of the north atlantic oscillation on a mediterranean karstic spring andreo et al 2006 and to investigate the effect of climate change on karstic systems charlier et al 2015 while those methods are very commonly used they have seldom been applied conjointly especially in karst hydrology reservoir models have been confronted to neural network with some success kong a siou et al 2014 wavelet analyses have been coupled with surface reservoir modelling by salerno salerno and tartari 2009 but to our knowledge this approach was never used on karstic system this study presents a combined use of those methods with the aim to provide new information for the interpretation of the functioning of karstic systems first we built a conceptual reservoirs model using precipitation and discharge time series measured at the site we used natural tracers time series turbidity as a tracer of conduit flow and electrical connectivity as a tracer of conduit and matrix flow measured at the spring to assess the model realism the hydrological meaning of the internal fluxes between reservoirs we decomposed the spring discharge into different components using multiresolution analysis and compared sums of the obtained details with internal flows from the conceptual model in parallel we studied the propagation of the spectral information within the model from rainfall to spring discharge using fourier spectra analysis the main objectives were to 1 investigate how the statistical spectral properties were acquired within the model and how they propagated through the different reservoirs 2 discuss the ability of the combination of the presented approaches to extract hydrologically meaningful components in order to better understand the behaviour of the studied karst system 2 studied site the norville karst system is located in normandy france near the seine river and between rouen and le havre fig 1 the site has been studied since 1999 dussart baptista et al 2003 fournier et al 2008 massei et al 2003 massei et al 2002 duran 2015 it is part of the national observation network on karstic systems sno karst of the french national center for scientific research cnrs the geology and geomorphology of the site is characteristic of the upper normandy the seine river cuts deeply into chalk plateaus this chalk aquifer is karstified locally on the top of these mesozoic chalks layer the weathered chalk has formed a clay with flint layer quite impervious and in some places covered with quaternary silts swallow holes are penetrating the clay and chalk layers the infiltration of water into the karstified chalk aquifer can be very quick more locally a n70e fault the triquerville fault affects the site with a net slip of 120 m south to the fault the formations are the ones described before while north to the fault cretaceous layers of clay alternate with sand formations the norville study site is a sinkhole spring system upstream the bebec creek drains a watershed of 10 km2 before infiltrating into the ground when reaching the fault fig 1 the bebec creek discharge is characterized by an important seasonal variability from 5 l s during low flow up to 400 l s during storms events downstream the perennial hannetôt spring at the bottom of the chalk cliffs has been proved to be the resurgence of the bebec river with multiple tracer tests karstic conduits are also likely to be found within the chalk aquifer between the spring and the seine river underneath alluvial aquifer hence a hydraulic connection between the karstic aquifer and the seine river is suspected fournier et al 2008 the variations of the level of a piezometer located south of the seine in the same chalk layer are correlated to the variations at the spring this study site is monitored for rainfall using two rain gauges in cumulated rainfall recording mode the first located a few hundred meters from the spring as the main gauge the second one located slightly out of the northern boundaries of the catchment on the plateau showing a small lag 1h with the main one used whenever there were gaps in the time series of the main rain gauge discharge was recorded using doppler flowmeters at the sinkhole site and at the spring with a 15 min time step interval and water level divers associated with a rating curve were used to check the reliability of the final discharge time series conductivity and turbidity were also monitored using a ctd diver and a turbidimeter both at 15 min time step etp was calculated using the thornwhaite formula the time series used in this study cover a bit more than two years 2013 2015 used at hourly time step 3 methods in this study the following approach was chosen 1 conduct a reservoir modelling and extract all internal flows from the model 2 compare those flows with natural tracers data measured at the spring conductivity and turbidity to discuss the realism of the model 3 analyse how the modeled spring discharge acquired its spectral signature within the model to gain knowledge on the model behaviour 4 compare internal flows of the reservoir model assumed realistic with decomposed details of the spring discharge to assess the ability of the decomposition method to access internal dynamics 5 interpret and discuss the consistency of the information provided by both methods used together 3 1 conceptual model and model set up 3 1 1 karstmod in order to perform a rainfall runoff modelling on the norville karstic basin the karstmod model jourde et al 2015 mazzilli et al 2017 was chosen based upon several characteristics i karstmod is a modular bucket style conceptual model that enables to activate deactivate several reservoirs depending on the knowledge of the karstic system ii this model has been developed specifically for karstic groundwater flow simulation by french institution insu cnrs sno karst it has been tested on various karstic systems with success baudement et al 2017 johannet et al 2015 loncar et al 2018 poulain et al 2018 and iii karstmod presents useful features like several possible time steps various discharge equations sensitivity analysis and uncertainty estimation karstmod includes four conceptual possible reservoirs epikarst e conduits c matrix m and low l this last reservoir standing for a lower level or sub system of the saturated zone than can represent for example a deeper system or slower dynamics depending on the conceptual model chosen by the user mazzilli et al 2017 fig 2 different flows can be implemented in order to characterise flow between the different reservoirs linear laws hysteretic behavior and thresholds flows the five balance equations are presented below 1a de dt p e t q loss q el q em q es q ec q hyec q hyes i f e 0 1b de dt p e t i f e min e 0 1c dl dt q el q ls q l pump 1d dm dt q em q mc q ms q m pump 1e dc dt q ec q hyec q mc q cs q c pump with e l and m l the water levels in the compartments e l and m respectively emin the minimum water level in the compartment e l p l t the precipitation rate et l t the evapotranspiration rate q l pump q m pump q c pump l t discharge rates removed from the compartments per unit surface area q loss q el q em q es q ec q hyec q ls q ms q cs q mc are internal discharge rates per unit surface area and the general form is given by the following equation 2a q ab k ab a l ref α ab i f a 0 2b q ab 0 o t h e r w i s e where the notation ab indicates a flow from reservoir a to reservoir b a is the water level in reservoir a is kab is the specific discharge coefficient for the linear discharge law from compartment a to compartment b lref is a unit length and αab is a positive exponent at last the discharge at the spring or outlet q s is given by 3 q s r a q es q ls q ms q cs q hyes q s pump with r a the recharge area of the catchment and q s pump the discharge rate removed at the outlet per unit surface area the model is calibrated using a quasi monte carlo procedure with a sobol sequence sampling of the parameter space sobol 1976 the procedure is stopped when either nmax parameter sets satisfying the objective function wobj greater than the user defined threshold value wobjmin are collected or when the elapsed simulation time reaches the maximum duration tmax specified by the user the user specifies both wobjmin and tmax additional information on the structure of the model threshold function internal fluxes exchange function qmc the available objectives functions calibration and optimization processes is available in the aforementioned references mazzilli et al 2017 and in the karstmod user s guide 3 1 2 model structure and calibration in order to assess the optimal structure of the model 336 tests with each of them 10 to 10 000 simulations were carried out with all possible configurations in a trial and error procedure by activating and deactivating the different reservoirs by allowing or not different internal fluxes hysteretic behavior or not by allowing exponents superior to 1 etc the general summary of these tests is presented in appendix 1 the optimal structure obtained at the end of the procedure included all reservoirs following the configuration presented in fig 2 with linear relationships between reservoirs except out of c to the spring where a power law is possible the warm up period was chosen at 1000 h 40 days the calibration period was chosen as roughly the first hydrological year 1000 h to 10 000 h and the validation as the second year starting just after the end of the drier months 10 001 h to 17040 h the objective function was a combination of nash sutcliffe efficiency nse and the modified balance error be perrin et al 2001 with respective weights of 0 7 and 0 3 the minimal value for the objective function wobjmin was set up to 0 6 while the number of required successful simulations was comprised between 10 and 1000 depending on wobjmin in our study q c pump was implemented into the model as a negative flow in order to represent the bebec creek entering the karstic system at the sinkhole the range of allowed variations for all parameters during the calibration once the model structure was selected is detailed in table 1 after the selection of the optimal configuration the kling gupta efficiency coefficient kge gupta et al 2009 kling et al 2012 was also calculated to further evaluate the performance of the model independently this new efficiency coefficient has been used recently by different authors in hydrology and karst hydrology because the orthogonal set up of the kge allows to separately compare correlation variability and bias of simulations and observations hartmann et al 2013c some of the problems than can rise due to a calibration based only on the nse for example its use of the observed mean as baseline which can lead to overestimation of model skill for highly seasonal variables see gupta et al 2009 are avoided here by the use of the multi objective function for the calibration and by the use of an independent efficiency coefficient a posteriori the kge aside from the performance criteria additional quantitative and qualitative evaluations of the model to reproduce the spring flow were carried out assessment peak flows baseflow the physical realism of the model was further explored by comparing internal flows with observed time series conductivity turbidity piezometry these proxy variables enabled to assess the ability of the model to capture real internal dynamics direct transfer drainage of the system exchanges between compartments and long term trends 3 2 signal analysis and processing we used different signal analysis methods on the simulated discharges classic fourier transform spectrum analysis and a signal decomposition method using wavelets the multiresolution analysis mra they were used to assess how the spectral information was transmitted and modified through the reservoir model and to determine if the components of the discharge signal or combination of those components obtained through mra could be compared with the internal discharges between the reservoirs of the model e m c and l thus gaining knowledge on the model behavior 3 2 1 fourier spectra fourier spectra of the different inputs and output of the model rainfall bebec discharge and spring discharge along with all the internal discharges were analyzed the fourier spectrum of a stochastic signal process represents the distribution of energy variance e of the process depending on the frequency ω with often e ωβ the β coefficient can give information on the signal β 0 is characteristic of a white noise energy and frequency are independent β 1 is a pink noise natural phenomenon of large spatial and temporal scales β 2 is characteristic of a red noise the successive points are independent for one another but they follow a statistical law β 2 is black noise the signal is not statistical but characteristic of exceptional events interrupting periods when another law was applying between 1 and 1 the signal is gaussian between 1 and 3 it is brownian a watershed transforms rainfall into discharge roughly from white to red noise if a system is partly linear this can be modelled by a convolution between the input rainfall and the pulse response function of this system resulting in a discharge variability at the outlet then the slope βout of the discharge spectrum shows the intensity of the smoothing of some frequencies by the system in comparison with the slope of the source signal here rainfall βin if a system is filtering primarily high over low frequencies the output fourier spectrum would show a change in slope with different β coefficients in the case of one change of slope occurring across a system two coefficients can be extracted from the discharge spectrum β1 for lower frequencies and β2 for higher frequencies by analysing the different fourier spectra and their associated beta coefficients one can describe the behaviour of the studied system and the different statistical laws applying to each part of the frequency domain of the time series separated by the cut off frequencies for which the linear adjustments for each beta coefficient intersect most hydrologic time series fourier spectrum including karst can be described with one two or three statistical laws dolgonosov et al 2008 fournillon 2013 hardy and beier 1994 labat et al 2000a mathevet et al 2004 the cut off frequency enables to define several spectral domains within the studied signal the method to adjust the linear law on the spectrum can be empirical labat et al 2000a or with other statistical methods little and bloomfield 2010 in this study the linear adjustments were done using an algorithm in r to detect change points killick et al 2012 lavielle 2005 before calculating the associated coefficients this approach was applied for the fourier spectra of the input in the model rainfall of the intermediary discharges in and out of each reservoir qec qem qmc etc and of the output discharge qsim in bi logarithmic scale 3 2 2 signal decomposition method multiresolution analysis mra wavelet multiresolution analysis mra was used to decompose the spring discharge in order to compare its spectral components with internal flows of the model and to study more precisely how well the model could reproduce all the spectral information contained in the observed data and what knowledge on the model and the karst system could be gained wavelet multiresolution analysis allows to project a signal on an orthogonal basis of wavelet and scale functions generated from a filter band following a dyadic scale and to build for each level of this scale the component of the signal that explains the variability at this scale the decomposition is orthogonal and therefore the sum of all components details and residue or smooth gives back the initial signal multiresolution analysis has been used quite extensively to study the stationarity and the different scales of variability in hydrologic and meteorological time series coulibaly and burn 2004 liu et al 2011 nakken 1999 smith et al 1998 more specifically in karst hydrology labat investigated the relationship of the details of multiresolution analysis of rainfall data and discharge data by cross correlation labat et al 2002 in our study the signal decomposition of the spring discharge was obtained using symlet 8 modified daubechies wavelets with increased symmetry resulting in twelve details and a residue or smooth these details were compared with the internal flows of the karstmod model first separately one detail compared to one internal discharge then by combining them sums of details compared with the internal discharges a script was built in r to test all combinations of details and single out the ones showing the best correlation coefficients with the internal flows 4 results of the combined approach 4 1 outputs and performance of the model the set of parameters associated with the best score of the objective function in karstmod is provided in table 2 the objective function final value was 0 75 one specificity of this configuration in regard with other tested french karstic systems of the insu network at the time of this work is the necessity of the implementation of the reservoir l in order to provide a satisfying efficiency this reservoir corresponds conceptually to the baseflow component of the spring discharge supplied by the local porous chalk aquifer this hypothesis has been checked using the eckhardt filter as described in appendix 1 the observed and simulated discharges of this final and optimal configuration are presented in fig 3 the kling gupta efficiency coefficient kge was calculated to further evaluate the performance of the model and its value was 0 71 slightly under the objective function the parameters associated with the kge coefficient are the correlation coefficient r its value is 0 78 the bias ratio β here 0 97 and the variability ratio γ 0 82 the mass balance is 0 9 in calibration period 6 in validation period and 3 4 overall the amplitude of high flows is acceptable on most flood events albeit slightly underestimated on three sets of events one way to estimate the ability of the model to reproduce higher flows correctly is to calculate the nash coefficient of the logarithm of the discharge here its value is 0 62 the simulated baseflow range of values is satisfying even though the end of recessions preceding lowest flows is slightly over estimated a portion of the lowest flows of the second hydrologic year is missing data gap which might have impacted the calibration of low flows and baseflow more specifically however the analysis and the comparisons described in the next sections are focusing on high flow events the internal discharges at the outlet of the four reservoirs e l m and c are presented on fig 4 it can be seen that the flow out of l is very smooth while the variability of qcs flow out of c at the spring is much higher and integrates the joint influence of qpump bebec creek discharge input as a negative pumping discharge qmc flow between m and c and qec flow from e to c the discharge qmc is particularly interesting as it represents the exchange between m and c and is alternatively positive and negative pulses from c to m and more gradual and continuous from m to c the discharge qms from reservoir m to the spring is extremely low mean qms 0 1 mean spring discharge most of the flow out of reservoir m is going to c via qmc in this configuration i reservoir l produces the baseflow ii reservoir c receives direct input from the bebec creek via negative pumping in c and delayed input through reservoir e and iii reservoir m enables to simulate correctly the multi dynamics observed in the data through a complex exchange flow 4 2 hydrological significance and consistency of modeled internal fluxes after the internal components of the model were singled out the next step was to assess if the temporal variability of these components could be representative of real flow dynamics within the karst system or if they were just fitting variables within the model the aim was to test the realism of the model by comparing internal flows and reservoirs levels with observed time series giving information on the aquifer dynamics turbidity conductivity piezometry the exchange flow between reservoirs m matrix and c conduits qmc was divided in its positive and negative components discharges qmc and qmc these two discharges were compared to conductivity and turbidity variations measured at the spring these comparisons were targeting extreme events when this inversion of the direction of flow was occurring the default behavior being a flow from m to c with occasional inversion from c to m in high flows conditions figs 5 and 6 present these variations on a restricted number of events but similar observations were made on most major events on fig 5 january 2015 the conductivity shows several quick drops consistent with spring discharge and rainfall events simultaneously qmc is peaking and qmc goes to zero that is to say an inversion of the flow between m and c occurs after the peaks the conductivity and the discharge qmc are showing similar variations they raise in the absence of rainfall events and drops in case of rainfall events the intercorrelation between conductivity and qmc on the whole event is 0 62 with a 14 h lag it is relevant to stress that qmc can be seen as a synthetic integrated discharge averaging the flow from reservoir m to c in the model this fraction of the flow still has to go across reservoir c before reaching the outlet i e the spring where the conductivity is observed so a lag between those two time series is consistent fig 6 january 2014 represents turbidity along with internal discharges several observations can be made i the two peaks of qmc are simultaneous with turbidity peaks ii each turbidity peak is associated with a drop in qmc and iii even though peaks are much correlated the dynamics of turbidity and qmc are quite different turbidity and qmc are anti correlated with a value of 0 45 and a lag of one hour it appears that at an event scale the conceptual model is showing consistency with the internal functioning of the karst system as some internal discharges are well correlated to natural tracers this point is further developed in the discussion below apart from events dynamics the ability of the model to reproduce a general trend was evaluated focusing on the evolution of reservoir l interpreted as seen in previous section and appendix as providing for the baseflow the evolution of water level in reservoir l was compared to the evolution of the piezometry within the chalk aquifer on the whole data set in terms of frequency content not amplitude the result is presented fig 7 both data sets show the same trend steady rise with a correlation coefficient of 0 94 and similar shape of variation the chalk aquifer was storing water during the two years of observations in the model reservoir l providing the baseflow of the simulated discharge is showing a similar trend for the optimal simulation still the data set available two years is a bit short in regards to the dynamics of variations within the chalk aquifer and those results should be tested on a larger time period to conclude on the performance of the conceptual model on this point the increase in l might be an artifact for this specific simulation in conclusion several elements tend to show the ability of the conceptual model developed to reproduce part of the internal behavior of the studied karstic system as internal components of the model are showing interesting and physically interpretable correlations with natural tracers in the system both on event scale and larger time scale the next step was to further study the model behaviour and more specifically its internal components using signal analysis methods 4 3 linking spectral characteristics of spring discharge to the modelled hydrological functioning subsequently we investigated the way the spectral information was propagating within the hydrological model our aim was to understand how the spectral information characterizing the output signal was acquired along the modeled hydrological flow path we studied the evolution within the model of spectral characteristics by analyzing the internal fluxes from the different reservoirs and the simulated discharge at the outlet of the conceptual model we then compared the internal fluxes of the model with details from the multiresolution analysis of the spring discharge in order to highlight possible common information obtained from both approaches and discuss the ability of multiresolution decomposition methods to extract hydrologically meaningful statistical components from the sole output signal here the modelled spring discharge 4 3 1 propagation of the spectral information through the hydrological model all fourier spectra of the different discharges are given in fig 8 two domains were deemed sufficient to fit all the spectra even though for qms qmc and less clearly qcs and qsim 3 slopes could also fit fell the data in order to make the comparison of the spectra and the analysis of the evolution of the filtering across the model more straightforward the evolution of β coefficient through the different reservoirs is illustrated and summarised on fig 9 on the left part of this figure an example of fourier spectrum of output discharge of one reservoir e with one change of slope two associated β coefficients β1 on the left part of the spectrum low frequencies and β2 on the right part of the spectrum high frequencies is given with an illustration of the modification of one slope through one reservoir when filtering occurs in high frequencies low periods the slope β is accentuated as the power of filtered frequencies lowers on the right part of this figure a graph sums up the changes in β coefficients fourier spectrum slope through all reservoirs from the input signals rainfall and sinkhole discharges to the final output signal simulated discharge at the spring the cut off periods in hours of the fourier spectra are indicated in the lower part for each signal it can be seen that β1 low frequencies and β2 high frequencies coefficients are affected differently across the model one could expect the input rainfall signal to have a flat spectrum β 0 as the rainfall is usually described as a white noise however here the rainfall fourier spectrum is not perfectly flat with a break of slope around the period 50 h this is due to the autocorrelation inside a rainfall event if it is raining several days in a row the rainfall from one day to another is correlated to the day before or after and has to be put in perspectives with the specificity of the regional hydrology in normandy still its two β coefficients are small in absolute values going through the model the rainfall is an input into reservoir e which modifies is into qec discharge out of e it can be seen on fig 9 that e has a major impact on both β coefficients reservoir e plays an important part in the acquisition of spectral characteristics of the output signal the second input signal is the discharge of the bebec creek entering the sinkhole this discharge does not cross reservoir e but enters directly reservoir c in the model this part of the signal is filtered only through reservoir c the output of l shows just one slope at last the m reservoir has a complex role as it interacts with c reservoir as well as e reservoir the major effect is on average to high frequencies β2 coefficients with much higher slopes than for the output from e this can be interpreted as a prevalent filtering of the average to high frequencies occurring in reservoir m eventually output signal qcs and qsim are presenting the same slope rupture and almost the same β coefficients the β1 coefficients of discharges out of l m and c reservoirs are in the brownian domain while out of reservoir e is just at the limit between gaussian and brownian it means that for low frequencies or longer time scales the simulated discharges are stochastic structured and fractal all the β2 coefficients high frequencies short time scales are in the brownian domain or beyond the coefficients in gaussian domain are the β1 and β2 of the rainfall β1 of qsinkhole from the bebec stream and just at the limit β1 of qmc which is impacted by qsinkhole in this domain the signals are purely stochastic and non structured for the discharges it indicates that for very short time scales the measuring devices might have a stronger influence than the natural system eventually the signals with β2 coefficients below 3 are characterised as black noise discharges out of m and c for short time scales these discharges are influenced by exceptional phenomena like specific floods this figure helps to analyse how the acquisition of the spectral characteristics of the simulated signal at the spring is related to the filtering through the reservoirs with the following precisions reservoir e has a general impact on the modification of the signal with a prominent filtering on low and intermediate frequencies to be compared with the conceptual interpretation of e as providing the diffuse infiltration component reservoir m interacting with reservoir e and c has a major impact on high frequencies smoothing them reservoir l contributing to the base discharge might slightly structure the low frequencies same β1 than qsim but does not have a major impact on the spectral signature of the final simulated discharge it does provide in amplitude though for the baseflow the input signal from the sinkhole bebec creek might explain why the qmc β coefficients are slightly higher than qms because qmc is a two ways flow between m and c with the input from the bebec bringing higher frequencies this analysis highlights the effect and relative importance of the different reservoirs on the spring discharge and how it acquires its characteristics 4 3 2 spectral components of spring discharge vs internal modeled hydrological components the mra decomposition method was then used to assess whether the information given by the wavelet details of multiresolution analysis could be linked to internal information of the reservoirs model as the conceptual model was shown to be able to reproduce some dynamics of the system as seen in section 4 2 this study was aiming at determining if signal analysis methods could also be used to further interpret the real internal hydrological functioning of a karstic system the simulated discharge and the observed discharge had a very close spectral content when compared detail to detail as further explained in the appendix 1 from there on it was possible to test combinations of details with internal discharges from the model indeed as shown by the fourier analysis in the previous section the different reservoirs models are filters in complex interactions transforming differently the flow spectral signature across the model so the approach was to assess if it was possible to compare combinations of details from the spring discharge that would match the flows out of each reservoirs every combination of possible wavelet details sum from multiresolution decomposition of the simulated discharge qsim details noted d1 to d12 was compared to each internal discharge the results are presented in table 3 the comparison between the optimal sum of details and the internal discharge is illustrated in fig 10 with discharge out of reservoir e qec the internal discharges of the reservoirs model are well correlated to the sums of details of the multiresolution analysis above 0 73 1 for all discharges except qmc the optimal signals are sums of consecutive frequencies and the separation is clear depending on the internal discharge considered the discharge out of e excludes the highest frequencies when the discharge out of m also excludes intermediate frequencies this is consistent with the position of the reservoirs and their supposed hydrogeological behavior it is not surprising that qmc is not well correlated to the sum of details of qsim as it is the discharge out of c to m so this discharge is not included in qsim as it is these results indicate that the multiresolution wavelet analysis is able to extract components that when grouped together is a specific way are showing the same variations or spectral content than internal discharges from reservoir modelling out of e m and c assuming that these internal flows between e m and c are reflecting the internal behavior of the system as suggested in section 4 2 it would imply that this decomposition method of the spring discharge could also help access some understanding of internal dynamics this will be further discussed in the next section 5 discussion 5 1 comparison between frequencies extracted from fourier spectra and from karstmod flows the results of the different approaches presented previously spectral analysis of the signals within the model reservoirs versus multiresolution analysis of the spring discharge show some interesting similarities and convergence points when superposing the frequencies associated to the best sums of detail of multiresolution analysis with the fourier spectrum of the final output of the model qsim it can be seen that they match the cut off frequency marking a break of slope of the fourier spectrum fig 11 the sum of details from 5 to the smooth best correlated to qec corresponds to the period 55 h and above this cut off period is the same for both qec and qsim the second characteristic period put in evidence in the previous section detail 7 associated with output discharge of m t 200 h can be compared with one of the cut off period for reservoir m tc 155 h for qms this cut off period is comprised between detail 6 t 110 h and detail 7 t 200 h interestingly it could also be associated with a second break of slope in the fourier spectra if two breaks of slopes and three linear regression were calculated from the spectrum as illustrated on fig 11 instead of two 5 2 hydrological interpretation of the effects of the reservoirs the similar variations between internal flows and conductivity and turbidity observed time series can lead to a possible interpretation of the hydrologic behavior of the karst system as follows during major events the karstic conduit cannot transfer all incoming discharge and becomes pressurised while usually the conduit is draining the connected chalk aquifer when the discharge within the conduit increases a threshold effect appears and the conduit starts to feed the fissured matrix nearby or upper paleo conduits connected punctually both geological settings being likely in the area rodet 2007 in these high flow conditions turbidity peaks can occur and electrical conductivity drops while in the model we have an inversion of the flow between reservoirs m and c from c to m when pressurized with a pulsatile qmc when the discharge within the karstic conduit decreases the conduit network goes back to draining the matrix and the conductivity rises slowly while in the model the flow switches back from m to c with a flow qmc following the behavior of conductivity at a time when the conduits would likely drain the surrounding aquifer the lag observed between this flow out of m and the conductivity observed at the spring 14 h would be roughly the time for the conceptually integrated along the system qmc flow to go across reservoir c to reach the outlet it can be put in perspectives with observed transfer dynamics in this karst aquifer tracer tests carried out at the same time of this study gave appearance time of tracers between 13 and 20 h from sinkhole to spring field observations at the sinkhole also concur with this interpretation of a saturation effect above 50 70 l s the sinkhole cannot absorb entirely the creek flow and starts filling up table 4 sums up the supposed physical internal functioning of the karstic system in parallel with the conceptual model internal flux variations more widely the conceptual role of reservoir e in providing the conceptually delayed diffuse infiltration component of the flow is consistent with its observed behaviour of filtering the higher frequencies short time scale it is also interesting to note that the reservoir e is transforming the rainfall signal its input signal into qec in a very similar way than a surface watershed the beta coefficients out of e are almost identical to the beta coefficients of qsinkhole measured discharge at the sinkhole result of the transformation of the rainfall into river discharge in the surface watershed of the bebec stream reservoir m has a more complex effect on the spectral signature of the signal its interaction with reservoir c and the high frequencies brought by qsinkhole shows a predominant filtering of higher frequencies periods below 53 h however its general dynamics have also an impact on intermediate frequencies as illustrated by the cut off period of qms at 155 h behavior comparable to the discharge qmc when m is drained by c not shown here as it is not a continuous signal the β2 coefficients of qsim and of the discharges out of m are in beyond the brownian domain and these parts of their signals are black noises as illustrated in section 4 3 1 these beta values are associated with exceptional events like floods for example they can be associated with the activation of an upper part of the conduits networks fournillon 2013 the pressure conditions within the system leading to possible water exchanges with the fissured matrix in this chalk context can be another possibility it is interesting to compare the information given by the spectral analysis exceptional events having an influence on higher frequencies with the information provided by the model threshold effect for major flood events leading to impulse discharges towards m and with what can be observed on the catchment itself extreme floods causing the sinkhole to overflow likely causing the conduit system to be fully pressurized lastly reservoir l is associated with the smooth of the multiresolution analysis and might have an influence on the lower frequencies of the simulated discharge as seen previously with the comparison with natural tracers the conceptual model seemed to reproduced reasonably well part of the internal hydrological functioning of the karstic system and its different dynamics the flow between m and c is well correlated to the measured conductivity signal while the impulsions occurring at the time of sudden inflow due to rainfall events is associated with an inversion of flow from c to m a drop in conductivity and turbidity peaks furthermore the multiresolution details combinations reproduce well the spectral information of the reservoir model with cut off frequencies readable on fourier spectrum the sums of details of mra are showing the same variations than the internal discharges of the conceptual model even though they do not match amplitude of the signal for each internal discharge this leads to the hypothesis that the signal analysis methods especially the modal decompositions methods are also able to give access to information from within the system and not only the final discharge at the spring if these results can be observed on other karstic systems this combination of methods might be used to interpret physically the functioning of a karstic system while compared and validated with various tracers analyses tracers tests analyses of conductivity turbidity and other biogeochemical data furthermore it supports the interest of developing new methods integrating quality data and different signatures to better constrain models and performance estimation as tested in the literature gupta et al 2008 hartmann et al 2013b 2013a the observations from high flows events measured at the spring discharge conductivity turbidity in parallel of the simulated internal flows through karstmod can lead to the following final interpretation of a flood event fig 12 on the different parts of the graph various stages following an important rain event on the studied system are illustrated 1 the steady state conditions with the sinkhole and the perennial flow contributing to the spring and the karstic conduit draining the chalk aquifer 2 the beginning of the flow event with an increase of the river flow resulting in an increase of the karstic spring discharge 3 following the discharge rise a turbidity peak occurs and the conduit is under water heads conditions above a certain threshold it would start feeding the surrounding matrix or upper conduit system as the internal discharge from the model qmc goes from c to m 4 at the end of the flow event the flow reverses and the surrounding matrix feeds the karstic network qmc in the model going from m to c these four steps are the interpretation of two sets of curves presented above the simulated data from karstmod internal discharges and the measured data at the spring turbidity and conductivity to elaborate further on the physical interpretation on the role of the model reservoirs the reservoir m can be seen as matrix well connected to the karstic network fissured chalk and or connected to more delayed infiltration points alteration roots in chalk and possible perched aquifers which is consistent which conceptual models of the development of chalk aquifers in this area rodet 2014 alternatively it could be interpreted as a secondary karstic network paleo conduits activated only in above a threshold but this last interpretation is less likely within the context of the site duran 2015 reservoir l would be interpreted as the more inertial chalk aquifer very low frequency and would contribute to the low frequencies part of the spectral signature of the discharge signal and essential to the simulation as it provides the baseflow of the spring discharge its signal is highly correlated to piezometer data the hypothesis of an exchange flow between the conduit network and the fissured chalk matrix happening in the karst network as it is observed in the model pulsatile in one direction and delayed in the other is interesting to discuss in regard of different references of the literature characterizing exchange flow in karst context examples of exchange between matrix and conduits during floods have been documented using geochemical analysis martin and dean 2001 and linked with an inversion of the hydraulic gradient between conduits and matrix see for example bailly comte 2009 reimann et al 2011 numerical models have been developed to investigate the impact of head gradients between the different parts of the system with for example the importance of this exchange flow on the karstification bauer et al 2000 or the role of the pressure transfer between conduit and aquifer porosity on the different responses at the spring bailly comte et al 2010 interpretation techniques based on tracer tests and or spatially lumped models like karstmod in this study enable to estimate physical non equilibrium exchange between conduits and matrix hartmann et al 2014a the role of the conduit interfaces pressure transfer and karst geometry on the exchanged water was explored by validating groundwater model with stable water isotopes binet et al 2016 the transient storage around the conduit described by these authors with an increasing pressure head in the conduit creating a transient local gradient around the conduit that blocks the water coming from the aquifer could similarly explain the observations and modelling results in our study 5 3 possible bias and limitations of the proposed approach these links between flows within the model spectral information and observed behaviour of the catchment are promising still several possible limits and bias need to be addressed the configuration and parameters set of the model used in this study were the ones showing the best performance it is not a unique solution however many models might encounter issues about the unicity identifiability and stability of the problem solution ebel and loague 2006 resulting in the fact that different representations of the modelled system can be considered as acceptable beven 2006 karstmod enables to investigate the parametric equifinality in several ways for each satisfying set of the sobol sequence the values of objective function are plotted against the values of parameters values of the performance criteria and various variance and sensitivity indexes to asses influence of the parameters on the model output to detect over parametrization mazzilli et al 2017 the use of a combined objective function nse and mass balance and the testing of all model configurations to single out the most efficient and scarce in number of parameters can limit the risk of over parametrization perrin et al 2001 which was done in this study but there is still as subjective choice in the multi objective function and uncertainties related to data measurement for example the conductivity and the turbidity time series were also used to assess qualitatively the behavior of the model another possible bias to underline is the rainfall data which already presents a small slope break near t 50 h this could be explained by the autocorrelation within a same rainy event the hydrology of the study zone is characterized by events lasting on a few days in a row other possible ameliorations may include slight modifications of the conceptual model allowing various loss discharges and not only in e even though the downstream influence tide is not likely to be integrated in the model the influence of the tide can account for approximately up to 5 of the discharge massei et al 2006 this might explain why a higher performance could not be achieved in the case of the studied catchment the simulation and analysis would also need to be operated on longer time series in order to check the trends highlighted such as reservoir l in parallel of piezometry as the regional variation of the water level in the chalk aquifer is pluri annual some of the findings of the present study are site related and might not be observed on other karst systems very specific patterns of flow inversion in parallel with hydrogeological context of the site however the approach presented might yield other insights on the behavior of many different karst systems the study of the modification of the spectral signature across a model here bucket style rainfall runoff model can bring further information and could also be applied to other types of models including semi distributed or distributed models the correspondence between cut off frequencies from the multiresolution analysis of the spring discharge and the internal flows of the model could be investigated on any karst catchment and for any other relevant variables 6 conclusion this study investigated the combined use of a signal decomposition analysis method multiresolution analysis and of a reservoir conceptual model karstmod to attempt to extract meaningful hydrological information from a karst spring discharge the internal discharges out of the model reservoirs were compared with both observed time series and with combination of the details of multiresolution analysis the results showed that the internal discharges of the conceptual model were consistent with observed times series at the spring turbidity conductivity and piezometry suggesting that the model was partly able to reproduce some internal dynamics taking place within the karstic system in parallel the spring discharge was studied through signal analysis and decomposition methods fourier analysis wavelet multiresolution the change of slope in fourier spectra for both observed and simulated discharge was analysed in regard with different hydrological behaviors of the karst system the sum of consecutive details from the cut off frequencies of visible on the spring discharge fourier spectrum up to the smooth is highly correlated to the internal discharges out of the reservoir e and respectively m from the conceptual model the way the input signal rainfall is gaining its spectral characteristics has been studied in detail with the modification of the slope of fourier spectra with its two associated β coefficients through the model this highlighted the different influences of reservoirs e epikarst m connected fractured matrix and l less connected matrix in the filtering of specific band of frequencies to obtain the final spring discharge signal on the specific studied karstic system this new combined approach confirmed or widened the interpretations about its hydrological behavior first the modular conceptual model highlighted the need for four different reservoirs adding a slow responding reservoir l to the pre existing ones e m and c this can be put in perspective with the particularity of normandy aquifers with a transmissive porous chalk aquifer combined with karstified systems and the observed changes in the piezometry of the aquifer this approach also revealed an alternate flow between reservoirs m and c illustrating the dominant draining of m by c and occasional inversion of the flow out of reservoir c toward m interpreted as the surrounding well connected matrix or secondary conduits network paleo karst or typical regional infiltration system these results open various perspectives this combined approach could be used on longer time series as well as other systems in order to check its general application the comparison with natural tracers could be widened using diverse biogeochemical data or tracing tests isotopes temperature analysis of common elements the use of multiple hydrologic signatures as characteristics of the system response gupta et al 2008 hartmann et al 2013b hartmann et al 2012 wagener et al 2007 yilmaz et al 2008 combined with the approach described in this study might provide additional understanding on the karst systems behaviours the use of a multi objective calibration with additional output variables including conductivity geochemical data isotopic data etc as investigated by many authors kuczera and mroczkowski 1998 seibert and mcdonnell 2002 son and sivapalan 2007 vaché and mcdonnell 2006 could also lead to further studies one of the perspectives of the combined application of multiresolution analysis and lumped modelling is another tool to discuss the performance of such a mumped bucket style model the mra and the analysis of fourier spectra modification across the model can be another way to assess the ability of the lumped model to simulate a discharge signal in all its spectral content to go further the use of distributed physic based models would prove very useful if internal flows of conceptual models and components of multiresolution analysis could be compared to internal flows within a numerical distributed model it could widen the use those methods which are much less data demanding and computer time consuming to improve the understanding of the functioning of karstic systems in terms of internal dynamics credit authorship contribution statement lea duran conceptualization methodology formal analysis investigation software writing original draft nicolas massei supervision validation writing review editing nicolas lecoq software validation matthieu fournier supervision validation david labat validation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was undertaken within the framework of the chalk karst observation site part of the karst observatory network www sokarst org initiated by the insu cnrs which aims to strengthen knowledge sharing and to promote cross disciplinary research on karst systems via intensive data monitoring and sharing this work was supported by the haute normandie region france appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2020 124625 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
5733,more than 25 of the world s population either lives on or obtains its water from karst aquifers which makes the understanding of these systems essential for water resources management however it is often difficult to observe and assess their internal dynamics precisely in this study we investigated 1 the ability of both conceptual model approach and signal decomposition methods to extract meaningful components from karst spring hydrographs and 2 how a modelled spring discharge acquires its spectral signature through a conceptual reservoirs model we used the conceptual modeling software karstmod to model the discharge of a small karst spring in normandy france we compared the model internal discharges with natural tracers of the system conductivity turbidity and piezometry the results suggested that the conceptual model was able to reproduce part of the internal hydrological behavior of the karst system especially the exchange dynamics between conduits and the surrounding aquifer we compared the internal flows of the model with wavelet details obtained from multiresolution analysis of the spring discharge these internal flows appeared strongly correlated to specific consecutive sums of wavelet details finally we studied how the modeled discharge at the spring was acquiring its spectral characteristics across the model using fourier spectra analysis cut off frequencies visible on the fourier spectrum of the spring were matching the frequencies obtained with the multiresolution analysis these results combined illustrate the ability of decomposition methods to reproduce the spectral variations of the different discharges within the model and likely within the observed karst system keywords karst aquifers conceptual reservoir modelling time series analysis signal decomposition methods 1 introduction karst aquifers account for around 25 of the world groundwater resources ford and williams 2007 forming in solute rocks like limestones dolomite and chalk they are characterized by their heterogeneity and a complex interplay between matrix fractures and conduits flow the estimation of this groundwater flow is essential in terms of management conservation and protection of these aquifers as a consequence different mathematical models have been used in order to better understand karst many conceptual models more specifically rainfall runoff reservoirs models are developed based on simple discharge equation between linked reservoirs routing the input signal the rainfall to an output signal the discharge hartmann et al 2014b their structure is generally based on a production function and a transfer function such models are widely used in river hydrology and have sometimes been adapted to karstic hydrology crec models cormary and guilobot 2010 bemer bezes 1976 gardenia thiéry 2003 hymke rimmer and salingar 2006 the reservoir model implemented in the vensim software fleury et al 2007 or one of its modified versions including thresholds based transfer functions pumping discharge or hysteretic behavior tritz et al 2011 geyer et al examined the interrelation of recharge function and spring hydrograph using a two reservoirs model geyer et al 2008 rrawflow long 2015 simulates the response to recharge by convolution and uses impulse response functions irfs some models conceptualize soil epikarst vadose and phreatic zone jukić and denić jukić 2009 reservoir models have also been coupled with conduits model with success chen and goldscheider 2014 five different lumped model structures have been used to assess climate change impact of a karst system over a 30 years period hartmann et al 2012 a modular structure including all those possibilities in order to be adjustable to various karstic systems has also been developed jourde et al 2015 some studies used non linear reservoir modelling to separate the baseflow eris and wittenberg 2015 single reservoirs model showed good performances using an infinite characteristic time transfer function guinot et al 2015 all those lumped models are widely use in hydrology and karst hydrology especially to simulate spring discharge time series to fill gaps or simulate different scenarii however initially they were not designed to finely study internal dynamics of the physical catchment itself some recent approaches have been developed to estimate realism of reservoirs models structures by evaluating performance identifiability and plausibility with respect to karst systems water quality and quantity signatures hartmann et al 2013c 2013b in a similar spirit exploring the links between internal time series of the reservoirs models and observed times series and quality data can be another tool to interpret modelling results explored in this study spectral and correlation methods have also been widely used in karst hydrology since the 1970s and have been powerful tools to achieve a better understanding of karst systems mangin 1984 padilla and pulido bosch 1995 panagopoulos and lambrakis 2006 larocque et al 1998 labat et al 2000a majone et al 2004 mathevet et al 2004 massei et al 2006 delbart et al 2014 li et al 2017 nevertheless they have some limits when it comes to taking into account temporal variability of statistical properties of hydrological time series intermittence for instance other methods have thus been tested out in hydrological context to address this problem especially wavelet analysis used initially in geosciences grossmann and morlet 1984 kumar and foufoula georgiou 1997 they provide a better understanding of hydrologic processes including karst hydrology from a temporal representation and scale labat 2005 wavelet transform continuous wavelet transform and multiresolution wavelet analysis or mra can help to interpret the temporal structures of the response of a karstic basin to rainfall by separating the fast response karstic network draining and a slower response diffuse infiltration labat et al 2000b among other applications transport properties and turbidity dynamics in karstic system have been studied with these methods massei et al 2006 as well as relationships between filtering properties and structural heterogeneity chinarro et al 2010 hao et al 2012 or catchment dynamics impacted by snowmelt mathevet et al 2004 they also enabled to highlight the effect of the north atlantic oscillation on a mediterranean karstic spring andreo et al 2006 and to investigate the effect of climate change on karstic systems charlier et al 2015 while those methods are very commonly used they have seldom been applied conjointly especially in karst hydrology reservoir models have been confronted to neural network with some success kong a siou et al 2014 wavelet analyses have been coupled with surface reservoir modelling by salerno salerno and tartari 2009 but to our knowledge this approach was never used on karstic system this study presents a combined use of those methods with the aim to provide new information for the interpretation of the functioning of karstic systems first we built a conceptual reservoirs model using precipitation and discharge time series measured at the site we used natural tracers time series turbidity as a tracer of conduit flow and electrical connectivity as a tracer of conduit and matrix flow measured at the spring to assess the model realism the hydrological meaning of the internal fluxes between reservoirs we decomposed the spring discharge into different components using multiresolution analysis and compared sums of the obtained details with internal flows from the conceptual model in parallel we studied the propagation of the spectral information within the model from rainfall to spring discharge using fourier spectra analysis the main objectives were to 1 investigate how the statistical spectral properties were acquired within the model and how they propagated through the different reservoirs 2 discuss the ability of the combination of the presented approaches to extract hydrologically meaningful components in order to better understand the behaviour of the studied karst system 2 studied site the norville karst system is located in normandy france near the seine river and between rouen and le havre fig 1 the site has been studied since 1999 dussart baptista et al 2003 fournier et al 2008 massei et al 2003 massei et al 2002 duran 2015 it is part of the national observation network on karstic systems sno karst of the french national center for scientific research cnrs the geology and geomorphology of the site is characteristic of the upper normandy the seine river cuts deeply into chalk plateaus this chalk aquifer is karstified locally on the top of these mesozoic chalks layer the weathered chalk has formed a clay with flint layer quite impervious and in some places covered with quaternary silts swallow holes are penetrating the clay and chalk layers the infiltration of water into the karstified chalk aquifer can be very quick more locally a n70e fault the triquerville fault affects the site with a net slip of 120 m south to the fault the formations are the ones described before while north to the fault cretaceous layers of clay alternate with sand formations the norville study site is a sinkhole spring system upstream the bebec creek drains a watershed of 10 km2 before infiltrating into the ground when reaching the fault fig 1 the bebec creek discharge is characterized by an important seasonal variability from 5 l s during low flow up to 400 l s during storms events downstream the perennial hannetôt spring at the bottom of the chalk cliffs has been proved to be the resurgence of the bebec river with multiple tracer tests karstic conduits are also likely to be found within the chalk aquifer between the spring and the seine river underneath alluvial aquifer hence a hydraulic connection between the karstic aquifer and the seine river is suspected fournier et al 2008 the variations of the level of a piezometer located south of the seine in the same chalk layer are correlated to the variations at the spring this study site is monitored for rainfall using two rain gauges in cumulated rainfall recording mode the first located a few hundred meters from the spring as the main gauge the second one located slightly out of the northern boundaries of the catchment on the plateau showing a small lag 1h with the main one used whenever there were gaps in the time series of the main rain gauge discharge was recorded using doppler flowmeters at the sinkhole site and at the spring with a 15 min time step interval and water level divers associated with a rating curve were used to check the reliability of the final discharge time series conductivity and turbidity were also monitored using a ctd diver and a turbidimeter both at 15 min time step etp was calculated using the thornwhaite formula the time series used in this study cover a bit more than two years 2013 2015 used at hourly time step 3 methods in this study the following approach was chosen 1 conduct a reservoir modelling and extract all internal flows from the model 2 compare those flows with natural tracers data measured at the spring conductivity and turbidity to discuss the realism of the model 3 analyse how the modeled spring discharge acquired its spectral signature within the model to gain knowledge on the model behaviour 4 compare internal flows of the reservoir model assumed realistic with decomposed details of the spring discharge to assess the ability of the decomposition method to access internal dynamics 5 interpret and discuss the consistency of the information provided by both methods used together 3 1 conceptual model and model set up 3 1 1 karstmod in order to perform a rainfall runoff modelling on the norville karstic basin the karstmod model jourde et al 2015 mazzilli et al 2017 was chosen based upon several characteristics i karstmod is a modular bucket style conceptual model that enables to activate deactivate several reservoirs depending on the knowledge of the karstic system ii this model has been developed specifically for karstic groundwater flow simulation by french institution insu cnrs sno karst it has been tested on various karstic systems with success baudement et al 2017 johannet et al 2015 loncar et al 2018 poulain et al 2018 and iii karstmod presents useful features like several possible time steps various discharge equations sensitivity analysis and uncertainty estimation karstmod includes four conceptual possible reservoirs epikarst e conduits c matrix m and low l this last reservoir standing for a lower level or sub system of the saturated zone than can represent for example a deeper system or slower dynamics depending on the conceptual model chosen by the user mazzilli et al 2017 fig 2 different flows can be implemented in order to characterise flow between the different reservoirs linear laws hysteretic behavior and thresholds flows the five balance equations are presented below 1a de dt p e t q loss q el q em q es q ec q hyec q hyes i f e 0 1b de dt p e t i f e min e 0 1c dl dt q el q ls q l pump 1d dm dt q em q mc q ms q m pump 1e dc dt q ec q hyec q mc q cs q c pump with e l and m l the water levels in the compartments e l and m respectively emin the minimum water level in the compartment e l p l t the precipitation rate et l t the evapotranspiration rate q l pump q m pump q c pump l t discharge rates removed from the compartments per unit surface area q loss q el q em q es q ec q hyec q ls q ms q cs q mc are internal discharge rates per unit surface area and the general form is given by the following equation 2a q ab k ab a l ref α ab i f a 0 2b q ab 0 o t h e r w i s e where the notation ab indicates a flow from reservoir a to reservoir b a is the water level in reservoir a is kab is the specific discharge coefficient for the linear discharge law from compartment a to compartment b lref is a unit length and αab is a positive exponent at last the discharge at the spring or outlet q s is given by 3 q s r a q es q ls q ms q cs q hyes q s pump with r a the recharge area of the catchment and q s pump the discharge rate removed at the outlet per unit surface area the model is calibrated using a quasi monte carlo procedure with a sobol sequence sampling of the parameter space sobol 1976 the procedure is stopped when either nmax parameter sets satisfying the objective function wobj greater than the user defined threshold value wobjmin are collected or when the elapsed simulation time reaches the maximum duration tmax specified by the user the user specifies both wobjmin and tmax additional information on the structure of the model threshold function internal fluxes exchange function qmc the available objectives functions calibration and optimization processes is available in the aforementioned references mazzilli et al 2017 and in the karstmod user s guide 3 1 2 model structure and calibration in order to assess the optimal structure of the model 336 tests with each of them 10 to 10 000 simulations were carried out with all possible configurations in a trial and error procedure by activating and deactivating the different reservoirs by allowing or not different internal fluxes hysteretic behavior or not by allowing exponents superior to 1 etc the general summary of these tests is presented in appendix 1 the optimal structure obtained at the end of the procedure included all reservoirs following the configuration presented in fig 2 with linear relationships between reservoirs except out of c to the spring where a power law is possible the warm up period was chosen at 1000 h 40 days the calibration period was chosen as roughly the first hydrological year 1000 h to 10 000 h and the validation as the second year starting just after the end of the drier months 10 001 h to 17040 h the objective function was a combination of nash sutcliffe efficiency nse and the modified balance error be perrin et al 2001 with respective weights of 0 7 and 0 3 the minimal value for the objective function wobjmin was set up to 0 6 while the number of required successful simulations was comprised between 10 and 1000 depending on wobjmin in our study q c pump was implemented into the model as a negative flow in order to represent the bebec creek entering the karstic system at the sinkhole the range of allowed variations for all parameters during the calibration once the model structure was selected is detailed in table 1 after the selection of the optimal configuration the kling gupta efficiency coefficient kge gupta et al 2009 kling et al 2012 was also calculated to further evaluate the performance of the model independently this new efficiency coefficient has been used recently by different authors in hydrology and karst hydrology because the orthogonal set up of the kge allows to separately compare correlation variability and bias of simulations and observations hartmann et al 2013c some of the problems than can rise due to a calibration based only on the nse for example its use of the observed mean as baseline which can lead to overestimation of model skill for highly seasonal variables see gupta et al 2009 are avoided here by the use of the multi objective function for the calibration and by the use of an independent efficiency coefficient a posteriori the kge aside from the performance criteria additional quantitative and qualitative evaluations of the model to reproduce the spring flow were carried out assessment peak flows baseflow the physical realism of the model was further explored by comparing internal flows with observed time series conductivity turbidity piezometry these proxy variables enabled to assess the ability of the model to capture real internal dynamics direct transfer drainage of the system exchanges between compartments and long term trends 3 2 signal analysis and processing we used different signal analysis methods on the simulated discharges classic fourier transform spectrum analysis and a signal decomposition method using wavelets the multiresolution analysis mra they were used to assess how the spectral information was transmitted and modified through the reservoir model and to determine if the components of the discharge signal or combination of those components obtained through mra could be compared with the internal discharges between the reservoirs of the model e m c and l thus gaining knowledge on the model behavior 3 2 1 fourier spectra fourier spectra of the different inputs and output of the model rainfall bebec discharge and spring discharge along with all the internal discharges were analyzed the fourier spectrum of a stochastic signal process represents the distribution of energy variance e of the process depending on the frequency ω with often e ωβ the β coefficient can give information on the signal β 0 is characteristic of a white noise energy and frequency are independent β 1 is a pink noise natural phenomenon of large spatial and temporal scales β 2 is characteristic of a red noise the successive points are independent for one another but they follow a statistical law β 2 is black noise the signal is not statistical but characteristic of exceptional events interrupting periods when another law was applying between 1 and 1 the signal is gaussian between 1 and 3 it is brownian a watershed transforms rainfall into discharge roughly from white to red noise if a system is partly linear this can be modelled by a convolution between the input rainfall and the pulse response function of this system resulting in a discharge variability at the outlet then the slope βout of the discharge spectrum shows the intensity of the smoothing of some frequencies by the system in comparison with the slope of the source signal here rainfall βin if a system is filtering primarily high over low frequencies the output fourier spectrum would show a change in slope with different β coefficients in the case of one change of slope occurring across a system two coefficients can be extracted from the discharge spectrum β1 for lower frequencies and β2 for higher frequencies by analysing the different fourier spectra and their associated beta coefficients one can describe the behaviour of the studied system and the different statistical laws applying to each part of the frequency domain of the time series separated by the cut off frequencies for which the linear adjustments for each beta coefficient intersect most hydrologic time series fourier spectrum including karst can be described with one two or three statistical laws dolgonosov et al 2008 fournillon 2013 hardy and beier 1994 labat et al 2000a mathevet et al 2004 the cut off frequency enables to define several spectral domains within the studied signal the method to adjust the linear law on the spectrum can be empirical labat et al 2000a or with other statistical methods little and bloomfield 2010 in this study the linear adjustments were done using an algorithm in r to detect change points killick et al 2012 lavielle 2005 before calculating the associated coefficients this approach was applied for the fourier spectra of the input in the model rainfall of the intermediary discharges in and out of each reservoir qec qem qmc etc and of the output discharge qsim in bi logarithmic scale 3 2 2 signal decomposition method multiresolution analysis mra wavelet multiresolution analysis mra was used to decompose the spring discharge in order to compare its spectral components with internal flows of the model and to study more precisely how well the model could reproduce all the spectral information contained in the observed data and what knowledge on the model and the karst system could be gained wavelet multiresolution analysis allows to project a signal on an orthogonal basis of wavelet and scale functions generated from a filter band following a dyadic scale and to build for each level of this scale the component of the signal that explains the variability at this scale the decomposition is orthogonal and therefore the sum of all components details and residue or smooth gives back the initial signal multiresolution analysis has been used quite extensively to study the stationarity and the different scales of variability in hydrologic and meteorological time series coulibaly and burn 2004 liu et al 2011 nakken 1999 smith et al 1998 more specifically in karst hydrology labat investigated the relationship of the details of multiresolution analysis of rainfall data and discharge data by cross correlation labat et al 2002 in our study the signal decomposition of the spring discharge was obtained using symlet 8 modified daubechies wavelets with increased symmetry resulting in twelve details and a residue or smooth these details were compared with the internal flows of the karstmod model first separately one detail compared to one internal discharge then by combining them sums of details compared with the internal discharges a script was built in r to test all combinations of details and single out the ones showing the best correlation coefficients with the internal flows 4 results of the combined approach 4 1 outputs and performance of the model the set of parameters associated with the best score of the objective function in karstmod is provided in table 2 the objective function final value was 0 75 one specificity of this configuration in regard with other tested french karstic systems of the insu network at the time of this work is the necessity of the implementation of the reservoir l in order to provide a satisfying efficiency this reservoir corresponds conceptually to the baseflow component of the spring discharge supplied by the local porous chalk aquifer this hypothesis has been checked using the eckhardt filter as described in appendix 1 the observed and simulated discharges of this final and optimal configuration are presented in fig 3 the kling gupta efficiency coefficient kge was calculated to further evaluate the performance of the model and its value was 0 71 slightly under the objective function the parameters associated with the kge coefficient are the correlation coefficient r its value is 0 78 the bias ratio β here 0 97 and the variability ratio γ 0 82 the mass balance is 0 9 in calibration period 6 in validation period and 3 4 overall the amplitude of high flows is acceptable on most flood events albeit slightly underestimated on three sets of events one way to estimate the ability of the model to reproduce higher flows correctly is to calculate the nash coefficient of the logarithm of the discharge here its value is 0 62 the simulated baseflow range of values is satisfying even though the end of recessions preceding lowest flows is slightly over estimated a portion of the lowest flows of the second hydrologic year is missing data gap which might have impacted the calibration of low flows and baseflow more specifically however the analysis and the comparisons described in the next sections are focusing on high flow events the internal discharges at the outlet of the four reservoirs e l m and c are presented on fig 4 it can be seen that the flow out of l is very smooth while the variability of qcs flow out of c at the spring is much higher and integrates the joint influence of qpump bebec creek discharge input as a negative pumping discharge qmc flow between m and c and qec flow from e to c the discharge qmc is particularly interesting as it represents the exchange between m and c and is alternatively positive and negative pulses from c to m and more gradual and continuous from m to c the discharge qms from reservoir m to the spring is extremely low mean qms 0 1 mean spring discharge most of the flow out of reservoir m is going to c via qmc in this configuration i reservoir l produces the baseflow ii reservoir c receives direct input from the bebec creek via negative pumping in c and delayed input through reservoir e and iii reservoir m enables to simulate correctly the multi dynamics observed in the data through a complex exchange flow 4 2 hydrological significance and consistency of modeled internal fluxes after the internal components of the model were singled out the next step was to assess if the temporal variability of these components could be representative of real flow dynamics within the karst system or if they were just fitting variables within the model the aim was to test the realism of the model by comparing internal flows and reservoirs levels with observed time series giving information on the aquifer dynamics turbidity conductivity piezometry the exchange flow between reservoirs m matrix and c conduits qmc was divided in its positive and negative components discharges qmc and qmc these two discharges were compared to conductivity and turbidity variations measured at the spring these comparisons were targeting extreme events when this inversion of the direction of flow was occurring the default behavior being a flow from m to c with occasional inversion from c to m in high flows conditions figs 5 and 6 present these variations on a restricted number of events but similar observations were made on most major events on fig 5 january 2015 the conductivity shows several quick drops consistent with spring discharge and rainfall events simultaneously qmc is peaking and qmc goes to zero that is to say an inversion of the flow between m and c occurs after the peaks the conductivity and the discharge qmc are showing similar variations they raise in the absence of rainfall events and drops in case of rainfall events the intercorrelation between conductivity and qmc on the whole event is 0 62 with a 14 h lag it is relevant to stress that qmc can be seen as a synthetic integrated discharge averaging the flow from reservoir m to c in the model this fraction of the flow still has to go across reservoir c before reaching the outlet i e the spring where the conductivity is observed so a lag between those two time series is consistent fig 6 january 2014 represents turbidity along with internal discharges several observations can be made i the two peaks of qmc are simultaneous with turbidity peaks ii each turbidity peak is associated with a drop in qmc and iii even though peaks are much correlated the dynamics of turbidity and qmc are quite different turbidity and qmc are anti correlated with a value of 0 45 and a lag of one hour it appears that at an event scale the conceptual model is showing consistency with the internal functioning of the karst system as some internal discharges are well correlated to natural tracers this point is further developed in the discussion below apart from events dynamics the ability of the model to reproduce a general trend was evaluated focusing on the evolution of reservoir l interpreted as seen in previous section and appendix as providing for the baseflow the evolution of water level in reservoir l was compared to the evolution of the piezometry within the chalk aquifer on the whole data set in terms of frequency content not amplitude the result is presented fig 7 both data sets show the same trend steady rise with a correlation coefficient of 0 94 and similar shape of variation the chalk aquifer was storing water during the two years of observations in the model reservoir l providing the baseflow of the simulated discharge is showing a similar trend for the optimal simulation still the data set available two years is a bit short in regards to the dynamics of variations within the chalk aquifer and those results should be tested on a larger time period to conclude on the performance of the conceptual model on this point the increase in l might be an artifact for this specific simulation in conclusion several elements tend to show the ability of the conceptual model developed to reproduce part of the internal behavior of the studied karstic system as internal components of the model are showing interesting and physically interpretable correlations with natural tracers in the system both on event scale and larger time scale the next step was to further study the model behaviour and more specifically its internal components using signal analysis methods 4 3 linking spectral characteristics of spring discharge to the modelled hydrological functioning subsequently we investigated the way the spectral information was propagating within the hydrological model our aim was to understand how the spectral information characterizing the output signal was acquired along the modeled hydrological flow path we studied the evolution within the model of spectral characteristics by analyzing the internal fluxes from the different reservoirs and the simulated discharge at the outlet of the conceptual model we then compared the internal fluxes of the model with details from the multiresolution analysis of the spring discharge in order to highlight possible common information obtained from both approaches and discuss the ability of multiresolution decomposition methods to extract hydrologically meaningful statistical components from the sole output signal here the modelled spring discharge 4 3 1 propagation of the spectral information through the hydrological model all fourier spectra of the different discharges are given in fig 8 two domains were deemed sufficient to fit all the spectra even though for qms qmc and less clearly qcs and qsim 3 slopes could also fit fell the data in order to make the comparison of the spectra and the analysis of the evolution of the filtering across the model more straightforward the evolution of β coefficient through the different reservoirs is illustrated and summarised on fig 9 on the left part of this figure an example of fourier spectrum of output discharge of one reservoir e with one change of slope two associated β coefficients β1 on the left part of the spectrum low frequencies and β2 on the right part of the spectrum high frequencies is given with an illustration of the modification of one slope through one reservoir when filtering occurs in high frequencies low periods the slope β is accentuated as the power of filtered frequencies lowers on the right part of this figure a graph sums up the changes in β coefficients fourier spectrum slope through all reservoirs from the input signals rainfall and sinkhole discharges to the final output signal simulated discharge at the spring the cut off periods in hours of the fourier spectra are indicated in the lower part for each signal it can be seen that β1 low frequencies and β2 high frequencies coefficients are affected differently across the model one could expect the input rainfall signal to have a flat spectrum β 0 as the rainfall is usually described as a white noise however here the rainfall fourier spectrum is not perfectly flat with a break of slope around the period 50 h this is due to the autocorrelation inside a rainfall event if it is raining several days in a row the rainfall from one day to another is correlated to the day before or after and has to be put in perspectives with the specificity of the regional hydrology in normandy still its two β coefficients are small in absolute values going through the model the rainfall is an input into reservoir e which modifies is into qec discharge out of e it can be seen on fig 9 that e has a major impact on both β coefficients reservoir e plays an important part in the acquisition of spectral characteristics of the output signal the second input signal is the discharge of the bebec creek entering the sinkhole this discharge does not cross reservoir e but enters directly reservoir c in the model this part of the signal is filtered only through reservoir c the output of l shows just one slope at last the m reservoir has a complex role as it interacts with c reservoir as well as e reservoir the major effect is on average to high frequencies β2 coefficients with much higher slopes than for the output from e this can be interpreted as a prevalent filtering of the average to high frequencies occurring in reservoir m eventually output signal qcs and qsim are presenting the same slope rupture and almost the same β coefficients the β1 coefficients of discharges out of l m and c reservoirs are in the brownian domain while out of reservoir e is just at the limit between gaussian and brownian it means that for low frequencies or longer time scales the simulated discharges are stochastic structured and fractal all the β2 coefficients high frequencies short time scales are in the brownian domain or beyond the coefficients in gaussian domain are the β1 and β2 of the rainfall β1 of qsinkhole from the bebec stream and just at the limit β1 of qmc which is impacted by qsinkhole in this domain the signals are purely stochastic and non structured for the discharges it indicates that for very short time scales the measuring devices might have a stronger influence than the natural system eventually the signals with β2 coefficients below 3 are characterised as black noise discharges out of m and c for short time scales these discharges are influenced by exceptional phenomena like specific floods this figure helps to analyse how the acquisition of the spectral characteristics of the simulated signal at the spring is related to the filtering through the reservoirs with the following precisions reservoir e has a general impact on the modification of the signal with a prominent filtering on low and intermediate frequencies to be compared with the conceptual interpretation of e as providing the diffuse infiltration component reservoir m interacting with reservoir e and c has a major impact on high frequencies smoothing them reservoir l contributing to the base discharge might slightly structure the low frequencies same β1 than qsim but does not have a major impact on the spectral signature of the final simulated discharge it does provide in amplitude though for the baseflow the input signal from the sinkhole bebec creek might explain why the qmc β coefficients are slightly higher than qms because qmc is a two ways flow between m and c with the input from the bebec bringing higher frequencies this analysis highlights the effect and relative importance of the different reservoirs on the spring discharge and how it acquires its characteristics 4 3 2 spectral components of spring discharge vs internal modeled hydrological components the mra decomposition method was then used to assess whether the information given by the wavelet details of multiresolution analysis could be linked to internal information of the reservoirs model as the conceptual model was shown to be able to reproduce some dynamics of the system as seen in section 4 2 this study was aiming at determining if signal analysis methods could also be used to further interpret the real internal hydrological functioning of a karstic system the simulated discharge and the observed discharge had a very close spectral content when compared detail to detail as further explained in the appendix 1 from there on it was possible to test combinations of details with internal discharges from the model indeed as shown by the fourier analysis in the previous section the different reservoirs models are filters in complex interactions transforming differently the flow spectral signature across the model so the approach was to assess if it was possible to compare combinations of details from the spring discharge that would match the flows out of each reservoirs every combination of possible wavelet details sum from multiresolution decomposition of the simulated discharge qsim details noted d1 to d12 was compared to each internal discharge the results are presented in table 3 the comparison between the optimal sum of details and the internal discharge is illustrated in fig 10 with discharge out of reservoir e qec the internal discharges of the reservoirs model are well correlated to the sums of details of the multiresolution analysis above 0 73 1 for all discharges except qmc the optimal signals are sums of consecutive frequencies and the separation is clear depending on the internal discharge considered the discharge out of e excludes the highest frequencies when the discharge out of m also excludes intermediate frequencies this is consistent with the position of the reservoirs and their supposed hydrogeological behavior it is not surprising that qmc is not well correlated to the sum of details of qsim as it is the discharge out of c to m so this discharge is not included in qsim as it is these results indicate that the multiresolution wavelet analysis is able to extract components that when grouped together is a specific way are showing the same variations or spectral content than internal discharges from reservoir modelling out of e m and c assuming that these internal flows between e m and c are reflecting the internal behavior of the system as suggested in section 4 2 it would imply that this decomposition method of the spring discharge could also help access some understanding of internal dynamics this will be further discussed in the next section 5 discussion 5 1 comparison between frequencies extracted from fourier spectra and from karstmod flows the results of the different approaches presented previously spectral analysis of the signals within the model reservoirs versus multiresolution analysis of the spring discharge show some interesting similarities and convergence points when superposing the frequencies associated to the best sums of detail of multiresolution analysis with the fourier spectrum of the final output of the model qsim it can be seen that they match the cut off frequency marking a break of slope of the fourier spectrum fig 11 the sum of details from 5 to the smooth best correlated to qec corresponds to the period 55 h and above this cut off period is the same for both qec and qsim the second characteristic period put in evidence in the previous section detail 7 associated with output discharge of m t 200 h can be compared with one of the cut off period for reservoir m tc 155 h for qms this cut off period is comprised between detail 6 t 110 h and detail 7 t 200 h interestingly it could also be associated with a second break of slope in the fourier spectra if two breaks of slopes and three linear regression were calculated from the spectrum as illustrated on fig 11 instead of two 5 2 hydrological interpretation of the effects of the reservoirs the similar variations between internal flows and conductivity and turbidity observed time series can lead to a possible interpretation of the hydrologic behavior of the karst system as follows during major events the karstic conduit cannot transfer all incoming discharge and becomes pressurised while usually the conduit is draining the connected chalk aquifer when the discharge within the conduit increases a threshold effect appears and the conduit starts to feed the fissured matrix nearby or upper paleo conduits connected punctually both geological settings being likely in the area rodet 2007 in these high flow conditions turbidity peaks can occur and electrical conductivity drops while in the model we have an inversion of the flow between reservoirs m and c from c to m when pressurized with a pulsatile qmc when the discharge within the karstic conduit decreases the conduit network goes back to draining the matrix and the conductivity rises slowly while in the model the flow switches back from m to c with a flow qmc following the behavior of conductivity at a time when the conduits would likely drain the surrounding aquifer the lag observed between this flow out of m and the conductivity observed at the spring 14 h would be roughly the time for the conceptually integrated along the system qmc flow to go across reservoir c to reach the outlet it can be put in perspectives with observed transfer dynamics in this karst aquifer tracer tests carried out at the same time of this study gave appearance time of tracers between 13 and 20 h from sinkhole to spring field observations at the sinkhole also concur with this interpretation of a saturation effect above 50 70 l s the sinkhole cannot absorb entirely the creek flow and starts filling up table 4 sums up the supposed physical internal functioning of the karstic system in parallel with the conceptual model internal flux variations more widely the conceptual role of reservoir e in providing the conceptually delayed diffuse infiltration component of the flow is consistent with its observed behaviour of filtering the higher frequencies short time scale it is also interesting to note that the reservoir e is transforming the rainfall signal its input signal into qec in a very similar way than a surface watershed the beta coefficients out of e are almost identical to the beta coefficients of qsinkhole measured discharge at the sinkhole result of the transformation of the rainfall into river discharge in the surface watershed of the bebec stream reservoir m has a more complex effect on the spectral signature of the signal its interaction with reservoir c and the high frequencies brought by qsinkhole shows a predominant filtering of higher frequencies periods below 53 h however its general dynamics have also an impact on intermediate frequencies as illustrated by the cut off period of qms at 155 h behavior comparable to the discharge qmc when m is drained by c not shown here as it is not a continuous signal the β2 coefficients of qsim and of the discharges out of m are in beyond the brownian domain and these parts of their signals are black noises as illustrated in section 4 3 1 these beta values are associated with exceptional events like floods for example they can be associated with the activation of an upper part of the conduits networks fournillon 2013 the pressure conditions within the system leading to possible water exchanges with the fissured matrix in this chalk context can be another possibility it is interesting to compare the information given by the spectral analysis exceptional events having an influence on higher frequencies with the information provided by the model threshold effect for major flood events leading to impulse discharges towards m and with what can be observed on the catchment itself extreme floods causing the sinkhole to overflow likely causing the conduit system to be fully pressurized lastly reservoir l is associated with the smooth of the multiresolution analysis and might have an influence on the lower frequencies of the simulated discharge as seen previously with the comparison with natural tracers the conceptual model seemed to reproduced reasonably well part of the internal hydrological functioning of the karstic system and its different dynamics the flow between m and c is well correlated to the measured conductivity signal while the impulsions occurring at the time of sudden inflow due to rainfall events is associated with an inversion of flow from c to m a drop in conductivity and turbidity peaks furthermore the multiresolution details combinations reproduce well the spectral information of the reservoir model with cut off frequencies readable on fourier spectrum the sums of details of mra are showing the same variations than the internal discharges of the conceptual model even though they do not match amplitude of the signal for each internal discharge this leads to the hypothesis that the signal analysis methods especially the modal decompositions methods are also able to give access to information from within the system and not only the final discharge at the spring if these results can be observed on other karstic systems this combination of methods might be used to interpret physically the functioning of a karstic system while compared and validated with various tracers analyses tracers tests analyses of conductivity turbidity and other biogeochemical data furthermore it supports the interest of developing new methods integrating quality data and different signatures to better constrain models and performance estimation as tested in the literature gupta et al 2008 hartmann et al 2013b 2013a the observations from high flows events measured at the spring discharge conductivity turbidity in parallel of the simulated internal flows through karstmod can lead to the following final interpretation of a flood event fig 12 on the different parts of the graph various stages following an important rain event on the studied system are illustrated 1 the steady state conditions with the sinkhole and the perennial flow contributing to the spring and the karstic conduit draining the chalk aquifer 2 the beginning of the flow event with an increase of the river flow resulting in an increase of the karstic spring discharge 3 following the discharge rise a turbidity peak occurs and the conduit is under water heads conditions above a certain threshold it would start feeding the surrounding matrix or upper conduit system as the internal discharge from the model qmc goes from c to m 4 at the end of the flow event the flow reverses and the surrounding matrix feeds the karstic network qmc in the model going from m to c these four steps are the interpretation of two sets of curves presented above the simulated data from karstmod internal discharges and the measured data at the spring turbidity and conductivity to elaborate further on the physical interpretation on the role of the model reservoirs the reservoir m can be seen as matrix well connected to the karstic network fissured chalk and or connected to more delayed infiltration points alteration roots in chalk and possible perched aquifers which is consistent which conceptual models of the development of chalk aquifers in this area rodet 2014 alternatively it could be interpreted as a secondary karstic network paleo conduits activated only in above a threshold but this last interpretation is less likely within the context of the site duran 2015 reservoir l would be interpreted as the more inertial chalk aquifer very low frequency and would contribute to the low frequencies part of the spectral signature of the discharge signal and essential to the simulation as it provides the baseflow of the spring discharge its signal is highly correlated to piezometer data the hypothesis of an exchange flow between the conduit network and the fissured chalk matrix happening in the karst network as it is observed in the model pulsatile in one direction and delayed in the other is interesting to discuss in regard of different references of the literature characterizing exchange flow in karst context examples of exchange between matrix and conduits during floods have been documented using geochemical analysis martin and dean 2001 and linked with an inversion of the hydraulic gradient between conduits and matrix see for example bailly comte 2009 reimann et al 2011 numerical models have been developed to investigate the impact of head gradients between the different parts of the system with for example the importance of this exchange flow on the karstification bauer et al 2000 or the role of the pressure transfer between conduit and aquifer porosity on the different responses at the spring bailly comte et al 2010 interpretation techniques based on tracer tests and or spatially lumped models like karstmod in this study enable to estimate physical non equilibrium exchange between conduits and matrix hartmann et al 2014a the role of the conduit interfaces pressure transfer and karst geometry on the exchanged water was explored by validating groundwater model with stable water isotopes binet et al 2016 the transient storage around the conduit described by these authors with an increasing pressure head in the conduit creating a transient local gradient around the conduit that blocks the water coming from the aquifer could similarly explain the observations and modelling results in our study 5 3 possible bias and limitations of the proposed approach these links between flows within the model spectral information and observed behaviour of the catchment are promising still several possible limits and bias need to be addressed the configuration and parameters set of the model used in this study were the ones showing the best performance it is not a unique solution however many models might encounter issues about the unicity identifiability and stability of the problem solution ebel and loague 2006 resulting in the fact that different representations of the modelled system can be considered as acceptable beven 2006 karstmod enables to investigate the parametric equifinality in several ways for each satisfying set of the sobol sequence the values of objective function are plotted against the values of parameters values of the performance criteria and various variance and sensitivity indexes to asses influence of the parameters on the model output to detect over parametrization mazzilli et al 2017 the use of a combined objective function nse and mass balance and the testing of all model configurations to single out the most efficient and scarce in number of parameters can limit the risk of over parametrization perrin et al 2001 which was done in this study but there is still as subjective choice in the multi objective function and uncertainties related to data measurement for example the conductivity and the turbidity time series were also used to assess qualitatively the behavior of the model another possible bias to underline is the rainfall data which already presents a small slope break near t 50 h this could be explained by the autocorrelation within a same rainy event the hydrology of the study zone is characterized by events lasting on a few days in a row other possible ameliorations may include slight modifications of the conceptual model allowing various loss discharges and not only in e even though the downstream influence tide is not likely to be integrated in the model the influence of the tide can account for approximately up to 5 of the discharge massei et al 2006 this might explain why a higher performance could not be achieved in the case of the studied catchment the simulation and analysis would also need to be operated on longer time series in order to check the trends highlighted such as reservoir l in parallel of piezometry as the regional variation of the water level in the chalk aquifer is pluri annual some of the findings of the present study are site related and might not be observed on other karst systems very specific patterns of flow inversion in parallel with hydrogeological context of the site however the approach presented might yield other insights on the behavior of many different karst systems the study of the modification of the spectral signature across a model here bucket style rainfall runoff model can bring further information and could also be applied to other types of models including semi distributed or distributed models the correspondence between cut off frequencies from the multiresolution analysis of the spring discharge and the internal flows of the model could be investigated on any karst catchment and for any other relevant variables 6 conclusion this study investigated the combined use of a signal decomposition analysis method multiresolution analysis and of a reservoir conceptual model karstmod to attempt to extract meaningful hydrological information from a karst spring discharge the internal discharges out of the model reservoirs were compared with both observed time series and with combination of the details of multiresolution analysis the results showed that the internal discharges of the conceptual model were consistent with observed times series at the spring turbidity conductivity and piezometry suggesting that the model was partly able to reproduce some internal dynamics taking place within the karstic system in parallel the spring discharge was studied through signal analysis and decomposition methods fourier analysis wavelet multiresolution the change of slope in fourier spectra for both observed and simulated discharge was analysed in regard with different hydrological behaviors of the karst system the sum of consecutive details from the cut off frequencies of visible on the spring discharge fourier spectrum up to the smooth is highly correlated to the internal discharges out of the reservoir e and respectively m from the conceptual model the way the input signal rainfall is gaining its spectral characteristics has been studied in detail with the modification of the slope of fourier spectra with its two associated β coefficients through the model this highlighted the different influences of reservoirs e epikarst m connected fractured matrix and l less connected matrix in the filtering of specific band of frequencies to obtain the final spring discharge signal on the specific studied karstic system this new combined approach confirmed or widened the interpretations about its hydrological behavior first the modular conceptual model highlighted the need for four different reservoirs adding a slow responding reservoir l to the pre existing ones e m and c this can be put in perspective with the particularity of normandy aquifers with a transmissive porous chalk aquifer combined with karstified systems and the observed changes in the piezometry of the aquifer this approach also revealed an alternate flow between reservoirs m and c illustrating the dominant draining of m by c and occasional inversion of the flow out of reservoir c toward m interpreted as the surrounding well connected matrix or secondary conduits network paleo karst or typical regional infiltration system these results open various perspectives this combined approach could be used on longer time series as well as other systems in order to check its general application the comparison with natural tracers could be widened using diverse biogeochemical data or tracing tests isotopes temperature analysis of common elements the use of multiple hydrologic signatures as characteristics of the system response gupta et al 2008 hartmann et al 2013b hartmann et al 2012 wagener et al 2007 yilmaz et al 2008 combined with the approach described in this study might provide additional understanding on the karst systems behaviours the use of a multi objective calibration with additional output variables including conductivity geochemical data isotopic data etc as investigated by many authors kuczera and mroczkowski 1998 seibert and mcdonnell 2002 son and sivapalan 2007 vaché and mcdonnell 2006 could also lead to further studies one of the perspectives of the combined application of multiresolution analysis and lumped modelling is another tool to discuss the performance of such a mumped bucket style model the mra and the analysis of fourier spectra modification across the model can be another way to assess the ability of the lumped model to simulate a discharge signal in all its spectral content to go further the use of distributed physic based models would prove very useful if internal flows of conceptual models and components of multiresolution analysis could be compared to internal flows within a numerical distributed model it could widen the use those methods which are much less data demanding and computer time consuming to improve the understanding of the functioning of karstic systems in terms of internal dynamics credit authorship contribution statement lea duran conceptualization methodology formal analysis investigation software writing original draft nicolas massei supervision validation writing review editing nicolas lecoq software validation matthieu fournier supervision validation david labat validation writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments this work was undertaken within the framework of the chalk karst observation site part of the karst observatory network www sokarst org initiated by the insu cnrs which aims to strengthen knowledge sharing and to promote cross disciplinary research on karst systems via intensive data monitoring and sharing this work was supported by the haute normandie region france appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2020 124625 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
5734,for integral ecological risk assessment of polycyclic aromatic hydrocarbons pahs the interdependence is always ignored by assuming concentration additivity ca and independent action ia this paper proposed a hybrid assessment model named hierarchical archimedean copula integral assessment hacia to treat inherent relations and assess the multi dimensional holistic ecological risks of exposure concentrations of 16 pahs the hacia model couples with hierarchical archimedean copulas hacs and ecological evaluation criteria sediment quality guidelines sqgs which depends on copulas to construe the connotations of correlations among exposure concentrations of pahs the two case studies the exposure concentrations of 16 pahs from taihu lake in china and the bay of bengal coast in bangladesh respectively are used to verify the adaptability and feasibility of hacia model and evaluate their comprehensive environmental risk in addition a theoretical and empirical comparison with three frequently used methods was provided objectively which focused on presuppositions principles evaluations and consequences the results indicated that the hacia model has good feasibility and applicability to evaluate the synthetic ecological risk according to pahs exposure concentrations remedies for the situation that the interdependence among other methods is neglected and can produce more deliberate classification of hazards theoretically and better demonstrations of assessment results than the other three although the more accurate risk assessments rely on detecting quantity of samples the integral ecological risk assessment based on the proposed hacia model is reliable and robust because of its reasonable structure keywords integral ecological risk assessment polycyclic aromatic hydrocarbons hierarchical archimedean copula risk quotient sediment quality guidelines 1 introduction polycyclic aromatic hydrocarbons pahs are a kind of organic environmental contaminants composed of two or more number of benzene rings barhoumi et al 2016 there are diverse toxicity including carcinogenicity teratogenicity and mutagenicity in partial of them which can cause acute to chronic toxicity in aquatic organisms and are potentially detrimental to human health like cancer effects colombo et al 2006 rajasekhar et al 2018 tiwari et al 2017 meanwhile pahs can be produced from both natural and anthropogenic activities including biomass burning coal and petroleum combustion coke and metal production and so on agudelo castaneda et al 2017 liang et al 2019 and they are ubiquitous as a result of bioaccumulation refractory degradation and long range transportation meng et al 2019 zhang et al 2016a due to these adverse characters pahs have attracted significant attentions from relevant agencies globally for example they are listed as priority substances by the european water framework directive sarria villa et al 2016 and sixteen pahs are regulated as priority pollutants by the united states environmental protection agency us epa ghanavati et al 2019 to measure the impact of pahs toxicity on aquatic organisms variety of sediment quality guidelines sqgs have been derived for protection of benthic life dos santos et al 2018 for instance threshold effect level probable effects level tels pels indicate the geometric means of the 15th percentile of the effects data with the 50th percentile of the no effects data and the 50th percentile of the effects data with 85th percentile of no effects data which are mainly for marine effect range low effect range median erls erms are provided by the us national oceanic and atmospheric administration noaa to evaluate the potential biological effects for sediments particularly for estuarine ecosystems corresponding to the 10th and 50th percentile of effects data as concentrations below which effects are infrequently and frequently observed mcgrath et al 2019 the maximum permissible concentrations mpcs are settled as concentrations in environmental compartments with no negative effect to be expected for ecosystems and negligible concentrations ncs are divided as mpcs 100 defining a safety margin excluding combination toxicity which are used to set environmental quality standards in the netherlands mcgrath et al 2019 according to the definition given by us epa environmental risk assessments typically include two areas human health and ecological and the evaluation of how likely the environment many be impacted by the exposure of pahs can be expressed by ecological risk assessment there are lots of assessment methods based on toxicological or ecological evaluation criteria have been proposed targeting single pahs or combined risk such as the total toxic bap equivalent teq bap risk quotient rq mean effects range median quotient m erm q and incremental lifetime cancer risk ilcr and so on the teq bap method is applied to quantify the carcinogenicity of all pahs relative to the equivalent doses of benzo a pyrene bap by converting concentrations of others into bap according to the toxic equivalency factors tefs nisbet and lagoy 1992 and compare to corresponding guidelines like canadian sqgs pheiffer et al 2018 or with other districts which have similar characteristics in context and detection substances the rq approach is widely used to rank the hazards of pahs individually and totally combining with ncs mpcs appropriate for all media with divergences in reference values for human health pahs can affect it through ingestion inhalation and dermal contact and the exposure risk can be quantitatively estimated by ilcr formulas which was developed based on the us epa standard models bandowe and nkansah 2016 wang et al 2011 researches about pahs are extensively studied by environmental scientists with various environmental compartments soltani et al 2015 such as water ajala et al 2019 santos et al 2018 sediments salazar coria et al 2010 soils tang et al 2005 and atmosphere li et al 2019b al mur 2019 described the situation of teq care in the marine coastal sediments of jeddah red sea and compared them in the northern middle and southern regions of the study area aghadadashi et al 2019 employed m erm q formula to calculate overall toxicological status depending on individual pahs liang et al 2019 adopt rq method based on the framework of us epa to evaluate eco toxicity of compounds in water samples most of above approaches are highly appropriate to individual pahs which is insufficient for the assessment of ensemble effects because of the occurrence of synergisms or antagonisms it is necessary to quantify the inherent relations and integral hazards of all 16 pahs amarillo et al 2014 on one hand regulatory safety concentrations may be inappropriate in the situations when chemicals occur in mixture and cannot provide sufficient protection beaumelle et al 2017 on the other some of approaches are settled for individual estimation of ecological risk especially rq qin et al 2013 and partial sqgs which can be used to analyze mixtures once the toxic effects of the individuals are linearly additive corresponding to two classical basic mixture models concentration additivity ca model and independent action ia model for mixtures which are independent or non interactive between each other there is a hypothesis that the compounds do not enhance or reduce the toxicity of one another in the mixture zwart and posthuma 2005 and both ca and ia models incorporate the assumption of additivity or non interaction beaumelle et al 2017 the hypothesis is under question and difficult to rationalize theoretically and empirically sheikh fakhradini et al 2019 besides ecological risk assessment entails fundamental uncertainties inherent in natural events sheikh fakhradini et al 2019 but the normal rq method and others do not incorporate ecological realism consequently zhang et al 2018 notwithstanding their logic transparence and easy application therefore an alternative statistical assessment method which can take the interdependence as presupposition and be appropriate to estimate the integral ecological risk of pahs need to be proposed copula functions are well known as mathematical functions to model dependence properties among multivariate based on sklar s theorem kraus and czado 2017 they are widely used in hydrology and water resources engineering favre et al 2004 kim and chung 2019 for example the risk evaluation of hydrological extreme events like flood yin et al 2018 and drought dash et al 2019 guo et al 2019 but they have not been applied in the ecological risk assessment the objective of this study is to develop a new integrate ecological risk assessment model from the statistical prospective the hierarchical archimedean copulas integral assessment hacia with combining copulas and ecological criteria ncs mpcs together which can reflect the interdependence structure among exposure concentrations of pahs by diverse copula functions and corresponding parameters the hydrophobic nature of pahs determines that exposure concentrations in sediments are appropriate to investigate toxic effects on aquatic organisms therefore on the basis of ecological evaluation criteria of pahs exposure concentrations and mathematical statistics the hacia model construed the connotations of interdependence of pahs concentrations in sediments and provided both qualitative and quantitative ecological risk probability analysis from concentration quality field combining the classification principles of rq method in other words the hacia model retained the essences of original ecological evaluation methods enhanced the flexibility and practicality of them and classified the hazard with more continuity and logicality apply the hacia model to measure the connotations of correlations among pahs concentrations evaluate the integral ecological risks of sediments from taihu lake of china and the bay of bengal coast of bangladesh respectively and compare its performance with three other methods rq teq bap and m erm q both in theory and practice and evaluate its validity and superiority 2 materials and methods 2 1 methods 2 1 1 archimedean copulas to match the precondition of interdependence copulas used widely in multivariate hydrological analyses because of the dependencies between quantities madadgar and moradkhani 2013 are capable of separating the effects of dependence and constructing multivariate joint distributions whose one dimensional margins are uniform on the interval 0 1 nelsen 1999 with diverse correlation and dependence structures for n dimensional copulas the threshold relationships between margins and final functions can be represented as c 0 1 n 0 1 according to sklar s theorem sklar 1959 a multivariate distribution expressed by copula can be indicated implicitly as follows madadgar and moradkhani 2013 1 f x 1 x 2 x n c f 1 x 1 f 2 x 2 f n x n c u 1 u 2 u n where f i x i means the marginal distribution function of the i th variate represented by u i by copula definition can be interpreted as f i x i p x i x i which shows the probability of the random variate when x i x i and c indicates the corresponding copula function let generator φ be a continuous strictly decreasing and monotonously convex function from 0 1 to 0 that means φ 1 0 and φ 0 then the copula function c of multivariate could be defined as follows 2 c n u φ 1 φ u 1 φ u 2 φ u n specially when generator φ is defined as φ t ln t θ where θ 1 φ t t θ 1 θ where θ 1 0 and φ t ln e θ t 1 e θ 1 where θ r 0 these shape the gumbel clayton and frank family of archimedean copulas respectively which is an important family of copulas with a wide range of applications because of many essential advantages embrechts et al 2001 such as clear construction a variety of configurations which are generalized to incorporate stable tail dependency and have closed form expressions hofert 2011 the archimedean copulas are widely applied in general archimedean copulas can describe bivariate dependence perfectly since they require variates to be strictly symmetric and associative when there are more than two variates or especially if variates might be asymmetric rigorous restrictions would be imposed on the joint distributions which weaken the practicability of copulas 2 1 2 hierarchical archimedean copulas hacs most of literatures on copulas are on bivariate cases aas and berg 2009 and due to their exchangeable property copulas are extremely limited in their flexibility and accuracy when the number of variables increases significantly cossette et al 2017 a different structure for building high dimensional copulas hierarchical archimedean copulas hacs is used frequently which provides an alternative to make up the deficiency mcneil 2008 presented a sampling algorithm of hac based on mixture representations involving laplace transforms and examples of clayton and gumbel copula family savu and trede 2009 indicated a flexible class of hacs which obtained multidimensional joint distribution models of asset returns with a stronger rank correlation structure than existing models and applied it for risk management the hac functions structure the whole dependence framework in a recursive strategy generally different from normal archimedean copula okhrin et al 2013 it also allows to adopt arbitrarily complex structures which contribute to their flexibility and effectiveness okhrin and ristig 2014 that is to say hacs synthesize the advantages of archimedean copulas which are compliant to non exchangeable dependence structures and other multivariate statistical methods simply and flexibly which indicate that they are adequate to fit multi dimensional distributions and cover all needed qualifications there are fully nested and partially nested hac whose 4 dimensional structures are drawn in fig 1 generally n dimensional fully nested non exchangeable hacs can be written as joe 1997 3 h u 1 u n h n 1 h n 2 h 1 u 1 u 2 u n 1 u n φ n 1 1 φ n 1 φ n 2 1 φ n 2 φ 1 1 φ 1 u 1 φ 1 u 2 φ n 1 u n 1 φ n 1 u n where h indicates the hac function u i means the marginal distribution function of the i th variate generator φ is a continuous strictly decreasing and monotonously convex function from 0 1 to 0 when n 4 the partially nested hac can be written as follows 4 h u 1 u 2 u 3 u 4 h 2 h 11 u 1 u 2 h 12 u 3 u 4 φ 2 1 φ 2 φ 11 1 φ 11 u 1 φ 11 u 2 φ 2 φ 12 1 φ 12 u 3 φ 12 u 4 obviously hacs are partially exchangeable and more flexible than corresponding same dimension simple copulas like multivariate archimedean copulas hacs also have restrictions such as all inverse generator φ 1 must be completely monotonic but if all generators are from one copula family the restrictions placed on the parameters can be averted easily savu and trede 2009 2 1 3 risk quotient as one of the classical approaches to assess ecological risk of individual pah the formula of risk quotient rq method is shown as follows sun et al 2009 5 rq c pahs c qv where rq indicates the risk level of corresponding pah c pahs means the concentration of a certain pah in the medium and c qv is the corresponding quality values of the certain pah in the medium the quality values are referred to a published article kalf et al 1997 which defined partial environmental quality objectives of pahs as an important instrument for effects oriented risk assessment there are two kinds of quality values negligible concentrations ncs and maximum permissible concentrations mpcs which has been defined before those values mpcs n c s 100 according to different benchmarks rq includes rq ncs and rq mpcs respectively which are shown as follows meng et al 2019 6 rq ncs c pahs c qv n c s 7 rq mpcs c pahs c qv m p c s in principle according to the values of rq ncs and rq mpcs the ecological risk of pahs has been divided into three degrees rq ncs 1 risk free means scarcely any serious ecological risk rq ncs 1 while rq mpcs 1 moderate risk indicates that there exist ecological risk but it s moderate and potential which needs attention rq mpcs 1 high risk signifies that the pahs in the medium have a high ecological risk and we should take some remedial measures the integral ecological risk of pahs can be calculated as follows 8 rq ncs i 1 16 rq i n c s rq i n c s 1 9 rq mpcs i 1 16 rq i m p c s rq i m p c s 1 the ecological risk classification of entire pahs has been differentiated by the values of rq ncs and rq mpcs rq ncs 0 risk free implying almost no ecological risk rq ncs 1 800 meanwhile rq mpcs 0 low risk meaning a little attention should be paid rq ncs 800 while rq mpcs 0 moderate risk 1 showing that the contamination of pahs is a little severe rq ncs 800 while rq mpcs 1 moderate risk 2 which is more hazardous than moderate risk 1 indicating that some control measures are supposed to be taken rq ncs 800 and rq mpcs 1 high risk predicating that the medium which pahs detected has severely harmful detriment for both human beings health and the environment which must be disposed immediately 2 1 4 total toxic bap equivalent teq bap total toxic equivalence teq was calculated to express the potential carcinogenicity of study areas meng et al 2019 bap is the most toxic one among 16 pahs and has sufficient toxicological data to obtain a carcinogenic potency factor habibullah al mamun et al 2019 therefore it was recognized as a benchmark and others were quantified by toxic equivalency factors tefs nisbet and lagoy 1992 derived from their carcinogenic levels compared to bap the equation of total toxic bap equivalent is as follows 10 teq bap i c i tef i where c i signifies the concentration of individual pah and tef i indicates the corresponding toxic equivalent factor the evaluation of ecological risk can be done by comparing with some guidelines or toxic levels of other districts 2 1 5 sediment quality guidelines sqgs numerous kinds of sqgs are used to evaluate the eco toxicological aspect of sediments and assess the risks of individual pahs and mixtures mcgrath et al 2019 no matter erls erms dauner et al 2018 long et al 1995 or tels pels menchaca et al 2014 both of them define the hazards into three different ranges barhoumi et al 2019 c pah under erls tels rare 10 adverse biological effects emerging on benthic organisms c pah between erls tels and erms pels adverse biological effects occurred occasionally c pah above erms pels frequently exceed 50 adverse biological effects may happen there are derivatives of erms and pels to estimate the ecological risks of total pahs mean erm quotient m erm q and mean pel quotient m pel q sheikh fakhradini et al 2019 combining the ecological risk with toxicity levels formulated as follows 11 m e r m q 1 n i c i erm i 12 m p e l q 1 n i c i pel i where n defines the number of pahs kinds computed the following are classifications of risk levels m e r m q 0 1 or m p e l q 0 1 low risk 0 1 m e r m q 0 5 or 0 1 m p e l q 1 5 medium low 0 5 m e r m q 1 5 or 1 5 m p e l q 2 3 medium high m e r m q 1 5 or m p e l q 2 3 high level 2 2 structure of hacia model to capture the connotations of correlations among exposure concentrations of pahs an alternative integral ecological risk assessment approach the hacia model was established as the construction which contains two phases coupled with copulas hacs and ncs mpcs flow chart in fig 2 2 2 1 pretreatment classification the preparatory step is classification to reduce dimensionality because more variables lead to more uncertainty 16 priority pahs can be classified according to their benzene rings number into four categories rings number equal to 2 3 and 4 define as 2 ring 3 ring and 4 ring respectively rings number equal to or greater than 5 defined as 5 ring in the first phase to reduce the fitting dimensionality 16 pahs were divided into four classes directly and the distribution functions of different benzene rings were calculated individually on the basis of classification 2 2 2 phase 1 construct joint distribution function of pahs respectively with different benzene rings to estimate parameters for multivariate copulas among several methods semiparametric approaches have shown to be superior genest et al 1995 to the maximum likelihood ml method which is known to be asymptotically unbiased and efficient and has minimum variance joe 2005 thereinto the maximum pseudo likelihood mpl is the only method that can be used in multivariate cases with multiplier simulation method and it shows the best performance according to the mean square error in all situations than others apart from small samples with weak dependence kojadinovic and yan 2010 because there are diverse copulas to choose we should do our best to decrease misspecification in applications which might cause erroneous statistical estimation and inference zhang et al 2016b therefore it is imperative to calculate goodness of fit tests to obtain the best choice of copulas which based on empirically comparing the empirical copula with a theoretical one with parametric estimation of the copula derived through the null hypothesis kojadinovic and yan 2009 with the aim of circumventing redundant calculations and obtaining the most accurate choice of copulas this paper applied mpl to estimate archimedean copulas the akaike information criterion aic and bayesian information criterion bic as criteria to choose copula functions and cramer von mises statistic s n to test the goodness of fit after computing the pseudo observations for original concentrations applied mpl algorithm to estimate parameter θ for each group then calculated aic bic and s n of each combination to selected the best fit copula function and visualized its goodness of fit and the corresponding copula structure would be established simultaneously finally if it meets the p value limit the internal correlations among each rings of pahs and their distribution functions could be obtained there the formulas of statistics aic bic and s n are as follows li et al 2019c zhang and singh 2007 13 aic n ln m s e 2 m 14 bic n ln m s e m l n n 15 s n 0 1 d c n u 2 d c n u where mse e p c p 0 2 p c means theoretical value and p 0 indicates empirical value m means the number of parameters c n n c n c θ n the minimum aic and bic which represented the best choice of copula function and minimal s n indicated the corresponding goodness of test which served as a reference when judging the optimal copula after this phase four different copula formula c ri i 2 3 4 5 have been obtained which can be indicated implicitly as follows 16 f ri x 1 x 2 x n c ri f r i 1 x 1 f r i 2 x 2 f ri n x n c ri u 1 u 2 u n where f ri x n means the marginal distribution function of the i th ring number classification of pahs represented by u n by copula definition can be interpreted as f ri x n p x n x n which shows the probability of the random variate when x n x n and c ri indicates the corresponding copula function therefore four theoretical cumulative distributions f ri i 2 3 4 5 which could be represented by u ri for each classification which is corresponding to four kinds of benzene rings of pahs could be inferred straightforward first and foremost in order to obtain the final distributions of each group the evaluations of distribution values should be done by r functions with known parameters and copula formulas then these new theoretical distribution samples were applied to structure the second phase model 2 2 3 phase 2 set up the integral cumulative distribution function of pahs based on hac to build an accurate hac model for application the modeling steps can be summarized as choosing estimate method defining the copula family and estimating the parameter of each hierarchy one by one from low level to high level and determining the structure of whole hac the recursive maximum likelihood rml method was recommended to solve the problems after considering the problems of parameters and structures okhrin et al 2013 and okhrin and ristig 2014 introduced a computationally efficient technique in r package which is more conveniently and friendly to do relevant computations about hac functions in applications therefore the type and parameters of hacs for each hierarchy were estimated by rml depending r studio as the result of four categories in pahs considering building a four dimensional hac four different θ would be produced for the first also the lowest hierarchy afterwards define θ 1 as the maximum among all options 17 θ 1 def m a x θ r 1 r 2 θ r 1 r 3 θ r 1 r 4 θ r 2 r 3 θ r 2 r 4 θ r 3 r 4 the larger the parameter is the stronger the dependency expressed by kendall s correlation coefficient is for most archimedean copulas okhrin and ristig 2014 if θ r 1 r 2 was the maximum among them we can get settled h 1 u r 1 u r 2 θ r 1 r 2 next hierarchy should be calculated in the same way which take h 1 in entirety u r 1 r 2 and as one of basic variable to compute interdependencies with others among these steps the final structure of hac namely the integral cumulative probability distribution of 16 pahs would be ascertained exampled as follows all optimal θ were assumed 1 fully nested hac structure 18 h h 3 h 2 h 1 u r 1 u r 2 θ r 1 r 2 u r 3 θ r 1 r 2 r 3 u r 4 θ r 1 r 2 r 3 r 4 2 partially nested hac structure 19 h h 3 h 1 u r 1 u r 2 θ r 1 r 2 h 2 u r 3 u r 4 θ r 3 r 4 θ r 1 r 2 r 3 r 4 r programme can oversimplify it and help to define the hac model 2 2 4 assessment classification of risk levels based on hacia model based on the foregoing disposition and final hac structures the comprehensive exposed concentration of 16 pahs for each sample was estimated and expressed as a probability h p pahs with a certain value in the range of 0 to 1 concentration from low to high to introduce ecological evaluation criteria into hazard assessment model the overall ecological risk of pahs was reckoned according to sediment quality guidelines ncs and mpcs which are one of standardized methods for evaluating environmental hazard limits considering bioavailability and suitable protection for benthic organisms to prevent adverse effects mcgrath et al 2019 substituting pseudo observations processed ncs mpcs values into the established hac model the upper and lower risk limits can be derived and represented by hp ncs and h p mpcs individually similar with rq method and the meaning of ncs mpcs if h p pahs hp ncs indicates no particularly serious ecological risk and moderate potential risky might happen when hp ncs h p pahs h p mpcs negative effect to be expected for ecosystems high probability if h p pahs h p mpcs the indicator of pahs ecological risk assessment expressed as ind pahs can be calculated as follows 20 ind pahs h p p a h s h p n c s h p n c s h p p a h s h p n c s h p p a h s h p n c s h p m p c s h p n c s h p n c s h p p a h s h p m p c s h p p a h s h p m p c s h p p a h s h p m p c s eq 20 expresses the joint probabilities of three different states for ecological risk assessment generally when hp pahs is in the midst of the inferior limit hp ncs and the upper level hp mpcs the result of ind pahs indicates how closely the synthesized risk reaches the maximum permissible concentration or exceeded the negligible concentration if hp pahs is lower than hp ncs ind pahs is a negative percentage and its absolute value shows how far away the situation is from the negligible concentration otherwise ind pahs would over 1 and presents how horrible the pollution risk is therefore to refine the levels according to the risk division principle of m erm q method based on the concrete value of ind pahs the integral ecological risk of 16 pahs was divided into four categories ind pahs 0 low free risk 0 ind pahs 0 5 mid low risk 0 5 ind pahs 1 mid high risk ind pahs 1 high risk finally as a result of structuring the whole hacia model and assessing holistic ecological risk of pahs we attained a flexible evaluation model which took the internal relationships between each pah into consideration estimated these relations and quantized them into an integrate copula function and combined with ecological evaluation criteria relying on this model the hybrid ecological risk assessment of pahs was accomplished accurately and defined by the joint probability distributions which exhibited the degree of risks directly by the evaluation results both qualitative and quantitative analysis were accomplished making the assessment more efficient and convenient 3 case studies in taihu lake and the bay of bengal coast two different type of study areas were taken in this paper the taihu lake in china and the bay of bengal in bangladesh taihu lake is the main drinking industrial and agricultural water sources of several important cities surrounded and the water quality in bays is really fragile because of the frequent human activities so both of them need to be paid attention to avoid the deterioration of the ecological environment 3 1 application of hacia model to case 1 surface sediments from taihu lake in china 3 1 1 study area and statistical analysis taihu lake surrounded by several important industrial developed cities in china wu et al 2020 is the paramount drinking water source and supplies water for agricultural and industrial water withdrawals wang et al 2018 the pollution problems in taihu lake ecosystems have been closely watched by relevant departments and widely investigated xu et al 2014 zhang et al 2012 there are several summaries and comparisons of pahs concentrations in taihu lake with different sampling time and medium as well as several other lakes in china extracted from literature displayed in table 1 because of the divergences in samples the concentrations of pahs seem like disorganized as time goes by the range and mean values of total pahs concentrations declined distinctly it might be attributed to the success of environmental measures adapted to local conditions implemented by the government from ca 2000 li et al 2019a compared with other domestic lakes the concentrations of pahs in surface sediments from taihu lake in recent years was at a low level relatively apparently the accumulation of pahs in winter was higher than that in summer wherever in surface sediments or water which perhaps is relative to the fact that the lower temperature tends to promote the formation of pahs wen et al 2018 or higher utilization of fossil fuels and biofuels for heating system meng et al 2019 28 surface sediments samples from taihu lake lei et al 2014 to gauge the exposure concentrations of pahs were extracted in june 2010 with professional collection methods and stringent chemical analysis approaches the concentrations of 16 priority pahs were detected statistically the total pahs concentrations p a h 16 varied from 179 5 ng g to 1669 3 ng g dw with a mean value of 533 3 ng g dw other information was found in literature lei et al 2014 the 28 16 dataset of concentrations of pahs was applied to evaluate the integrated ecological risk by the hacia model and the ordinary rq method to confirm its feasibility compare the assessment consequences advantages and disadvantages between them and to attest that the hacia model had better performance than did the others 3 1 2 modelling hacia 3 1 2 1 correlation coefficient of exposure concentrations as mentioned above the exposure concentrations of pahs in sediments were classified into four groups first the correlation coefficients between every two pah homologues are shown in table 2 calculated by spearman rank correlation as well as multivariate kendall coefficients w among each classification with the exception of pyr all bivariate correlation coefficients were above 0 50 and even over 0 8 for most of the relations with increasing particle size the correlation coefficient was positive for multivariate kendall coefficients all of them were greater than or equal to 0 79 particularly reached 0 95 in 5 ring there were strong and significant correlations between almost all every two concentrations of pahs and definite interdependency in each group correspondingly which meant that there are dependencies in the interior of different benzene rings namely for most of pahs the concentrations of homologues were not independent and the correlations among them were large enough that could not be neglected which were inadequate for the hypothesis of additivity or non interaction the same as ca and ia models as mentioned above so it was completely appropriate to apply the copula families to construct the multi dimensional joint distribution models of exposure concentrations of pahs adequately considering the correlations among them 3 1 2 2 copulas selection in order to test the fitting effect of copula functions we fitted all bivariate combinations in each group firstly based on the mpl algorithm compute all parameters and goodness of fit indicators by loops iterating whole possible combinations may exist with three kinds of archimedean copula functions then in terms of the goodness of fit criteria the best fit copula function could be selected table s1 shows the calculated results of selected bivariate copulas including parameters aic and s n in accordance with each classification choose the minimum aic which represents the best choice of copula function and minimal s n indicates the corresponding goodness of test which can be a reference when judging the optimal copula the optimization has been identified as bold type with the benzene rings going up clayton copula exhibited a tendency of mis convergence with the emergence of na although the clayton performed well in respect of aic especially in 4 ring for s n all of them took precedence over both in 4 ring and 5 ring frank copula had poor performance in both indicators and gumbel copula fitted most of the exposure concentrations of pahs much better than did others in archimedean copula family taking 5 ring pahs as examples fig 3 shows the fitting effect of 6 pairs of bivariate 5 ring pahs by the optimal copula respectively which demonstrates that it had excellent fitting effectiveness to 5 ring pahs because all analog values were surrounding the empirical sites and characterizing their distribution features clearly in other groups there were analogous incidents that appeared similarly then according to the values of θ aic bic and s n different copulas were selected as the most optimal fitting functions in each group to structure the foundation of hacia model the results are shown in table 3 and all estimated model parameters satisfied the requirement of p value 3 1 2 3 configuration of hacia using the gumbel copula and parameters of each classification the joint distributions were obtained for every benzene ring the best fitted hac model was constructed by r studio and was indicated that gumbel copula performed the best in this case eq 21 showed the optimal structure of hac in taihu lake 21 h 3 h 2 h 1 u r 3 u r 4 4 1178 u r 5 3 3977 u r 2 2 4617 according to the final hac model the integral distribution considering internal correlations of multiform pahs at every site was estimated and presented by hp pahs hence these distribution numbers implied their appropriate levels of joint pollutant concentrations among all samples to evaluate the integral ecological risk of 16 pahs the synthesized quotas of toxicant reference values containing hp ncs and hp mpcs were obligatory to take into the whole model and obtain their values respectively relying on the values of hp pahs hp ncs and hp mpcs the final indicator of integral ecological risk in each sample site was assessed by eq 20 and the corresponding category could be identified due to the small number of samples of pahs exposure concentrations uncertainty analysis was estimated by the parametric bootstrap method jiang et al 2019 and the 95 uncertainty intervals were adopted 3 2 application of hacia model to case 2 surficial sediments from the bay of bengal coast areas in bangladesh rapid industrial development and economic transformation have brought a large amount of pollution in the developing world which has discharged into coastal areas including bangladesh liu et al 2019 the details of the bay of bengal and samples collection can be found in habibullah al mamun et al 2019 the concentrations of 16 pahs were compared between winter and summer with 349 8 to 11 058 8 ng g dw 4571 0 ng g dw in winter and 199 9 to 17 089 1 ng g dw 5729 0 ng g dw in summer for 28 samples in total on account of the structure progress were analogous with earlier details are omitted here the spearman rank correlation coefficients and multivariate kendall coefficients of 16 pahs in the bay of bengal coast areas in winter table s2 and summer table s3 indicate that there are close correlations among multiple compounds although the relationships of pahs exposure concentrations in summer are stronger than that in winter they are slightly weaker than taihu and not as significant as that but most of them perform obviously positive correlations with all w above 0 5 thus the hacia model was feasible here in terms of the goodness of fit test results of the pahs exposure concentrations on the copula functions in each group the optimal archimedean copulas and the values of parameter for each category in winter and summer are shown in table 4 clayton copula was the best fit function among archimedean copulas for pahs in winter while for pahs exposure concentrations in summer gumbel and frank copula each accounted for half of the optimal fitting function the structures of hacia for winter and summer are displayed in fig 4 and gumbel hac was the most suitable choice for both of them the uncertainty analysis was calculated as same as the previous example 4 results and discussion 4 1 case 1 surface sediments from taihu lake in china 4 1 1 integral ecological risk assessed by hacia model for the taihu lake in china the final indicator values of integral environmental risk of 16 pahs exposure concentrations in each sediment sample assessed by hacia model and the uncertainty analysis of hp pahs are illustrated in figs 5 and 6 respectively the higher value of ind pahs indicates the more dangerous risk otherwise with lower risk as seen from the results of hacia model the joint exposure concentrations of 16 pahs of all samples are under the maximum permissible reference value indicating that no place had the affiliation to high risk level and most of integral risks belonged to the middle interval with hp ncs hp pahs hp mpcs meant that the majority of sites had moderate integral ecological risk of the exposure concentration of pahs obviously s21 and s27 were in absolutely good conditions low free risk level and s4 and s8 were relatively safe whose indicator values were quite close to 0 so those samples were almost without any worries about the integral ecological risk of pahs by contrast s6 and s7 were nearly approaching the upper level whose indicator values as much as 85 51 and up to 86 81 respectively followed by s8 73 82 s13 69 62 and s19 67 98 s3 s5 to s9 s13 and s19 all of them in mid high risk level had dramatically higher risks than others represented more abominable situations which meant that they were facing correspondingly grave ecological problems of pahs others belonged to mid low risk level overall according to the hacia model almost all of the synthesized ecological risks of pahs in the study area belonged to moderate risk level especially mid low risk indicating that they needed to be paid close attention to improve the status in general two samples among them were regarded as totally safe conditions and the other two were relatively secure but several additional sites were approximate to the upper limit which needed to take reasonable measures to suppress the feasibility of deteriorating trend in other words the holistic ecological situation of pahs in our study area was still sanguine but some symptomatic measures needed to be taken at several sites 4 1 2 results construct with hacia model and normal rq method table 5 demonstrates the additive risk evaluated by the normal rq method from the judgement criteria and computed results it was found that all values of rq ncs were under the standard 800 but more than 0 and most of them fluctuated around 100 almost all rq mpcs equaled 0 with the exception of s7 taking the risk levels into account it was clear that s7 belonged to moderate risk 2 level and others were low risk from the diagram it can be visibly founded that the values of rq ncs in s5 s9 s13 s19 far outweighed the others among those sites marked as low risk level but we couldn t acquire the disparities between each of them on the basis of assessment results merely by the rq method likewise although the evaluated values of several of them were quite tiny and their risk levels were low enough seemingly like s4 and s27 it was unable to ascertain how low the risks were actually both methods estimated the integral ecological risk of 16 pahs in 28 samples and derived evaluations individually they all obtained identical conclusions that the holistic environmental status of pahs in our study area belonged to lower risk level without high risk stations and s7 had the most serious ecological risk although s6 was the maximum of total pahs concentrations among all samples dissimilarities existed as well first comparing the judgements of ecological risk assessment scale there was only s7 which was sorted into moderate risk 2 level by the rq method and others were two grades lower but 8 sites including s7 were categorized as mid high risk by the hacia model for which all 8 sites had higher values of rq ncs in the rq method second there were no sites judged as moderate risk 1 level in the rq method with both low risk and moderate risk 2 levels existing which was absurd because s1 s9 were closely distributed in meiliang bay continuously and evenly and their concentrations varied consecutively in theory it was unlikely to have such a leap third contrary tendencies between site pairs emerged in hacia and rq methods as a result of whether calculate their intrinsic correlation among pahs fig 7 demonstrates the comparison of total concentration rq ncs and ind pahs along sample sites clearly there were analogous variation tendencies between rq ncs and total concentrations except for s6 and s7 somewhat differently for example although rq ncs of s3 was lower than s5 the same as changing in total concentration which was finally judged as low risk by rq while it was deemed to be more hazardous than s5 reckoned by hacia other pairs like s1 and s2 s9 and s5 and so on last but not the least in fig 5 the joint ecological risks of pahs for each site are demonstrated prominently by quantitatively providing useful information directly including whether there was an ecological risk of integral pahs or not to what extent the risk influences our ecosystem and the comparison with other sites indicated clearly the values beside the bar indicate the integral ecological risks of each site and the division lines of risk levels are shown directly in the figure which could help to classify and identify them of samples accordingly to be frank all the above were superior to the illustration of rq method which was monotonous and ambiguous 4 2 case 2 surficial sediments from the bay of bengal coast areas in bangladesh 4 2 1 the integral ecological risk assessment by hacia model the assessment of integral ecological risk in winter and summer evaluated by hacia model could be found in fig 8 and their uncertainty analysis could be found in fig 9 the holistic status of pahs contamination manifested unsatisfactory situations either in winter or in summer almost half of the samples belonged to the mid low risk level but cx3 ct1 ct3 in both seasons and ct2 in winter exceeded the overall maximum permissible guideline high risk ct1 had both maximum values of integral ecological risks in winter and summer beyond the guideline over 60 far away higher than another in summer cx3 just surpassed the criterion and ct3 was higher in both seasons for ct2 the hazard increased dramatically in winter ranked the second to ct1 than in summer which was mid high risk in summer provisionally the vicinities surrounding all the above sites should adopt mitigation measures immediately to impose restrictions on anthropogenic activities which can emit quantities of pahs petrogenic and pyrogenic sources zakaria et al 2002 particularly incomplete combustion huang and batterman 2014 others need relevant attention to restore or maintain the ecological environment me3 had the lowest integral hazard of pahs followed by me2 and cx2 which were relatively safer and less misgivings about pahs compared to others without recognizable regulations of the risk entirety influenced by season several sites had higher risk in summer and others were on the contrary some of them had conspicuous discrepancy between winter and summer such as ct2 especially sn2 and cx4 but the majority of them had approximate hazards whose risk differences were under 20 4 2 2 results construct with hacia model teq bap erls erms and m erm q methods the results of hazard estimation by teq bap and erls erms methods were found in the proceeding literature habibullah al mamun et al 2019 the general tendencies of holistic ecological risk assessment consequences by hacia teqbap and m erm q approaches fig 8 were basically concordant whereas the hierarchical classification of sample risks had distinctions after calculation all p a h s were below e r m s and almost half of them under e r l s with several concentrations of individual pah surpassed the corresponding erm which indicated that the potential biological effects emerged frequently at those sites in m erm q method only ct1 in summer was judged as medium high level with a large proportion of samples divided into medium low class and others belonging to low risk level me3 me2 and cx2 were acknowledged as the top three safe sites by all four approaches and ct1 had the most severe status of toxicity especially in summer there were some differences in the judgements about sn2 and sn3 from the results of teq bap both of them had severe carcinogenicity in winter even more dangerous than or much the same as ct1 in spite of their p a h s being far below but the results of hacia revealed that although the hazards of both sites in winter were more serious than in summer they were not threatening enough to overwhelm ct1 or some others nevertheless opposite trends about seasons were demonstrated in m erm q method with both of them belonging to medium low risk level 4 3 discussion due to the lack of standard methods for ecological risk assessment the ecological risk assessment methods used in this paper is compared according to the theory and practice namely the comparison between haica and other methods is provided from the theoretical assumptions and practical conditions such as the relationship of location and concentration or influencing factors to argue the degree of conformity with the facts and justice whether the results are reasonable accurate and reliable theoretically rq method has concise principle simple calculation steps and intelligible graphical results meanwhile with a few fundamental but important limitations both in theory and practice first in respect of risk division standards there was a rigorous standard to meet risk free level that rq ncs must equal to 0 which is a definitely infrequent phenomenon pahs are ubiquitous environmental pollutants and could directly emanate from anthropogenic activities and natural pathways kalisa et al 2018 and insignificant amounts of them could be degraded by soil indigenous microbial populations in the environment the description in hacia model of it when hp pahs was under hp ncs is seemingly more rational second pah items in calculation were restricted when computing the values of rq mpcs only cumulative partial rq mpcs satisfied rq mpcs 1 instead of the whole the behavior ensured to protrude those sites with higher risks that focused on single or several pah s but to the dispersive risk situations where most risks of pahs almost reached the criteria or below rq mpcs was incapable to accurately indicate their ecological risks even if rq ncs 800 it would be judged as moderate risk 1 level which is unreasonable in reality what s more rq method is founded on the hypotheses of independency and additivity of pahs radically which is inconsistent with the reality attested by the correlation coefficients between and among pahs calculated above the occurrences of divergent tendencies also testified that the holistic ecological risk was not always positively correlated with cumulative concentrations therefore the normal rq method is deficient in assessing the joint ecological risk of pahs compared to erms the values of mpcs were much smaller for lots of pahs resulting in probably conservative conclusions with regard to ecological risk assessment which were more rigorous and beneficial to the ecosystem needing consume enormous resources to curb environmental pollution the tef value of ip was 0 1 far outweighing partial of pahs which had to been neglected when estimating ecological risks by erls erms and m erm q method because of the deficiency of corresponding guidelines in sqgs this may cause consequences of holistic hazard assessment to be underestimated and inaccurate besides the mean calculation weakened the influence of maximum values which caused that the consequences could not indicate the individually swinging risks by the m erm q method in general the results of teq bap can only roughly reflect the status of carcinogenicity in the study area by statistics of sectional pahs instead of the entirety far from the complete toxicity moreover the lack of classification standards of hazards constrains the scope of application on the contrary the hacia model compensates for several of the above weaknesses taking the existing correlations among pahs as presuppose and combine with ecology evaluation criteria are the quintessence of hacia model relying on the flexibility and friendliness of hac the holistic exposure concentrations were figured out and the correlations of pahs were reckoned with throughout depending on the inherent features of copula functions whose conclusion is different from direct accumulation or weighting besides the assessment model was more feasible in practice with different distributions of 16 pahs concentrations the integral values of guidelines hp ncs and hp mpcs were disparate varying with the calculated functions defined by hacia model which enhancing the flexibility of guidelines the computation indicated the extent of integral concentrations overstepped the ecological index by probabilities directly which can characterize risk levels more consecutively and logically conformed with the actual demand the joint risk degrees would not be dominated by several centralized high hazard merely the hacia model considered both qualitative and quantitative analysis quantitative analysis while only the former by other methods without impractical hypotheses although slightly intricate in computational processes the hacia model is feasible to evaluate the comprehensive hazards of pahs and its grading standards are more consecutive and logical which is a preeminent assessment model for integral ecological risk than traditional assessment methods like rq teq bap and m erm q focusing on holistic ecological risks instead of the particular which guarantees that the final decisions of risk grades would not be controlled by several pahs with merely higher risks or omitting the maximum values although the hacia model does not consider the toxicological property of each pahs to evaluate holistic ecological risk straightaway with no method can cover them actually the evaluation results obtained by the overall analysis of concentrations are more viable theoretically handled all pahs in application which outbalanced than teq bap and others distinctly 5 conclusions in this paper a new hybrid assessment model the hierarchical archimedean copulas integral assessment hacia was developed which can indicate the interdependence among pahs exposure concentrations it couples hierarchical archimedean copulas hacs and ecological evaluation criteria sediment quality guidelines sqgs together to construe the connotations of correlations among exposure concentrations of pahs and assess the multi dimensional holistic ecological risks of all 16 pahs the two case studies the exposure concentrations of 16 pahs from taihu lake in china and the bay of bengal coast in bangladesh respectively are used to verify the adaptability and feasibility of hacia model and evaluate their comprehensive ecological risk according to pahs exposure concentrations then provided a theoretical and empirical comparison with three frequently used methods rq teq bap and m erm q from presuppositions principles evaluations and consequences respectively the results indicated that the hacia model is feasible to evaluate the integral ecological risk of pahs and is more preeminent than the other three methods it takes the interdependence among variates into account instead of improper direct addition or weighting calculation which guaranteed comprehensiveness and reliability of evaluation results what s more the grading standards of hacia are more reasonable the situation in which the assessment results are dominated by one or several indicators is avoided and the classification could disclose the degree of overall risk than other methods the hacia model construes the connotations of correlations among pahs exposure concentrations so there is a requirement that more sample capacity needs to be extracted in the same areas to hacia model that to fit the distributions of pahs small samples might result in enormous uncertainty in the conclusions of risk assessment obviously due to the decrease in sample size the uncertainty of the evaluation results in the bay study area increased significantly additional measures are needed to improve model assessments consequently although high quality detecting samples are needed to obtain more accurate risk assessments it deserves to be promoted and further research to evaluate the integral ecological risk of pahs especially when the number of samples obtained is not enough due to limited monitoring conditions credit authorship contribution statement wenyue liu methodology formal analysis writing original draft writing review editing dong wang conceptualization writing review editing validation project administration supervision vijay p singh formal analysis writing review editing yuankun wang writing review editing validation xiankui zeng writing review editing lingling ni data curation yuwei tao investigation jichun wu validation jiufu liu validation ying zou visualization ruimin he visualization jianyun zhang visualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by national key research and development program of china 2017yfc1502704 2016yfc0401501 the national natural science fund of china no 41571017 51679118 91647203 and the jiangsu province 333 project bra2018060 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2020 124612 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
5734,for integral ecological risk assessment of polycyclic aromatic hydrocarbons pahs the interdependence is always ignored by assuming concentration additivity ca and independent action ia this paper proposed a hybrid assessment model named hierarchical archimedean copula integral assessment hacia to treat inherent relations and assess the multi dimensional holistic ecological risks of exposure concentrations of 16 pahs the hacia model couples with hierarchical archimedean copulas hacs and ecological evaluation criteria sediment quality guidelines sqgs which depends on copulas to construe the connotations of correlations among exposure concentrations of pahs the two case studies the exposure concentrations of 16 pahs from taihu lake in china and the bay of bengal coast in bangladesh respectively are used to verify the adaptability and feasibility of hacia model and evaluate their comprehensive environmental risk in addition a theoretical and empirical comparison with three frequently used methods was provided objectively which focused on presuppositions principles evaluations and consequences the results indicated that the hacia model has good feasibility and applicability to evaluate the synthetic ecological risk according to pahs exposure concentrations remedies for the situation that the interdependence among other methods is neglected and can produce more deliberate classification of hazards theoretically and better demonstrations of assessment results than the other three although the more accurate risk assessments rely on detecting quantity of samples the integral ecological risk assessment based on the proposed hacia model is reliable and robust because of its reasonable structure keywords integral ecological risk assessment polycyclic aromatic hydrocarbons hierarchical archimedean copula risk quotient sediment quality guidelines 1 introduction polycyclic aromatic hydrocarbons pahs are a kind of organic environmental contaminants composed of two or more number of benzene rings barhoumi et al 2016 there are diverse toxicity including carcinogenicity teratogenicity and mutagenicity in partial of them which can cause acute to chronic toxicity in aquatic organisms and are potentially detrimental to human health like cancer effects colombo et al 2006 rajasekhar et al 2018 tiwari et al 2017 meanwhile pahs can be produced from both natural and anthropogenic activities including biomass burning coal and petroleum combustion coke and metal production and so on agudelo castaneda et al 2017 liang et al 2019 and they are ubiquitous as a result of bioaccumulation refractory degradation and long range transportation meng et al 2019 zhang et al 2016a due to these adverse characters pahs have attracted significant attentions from relevant agencies globally for example they are listed as priority substances by the european water framework directive sarria villa et al 2016 and sixteen pahs are regulated as priority pollutants by the united states environmental protection agency us epa ghanavati et al 2019 to measure the impact of pahs toxicity on aquatic organisms variety of sediment quality guidelines sqgs have been derived for protection of benthic life dos santos et al 2018 for instance threshold effect level probable effects level tels pels indicate the geometric means of the 15th percentile of the effects data with the 50th percentile of the no effects data and the 50th percentile of the effects data with 85th percentile of no effects data which are mainly for marine effect range low effect range median erls erms are provided by the us national oceanic and atmospheric administration noaa to evaluate the potential biological effects for sediments particularly for estuarine ecosystems corresponding to the 10th and 50th percentile of effects data as concentrations below which effects are infrequently and frequently observed mcgrath et al 2019 the maximum permissible concentrations mpcs are settled as concentrations in environmental compartments with no negative effect to be expected for ecosystems and negligible concentrations ncs are divided as mpcs 100 defining a safety margin excluding combination toxicity which are used to set environmental quality standards in the netherlands mcgrath et al 2019 according to the definition given by us epa environmental risk assessments typically include two areas human health and ecological and the evaluation of how likely the environment many be impacted by the exposure of pahs can be expressed by ecological risk assessment there are lots of assessment methods based on toxicological or ecological evaluation criteria have been proposed targeting single pahs or combined risk such as the total toxic bap equivalent teq bap risk quotient rq mean effects range median quotient m erm q and incremental lifetime cancer risk ilcr and so on the teq bap method is applied to quantify the carcinogenicity of all pahs relative to the equivalent doses of benzo a pyrene bap by converting concentrations of others into bap according to the toxic equivalency factors tefs nisbet and lagoy 1992 and compare to corresponding guidelines like canadian sqgs pheiffer et al 2018 or with other districts which have similar characteristics in context and detection substances the rq approach is widely used to rank the hazards of pahs individually and totally combining with ncs mpcs appropriate for all media with divergences in reference values for human health pahs can affect it through ingestion inhalation and dermal contact and the exposure risk can be quantitatively estimated by ilcr formulas which was developed based on the us epa standard models bandowe and nkansah 2016 wang et al 2011 researches about pahs are extensively studied by environmental scientists with various environmental compartments soltani et al 2015 such as water ajala et al 2019 santos et al 2018 sediments salazar coria et al 2010 soils tang et al 2005 and atmosphere li et al 2019b al mur 2019 described the situation of teq care in the marine coastal sediments of jeddah red sea and compared them in the northern middle and southern regions of the study area aghadadashi et al 2019 employed m erm q formula to calculate overall toxicological status depending on individual pahs liang et al 2019 adopt rq method based on the framework of us epa to evaluate eco toxicity of compounds in water samples most of above approaches are highly appropriate to individual pahs which is insufficient for the assessment of ensemble effects because of the occurrence of synergisms or antagonisms it is necessary to quantify the inherent relations and integral hazards of all 16 pahs amarillo et al 2014 on one hand regulatory safety concentrations may be inappropriate in the situations when chemicals occur in mixture and cannot provide sufficient protection beaumelle et al 2017 on the other some of approaches are settled for individual estimation of ecological risk especially rq qin et al 2013 and partial sqgs which can be used to analyze mixtures once the toxic effects of the individuals are linearly additive corresponding to two classical basic mixture models concentration additivity ca model and independent action ia model for mixtures which are independent or non interactive between each other there is a hypothesis that the compounds do not enhance or reduce the toxicity of one another in the mixture zwart and posthuma 2005 and both ca and ia models incorporate the assumption of additivity or non interaction beaumelle et al 2017 the hypothesis is under question and difficult to rationalize theoretically and empirically sheikh fakhradini et al 2019 besides ecological risk assessment entails fundamental uncertainties inherent in natural events sheikh fakhradini et al 2019 but the normal rq method and others do not incorporate ecological realism consequently zhang et al 2018 notwithstanding their logic transparence and easy application therefore an alternative statistical assessment method which can take the interdependence as presupposition and be appropriate to estimate the integral ecological risk of pahs need to be proposed copula functions are well known as mathematical functions to model dependence properties among multivariate based on sklar s theorem kraus and czado 2017 they are widely used in hydrology and water resources engineering favre et al 2004 kim and chung 2019 for example the risk evaluation of hydrological extreme events like flood yin et al 2018 and drought dash et al 2019 guo et al 2019 but they have not been applied in the ecological risk assessment the objective of this study is to develop a new integrate ecological risk assessment model from the statistical prospective the hierarchical archimedean copulas integral assessment hacia with combining copulas and ecological criteria ncs mpcs together which can reflect the interdependence structure among exposure concentrations of pahs by diverse copula functions and corresponding parameters the hydrophobic nature of pahs determines that exposure concentrations in sediments are appropriate to investigate toxic effects on aquatic organisms therefore on the basis of ecological evaluation criteria of pahs exposure concentrations and mathematical statistics the hacia model construed the connotations of interdependence of pahs concentrations in sediments and provided both qualitative and quantitative ecological risk probability analysis from concentration quality field combining the classification principles of rq method in other words the hacia model retained the essences of original ecological evaluation methods enhanced the flexibility and practicality of them and classified the hazard with more continuity and logicality apply the hacia model to measure the connotations of correlations among pahs concentrations evaluate the integral ecological risks of sediments from taihu lake of china and the bay of bengal coast of bangladesh respectively and compare its performance with three other methods rq teq bap and m erm q both in theory and practice and evaluate its validity and superiority 2 materials and methods 2 1 methods 2 1 1 archimedean copulas to match the precondition of interdependence copulas used widely in multivariate hydrological analyses because of the dependencies between quantities madadgar and moradkhani 2013 are capable of separating the effects of dependence and constructing multivariate joint distributions whose one dimensional margins are uniform on the interval 0 1 nelsen 1999 with diverse correlation and dependence structures for n dimensional copulas the threshold relationships between margins and final functions can be represented as c 0 1 n 0 1 according to sklar s theorem sklar 1959 a multivariate distribution expressed by copula can be indicated implicitly as follows madadgar and moradkhani 2013 1 f x 1 x 2 x n c f 1 x 1 f 2 x 2 f n x n c u 1 u 2 u n where f i x i means the marginal distribution function of the i th variate represented by u i by copula definition can be interpreted as f i x i p x i x i which shows the probability of the random variate when x i x i and c indicates the corresponding copula function let generator φ be a continuous strictly decreasing and monotonously convex function from 0 1 to 0 that means φ 1 0 and φ 0 then the copula function c of multivariate could be defined as follows 2 c n u φ 1 φ u 1 φ u 2 φ u n specially when generator φ is defined as φ t ln t θ where θ 1 φ t t θ 1 θ where θ 1 0 and φ t ln e θ t 1 e θ 1 where θ r 0 these shape the gumbel clayton and frank family of archimedean copulas respectively which is an important family of copulas with a wide range of applications because of many essential advantages embrechts et al 2001 such as clear construction a variety of configurations which are generalized to incorporate stable tail dependency and have closed form expressions hofert 2011 the archimedean copulas are widely applied in general archimedean copulas can describe bivariate dependence perfectly since they require variates to be strictly symmetric and associative when there are more than two variates or especially if variates might be asymmetric rigorous restrictions would be imposed on the joint distributions which weaken the practicability of copulas 2 1 2 hierarchical archimedean copulas hacs most of literatures on copulas are on bivariate cases aas and berg 2009 and due to their exchangeable property copulas are extremely limited in their flexibility and accuracy when the number of variables increases significantly cossette et al 2017 a different structure for building high dimensional copulas hierarchical archimedean copulas hacs is used frequently which provides an alternative to make up the deficiency mcneil 2008 presented a sampling algorithm of hac based on mixture representations involving laplace transforms and examples of clayton and gumbel copula family savu and trede 2009 indicated a flexible class of hacs which obtained multidimensional joint distribution models of asset returns with a stronger rank correlation structure than existing models and applied it for risk management the hac functions structure the whole dependence framework in a recursive strategy generally different from normal archimedean copula okhrin et al 2013 it also allows to adopt arbitrarily complex structures which contribute to their flexibility and effectiveness okhrin and ristig 2014 that is to say hacs synthesize the advantages of archimedean copulas which are compliant to non exchangeable dependence structures and other multivariate statistical methods simply and flexibly which indicate that they are adequate to fit multi dimensional distributions and cover all needed qualifications there are fully nested and partially nested hac whose 4 dimensional structures are drawn in fig 1 generally n dimensional fully nested non exchangeable hacs can be written as joe 1997 3 h u 1 u n h n 1 h n 2 h 1 u 1 u 2 u n 1 u n φ n 1 1 φ n 1 φ n 2 1 φ n 2 φ 1 1 φ 1 u 1 φ 1 u 2 φ n 1 u n 1 φ n 1 u n where h indicates the hac function u i means the marginal distribution function of the i th variate generator φ is a continuous strictly decreasing and monotonously convex function from 0 1 to 0 when n 4 the partially nested hac can be written as follows 4 h u 1 u 2 u 3 u 4 h 2 h 11 u 1 u 2 h 12 u 3 u 4 φ 2 1 φ 2 φ 11 1 φ 11 u 1 φ 11 u 2 φ 2 φ 12 1 φ 12 u 3 φ 12 u 4 obviously hacs are partially exchangeable and more flexible than corresponding same dimension simple copulas like multivariate archimedean copulas hacs also have restrictions such as all inverse generator φ 1 must be completely monotonic but if all generators are from one copula family the restrictions placed on the parameters can be averted easily savu and trede 2009 2 1 3 risk quotient as one of the classical approaches to assess ecological risk of individual pah the formula of risk quotient rq method is shown as follows sun et al 2009 5 rq c pahs c qv where rq indicates the risk level of corresponding pah c pahs means the concentration of a certain pah in the medium and c qv is the corresponding quality values of the certain pah in the medium the quality values are referred to a published article kalf et al 1997 which defined partial environmental quality objectives of pahs as an important instrument for effects oriented risk assessment there are two kinds of quality values negligible concentrations ncs and maximum permissible concentrations mpcs which has been defined before those values mpcs n c s 100 according to different benchmarks rq includes rq ncs and rq mpcs respectively which are shown as follows meng et al 2019 6 rq ncs c pahs c qv n c s 7 rq mpcs c pahs c qv m p c s in principle according to the values of rq ncs and rq mpcs the ecological risk of pahs has been divided into three degrees rq ncs 1 risk free means scarcely any serious ecological risk rq ncs 1 while rq mpcs 1 moderate risk indicates that there exist ecological risk but it s moderate and potential which needs attention rq mpcs 1 high risk signifies that the pahs in the medium have a high ecological risk and we should take some remedial measures the integral ecological risk of pahs can be calculated as follows 8 rq ncs i 1 16 rq i n c s rq i n c s 1 9 rq mpcs i 1 16 rq i m p c s rq i m p c s 1 the ecological risk classification of entire pahs has been differentiated by the values of rq ncs and rq mpcs rq ncs 0 risk free implying almost no ecological risk rq ncs 1 800 meanwhile rq mpcs 0 low risk meaning a little attention should be paid rq ncs 800 while rq mpcs 0 moderate risk 1 showing that the contamination of pahs is a little severe rq ncs 800 while rq mpcs 1 moderate risk 2 which is more hazardous than moderate risk 1 indicating that some control measures are supposed to be taken rq ncs 800 and rq mpcs 1 high risk predicating that the medium which pahs detected has severely harmful detriment for both human beings health and the environment which must be disposed immediately 2 1 4 total toxic bap equivalent teq bap total toxic equivalence teq was calculated to express the potential carcinogenicity of study areas meng et al 2019 bap is the most toxic one among 16 pahs and has sufficient toxicological data to obtain a carcinogenic potency factor habibullah al mamun et al 2019 therefore it was recognized as a benchmark and others were quantified by toxic equivalency factors tefs nisbet and lagoy 1992 derived from their carcinogenic levels compared to bap the equation of total toxic bap equivalent is as follows 10 teq bap i c i tef i where c i signifies the concentration of individual pah and tef i indicates the corresponding toxic equivalent factor the evaluation of ecological risk can be done by comparing with some guidelines or toxic levels of other districts 2 1 5 sediment quality guidelines sqgs numerous kinds of sqgs are used to evaluate the eco toxicological aspect of sediments and assess the risks of individual pahs and mixtures mcgrath et al 2019 no matter erls erms dauner et al 2018 long et al 1995 or tels pels menchaca et al 2014 both of them define the hazards into three different ranges barhoumi et al 2019 c pah under erls tels rare 10 adverse biological effects emerging on benthic organisms c pah between erls tels and erms pels adverse biological effects occurred occasionally c pah above erms pels frequently exceed 50 adverse biological effects may happen there are derivatives of erms and pels to estimate the ecological risks of total pahs mean erm quotient m erm q and mean pel quotient m pel q sheikh fakhradini et al 2019 combining the ecological risk with toxicity levels formulated as follows 11 m e r m q 1 n i c i erm i 12 m p e l q 1 n i c i pel i where n defines the number of pahs kinds computed the following are classifications of risk levels m e r m q 0 1 or m p e l q 0 1 low risk 0 1 m e r m q 0 5 or 0 1 m p e l q 1 5 medium low 0 5 m e r m q 1 5 or 1 5 m p e l q 2 3 medium high m e r m q 1 5 or m p e l q 2 3 high level 2 2 structure of hacia model to capture the connotations of correlations among exposure concentrations of pahs an alternative integral ecological risk assessment approach the hacia model was established as the construction which contains two phases coupled with copulas hacs and ncs mpcs flow chart in fig 2 2 2 1 pretreatment classification the preparatory step is classification to reduce dimensionality because more variables lead to more uncertainty 16 priority pahs can be classified according to their benzene rings number into four categories rings number equal to 2 3 and 4 define as 2 ring 3 ring and 4 ring respectively rings number equal to or greater than 5 defined as 5 ring in the first phase to reduce the fitting dimensionality 16 pahs were divided into four classes directly and the distribution functions of different benzene rings were calculated individually on the basis of classification 2 2 2 phase 1 construct joint distribution function of pahs respectively with different benzene rings to estimate parameters for multivariate copulas among several methods semiparametric approaches have shown to be superior genest et al 1995 to the maximum likelihood ml method which is known to be asymptotically unbiased and efficient and has minimum variance joe 2005 thereinto the maximum pseudo likelihood mpl is the only method that can be used in multivariate cases with multiplier simulation method and it shows the best performance according to the mean square error in all situations than others apart from small samples with weak dependence kojadinovic and yan 2010 because there are diverse copulas to choose we should do our best to decrease misspecification in applications which might cause erroneous statistical estimation and inference zhang et al 2016b therefore it is imperative to calculate goodness of fit tests to obtain the best choice of copulas which based on empirically comparing the empirical copula with a theoretical one with parametric estimation of the copula derived through the null hypothesis kojadinovic and yan 2009 with the aim of circumventing redundant calculations and obtaining the most accurate choice of copulas this paper applied mpl to estimate archimedean copulas the akaike information criterion aic and bayesian information criterion bic as criteria to choose copula functions and cramer von mises statistic s n to test the goodness of fit after computing the pseudo observations for original concentrations applied mpl algorithm to estimate parameter θ for each group then calculated aic bic and s n of each combination to selected the best fit copula function and visualized its goodness of fit and the corresponding copula structure would be established simultaneously finally if it meets the p value limit the internal correlations among each rings of pahs and their distribution functions could be obtained there the formulas of statistics aic bic and s n are as follows li et al 2019c zhang and singh 2007 13 aic n ln m s e 2 m 14 bic n ln m s e m l n n 15 s n 0 1 d c n u 2 d c n u where mse e p c p 0 2 p c means theoretical value and p 0 indicates empirical value m means the number of parameters c n n c n c θ n the minimum aic and bic which represented the best choice of copula function and minimal s n indicated the corresponding goodness of test which served as a reference when judging the optimal copula after this phase four different copula formula c ri i 2 3 4 5 have been obtained which can be indicated implicitly as follows 16 f ri x 1 x 2 x n c ri f r i 1 x 1 f r i 2 x 2 f ri n x n c ri u 1 u 2 u n where f ri x n means the marginal distribution function of the i th ring number classification of pahs represented by u n by copula definition can be interpreted as f ri x n p x n x n which shows the probability of the random variate when x n x n and c ri indicates the corresponding copula function therefore four theoretical cumulative distributions f ri i 2 3 4 5 which could be represented by u ri for each classification which is corresponding to four kinds of benzene rings of pahs could be inferred straightforward first and foremost in order to obtain the final distributions of each group the evaluations of distribution values should be done by r functions with known parameters and copula formulas then these new theoretical distribution samples were applied to structure the second phase model 2 2 3 phase 2 set up the integral cumulative distribution function of pahs based on hac to build an accurate hac model for application the modeling steps can be summarized as choosing estimate method defining the copula family and estimating the parameter of each hierarchy one by one from low level to high level and determining the structure of whole hac the recursive maximum likelihood rml method was recommended to solve the problems after considering the problems of parameters and structures okhrin et al 2013 and okhrin and ristig 2014 introduced a computationally efficient technique in r package which is more conveniently and friendly to do relevant computations about hac functions in applications therefore the type and parameters of hacs for each hierarchy were estimated by rml depending r studio as the result of four categories in pahs considering building a four dimensional hac four different θ would be produced for the first also the lowest hierarchy afterwards define θ 1 as the maximum among all options 17 θ 1 def m a x θ r 1 r 2 θ r 1 r 3 θ r 1 r 4 θ r 2 r 3 θ r 2 r 4 θ r 3 r 4 the larger the parameter is the stronger the dependency expressed by kendall s correlation coefficient is for most archimedean copulas okhrin and ristig 2014 if θ r 1 r 2 was the maximum among them we can get settled h 1 u r 1 u r 2 θ r 1 r 2 next hierarchy should be calculated in the same way which take h 1 in entirety u r 1 r 2 and as one of basic variable to compute interdependencies with others among these steps the final structure of hac namely the integral cumulative probability distribution of 16 pahs would be ascertained exampled as follows all optimal θ were assumed 1 fully nested hac structure 18 h h 3 h 2 h 1 u r 1 u r 2 θ r 1 r 2 u r 3 θ r 1 r 2 r 3 u r 4 θ r 1 r 2 r 3 r 4 2 partially nested hac structure 19 h h 3 h 1 u r 1 u r 2 θ r 1 r 2 h 2 u r 3 u r 4 θ r 3 r 4 θ r 1 r 2 r 3 r 4 r programme can oversimplify it and help to define the hac model 2 2 4 assessment classification of risk levels based on hacia model based on the foregoing disposition and final hac structures the comprehensive exposed concentration of 16 pahs for each sample was estimated and expressed as a probability h p pahs with a certain value in the range of 0 to 1 concentration from low to high to introduce ecological evaluation criteria into hazard assessment model the overall ecological risk of pahs was reckoned according to sediment quality guidelines ncs and mpcs which are one of standardized methods for evaluating environmental hazard limits considering bioavailability and suitable protection for benthic organisms to prevent adverse effects mcgrath et al 2019 substituting pseudo observations processed ncs mpcs values into the established hac model the upper and lower risk limits can be derived and represented by hp ncs and h p mpcs individually similar with rq method and the meaning of ncs mpcs if h p pahs hp ncs indicates no particularly serious ecological risk and moderate potential risky might happen when hp ncs h p pahs h p mpcs negative effect to be expected for ecosystems high probability if h p pahs h p mpcs the indicator of pahs ecological risk assessment expressed as ind pahs can be calculated as follows 20 ind pahs h p p a h s h p n c s h p n c s h p p a h s h p n c s h p p a h s h p n c s h p m p c s h p n c s h p n c s h p p a h s h p m p c s h p p a h s h p m p c s h p p a h s h p m p c s eq 20 expresses the joint probabilities of three different states for ecological risk assessment generally when hp pahs is in the midst of the inferior limit hp ncs and the upper level hp mpcs the result of ind pahs indicates how closely the synthesized risk reaches the maximum permissible concentration or exceeded the negligible concentration if hp pahs is lower than hp ncs ind pahs is a negative percentage and its absolute value shows how far away the situation is from the negligible concentration otherwise ind pahs would over 1 and presents how horrible the pollution risk is therefore to refine the levels according to the risk division principle of m erm q method based on the concrete value of ind pahs the integral ecological risk of 16 pahs was divided into four categories ind pahs 0 low free risk 0 ind pahs 0 5 mid low risk 0 5 ind pahs 1 mid high risk ind pahs 1 high risk finally as a result of structuring the whole hacia model and assessing holistic ecological risk of pahs we attained a flexible evaluation model which took the internal relationships between each pah into consideration estimated these relations and quantized them into an integrate copula function and combined with ecological evaluation criteria relying on this model the hybrid ecological risk assessment of pahs was accomplished accurately and defined by the joint probability distributions which exhibited the degree of risks directly by the evaluation results both qualitative and quantitative analysis were accomplished making the assessment more efficient and convenient 3 case studies in taihu lake and the bay of bengal coast two different type of study areas were taken in this paper the taihu lake in china and the bay of bengal in bangladesh taihu lake is the main drinking industrial and agricultural water sources of several important cities surrounded and the water quality in bays is really fragile because of the frequent human activities so both of them need to be paid attention to avoid the deterioration of the ecological environment 3 1 application of hacia model to case 1 surface sediments from taihu lake in china 3 1 1 study area and statistical analysis taihu lake surrounded by several important industrial developed cities in china wu et al 2020 is the paramount drinking water source and supplies water for agricultural and industrial water withdrawals wang et al 2018 the pollution problems in taihu lake ecosystems have been closely watched by relevant departments and widely investigated xu et al 2014 zhang et al 2012 there are several summaries and comparisons of pahs concentrations in taihu lake with different sampling time and medium as well as several other lakes in china extracted from literature displayed in table 1 because of the divergences in samples the concentrations of pahs seem like disorganized as time goes by the range and mean values of total pahs concentrations declined distinctly it might be attributed to the success of environmental measures adapted to local conditions implemented by the government from ca 2000 li et al 2019a compared with other domestic lakes the concentrations of pahs in surface sediments from taihu lake in recent years was at a low level relatively apparently the accumulation of pahs in winter was higher than that in summer wherever in surface sediments or water which perhaps is relative to the fact that the lower temperature tends to promote the formation of pahs wen et al 2018 or higher utilization of fossil fuels and biofuels for heating system meng et al 2019 28 surface sediments samples from taihu lake lei et al 2014 to gauge the exposure concentrations of pahs were extracted in june 2010 with professional collection methods and stringent chemical analysis approaches the concentrations of 16 priority pahs were detected statistically the total pahs concentrations p a h 16 varied from 179 5 ng g to 1669 3 ng g dw with a mean value of 533 3 ng g dw other information was found in literature lei et al 2014 the 28 16 dataset of concentrations of pahs was applied to evaluate the integrated ecological risk by the hacia model and the ordinary rq method to confirm its feasibility compare the assessment consequences advantages and disadvantages between them and to attest that the hacia model had better performance than did the others 3 1 2 modelling hacia 3 1 2 1 correlation coefficient of exposure concentrations as mentioned above the exposure concentrations of pahs in sediments were classified into four groups first the correlation coefficients between every two pah homologues are shown in table 2 calculated by spearman rank correlation as well as multivariate kendall coefficients w among each classification with the exception of pyr all bivariate correlation coefficients were above 0 50 and even over 0 8 for most of the relations with increasing particle size the correlation coefficient was positive for multivariate kendall coefficients all of them were greater than or equal to 0 79 particularly reached 0 95 in 5 ring there were strong and significant correlations between almost all every two concentrations of pahs and definite interdependency in each group correspondingly which meant that there are dependencies in the interior of different benzene rings namely for most of pahs the concentrations of homologues were not independent and the correlations among them were large enough that could not be neglected which were inadequate for the hypothesis of additivity or non interaction the same as ca and ia models as mentioned above so it was completely appropriate to apply the copula families to construct the multi dimensional joint distribution models of exposure concentrations of pahs adequately considering the correlations among them 3 1 2 2 copulas selection in order to test the fitting effect of copula functions we fitted all bivariate combinations in each group firstly based on the mpl algorithm compute all parameters and goodness of fit indicators by loops iterating whole possible combinations may exist with three kinds of archimedean copula functions then in terms of the goodness of fit criteria the best fit copula function could be selected table s1 shows the calculated results of selected bivariate copulas including parameters aic and s n in accordance with each classification choose the minimum aic which represents the best choice of copula function and minimal s n indicates the corresponding goodness of test which can be a reference when judging the optimal copula the optimization has been identified as bold type with the benzene rings going up clayton copula exhibited a tendency of mis convergence with the emergence of na although the clayton performed well in respect of aic especially in 4 ring for s n all of them took precedence over both in 4 ring and 5 ring frank copula had poor performance in both indicators and gumbel copula fitted most of the exposure concentrations of pahs much better than did others in archimedean copula family taking 5 ring pahs as examples fig 3 shows the fitting effect of 6 pairs of bivariate 5 ring pahs by the optimal copula respectively which demonstrates that it had excellent fitting effectiveness to 5 ring pahs because all analog values were surrounding the empirical sites and characterizing their distribution features clearly in other groups there were analogous incidents that appeared similarly then according to the values of θ aic bic and s n different copulas were selected as the most optimal fitting functions in each group to structure the foundation of hacia model the results are shown in table 3 and all estimated model parameters satisfied the requirement of p value 3 1 2 3 configuration of hacia using the gumbel copula and parameters of each classification the joint distributions were obtained for every benzene ring the best fitted hac model was constructed by r studio and was indicated that gumbel copula performed the best in this case eq 21 showed the optimal structure of hac in taihu lake 21 h 3 h 2 h 1 u r 3 u r 4 4 1178 u r 5 3 3977 u r 2 2 4617 according to the final hac model the integral distribution considering internal correlations of multiform pahs at every site was estimated and presented by hp pahs hence these distribution numbers implied their appropriate levels of joint pollutant concentrations among all samples to evaluate the integral ecological risk of 16 pahs the synthesized quotas of toxicant reference values containing hp ncs and hp mpcs were obligatory to take into the whole model and obtain their values respectively relying on the values of hp pahs hp ncs and hp mpcs the final indicator of integral ecological risk in each sample site was assessed by eq 20 and the corresponding category could be identified due to the small number of samples of pahs exposure concentrations uncertainty analysis was estimated by the parametric bootstrap method jiang et al 2019 and the 95 uncertainty intervals were adopted 3 2 application of hacia model to case 2 surficial sediments from the bay of bengal coast areas in bangladesh rapid industrial development and economic transformation have brought a large amount of pollution in the developing world which has discharged into coastal areas including bangladesh liu et al 2019 the details of the bay of bengal and samples collection can be found in habibullah al mamun et al 2019 the concentrations of 16 pahs were compared between winter and summer with 349 8 to 11 058 8 ng g dw 4571 0 ng g dw in winter and 199 9 to 17 089 1 ng g dw 5729 0 ng g dw in summer for 28 samples in total on account of the structure progress were analogous with earlier details are omitted here the spearman rank correlation coefficients and multivariate kendall coefficients of 16 pahs in the bay of bengal coast areas in winter table s2 and summer table s3 indicate that there are close correlations among multiple compounds although the relationships of pahs exposure concentrations in summer are stronger than that in winter they are slightly weaker than taihu and not as significant as that but most of them perform obviously positive correlations with all w above 0 5 thus the hacia model was feasible here in terms of the goodness of fit test results of the pahs exposure concentrations on the copula functions in each group the optimal archimedean copulas and the values of parameter for each category in winter and summer are shown in table 4 clayton copula was the best fit function among archimedean copulas for pahs in winter while for pahs exposure concentrations in summer gumbel and frank copula each accounted for half of the optimal fitting function the structures of hacia for winter and summer are displayed in fig 4 and gumbel hac was the most suitable choice for both of them the uncertainty analysis was calculated as same as the previous example 4 results and discussion 4 1 case 1 surface sediments from taihu lake in china 4 1 1 integral ecological risk assessed by hacia model for the taihu lake in china the final indicator values of integral environmental risk of 16 pahs exposure concentrations in each sediment sample assessed by hacia model and the uncertainty analysis of hp pahs are illustrated in figs 5 and 6 respectively the higher value of ind pahs indicates the more dangerous risk otherwise with lower risk as seen from the results of hacia model the joint exposure concentrations of 16 pahs of all samples are under the maximum permissible reference value indicating that no place had the affiliation to high risk level and most of integral risks belonged to the middle interval with hp ncs hp pahs hp mpcs meant that the majority of sites had moderate integral ecological risk of the exposure concentration of pahs obviously s21 and s27 were in absolutely good conditions low free risk level and s4 and s8 were relatively safe whose indicator values were quite close to 0 so those samples were almost without any worries about the integral ecological risk of pahs by contrast s6 and s7 were nearly approaching the upper level whose indicator values as much as 85 51 and up to 86 81 respectively followed by s8 73 82 s13 69 62 and s19 67 98 s3 s5 to s9 s13 and s19 all of them in mid high risk level had dramatically higher risks than others represented more abominable situations which meant that they were facing correspondingly grave ecological problems of pahs others belonged to mid low risk level overall according to the hacia model almost all of the synthesized ecological risks of pahs in the study area belonged to moderate risk level especially mid low risk indicating that they needed to be paid close attention to improve the status in general two samples among them were regarded as totally safe conditions and the other two were relatively secure but several additional sites were approximate to the upper limit which needed to take reasonable measures to suppress the feasibility of deteriorating trend in other words the holistic ecological situation of pahs in our study area was still sanguine but some symptomatic measures needed to be taken at several sites 4 1 2 results construct with hacia model and normal rq method table 5 demonstrates the additive risk evaluated by the normal rq method from the judgement criteria and computed results it was found that all values of rq ncs were under the standard 800 but more than 0 and most of them fluctuated around 100 almost all rq mpcs equaled 0 with the exception of s7 taking the risk levels into account it was clear that s7 belonged to moderate risk 2 level and others were low risk from the diagram it can be visibly founded that the values of rq ncs in s5 s9 s13 s19 far outweighed the others among those sites marked as low risk level but we couldn t acquire the disparities between each of them on the basis of assessment results merely by the rq method likewise although the evaluated values of several of them were quite tiny and their risk levels were low enough seemingly like s4 and s27 it was unable to ascertain how low the risks were actually both methods estimated the integral ecological risk of 16 pahs in 28 samples and derived evaluations individually they all obtained identical conclusions that the holistic environmental status of pahs in our study area belonged to lower risk level without high risk stations and s7 had the most serious ecological risk although s6 was the maximum of total pahs concentrations among all samples dissimilarities existed as well first comparing the judgements of ecological risk assessment scale there was only s7 which was sorted into moderate risk 2 level by the rq method and others were two grades lower but 8 sites including s7 were categorized as mid high risk by the hacia model for which all 8 sites had higher values of rq ncs in the rq method second there were no sites judged as moderate risk 1 level in the rq method with both low risk and moderate risk 2 levels existing which was absurd because s1 s9 were closely distributed in meiliang bay continuously and evenly and their concentrations varied consecutively in theory it was unlikely to have such a leap third contrary tendencies between site pairs emerged in hacia and rq methods as a result of whether calculate their intrinsic correlation among pahs fig 7 demonstrates the comparison of total concentration rq ncs and ind pahs along sample sites clearly there were analogous variation tendencies between rq ncs and total concentrations except for s6 and s7 somewhat differently for example although rq ncs of s3 was lower than s5 the same as changing in total concentration which was finally judged as low risk by rq while it was deemed to be more hazardous than s5 reckoned by hacia other pairs like s1 and s2 s9 and s5 and so on last but not the least in fig 5 the joint ecological risks of pahs for each site are demonstrated prominently by quantitatively providing useful information directly including whether there was an ecological risk of integral pahs or not to what extent the risk influences our ecosystem and the comparison with other sites indicated clearly the values beside the bar indicate the integral ecological risks of each site and the division lines of risk levels are shown directly in the figure which could help to classify and identify them of samples accordingly to be frank all the above were superior to the illustration of rq method which was monotonous and ambiguous 4 2 case 2 surficial sediments from the bay of bengal coast areas in bangladesh 4 2 1 the integral ecological risk assessment by hacia model the assessment of integral ecological risk in winter and summer evaluated by hacia model could be found in fig 8 and their uncertainty analysis could be found in fig 9 the holistic status of pahs contamination manifested unsatisfactory situations either in winter or in summer almost half of the samples belonged to the mid low risk level but cx3 ct1 ct3 in both seasons and ct2 in winter exceeded the overall maximum permissible guideline high risk ct1 had both maximum values of integral ecological risks in winter and summer beyond the guideline over 60 far away higher than another in summer cx3 just surpassed the criterion and ct3 was higher in both seasons for ct2 the hazard increased dramatically in winter ranked the second to ct1 than in summer which was mid high risk in summer provisionally the vicinities surrounding all the above sites should adopt mitigation measures immediately to impose restrictions on anthropogenic activities which can emit quantities of pahs petrogenic and pyrogenic sources zakaria et al 2002 particularly incomplete combustion huang and batterman 2014 others need relevant attention to restore or maintain the ecological environment me3 had the lowest integral hazard of pahs followed by me2 and cx2 which were relatively safer and less misgivings about pahs compared to others without recognizable regulations of the risk entirety influenced by season several sites had higher risk in summer and others were on the contrary some of them had conspicuous discrepancy between winter and summer such as ct2 especially sn2 and cx4 but the majority of them had approximate hazards whose risk differences were under 20 4 2 2 results construct with hacia model teq bap erls erms and m erm q methods the results of hazard estimation by teq bap and erls erms methods were found in the proceeding literature habibullah al mamun et al 2019 the general tendencies of holistic ecological risk assessment consequences by hacia teqbap and m erm q approaches fig 8 were basically concordant whereas the hierarchical classification of sample risks had distinctions after calculation all p a h s were below e r m s and almost half of them under e r l s with several concentrations of individual pah surpassed the corresponding erm which indicated that the potential biological effects emerged frequently at those sites in m erm q method only ct1 in summer was judged as medium high level with a large proportion of samples divided into medium low class and others belonging to low risk level me3 me2 and cx2 were acknowledged as the top three safe sites by all four approaches and ct1 had the most severe status of toxicity especially in summer there were some differences in the judgements about sn2 and sn3 from the results of teq bap both of them had severe carcinogenicity in winter even more dangerous than or much the same as ct1 in spite of their p a h s being far below but the results of hacia revealed that although the hazards of both sites in winter were more serious than in summer they were not threatening enough to overwhelm ct1 or some others nevertheless opposite trends about seasons were demonstrated in m erm q method with both of them belonging to medium low risk level 4 3 discussion due to the lack of standard methods for ecological risk assessment the ecological risk assessment methods used in this paper is compared according to the theory and practice namely the comparison between haica and other methods is provided from the theoretical assumptions and practical conditions such as the relationship of location and concentration or influencing factors to argue the degree of conformity with the facts and justice whether the results are reasonable accurate and reliable theoretically rq method has concise principle simple calculation steps and intelligible graphical results meanwhile with a few fundamental but important limitations both in theory and practice first in respect of risk division standards there was a rigorous standard to meet risk free level that rq ncs must equal to 0 which is a definitely infrequent phenomenon pahs are ubiquitous environmental pollutants and could directly emanate from anthropogenic activities and natural pathways kalisa et al 2018 and insignificant amounts of them could be degraded by soil indigenous microbial populations in the environment the description in hacia model of it when hp pahs was under hp ncs is seemingly more rational second pah items in calculation were restricted when computing the values of rq mpcs only cumulative partial rq mpcs satisfied rq mpcs 1 instead of the whole the behavior ensured to protrude those sites with higher risks that focused on single or several pah s but to the dispersive risk situations where most risks of pahs almost reached the criteria or below rq mpcs was incapable to accurately indicate their ecological risks even if rq ncs 800 it would be judged as moderate risk 1 level which is unreasonable in reality what s more rq method is founded on the hypotheses of independency and additivity of pahs radically which is inconsistent with the reality attested by the correlation coefficients between and among pahs calculated above the occurrences of divergent tendencies also testified that the holistic ecological risk was not always positively correlated with cumulative concentrations therefore the normal rq method is deficient in assessing the joint ecological risk of pahs compared to erms the values of mpcs were much smaller for lots of pahs resulting in probably conservative conclusions with regard to ecological risk assessment which were more rigorous and beneficial to the ecosystem needing consume enormous resources to curb environmental pollution the tef value of ip was 0 1 far outweighing partial of pahs which had to been neglected when estimating ecological risks by erls erms and m erm q method because of the deficiency of corresponding guidelines in sqgs this may cause consequences of holistic hazard assessment to be underestimated and inaccurate besides the mean calculation weakened the influence of maximum values which caused that the consequences could not indicate the individually swinging risks by the m erm q method in general the results of teq bap can only roughly reflect the status of carcinogenicity in the study area by statistics of sectional pahs instead of the entirety far from the complete toxicity moreover the lack of classification standards of hazards constrains the scope of application on the contrary the hacia model compensates for several of the above weaknesses taking the existing correlations among pahs as presuppose and combine with ecology evaluation criteria are the quintessence of hacia model relying on the flexibility and friendliness of hac the holistic exposure concentrations were figured out and the correlations of pahs were reckoned with throughout depending on the inherent features of copula functions whose conclusion is different from direct accumulation or weighting besides the assessment model was more feasible in practice with different distributions of 16 pahs concentrations the integral values of guidelines hp ncs and hp mpcs were disparate varying with the calculated functions defined by hacia model which enhancing the flexibility of guidelines the computation indicated the extent of integral concentrations overstepped the ecological index by probabilities directly which can characterize risk levels more consecutively and logically conformed with the actual demand the joint risk degrees would not be dominated by several centralized high hazard merely the hacia model considered both qualitative and quantitative analysis quantitative analysis while only the former by other methods without impractical hypotheses although slightly intricate in computational processes the hacia model is feasible to evaluate the comprehensive hazards of pahs and its grading standards are more consecutive and logical which is a preeminent assessment model for integral ecological risk than traditional assessment methods like rq teq bap and m erm q focusing on holistic ecological risks instead of the particular which guarantees that the final decisions of risk grades would not be controlled by several pahs with merely higher risks or omitting the maximum values although the hacia model does not consider the toxicological property of each pahs to evaluate holistic ecological risk straightaway with no method can cover them actually the evaluation results obtained by the overall analysis of concentrations are more viable theoretically handled all pahs in application which outbalanced than teq bap and others distinctly 5 conclusions in this paper a new hybrid assessment model the hierarchical archimedean copulas integral assessment hacia was developed which can indicate the interdependence among pahs exposure concentrations it couples hierarchical archimedean copulas hacs and ecological evaluation criteria sediment quality guidelines sqgs together to construe the connotations of correlations among exposure concentrations of pahs and assess the multi dimensional holistic ecological risks of all 16 pahs the two case studies the exposure concentrations of 16 pahs from taihu lake in china and the bay of bengal coast in bangladesh respectively are used to verify the adaptability and feasibility of hacia model and evaluate their comprehensive ecological risk according to pahs exposure concentrations then provided a theoretical and empirical comparison with three frequently used methods rq teq bap and m erm q from presuppositions principles evaluations and consequences respectively the results indicated that the hacia model is feasible to evaluate the integral ecological risk of pahs and is more preeminent than the other three methods it takes the interdependence among variates into account instead of improper direct addition or weighting calculation which guaranteed comprehensiveness and reliability of evaluation results what s more the grading standards of hacia are more reasonable the situation in which the assessment results are dominated by one or several indicators is avoided and the classification could disclose the degree of overall risk than other methods the hacia model construes the connotations of correlations among pahs exposure concentrations so there is a requirement that more sample capacity needs to be extracted in the same areas to hacia model that to fit the distributions of pahs small samples might result in enormous uncertainty in the conclusions of risk assessment obviously due to the decrease in sample size the uncertainty of the evaluation results in the bay study area increased significantly additional measures are needed to improve model assessments consequently although high quality detecting samples are needed to obtain more accurate risk assessments it deserves to be promoted and further research to evaluate the integral ecological risk of pahs especially when the number of samples obtained is not enough due to limited monitoring conditions credit authorship contribution statement wenyue liu methodology formal analysis writing original draft writing review editing dong wang conceptualization writing review editing validation project administration supervision vijay p singh formal analysis writing review editing yuankun wang writing review editing validation xiankui zeng writing review editing lingling ni data curation yuwei tao investigation jichun wu validation jiufu liu validation ying zou visualization ruimin he visualization jianyun zhang visualization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this study was supported by national key research and development program of china 2017yfc1502704 2016yfc0401501 the national natural science fund of china no 41571017 51679118 91647203 and the jiangsu province 333 project bra2018060 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2020 124612 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
