index,text
25430,internal feedback of nutrients may impede timely improvement in lake water quality we describe a parsimonious mechanistic framework for modeling lag times to recovery of phosphorus enriched lakes given decreases in external loading the approach assumes first order kinetics in a two compartment system taking account of phosphorus storage in and loading from benthic sediments bayesian parameter modeling published sediment phosphorus release rates and a prior dynamic calibration for one lake are used to derive estimates of key parameters applications are developed for an example lake as are maps displaying estimated times to attainment of a phosphorus criterion in lakes across a midwestern state and lag time estimates for fractional water column concentration decrease averaged over huc 8s mean lag times to 50 and 75 declines in water column phosphorus concentration were estimated as 13 1 and 39 0 years respectively across more than 70 000 lentic water bodies in the continental united states keywords lake phosphorus sediment feedback legacy nutrients time lag data availability data will be made available on request 1 introduction eutrophication remains a pervasive cause of impairment for many aquatic ecosystems and negatively impacts the numerous ecosystem services that they provide for the environment and human well being strategies to combat eutrophication have shifted over time as the roles of point source diffuse source and legacy nutrients have come to light as interacting causes of nutrient enrichment underlying eutrophication chen et al 2018 jarvie et al 2013 le moal et al 2019 sharpley et al 2013 over the past four decades multiple approaches have been used to improve nutrient use efficiency associated with agricultural food production and to reduce nutrient loading to waterbodies from both point and nonpoint sources ivahenko 2017 sharpley et al 2019 sabo et al 2021 while there have been positive directional changes in some areas eutrophication of lakes and reservoirs continues to be a widespread and wicked problem for environmental management thornton et al 2013 seemingly less than expected progress is not just a disheartening reality faced by adopters of conservation practices but the basis of increasing cases of litigation and divisiveness among stakeholders sharpley 2018 understanding limitations of nutrient management strategies and setting realistic expectations for lake recovery require a better understanding of the behavior of legacy nutrients in both watersheds and waterbodies kleinman et al 2011 the importance of phosphorus interactions with benthic sediments in mediating eutrophication in lakes has been recognized for decades as reductions in external phosphorus loading have often failed to quickly bring about commensurate reductions in lake water column phosphorus concentrations jeppesen et al 2005 søndergaard et al 1999 sharpley 2018 eutrophication dynamics in lakes are often complex with phytoplankton biomass influenced by factors including zooplankton grazing lake morphometry temperature and food web structure lepori and roberts 2017 despite this complexity phosphorus is usually recognized as an important limiting nutrient for phytoplankton growth in freshwater systems elser et al 2007 and efforts to reverse eutrophication in such systems have generally focused on trying to reduce input loading of phosphorus thornton et al 2013 the failure of some such efforts to promptly achieve desired improvements has led researchers to study the dynamics of phosphorus within lakes including its interactions with various chemical components of benthic sediments and its exchange across the sediment water interface redox sensitive iron complexes particularly have been recognized as playing a role in retaining phosphorus within the sediments gächter and müller 2003 jensen et al 1992 ogdahl et al 2014 søndergaard et al 2001 and release rates of phosphate to the water column have been found to vary seasonally in lakes that experience seasonal hypoxia irrespective of the presence or absence of thermal stratification ripl 1986 welch and cooke 1995 the phenomenon of phosphorus retention in lakes has been studied for decades by researchers around the world who have explored various competing mechanistic and statistical models most often based on a presumption of static conditions using published data from 305 lakes brett and benjamin 2008 evaluated five such models or hypotheses all of the following general form 1 t p l a k e a t p i n 1 k τ equation 1 relates in lake water column total phosphorus concentration tplake to influent phosphorus concentration tpin residence time τ first order removal rate k and a factor a that can empirically account for the loss of a fixed fraction i e 1 a of influent tp note that when a 1 eq 1 is identical to the mass balance based formula for concentration in a continuously stirred tank reactor cstr four of the five hypotheses evaluated by brett and benjamin 2008 concerned solely the nature of k including its possible relationships with other quantities e g residence time water column mean depth while a single hypothesis concerned the possibility of non unity values of parameter a of the five hypotheses the authors found the best performance in terms of matching monitoring data was for a model in which k was a power function of residence time with an exponent close to 0 5 in other words k was found to be nearly proportional to the inverse square root of residence time khorasani and zhu 2021 used an even larger dataset 738 lakes similarly to examine the performance of 39 static lake phosphorus models representations included assumptions of both complete and plug flow mixing first and second order tp loss kinetics and loss rate coefficients that are functions of other measured quantities based on bayesian information criteria bic the authors found their best fit to data with a semi mechanistic model that assumes compete mixing and second order kinetics and that treats k as a complicated power function of mean water column depth residence time and influent tp concentration itself like brett and benjamin 2008 these authors found tp removal to be better represented as a volumetric process i e rate coefficient having units of inverse time than a settling based one i e fixed settling velocity by contrast with static or steady state models efforts to simulate lake phosphorus concentrations dynamically including addressing internal nutrient loading and or times to recovery from eutrophication have often involved the development of mechanistic models focused either on specific lakes edlund et al 2017 lewis et al 2007 wang et al 2019 or on lakes that have specific kinds of biological and or morphological attributes e g cottingham et al 2015 relatively few approaches have incorporated sufficient complexity to allow realistic representation of benthic sediments as dynamic sources and sinks of phosphorus while requiring sufficiently minimal parameterization that rapid application to large numbers of lakes might be feasible the primary objective of this study was to develop a simple generic approach that incorporates the influence of benthic sediments but requires minimal site specific parameterization for predicting plausible trajectories of water quality improvement in lakes following hypothetical sustained decreases in influent tp loading despite the fact that better performance for static models has been found in those that incorporate more predictive variables khorasani and zhu 2021 we chose an approach centered on a simplified version of the two compartment chapra and canale 1991 model because this model is dynamic parsimonious and most importantly incorporates a mass balance based representation of phosphorus in the benthic zone the model thus addresses in a quantitative mechanistic manner the dominant source of internal phosphorus loading in lakes our implementation of the chapra canale model uses idealized representations of influent load change for the sake of general applicability and bayesian estimation to address the impact of uncertainty on parameters and thus on modeled recovery lag times in keeping with the finding that they provide better representations for static lake data brett and benjamin 2008 khorasani and zhu 2021 our approach also employs volume based rather than velocity based rate coefficients in the governing process equations the application of this approach to many lakes allowed for rapid estimation and mapping of predicted recovery times for thousands of lentic water bodies in wisconsin and tens of thousands across the conterminous u s the maps provide visualizations of expected times to reach target lake tp concentrations given specific decreases in tp loading to improve understanding of lag times and to inform expectations for lake recovery 2 methods our framework for modeling lag times to lake recovery centers on a simplified version of a two compartment model developed by chapra and canale 1991 the model included the following mass balance based representation of tp in a lake s water column and active benthic zone 2 v 1 d c 1 d t w q c 1 v s a c 1 v r a c 2 v 2 d c 2 d t v s a c 1 v r a c 2 v b a c 2 where c1 and c2 are water column and benthic zone tp concentrations respectively w is external loading e g in mg day units q is flow e g in m3 d units v1 and v2 are water column and benthic zone volumes e g m3 units respectively a area of benthic water column interface e g in m2 units v s phosphorus settling velocity e g in m d units v r recycle velocity and v b burial velocity by dividing out the compartment volumes this may be expressed in the form 3 d c 1 d t 1 τ c i 1 τ k s c 1 k r d 2 d 1 c 2 d c 2 d t k s d 1 d 2 c 1 k r k b c 2 where τ is hydraulic residence time e g in days ci is influent concentration e g in mg m3 units d1 and d2 are water column and benthic zone depths and ks kr and kb are phosphorus settling recycle and burial rate coefficients e g in day 1 units respectively solutions for the steady state case an idealized construct in which flow influent and internal concentrations are unchanging with time may be derived by setting the derivatives equal to zero and solving for c1 and c2 algebraically yielding 4 c 1 c i 1 k s k b k r k b τ and 5 c 2 c i d 2 d 1 k s k r k b 1 k s τ like eq 1 when a 1 eq 4 is identical to the expression for steady state concentration in a cstr in this case where the effective static first order tp loss or retention coefficient keff is itself a function of the three dynamic process coefficients in eq 3 6 k e f f k s k r k b 1 for dynamically changing influent i e non steady state eq 3 may be solved for c1 and c2 using numerical integration when flow and therefore τ but not influent concentration is constant solutions may also be obtained more directly via convolution carleton 2015 briefly the first step in this approach involves using matrix manipulation to disentangle eq 3 into separate expressions for c1 and c2 with ci set to zero by setting the initial compartment concentrations c1 0 and c2 0 to 1 and 0 respectively the expressions thus obtained are rendered equivalent to impulse response functions irfs for each of the compartments with respect to a constituent in this case tp that enters compartment 1 i e the water column in the influent for example 7 i 1 t 1 τ λ 2 a λ 2 λ 1 e λ 1 t λ 1 a λ 2 λ 1 e λ 2 t represents the water column s response to a unit influent concentration pulse at t 0 where λ1 and λ2 are eigenvalues of the 2 2 coefficient matrix for the system of equations 8 d c 1 d t a c 1 b c 2 d c 2 d t e c 1 f c 2 in this representation a b e and f are shorthand for the corresponding coefficients in eq 3 water column or benthic compartment concentration cx generated by a fluctuating influent concentration time series ci t may then be calculated as a function of time using 9 c x t 0 c i t θ i x θ d θ where ix represents the compartment s irf for an influent concentration time series of arbitrary complexity the corresponding internal compartment time series cx may be obtained via the inverse fast fourier transform fft of the product of the fft s of ci and ix between the extremes of steady and continually fluctuating inputs to a linear system such as this two compartment lake model one can envision cases of intermediate complexity in terms of external forcing wherein a system that is initially at steady state is perturbed by an abrupt change in its influent at some time zero in the post time zero condition which we refer to as perturbed steady state for some idealized forms of influent concentration change exact expressions for resulting compartment concentration cx as a function of time may be derived via forward and inverse laplace transform applied to eq 9 in the simplest example of this idea we consider a two compartment lake that is initially at steady state with fixed influent concentration ci 0 and for which ci undergoes an instantaneous step change to a new fixed lower concentration αci at t 0 the post time zero water column concentration response thus derived is 10 c 1 t m 1 e λ 1 t m 2 e λ 2 t m 3 where the coefficients may be expressed as functions of initial pre time zero i e steady state concentration in either the influent ci 0 or water column c1 0 11 m 1 λ 2 a b e f 1 α λ 2 λ 1 c 1 0 λ 2 a α 1 λ 2 λ 1 c i 0 τ λ 1 12 m 2 a λ 1 b e f 1 α λ 2 λ 1 c 1 0 a λ 1 α 1 λ 2 λ 1 c i 0 τ λ 2 13 m 3 α c 1 0 α c i 0 f τ b e a f the corresponding expression for the benthic compartment is 14 c 2 t λ 1 a b m 1 e λ 1 t λ 2 a b m 2 e λ 2 t e f m 3 3 example concentration decline simulations chapra and canale 1991 calibrated a dynamic implementation of their model to match tp concentration data from the water column of shagawa lake minnesota sampled over the course of the year 1972 their fig 5 their calibrated parameter values were as follows tp settling velocity vs 42 2 m yr burial velocity vb 8 03e 4 m yr summer anaerobic recycle velocity vr 0 0115 m yr and winter aerobic recycle velocity vr 0 00494 m yr the authors listed shagawa lake s mean water column depth as 5 5 m and they assumed a benthic compartment depth of 0 1 m using these depths their calibrated parameters translate for our purposes to ks 0 021 d 1 kb 2 2e 5 d 1 summer anaerobic kr 3 15e 4 d 1 and winter aerobic kr 1 35e 4 d 1 their fig 5 suggests that summer anaerobic conditions prevailed for 195 days and winter aerobic conditions prevailed for 170 days of the year which equates to an annual mean kr of 2 31 e 4 d 1 their paper also lists lake volume and mean flow rate for 1967 1972 that equate to a mean hydraulic residence time of 228 days as well as a mean influent tp concentration of 84 4 μg l during that year fig 1 displays results for a water column eq 10 simulation employing these values with α 0 i e influent concentration at zero for all positive time an equivalent cstr type i e water column only representation is also shown for the purpose of comparison similar to their fig 8 with washout at the usual 1 τ rate plus non washout first order tp loss at the corresponding keff eq 6 rate in the two compartment representation concentration initially declines at a rate similar to that in the cstr model but that rate quickly transitions to much slower decline over subsequent years and decades as the benthic compartment gradually bleeds off its store of tp into the water column two compartment model results eqs 10 and 14 for the same system with for illustrative purposes arbitrary assumed α values of 0 7 i e 30 step drop in influent tp and 0 3 i e 70 drop in influent tp are shown in fig 2 a b and c d respectively in both compartments new non zero steady state concentrations are approached asymptotically with time an instantaneous influent concentration or loading step drop equivalent under the assumed steady flow conditions may be less physically plausible than a more gradual influent decline over some time period e g as watershed management measures gradually decrease nonpoint source loading one simple idealized representation of such behavior is first order decline from ci 0 toward αci 0 where α again is a fraction between 0 and 1 15 c i t 1 α c i 0 e ε t α c i 0 in which case the following expressions analogous to eqs 10 and 14 respectively may be derived for the water column and benthic concentration responses 16 c 1 t n 1 e ε t n 2 e λ 1 t n 3 e λ 2 t m 3 17 c 2 t λ 1 a b λ 2 a f ε n 1 e ε t λ 1 a b n 2 e λ 1 t λ 2 a b n 3 e λ 2 t e f m 3 where 18 n 1 f ε λ 2 λ 1 λ 1 λ 1 ε λ 2 ε λ 2 a m 1 19 n 2 ε λ 1 ε m 1 20 n 3 ε λ 2 ε m 2 as one might suspect from visual inspection of fig 2 a vs c and 2 b vs d for a given set of parameters the decline curves within the range defined by α are simply rescaled versions of the same shape with the scaling factor defined by the magnitude of α in other words the lag time to attain a given percent decline in water column concentration is a function of the percent decline in the influent concentration and not of the initial and target concentrations per se thus for example the time required to reach 90 of an asymptotic 50 decline in water column concentration following a 50 drop in influent concentration is the same as that required to reach 90 of an asymptotic 10 decline in water column concentration following a 10 drop in influent concentration eqs 10 and 14 may be converted to dimensionless form i e water column concentration decrease as a fraction of influent concentration decrease by normalizing over both initial concentration and the quantity 1 α i e the maximum potential decline for the water column we first normalize concentration by its value at time zero 21 β 1 t c 1 t c 1 0 further normalizing by 1 α eq 10 becomes 22 1 β 1 t 1 α 1 m 1 e λ 1 t m 2 e λ 2 t where 23 m 1 λ 2 a b e f λ 2 λ 1 m 1 1 α c 1 0 and 24 m 2 a λ 1 b e f λ 2 λ 1 m 2 1 α c 1 0 to reiterate in eq 22 1 β1 t is the time course of fractional decrease in water column concentration that occurs in response to a 1 α fractional decrease in the influent concentration starting at t 0 the form of eq 22 reflects the fact that the water column concentration decline trajectory scales with the magnitude of the influent drop an expression for the benthic concentration derived analogously is 25 1 β 2 t 1 α 1 f b e a λ 1 m 1 e λ 1 t a λ 2 m 2 e λ 2 t dimensionless versions of the expressions for exponential influent decline eqs 14 and 15 may be obtained in the same manner eqs 26 and 27 26 1 β 1 t 1 α 1 n 1 e ε t n 2 e λ 1 t n 3 e λ 2 t 27 1 β 2 t 1 α 1 f b e a λ 1 λ 2 a f ε n 1 e ε t a λ 1 n 2 e λ 1 t a λ 2 n 3 e λ 2 t where 28 n 1 n 1 1 α c 1 0 29 n 2 n 2 1 α c 1 0 30 n 3 n 3 1 α c 1 0 fig 3 shows example dimensionless water column and benthic compartment concentration decline curves following both a step drop and an exponential drop eq 15 in influent tp the exponential influent tp decline in this example corresponds for illustrative purposes with a rate of 10 year ε 2 738e 4 d 1 as expected for both the water column and benthic zone the impact of an exponential influent decline as compared with an instantaneous drop is a rightward shift in the curve to longer recovery times fig 3 4 determining representative kinetic parameter values because every term that includes b or e eqs 11 13 25 27 involves their product the compartment depth ratios eq 3 cancel and every term in eq 10 and every term but the influent concentration decline rate ε in eq 16 is actually a function of just five variables c1 0 extant mean water column tp concentration τ hydraulic residence time and the three dynamic rate coefficients ks kr and kb values of extant water column tp and residence time may be either available from measured data or estimatable based on other available information for many lakes throughout the u s and elsewhere but values for ks kr and kb are not thus the major challenge in applying eq 10 or 16 is obtaining defensible estimates of these three rate coefficients preferably along with some measure of their uncertainty to derive credible estimates of ks kr and kb we started with brett and benjamin s 2008 tp data from lakes presumed to be in steady state we used these data to extract information on the dynamic coefficients relative magnitudes as the three coefficients are found in the form of ratios that comprise the expression for the static condition tp loss coefficient eq 6 as noted eq 1 with α 1 is identical to the steady state formula for concentration in a and brett and benjamin 2008 obtained their best fit to these data with a model eq 31 with τ in units of days of this form wherein effective k was approximately proportional to the inverse square root of τ their hypothesis 4 31 k e f f 0 048 τ 0 47 this essentially reprises the findings of vollenweider 1976 and larsen and mercier 1976 based on other data making use of this general result without positing any particular mechanistic rationale to explain it we assume that the power function form manifests specifically from a relationship between ks eq 6 and τ 32 k s θ τ φ thus we assume that the effective first order water column tp loss coefficient is of the form 33 k e f f θ k r k b 1 τ φ which means water column tp concentrations for lakes in steady state are estimated as 34 c 1 c i 1 θ k r k b 1 τ φ 1 given paired measurements of ci and c1 from a number of steady state lakes one may obtain estimates of θ kr kb and φ by optimizing predicted c1 against observed c1 as a function of observed ci and τ this may be facilitated by using a log transformed version of eq 34 35 ln c 1 ln c i ln 1 θ k r k b 1 τ φ 1 alternatively estimates of θ kr kb and φ may be obtained by optimizing estimated keff against observed i e back calculated from data keff as a function of τ e g by using a log transformed version of eq 33 36 ln k e f f ln θ φ ln τ ln k r k b 1 in this conceptualization for any given lake the settling coefficient is a function solely of residence time with τ either known or estimated it is possible to estimate ks and the ratio kr kb by optimizing either predicted concentration or effective k i e eq 35 or 36 against steady state data if an independent estimate of either kr or kb is also available then we effectively have estimates of all three coefficients and thus everything needed for calculating recovery times using for example eq 10 if c1 0 is also known or eq 22 for specific representative estimates of ks kr and kb we started with ks and kr kb values based on the dynamic lake shagawa calibration as reported by chapra and canale 1991 we used these values assuming φ 0 5 to specify the means of prior distributions of θ and kr kb in bayesian parameter modeling conducted as described above using steady state brett and benjamin 2008 lake tp data we chose fairly tight low variance priors rather than uninformative ones to ensure that the posterior distributions were centered relatively close to the chapra and canale 1991 based ks and kr kb estimates this process generated posteriors for θ φ and kr kb that are informed by the chapra and canale calibration and that are also intended to embody the variance present in static input output lake tp monitoring data results were captured in mcmc output which was stored upon generation and subsequently made use of together with literature based estimates of kr in producing essentially distributions of recovery times that reflect the impact of parameter uncertainty we note that infinitely many combinations of hypothetical values of dynamic parameters ks and therefore θ and φ kr and kb exist that correspond with a given value of the static parameter keff and each such combination will result in a different lake recovery trajectory per eq 10 etc best fit based parameter ratios derived from static datasets thus provide information that only partially constrains potential magnitudes of the dynamic process parameters i e ks kr and kb fig 4 illustrates this concept showing as solid black lines ranges of combinations of θ and kr kb consistent with the spreadsheet optimized in a squared error minimization sense values of the ratio θ kr kb 1 0 0701 and 0 1388 for subfigures a and b respectively parameter values derived from and consistent with the chapra and canale 1991 shagawa lake calibration are indicated by green diamonds with θ calculated based on the assumptions ks 0 021 d 1 τ 228 days and φ 0 53 and 0 65 exponent estimates also obtained via squared error minimization in subplots a and b respectively post burn in markov chain monte carlo mcmc samples generated as described in section 5 are shown as gray dots and their mean values are shown as red triangles fig 4 a shows parameter estimates and mcmc results from the first bayesian model eq 35 while fig 4 b shows the same from the second eq 36 the figure indicates how in both cases we use parameter values believed to adequately represent dynamic tp behavior in one lake the green diamonds to help define plausible values of parameters whose values we can infer only in the form of ratios see eq 6 based upon steady state data from hundreds of other lakes the variability of observed tp data in this lake dataset helps inform the uncertainty associated with dynamic model parameters θ and kr kb and with associated recovery lag times measurements of phosphorus burial and water column settling rates independent of sediment recycling are not generally available and would be challenging to directly measure however laboratory procedures for straightforwardly measuring sediment tp release rates do exist and indeed such measurements from the sediments of many lakes have been tabulated in the literature nürnberg 1988 measured p release rates under anoxic conditions from sediments of seven north american lakes and showed that release rates mg m2 d were strongly correlated with total sediment phosphorus concentrations mg g dry weight tabulated p release rates paired with sediment tp concentrations from 63 lakes in the literature were also provided table 6 in nürnberg 1988 van der molen and boers 1994 similarly reported internal phosphorus loading rates g m2 yr from 49 shallow mean depth 6 m lakes as well as sediment p concentrations g kg dry weight for 45 of the lakes together these datasets comprise a total of 115 paired lake sediment p concentration and release measurements from each of which a kr value d 1 can be calculated using the formula 37 k r r r c a where rr is the sediment p release rate mg m2 d and ca is areal sediment p concentration mg m2 ca is calculated from sediment p concentration via 38 c a t p s b d d 2 10 6 where tps is total sediment phosphorus concentration mg g dry weight bd is sediment dry bulk density g cm3 d2 is active sediment layer depth assumed here to be 0 1 m and 106 is a unit conversion factor sediment dry bulk density is unfortunately not reported by either nürnberg 1988 or van der molen and boers 1994 and constitutes an uncertainty in kr calculation as kr is inversely proportional to it when a fixed value of bd is assumed in the calculations the resulting set of kr estimates is well fit by a gamma distribution after removal of the lake ijsselmeer z entry from the van der molen and boers article which produces what appears based upon the authors subjective judgement to be an anomalously high kr estimate with a shape parameter of 1 177 alteration of the assumed bd value changes the mean of the distribution and thus its scale parameter but does not change the distribution s shape parameter for example if a value of 0 211 g cm3 is assumed for bd the mean of the resulting distribution is 3 46e 4 d 1 making the admittedly bold assumptions that each of the tabulated observations represents an anoxic release rate that the corresponding rate for the same sediment under oxic conditions would be 1 3 the anoxic rate roughly equal to that in chapra and canale s 1991 lake shagawa calibration and that anoxic conditions prevail during exactly half the year the corresponding annual mean of the kr observations equates to 2 3 of the anoxic rate or 2 31e 4 d 1 which equals the annual mean kr from the chapra canale calibration these values are as mentioned well fit by a gamma distribution fig 5 for convenience and the sake of illustration all simulations illustrated in this report employ this fitted distribution and or the mean of the individual observations to represent kr 5 bayesian parameter estimation bayesian techniques have been employed to calibrate and assess the uncertainty associated with parameters of aquatic ecosystem process models encompassing varying degrees of mechanistic complexity dietzel and reichert 2012 katin et al 2019 kotamäki et al 2015 malve et al 2007 ramin et al 2011 zadeh et al 2019 for our perturbed steady state post step drop implementation of the chapra canale model i e eq 10 we evaluated the ranges of possible values of θ kr kb and φ within the context of the hypothesized steady state relationship between decay coefficient and residence time eq 33 using bayesian representations of eqs 35 and 36 this is analogous to the approach taken by kotamäki et al 2015 in calibrating the settling velocity of a cstr model their eq 1 but in our case involves the simultaneous calibration of three parameters θ kr kb and φ rather than one the bayesian approach employs the implicit assumption that all systems lakes in this case have a common set of parameters cheng et al 2010 the bayesian representation of eq 35 in our implementation can be summarized as 39 ln t p l a k e j n f θ k r k b φ τ j t p i n f l u e n t j ζ 40 f θ k r k b φ τ j t p i n f l u e n t j ln t p i n f l u e n t j ln 1 θ k r k b 1 τ j φ 1 41 θ g a m m a a 1 b 1 42 k r k b g a m m a a 2 b 2 43 φ n o r m a l 0 5 1 44 ζ g a m m a 0 5 0 5 where gamma distributions were used as priors for the presumably positively valued parameters θ and kr kb a1 b1 a2 b2 are parameters of these priors and for the precision inverse variance parameter ζ while a normally distributed prior was employed to represent the potentially negatively or positively valued parameter φ again the shagawa lake calibration chapra and canale 1991 was used as a starting point for specification of the prior distributions which were centered on θ 0 316 φ 0 5 together these give ks 0 021 d 1 for a lake with residence time 228 days i e shagawa lake and kr kb 10 5 equivalent to the shagawa lake calibration s ratio of mean annual kr to kb these prior distributions are shown as dotted lines in fig 6 a c posterior distributions for the same parameters were obtained using the rjags package in r plummer 2019 these were generated by modeling water column tp as a function of influent tp eq 33 in the brett and benjamin 2008 305 lake dataset and are shown as solid lines in fig 6 a c simulations employed three rounds of 5000 mcmc iterations each for a total of 15 000 iterations which were stored in a data frame upon generation mean values from the posterior distributions are indicated in fig 6 with blue dashed vertical lines fig 6 d shows water column tp for the lakes in the brett benjamin dataset as predicted using these mean parameter values θ 0 561 kr kb 8 603 φ 0 504 plotted against the measured tp in the lakes although the mean posterior values of θ and kr kb together correspond with a slightly different value of the multiplier in the power function relationship between keff and τ than the one optimized by brett and benjamin 0 058 vs 0 048 and our mean posterior φ value 0 504 is also slightly different from their optimized exponent 0 47 the quality of the estimated tp vs observed tp fit using our mean values r2 0 8526 is nearly as good as that provided by their hypothesis 4 model r2 0 8528 obtained via summed squared error minimization for each mcmc iteration a value of ks was calculated for τ based on shagawa lake 228 days using eq 35 fig 7 a values of kb were generated at each iteration by dividing kr by kr kb in two ways 1 using mean kr dashed blue line in fig 7 b and 2 using a random kr value black line in fig 7 b selected from the fitted gamma data distribution fig 5 using the rgamma function in r version 4 05 r core team 2020 the resulting kb distributions based on mean and random kr are shown in fig 6 c and d respectively finally for each iteration the lag time to attain target dimensionless 1 β 1 α recoveries ranging from 0 011 to 0 99 in increments of 0 01 were back calculated from eq 25 using the uniroot function in r for kb distributions based on both mean and random kr fig 6 c and d respectively tenth median and 90th percentiles from the 15 000 estimates of each recovery time thus generated at each 1 β 1 α target are shown in fig 6 e and f for mean and random kr respectively analogously the bayesian representation of eq 36 in our implementation can be summarized as 45 ln k e f f j n f θ k r k b φ τ j ζ 46 f θ k r k b φ τ j ln θ φ ln τ j ln k r k b 1 together with eqs 41 44 priors defined the same as previously and based on shagawa lake these distributions are shown as dotted lines in fig 8 a c with posteriors obtained using rjags shown as black lines and whose mean values are indicated with vertical dashed blue lines unlike with the tp based approach of eqs 39 44 the modeling of keff employed the subset of 269 lakes in the steady state lake dataset brett and benjamin 2008 for which ci c1 that is in which tp retention was positive in the sense that influent tp exceeded water column tp this was done because observed keff is negative when retention is negative i e ci c1 and ln keff is therefore undefined when this is the case fig 8 d shows lake tp predicted using these mean parameter values θ 0 836 kr kb 6 121 φ 0 625 plotted against observed i e back calculated from tp input output data keff in the lake dataset distributions of ks and kb were generated from mean and random kr as before fig 8 a d and recovery time curves were generated in the same manner fig 8 e and f the curves in fig 6 e and f and 8 e f may be compared against the curve shown with a solid line in fig 3 a the bayesian model results essentially place uncertainty bounds around the deterministic recovery times shown in the latter despite using data from different though overlapping sets of lakes results obtained using the two estimation approaches eqs 35 and 36 were quite similar to each other for purposes of illustration the results mapping exercises discussed next refer specifically to analyses employing the former estimation approach i e eq 35 6 mapping projected lake recovery lag times example applications of the above techniques were developed in generating maps of deterministic estimated recovery lag times following a hypothetical step drop in influent tp loading concentration for lakes ponds and reservoirs across the united states these examples make use of the medium resolution 1 100 000 scale nhdplusv2 dataset the approach employed which was similar to that of milstead et al 2013 may be described in brief as follows hydrologic data i e nhdsnapshot polygons and nhdplusattributes tables for each of the 20 huc2 regions that comprise the conterminous united states were downloaded from https nhdplus com nhdplus nhdplusv2 data php on 2021 april 03 and unzipped for each huc2 the pluswaterbodylakemorphology tables which include waterbody volume estimates were joined on the comid field to the corresponding waterbody shapefile in arcgis the select by location function was used to select all reaches in the flowlines shapefile that intersect each waterbody next the attributes of these waterbodies were added to those of the intersecting reaches using the arcgis spatial join function and the attribute tables were exported as text csv files scripts written in r were used to join either the output from sparrow spatially referenced regression on watershed attributes tp models of five aggregated regions that comprise the conterminous u s https www usgs gov mission areas water resources science sparrow modeling estimating nutrient sediment and dissolved qt science center objects 0 qt science center objects mapping example 1 or nhd erom ma001 files mapping example 2 to the exported nhd reach attribute tables the sparrow model output which conveniently was calculated for the same nhdplusv2 flowlines includes estimated mean annual flow rate flowcfs and estimated mean annual tp loading rate in kg yr per reach based on the year 2012 the nhd erom ma001 files include independently estimated mean annual flow rates also in ft3 s in the q0001e field after joining the sparrow output or nhd erom information to the exported reach attribute tables the data were filtered to exclude all reaches except those of ftype artificialpath which conceptually represent flow paths through the lentic waterbodies then filtered again to exclude all but the artificial path having the largest flow rate intersecting each waterbody which was presumed to represent that waterbody s effluent stream and therefore its flow rate finally for each waterbody hydraulic residence time was estimated with application of suitable unit conversions from the tabulated lake volumes and artificial path flows from either sparrow or nhd erom based flows and lake water column tp concentrations were similarly calculated from the artificial path flows and sparrow tp loads under the assumption of complete internal mixing i e so that a water body s effluent concentration is the same as its water column concentration to attempt to limit calculations to systems that are genuinely lentic in character waterbodies with estimated residence times less than 7 days were eliminated from further consideration sparrow output based estimates of residence time and water column tp concentration were thereby tabulated for over 70 000 water bodies across the conterminous u s including 1675 in the state of wisconsin nhd based estimates of residence time were tabulated for a similar number of water bodies across the conterminous u s the compiled nhd and sparrow flow based datasets in comma separated value textfiles along with r scripts for interactively calculating and mapping concentration based or dimensionless recovery time estimates based upon both input step drop eqs 10 and 22 respectively and exponential drop eqs 16 and 26 respectively scenarios are provided in the supplementary information for convenience and purposes of illustration kinetic parameter values employed in the lag time estimation examples displayed herein were based on those derived from the original shagawa lake calibration combined with the tp based eqs 38 44 bayesian parameter modeling described above kr was set to 2 31e 4 d 1 the approximate shagawa lake annual mean kb was set to 2 69e 5 d 1 based on posterior mean kr kb 8 603 and ks was lake specific calculated as 0 561τ 0 504 based on mean posterior estimates of parameters θ and φ it must be emphasized that recovery lag time projections such as those shown in the mapped examples are speculative in nature based on both limited data and a limited representation and understanding of physical reality there is as yet no evidence that our approach can be confidently applied to represent recovery trajectories in lakes of all kinds in locations throughout the continental u s or elsewhere 6 1 mapping example 1 the first example is focused on the state of wisconsin and entails calculation of recovery lag times to a specific usually lower target tp concentration compared with current i e 2012 based lake specific sparrow estimated water column concentrations via eq 19 with an assumed α value of 0 2 i e an 80 decrease compared with the current tp influent the target concentration in this example is 40 μg l which corresponds to the state s numeric water quality criterion for tp in unstratified lakes https www epa gov sites production files 2014 12 documents wiwqs nr102 pdf many of the considered lakes particularly in the northern part of the state already meet at least according to the sparrow based estimates this criterion fig 10 c red symbols whereas many others throughout the state are projected to never be able to reach this target fig 10 c black symbols given an influent reduction of only 80 in other words influent reductions must be greater than 80 for lakes to reach 40 μg l if their starting water column tp concentrations are above 200 μg l of the remaining lakes 91 3 have recovery lags of less than a century fig 10 d blueish symbols while a few 8 7 have longer recovery lag times up to a maximum of 271 years fig 3 d reddish symbols 6 2 mapping example 2 the second mapping example includes lakes throughout the entire conterminous u s and entails calculation of recovery lag times to dimensionless target 1 β 1 α values irrespective of current water column tp concentrations via eq 22 because this example includes estimates for over 70 000 lakes which would be impossible to meaningfully display in a static figure at a national scale the results were averaged by huc8 fig 10 a and b show huc averaged times to dimensionless target recoveries of 0 5 and 0 75 respectively when the hypothetical reduction targets were increased from 0 5 to 0 75 the maximum huc averaged time to recovery increased by more than two fold from 65 5 to over 140 years with kr kb and 1 β 1 α fixed t in eq 22 is a deterministic function solely of τ as shown in fig 11 c the spatial patterns in huc8 mean recovery times seen in figs 11 a and 10 b are therefore direct reflections of patterns in estimated lake residence times as seen in fig 11 d lake residence times in turn seem likely to be functions at least in part of precipitation patterns with lakes in the arid west for example having longer residence times for a given water body volume due simply to lower annual precipitation and streamflow 7 discussion the physical conceptualization employed in our chapra and canale 1991 model based approach e g eq 3 is arguably the simplest possible mechanistic representation of dynamic lake water column tp behavior taking into quantitative account sediment phosphorus storage and feedback by expressing the kinetics of the settling recycle and burial processes in volumetric e g d 1 rather than velocity e g m d terms the model is also made generic with respect to water column and benthic compartment depths in the sense that they have no effect on calculations the assumption is that volume based kinetics provide a more general representation of the underlying physics than do velocity based kinetics in support of this idea brett and benjamin 2008 and later khorasani and zhu 2021 both found that the datasets they each looked at do not support the notion of a fixed i e across lakes tp settling velocity although such a conception is as brett and benjamin 2008 note widely accepted in the limnological literature similarly in their analysis of data from 35 european lakes that had undergone reductions in external tp loading jeppesen et al 2005 found no clear effect of lake depth on recovery lag times if a common set of values or distributions of the coefficients θ φ kr kb and kr can be assumed to adequately represent tp behavior across a range of lakes with varying depths and other properties then differences in calculated recovery times become solely a function of differences in hydraulic residence time with the former a monotonically increasing function of the latter fig 11 c despite the simplicity of this representation projected recovery times in the examples explored in this study e g figs 2 3 9 and 10 are in rough general agreement with observed behavior in lakes that have experienced reductions of influent phosphorus loading for which recovery times have been on the order of decades spears et al 2012 estimated a recovery period of at least 20 years for loch leven scotland which experienced 60 decline in external phosphorus loading beginning in the 1970s in their summary of the literature on lake internal phosphorus loading søndergaard et al 2001 noted that recovery following external load reduction may require decades for lakes that have high sediment phosphorus accumulation jeppesen et al 2005 found that most of the 35 lakes they examined had transitioned to lower stable in lake tp concentrations within 10 15 years our own analysis estimates lag times of 13 1 9 6 and 39 0 18 4 years mean s d for in lake tp concentrations to decline by 50 and 75 respectively in 70 858 u s lakes huc8 averaged values are shown in fig 10 a and b respectively the derivations described in this study proceed from an assumption of system homogeneity and stasis with respect to everything except influent concentration for example the water column is treated as a single compartment governed by fixed parameters such that seasonal stratification temperature effects and shifting redox conditions are not directly represented given the nearly infinite variety of potential permutations of such variables not to mention influent time series this is intentional and in keeping with our goal of producing an approach that is generically applicable to lentic waterbodies the fixed parameters ks kr kb τ employed in the recovery curve calculations eqs 10 and 14 etc are best thought of as annual or longer term mean properties of the systems they represent and the water compartment concentration similarly as representing the whole water column s mean concentration even if changing in a long term sense irrespective of thermal stratification present or not and any periodic or seasonal variations the response in a real lake to influent tp reductions of the sort envisioned in this study would be water concentrations that fluctuate on shorter time scales e g daily seasonal as real concentrations tend to rather than smooth monotonic curves such as shown in figs 2 4 however our intention is that these curves represent plausible longer term concentration trends around which individual measurements would fluctuate on daily to seasonal or longer time scales preliminary numerical simulations not shown of multi compartment systems epilimnion hypolimnion and benthic zone with seasonally fluctuating intercompartmental kinetics and driven by daily varying influent concentrations appear to confirm this expectation the linear nature of the governing mass balance equations permits superposition or averaging of outputs given multiple simultaneous inputs so this result is as expected the approach to recovery time estimation described in this study also builds upon the assumption that a lake starts from an extant condition of steady state in which concentrations in the influent and internal compartments water benthic zone are unchanging as is the flow rate and thus residence time this is a useful abstraction to make the derivations tractable but is an idealized representation of conditions that can never be fully met in practice and at best probably only approximates the behavior of some lakes over long term e g multi annual time periods indeed the solutions thus derived posit that new steady state conditions are never fully achieved but only approached asymptotically as time proceeds toward infinity by extension the steady state condition that is assumed to prevail before the influent concentration is perturbed i e before t 0 can only have been reached after infinite pre zero time under fixed influent loading the initial steady state assumption e g eqs 19 and 20 can be shown to be the limiting case for the circumstance where influent concentration ramps from zero toward ci 0 at some fixed linear rate as that rate tends toward zero and the corresponding time required tends toward infinity to the extent that influent tp loadings to real lakes have generally increased over recent decades and years e g due to anthropogenic activities the approach described in this study should tend to overestimate lake recovery times with respect to tp assuming influent tp can be meaningfully decreased in the first place perhaps the most appropriate use of the approach es described in this study would be in providing first approximations of anticipated recovery lag times for specific enriched lakes or groups of lakes in light of ongoing or envisioned reductions in influent tp with the collection of additional site specific data recovery trajectory projections could be refined and more detailed models potentially developed for representing particular lakes perhaps the greatest source of uncertainty in projected lake phosphorus recovery trajectories using the approach described in this study is the magnitude of the tp recycle coefficient kr as illustrated by differences between the widths of the uncertainty windows i e the gap between 10th and 90th percentile dimensionless recovery time curves which are in the range of decades when using a single kr value and centuries when using the full available kr distribution fig 6 e and f and 8 e f this suggests that in order to make the most accurate lake specific recovery projections lake specific measurements of sediment tp release rates should be employed ideally such measurements would include samples collected from a sufficient range of representative sections of lake bottom and measured under a sufficiently representative set of redox conditions to permit calculation of spatially and temporally averaged rates that reflect a lake s mean behavior on an annual or longer time frame ideally as well measurement of sample dry bulk density would be included as part of sediment tp sampling to permit more accurate extrapolation of volumetric kr from measured sediment tp flux rates and concentrations eqs 36 and 37 by employing the distribution of individual sample based calculated kr values fig 5 in the manner presented in our examples we have essentially substituted inter sample variability for inter lake variability although it is far from certain that the former constitutes an appropriate proxy for the latter the assumption that mean kr happens to coincide with a value derived from a prior calibration based on one specific lake i e shagawa is also certainly questionable and likely introduces bias into calculated recovery times the assumption that the values of the dynamic coefficients are fixed over indefinite time periods is another important limitation that seems likely to bias model predictions most importantly improvement in lake trophic status should generally accompany increased sediment redox potential with an associated decrease in sediment phosphorus release rate nürnberg 1984 2009 and thus declining kr as water column tp falls to the extent that this occurs the approach described in this study seems likely to overestimate recovery lag times employing a different though similarly formulated model developed by jensen et al 2006 robertson and diebel 2020 estimated about 20 years for mean summer tp concentrations in lake winnebago wi to decline by 37 5 following an immediate 75 decrease in the lake s external tp loading their scenario j12 they projected lake summer mean tp concentrations to fall by 69 3 i e 1 β 1 α 69 3 75 0 924 125 years following such an influent drop employing a residence time of 187 5 days with θ φ and kr kb at the mean bayesian approach 1 values fig 6 and kr set to the shagawa lake based annual mean our eq 25 gives an estimate of 23 5 years for lake winnebago s tp to decline by half the magnitude of an influent step drop and 119 years for it to decline by 92 4 of one the similarity of these results to robertson and diebel s 2020 suggests that the shagawa lake based kr value 2 31e 4 d 1 may indeed provide a reasonable representation of benthic p release in at least some other lakes by contrast lewis et al 2007 developed a more detailed multilayered mechanistic model of sediment p behavior in lake onondaga ny and in using it predicted lag times of 19 and 26 years for that lake to reach 90 and 95 of steady state i e 1 β 1 α 0 9 and 0 95 respectively following a step drop in influent loading assuming a residence time of 90 days for that lake taner et al 2011 our approach eq 25 estimates recovery times identical to these 19 and 26 years if one employs a value of 1 05e 3 d 1 for kr i e 4 5 fold greater than the 2 31e 4 d 1 shagawa lake based value this suggests that the examples developed in this study based upon the shagawa lake calibration may underestimate tp recycle rates and thus overestimate recovery times for some lakes for the time being the true value of kr on either a lake specific or population mean basis is unknown future investigations and or lake specific model calibrations may help better define appropriate values of this key parameter 8 conclusions internal phosphorus loading from benthic sediments is a phenomenon that has frustrated efforts at restoring enriched lakes by restricting their influent nutrient loads estimating realistic lake recovery lag times in light of this internal loading by a means that requires minimal expense or site specific input data could be useful to resource managers equations 10 16 22 and 26 provide such a means for concentration based as well as dimensionless i e water concentration decrease as a fraction of influent decrease recovery time estimates and for idealized cases in which influent loading decreases in both a step drop and exponential fashion the mapped examples in this study focus on the water column s response to an influent step drop however expressions for the sediment response are also included which may help inform understanding of the role that sediments play in phosphorus storage and feedback similarly expressions representing both water column and sediment compartments responses to more gradual influent loading declines may provide lake resource managers with tools to anticipate more realistic time frames for recovery from eutrophication in response to planned or ongoing watershed management and restoration efforts this study describes an approach for calculating and visualizing scenarios of change in influent and water column tp concentrations that can be used to improve understanding of potential concentration decrease lag times and help inform expectations for lake recovery software and data availability equation implementation parameter estimation and example applications were conducted with the statistical software r r core team 2020 compiled nhd and sparrow based lake property datasets are provided in the supplementary information as are example r scripts that implement concentration based and dimensionless solutions for both step drop and exponential decline in influent i e eqs 10 and 16 with interactive mapping of associated lake recovery lag time estimates declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we are grateful to michael brett for sharing his lake tp data with us to lester yuan and lana kashuba whose feedback on initial drafts made this a better manuscript and to paul hanson for general inspiration the views expressed in this paper are those of the authors alone and do not necessarily represent the views or policies of the u s environmental protection agency any mention of trade names products or services does not imply an endorsement by the u s government or the u s epa appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 multimedia component 4 multimedia component 4 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105642 
25430,internal feedback of nutrients may impede timely improvement in lake water quality we describe a parsimonious mechanistic framework for modeling lag times to recovery of phosphorus enriched lakes given decreases in external loading the approach assumes first order kinetics in a two compartment system taking account of phosphorus storage in and loading from benthic sediments bayesian parameter modeling published sediment phosphorus release rates and a prior dynamic calibration for one lake are used to derive estimates of key parameters applications are developed for an example lake as are maps displaying estimated times to attainment of a phosphorus criterion in lakes across a midwestern state and lag time estimates for fractional water column concentration decrease averaged over huc 8s mean lag times to 50 and 75 declines in water column phosphorus concentration were estimated as 13 1 and 39 0 years respectively across more than 70 000 lentic water bodies in the continental united states keywords lake phosphorus sediment feedback legacy nutrients time lag data availability data will be made available on request 1 introduction eutrophication remains a pervasive cause of impairment for many aquatic ecosystems and negatively impacts the numerous ecosystem services that they provide for the environment and human well being strategies to combat eutrophication have shifted over time as the roles of point source diffuse source and legacy nutrients have come to light as interacting causes of nutrient enrichment underlying eutrophication chen et al 2018 jarvie et al 2013 le moal et al 2019 sharpley et al 2013 over the past four decades multiple approaches have been used to improve nutrient use efficiency associated with agricultural food production and to reduce nutrient loading to waterbodies from both point and nonpoint sources ivahenko 2017 sharpley et al 2019 sabo et al 2021 while there have been positive directional changes in some areas eutrophication of lakes and reservoirs continues to be a widespread and wicked problem for environmental management thornton et al 2013 seemingly less than expected progress is not just a disheartening reality faced by adopters of conservation practices but the basis of increasing cases of litigation and divisiveness among stakeholders sharpley 2018 understanding limitations of nutrient management strategies and setting realistic expectations for lake recovery require a better understanding of the behavior of legacy nutrients in both watersheds and waterbodies kleinman et al 2011 the importance of phosphorus interactions with benthic sediments in mediating eutrophication in lakes has been recognized for decades as reductions in external phosphorus loading have often failed to quickly bring about commensurate reductions in lake water column phosphorus concentrations jeppesen et al 2005 søndergaard et al 1999 sharpley 2018 eutrophication dynamics in lakes are often complex with phytoplankton biomass influenced by factors including zooplankton grazing lake morphometry temperature and food web structure lepori and roberts 2017 despite this complexity phosphorus is usually recognized as an important limiting nutrient for phytoplankton growth in freshwater systems elser et al 2007 and efforts to reverse eutrophication in such systems have generally focused on trying to reduce input loading of phosphorus thornton et al 2013 the failure of some such efforts to promptly achieve desired improvements has led researchers to study the dynamics of phosphorus within lakes including its interactions with various chemical components of benthic sediments and its exchange across the sediment water interface redox sensitive iron complexes particularly have been recognized as playing a role in retaining phosphorus within the sediments gächter and müller 2003 jensen et al 1992 ogdahl et al 2014 søndergaard et al 2001 and release rates of phosphate to the water column have been found to vary seasonally in lakes that experience seasonal hypoxia irrespective of the presence or absence of thermal stratification ripl 1986 welch and cooke 1995 the phenomenon of phosphorus retention in lakes has been studied for decades by researchers around the world who have explored various competing mechanistic and statistical models most often based on a presumption of static conditions using published data from 305 lakes brett and benjamin 2008 evaluated five such models or hypotheses all of the following general form 1 t p l a k e a t p i n 1 k τ equation 1 relates in lake water column total phosphorus concentration tplake to influent phosphorus concentration tpin residence time τ first order removal rate k and a factor a that can empirically account for the loss of a fixed fraction i e 1 a of influent tp note that when a 1 eq 1 is identical to the mass balance based formula for concentration in a continuously stirred tank reactor cstr four of the five hypotheses evaluated by brett and benjamin 2008 concerned solely the nature of k including its possible relationships with other quantities e g residence time water column mean depth while a single hypothesis concerned the possibility of non unity values of parameter a of the five hypotheses the authors found the best performance in terms of matching monitoring data was for a model in which k was a power function of residence time with an exponent close to 0 5 in other words k was found to be nearly proportional to the inverse square root of residence time khorasani and zhu 2021 used an even larger dataset 738 lakes similarly to examine the performance of 39 static lake phosphorus models representations included assumptions of both complete and plug flow mixing first and second order tp loss kinetics and loss rate coefficients that are functions of other measured quantities based on bayesian information criteria bic the authors found their best fit to data with a semi mechanistic model that assumes compete mixing and second order kinetics and that treats k as a complicated power function of mean water column depth residence time and influent tp concentration itself like brett and benjamin 2008 these authors found tp removal to be better represented as a volumetric process i e rate coefficient having units of inverse time than a settling based one i e fixed settling velocity by contrast with static or steady state models efforts to simulate lake phosphorus concentrations dynamically including addressing internal nutrient loading and or times to recovery from eutrophication have often involved the development of mechanistic models focused either on specific lakes edlund et al 2017 lewis et al 2007 wang et al 2019 or on lakes that have specific kinds of biological and or morphological attributes e g cottingham et al 2015 relatively few approaches have incorporated sufficient complexity to allow realistic representation of benthic sediments as dynamic sources and sinks of phosphorus while requiring sufficiently minimal parameterization that rapid application to large numbers of lakes might be feasible the primary objective of this study was to develop a simple generic approach that incorporates the influence of benthic sediments but requires minimal site specific parameterization for predicting plausible trajectories of water quality improvement in lakes following hypothetical sustained decreases in influent tp loading despite the fact that better performance for static models has been found in those that incorporate more predictive variables khorasani and zhu 2021 we chose an approach centered on a simplified version of the two compartment chapra and canale 1991 model because this model is dynamic parsimonious and most importantly incorporates a mass balance based representation of phosphorus in the benthic zone the model thus addresses in a quantitative mechanistic manner the dominant source of internal phosphorus loading in lakes our implementation of the chapra canale model uses idealized representations of influent load change for the sake of general applicability and bayesian estimation to address the impact of uncertainty on parameters and thus on modeled recovery lag times in keeping with the finding that they provide better representations for static lake data brett and benjamin 2008 khorasani and zhu 2021 our approach also employs volume based rather than velocity based rate coefficients in the governing process equations the application of this approach to many lakes allowed for rapid estimation and mapping of predicted recovery times for thousands of lentic water bodies in wisconsin and tens of thousands across the conterminous u s the maps provide visualizations of expected times to reach target lake tp concentrations given specific decreases in tp loading to improve understanding of lag times and to inform expectations for lake recovery 2 methods our framework for modeling lag times to lake recovery centers on a simplified version of a two compartment model developed by chapra and canale 1991 the model included the following mass balance based representation of tp in a lake s water column and active benthic zone 2 v 1 d c 1 d t w q c 1 v s a c 1 v r a c 2 v 2 d c 2 d t v s a c 1 v r a c 2 v b a c 2 where c1 and c2 are water column and benthic zone tp concentrations respectively w is external loading e g in mg day units q is flow e g in m3 d units v1 and v2 are water column and benthic zone volumes e g m3 units respectively a area of benthic water column interface e g in m2 units v s phosphorus settling velocity e g in m d units v r recycle velocity and v b burial velocity by dividing out the compartment volumes this may be expressed in the form 3 d c 1 d t 1 τ c i 1 τ k s c 1 k r d 2 d 1 c 2 d c 2 d t k s d 1 d 2 c 1 k r k b c 2 where τ is hydraulic residence time e g in days ci is influent concentration e g in mg m3 units d1 and d2 are water column and benthic zone depths and ks kr and kb are phosphorus settling recycle and burial rate coefficients e g in day 1 units respectively solutions for the steady state case an idealized construct in which flow influent and internal concentrations are unchanging with time may be derived by setting the derivatives equal to zero and solving for c1 and c2 algebraically yielding 4 c 1 c i 1 k s k b k r k b τ and 5 c 2 c i d 2 d 1 k s k r k b 1 k s τ like eq 1 when a 1 eq 4 is identical to the expression for steady state concentration in a cstr in this case where the effective static first order tp loss or retention coefficient keff is itself a function of the three dynamic process coefficients in eq 3 6 k e f f k s k r k b 1 for dynamically changing influent i e non steady state eq 3 may be solved for c1 and c2 using numerical integration when flow and therefore τ but not influent concentration is constant solutions may also be obtained more directly via convolution carleton 2015 briefly the first step in this approach involves using matrix manipulation to disentangle eq 3 into separate expressions for c1 and c2 with ci set to zero by setting the initial compartment concentrations c1 0 and c2 0 to 1 and 0 respectively the expressions thus obtained are rendered equivalent to impulse response functions irfs for each of the compartments with respect to a constituent in this case tp that enters compartment 1 i e the water column in the influent for example 7 i 1 t 1 τ λ 2 a λ 2 λ 1 e λ 1 t λ 1 a λ 2 λ 1 e λ 2 t represents the water column s response to a unit influent concentration pulse at t 0 where λ1 and λ2 are eigenvalues of the 2 2 coefficient matrix for the system of equations 8 d c 1 d t a c 1 b c 2 d c 2 d t e c 1 f c 2 in this representation a b e and f are shorthand for the corresponding coefficients in eq 3 water column or benthic compartment concentration cx generated by a fluctuating influent concentration time series ci t may then be calculated as a function of time using 9 c x t 0 c i t θ i x θ d θ where ix represents the compartment s irf for an influent concentration time series of arbitrary complexity the corresponding internal compartment time series cx may be obtained via the inverse fast fourier transform fft of the product of the fft s of ci and ix between the extremes of steady and continually fluctuating inputs to a linear system such as this two compartment lake model one can envision cases of intermediate complexity in terms of external forcing wherein a system that is initially at steady state is perturbed by an abrupt change in its influent at some time zero in the post time zero condition which we refer to as perturbed steady state for some idealized forms of influent concentration change exact expressions for resulting compartment concentration cx as a function of time may be derived via forward and inverse laplace transform applied to eq 9 in the simplest example of this idea we consider a two compartment lake that is initially at steady state with fixed influent concentration ci 0 and for which ci undergoes an instantaneous step change to a new fixed lower concentration αci at t 0 the post time zero water column concentration response thus derived is 10 c 1 t m 1 e λ 1 t m 2 e λ 2 t m 3 where the coefficients may be expressed as functions of initial pre time zero i e steady state concentration in either the influent ci 0 or water column c1 0 11 m 1 λ 2 a b e f 1 α λ 2 λ 1 c 1 0 λ 2 a α 1 λ 2 λ 1 c i 0 τ λ 1 12 m 2 a λ 1 b e f 1 α λ 2 λ 1 c 1 0 a λ 1 α 1 λ 2 λ 1 c i 0 τ λ 2 13 m 3 α c 1 0 α c i 0 f τ b e a f the corresponding expression for the benthic compartment is 14 c 2 t λ 1 a b m 1 e λ 1 t λ 2 a b m 2 e λ 2 t e f m 3 3 example concentration decline simulations chapra and canale 1991 calibrated a dynamic implementation of their model to match tp concentration data from the water column of shagawa lake minnesota sampled over the course of the year 1972 their fig 5 their calibrated parameter values were as follows tp settling velocity vs 42 2 m yr burial velocity vb 8 03e 4 m yr summer anaerobic recycle velocity vr 0 0115 m yr and winter aerobic recycle velocity vr 0 00494 m yr the authors listed shagawa lake s mean water column depth as 5 5 m and they assumed a benthic compartment depth of 0 1 m using these depths their calibrated parameters translate for our purposes to ks 0 021 d 1 kb 2 2e 5 d 1 summer anaerobic kr 3 15e 4 d 1 and winter aerobic kr 1 35e 4 d 1 their fig 5 suggests that summer anaerobic conditions prevailed for 195 days and winter aerobic conditions prevailed for 170 days of the year which equates to an annual mean kr of 2 31 e 4 d 1 their paper also lists lake volume and mean flow rate for 1967 1972 that equate to a mean hydraulic residence time of 228 days as well as a mean influent tp concentration of 84 4 μg l during that year fig 1 displays results for a water column eq 10 simulation employing these values with α 0 i e influent concentration at zero for all positive time an equivalent cstr type i e water column only representation is also shown for the purpose of comparison similar to their fig 8 with washout at the usual 1 τ rate plus non washout first order tp loss at the corresponding keff eq 6 rate in the two compartment representation concentration initially declines at a rate similar to that in the cstr model but that rate quickly transitions to much slower decline over subsequent years and decades as the benthic compartment gradually bleeds off its store of tp into the water column two compartment model results eqs 10 and 14 for the same system with for illustrative purposes arbitrary assumed α values of 0 7 i e 30 step drop in influent tp and 0 3 i e 70 drop in influent tp are shown in fig 2 a b and c d respectively in both compartments new non zero steady state concentrations are approached asymptotically with time an instantaneous influent concentration or loading step drop equivalent under the assumed steady flow conditions may be less physically plausible than a more gradual influent decline over some time period e g as watershed management measures gradually decrease nonpoint source loading one simple idealized representation of such behavior is first order decline from ci 0 toward αci 0 where α again is a fraction between 0 and 1 15 c i t 1 α c i 0 e ε t α c i 0 in which case the following expressions analogous to eqs 10 and 14 respectively may be derived for the water column and benthic concentration responses 16 c 1 t n 1 e ε t n 2 e λ 1 t n 3 e λ 2 t m 3 17 c 2 t λ 1 a b λ 2 a f ε n 1 e ε t λ 1 a b n 2 e λ 1 t λ 2 a b n 3 e λ 2 t e f m 3 where 18 n 1 f ε λ 2 λ 1 λ 1 λ 1 ε λ 2 ε λ 2 a m 1 19 n 2 ε λ 1 ε m 1 20 n 3 ε λ 2 ε m 2 as one might suspect from visual inspection of fig 2 a vs c and 2 b vs d for a given set of parameters the decline curves within the range defined by α are simply rescaled versions of the same shape with the scaling factor defined by the magnitude of α in other words the lag time to attain a given percent decline in water column concentration is a function of the percent decline in the influent concentration and not of the initial and target concentrations per se thus for example the time required to reach 90 of an asymptotic 50 decline in water column concentration following a 50 drop in influent concentration is the same as that required to reach 90 of an asymptotic 10 decline in water column concentration following a 10 drop in influent concentration eqs 10 and 14 may be converted to dimensionless form i e water column concentration decrease as a fraction of influent concentration decrease by normalizing over both initial concentration and the quantity 1 α i e the maximum potential decline for the water column we first normalize concentration by its value at time zero 21 β 1 t c 1 t c 1 0 further normalizing by 1 α eq 10 becomes 22 1 β 1 t 1 α 1 m 1 e λ 1 t m 2 e λ 2 t where 23 m 1 λ 2 a b e f λ 2 λ 1 m 1 1 α c 1 0 and 24 m 2 a λ 1 b e f λ 2 λ 1 m 2 1 α c 1 0 to reiterate in eq 22 1 β1 t is the time course of fractional decrease in water column concentration that occurs in response to a 1 α fractional decrease in the influent concentration starting at t 0 the form of eq 22 reflects the fact that the water column concentration decline trajectory scales with the magnitude of the influent drop an expression for the benthic concentration derived analogously is 25 1 β 2 t 1 α 1 f b e a λ 1 m 1 e λ 1 t a λ 2 m 2 e λ 2 t dimensionless versions of the expressions for exponential influent decline eqs 14 and 15 may be obtained in the same manner eqs 26 and 27 26 1 β 1 t 1 α 1 n 1 e ε t n 2 e λ 1 t n 3 e λ 2 t 27 1 β 2 t 1 α 1 f b e a λ 1 λ 2 a f ε n 1 e ε t a λ 1 n 2 e λ 1 t a λ 2 n 3 e λ 2 t where 28 n 1 n 1 1 α c 1 0 29 n 2 n 2 1 α c 1 0 30 n 3 n 3 1 α c 1 0 fig 3 shows example dimensionless water column and benthic compartment concentration decline curves following both a step drop and an exponential drop eq 15 in influent tp the exponential influent tp decline in this example corresponds for illustrative purposes with a rate of 10 year ε 2 738e 4 d 1 as expected for both the water column and benthic zone the impact of an exponential influent decline as compared with an instantaneous drop is a rightward shift in the curve to longer recovery times fig 3 4 determining representative kinetic parameter values because every term that includes b or e eqs 11 13 25 27 involves their product the compartment depth ratios eq 3 cancel and every term in eq 10 and every term but the influent concentration decline rate ε in eq 16 is actually a function of just five variables c1 0 extant mean water column tp concentration τ hydraulic residence time and the three dynamic rate coefficients ks kr and kb values of extant water column tp and residence time may be either available from measured data or estimatable based on other available information for many lakes throughout the u s and elsewhere but values for ks kr and kb are not thus the major challenge in applying eq 10 or 16 is obtaining defensible estimates of these three rate coefficients preferably along with some measure of their uncertainty to derive credible estimates of ks kr and kb we started with brett and benjamin s 2008 tp data from lakes presumed to be in steady state we used these data to extract information on the dynamic coefficients relative magnitudes as the three coefficients are found in the form of ratios that comprise the expression for the static condition tp loss coefficient eq 6 as noted eq 1 with α 1 is identical to the steady state formula for concentration in a and brett and benjamin 2008 obtained their best fit to these data with a model eq 31 with τ in units of days of this form wherein effective k was approximately proportional to the inverse square root of τ their hypothesis 4 31 k e f f 0 048 τ 0 47 this essentially reprises the findings of vollenweider 1976 and larsen and mercier 1976 based on other data making use of this general result without positing any particular mechanistic rationale to explain it we assume that the power function form manifests specifically from a relationship between ks eq 6 and τ 32 k s θ τ φ thus we assume that the effective first order water column tp loss coefficient is of the form 33 k e f f θ k r k b 1 τ φ which means water column tp concentrations for lakes in steady state are estimated as 34 c 1 c i 1 θ k r k b 1 τ φ 1 given paired measurements of ci and c1 from a number of steady state lakes one may obtain estimates of θ kr kb and φ by optimizing predicted c1 against observed c1 as a function of observed ci and τ this may be facilitated by using a log transformed version of eq 34 35 ln c 1 ln c i ln 1 θ k r k b 1 τ φ 1 alternatively estimates of θ kr kb and φ may be obtained by optimizing estimated keff against observed i e back calculated from data keff as a function of τ e g by using a log transformed version of eq 33 36 ln k e f f ln θ φ ln τ ln k r k b 1 in this conceptualization for any given lake the settling coefficient is a function solely of residence time with τ either known or estimated it is possible to estimate ks and the ratio kr kb by optimizing either predicted concentration or effective k i e eq 35 or 36 against steady state data if an independent estimate of either kr or kb is also available then we effectively have estimates of all three coefficients and thus everything needed for calculating recovery times using for example eq 10 if c1 0 is also known or eq 22 for specific representative estimates of ks kr and kb we started with ks and kr kb values based on the dynamic lake shagawa calibration as reported by chapra and canale 1991 we used these values assuming φ 0 5 to specify the means of prior distributions of θ and kr kb in bayesian parameter modeling conducted as described above using steady state brett and benjamin 2008 lake tp data we chose fairly tight low variance priors rather than uninformative ones to ensure that the posterior distributions were centered relatively close to the chapra and canale 1991 based ks and kr kb estimates this process generated posteriors for θ φ and kr kb that are informed by the chapra and canale calibration and that are also intended to embody the variance present in static input output lake tp monitoring data results were captured in mcmc output which was stored upon generation and subsequently made use of together with literature based estimates of kr in producing essentially distributions of recovery times that reflect the impact of parameter uncertainty we note that infinitely many combinations of hypothetical values of dynamic parameters ks and therefore θ and φ kr and kb exist that correspond with a given value of the static parameter keff and each such combination will result in a different lake recovery trajectory per eq 10 etc best fit based parameter ratios derived from static datasets thus provide information that only partially constrains potential magnitudes of the dynamic process parameters i e ks kr and kb fig 4 illustrates this concept showing as solid black lines ranges of combinations of θ and kr kb consistent with the spreadsheet optimized in a squared error minimization sense values of the ratio θ kr kb 1 0 0701 and 0 1388 for subfigures a and b respectively parameter values derived from and consistent with the chapra and canale 1991 shagawa lake calibration are indicated by green diamonds with θ calculated based on the assumptions ks 0 021 d 1 τ 228 days and φ 0 53 and 0 65 exponent estimates also obtained via squared error minimization in subplots a and b respectively post burn in markov chain monte carlo mcmc samples generated as described in section 5 are shown as gray dots and their mean values are shown as red triangles fig 4 a shows parameter estimates and mcmc results from the first bayesian model eq 35 while fig 4 b shows the same from the second eq 36 the figure indicates how in both cases we use parameter values believed to adequately represent dynamic tp behavior in one lake the green diamonds to help define plausible values of parameters whose values we can infer only in the form of ratios see eq 6 based upon steady state data from hundreds of other lakes the variability of observed tp data in this lake dataset helps inform the uncertainty associated with dynamic model parameters θ and kr kb and with associated recovery lag times measurements of phosphorus burial and water column settling rates independent of sediment recycling are not generally available and would be challenging to directly measure however laboratory procedures for straightforwardly measuring sediment tp release rates do exist and indeed such measurements from the sediments of many lakes have been tabulated in the literature nürnberg 1988 measured p release rates under anoxic conditions from sediments of seven north american lakes and showed that release rates mg m2 d were strongly correlated with total sediment phosphorus concentrations mg g dry weight tabulated p release rates paired with sediment tp concentrations from 63 lakes in the literature were also provided table 6 in nürnberg 1988 van der molen and boers 1994 similarly reported internal phosphorus loading rates g m2 yr from 49 shallow mean depth 6 m lakes as well as sediment p concentrations g kg dry weight for 45 of the lakes together these datasets comprise a total of 115 paired lake sediment p concentration and release measurements from each of which a kr value d 1 can be calculated using the formula 37 k r r r c a where rr is the sediment p release rate mg m2 d and ca is areal sediment p concentration mg m2 ca is calculated from sediment p concentration via 38 c a t p s b d d 2 10 6 where tps is total sediment phosphorus concentration mg g dry weight bd is sediment dry bulk density g cm3 d2 is active sediment layer depth assumed here to be 0 1 m and 106 is a unit conversion factor sediment dry bulk density is unfortunately not reported by either nürnberg 1988 or van der molen and boers 1994 and constitutes an uncertainty in kr calculation as kr is inversely proportional to it when a fixed value of bd is assumed in the calculations the resulting set of kr estimates is well fit by a gamma distribution after removal of the lake ijsselmeer z entry from the van der molen and boers article which produces what appears based upon the authors subjective judgement to be an anomalously high kr estimate with a shape parameter of 1 177 alteration of the assumed bd value changes the mean of the distribution and thus its scale parameter but does not change the distribution s shape parameter for example if a value of 0 211 g cm3 is assumed for bd the mean of the resulting distribution is 3 46e 4 d 1 making the admittedly bold assumptions that each of the tabulated observations represents an anoxic release rate that the corresponding rate for the same sediment under oxic conditions would be 1 3 the anoxic rate roughly equal to that in chapra and canale s 1991 lake shagawa calibration and that anoxic conditions prevail during exactly half the year the corresponding annual mean of the kr observations equates to 2 3 of the anoxic rate or 2 31e 4 d 1 which equals the annual mean kr from the chapra canale calibration these values are as mentioned well fit by a gamma distribution fig 5 for convenience and the sake of illustration all simulations illustrated in this report employ this fitted distribution and or the mean of the individual observations to represent kr 5 bayesian parameter estimation bayesian techniques have been employed to calibrate and assess the uncertainty associated with parameters of aquatic ecosystem process models encompassing varying degrees of mechanistic complexity dietzel and reichert 2012 katin et al 2019 kotamäki et al 2015 malve et al 2007 ramin et al 2011 zadeh et al 2019 for our perturbed steady state post step drop implementation of the chapra canale model i e eq 10 we evaluated the ranges of possible values of θ kr kb and φ within the context of the hypothesized steady state relationship between decay coefficient and residence time eq 33 using bayesian representations of eqs 35 and 36 this is analogous to the approach taken by kotamäki et al 2015 in calibrating the settling velocity of a cstr model their eq 1 but in our case involves the simultaneous calibration of three parameters θ kr kb and φ rather than one the bayesian approach employs the implicit assumption that all systems lakes in this case have a common set of parameters cheng et al 2010 the bayesian representation of eq 35 in our implementation can be summarized as 39 ln t p l a k e j n f θ k r k b φ τ j t p i n f l u e n t j ζ 40 f θ k r k b φ τ j t p i n f l u e n t j ln t p i n f l u e n t j ln 1 θ k r k b 1 τ j φ 1 41 θ g a m m a a 1 b 1 42 k r k b g a m m a a 2 b 2 43 φ n o r m a l 0 5 1 44 ζ g a m m a 0 5 0 5 where gamma distributions were used as priors for the presumably positively valued parameters θ and kr kb a1 b1 a2 b2 are parameters of these priors and for the precision inverse variance parameter ζ while a normally distributed prior was employed to represent the potentially negatively or positively valued parameter φ again the shagawa lake calibration chapra and canale 1991 was used as a starting point for specification of the prior distributions which were centered on θ 0 316 φ 0 5 together these give ks 0 021 d 1 for a lake with residence time 228 days i e shagawa lake and kr kb 10 5 equivalent to the shagawa lake calibration s ratio of mean annual kr to kb these prior distributions are shown as dotted lines in fig 6 a c posterior distributions for the same parameters were obtained using the rjags package in r plummer 2019 these were generated by modeling water column tp as a function of influent tp eq 33 in the brett and benjamin 2008 305 lake dataset and are shown as solid lines in fig 6 a c simulations employed three rounds of 5000 mcmc iterations each for a total of 15 000 iterations which were stored in a data frame upon generation mean values from the posterior distributions are indicated in fig 6 with blue dashed vertical lines fig 6 d shows water column tp for the lakes in the brett benjamin dataset as predicted using these mean parameter values θ 0 561 kr kb 8 603 φ 0 504 plotted against the measured tp in the lakes although the mean posterior values of θ and kr kb together correspond with a slightly different value of the multiplier in the power function relationship between keff and τ than the one optimized by brett and benjamin 0 058 vs 0 048 and our mean posterior φ value 0 504 is also slightly different from their optimized exponent 0 47 the quality of the estimated tp vs observed tp fit using our mean values r2 0 8526 is nearly as good as that provided by their hypothesis 4 model r2 0 8528 obtained via summed squared error minimization for each mcmc iteration a value of ks was calculated for τ based on shagawa lake 228 days using eq 35 fig 7 a values of kb were generated at each iteration by dividing kr by kr kb in two ways 1 using mean kr dashed blue line in fig 7 b and 2 using a random kr value black line in fig 7 b selected from the fitted gamma data distribution fig 5 using the rgamma function in r version 4 05 r core team 2020 the resulting kb distributions based on mean and random kr are shown in fig 6 c and d respectively finally for each iteration the lag time to attain target dimensionless 1 β 1 α recoveries ranging from 0 011 to 0 99 in increments of 0 01 were back calculated from eq 25 using the uniroot function in r for kb distributions based on both mean and random kr fig 6 c and d respectively tenth median and 90th percentiles from the 15 000 estimates of each recovery time thus generated at each 1 β 1 α target are shown in fig 6 e and f for mean and random kr respectively analogously the bayesian representation of eq 36 in our implementation can be summarized as 45 ln k e f f j n f θ k r k b φ τ j ζ 46 f θ k r k b φ τ j ln θ φ ln τ j ln k r k b 1 together with eqs 41 44 priors defined the same as previously and based on shagawa lake these distributions are shown as dotted lines in fig 8 a c with posteriors obtained using rjags shown as black lines and whose mean values are indicated with vertical dashed blue lines unlike with the tp based approach of eqs 39 44 the modeling of keff employed the subset of 269 lakes in the steady state lake dataset brett and benjamin 2008 for which ci c1 that is in which tp retention was positive in the sense that influent tp exceeded water column tp this was done because observed keff is negative when retention is negative i e ci c1 and ln keff is therefore undefined when this is the case fig 8 d shows lake tp predicted using these mean parameter values θ 0 836 kr kb 6 121 φ 0 625 plotted against observed i e back calculated from tp input output data keff in the lake dataset distributions of ks and kb were generated from mean and random kr as before fig 8 a d and recovery time curves were generated in the same manner fig 8 e and f the curves in fig 6 e and f and 8 e f may be compared against the curve shown with a solid line in fig 3 a the bayesian model results essentially place uncertainty bounds around the deterministic recovery times shown in the latter despite using data from different though overlapping sets of lakes results obtained using the two estimation approaches eqs 35 and 36 were quite similar to each other for purposes of illustration the results mapping exercises discussed next refer specifically to analyses employing the former estimation approach i e eq 35 6 mapping projected lake recovery lag times example applications of the above techniques were developed in generating maps of deterministic estimated recovery lag times following a hypothetical step drop in influent tp loading concentration for lakes ponds and reservoirs across the united states these examples make use of the medium resolution 1 100 000 scale nhdplusv2 dataset the approach employed which was similar to that of milstead et al 2013 may be described in brief as follows hydrologic data i e nhdsnapshot polygons and nhdplusattributes tables for each of the 20 huc2 regions that comprise the conterminous united states were downloaded from https nhdplus com nhdplus nhdplusv2 data php on 2021 april 03 and unzipped for each huc2 the pluswaterbodylakemorphology tables which include waterbody volume estimates were joined on the comid field to the corresponding waterbody shapefile in arcgis the select by location function was used to select all reaches in the flowlines shapefile that intersect each waterbody next the attributes of these waterbodies were added to those of the intersecting reaches using the arcgis spatial join function and the attribute tables were exported as text csv files scripts written in r were used to join either the output from sparrow spatially referenced regression on watershed attributes tp models of five aggregated regions that comprise the conterminous u s https www usgs gov mission areas water resources science sparrow modeling estimating nutrient sediment and dissolved qt science center objects 0 qt science center objects mapping example 1 or nhd erom ma001 files mapping example 2 to the exported nhd reach attribute tables the sparrow model output which conveniently was calculated for the same nhdplusv2 flowlines includes estimated mean annual flow rate flowcfs and estimated mean annual tp loading rate in kg yr per reach based on the year 2012 the nhd erom ma001 files include independently estimated mean annual flow rates also in ft3 s in the q0001e field after joining the sparrow output or nhd erom information to the exported reach attribute tables the data were filtered to exclude all reaches except those of ftype artificialpath which conceptually represent flow paths through the lentic waterbodies then filtered again to exclude all but the artificial path having the largest flow rate intersecting each waterbody which was presumed to represent that waterbody s effluent stream and therefore its flow rate finally for each waterbody hydraulic residence time was estimated with application of suitable unit conversions from the tabulated lake volumes and artificial path flows from either sparrow or nhd erom based flows and lake water column tp concentrations were similarly calculated from the artificial path flows and sparrow tp loads under the assumption of complete internal mixing i e so that a water body s effluent concentration is the same as its water column concentration to attempt to limit calculations to systems that are genuinely lentic in character waterbodies with estimated residence times less than 7 days were eliminated from further consideration sparrow output based estimates of residence time and water column tp concentration were thereby tabulated for over 70 000 water bodies across the conterminous u s including 1675 in the state of wisconsin nhd based estimates of residence time were tabulated for a similar number of water bodies across the conterminous u s the compiled nhd and sparrow flow based datasets in comma separated value textfiles along with r scripts for interactively calculating and mapping concentration based or dimensionless recovery time estimates based upon both input step drop eqs 10 and 22 respectively and exponential drop eqs 16 and 26 respectively scenarios are provided in the supplementary information for convenience and purposes of illustration kinetic parameter values employed in the lag time estimation examples displayed herein were based on those derived from the original shagawa lake calibration combined with the tp based eqs 38 44 bayesian parameter modeling described above kr was set to 2 31e 4 d 1 the approximate shagawa lake annual mean kb was set to 2 69e 5 d 1 based on posterior mean kr kb 8 603 and ks was lake specific calculated as 0 561τ 0 504 based on mean posterior estimates of parameters θ and φ it must be emphasized that recovery lag time projections such as those shown in the mapped examples are speculative in nature based on both limited data and a limited representation and understanding of physical reality there is as yet no evidence that our approach can be confidently applied to represent recovery trajectories in lakes of all kinds in locations throughout the continental u s or elsewhere 6 1 mapping example 1 the first example is focused on the state of wisconsin and entails calculation of recovery lag times to a specific usually lower target tp concentration compared with current i e 2012 based lake specific sparrow estimated water column concentrations via eq 19 with an assumed α value of 0 2 i e an 80 decrease compared with the current tp influent the target concentration in this example is 40 μg l which corresponds to the state s numeric water quality criterion for tp in unstratified lakes https www epa gov sites production files 2014 12 documents wiwqs nr102 pdf many of the considered lakes particularly in the northern part of the state already meet at least according to the sparrow based estimates this criterion fig 10 c red symbols whereas many others throughout the state are projected to never be able to reach this target fig 10 c black symbols given an influent reduction of only 80 in other words influent reductions must be greater than 80 for lakes to reach 40 μg l if their starting water column tp concentrations are above 200 μg l of the remaining lakes 91 3 have recovery lags of less than a century fig 10 d blueish symbols while a few 8 7 have longer recovery lag times up to a maximum of 271 years fig 3 d reddish symbols 6 2 mapping example 2 the second mapping example includes lakes throughout the entire conterminous u s and entails calculation of recovery lag times to dimensionless target 1 β 1 α values irrespective of current water column tp concentrations via eq 22 because this example includes estimates for over 70 000 lakes which would be impossible to meaningfully display in a static figure at a national scale the results were averaged by huc8 fig 10 a and b show huc averaged times to dimensionless target recoveries of 0 5 and 0 75 respectively when the hypothetical reduction targets were increased from 0 5 to 0 75 the maximum huc averaged time to recovery increased by more than two fold from 65 5 to over 140 years with kr kb and 1 β 1 α fixed t in eq 22 is a deterministic function solely of τ as shown in fig 11 c the spatial patterns in huc8 mean recovery times seen in figs 11 a and 10 b are therefore direct reflections of patterns in estimated lake residence times as seen in fig 11 d lake residence times in turn seem likely to be functions at least in part of precipitation patterns with lakes in the arid west for example having longer residence times for a given water body volume due simply to lower annual precipitation and streamflow 7 discussion the physical conceptualization employed in our chapra and canale 1991 model based approach e g eq 3 is arguably the simplest possible mechanistic representation of dynamic lake water column tp behavior taking into quantitative account sediment phosphorus storage and feedback by expressing the kinetics of the settling recycle and burial processes in volumetric e g d 1 rather than velocity e g m d terms the model is also made generic with respect to water column and benthic compartment depths in the sense that they have no effect on calculations the assumption is that volume based kinetics provide a more general representation of the underlying physics than do velocity based kinetics in support of this idea brett and benjamin 2008 and later khorasani and zhu 2021 both found that the datasets they each looked at do not support the notion of a fixed i e across lakes tp settling velocity although such a conception is as brett and benjamin 2008 note widely accepted in the limnological literature similarly in their analysis of data from 35 european lakes that had undergone reductions in external tp loading jeppesen et al 2005 found no clear effect of lake depth on recovery lag times if a common set of values or distributions of the coefficients θ φ kr kb and kr can be assumed to adequately represent tp behavior across a range of lakes with varying depths and other properties then differences in calculated recovery times become solely a function of differences in hydraulic residence time with the former a monotonically increasing function of the latter fig 11 c despite the simplicity of this representation projected recovery times in the examples explored in this study e g figs 2 3 9 and 10 are in rough general agreement with observed behavior in lakes that have experienced reductions of influent phosphorus loading for which recovery times have been on the order of decades spears et al 2012 estimated a recovery period of at least 20 years for loch leven scotland which experienced 60 decline in external phosphorus loading beginning in the 1970s in their summary of the literature on lake internal phosphorus loading søndergaard et al 2001 noted that recovery following external load reduction may require decades for lakes that have high sediment phosphorus accumulation jeppesen et al 2005 found that most of the 35 lakes they examined had transitioned to lower stable in lake tp concentrations within 10 15 years our own analysis estimates lag times of 13 1 9 6 and 39 0 18 4 years mean s d for in lake tp concentrations to decline by 50 and 75 respectively in 70 858 u s lakes huc8 averaged values are shown in fig 10 a and b respectively the derivations described in this study proceed from an assumption of system homogeneity and stasis with respect to everything except influent concentration for example the water column is treated as a single compartment governed by fixed parameters such that seasonal stratification temperature effects and shifting redox conditions are not directly represented given the nearly infinite variety of potential permutations of such variables not to mention influent time series this is intentional and in keeping with our goal of producing an approach that is generically applicable to lentic waterbodies the fixed parameters ks kr kb τ employed in the recovery curve calculations eqs 10 and 14 etc are best thought of as annual or longer term mean properties of the systems they represent and the water compartment concentration similarly as representing the whole water column s mean concentration even if changing in a long term sense irrespective of thermal stratification present or not and any periodic or seasonal variations the response in a real lake to influent tp reductions of the sort envisioned in this study would be water concentrations that fluctuate on shorter time scales e g daily seasonal as real concentrations tend to rather than smooth monotonic curves such as shown in figs 2 4 however our intention is that these curves represent plausible longer term concentration trends around which individual measurements would fluctuate on daily to seasonal or longer time scales preliminary numerical simulations not shown of multi compartment systems epilimnion hypolimnion and benthic zone with seasonally fluctuating intercompartmental kinetics and driven by daily varying influent concentrations appear to confirm this expectation the linear nature of the governing mass balance equations permits superposition or averaging of outputs given multiple simultaneous inputs so this result is as expected the approach to recovery time estimation described in this study also builds upon the assumption that a lake starts from an extant condition of steady state in which concentrations in the influent and internal compartments water benthic zone are unchanging as is the flow rate and thus residence time this is a useful abstraction to make the derivations tractable but is an idealized representation of conditions that can never be fully met in practice and at best probably only approximates the behavior of some lakes over long term e g multi annual time periods indeed the solutions thus derived posit that new steady state conditions are never fully achieved but only approached asymptotically as time proceeds toward infinity by extension the steady state condition that is assumed to prevail before the influent concentration is perturbed i e before t 0 can only have been reached after infinite pre zero time under fixed influent loading the initial steady state assumption e g eqs 19 and 20 can be shown to be the limiting case for the circumstance where influent concentration ramps from zero toward ci 0 at some fixed linear rate as that rate tends toward zero and the corresponding time required tends toward infinity to the extent that influent tp loadings to real lakes have generally increased over recent decades and years e g due to anthropogenic activities the approach described in this study should tend to overestimate lake recovery times with respect to tp assuming influent tp can be meaningfully decreased in the first place perhaps the most appropriate use of the approach es described in this study would be in providing first approximations of anticipated recovery lag times for specific enriched lakes or groups of lakes in light of ongoing or envisioned reductions in influent tp with the collection of additional site specific data recovery trajectory projections could be refined and more detailed models potentially developed for representing particular lakes perhaps the greatest source of uncertainty in projected lake phosphorus recovery trajectories using the approach described in this study is the magnitude of the tp recycle coefficient kr as illustrated by differences between the widths of the uncertainty windows i e the gap between 10th and 90th percentile dimensionless recovery time curves which are in the range of decades when using a single kr value and centuries when using the full available kr distribution fig 6 e and f and 8 e f this suggests that in order to make the most accurate lake specific recovery projections lake specific measurements of sediment tp release rates should be employed ideally such measurements would include samples collected from a sufficient range of representative sections of lake bottom and measured under a sufficiently representative set of redox conditions to permit calculation of spatially and temporally averaged rates that reflect a lake s mean behavior on an annual or longer time frame ideally as well measurement of sample dry bulk density would be included as part of sediment tp sampling to permit more accurate extrapolation of volumetric kr from measured sediment tp flux rates and concentrations eqs 36 and 37 by employing the distribution of individual sample based calculated kr values fig 5 in the manner presented in our examples we have essentially substituted inter sample variability for inter lake variability although it is far from certain that the former constitutes an appropriate proxy for the latter the assumption that mean kr happens to coincide with a value derived from a prior calibration based on one specific lake i e shagawa is also certainly questionable and likely introduces bias into calculated recovery times the assumption that the values of the dynamic coefficients are fixed over indefinite time periods is another important limitation that seems likely to bias model predictions most importantly improvement in lake trophic status should generally accompany increased sediment redox potential with an associated decrease in sediment phosphorus release rate nürnberg 1984 2009 and thus declining kr as water column tp falls to the extent that this occurs the approach described in this study seems likely to overestimate recovery lag times employing a different though similarly formulated model developed by jensen et al 2006 robertson and diebel 2020 estimated about 20 years for mean summer tp concentrations in lake winnebago wi to decline by 37 5 following an immediate 75 decrease in the lake s external tp loading their scenario j12 they projected lake summer mean tp concentrations to fall by 69 3 i e 1 β 1 α 69 3 75 0 924 125 years following such an influent drop employing a residence time of 187 5 days with θ φ and kr kb at the mean bayesian approach 1 values fig 6 and kr set to the shagawa lake based annual mean our eq 25 gives an estimate of 23 5 years for lake winnebago s tp to decline by half the magnitude of an influent step drop and 119 years for it to decline by 92 4 of one the similarity of these results to robertson and diebel s 2020 suggests that the shagawa lake based kr value 2 31e 4 d 1 may indeed provide a reasonable representation of benthic p release in at least some other lakes by contrast lewis et al 2007 developed a more detailed multilayered mechanistic model of sediment p behavior in lake onondaga ny and in using it predicted lag times of 19 and 26 years for that lake to reach 90 and 95 of steady state i e 1 β 1 α 0 9 and 0 95 respectively following a step drop in influent loading assuming a residence time of 90 days for that lake taner et al 2011 our approach eq 25 estimates recovery times identical to these 19 and 26 years if one employs a value of 1 05e 3 d 1 for kr i e 4 5 fold greater than the 2 31e 4 d 1 shagawa lake based value this suggests that the examples developed in this study based upon the shagawa lake calibration may underestimate tp recycle rates and thus overestimate recovery times for some lakes for the time being the true value of kr on either a lake specific or population mean basis is unknown future investigations and or lake specific model calibrations may help better define appropriate values of this key parameter 8 conclusions internal phosphorus loading from benthic sediments is a phenomenon that has frustrated efforts at restoring enriched lakes by restricting their influent nutrient loads estimating realistic lake recovery lag times in light of this internal loading by a means that requires minimal expense or site specific input data could be useful to resource managers equations 10 16 22 and 26 provide such a means for concentration based as well as dimensionless i e water concentration decrease as a fraction of influent decrease recovery time estimates and for idealized cases in which influent loading decreases in both a step drop and exponential fashion the mapped examples in this study focus on the water column s response to an influent step drop however expressions for the sediment response are also included which may help inform understanding of the role that sediments play in phosphorus storage and feedback similarly expressions representing both water column and sediment compartments responses to more gradual influent loading declines may provide lake resource managers with tools to anticipate more realistic time frames for recovery from eutrophication in response to planned or ongoing watershed management and restoration efforts this study describes an approach for calculating and visualizing scenarios of change in influent and water column tp concentrations that can be used to improve understanding of potential concentration decrease lag times and help inform expectations for lake recovery software and data availability equation implementation parameter estimation and example applications were conducted with the statistical software r r core team 2020 compiled nhd and sparrow based lake property datasets are provided in the supplementary information as are example r scripts that implement concentration based and dimensionless solutions for both step drop and exponential decline in influent i e eqs 10 and 16 with interactive mapping of associated lake recovery lag time estimates declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements we are grateful to michael brett for sharing his lake tp data with us to lester yuan and lana kashuba whose feedback on initial drafts made this a better manuscript and to paul hanson for general inspiration the views expressed in this paper are those of the authors alone and do not necessarily represent the views or policies of the u s environmental protection agency any mention of trade names products or services does not imply an endorsement by the u s government or the u s epa appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 multimedia component 3 multimedia component 3 multimedia component 4 multimedia component 4 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105642 
25431,cyclone track forecasting is a critical climate science problem involving time series prediction of cyclone location and intensity machine learning methods have shown much promise in this domain especially deep learning methods such as recurrent neural networks rnns however these methods generally make single point predictions with little focus on uncertainty quantification although markov chain monte carlo mcmc methods have often been used for quantifying uncertainty in neural network predictions these methods are computationally expensive variational inference vi is an alternative to mcmc sampling that approximates the posterior distribution of parameters by minimizing a kl divergence loss between the estimate and the true posterior in this paper we present variational rnns for cyclone track and intensity prediction in four different regions across the globe we utilise simple rnns and long short term memory lstm rnns and use the energy score es to evaluate multivariate probabilistic predictions the results show that variational rnns provide a good approximation with uncertainty quantification when compared to conventional rnns while maintaining prediction accuracy keywords cyclone track prediction recurrent neural networks bayesian neural networks long short term memory variational inference data availability the link to data and code has been shared as part of the submission 1 introduction in the last few decades the devastating impact of climate change became more obvious with rise in adverse meteorological events such as droughts heat waves hurricanes and tropical cyclones mendelsohn et al 2012 and bushfires the devastating effects of tropical cyclones and hurricanes include flooding landslides detrimental winds and storm surges which makes them a dominant meteorological hazard needham et al 2015 schrum et al 2020 apart from these tropical cyclones cause a significant amount of economic and environmental damage pielke 2007 fengjin and ziniu 2010 lionello et al 2006 harmelin vivien 1994 forecasting cyclone tracks trajectory and intensities are critical for lowering injuries and saving lives apart from prevention of damage to infrastructure and mitigation of economic losses mcbride and holland 1987 cyclones typhoons and hurricanes are all tropical storms with the distinguishing characteristic being the geographical location where they form tropical storms that form over north atlantic ocean and north east pacific are known as hurricanes and those developing in the north west pacific are known as typhoons while cyclones are the storms that form over the south pacific and the indian ocean emanuel 2003 cyclone track forecasting is a multi dimensional time series prediction problem with cyclone location represented by latitude and longitude and its wind intensity recorded at each time step roy et al presented a comprehensive review of tropical cyclone track forecasting methods roy and kovordányi 2012a more recently chen et al presented a machine learning focused review of tropical cyclone forecast modelling chen et al 2020 traditionally numerical yablonsky et al 2015 and statistical methods techniques mcadie and lawrence 2000 roy and kovordányi 2012b have been utilized for forecasting cyclone trajectories regression models have commonly been used for cyclone trajectory neumann 1972 as well as intensity prediction atkinson and holliday 1977 atkinson and holliday 1977 derive a high degree polynomial equation to model the wind speed pressure relationship for tropical cyclones in the western north pacific ocean due to the chaotic nature of cyclones estimating the risk of cyclones reaching landfall along with their wind intensities has been challenging in previous studies hall and jewson 2007 used a non parametric model and utilized the complete set of cyclones instead of using only those that reached the landfall for training and outperformed previous approaches that used multi regression models vickery et al 2000 as an alternative to stochastic methods deterministic models have also been used for trajectory forecasting of tropical cyclones fiorino and elsberry 1989 demaria 1987 mohanty 1994 the literature also includes methods that combined forecasts from multiple models to generate a consensus prediction weber 2003 presented a statistical ensemble of numerical track prediction models assuming dependence on the storm structure location and movement goerss 2000 and weber 2003 showed that predictions from an ensemble approach may be more accurate than predictions from individual models machine learning models have more recently been established as a reliable alternative to other statistical techniques for cyclone trajectory and wind intensity prediction zhang et al 2013 used decision trees to predict the diversion of cyclone trajectories from landing in the west pacific ocean lee and liu 2000 initially predicted cyclone tracks with radial basis function rbf neural networks they also proposed the integration of dynamic link architecture dla for neural dynamics for pattern classification of cyclone tracks ali et al 2007 used simple neural networks multilayer perceptron to predict the position of cyclones in the indian ocean 24 h in advance given the past 12 h of observations the predictions were more accurate when compared to traditional models such as climatology and persistence cliper neumann 1972 limited area model lam and the quasi lagrangian model qlm mathur 1991 ali et al 2012 used simple neural networks to predict the tropical cyclone heat potential which is a crucial factor influencing the cyclone intensity although machine learning methods show promising results for cyclone track and intensity forecasting problems they generate single point predictions that lack uncertainty quantification bayesian inference provides a mechanism for estimating unknown model parameters and quantifying uncertainty in predictions mackay 1996 in comparison to single point estimates such as gradient descent methods bayesian inference represents the unknown model parameters using probability posterior distributions and applies computational methods such as variational inference jordan et al 1999 blei et al 2017 rezende et al 2014 and markov chain monte carlo mcmc methods metropolis et al 1953 hastings 1970 andrieu et al 2003 to sample estimate them bayesian inference methods have commonly been used for uncertainty quantification in geoscientific modelling pall et al 2020 chandra et al 2019a bayesian neural networks bnns specht 1990 richard and lippmann 1991 wan 1990 mackay 1995 refer to the use of bayesian inference for inference training neural network weights and biases zhu et al 2016 proposed a bagging approach with multiple two layered bayesian neural networks as members for forecasting cyclone tracks in the south china sea a scaled conjugate gradients algorithm is used to obtain the maximum a posteriori approximation map of the neural network weights while laplace approximation is used to approximate a gaussian posterior distribution deo and chandra 2019 used bnns via langevin gradient mcmc sampling for multi step ahead prediction of cyclone intensities however there is no work for uncertainty quantification using bnns for cyclone trajectory prediction a significant amount of work has been done in the area of bayesian neural networks and bayesian deep learning using variational inference vi techniques as they can be more easily integrated when compared to mcmc with gradient based optimization methods backpropagation methods such as bayes by backprop blundell et al 2015 and variational autoencoders kingma and welling 2013 are prominent implementations of variational inference however variational inference provides an approximate inference since it treats the marginalization needed while performing bayesian inference as an optimization problem jordan et al 1999 wainwright and jordan 2008 blei et al 2017 and does not directly sample from the posterior distribution as in the case of mcmc sampling although uncertainty quantification in model predictions is vital due to the chaotic nature of cyclones and related extreme events fraedrich and leslie 1989 there is limited work using bnns and variational inference in modelling and prediction we note that deep learning methods such as long short term memory lstm hochreiter and schmidhuber 1997 recurrent neural networks rnns have been prominent in modelling temporal sequences and hence have been popular for storms and cyclones gao et al 2018 pan et al 2019 alemany et al 2019 moradi kordmahalleh et al 2016 the progress in the last decade with variational inference and deep learning methods motivates their application for uncertainty quantification in modelling cyclones in this paper we implement variational inference via two selected rnn models to jointly predict tropical cyclone and hurricane trajectory path and intensity we formulate this as a three dimensional time series prediction problem where the first two dimensions represent the latitude and the longitude trajectory and the third dimension represents the wind intensity of the cyclone we refer to the variational inference framework as bayes rnn and bayes lstm which implement simple rnns and lstm models respectively we evaluate the performance of the framework on cyclones and hurricanes that appeared in the last four decades in four selected regions that cover india and the pacific ocean we also investigate key model parameters and report prediction accuracy uncertainty projections and computational efficiency the rest of the paper is organized as follows section 2 provides a background on related methods and section 3 presents the data pre processing and the proposed method section 4 presents experiments and the results section 5 discusses the results and section 6 concludes with future research directions 2 related work 2 1 machine learning for cyclones computational intelligence and machine learning methods have been very promising in predicting cyclone trajectories chaudhuri et al 2015 carr iii et al 2001 kovordányi and roy 2009 evolutionary algorithms are optimization methods hruschka et al 2009 zhou et al 2011 črepinšek et al 2013 that provide gradient free optimization which have been used for training neural network models floreano et al 2008 stanley et al 2019 galván and mooney 2021 neuroevolution has been a suitable alternative to backpropagation however they face the limitation of requiring excessive computational time since they heuristically approximate the gradients using evolutionary operators such as crossover and mutation back and schwefel 1996 munk et al 2015 apart from this neuroevolution has the ability to be easily applied for training diverse neural network architectures and can be used to address problems that have changing dynamics such as reinforcement learning such et al 2017 stanley et al 2019 neuroevolution has shown promising results in time series prediction problems studies chandra 2015 du et al 2014 chandra and zhang 2012 and has the ability to model dynamic time series where the model can handle dynamic length of the time series i e the model can be used for cyclones to make predictions with minimal data chandra et al 2018b rnns are well suited for modelling temporal relationships in data due to their structural properties elman 1990 this property makes rnns quite popular for cyclone track and intensity prediction problems alemany et al 2019 pan et al 2019 moradi kordmahalleh et al 2016 igarashi and tajima 2021 evolutionary algorithms have also been utilized to train rnn models to predict hurricane trajectories kordmahalleh et al 2015 chandra et al chandra et al 2015 chandra and dayal 2015 employed neuro evolution of rnns for cyclone path prediction for the south pacific ocean deo and chandra 2016 present a study on identifying the minimal timespan required for robust predictions of cyclone wind intensity using rnns later deo and chandra utilized stacked ensemble learning for cyclone intensity prediction deo et al 2017 convolutional neural networks cnns have the ability to preserve spatial correlations and have been prominent in computer vision tasks gu et al 2018 cnn s have also been used to incorporate the spatial dependence caused by different starting locations of various cyclones while training lecun et al 1990 hong et al 2017 analysed the hyper tensional satellite images via a cnn model to track cyclones in north pacific zhang et al 2018 used matrix neural networks that preserve the spatial correlation by representing inputs as matrices which yielded better accuracy when compared to rnns such as gated recurrent units grus and lstms 2 2 bayesian inference for deep learning in the case of bayesian inference mcmc methods face issues with convergence scalability and multi modal posterior making them unsuitable for big data problems with high dimensional parameter space sampling from complex posterior distribution is computationally expensive where large number of samples need to be drawn to efficiently sample the posterior distribution in order to address these issues a number of strategies have been developed including combining mcmc with gradient based langevin and hamiltonian mcmc girolami and calderhead 2011 roberts and rosenthal 1998 neal et al 2011 welling and teh 2011 chandra et al 2018a and meta heuristic approaches drugan and thierens 2003 strens 2003 ter braak 2006 ter braak and vrugt 2008 strens 2003 to form efficient proposal distribution in mcmc sampling other approaches implement structural changes to mcmc sampling that balance exploration with exploitation such as nested sampling skilling et al 2006 and parallel tempering mcmc swendsen and wang 1986 hukushima and nemoto 1996 although mcmc sampling struggles computationally with a large number of parameters recent progress with parallel tempering and langevin gradients has shown to address these limitations for graph cnns chandra et al 2021a and deep autoencoders chandra et al 2021b furthermore chandra et al 2019b presented a multi processing framework for langevin gradient proposal distribution with parallel tempering mcmc for bayesian neural networks for chaotic time series prediction and pattern classification problems apart from mcmc we have variational inference for implementing bayesian inference variational inference assumes the shape of the posterior distribution and designs an optimization problem to find the assumed variational densities that are closest to the true posterior densities by minimizing kullback leibler kl divergence it introduces tractability to the bayesian inference problem and solves the problem of slow convergence for high dimensional parameter spaces initial variational inference approaches for bnns used mean field variational bayes mfvb for analytical approximations for regression problems with a single hidden layer neural network barber and bishop 1998 hinton and van camp 1993 variational inference gained popularity in the more recent deep learning revolution due to the need for robust uncertainty quantification in predictions graves 2011 presented the computation of derivatives of expectations in the variational objective function also known as the evidence lower bound elbo bundell et al proposed bayes by backprop that provided a simple implementation of variational inference for neural networks blundell et al 2015 as the sampling operation is not deterministic non differentiable in order to perform gradient optimization over the variational loss the re parameterization trick is employed kingma and welling 2013 rezende et al 2014 kingma and welling 2013 show that re parameterization of variational lower bound enables the representation of random variables trainable parameters as deterministic functions with noise and yields an estimator that can be conveniently optimized via stochastic gradient descent bayes by backprop also utilizes the re parameterization trick to estimate the derivatives of the expectations the weight uncertainty is later utilized to push the trade off between exploration and exploitation in reinforcement learning the results in blundell et al 2015 show that bayes by backprop outperforms dropout regularization with a rather simple formulation of the loss also known as the variational free energy 3 data and methodology 3 1 data pre processing we consider four cyclone datasets for evaluating the proposed variational inference framework these datasets include cyclone tracks in the north indian ocean south pacific ocean north west pacific ocean and south indian ocean which are taken from the joint typhoon warning center jtwc anon 2015 the jtwc has the responsibility of issuing tropical cyclone warnings in these regions for the united states department of defense and other government agencies they primarily protect military ships and aircraft jointly operated with the united states and other countries around the world the website features yearly data as an archive consisting of individual cyclone tracks as text files each track in these datasets features cyclone latitude longitude and wind intensity recorded at regular intervals for the duration of the cyclone the location and the wind intensity of i th cyclone in the dataset at time step t is represented as a vector x 1 i t x 2 i t x 3 i t each cyclone in the dataset has an initial location represented by latitude and longitude pair x 1 i 0 x 2 i 0 and duration given by the total time time steps t i the unique initial positions of the cyclones thus introduce a spatial artefact in the dataset as a consequence of the spatial artefact two cyclones with similar trajectory patterns and differing initial positions are interpreted as different cyclones by the model we adopt a phase shift reconstruction of the trajectories to remove any spatial dependence introduced by differences in starting positions since we are only concerned with the trajectory of the cyclone and not the starting position location we recenter the cyclone tracks such that all the cyclones start at the same location i e 0 0 this is done with the help of the following transformation to the original tracks 1 x 1 i t x 2 i t x 1 i t x 1 i 0 x 2 i t x 2 i 0 where i 0 and the pair x 1 i t x 2 i t represents the latitude and the longitude of the cyclone i at the time step t a similar transformation can be applied to the wind intensity values as well 2 x 3 i t x 3 i t x 3 i 0 similar to the previous notations x 3 i t denotes the wind intensity of cyclone i at time step t while x 3 i 0 is the initial wind intensity of the cyclone note that x 1 i x 2 i x 3 i are all column vectors of span r t i we apply taken s embedding theorem takens 1981 for state space reconstruction windowing of time series data the theorem uses a sliding window of size α at regular time interval β since the cyclone track data is a multivariate time series each cyclone track for our respective bayes rnn model is represented as a three dimensional tensor of shape t i α α 3 using this three dimensional notation the cyclone track is represented as t i α windows of length α containing the location of cyclone x 1 x 2 x 3 we only consider single step ahead prediction where each training example in track i at time step t is represented using an input matrix x t i of shape α 3 and an output vector y t i of size 3 for a single step ahead time step 3 x t i x 1 i t 1 x 2 i t 1 x 3 i t 1 x 1 i t 2 x 2 i t 2 x 3 i t 2 x 1 i t α x 2 i t α x 3 i t α y t i x 1 i t α 1 x 2 i t α 1 x 3 i t α 1 where t 0 1 2 t i α 1 in the case of multi step m ahead cyclone track prediction not implemented in this work the target is a matrix of shape m 3 3 2 recurrent neural networks rnns have the capability to model dynamic temporal dependencies in sequential data using internal representation of previous states and current inputs elman 1990 this behaviour allows rnns to predict future states of a time series by modelling them as a function of previous states the output of an rnn at time step t is computed as 4 h t tanh w i i x t b i i w h i h t 1 b h i 5 o t tanh w o h h t b o h where h t and x t are the current hidden state and input vectors and h t 1 is the hidden state from previous time step as shown in eq 4 the hidden state at time step t is computed as a function of the weighted sum of previous hidden state h t 1 and input vector x t the weights for this computation are given by w i and bias b h finally the output o t is computed as a weighted sum of the hidden states at time step t as shown in eq 5 the weights for this computation are given by w o and the bias b o see fig 1 3 3 long short term memory despite their extraordinary effectiveness rnns tend to suffer from the problem of vanishing gradients wherein the gradient used for backpropagation decreases exponentially with the number of hidden layers this causes rnns to become less useful when it comes to learning relatively longer temporal dependencies long short term memory lstm networks deal with this problem by dividing each unit into multiple gates a conventional lstm unit includes an input gate a cell state a forget gate and an output gate the cell state is responsible for storing information over arbitrary time intervals whereas the other gates regulate the flow of this information the input to the lstm at time step t is processed through these gates by 6 i t σ w i i x t b i i w i h h t 1 b i h 7 f t σ w f i x t b f i w f h h t 1 b f h 8 o t σ w o i x t b o i w o h h t 1 b o h 9 c ˆ t tanh w o i x t b o i w o h h t 1 b o h where x t is the current input state and h t is the current hidden state which also acts as the output for the lstm state the forget input and output gate are represented by f t i t and o t respectively the output from these gates is used to update the hidden and the memory states at time step t as 10 c t f t c t 1 i t c ˆ t 11 h t o t tanh c t where c ˆ t is referred to as the candidate or memory state and c t refers to the cell state as shown in the eqs 6 the input gate uses separate weights and bias w i i b i i and w i h b i h for the inputs x t and previous hidden states h t 1 respectively the forget gate eq 7 and the output gate eq 8 similarly compute a linear combination of hidden and input states along with a sigmoid activation function although the candidate state eq 9 is also computed as a function of hidden and input states the activation function used is a hyperbolic tangent function t a n h instead of a sigmoid the cell state is then computed by adding the matrix dot products of f t and c t 1 to the matrix dot product of i t and c ˆ t finally the hidden state is computed by taking the matrix dot product of o t and hyperbolic tan of c t see fig 2 3 4 variational inference for recurrent neural networks given the input vector x χ the target labels y y and the neural network parameters θ the output of neural network model is given by the objective function f x θ here χ is the feature space and y is the label space in this formulation the model tries to learn the objective function f with parameters θ for the pair x y during the training process in the case of a single layer rnn model the model parameters are given as θ u v w b h b o this rnn model can be viewed as a probabilistic model with output probability p y x θ given the input x and parameters θ we note that in the case of classification problems p y x θ represents a categorical distribution whereas in the case of regression problems the target is a real value i e y r and p y x θ follows a gaussian distribution we can learn the rnn model parameters weights and biases by finding the maximum likelihood estimate mle of the parameters θ ˆ given as 12 θ ˆ arg max θ i p y i x i θ arg max θ i log p y i x i θ this can be achieved with the help of a gradient based optimization which is also known as backpropagation provided p y x is differentiable with respect to θ in order to introduce regularization we can assume a prior distribution for the parameters θ given by p θ θ ˆ then represents the maximum a posteriori map estimate of the parameters given as 13 θ ˆ arg max θ i log p y i x i θ log p θ we use a gaussian prior with a zero mean and a standard deviation τ to introduce l2 regularization which is also known as weight decay krogh and hertz 1992 in the backpropagation literature the prior density is then given as 14 p θ 1 2 π τ 2 l 2 exp 1 2 τ 2 l 1 l θ l where l is the total number of trainable parameters in the neural network the above formulation results in the single point estimate of θ that maximizes the posterior density however to infer the posterior distribution of the model parameters we can use variational inference to approximate the posterior distribution we begin by assuming a variational posterior on the neural network weights given by q θ δ parameterized by δ variational inference learning is then used to find the value of δ that minimize the kl divergence between the variational posterior and the true posterior 15 δ ˆ arg min δ kl q θ δ p θ d 16 arg min δ kl q θ δ p θ e q θ δ log p d θ where d represent the data containing the input and output pairs x y p θ d is the true posterior and p d θ is the likelihood the loss function then is given as 17 l kl q θ δ p θ e q θ δ log p d θ we can use the formulation in blundell et al 2015 to approximate this loss function using monte carlo sampling given by 18 l i 1 m log q θ i δ log p θ i p d θ i where θ i represents the i th sample drawn from the variational posterior q θ δ the loss value depends directly on the particular weights drawn from the variation posterior 3 5 variational inference framework for cyclone track and intensity prediction the raw cyclone trajectories given by latitude longitude and wind intensity taken from the jtwc are re centred and reconstructed by windowing the sequence using taken s theorem as discussed in the data pre processing section fig 4 shows the framework for cyclone trajectory prediction using bayes rnn and bayes lstm we note that the cyclones are concatenated and divided into a training and test set defined by year we present the input sequences represented by matrix x t into two bayes rnn models to generate one step ahead predictions one of the models generates the predictions for latitude and longitude via two output neurons the other generates the predictions for wind intensity note that the trailing data points that do not make it into a window are discarded from training data we then compute the log of likelihood prior and the variational density using the bayes rnn predictions and the corresponding one step ahead target actual value the variational density is calculated by adding the log probability over the gaussian distributions parameterized by the mean and variance values given in the weight matrix the likelihood prior and the variational density are then used to calculate the loss as described in eq 18 algorithm 1 provides the details for training the bayes rnn model for cyclone trajectory and wind intensity prediction tasks we note that the specific rnn model can be either simple rnn or an lstm based rnn the algorithm begins by defining the rnn model f x θ initializing the variational parameters δ and setting the hyper parameters i e number of training epochs n e p o c h s number of monte carlo samples for computing the variational loss n s a m p l e and the prior variance τ 2 we consider the variational posterior to be a diagonal gaussian distribution therefore the rnn parameters are given as θ n μ σ 2 where σ log 1 exp ρ the function f x log 1 exp x is known as the softplus which is used to ensure that the value of σ is always positive in order to sample the parameters θ we first sample from a standard gaussian and shift it by mean μ and scale by the standard deviation σ as shown in steps 1 and 2 of the algorithm steps 1 and 2 are repeated to generate n s a m p l e s model predictions in step 3 the model prediction is used to compute the variational loss eq 18 the loss function is optimized using gradient based optimization also known as backpropagation fig 3 shows the bayes rnn framework for cyclone trajectory and intensity prediction the neural network architecture consists of four layers an input layer an rnn or lstm layer a dense layer and an output layer the input layer as well as the output layer contain three neurons while the number of recurrent and dense layer neurons are decided set empirically for each dataset we note that we use backpropagation to learn the variational parameters δ μ ρ that parameterize the variational posterior distribution for the rnn model parameters θ thus by learning the parameters such as μ and ρ we learn the distribution over the rnn model parameters θ 4 experiments and results in this section we present the details of the dataset the experiments and the results from bayes rnn we compare the performance of the bayes rnn that utilize simple rnn and lstm models with their non bayesian counterparts 4 1 dataset table 1 shows the four selected regions from the jtwc with the number of unique cyclone trajectories for each region our processed version of the jtwc dataset used for this work is available on github 1 1 https github com sydney machine learning cyclonedatasets we note that the terms hurricane and cyclone are merely distinguished by their names in the dataset and there are no major differences between these terms we refer to the hurricanes used in the datasets as cyclones hereafter the raw dataset consists of cyclone parameters cyclone no date time latitude longitude and the wind speed in knots for each cyclone recorded at 6 hour time intervals fig 5 shows the movement of 20 randomly sampled cyclones in each region aside from the data pre processing mentioned in section 3 1 we also normalize the coordinates of each cyclone by subtracting the mean value over the region and dividing it by the standard deviation the dataset includes cyclones from 1985 to 2019 out of which 70 of the data points are used for training while the remaining as test samples fig 6 presents the histogram of the lengths of different tracks we observe that more than half of cyclones last for up to 40 to 80 h in duration while the number of cyclones in the north west pacific ocean and the south pacific ocean differs the cyclone duration in these two regions shows a similar pattern a similar observation can be made for north indian and south indian oceans as well 4 2 experiment setting we divide the experiments into two major tasks with two different models i e cyclone track prediction and intensity prediction hence a model is used for track prediction and a separate one is used for wind intensity we compare the performance of 4 different types of models rnn lstm bayes rnn bayes lstm for the track and intensity prediction for each of the ocean regions as described in section 3 1 in order to maintain uniformity amongst the models and for a fair comparison we take the most basic form of each model with just a single hidden layer and a minimal number of hidden neurons to represent the problem obtained from trial experiments the training data consists of time series from each cyclone taken with a window of size 4 and the corresponding target to be predicted since each data point is recorded at a 6 hour interval a window size of 4 represents a 24 hour timeframe in the case of the track prediction task each element of the sequence consists of a pair of values the corresponding latitude and longitude whereas for the intensity prediction task each element in the sequence consists of only the intensity value in practice we have noticed that this form of optimization is highly sensitive to the hyper parameters likelihood function and prior density function blundell et al 2015 use a scaled mixture of two gaussians as the prior distribution however in our case we empirically found that a high variance gaussian prior yielded better accuracy performance compared to the sub optimal results shown by the mixture prior thus we define the gaussian prior with zero mean and variance equal to 36 for model training finally we implement the respective models using the pytorch machine learning library 2 2 pytorch org and train the models using intel core i5 1145g7 processor with 8 cores along with 16 gigabyte random access memory ram 4 3 evaluation of predictions continuous ranked probability score crps matheson and winkler 1976 is often used to measure the performance of probabilistic predictions that are scalar continuous values crps is a measure of quadratic distance between the forecast cumulative distribution function cdf and the empirical cdf of the observation for a random variable x with cdf f the crps is computed as follows 19 c r p s f x f x 1 y x 2 d y where 1 is the heaviside step function along real line whose value is given by 20 1 z 1 if z 0 0 otherwise gneiting and raftery 2007 show that the crps can be equivalently written as follows 21 c r p s f x e x x 1 2 x x where x and x are independent copies of the random variable associated with f crps value close to zero is generally desirable although crps is common with univariate forecasts it cannot be used for multivariate predictions as in our case therefore we use energy score es gneiting et al 2008 which can be viewed as a generalization of crps suitable for multivariate predictions and is computed as 22 e s f x e x x 1 2 x x where is the euclidean norm in the case of ensemble predictions of size m es is evaluated as 23 e s f x 1 m j 1 m x j x 1 2 m 2 i 1 m j 1 m x i x j the energy score provides a direct method of comparison between deterministic ensemble and density forecasts in addition to the energy score we also report the root mean squared error rmse of the predictions which are computed as 24 r m s e i 0 n y i y i 2 n where y i refers to the predicted value y i refers to the target value and n is the total number of training samples windows we chose the rmse as one of the performance metrics because it is commonly used to benchmark continuous forecasting models in a spatio temporal setting 4 4 results tables 2 and 3 present the results rmse and es for train and test sets of the track and wind intensity prediction tasks respectively for cyclones in the four different ocean regions they feature the mean and confidence interval of rmse denoted by along with the energy scores es across 100 model predictions using rnn bayes rnn lstm and bayes lstm the model weights for bayes rnn and bayes lstm were drawn from their variational posterior distribution 100 times and then used for prediction on the test set in the case of rnn and lstm models 100 models were trained independently we observe that in the case of intensity prediction for the north indian ocean dataset lstm yields the best ensemble es and rmse values on the train and test sets bayes rnn and bayes lstm perform slightly worse than their single point estimate counterparts however in all the other problem sets both track and intensity prediction the variational counterparts bayes lstm and bayes rnn show a lower value of es and rmse values on both train and test sets it is clear that the variational models provide a good approximation of the empirically observed distribution of the single point rnn and lstm models we also notice that in many cases the bayes rnn and bayes lstm provide a lower mean and variance of rmse values compared to the rnn and lstm models this is primarily because rnn and lstm models have a tendency to get stuck in the local minima which affects the overall distribution of the rmse values leading to a higher mean and variance the variational models do not suffer from this issue and provide a good approximation of the true posterior distribution fig 7 presents a barplot of rmse values reported by randomly selected models of each type on 10 random cyclone tracks of each ocean for the cyclone trajectory prediction task in the case of rnn and lstm models we randomly select a model from 100 independently trained models while in bayes rnn bayes lstm models we randomly sample weights from the learned variational distribution of parameters we can observe that the lstm models give consistently better performance lower rmse values as compared to the rnn models the bayesian models also give better or similar performance as compared to their vanilla counterparts for most tracks similarly fig 8 is a barplot of rmse values reported by the randomly selected models of each type on 10 random cyclone tracks of each ocean for the cyclone intensity prediction task we can observe similar trends in model performance as in the trajectory prediction task however we also observe that for some of the trajectories the vanilla rnn provides the lowest rmse value for intensity predictions north west pacific ocean this differs from the trend observed in the reported mean of rmse in table 2 vanilla rnn often suffers from the problem of local optima and due to this some of the samples of rnn fail to reach the global optimum solution during training this leads to a higher overall mean and variance in the rmse of predictions furthermore the intensity values are highly discretized in the multiples of 5 as compared to the trajectory coordinates this leads to a much more uneven dataset and so the disparity in rmse values of different cyclones is also much more prominent in the intensity prediction task as compared to trajectory prediction for example cyclone track no 6 for the north west pacific ocean in the sub fig 8 a has a disproportionally large rmse value when compared with other cyclones fig 9 contains the subplots of actual trajectories as well as the trajectories predicted by the rnn lstm bayes lstm and bayes rnn models for random cyclones from the 4 respective ocean regions the dates when the recording for each cyclone started have also been provided in the captions do note that the model was trained on neither of these cyclones prior to prediction here we can easily observe that for the sampled cyclone in the north west pacific ocean the bayes lstm and the lstm models predict the trajectory very accurately for the most part much more so than the bayes rnn and rnn models we see a somewhat larger discrepancy between the predicted tracks and the actual track for the north indian ocean but the general trend for the cyclone has still been captured for the south indian and south pacific oceans we observe very similar predictions by the models and the actual trajectory has to a large extent been accurately predicted fig 10 allows us to see how the prediction by bayesian models would probably work in real time by showing the true position of the cyclone and the possible positions as predicted by the bayes lstm model in form of a probability distribution visualized by a contour fig 11 further plots full cyclones selected randomly from each ocean along with their true values and the probability distribution of the prediction at each point as given by the bayes lstm and bayes rnn models in the form of a contour 5 discussion given the chaotic and spatio temporal nature of cyclones fraedrich and leslie 1989 zhang and tao 2013 recurrent neural networks including simple rnns and lstm networks are generally preferred models for prediction of cyclone trajectories the results of cyclone track and intensity prediction show that bayes rnn and bayes lstm models powered via variational inference show either comparable or better prediction accuracy in most of the cases when compared to canonical simple rnn and lstm models we were able to approximate the posterior distribution of rnn model weights via variational inference which has been used to quantify uncertainty in the prediction using the predictive posterior distribution bayes rnn and bayes lstm models via variational inference can be highly sensitive to hyper parameters blundell et al 2015 such as prior variance τ 2 learning rate number of markov samples n s a m p l e s thus the choice of hyper parameters can lead to drastically different performance results and may affect the convergence although we have used a rather less informative prior i e a zero centred isotropic gaussian with large variance a more informative prior may provide a high level of regularization leading to difference in the test performance variational inference methodologies assumes a family of distributions whose parameters are learnt as a result of the optimization process on the other hand mcmc methods draw samples directly from the posterior distribution hence a limitation of bayes rnn is that we are restricted by the family of distribution we assume as the variational distribution although with infinite compute capability mcmc may provide the true posterior via the drawn samples mcmc methods face scalability issues with high dimensional parameter such as in the case of neural networks however these are continuously being addressed with efficient proposal distributions and sampling schemes chandra et al 2019b chandra and kapoor 2020 chandra et al 2021a b overall we found that the proposed approach provides a good approximation of the posterior distribution given the family of variational distribution in our case we assumed gaussian distribution over the neural network weights however this assumption can be updated later with more insights rnn and lstm models are excellent models for capturing temporal dependencies however these models lack the ability to capture spatial correlation in the cyclone data as shown by zhang et al 2018 recently it has been shown that temporal convolutional neural networks tcnn excel in preserving the spatial correlations while also learning the temporal dependencies in such data lea et al 2017 yan et al 2020 however it has not been specifically tested for cyclone prediction in future work the proposed bayesian framework via variational inference could be extended to tcnn models for improved spatial correlation representation we model the cyclone track prediction as a multivariate time series forecasting problem where we only use historical values of latitude and longitude information the wind intensity is modelled as a univariate prediction problem this methodology can be extended to multi step ahead prediction and addition to this a multivariate approach could include more features such as the distance of the cyclone to the landfall the sea surface temperature which are not taken into account in this study these features could possibly lead to better predictions which can be a part of future studies 6 conclusion and future work we presented a bayesian framework via variational inference for approximating the posterior distribution of deep learning model parameters for the cyclone trajectory and wind intensity prediction problem we first removed the spatial artefact in the original cyclone data by re centring the cyclone tracks to standardize the starting position we provided a comparison of the proposed bayesian framework and the results showed better accuracy in most cases when compared to the canonical methods rnn and lstm we provided a visualization to quantify uncertainty in predictions for the case of track prediction which has been challenging since it is a spatio temporal prediction problem and therefore requires the model to capture trends along both the time and the spatial dimension simultaneously in addition the uncertainty in track prediction is difficult to represent on a two dimensional graph of latitude and longitude axis when the data is represented chronologically due to the spatial correlation software and data availability we provide open source code and data for our proposed framework via github repository with following meta information name varrnn cyclones developers arpit kapoor and anshul negi contact email kapoor arpit97 gmail com compatible operating system mac linux windows developed and tested ubuntu 20 04 linux size of repository 1 55mb year published 2022 source github 3 3 https github com dare ml variational rnn cyclones declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment we thank the australian government for supporting this research through the australian research council s industrial transformation training centre in data analytics for resources and environments dare project ic190100031 
25431,cyclone track forecasting is a critical climate science problem involving time series prediction of cyclone location and intensity machine learning methods have shown much promise in this domain especially deep learning methods such as recurrent neural networks rnns however these methods generally make single point predictions with little focus on uncertainty quantification although markov chain monte carlo mcmc methods have often been used for quantifying uncertainty in neural network predictions these methods are computationally expensive variational inference vi is an alternative to mcmc sampling that approximates the posterior distribution of parameters by minimizing a kl divergence loss between the estimate and the true posterior in this paper we present variational rnns for cyclone track and intensity prediction in four different regions across the globe we utilise simple rnns and long short term memory lstm rnns and use the energy score es to evaluate multivariate probabilistic predictions the results show that variational rnns provide a good approximation with uncertainty quantification when compared to conventional rnns while maintaining prediction accuracy keywords cyclone track prediction recurrent neural networks bayesian neural networks long short term memory variational inference data availability the link to data and code has been shared as part of the submission 1 introduction in the last few decades the devastating impact of climate change became more obvious with rise in adverse meteorological events such as droughts heat waves hurricanes and tropical cyclones mendelsohn et al 2012 and bushfires the devastating effects of tropical cyclones and hurricanes include flooding landslides detrimental winds and storm surges which makes them a dominant meteorological hazard needham et al 2015 schrum et al 2020 apart from these tropical cyclones cause a significant amount of economic and environmental damage pielke 2007 fengjin and ziniu 2010 lionello et al 2006 harmelin vivien 1994 forecasting cyclone tracks trajectory and intensities are critical for lowering injuries and saving lives apart from prevention of damage to infrastructure and mitigation of economic losses mcbride and holland 1987 cyclones typhoons and hurricanes are all tropical storms with the distinguishing characteristic being the geographical location where they form tropical storms that form over north atlantic ocean and north east pacific are known as hurricanes and those developing in the north west pacific are known as typhoons while cyclones are the storms that form over the south pacific and the indian ocean emanuel 2003 cyclone track forecasting is a multi dimensional time series prediction problem with cyclone location represented by latitude and longitude and its wind intensity recorded at each time step roy et al presented a comprehensive review of tropical cyclone track forecasting methods roy and kovordányi 2012a more recently chen et al presented a machine learning focused review of tropical cyclone forecast modelling chen et al 2020 traditionally numerical yablonsky et al 2015 and statistical methods techniques mcadie and lawrence 2000 roy and kovordányi 2012b have been utilized for forecasting cyclone trajectories regression models have commonly been used for cyclone trajectory neumann 1972 as well as intensity prediction atkinson and holliday 1977 atkinson and holliday 1977 derive a high degree polynomial equation to model the wind speed pressure relationship for tropical cyclones in the western north pacific ocean due to the chaotic nature of cyclones estimating the risk of cyclones reaching landfall along with their wind intensities has been challenging in previous studies hall and jewson 2007 used a non parametric model and utilized the complete set of cyclones instead of using only those that reached the landfall for training and outperformed previous approaches that used multi regression models vickery et al 2000 as an alternative to stochastic methods deterministic models have also been used for trajectory forecasting of tropical cyclones fiorino and elsberry 1989 demaria 1987 mohanty 1994 the literature also includes methods that combined forecasts from multiple models to generate a consensus prediction weber 2003 presented a statistical ensemble of numerical track prediction models assuming dependence on the storm structure location and movement goerss 2000 and weber 2003 showed that predictions from an ensemble approach may be more accurate than predictions from individual models machine learning models have more recently been established as a reliable alternative to other statistical techniques for cyclone trajectory and wind intensity prediction zhang et al 2013 used decision trees to predict the diversion of cyclone trajectories from landing in the west pacific ocean lee and liu 2000 initially predicted cyclone tracks with radial basis function rbf neural networks they also proposed the integration of dynamic link architecture dla for neural dynamics for pattern classification of cyclone tracks ali et al 2007 used simple neural networks multilayer perceptron to predict the position of cyclones in the indian ocean 24 h in advance given the past 12 h of observations the predictions were more accurate when compared to traditional models such as climatology and persistence cliper neumann 1972 limited area model lam and the quasi lagrangian model qlm mathur 1991 ali et al 2012 used simple neural networks to predict the tropical cyclone heat potential which is a crucial factor influencing the cyclone intensity although machine learning methods show promising results for cyclone track and intensity forecasting problems they generate single point predictions that lack uncertainty quantification bayesian inference provides a mechanism for estimating unknown model parameters and quantifying uncertainty in predictions mackay 1996 in comparison to single point estimates such as gradient descent methods bayesian inference represents the unknown model parameters using probability posterior distributions and applies computational methods such as variational inference jordan et al 1999 blei et al 2017 rezende et al 2014 and markov chain monte carlo mcmc methods metropolis et al 1953 hastings 1970 andrieu et al 2003 to sample estimate them bayesian inference methods have commonly been used for uncertainty quantification in geoscientific modelling pall et al 2020 chandra et al 2019a bayesian neural networks bnns specht 1990 richard and lippmann 1991 wan 1990 mackay 1995 refer to the use of bayesian inference for inference training neural network weights and biases zhu et al 2016 proposed a bagging approach with multiple two layered bayesian neural networks as members for forecasting cyclone tracks in the south china sea a scaled conjugate gradients algorithm is used to obtain the maximum a posteriori approximation map of the neural network weights while laplace approximation is used to approximate a gaussian posterior distribution deo and chandra 2019 used bnns via langevin gradient mcmc sampling for multi step ahead prediction of cyclone intensities however there is no work for uncertainty quantification using bnns for cyclone trajectory prediction a significant amount of work has been done in the area of bayesian neural networks and bayesian deep learning using variational inference vi techniques as they can be more easily integrated when compared to mcmc with gradient based optimization methods backpropagation methods such as bayes by backprop blundell et al 2015 and variational autoencoders kingma and welling 2013 are prominent implementations of variational inference however variational inference provides an approximate inference since it treats the marginalization needed while performing bayesian inference as an optimization problem jordan et al 1999 wainwright and jordan 2008 blei et al 2017 and does not directly sample from the posterior distribution as in the case of mcmc sampling although uncertainty quantification in model predictions is vital due to the chaotic nature of cyclones and related extreme events fraedrich and leslie 1989 there is limited work using bnns and variational inference in modelling and prediction we note that deep learning methods such as long short term memory lstm hochreiter and schmidhuber 1997 recurrent neural networks rnns have been prominent in modelling temporal sequences and hence have been popular for storms and cyclones gao et al 2018 pan et al 2019 alemany et al 2019 moradi kordmahalleh et al 2016 the progress in the last decade with variational inference and deep learning methods motivates their application for uncertainty quantification in modelling cyclones in this paper we implement variational inference via two selected rnn models to jointly predict tropical cyclone and hurricane trajectory path and intensity we formulate this as a three dimensional time series prediction problem where the first two dimensions represent the latitude and the longitude trajectory and the third dimension represents the wind intensity of the cyclone we refer to the variational inference framework as bayes rnn and bayes lstm which implement simple rnns and lstm models respectively we evaluate the performance of the framework on cyclones and hurricanes that appeared in the last four decades in four selected regions that cover india and the pacific ocean we also investigate key model parameters and report prediction accuracy uncertainty projections and computational efficiency the rest of the paper is organized as follows section 2 provides a background on related methods and section 3 presents the data pre processing and the proposed method section 4 presents experiments and the results section 5 discusses the results and section 6 concludes with future research directions 2 related work 2 1 machine learning for cyclones computational intelligence and machine learning methods have been very promising in predicting cyclone trajectories chaudhuri et al 2015 carr iii et al 2001 kovordányi and roy 2009 evolutionary algorithms are optimization methods hruschka et al 2009 zhou et al 2011 črepinšek et al 2013 that provide gradient free optimization which have been used for training neural network models floreano et al 2008 stanley et al 2019 galván and mooney 2021 neuroevolution has been a suitable alternative to backpropagation however they face the limitation of requiring excessive computational time since they heuristically approximate the gradients using evolutionary operators such as crossover and mutation back and schwefel 1996 munk et al 2015 apart from this neuroevolution has the ability to be easily applied for training diverse neural network architectures and can be used to address problems that have changing dynamics such as reinforcement learning such et al 2017 stanley et al 2019 neuroevolution has shown promising results in time series prediction problems studies chandra 2015 du et al 2014 chandra and zhang 2012 and has the ability to model dynamic time series where the model can handle dynamic length of the time series i e the model can be used for cyclones to make predictions with minimal data chandra et al 2018b rnns are well suited for modelling temporal relationships in data due to their structural properties elman 1990 this property makes rnns quite popular for cyclone track and intensity prediction problems alemany et al 2019 pan et al 2019 moradi kordmahalleh et al 2016 igarashi and tajima 2021 evolutionary algorithms have also been utilized to train rnn models to predict hurricane trajectories kordmahalleh et al 2015 chandra et al chandra et al 2015 chandra and dayal 2015 employed neuro evolution of rnns for cyclone path prediction for the south pacific ocean deo and chandra 2016 present a study on identifying the minimal timespan required for robust predictions of cyclone wind intensity using rnns later deo and chandra utilized stacked ensemble learning for cyclone intensity prediction deo et al 2017 convolutional neural networks cnns have the ability to preserve spatial correlations and have been prominent in computer vision tasks gu et al 2018 cnn s have also been used to incorporate the spatial dependence caused by different starting locations of various cyclones while training lecun et al 1990 hong et al 2017 analysed the hyper tensional satellite images via a cnn model to track cyclones in north pacific zhang et al 2018 used matrix neural networks that preserve the spatial correlation by representing inputs as matrices which yielded better accuracy when compared to rnns such as gated recurrent units grus and lstms 2 2 bayesian inference for deep learning in the case of bayesian inference mcmc methods face issues with convergence scalability and multi modal posterior making them unsuitable for big data problems with high dimensional parameter space sampling from complex posterior distribution is computationally expensive where large number of samples need to be drawn to efficiently sample the posterior distribution in order to address these issues a number of strategies have been developed including combining mcmc with gradient based langevin and hamiltonian mcmc girolami and calderhead 2011 roberts and rosenthal 1998 neal et al 2011 welling and teh 2011 chandra et al 2018a and meta heuristic approaches drugan and thierens 2003 strens 2003 ter braak 2006 ter braak and vrugt 2008 strens 2003 to form efficient proposal distribution in mcmc sampling other approaches implement structural changes to mcmc sampling that balance exploration with exploitation such as nested sampling skilling et al 2006 and parallel tempering mcmc swendsen and wang 1986 hukushima and nemoto 1996 although mcmc sampling struggles computationally with a large number of parameters recent progress with parallel tempering and langevin gradients has shown to address these limitations for graph cnns chandra et al 2021a and deep autoencoders chandra et al 2021b furthermore chandra et al 2019b presented a multi processing framework for langevin gradient proposal distribution with parallel tempering mcmc for bayesian neural networks for chaotic time series prediction and pattern classification problems apart from mcmc we have variational inference for implementing bayesian inference variational inference assumes the shape of the posterior distribution and designs an optimization problem to find the assumed variational densities that are closest to the true posterior densities by minimizing kullback leibler kl divergence it introduces tractability to the bayesian inference problem and solves the problem of slow convergence for high dimensional parameter spaces initial variational inference approaches for bnns used mean field variational bayes mfvb for analytical approximations for regression problems with a single hidden layer neural network barber and bishop 1998 hinton and van camp 1993 variational inference gained popularity in the more recent deep learning revolution due to the need for robust uncertainty quantification in predictions graves 2011 presented the computation of derivatives of expectations in the variational objective function also known as the evidence lower bound elbo bundell et al proposed bayes by backprop that provided a simple implementation of variational inference for neural networks blundell et al 2015 as the sampling operation is not deterministic non differentiable in order to perform gradient optimization over the variational loss the re parameterization trick is employed kingma and welling 2013 rezende et al 2014 kingma and welling 2013 show that re parameterization of variational lower bound enables the representation of random variables trainable parameters as deterministic functions with noise and yields an estimator that can be conveniently optimized via stochastic gradient descent bayes by backprop also utilizes the re parameterization trick to estimate the derivatives of the expectations the weight uncertainty is later utilized to push the trade off between exploration and exploitation in reinforcement learning the results in blundell et al 2015 show that bayes by backprop outperforms dropout regularization with a rather simple formulation of the loss also known as the variational free energy 3 data and methodology 3 1 data pre processing we consider four cyclone datasets for evaluating the proposed variational inference framework these datasets include cyclone tracks in the north indian ocean south pacific ocean north west pacific ocean and south indian ocean which are taken from the joint typhoon warning center jtwc anon 2015 the jtwc has the responsibility of issuing tropical cyclone warnings in these regions for the united states department of defense and other government agencies they primarily protect military ships and aircraft jointly operated with the united states and other countries around the world the website features yearly data as an archive consisting of individual cyclone tracks as text files each track in these datasets features cyclone latitude longitude and wind intensity recorded at regular intervals for the duration of the cyclone the location and the wind intensity of i th cyclone in the dataset at time step t is represented as a vector x 1 i t x 2 i t x 3 i t each cyclone in the dataset has an initial location represented by latitude and longitude pair x 1 i 0 x 2 i 0 and duration given by the total time time steps t i the unique initial positions of the cyclones thus introduce a spatial artefact in the dataset as a consequence of the spatial artefact two cyclones with similar trajectory patterns and differing initial positions are interpreted as different cyclones by the model we adopt a phase shift reconstruction of the trajectories to remove any spatial dependence introduced by differences in starting positions since we are only concerned with the trajectory of the cyclone and not the starting position location we recenter the cyclone tracks such that all the cyclones start at the same location i e 0 0 this is done with the help of the following transformation to the original tracks 1 x 1 i t x 2 i t x 1 i t x 1 i 0 x 2 i t x 2 i 0 where i 0 and the pair x 1 i t x 2 i t represents the latitude and the longitude of the cyclone i at the time step t a similar transformation can be applied to the wind intensity values as well 2 x 3 i t x 3 i t x 3 i 0 similar to the previous notations x 3 i t denotes the wind intensity of cyclone i at time step t while x 3 i 0 is the initial wind intensity of the cyclone note that x 1 i x 2 i x 3 i are all column vectors of span r t i we apply taken s embedding theorem takens 1981 for state space reconstruction windowing of time series data the theorem uses a sliding window of size α at regular time interval β since the cyclone track data is a multivariate time series each cyclone track for our respective bayes rnn model is represented as a three dimensional tensor of shape t i α α 3 using this three dimensional notation the cyclone track is represented as t i α windows of length α containing the location of cyclone x 1 x 2 x 3 we only consider single step ahead prediction where each training example in track i at time step t is represented using an input matrix x t i of shape α 3 and an output vector y t i of size 3 for a single step ahead time step 3 x t i x 1 i t 1 x 2 i t 1 x 3 i t 1 x 1 i t 2 x 2 i t 2 x 3 i t 2 x 1 i t α x 2 i t α x 3 i t α y t i x 1 i t α 1 x 2 i t α 1 x 3 i t α 1 where t 0 1 2 t i α 1 in the case of multi step m ahead cyclone track prediction not implemented in this work the target is a matrix of shape m 3 3 2 recurrent neural networks rnns have the capability to model dynamic temporal dependencies in sequential data using internal representation of previous states and current inputs elman 1990 this behaviour allows rnns to predict future states of a time series by modelling them as a function of previous states the output of an rnn at time step t is computed as 4 h t tanh w i i x t b i i w h i h t 1 b h i 5 o t tanh w o h h t b o h where h t and x t are the current hidden state and input vectors and h t 1 is the hidden state from previous time step as shown in eq 4 the hidden state at time step t is computed as a function of the weighted sum of previous hidden state h t 1 and input vector x t the weights for this computation are given by w i and bias b h finally the output o t is computed as a weighted sum of the hidden states at time step t as shown in eq 5 the weights for this computation are given by w o and the bias b o see fig 1 3 3 long short term memory despite their extraordinary effectiveness rnns tend to suffer from the problem of vanishing gradients wherein the gradient used for backpropagation decreases exponentially with the number of hidden layers this causes rnns to become less useful when it comes to learning relatively longer temporal dependencies long short term memory lstm networks deal with this problem by dividing each unit into multiple gates a conventional lstm unit includes an input gate a cell state a forget gate and an output gate the cell state is responsible for storing information over arbitrary time intervals whereas the other gates regulate the flow of this information the input to the lstm at time step t is processed through these gates by 6 i t σ w i i x t b i i w i h h t 1 b i h 7 f t σ w f i x t b f i w f h h t 1 b f h 8 o t σ w o i x t b o i w o h h t 1 b o h 9 c ˆ t tanh w o i x t b o i w o h h t 1 b o h where x t is the current input state and h t is the current hidden state which also acts as the output for the lstm state the forget input and output gate are represented by f t i t and o t respectively the output from these gates is used to update the hidden and the memory states at time step t as 10 c t f t c t 1 i t c ˆ t 11 h t o t tanh c t where c ˆ t is referred to as the candidate or memory state and c t refers to the cell state as shown in the eqs 6 the input gate uses separate weights and bias w i i b i i and w i h b i h for the inputs x t and previous hidden states h t 1 respectively the forget gate eq 7 and the output gate eq 8 similarly compute a linear combination of hidden and input states along with a sigmoid activation function although the candidate state eq 9 is also computed as a function of hidden and input states the activation function used is a hyperbolic tangent function t a n h instead of a sigmoid the cell state is then computed by adding the matrix dot products of f t and c t 1 to the matrix dot product of i t and c ˆ t finally the hidden state is computed by taking the matrix dot product of o t and hyperbolic tan of c t see fig 2 3 4 variational inference for recurrent neural networks given the input vector x χ the target labels y y and the neural network parameters θ the output of neural network model is given by the objective function f x θ here χ is the feature space and y is the label space in this formulation the model tries to learn the objective function f with parameters θ for the pair x y during the training process in the case of a single layer rnn model the model parameters are given as θ u v w b h b o this rnn model can be viewed as a probabilistic model with output probability p y x θ given the input x and parameters θ we note that in the case of classification problems p y x θ represents a categorical distribution whereas in the case of regression problems the target is a real value i e y r and p y x θ follows a gaussian distribution we can learn the rnn model parameters weights and biases by finding the maximum likelihood estimate mle of the parameters θ ˆ given as 12 θ ˆ arg max θ i p y i x i θ arg max θ i log p y i x i θ this can be achieved with the help of a gradient based optimization which is also known as backpropagation provided p y x is differentiable with respect to θ in order to introduce regularization we can assume a prior distribution for the parameters θ given by p θ θ ˆ then represents the maximum a posteriori map estimate of the parameters given as 13 θ ˆ arg max θ i log p y i x i θ log p θ we use a gaussian prior with a zero mean and a standard deviation τ to introduce l2 regularization which is also known as weight decay krogh and hertz 1992 in the backpropagation literature the prior density is then given as 14 p θ 1 2 π τ 2 l 2 exp 1 2 τ 2 l 1 l θ l where l is the total number of trainable parameters in the neural network the above formulation results in the single point estimate of θ that maximizes the posterior density however to infer the posterior distribution of the model parameters we can use variational inference to approximate the posterior distribution we begin by assuming a variational posterior on the neural network weights given by q θ δ parameterized by δ variational inference learning is then used to find the value of δ that minimize the kl divergence between the variational posterior and the true posterior 15 δ ˆ arg min δ kl q θ δ p θ d 16 arg min δ kl q θ δ p θ e q θ δ log p d θ where d represent the data containing the input and output pairs x y p θ d is the true posterior and p d θ is the likelihood the loss function then is given as 17 l kl q θ δ p θ e q θ δ log p d θ we can use the formulation in blundell et al 2015 to approximate this loss function using monte carlo sampling given by 18 l i 1 m log q θ i δ log p θ i p d θ i where θ i represents the i th sample drawn from the variational posterior q θ δ the loss value depends directly on the particular weights drawn from the variation posterior 3 5 variational inference framework for cyclone track and intensity prediction the raw cyclone trajectories given by latitude longitude and wind intensity taken from the jtwc are re centred and reconstructed by windowing the sequence using taken s theorem as discussed in the data pre processing section fig 4 shows the framework for cyclone trajectory prediction using bayes rnn and bayes lstm we note that the cyclones are concatenated and divided into a training and test set defined by year we present the input sequences represented by matrix x t into two bayes rnn models to generate one step ahead predictions one of the models generates the predictions for latitude and longitude via two output neurons the other generates the predictions for wind intensity note that the trailing data points that do not make it into a window are discarded from training data we then compute the log of likelihood prior and the variational density using the bayes rnn predictions and the corresponding one step ahead target actual value the variational density is calculated by adding the log probability over the gaussian distributions parameterized by the mean and variance values given in the weight matrix the likelihood prior and the variational density are then used to calculate the loss as described in eq 18 algorithm 1 provides the details for training the bayes rnn model for cyclone trajectory and wind intensity prediction tasks we note that the specific rnn model can be either simple rnn or an lstm based rnn the algorithm begins by defining the rnn model f x θ initializing the variational parameters δ and setting the hyper parameters i e number of training epochs n e p o c h s number of monte carlo samples for computing the variational loss n s a m p l e and the prior variance τ 2 we consider the variational posterior to be a diagonal gaussian distribution therefore the rnn parameters are given as θ n μ σ 2 where σ log 1 exp ρ the function f x log 1 exp x is known as the softplus which is used to ensure that the value of σ is always positive in order to sample the parameters θ we first sample from a standard gaussian and shift it by mean μ and scale by the standard deviation σ as shown in steps 1 and 2 of the algorithm steps 1 and 2 are repeated to generate n s a m p l e s model predictions in step 3 the model prediction is used to compute the variational loss eq 18 the loss function is optimized using gradient based optimization also known as backpropagation fig 3 shows the bayes rnn framework for cyclone trajectory and intensity prediction the neural network architecture consists of four layers an input layer an rnn or lstm layer a dense layer and an output layer the input layer as well as the output layer contain three neurons while the number of recurrent and dense layer neurons are decided set empirically for each dataset we note that we use backpropagation to learn the variational parameters δ μ ρ that parameterize the variational posterior distribution for the rnn model parameters θ thus by learning the parameters such as μ and ρ we learn the distribution over the rnn model parameters θ 4 experiments and results in this section we present the details of the dataset the experiments and the results from bayes rnn we compare the performance of the bayes rnn that utilize simple rnn and lstm models with their non bayesian counterparts 4 1 dataset table 1 shows the four selected regions from the jtwc with the number of unique cyclone trajectories for each region our processed version of the jtwc dataset used for this work is available on github 1 1 https github com sydney machine learning cyclonedatasets we note that the terms hurricane and cyclone are merely distinguished by their names in the dataset and there are no major differences between these terms we refer to the hurricanes used in the datasets as cyclones hereafter the raw dataset consists of cyclone parameters cyclone no date time latitude longitude and the wind speed in knots for each cyclone recorded at 6 hour time intervals fig 5 shows the movement of 20 randomly sampled cyclones in each region aside from the data pre processing mentioned in section 3 1 we also normalize the coordinates of each cyclone by subtracting the mean value over the region and dividing it by the standard deviation the dataset includes cyclones from 1985 to 2019 out of which 70 of the data points are used for training while the remaining as test samples fig 6 presents the histogram of the lengths of different tracks we observe that more than half of cyclones last for up to 40 to 80 h in duration while the number of cyclones in the north west pacific ocean and the south pacific ocean differs the cyclone duration in these two regions shows a similar pattern a similar observation can be made for north indian and south indian oceans as well 4 2 experiment setting we divide the experiments into two major tasks with two different models i e cyclone track prediction and intensity prediction hence a model is used for track prediction and a separate one is used for wind intensity we compare the performance of 4 different types of models rnn lstm bayes rnn bayes lstm for the track and intensity prediction for each of the ocean regions as described in section 3 1 in order to maintain uniformity amongst the models and for a fair comparison we take the most basic form of each model with just a single hidden layer and a minimal number of hidden neurons to represent the problem obtained from trial experiments the training data consists of time series from each cyclone taken with a window of size 4 and the corresponding target to be predicted since each data point is recorded at a 6 hour interval a window size of 4 represents a 24 hour timeframe in the case of the track prediction task each element of the sequence consists of a pair of values the corresponding latitude and longitude whereas for the intensity prediction task each element in the sequence consists of only the intensity value in practice we have noticed that this form of optimization is highly sensitive to the hyper parameters likelihood function and prior density function blundell et al 2015 use a scaled mixture of two gaussians as the prior distribution however in our case we empirically found that a high variance gaussian prior yielded better accuracy performance compared to the sub optimal results shown by the mixture prior thus we define the gaussian prior with zero mean and variance equal to 36 for model training finally we implement the respective models using the pytorch machine learning library 2 2 pytorch org and train the models using intel core i5 1145g7 processor with 8 cores along with 16 gigabyte random access memory ram 4 3 evaluation of predictions continuous ranked probability score crps matheson and winkler 1976 is often used to measure the performance of probabilistic predictions that are scalar continuous values crps is a measure of quadratic distance between the forecast cumulative distribution function cdf and the empirical cdf of the observation for a random variable x with cdf f the crps is computed as follows 19 c r p s f x f x 1 y x 2 d y where 1 is the heaviside step function along real line whose value is given by 20 1 z 1 if z 0 0 otherwise gneiting and raftery 2007 show that the crps can be equivalently written as follows 21 c r p s f x e x x 1 2 x x where x and x are independent copies of the random variable associated with f crps value close to zero is generally desirable although crps is common with univariate forecasts it cannot be used for multivariate predictions as in our case therefore we use energy score es gneiting et al 2008 which can be viewed as a generalization of crps suitable for multivariate predictions and is computed as 22 e s f x e x x 1 2 x x where is the euclidean norm in the case of ensemble predictions of size m es is evaluated as 23 e s f x 1 m j 1 m x j x 1 2 m 2 i 1 m j 1 m x i x j the energy score provides a direct method of comparison between deterministic ensemble and density forecasts in addition to the energy score we also report the root mean squared error rmse of the predictions which are computed as 24 r m s e i 0 n y i y i 2 n where y i refers to the predicted value y i refers to the target value and n is the total number of training samples windows we chose the rmse as one of the performance metrics because it is commonly used to benchmark continuous forecasting models in a spatio temporal setting 4 4 results tables 2 and 3 present the results rmse and es for train and test sets of the track and wind intensity prediction tasks respectively for cyclones in the four different ocean regions they feature the mean and confidence interval of rmse denoted by along with the energy scores es across 100 model predictions using rnn bayes rnn lstm and bayes lstm the model weights for bayes rnn and bayes lstm were drawn from their variational posterior distribution 100 times and then used for prediction on the test set in the case of rnn and lstm models 100 models were trained independently we observe that in the case of intensity prediction for the north indian ocean dataset lstm yields the best ensemble es and rmse values on the train and test sets bayes rnn and bayes lstm perform slightly worse than their single point estimate counterparts however in all the other problem sets both track and intensity prediction the variational counterparts bayes lstm and bayes rnn show a lower value of es and rmse values on both train and test sets it is clear that the variational models provide a good approximation of the empirically observed distribution of the single point rnn and lstm models we also notice that in many cases the bayes rnn and bayes lstm provide a lower mean and variance of rmse values compared to the rnn and lstm models this is primarily because rnn and lstm models have a tendency to get stuck in the local minima which affects the overall distribution of the rmse values leading to a higher mean and variance the variational models do not suffer from this issue and provide a good approximation of the true posterior distribution fig 7 presents a barplot of rmse values reported by randomly selected models of each type on 10 random cyclone tracks of each ocean for the cyclone trajectory prediction task in the case of rnn and lstm models we randomly select a model from 100 independently trained models while in bayes rnn bayes lstm models we randomly sample weights from the learned variational distribution of parameters we can observe that the lstm models give consistently better performance lower rmse values as compared to the rnn models the bayesian models also give better or similar performance as compared to their vanilla counterparts for most tracks similarly fig 8 is a barplot of rmse values reported by the randomly selected models of each type on 10 random cyclone tracks of each ocean for the cyclone intensity prediction task we can observe similar trends in model performance as in the trajectory prediction task however we also observe that for some of the trajectories the vanilla rnn provides the lowest rmse value for intensity predictions north west pacific ocean this differs from the trend observed in the reported mean of rmse in table 2 vanilla rnn often suffers from the problem of local optima and due to this some of the samples of rnn fail to reach the global optimum solution during training this leads to a higher overall mean and variance in the rmse of predictions furthermore the intensity values are highly discretized in the multiples of 5 as compared to the trajectory coordinates this leads to a much more uneven dataset and so the disparity in rmse values of different cyclones is also much more prominent in the intensity prediction task as compared to trajectory prediction for example cyclone track no 6 for the north west pacific ocean in the sub fig 8 a has a disproportionally large rmse value when compared with other cyclones fig 9 contains the subplots of actual trajectories as well as the trajectories predicted by the rnn lstm bayes lstm and bayes rnn models for random cyclones from the 4 respective ocean regions the dates when the recording for each cyclone started have also been provided in the captions do note that the model was trained on neither of these cyclones prior to prediction here we can easily observe that for the sampled cyclone in the north west pacific ocean the bayes lstm and the lstm models predict the trajectory very accurately for the most part much more so than the bayes rnn and rnn models we see a somewhat larger discrepancy between the predicted tracks and the actual track for the north indian ocean but the general trend for the cyclone has still been captured for the south indian and south pacific oceans we observe very similar predictions by the models and the actual trajectory has to a large extent been accurately predicted fig 10 allows us to see how the prediction by bayesian models would probably work in real time by showing the true position of the cyclone and the possible positions as predicted by the bayes lstm model in form of a probability distribution visualized by a contour fig 11 further plots full cyclones selected randomly from each ocean along with their true values and the probability distribution of the prediction at each point as given by the bayes lstm and bayes rnn models in the form of a contour 5 discussion given the chaotic and spatio temporal nature of cyclones fraedrich and leslie 1989 zhang and tao 2013 recurrent neural networks including simple rnns and lstm networks are generally preferred models for prediction of cyclone trajectories the results of cyclone track and intensity prediction show that bayes rnn and bayes lstm models powered via variational inference show either comparable or better prediction accuracy in most of the cases when compared to canonical simple rnn and lstm models we were able to approximate the posterior distribution of rnn model weights via variational inference which has been used to quantify uncertainty in the prediction using the predictive posterior distribution bayes rnn and bayes lstm models via variational inference can be highly sensitive to hyper parameters blundell et al 2015 such as prior variance τ 2 learning rate number of markov samples n s a m p l e s thus the choice of hyper parameters can lead to drastically different performance results and may affect the convergence although we have used a rather less informative prior i e a zero centred isotropic gaussian with large variance a more informative prior may provide a high level of regularization leading to difference in the test performance variational inference methodologies assumes a family of distributions whose parameters are learnt as a result of the optimization process on the other hand mcmc methods draw samples directly from the posterior distribution hence a limitation of bayes rnn is that we are restricted by the family of distribution we assume as the variational distribution although with infinite compute capability mcmc may provide the true posterior via the drawn samples mcmc methods face scalability issues with high dimensional parameter such as in the case of neural networks however these are continuously being addressed with efficient proposal distributions and sampling schemes chandra et al 2019b chandra and kapoor 2020 chandra et al 2021a b overall we found that the proposed approach provides a good approximation of the posterior distribution given the family of variational distribution in our case we assumed gaussian distribution over the neural network weights however this assumption can be updated later with more insights rnn and lstm models are excellent models for capturing temporal dependencies however these models lack the ability to capture spatial correlation in the cyclone data as shown by zhang et al 2018 recently it has been shown that temporal convolutional neural networks tcnn excel in preserving the spatial correlations while also learning the temporal dependencies in such data lea et al 2017 yan et al 2020 however it has not been specifically tested for cyclone prediction in future work the proposed bayesian framework via variational inference could be extended to tcnn models for improved spatial correlation representation we model the cyclone track prediction as a multivariate time series forecasting problem where we only use historical values of latitude and longitude information the wind intensity is modelled as a univariate prediction problem this methodology can be extended to multi step ahead prediction and addition to this a multivariate approach could include more features such as the distance of the cyclone to the landfall the sea surface temperature which are not taken into account in this study these features could possibly lead to better predictions which can be a part of future studies 6 conclusion and future work we presented a bayesian framework via variational inference for approximating the posterior distribution of deep learning model parameters for the cyclone trajectory and wind intensity prediction problem we first removed the spatial artefact in the original cyclone data by re centring the cyclone tracks to standardize the starting position we provided a comparison of the proposed bayesian framework and the results showed better accuracy in most cases when compared to the canonical methods rnn and lstm we provided a visualization to quantify uncertainty in predictions for the case of track prediction which has been challenging since it is a spatio temporal prediction problem and therefore requires the model to capture trends along both the time and the spatial dimension simultaneously in addition the uncertainty in track prediction is difficult to represent on a two dimensional graph of latitude and longitude axis when the data is represented chronologically due to the spatial correlation software and data availability we provide open source code and data for our proposed framework via github repository with following meta information name varrnn cyclones developers arpit kapoor and anshul negi contact email kapoor arpit97 gmail com compatible operating system mac linux windows developed and tested ubuntu 20 04 linux size of repository 1 55mb year published 2022 source github 3 3 https github com dare ml variational rnn cyclones declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgment we thank the australian government for supporting this research through the australian research council s industrial transformation training centre in data analytics for resources and environments dare project ic190100031 
25432,morphometric analysis of a basin or watershed describes the physical characteristics of the basin that are useful for environmental studies watershed morphometric characteristics are based on the terrain processing that must be done for each different watershed there is no application that provides basin morphometric characteristics basin borders and streams in the basin on a global scale so the development of a new application has become a necessity in this research an online google earth engine gee application has been developed that determines the morphometric characteristics of any watershed on a global scale using digital elevation models basin boundaries and river stream data the meriç basin was selected for testing the application the results of the application were validated by comparing them to arcgis the findings demonstrate that in the absence of gis software or data gee may be used to determine basin morphometric characteristics on a worldwide scale keywords morphological characteristics hydrology gis gee watershed basin data availability i have shared the link of my code 1 introduction morphometric analysis of a basin or watershed describes the physical characteristics of the watershed which are useful for environmental studies such as in the areas of land use planning terrain elevation soil conservation and soil erosion ingewar et al 2017 the processing of dem to delineate watersheds is referred to as terrain pre processing and extracting the morphometric characteristics of the watershed and streams using terrain pre processing is a time consuming process while watershed delineation can be obtained through the use of automated methods with the availability of digital elevation models dem and geographic information systems gis tools most of the morphometric parameters still have to be calculated manually benda et al 2007 vinet and zhedanov 2011 omran et al 2016 safa ahmed abd elgader 2016 shen et al 2016 wilson 2018 merwade 2019 safanelli et al 2020 since each watershed or basin is unique there have been numerous studies that have investigated the morphometric characteristics of watersheds and basins aadil hamid 2013 khare 2014 altıparmak and türkoğlu 2018 wilson 2018 üzülmez 2019 moreover when dems with different resolutions and thresholds are used in the stream definition process as a consequence of terrain pre processing the watershed delineation and stream definition procedures must be redone for the same watershed and the studies must be updated additionally the number of streams at each order is still determined by the users through the manual merging of stream order segments for determining the bifurcation ratio which is an important parameter that expresses the degree of ramification of drainage network beg 2016 although there are several interactive web applications developed by various organisations around the world to monitor and introduce the hydrological properties of basins watersheds and streams usgs 2007 harvard university 2008 conservation biology institute 2010 environment canada 2012 un 2016 wwf 2016 booker and whitehead 2017 the rivers trust 2020 canhn 2021 epa 2021 trust 2021 us epa 2021 usgs 2021 environment agency 2022 ncdeq 2022 when viewed on a global scale it is clear that these applications are insufficient particularly in terms of morphometric characteristics because they only cover the borders of that country or state use limited data and none exist for obtaining watershed morphometric characteristics on the other hand for end users who focus on watershed and stream morphometric characteristics obtaining these parameters requires computer hardware gis software resources and gis expertise since desktop gis software s big data requirements create problems and don t allow high volume studies they need a gis software or experience and since there is no application that provides basin morphometric characteristics basin borders and streams in the basin on a global scale the development of a new application has become a necessity an alternative to using desktop gis or remote sensing applications with topography from digital elevation data dem and country boundaries is the google earth engine gee among the other capabilities of gee is the inclusion of data sets on river basins on a global scale and there are several applications developed using this environment donchyts 2020 nilsen 2020 scott and shokoufandeh 2020 despite its advantages the gee platform does not seem to be an easy tool for end users with limited programming knowledge the aims of this study are without the need of any desktop gis software dem or any dataset 1 developing an online gee application to obtain morphometric characteristics including stream order classification at different basin levels on a global scale via global hydrobasins hydrorivers and dems within the scope of continent country province and district borders 2 determining administrative units scope of watersheds 3 generating the upstream and downstream flow lines of any stream segment in watershed 4 generating the longitudinal profile of the downstream flow line 5 creating and exporting morphometric characteristics as tables administrative unit boundaries watersheds and all stream data as vectors 2 materials and methods the current study will describe the steps involved in creating the gee application named nku man which focuses on watershed morphometric analysis within the scope of continent country province or district borders and is written in javascript accordingly the main essential steps in the development of the application are the determination of calculation equations and formulas of morphometric characteristics that can be adapted to gee with global watersheds stream datasets and dem s the results of which must meet the requirements and the execution of the application must be simple and obvious to users 2 1 development environment and validation the development environment used in this research is gee a cloud based platform for environmental data analysis and the api allows the user to access the functionality of gee in both javascript and python gorelick et al 2017 gee s cloud based computing infrastructure increases operational efficiency and opens up unique possibilities for global studies including hydrological processes campos taberner et al 2018 the results obtained from the developed application were compared and verified using arcgis desktop esri 2011 with the archhydro plugin 2 2 datasets among the other capabilities of gee is the inclusion of numerous datasets that have been developed by different institutions and can form the basis for the morphometric characteristics of the basin on a global scale while planning this study different public datasets table 1 with different scales that gee has were investigated and included in this application watershed delineation and stream definition within the watershed boundaries depend on the resolution of the dem used in the terrain processing and the threshold area value determined for the stream definition at the global level hydrobasins represents a collection of vectorized polygon layers representing sub basin boundaries at a resolution of 15 arc seconds hydrobasins were derived from the gridded hydrosheds core layers lehner 2014 the hydrorivers v1 0 is a database aiming to provide the vectorized line network of all global rivers that have a catchment area of at least 10 km2 or an average river flow of 0 1 cubic meters per second or both wwf 2021 free flowing rivers were defined to start at every pixel where the accumulated upstream catchment area exceeds 10 km2 grill et al 2019 alos dsm v3 2 is a global digital surface model dsm dataset with a horizontal resolution of approximately 30 m and released in january 2021 gee 2019 hydrosheds s hydrologically conditioned dem with 3s resolution is used for comparing the study results 2 3 developing application 2 3 1 the application graphical user interface gui as already stated the aim of this study was to create an online application that was simple enough for users to use without any prior knowledge of coding or gis software therefore it has become necessary to create a gui interface that addresses the need for coding the conceptual architecture of the application is shown in fig 1 2 3 2 development of javascript functions in the application morphologic analyses are based on four aspects watershed geometric characteristics watershed relief characteristics drainage texture characteristics and drainage network characteristics it has been developed by several functions for visualization watersheds and streams computing watershed morphometric characteristics tracing and monitoring upstream and downstream with linear characteristics of streams in the gee platform these functions can be divided into six groups according to their functionality 1 selecting supported relief datasets 2 selecting aoi 3 determining continental or administrative units watershed geometric and relief characteristics 4 determining the watershed drainage texture drainage network characteristics including stream order classification 5 generating upstream and downstream characteristics and longitudinal graph of downstream 6 exporting functions the results as tables and vectors the calculation equations and formulas used in the morphometric analysis were taken from their original references all characteristics were determined using mathematical operators provided by the gee environment and are now prepared and available for download as tables or vector files 2 3 2 1 selecting supported relief datasets the dem dataset are used for determining relief morphometric characteristics three gee ui select widgets are used for selection of dem flow accumulation acc and flow direction datasets dir in the select supported dataset tab aspect and slope layers are generated with gee s ee terrain slope and ee terrain aspect functions the isolines layer is generated with the getisolines function donchyts 2020 2 3 2 2 selecting area of interest the application focuses on watershed morphometric analysis within the scope of continent country province or district borders gee s ui select widgets are used for selecting continental borders and to investigate other countries provinces or districts respectively large scale international boundary usds 2022 and global administrative unit layers 2015 fao un 2015 used for selecting aoi 2 3 2 3 calculation of geometric and relief characteristics of the watershed the workflow of the calculation of the geometric and relief characteristics of the watershed can be summarised as follows fig 2 watershed geometric characteristics are measurements of the linear features of the watershed or basin the main variables of the watershed hybas id and sub area are taken from hydrobasin feature attributes lehner 2014 the processing and calculation equations and formulas used in the geometric and relief characteristics are given in the tables below tables 2 and 3 watershed relief characteristics are calculated with gee s reduce region function using dem 2 3 2 4 calculating watershed drainage texture and drainage network characteristics the workflow of calculating watershed drainage texture and drainage network characteristics of the stream network can be summarised in fig 3 the calculation equations and formulas used in the watershed drainage texture and drainage network analysis are given in the tables below tables 4 and 5 the crucial stage of morphometric analysis is the classification of stream order the discipline of morphometric research frequently makes use of strahler s ordering method a stream segment that originates from the stream source and has no tributaries is designated as a first order segment in strahler s classification system a second order segment is made by joining two first order segments a third order segment is made by joining 2 s order segments and so on the strahler id field ord stra of each segment in the hydroriver dataset is used to calculate the stream ordering the ratio of the number of stream segments in a given order to the number of stream segments in the next higher order is designated as the bifurcation ratio schumm 1956 only watershed drainage texture characteristics can be calculated in river or stream data sets without the strahler order attribute for these datasets the workflow for calculating the watershed drainage texture of the stream network is 1 5 steps in fig 5 2 3 2 5 generating upstream and downstream characteristics and a longitudinal graph of downstream generating upstream and downstream routines have been developed using the gee functionalities to get upstream and downstream of selected segments of streams these functions are recursive functions appendix a and they have been used with hyriv id next down from the hydrorivers dataset lehner et al 2008 schmidt and jones 2020 and noid ndoid fields from the free following rivers dataset the longitudinal profile is the graph of distance versus elevation and its construction provides an interpretation of surface history as the erosional curves and the river course flow from the upstream to the outlet of the basin at any stage of evolution beg 2016 in the current study a function has been developed for drawing the longitudinal profile based on the downstream flow direction using the dem layer 2 3 2 6 exporting functions the results as tables and vectors the results of application are prepared with gee s getdownloadurl function in different file formats the results of morphometric analysis generated as tables csv border of aoi watershed boundary stream network downstream and upstream segments as vectors kml longitudinal graph as figure csv png svg 3 a case study in meriç river basin to evaluate nku man experiments have been carried out using different continental countries provinces and district borders the gee functions given in the previous section have been implemented to study the morphometric characteristics of the meric river basin the analysis of meric river basin as a sample with wwf level 5 watershed hydrorivers and alos30 dem from the analysed basins will be used for discussion the results of application execution in this research the findings of the morphometric characteristics obtained from the application will be summarised this study focuses on the evaluation of this application 3 1 study area although the application is universal and can work globally when evaluating the results of nku man the meric river basin was used as an example fig 4 the meric maritsa evros is the largest river on the balkan peninsula and it originates in the rila mountains in western bulgaria and marks the border between turkey and greece the meriç river is a transboundary waterway it flows east southeast between the balkans and the rhodopes through plovdiv and dimitrovgrad to edirne in turkey where it empties into the sea the meric river is one of the most important drainage systems of the balkans in southeast europe and is the largest river on the balkan peninsula the meriç river has a basin of 52 600 km2 that stores water and its length is 550 km yildiz 2019 the meriç basin 42 52 40 40 37 20 n 23 35 31 28 14 36 e is located within the borders of three countries bulgaria turkey and greece in the mediterranean basin and is one of the largest transboundary river basins of eastern europe and the balkan peninsula fig 4 intensive farming is widespread in the meric basin the basin is also highly industrialised and densely populated with the largest cities tombul 2015 ozsahin et al 2018 yildiz 2019 3 2 results the interface of the application developed at the end of the study is shown in fig 5 the application gui provides interactive web based interfaces for users to select dem datasets area of interest aoi borders watersheds and river datasets perform morphometric analysis and present results the default settings of nku man are that the active aoi boundary is turkey the active dem layer is alos30 the active watershed boundaries is hydrobasin and the active river layer is hydrorivers the results obtained from the application were verified by comparing the morphometric characteristics obtained from terrain processing with arcgis desktop software and the archydro plugin using the same basin boundaries as hydrologically conditioned dem from hydrosheds core products 3 2 1 selecting supported relief datasets and selecting aoi the default settings of application are used for selecting relief and aoi functions the active dem layer is alos30 and the active aoi boundary is turkey 3 2 2 determination of administrative units calculation of geometric and relief characteristics of watershed generally the river basin or watershed boundaries are different from the country provincial or district boundaries and the neighbouring countries and administrative units within the river basin boundaries should cooperate on the river basin development plans the developed application provides the names of the countries provinces and districts within the boundaries of the basin or watershed boundary the results of administrative units within the borders of the meriç basin were displayed in alphabetical order by countries table 6 there are 20 provinces and 137 districts from turkey bulgaria and greece within the meriç basin the watershed geometric characteristics obtained from the application are given in table 7 the same data was used in the arcgis software to verify the results the geometric characteristics obtained from the application and arcgis software are the same the watershed area represents the amount of water that may be produced in the watershed by rainfall and the length of the stream that drains it kumar et al 2019 the meric basin s area was determined to be 52511 4 km2 the basin area is specified differently in different sources 53475 km2 in özdemir 2015 52600 km2 in tombul 2015 and 53000 km2 in ozsahin et al 2018 the basin length is the distance from the furthest point in the basin to its outlet the meric basin s length was determined to be 629 56 km basin length is critical to predicting the chances of the basin being flooded lower flooding risk in the basin is associated with greater basin length sayl et al 2019 the watershed perimeter is the outside boundary of a watershed that encloses the area and serves as a measurement for the watershed s size and shape lakshminarayana et al 2022 the meric basin s perimeter was found to be 1492 51 km long the shape factor was defined as the ratio of the main stream length to the diameter of the circle having the same area as the watershed the analysis reveals that the meric basin shape factor is 7 548 the lower value shows the higher run off and is circular in shape and the higher value shows low run off and is elongated in shape it also reflects the drainage pattern of the study area the form factor is defined as the ratio of basin area to the square of basin length horton 1945 the value of form factor would always be less than 0 7854 for a perfectly circular basin in the study area it is 0 132 and the meric basin has a low form factor the elongation ratio is defined as the ratio between the diameter of a circle with the same area as the basin and basin length the value of elongation ratio in the study is 0 232 according to pareta and pareta 2011 the meric basin is in more elongated class 0 5 the compactness coefficient is the ratio of watershed perimeter and equivalent circular area circumference of the watershed it relies only on watershed slope and does not depend on watershed size horton 1945 the compactness coefficient value in the study is 1 837 which means that the shape is approximately a rectangle while the value for basin with a perfect circle form is 1 00 the circulatory ratio is defined as the ratio of the basin area to the area of a circle whose circumference is equal to the perimeter of the basin it is influenced by the length and frequency of streams geological structures land use cover climate relief and slope of the basin the circulatory ratio approaching 1 resembles a basin shaped like a circle the circularity ratio in the study area is 0 296 the higher circularity ratio also indicates a larger amount of flow and vice versa arulbalaji and padmalal 2020 stated that the shape parameters like elongation ratio compactness coefficient shape factor circularity ratio and form factor have an inverse relationship with the erodibility relief characteristics indicate the direction of stream water flow and represent the progression of denudation that occurs within the watershed based on the geophysical and topographical conditions of the terrain the results of the execution functions of the watershed relief characteristics compared with different dem s and with arcgis are given in table 8 although there are differences between the relief characteristics obtained with gee and arcgis due to dems and projection systems the results are close to each other the ratio between the basin relief and the basin length is known as the relief ratio the relief ratio of the meric basin is 3 215 higher relief values point to low infiltration and high run off conditions the relative relief is defined as the ratio of the basin relief to the length of the perimeter relative relief is an indicator of the general steepness of a basin from summit to mouth the meric basin has a relative relief of 135 61 the basin relief is the maximum vertical distance from the stream mouth to the highest point on the divide and it is 2024 for meric basin the dissection index is defined as the ratio of relative to absolute relief it indicates the vertical erosion and dissected characteristics of a basin mahala 2020 the dissection index of the meriç basin is found to be 0 703 3 2 3 drainage texture and drainage network characteristics for a better knowledge of watersheds the drainage texture analysis categorises the land surface pattern and its distinct character it generally focuses on the characteristics of geomorphic conditions such as rough or smooth terrain additionally it combines both linear and aerial elements the results are validated with arcgis the linear characteristics such as drainage density bifurcation ratio stream frequency drainage texture and length of overland flow have a direct bearing on the erodibility the greater the erodibility the higher the values arulbalaji and padmalal 2020 the results of the execution functions of the drainage texture characteristics are given in table 9 the length of overland flow is one of the most important dependent variables affecting both the hydrologic and physiographic development of drainage basins horton 1945 the length of overland flow in the meric basin is 2 047 which shows that sheet erosion is the prominent factor for hydrologic and physiographic development the drainage density is the ratio of the total length of all streams in the watershed to the watershed area it helps in determining the permeability and porosity of the watershed low drainage density is generally preferred in areas with high resistance or highly permeable subsoil materials dense vegetation cover and low relief high drainage density is preferred in regions of weak or impermeable surface materials sparse vegetation and mountainous relief the drainage density is governed by the factors like rock type runoff intensity soil type infiltration capacity and percentage of rocky area the drainage density value of the study area is 0 244 according to schumm 1965 texture ratio is an important factor in the drainage morphometric analysis which is dependent on the underlying lithology infiltration capacity and relief aspect of the terrain the texture ratio of the meric basin is 2 047 horton recognised infiltration capacity as the single most important factor that influences drainage texture ratio and considered drainage texture ratio to include drainage density and stream frequency kumar et al 2019 meric basin s drainage texture ratio is 1 703 according to smith s classification method meric basin has a very coarse texture the stream frequency is defined as the number of streams per unit area and it has a low stream frequency of 0 048 the total length of stream orders in km is 12 826 57 and total number of streams is 2542 the constant of channel maintenance means that 4 094 m2 of surfaces in meric basin require maintenance along 1 m of channel the higher value indicates that the channel capacity should be large enough to carry a higher discharge resulting from the bigger drainage area to address the hydrodynamic characteristics of a basin segmentation and hierarchical ordering of streams are required the results of stream orders stream numbers bifurcation ratios and weighted bifurcation ratios obtained from the horton diagram are given in table 10 the results are validated with arcgis the results of the stream order stream number stream length and stream length ratio mean stream length stream length used in the ratio including the weighed mean stream length ratio is shown in table 11 the horton diagram of stream network is shown in fig 6 the relationship between stream order and logarithmic stream number was investigated it appears to be in geometric progression and agrees with horton s law of stream length for the present study this graph was plotted for the meric basin the graph shows a straight line satisfying horton s law kumar et al 2007 it is evident that the correlation coefficient and co efficient of determination for the straight line fit for the watershed are 0 92 which are quite satisfactory according to the study the total length of the stream decreases with increasing stream order 3 2 4 results of downstream and upstream monitoring functions especially when working on a river that has not been explored before where the study area is not fully defined or when conducting a study on a section of a long river determining the source and discharge mouth of this stream may be essential for the planning of the region under consideration tracking the river or each segment of the stream upstream and downstream rather than the entire stream can be useful in partial analysis especially in large river networks the results of downstream analysis of the selected segment in red from the meric river are shown in fig 7 a the longitudinal profile graph along the route of the river downstream to the point where it empties into the sea is shown in fig 7b the longitudinal profile is one of the important sections for morphometric analysis it shows the characteristics of the river or valley along the water flow direction the longitudinal profile represents an important way to characterise average stream slopes and to determine the stage of evolution the length of the chosen segment in the red circle was calculated to be 11 33 km the number of streams nu to be 133 and the sum of stream lengths lu to be 496 17 km in the longitudional graph fig 7b the height is expected to decrease in a sloping manner peaks in the graph that do not correspond to this trend indicate topological errors in river data or pixel errors in dem data the longitudinal profile graph is user interactive and these peak points can be clicked on the graph to examine these segments of the river for comparison the longitudinal profile graph created with a 1 km2 stream definition threshold from local 5m resolution dem data is shown in fig 8 the results upstream of the selected segment in red from the meric river are shown in fig 9 the length of the chosen segment was calculated to be 2 34 km the number of streams nu to be 2542 and the sum of stream lengths lu to be 12826 61 km with the help of this function the upstream flow route of any segment from the meric river to the water sources is shown 3 2 5 the results of exporting functions the results of exporting functions for generating the border of the aoi watershed boundary stream network and downstream and upstream segments as vectors kml are shown in fig 10 the morphometric analysis results are also generated as tables csv and a longitudinal graph csv png svg the exported kml files were opened and viewed in google earth pro as sample gis software 4 discussion morphometric analysis is a quantitative method that helps evaluate the hydrological characteristics for watershed management these characteristics can be used to interpret the geological conditions responsible for the hydrology of the drainage basin morphometric analysis is about exploring the mathematical relationships between different stream attributes used to compare streams and identify factors that may be the cause of differences this is an important factor in planning any watershed development this study provides an important alternative to obtaining these characteristics which are mostly calculated manually using desktop software in a few minutes using gee and dem datasets this study focused on the evaluation of the application developed with gee for watershed morphometric analysis essentially gis software and dem are used for watershed delineation inspecting watersheds and streams while similar applications can only be used for country borders this application can be used for administrative borders worldwide this application with global data sets enables the user to present the watershed boundaries and flows created by the user in an online interactive way that is accessible to everyone also it has been given an option for organisations and users to work with their own rasters shape files as data sets and use it for monitoring basins and streams without using any gis software the nku man allows users to perform watershed morphometric analysis determine the upstream route of stream segments to water resources and determine the downstream route of stream segments flow into the seas all streams from watersheds that empty to the sea or lakes can be determined and the source of the streams can be traced users can also trace downstream to where they empty and upstream to water sources thanks to this interface users have had the opportunity to perform their analysis and share the results without having any specific coding knowledge in order to create a longitudinal profile in gis software the user must manually click each point along the profile to trace the river or valley this process typically takes a long time and is inaccurate the river longitudinal profile graph is considered to be so useful that it can also be used to detect errors in stream vector data the generated aoi watershed boundary stream network and downstream and upstream flow segments can be downloaded in kml format the morphometric analysis findings from the gee application for the meriç basin were validated using arcgis desktop since the same datasets were used in both environments the findings were similar the analysis of the results shows that the application provides a globally simple new approach for calculating morphometric characteristics saves processing time and reduces measurements data sets and efforts the application that allows the morphometric characteristics of any basin in the world to be obtained in a very short period of time despite all these possibilities of gee the biggest problem encountered is the inability to calculate flow characteristics in large basins with more than 5000 segments however this application can be easily used in basins with smaller segment numbers the research shows the importance of using validated global watershed boundaries and global stream datasets for comparing morphometric characteristics it also shows that different global height data used globally can be useful in calculating basin morphometric relief characteristics 5 conclusion there is a strong need for a novel approach particularly for morphometric analysis of watersheds to monitoring watershed borders and streams within watersheds as well as obtaining basin and stream vector data of watersheds on a global scale for researchers students local governments and users worldwide this study introduced a new approach for inspecting watersheds calculating morphometric characteristics and demonstrating the potential of the gee web based tools this significantly increases the data s and application s usability the user can calculate watershed relief and stream morphometric characteristics using different watershed boundaries river or stream datasets and dem rasters and can compare results across the world the gee does not require any additional software installation so it helps overcome compatibility limitations and allows users to access the available codes and data from any computer connected to the internet with this user friendly application morphometric characteristics of watersheds worldwide can be researched on a very large scale even by users with limited gis and rs knowledge and the results can be obtained in various formats for different applications the application can be considered an exemplary environment where different organisations can present their own prepared watershed boundaries and river data to users as a result this application could enable scientists policymakers and the general public to explore basins watersheds rivers streams and any location in the world without using any gis application author statement mehmet şener conceptualization methodology resources supervision validation writing original draft preparation mehmet cengiz arslanoğlu software data curation writing reviewing and editing visualization investigation software availability name of application nku man namık kemal university morphometric analysis for watersheds streams developer mehmet cengiz arslanoğlu first available 2022 source language javascript requirements google earth engine availability the nku man application link and source code is available for free and can be downloaded from https github com mcarslanoglu nkuman declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors wish to thank to mervan savga ahmet berk taş burak akıncı fatih konukçu whose valuable comments helped to improve this manuscript google earth engine for providing cloud computation service usgs and the authors developers of the hydro sheds dataset appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105640 appendix a recursive functions javascript code for determining upstream and downstream segments of stream image 1 
25432,morphometric analysis of a basin or watershed describes the physical characteristics of the basin that are useful for environmental studies watershed morphometric characteristics are based on the terrain processing that must be done for each different watershed there is no application that provides basin morphometric characteristics basin borders and streams in the basin on a global scale so the development of a new application has become a necessity in this research an online google earth engine gee application has been developed that determines the morphometric characteristics of any watershed on a global scale using digital elevation models basin boundaries and river stream data the meriç basin was selected for testing the application the results of the application were validated by comparing them to arcgis the findings demonstrate that in the absence of gis software or data gee may be used to determine basin morphometric characteristics on a worldwide scale keywords morphological characteristics hydrology gis gee watershed basin data availability i have shared the link of my code 1 introduction morphometric analysis of a basin or watershed describes the physical characteristics of the watershed which are useful for environmental studies such as in the areas of land use planning terrain elevation soil conservation and soil erosion ingewar et al 2017 the processing of dem to delineate watersheds is referred to as terrain pre processing and extracting the morphometric characteristics of the watershed and streams using terrain pre processing is a time consuming process while watershed delineation can be obtained through the use of automated methods with the availability of digital elevation models dem and geographic information systems gis tools most of the morphometric parameters still have to be calculated manually benda et al 2007 vinet and zhedanov 2011 omran et al 2016 safa ahmed abd elgader 2016 shen et al 2016 wilson 2018 merwade 2019 safanelli et al 2020 since each watershed or basin is unique there have been numerous studies that have investigated the morphometric characteristics of watersheds and basins aadil hamid 2013 khare 2014 altıparmak and türkoğlu 2018 wilson 2018 üzülmez 2019 moreover when dems with different resolutions and thresholds are used in the stream definition process as a consequence of terrain pre processing the watershed delineation and stream definition procedures must be redone for the same watershed and the studies must be updated additionally the number of streams at each order is still determined by the users through the manual merging of stream order segments for determining the bifurcation ratio which is an important parameter that expresses the degree of ramification of drainage network beg 2016 although there are several interactive web applications developed by various organisations around the world to monitor and introduce the hydrological properties of basins watersheds and streams usgs 2007 harvard university 2008 conservation biology institute 2010 environment canada 2012 un 2016 wwf 2016 booker and whitehead 2017 the rivers trust 2020 canhn 2021 epa 2021 trust 2021 us epa 2021 usgs 2021 environment agency 2022 ncdeq 2022 when viewed on a global scale it is clear that these applications are insufficient particularly in terms of morphometric characteristics because they only cover the borders of that country or state use limited data and none exist for obtaining watershed morphometric characteristics on the other hand for end users who focus on watershed and stream morphometric characteristics obtaining these parameters requires computer hardware gis software resources and gis expertise since desktop gis software s big data requirements create problems and don t allow high volume studies they need a gis software or experience and since there is no application that provides basin morphometric characteristics basin borders and streams in the basin on a global scale the development of a new application has become a necessity an alternative to using desktop gis or remote sensing applications with topography from digital elevation data dem and country boundaries is the google earth engine gee among the other capabilities of gee is the inclusion of data sets on river basins on a global scale and there are several applications developed using this environment donchyts 2020 nilsen 2020 scott and shokoufandeh 2020 despite its advantages the gee platform does not seem to be an easy tool for end users with limited programming knowledge the aims of this study are without the need of any desktop gis software dem or any dataset 1 developing an online gee application to obtain morphometric characteristics including stream order classification at different basin levels on a global scale via global hydrobasins hydrorivers and dems within the scope of continent country province and district borders 2 determining administrative units scope of watersheds 3 generating the upstream and downstream flow lines of any stream segment in watershed 4 generating the longitudinal profile of the downstream flow line 5 creating and exporting morphometric characteristics as tables administrative unit boundaries watersheds and all stream data as vectors 2 materials and methods the current study will describe the steps involved in creating the gee application named nku man which focuses on watershed morphometric analysis within the scope of continent country province or district borders and is written in javascript accordingly the main essential steps in the development of the application are the determination of calculation equations and formulas of morphometric characteristics that can be adapted to gee with global watersheds stream datasets and dem s the results of which must meet the requirements and the execution of the application must be simple and obvious to users 2 1 development environment and validation the development environment used in this research is gee a cloud based platform for environmental data analysis and the api allows the user to access the functionality of gee in both javascript and python gorelick et al 2017 gee s cloud based computing infrastructure increases operational efficiency and opens up unique possibilities for global studies including hydrological processes campos taberner et al 2018 the results obtained from the developed application were compared and verified using arcgis desktop esri 2011 with the archhydro plugin 2 2 datasets among the other capabilities of gee is the inclusion of numerous datasets that have been developed by different institutions and can form the basis for the morphometric characteristics of the basin on a global scale while planning this study different public datasets table 1 with different scales that gee has were investigated and included in this application watershed delineation and stream definition within the watershed boundaries depend on the resolution of the dem used in the terrain processing and the threshold area value determined for the stream definition at the global level hydrobasins represents a collection of vectorized polygon layers representing sub basin boundaries at a resolution of 15 arc seconds hydrobasins were derived from the gridded hydrosheds core layers lehner 2014 the hydrorivers v1 0 is a database aiming to provide the vectorized line network of all global rivers that have a catchment area of at least 10 km2 or an average river flow of 0 1 cubic meters per second or both wwf 2021 free flowing rivers were defined to start at every pixel where the accumulated upstream catchment area exceeds 10 km2 grill et al 2019 alos dsm v3 2 is a global digital surface model dsm dataset with a horizontal resolution of approximately 30 m and released in january 2021 gee 2019 hydrosheds s hydrologically conditioned dem with 3s resolution is used for comparing the study results 2 3 developing application 2 3 1 the application graphical user interface gui as already stated the aim of this study was to create an online application that was simple enough for users to use without any prior knowledge of coding or gis software therefore it has become necessary to create a gui interface that addresses the need for coding the conceptual architecture of the application is shown in fig 1 2 3 2 development of javascript functions in the application morphologic analyses are based on four aspects watershed geometric characteristics watershed relief characteristics drainage texture characteristics and drainage network characteristics it has been developed by several functions for visualization watersheds and streams computing watershed morphometric characteristics tracing and monitoring upstream and downstream with linear characteristics of streams in the gee platform these functions can be divided into six groups according to their functionality 1 selecting supported relief datasets 2 selecting aoi 3 determining continental or administrative units watershed geometric and relief characteristics 4 determining the watershed drainage texture drainage network characteristics including stream order classification 5 generating upstream and downstream characteristics and longitudinal graph of downstream 6 exporting functions the results as tables and vectors the calculation equations and formulas used in the morphometric analysis were taken from their original references all characteristics were determined using mathematical operators provided by the gee environment and are now prepared and available for download as tables or vector files 2 3 2 1 selecting supported relief datasets the dem dataset are used for determining relief morphometric characteristics three gee ui select widgets are used for selection of dem flow accumulation acc and flow direction datasets dir in the select supported dataset tab aspect and slope layers are generated with gee s ee terrain slope and ee terrain aspect functions the isolines layer is generated with the getisolines function donchyts 2020 2 3 2 2 selecting area of interest the application focuses on watershed morphometric analysis within the scope of continent country province or district borders gee s ui select widgets are used for selecting continental borders and to investigate other countries provinces or districts respectively large scale international boundary usds 2022 and global administrative unit layers 2015 fao un 2015 used for selecting aoi 2 3 2 3 calculation of geometric and relief characteristics of the watershed the workflow of the calculation of the geometric and relief characteristics of the watershed can be summarised as follows fig 2 watershed geometric characteristics are measurements of the linear features of the watershed or basin the main variables of the watershed hybas id and sub area are taken from hydrobasin feature attributes lehner 2014 the processing and calculation equations and formulas used in the geometric and relief characteristics are given in the tables below tables 2 and 3 watershed relief characteristics are calculated with gee s reduce region function using dem 2 3 2 4 calculating watershed drainage texture and drainage network characteristics the workflow of calculating watershed drainage texture and drainage network characteristics of the stream network can be summarised in fig 3 the calculation equations and formulas used in the watershed drainage texture and drainage network analysis are given in the tables below tables 4 and 5 the crucial stage of morphometric analysis is the classification of stream order the discipline of morphometric research frequently makes use of strahler s ordering method a stream segment that originates from the stream source and has no tributaries is designated as a first order segment in strahler s classification system a second order segment is made by joining two first order segments a third order segment is made by joining 2 s order segments and so on the strahler id field ord stra of each segment in the hydroriver dataset is used to calculate the stream ordering the ratio of the number of stream segments in a given order to the number of stream segments in the next higher order is designated as the bifurcation ratio schumm 1956 only watershed drainage texture characteristics can be calculated in river or stream data sets without the strahler order attribute for these datasets the workflow for calculating the watershed drainage texture of the stream network is 1 5 steps in fig 5 2 3 2 5 generating upstream and downstream characteristics and a longitudinal graph of downstream generating upstream and downstream routines have been developed using the gee functionalities to get upstream and downstream of selected segments of streams these functions are recursive functions appendix a and they have been used with hyriv id next down from the hydrorivers dataset lehner et al 2008 schmidt and jones 2020 and noid ndoid fields from the free following rivers dataset the longitudinal profile is the graph of distance versus elevation and its construction provides an interpretation of surface history as the erosional curves and the river course flow from the upstream to the outlet of the basin at any stage of evolution beg 2016 in the current study a function has been developed for drawing the longitudinal profile based on the downstream flow direction using the dem layer 2 3 2 6 exporting functions the results as tables and vectors the results of application are prepared with gee s getdownloadurl function in different file formats the results of morphometric analysis generated as tables csv border of aoi watershed boundary stream network downstream and upstream segments as vectors kml longitudinal graph as figure csv png svg 3 a case study in meriç river basin to evaluate nku man experiments have been carried out using different continental countries provinces and district borders the gee functions given in the previous section have been implemented to study the morphometric characteristics of the meric river basin the analysis of meric river basin as a sample with wwf level 5 watershed hydrorivers and alos30 dem from the analysed basins will be used for discussion the results of application execution in this research the findings of the morphometric characteristics obtained from the application will be summarised this study focuses on the evaluation of this application 3 1 study area although the application is universal and can work globally when evaluating the results of nku man the meric river basin was used as an example fig 4 the meric maritsa evros is the largest river on the balkan peninsula and it originates in the rila mountains in western bulgaria and marks the border between turkey and greece the meriç river is a transboundary waterway it flows east southeast between the balkans and the rhodopes through plovdiv and dimitrovgrad to edirne in turkey where it empties into the sea the meric river is one of the most important drainage systems of the balkans in southeast europe and is the largest river on the balkan peninsula the meriç river has a basin of 52 600 km2 that stores water and its length is 550 km yildiz 2019 the meriç basin 42 52 40 40 37 20 n 23 35 31 28 14 36 e is located within the borders of three countries bulgaria turkey and greece in the mediterranean basin and is one of the largest transboundary river basins of eastern europe and the balkan peninsula fig 4 intensive farming is widespread in the meric basin the basin is also highly industrialised and densely populated with the largest cities tombul 2015 ozsahin et al 2018 yildiz 2019 3 2 results the interface of the application developed at the end of the study is shown in fig 5 the application gui provides interactive web based interfaces for users to select dem datasets area of interest aoi borders watersheds and river datasets perform morphometric analysis and present results the default settings of nku man are that the active aoi boundary is turkey the active dem layer is alos30 the active watershed boundaries is hydrobasin and the active river layer is hydrorivers the results obtained from the application were verified by comparing the morphometric characteristics obtained from terrain processing with arcgis desktop software and the archydro plugin using the same basin boundaries as hydrologically conditioned dem from hydrosheds core products 3 2 1 selecting supported relief datasets and selecting aoi the default settings of application are used for selecting relief and aoi functions the active dem layer is alos30 and the active aoi boundary is turkey 3 2 2 determination of administrative units calculation of geometric and relief characteristics of watershed generally the river basin or watershed boundaries are different from the country provincial or district boundaries and the neighbouring countries and administrative units within the river basin boundaries should cooperate on the river basin development plans the developed application provides the names of the countries provinces and districts within the boundaries of the basin or watershed boundary the results of administrative units within the borders of the meriç basin were displayed in alphabetical order by countries table 6 there are 20 provinces and 137 districts from turkey bulgaria and greece within the meriç basin the watershed geometric characteristics obtained from the application are given in table 7 the same data was used in the arcgis software to verify the results the geometric characteristics obtained from the application and arcgis software are the same the watershed area represents the amount of water that may be produced in the watershed by rainfall and the length of the stream that drains it kumar et al 2019 the meric basin s area was determined to be 52511 4 km2 the basin area is specified differently in different sources 53475 km2 in özdemir 2015 52600 km2 in tombul 2015 and 53000 km2 in ozsahin et al 2018 the basin length is the distance from the furthest point in the basin to its outlet the meric basin s length was determined to be 629 56 km basin length is critical to predicting the chances of the basin being flooded lower flooding risk in the basin is associated with greater basin length sayl et al 2019 the watershed perimeter is the outside boundary of a watershed that encloses the area and serves as a measurement for the watershed s size and shape lakshminarayana et al 2022 the meric basin s perimeter was found to be 1492 51 km long the shape factor was defined as the ratio of the main stream length to the diameter of the circle having the same area as the watershed the analysis reveals that the meric basin shape factor is 7 548 the lower value shows the higher run off and is circular in shape and the higher value shows low run off and is elongated in shape it also reflects the drainage pattern of the study area the form factor is defined as the ratio of basin area to the square of basin length horton 1945 the value of form factor would always be less than 0 7854 for a perfectly circular basin in the study area it is 0 132 and the meric basin has a low form factor the elongation ratio is defined as the ratio between the diameter of a circle with the same area as the basin and basin length the value of elongation ratio in the study is 0 232 according to pareta and pareta 2011 the meric basin is in more elongated class 0 5 the compactness coefficient is the ratio of watershed perimeter and equivalent circular area circumference of the watershed it relies only on watershed slope and does not depend on watershed size horton 1945 the compactness coefficient value in the study is 1 837 which means that the shape is approximately a rectangle while the value for basin with a perfect circle form is 1 00 the circulatory ratio is defined as the ratio of the basin area to the area of a circle whose circumference is equal to the perimeter of the basin it is influenced by the length and frequency of streams geological structures land use cover climate relief and slope of the basin the circulatory ratio approaching 1 resembles a basin shaped like a circle the circularity ratio in the study area is 0 296 the higher circularity ratio also indicates a larger amount of flow and vice versa arulbalaji and padmalal 2020 stated that the shape parameters like elongation ratio compactness coefficient shape factor circularity ratio and form factor have an inverse relationship with the erodibility relief characteristics indicate the direction of stream water flow and represent the progression of denudation that occurs within the watershed based on the geophysical and topographical conditions of the terrain the results of the execution functions of the watershed relief characteristics compared with different dem s and with arcgis are given in table 8 although there are differences between the relief characteristics obtained with gee and arcgis due to dems and projection systems the results are close to each other the ratio between the basin relief and the basin length is known as the relief ratio the relief ratio of the meric basin is 3 215 higher relief values point to low infiltration and high run off conditions the relative relief is defined as the ratio of the basin relief to the length of the perimeter relative relief is an indicator of the general steepness of a basin from summit to mouth the meric basin has a relative relief of 135 61 the basin relief is the maximum vertical distance from the stream mouth to the highest point on the divide and it is 2024 for meric basin the dissection index is defined as the ratio of relative to absolute relief it indicates the vertical erosion and dissected characteristics of a basin mahala 2020 the dissection index of the meriç basin is found to be 0 703 3 2 3 drainage texture and drainage network characteristics for a better knowledge of watersheds the drainage texture analysis categorises the land surface pattern and its distinct character it generally focuses on the characteristics of geomorphic conditions such as rough or smooth terrain additionally it combines both linear and aerial elements the results are validated with arcgis the linear characteristics such as drainage density bifurcation ratio stream frequency drainage texture and length of overland flow have a direct bearing on the erodibility the greater the erodibility the higher the values arulbalaji and padmalal 2020 the results of the execution functions of the drainage texture characteristics are given in table 9 the length of overland flow is one of the most important dependent variables affecting both the hydrologic and physiographic development of drainage basins horton 1945 the length of overland flow in the meric basin is 2 047 which shows that sheet erosion is the prominent factor for hydrologic and physiographic development the drainage density is the ratio of the total length of all streams in the watershed to the watershed area it helps in determining the permeability and porosity of the watershed low drainage density is generally preferred in areas with high resistance or highly permeable subsoil materials dense vegetation cover and low relief high drainage density is preferred in regions of weak or impermeable surface materials sparse vegetation and mountainous relief the drainage density is governed by the factors like rock type runoff intensity soil type infiltration capacity and percentage of rocky area the drainage density value of the study area is 0 244 according to schumm 1965 texture ratio is an important factor in the drainage morphometric analysis which is dependent on the underlying lithology infiltration capacity and relief aspect of the terrain the texture ratio of the meric basin is 2 047 horton recognised infiltration capacity as the single most important factor that influences drainage texture ratio and considered drainage texture ratio to include drainage density and stream frequency kumar et al 2019 meric basin s drainage texture ratio is 1 703 according to smith s classification method meric basin has a very coarse texture the stream frequency is defined as the number of streams per unit area and it has a low stream frequency of 0 048 the total length of stream orders in km is 12 826 57 and total number of streams is 2542 the constant of channel maintenance means that 4 094 m2 of surfaces in meric basin require maintenance along 1 m of channel the higher value indicates that the channel capacity should be large enough to carry a higher discharge resulting from the bigger drainage area to address the hydrodynamic characteristics of a basin segmentation and hierarchical ordering of streams are required the results of stream orders stream numbers bifurcation ratios and weighted bifurcation ratios obtained from the horton diagram are given in table 10 the results are validated with arcgis the results of the stream order stream number stream length and stream length ratio mean stream length stream length used in the ratio including the weighed mean stream length ratio is shown in table 11 the horton diagram of stream network is shown in fig 6 the relationship between stream order and logarithmic stream number was investigated it appears to be in geometric progression and agrees with horton s law of stream length for the present study this graph was plotted for the meric basin the graph shows a straight line satisfying horton s law kumar et al 2007 it is evident that the correlation coefficient and co efficient of determination for the straight line fit for the watershed are 0 92 which are quite satisfactory according to the study the total length of the stream decreases with increasing stream order 3 2 4 results of downstream and upstream monitoring functions especially when working on a river that has not been explored before where the study area is not fully defined or when conducting a study on a section of a long river determining the source and discharge mouth of this stream may be essential for the planning of the region under consideration tracking the river or each segment of the stream upstream and downstream rather than the entire stream can be useful in partial analysis especially in large river networks the results of downstream analysis of the selected segment in red from the meric river are shown in fig 7 a the longitudinal profile graph along the route of the river downstream to the point where it empties into the sea is shown in fig 7b the longitudinal profile is one of the important sections for morphometric analysis it shows the characteristics of the river or valley along the water flow direction the longitudinal profile represents an important way to characterise average stream slopes and to determine the stage of evolution the length of the chosen segment in the red circle was calculated to be 11 33 km the number of streams nu to be 133 and the sum of stream lengths lu to be 496 17 km in the longitudional graph fig 7b the height is expected to decrease in a sloping manner peaks in the graph that do not correspond to this trend indicate topological errors in river data or pixel errors in dem data the longitudinal profile graph is user interactive and these peak points can be clicked on the graph to examine these segments of the river for comparison the longitudinal profile graph created with a 1 km2 stream definition threshold from local 5m resolution dem data is shown in fig 8 the results upstream of the selected segment in red from the meric river are shown in fig 9 the length of the chosen segment was calculated to be 2 34 km the number of streams nu to be 2542 and the sum of stream lengths lu to be 12826 61 km with the help of this function the upstream flow route of any segment from the meric river to the water sources is shown 3 2 5 the results of exporting functions the results of exporting functions for generating the border of the aoi watershed boundary stream network and downstream and upstream segments as vectors kml are shown in fig 10 the morphometric analysis results are also generated as tables csv and a longitudinal graph csv png svg the exported kml files were opened and viewed in google earth pro as sample gis software 4 discussion morphometric analysis is a quantitative method that helps evaluate the hydrological characteristics for watershed management these characteristics can be used to interpret the geological conditions responsible for the hydrology of the drainage basin morphometric analysis is about exploring the mathematical relationships between different stream attributes used to compare streams and identify factors that may be the cause of differences this is an important factor in planning any watershed development this study provides an important alternative to obtaining these characteristics which are mostly calculated manually using desktop software in a few minutes using gee and dem datasets this study focused on the evaluation of the application developed with gee for watershed morphometric analysis essentially gis software and dem are used for watershed delineation inspecting watersheds and streams while similar applications can only be used for country borders this application can be used for administrative borders worldwide this application with global data sets enables the user to present the watershed boundaries and flows created by the user in an online interactive way that is accessible to everyone also it has been given an option for organisations and users to work with their own rasters shape files as data sets and use it for monitoring basins and streams without using any gis software the nku man allows users to perform watershed morphometric analysis determine the upstream route of stream segments to water resources and determine the downstream route of stream segments flow into the seas all streams from watersheds that empty to the sea or lakes can be determined and the source of the streams can be traced users can also trace downstream to where they empty and upstream to water sources thanks to this interface users have had the opportunity to perform their analysis and share the results without having any specific coding knowledge in order to create a longitudinal profile in gis software the user must manually click each point along the profile to trace the river or valley this process typically takes a long time and is inaccurate the river longitudinal profile graph is considered to be so useful that it can also be used to detect errors in stream vector data the generated aoi watershed boundary stream network and downstream and upstream flow segments can be downloaded in kml format the morphometric analysis findings from the gee application for the meriç basin were validated using arcgis desktop since the same datasets were used in both environments the findings were similar the analysis of the results shows that the application provides a globally simple new approach for calculating morphometric characteristics saves processing time and reduces measurements data sets and efforts the application that allows the morphometric characteristics of any basin in the world to be obtained in a very short period of time despite all these possibilities of gee the biggest problem encountered is the inability to calculate flow characteristics in large basins with more than 5000 segments however this application can be easily used in basins with smaller segment numbers the research shows the importance of using validated global watershed boundaries and global stream datasets for comparing morphometric characteristics it also shows that different global height data used globally can be useful in calculating basin morphometric relief characteristics 5 conclusion there is a strong need for a novel approach particularly for morphometric analysis of watersheds to monitoring watershed borders and streams within watersheds as well as obtaining basin and stream vector data of watersheds on a global scale for researchers students local governments and users worldwide this study introduced a new approach for inspecting watersheds calculating morphometric characteristics and demonstrating the potential of the gee web based tools this significantly increases the data s and application s usability the user can calculate watershed relief and stream morphometric characteristics using different watershed boundaries river or stream datasets and dem rasters and can compare results across the world the gee does not require any additional software installation so it helps overcome compatibility limitations and allows users to access the available codes and data from any computer connected to the internet with this user friendly application morphometric characteristics of watersheds worldwide can be researched on a very large scale even by users with limited gis and rs knowledge and the results can be obtained in various formats for different applications the application can be considered an exemplary environment where different organisations can present their own prepared watershed boundaries and river data to users as a result this application could enable scientists policymakers and the general public to explore basins watersheds rivers streams and any location in the world without using any gis application author statement mehmet şener conceptualization methodology resources supervision validation writing original draft preparation mehmet cengiz arslanoğlu software data curation writing reviewing and editing visualization investigation software availability name of application nku man namık kemal university morphometric analysis for watersheds streams developer mehmet cengiz arslanoğlu first available 2022 source language javascript requirements google earth engine availability the nku man application link and source code is available for free and can be downloaded from https github com mcarslanoglu nkuman declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors wish to thank to mervan savga ahmet berk taş burak akıncı fatih konukçu whose valuable comments helped to improve this manuscript google earth engine for providing cloud computation service usgs and the authors developers of the hydro sheds dataset appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2023 105640 appendix a recursive functions javascript code for determining upstream and downstream segments of stream image 1 
25433,performing large scale simulation analyses using complex process driven models can be very time consuming and incur significant computational expense these analyses involve generating synthetic datasets and include processes such as impacts analysis ia and variance based sensitivity analysis sa machine learning ml provides a potential alternative path to reduce computational costs incurred when generating output from large simulation experiments we assessed the accuracy and computational efficiency of three ml based emulators mles artificial neural networks multivariate adaptive regression splines and random forest algorithms to replicate the outputs of the apsim nextgen chickpea crop model the mles were trained to predict seven outputs of the process driven model all the mles performed well r2 0 95 for predicting outputs for the training data set locations but did not perform well for previously unseen test locations these findings indicate that modellers using process driven models can benefit from using mles for efficient data generation provided suitable training data is provided keywords metamodels surrogates data availability data will be made available on request 1 introduction the agricultural and environmental science disciplines have long utilised the power of computer modelling for scientific enquiry and knowledge advancement jones et al 2016 mechanistic models have been developed for many biological and environmental processes and these models have subsequently been integrated to form whole of system simulation computing environments which are complex and computationally expensive to configure validate and run keating et al 2003 holzworth et al 2014 new developments in computer modelling are often driven by the need for cost reduction and improved efficiencies as these two concepts are integral in the functioning of most modern economies and exist as non negotiable goals for most projects as computing costs have progressively reduced over the past few decades the size and complexity of experiments and analysis based on computer modelling has grown these simulation experiments can require the running of many thousands or even millions of model runs and produce extensive amounts of data e g phelan et al 2018 and casadebaig et al 2016 a reduction in the computational costs of producing large amounts of data is one area that is a target of improved efficiency efforts machine learning ml approaches for predictive modelling are having a significant impact on many areas of society including areas of scientific research not the least of which are agricultural and environmental sciences computational efficiency in producing predicted outcomes is one benefit of ml algorithms balakrishnan and muthukumarasamy 2016 karandish and šimůnek 2016 shastry et al 2016 singh et al 2017 ryan et al 2018 feng et al 2019 niazian and niedbała 2020 much research involving ml technologies revolves around the approaches being able to take diverse data sources such as remote imaging and multiple sensor inputs and predict outcomes such as vegetation type soil water content biomass and crop health shakoor et al 2017 prasad et al 2018 lawes et al 2019 feng et al 2020 obsie et al 2020 zhang et al 2020 fajardo and whelan 2021 guo et al 2021 paudel et al 2021 while the potential computational efficiency gains have received much less attention systems modelling be it for weather environmental or agricultural systems are undertaken using complex process driven models the agricultural production systems simulator apsim nextgen holzworth et al 2018 is one such modelling system in the agricultural and environmental sciences domain while process driven modelling systems like apsim nextgen provide extensive modelling and research opportunities due to their complexity and flexible configuration they are computationally expensive this limits experimental designs where resources are insufficient to run large numbers of simulations e g casadebaig et al 2016 impacts analysis ia and sensitivity analysis sa are two examples of processes that often requires large numbers of simulations to evaluate the interactions between changes in input factor values and the effects these have on target output values while the expectations and requirements to validate models using sa continues to grow razavi et al 2021 the ability to undertake thorough sa of complex systems models is compromised by the limitations imposed by computing power this is just one example of how expanded output from crop models might be used previous studies have utilised ml emulators or meta models as they are also referred to for the computationally efficient expansion of crop modelling outputs for addressing research questions that required analysing very large datasets for example shahhosseini et al 2019 compared four ml algorithms for the prediction of maize yield and nitrate loss and generated a simulated dataset of more than three million data points results varied between which ml algorithm was best for predicting yield extreme gradient boosting algorithm versus nitrate loss random forest while the ideal size of the training dataset and the sensitivity to different input variables also varied between algorithms mandrini et al 2021 used a large synthetic dataset to compare static nitrogen recommendation tools to ml based dynamic recommendation tools the dynamic recommendation tools lacked the accuracy in predictions and were therefore found to be of less usefulness in many situations than the static recommendations there have also been studies which considered the use of emulators to improve the efficiency of performing sa on complex environmental models for example stanfill et al 2015 and ryan et al 2018 both used the statistical approach of generalised additive models to improve computational efficiency of sa applications wallach and thorburn 2017 and sexton et al 2017 discuss the relatively new approach at least in crop modelling research of utilising machine learning based emulators mles to improve computational efficiency in uncertainty analysis these studies highlight the early stage that research into the potential of using ml approaches to improve the computational efficiency of generating expanded synthetic datasets of complex process driven biophysical models is currently at more research is required to assess what range of biophysical modelling scenarios and analyses might benefit from expanded crop modelling output using ml techniques underlying these questions is the issue of whether any particular ml approach is better able to be trained to predict the outputs of complex systems models the objective of this research was to demonstrate that by using input parameters used to configure and run apsim nextgen chickpea crop simulations mles could be developed which are able to predict selected apsim model outputs if this is demonstrated then the use of these mles would allow the substitution of the apsim system model for the specific and limited purpose of generating synthetic data sets with a small and efficient predictive model that is effective for the range of input parameter variations used in the training data set these mles could then be used to generate large synthetic data sets these datasets might be suitable for undertaking a variety of analyses of the underlying modelled relationships analysis of impacts of varying input settings and potentially for aiding in developing hybrid modelling approaches which could open new areas of modelling research a further objective was to test if the mles developed were robust enough to be able to accurately predict crop outputs for all locations within the regions covered by the training data set this required the input parameters used to develop the mles to be diverse enough and contain enough variation in values used to cover the expected ranges of values for all locations of interest these objectives differ from the previous work of mandrini et al 2021 by evaluating mles ability to predict apsim outputs rather than comparing the performance of the two modelling approaches against real world observed data it also varies from the work of shahhosseini et al 2019 by evaluating mles predictive ability across different phases of a crops entire lifecycle and by including metrics to compare the computational costs of developing the mles and their statistical accuracy of predicting apsim outputs to fulfil these objectives the apsim nextgen chickpea model was configured to simulate crop production over a 120 year period at seven locations throughout the chickpea production regions in australia six model outputs were reported and further used to train emulators based on three ml algorithms 1 artificial neural network ann 2 multivariate adaptive regression splines mars and 3 a random forest rf using 24 input factors from the apsim simulations the mles were assessed for predictive accuracy input variable importance and computational effort the assessments of model performances were conducted for the locations for which the mles were trained as well as two additional locations not included in the training data set to test emulator robustness 2 methods three mles representing different ml algorithmic approaches were developed from data generated from apsim simulations of chickpea growth development and yield for seven locations in the australian chickpea production regions the mles were trained on a subset of 80 of the randomised generated data and then validated using the remaining 20 of data a bootstrap process was used to repeat this randomisation and model generation ten times to assess the consistency of the mles developed the workflow of this process is summarised by the flowchart in fig 1 goodness of fit of emulator generated data against the original apsim data for six model outputs were analysed and are presented in the results section the output targets were as follows 1 days from sowing to emergence emergencedas 2 days from sowing to flowering floweringdas 3 days from sowing to first fruiting pod poddingdas 4 days from sowing to crop maturity maturitydas 5 above ground crop biomass at harvest kg ha biomass and 6 weight of harvested grain kg ha grainwt these results cover some of the more significant chickpea model outputs for monitoring and assessing crop growth from emergence to harvest additionally two test locations within the chickpea production area but not included in the original seven locations were used to generate the ml data that was then compared against the apsim generated outputs for further benchmarking purposes 2 1 computing environment all simulations and data analyses were performed on an intel core i7 7600u cpu 2 9 ghz based computer with 16 gb ram running microsoft windows 10 operating system the apsim version used was apsim nextgen version 2020 02 05 4679 holzworth et al 2018 the apsim nextgen prototype chickpea model was used as the crop model built in features of the apsim nextgen user interface were used to configure and execute factorial simulation experiments which generated the data used for building the mles 2 2 machine learning based emulators the mles were developed and run in an installation of r version 4 0 3 2020 10 10 r core team 2020 in microsoft windows the r environment was also used for data preparation and manipulation reporting and graphics generation with the packages ggplot2 version 3 3 3 wickham 2016 and other packages from the tidyverse library version 1 3 0 wickham et al 2019 primarily used for these functions the three mles which are detailed below were nnet representing an ann earth representing a mars implementation and a random forest representing a decision tree implementation 2 2 1 artificial neural network ann artificial neural networks anns are some of the earliest ml algorithms they represent a computing paradigm which consists of a massively interconnected network of nodes acting in parallel which simulate the actions of biological neurons each network connection is characterised by a weighting factor each neuron calculates the sum of its weighted inputs and produces an activation level output value via a generally nonlinear activation function models based on anns are developed by adjusting the number of neurons number of layers of neurons topology neuron characteristics of activation functions and bias and the sensitivities to training responses lippmann 1987 they are typically characterised by the black box phenomena in ml where networks are trained on input data and automatically self calibrate to classify or predict output values the internals of the ml model generally not being able to be observed by a user of the system artificial neural networks have been used to predict outputs such as yield from biological and environmental systems shastry et al 2016 ghimire et al 2018 sanikhani et al 2018 nettleton et al 2019 shahhosseini et al 2021 and were found to be the third most used ml approach in a review of big data applications in agriculture cravero and sepúlveda 2021 in this experiment the standard r library nnet version 7 3 15 2021 01 21 based on the work of venables and ripley 2002 has been used to implement a feed forward neural network with 20 nodes in its hidden layer and utilising 100 iterations for self configuration these settings were established by trial and error as optimal for predictive accuracy the number of nodes was tested over the range of 10 nodes to 40 nodes using increments of 2 nodes the iterations for self configuration were tested over a range of 50 200 in increments of 10 default settings were utilised for all other model parameters the ann algorithm has been included in this study because of its general applicability in environmental and biological studies and its wide use as a baseline for comparative ml studies 2 2 2 multivariate adaptive regression splines mars the multivariate adaptive regression splines mars method for modelling developed by friedman 1991 and further described in friedman and roosen 1995 is a flexible regression modelling approach which has its roots in the recursive partitioning approach used in some forms of regression analysis continuous models with continuous derivatives are generated by repeatedly splitting product regression splines and introducing new basis functions for additional splines this continues until the addition of more splines fails to improve the fitting of the response curves to the sampled data friedman 1991 the method meets the criteria for a ml data analysis tool as the resulting model is automatically determined by the data used to generate the model and does not require additional programming to address the specific problem that the data relates to for this study the earth package version 5 3 0 milborrow 2020 in r was used to implement the mars algorithm the mars algorithm has been included in the ml approaches for this study as it provides an interesting comparison for computational performance and predictive accuracy with the other two pure ml based approaches 2 2 3 random forest rf random forests rf are a computing paradigm based on an ensemble of decision trees a random selection of features is used to split each node with the accuracy of prediction used to weight the strength of each tree the generalisation error for forests reduces as the number of trees increase and correlation between strong individual trees increases random forests have been shown to be quite robust with respect to outlier data points and noise within datasets breiman 2001 sexton and laake 2009 they are one of the most widely used forms of ml frameworks for both classification and regression with cravero and sepúlveda 2021 finding that they are the second most referenced technique for analysis of big data in agriculture there are many examples in agriculture of rf models being used for soil models gebauer et al 2019 hussein et al 2020 yield forecasting kouadio et al 2018 feng et al 2019 2020 obsie et al 2020 guo et al 2021 and analysis of remote sensing belgiu and drăguţ 2016 dahms et al 2016 the rf algorithm has been included in this study because of its wide applicability and use in agricultural and environmental modelling the implementation of the rf algorithm used was the randomforest package version 4 6 14 2018 03 22 liaw and wiener 2018 in the r environment default values were used for all model settings the default settings include that the number of features to be included in each decision tree is p 3 where p is the number of input parameters the default settings also specify that the algorithm calculates via its internal code the number of decision trees that are formed to optimise its predictive accuracy during its learning phase the rf algorithm has been included in this study because of its wide applicability and use in agricultural and environmental modelling 2 3 simulation configuration simulations of chickpea crops were configured in apsim nextgen for seven locations throughout the chickpea growing regions in australia fig 2 for six chickpea cultivars sown on 26 sow dates for each of 120 years 1900 2019 reports were configured in apsim to record all relevant input settings summarise weather details report the days after sowing of key crop development phases and report final above ground biomass and grain yield for each combination of year location cultivar and sowing date one apsim simulation was performed with a total of 131 040 simulations each of those simulations produced one report with summarised output each report is considered one unit of observation in our analysis in the end a large database was obtained where each row was one simulation the input settings and summarised weather details were used as the inputs to train the mles with the crop development times biomass and yield details used as the output targets for training and testing in addition to the seven locations used to train the mles two extra test locations not included in the training and testing data sets were used to test the robustness of the mles for locations outside the development data set 2 3 1 apsim simulation configuration a typical soil type for the area was selected for each location the details of these are shown in table 2 all simulations had plant available soil water reset to 70 capacity on 1st march in each simulation year sowing dates were simulated for each 5 day interval from 30 march until 5 august row spacing was consistent at 0 5 m sowing depth was 50 mm and plant population was 30 plants m2 for northern sites above 32 s and 40 plants m2 for southern sites below 32 s two chickpea genotypes desi and kabuli were sown at each location with three varieties for each genotype seamer hattrick and cica1521 for desi monarch almaz and kalkee for kabuli the genotypes differed from each other in four phenological parameters each defined in terms of thermal time shootlag vegtarget latevegtarget and floweringtarget the cultivar parameters used in apsim are shown in table 1 2 3 2 machine learning emulator inputs the mles were developed and assessed for six output targets of interest for chickpea production emergencedas floweringdas poddingdas maturitydas biomass and grainwt the models were then evaluated using data for seven production locations around australia with additional testing of the mles undertaken using two additional locations which were not included in the training and validation data set input factors table 3 used to train the mles were sourced from the reports generated by apsim nextgen weather details were summarised for each simulation for three blocks of time from the day of sowing 0 30 days 31 60 days and 61 90 days temperatures both maximum and minimum were averaged for each time block while rain and solar radiation were summed to give totals for each time block soil water was represented in two ways firstly a single value of how much plant extractable soil water mm was present at sowing sowingesw was included secondly the soil s water holding capacity measured as the plant available water capacity mm pawcmm and the sowing water content as a fractional value of this fracpawcmm were included in the input parameters these two measures are highly correlated within a soil type but variable between soil types 2 3 3 machine learning emulator targets six apsim nextgen chickpea model outputs were recorded in the apsim reports along with their corresponding input factor values to create observed data sets each of the three ml approaches was assessed on how well an emulator could predict the output values generated by the apsim nextgen simulation as well as assessing the time taken indicating computational effort required to develop each ml emulator this was undertaken on a comparative basis to assess differences between the various approaches 2 4 statistical measures for goodness of fit the goodness of fit between the apsim generated target values and those generated by the mles was assessed using the following statistical measures 1 mean bias mb 2 mean absolute error mae 3 root mean squared error rmse 4 coefficient of determination r2 and 5 coefficient of efficiency coelm also known as legates mccabe index legates and mccabe jr 1999 these metrics were used to compare the ml predicted versus apsim generated value datasets to determine the degree of match between the tested datasets mean bias mb measured in days or kg ha depending on the output 1 m b i 1 n y i x i n mean absolute error mae measured in days or kg ha depending on the output 2 m a e i 1 n y i x i n root mean squared error rmse measured in days or kg ha depending on the output 3 r m s e i 1 n y i x i 2 n coefficient of determination r2 4 r 2 n σ x y σ x σ y n σ x 2 σ x 2 n σ y 2 σ y 2 2 coefficient of efficiency coelm legates mccabe index 5 c o e l m 1 i 1 n y i x i i 1 n x i x in equation 1 through 5 n is the number of pairs of apsim generated x predicted y values where apsim generated values are the model output values generated by apsim and predicted are the ml emulator simulated value for the model output i is the output generated from the ith set of input parameters the six target outputs generated were emergencedas floweringdas poddingdas maturitydas biomass and grainwt 2 5 variable importance the contribution that each input factor table 3 has towards the value of the output target emergencedas floweringdas poddingdas maturitydas biomass or grainwt is calculated by each of the ml algorithms the values reported and presented as a heat map fig 4 have been standardised so that the most significant input is assigned an importance index value of 100 non contributing inputs are given a value of zero 0 and all other inputs are rated with index values proportionate to the most influential input each of these routines was configured to report index values rated on the reduction in the residual sum of squares rss value of generated predictions versus the actual target values when the input parameter being assessed was included in the model that is the input that resulted in the greatest reduction in the rss when it was added to the algorithm was assigned an importance index of 100 3 results 3 1 performance based on training data set results from the training data set where the mles were trained on a random subset of 80 of the data and then validated on the unused 20 of data showed that each of the three ml approaches ann mars and rf algorithms can produce mles with significant predictive accuracy for each of the six crop output targets table 4 there were no observed occurrences of any model encountering overfitting issues which would have been evidenced by the accuracy of the predictions of the validation data set being significantly lower than the accuracy for the training data sets all reported values are those for the validation data sets for each mle the accuracy of prediction the importance of input variables used to achieve these predictions and the computational effort required to develop the mles did vary between the approaches across all outputs the rf emulators showed the best and most consistent accuracy at prediction this however come at significant computational investment 3 1 1 graphical and statistical analysis of ml approaches a visual inspection of the plots of ml predicted versus apsim generated data as hexbin plots fig 3 confirmed the accuracy of the dataset of validation predictions for the six target outputs emergencedas floweringdas poddingdas maturitydas biomass and grainwt the corresponding values from the statistical analyses of the data of these graphs is presented in table 4 of note is the superiority of the rf emulators predictions for each output target all three mles produced exceptional results for predicting the start of flowering floweringdas regional variations are evident for each ml emulator with northern locations flowering after a shorter duration than locations with more southern latitudes cooler climates predictions of podding date were much less precise for each of the mles with noticeably wider variations occurring at mingenew this indicates that some crop growth factor s used within apsim which affected early pod development were possibly not included in the input parameter details one apsim parameter that was omitted and fits this profile is the phenology budding target xypairs values these values represent the budding target response curve measured in thermal time this response relationship was not converted into an equivalent single response numeric suitable of inclusion as an ml predictor variable and so was not included in the input parameter list for developing the mles while producing the most accurate predictions of podding date for most locations most noticeably for horsham the rf emulator s predictions for bongeen were slightly less accuracy than other mles there was no clear indication as to why this was the case the black box nature of ml models makes the analysis of outputs and explanation of model performance challenging the above ground crop biomass and the crop yield reported as grainwt were the least predictable outputs for each ml algorithm the mars emulators on average had the greatest tendency to under predict the output values as indicated by the negative mean bias values table 4 the rf emulators had about one quarter the amount of variance of the other two mles as shown by the mean absolute error mae values table 4 the ann and mars emulators each produced predictions with a wider distribution around the apsim predicted values than the predictions of the rf emulators the data points however are still most densely clustered along the one to one line as fig 3 shows again rf emulators did a noticeably better job of predicting each of these outputs than emulators based on the other ml algorithms further analysis of the least accurate ten percent of predictions for each mle for the outputs biomass and crop yield showed highly variable results between the three mles for the ann emulators the least accurate predictions generally resulted in significant under predictions of biomass and crop yield these results were strongly associated with late maturing crops with a mean maturitydas value of 172 days compared to an average for the rest of the simulations of 148 days a likely cause of such errors is that environmental factors that caused a decrease in the above ground crop biomass and yield in the apsim simulations occurred late in the crop lifecycle with ml weather inputs only recording meteorological data up to 90 days after sowing weather events or dry conditions late in the crop cycle would not have been considered by the ann emulators for the mars emulators the least accurate predictions also tended to result in under prediction of biomass and crop yield but these were not biased towards late maturing crops instead these simulations tended to have drier soil conditions at sowing low sowingesw and lower solar radiation levels later in the crop s life the rf emulators showed a very different pattern again with the least accurate ten percent of predicted biomass and crop yield values generally being associated with over prediction of values for the rf emulators the poor predictions were more strongly associated with elevated soil water at sowing high sowingesw higher than average rainfall beyond 60 days and lower solar radiation during the same period poor prediction of biomass was also associated with earlier sowing dates and small latevegtarget parameter values the black box nature of ml models makes detailed and accurate investigation of underlying model issues impossible in the case of the rf emulators it is worth noting that the outlier values only represent between 20 and 40 data points out of a set of 26 185 data points indicating that any visual impact of these points might have in data plots is overstating their importance this is confirmed by considering the hexbin plot of the distribution density of the data points fig 3 low numbers of data points are seen in a clearer perspective of their importance one interesting aspect to note that differs between the mles is the generation of a small number of erroneous negative values for grainwt rf did not suffer from this feature while the mars emulators showed this feature for both the crop yield and above ground biomass predictions one of the noted strengths of the rf algorithm is bootstrap aggregation also known as bagging which results in an ensemble of rf models this approach has the benefits of reducing bias and variance in the resulting prediction model and producing a more representative outcome for variable data sexton and laake 2009 biau and scornet 2016 a disadvantage of this ensemble approach is the increased computational effort required the pattern of fast emulators being the least accurate in both bias and error statistics calculated as well as the accuracy of predicted target values is observed in the data presented in table 4 this is most likely a reflection of the fact that accurate predictions are more consistently produced when greater numbers of values are processed and averaged there appears to be a generalised inverse relationship between emulator speed and accuracy of prediction 3 1 2 variable importance by comparing the influence that the input factors have on the outputs across each of the mles patterns and variations can be observed in what is driving each emulator fig 4 highlights the patterns of the index values for emergencedas all three mles were strongly influenced by the maximum and minimum temperatures during the first 30 days after sowing this is expected as emergence is primarily a temperature driven response in the chickpea model and it occurs in the first 30 days of the crop simulation the mars algorithm was shown to be significantly more sensitive to the input variable shootlag than were the other two mles for the prediction of emergencedas with the shootlag input having an input variable importance fig 4 of 93 for the mars emulator but values of only nine and six for the other emulators interestingly the ann and mars emulators had consistent values for the accuracy of emergencedas predictions table 4 with r2 values of 0 95 and coelm values of 0 79 which is a clear demonstration that different ml algorithms can use different input information to achieve similarly accurate predictions this finding is consistent with the findings of shahhosseini et al 2019 who also found that ml models differed in their sensitivities to input variables other output targets showed greater diversity in the input variables identified as most important for the ann emulators the input sowingdoy was very significant for predicting the output target floweringdas while the mars and rf emulators rated average maximum temperatures between 31 and 60 days after sowing as highly influential poddingdas and maturitydas showed something of a consist pattern between mles with sowingdoy being most important for the ann emulator while average maximum temperature for 61 to 90 das was the most significant for the mars and rf emulators the rf emulator was the only one to have an additional value over 50 that of avgmint61 90 the patterns of rating significance for both biomass and crop yield were similar in each of the mles above ground biomass and crop yield were both strongly influenced by sowingesw by all ml algorithms although rf emulators used the closely correlated fracpawcmm input instead only the rf emulator rated the latitude lat variable as a significantly important input which it did for both above ground biomass and crop yield both the mars and the rf emulators used the avgmaxt61 90 for crop yield prediction while no temperature rainfall or radiation inputs were rated above an importance of 36 by the ann emulator for crop yield 3 1 3 computational requirements the time taken to train the mles is an indicator of the computational costs associated with developing each emulator system table 5 shows that there was a great spread in the computational requirements needed to develop each type of emulator times ranged from 12 1 s for the mars algorithm to develop a predictive emulator for the output emergencedas to a high of 17 644 8 s 4hrs 54mins for the rf algorithm to produce a predictive emulator for the same output on average mars emulators were developed with least computational effort ann emulators were almost three times more costly and rf emulators were approximately 500 times more costly this observation is based solely on the performance measured for the code libraries and computing environment used for this study 3 2 performance at test locations the mles developed using data from seven locations within the australian chickpea production regions were tested using data from two additional locations also within the same production regions hexbin plots of the apsim generated values plotted against the values generated by the predictive mles are shown in fig 5 with the statistical analyses of the goodness of fit of the data values provided in table 6 for predictions of emergencedas and floweringdas the three ml algorithms ann mars and rf all performed well with consistent r2 values of 0 91 for emergencedas and of 0 98 for floweringdas the corresponding values of coelm ranged between 0 72 and 0 73 for emergencedas and between 0 86 and 0 88 for floweringdas values for each test location were equally well predicted for the three ml emulators ann mars and rf the prediction of maturitydas was the next most accurate output with r2 values of 0 95 and 0 96 and coelm values ranging from 0 72 to 0 82 this was followed by the predictions for poddingdas with r2 values ranging from 0 87 to 0 90 and coelm values ranging from 0 60 to 0 72 all three ml approaches however failed to accurately predict above ground biomass and crop yield at the unseen test locations although the ann models did come close to being acceptable the mles were incapable of making accurate predictions for the test locations based on the data from the training locations given that biomass and crop yield were both strongly influenced by soil water holding capacity and soil water content at sowing fig 4 it is most likely that insufficient soil types and soil water conditions were included in the original data set to allow the test locations to be accurately modelled the test locations effectively fell outside the parameter value ranges and effects observed at the training locations and so predicted values were nonsense this highlights a failing of the input data used not of the mles or the modelling approach 4 discussion 4 1 performance with training data set the focus of this research is on the accuracy of mles to predict apsim generated outputs and the computational costs associated with developing the mles the discussion that follows concentrates on these issues and does not attempt to review or discuss the implications of the agronomic or environmental results which would involve shifting the focus to a review of the apsim nextgen chickpea model itself the results of this study have shown that mles can be developed that can aid in expanding biophysical crop modelling systems such as apsim by providing a computationally efficient approach for the generation of very large synthetic datasets such as are required for ia and variance based sa they show that all three ml approaches reviewed are capable of being used to generate predictive regression mles for the crop model outputs tested the floweringdas prediction was the most accurate output for each of the mles indicating that the input factors included did cover all the important driving variables for this output it is revealing that the importance of the input variables fig 4 panel floweringdas was not consistent between the different algorithms for floweringdas the ann emulator was heavily reliant upon the time of sowing with no other input coming close to having as significant an impact the mars emulator relied almost entirely on mid season maximum temperatures with its next most important input time of sowing rated as only half as important the rf emulator was most strongly influenced by mid and late season maximum temperature this shows clearly that great care must be taken if interpreting the input importance values for mles as being an accurate predictor of the importance of input factors for an underlying model different algorithms can and do predict the correct answer in the majority of instances using significantly different importance weightings of input values boehmke and greenwell 2019 have previously warned that algorithms like that used in the mars approach can give misleading results for variable importance where there are closely correlated input factors this is due to the algorithms approach of selecting input factors based on their contribution to an output value and discarding additional inputs if they do not improve the prediction by some given marginal amount this can result in only one of a closely correlated set of inputs being used to predict output values with the other inputs although equally as influential on the output rated as not used or of low importance breiman 2001 and dumancas and bello 2015 indicate that the rf algorithm is well suited to cope with multico linearity of inputs and so is not subject to this limitation to the same degree as the mars algorithm for neural networks which is represented by the ann algorithm the robustness and accuracy of their predictions have been found to be adversely affected by co linearity between input factors dumancas and bello 2015 samarasinghe 2016 these authors advise that feature selection needs to be undertaken in order to remove non influential inputs and inputs that exhibit co linearity from the data set before reliable neural net models can be built for the purpose of comparing the ml algorithms based on a consistent approach this step was not undertaken in this study the greatest differences between the accuracy of predictions of the mles was for the outputs of above ground biomass and grain weight yield these two outputs are the ones in the output set most influenced by a wide range of crop environmental and management factors and represent the sum of everything the crop has experienced they are key outputs for most crop models stöckle et al 1994 2003 jones et al 2003 keating et al 2003 for these two outputs the rf emulator was clearly a superior predictor than the emulators produced by the other two ml algorithms the reasons for this difference in accuracy are not easily determined contributing factors are likely to include the inherent suitability of the underlying ml algorithm for the data being analysed and the extent to which the data set has been optimised for the ml approach one factor that was identified during analysis of this data was that the summary climate details were only recorded until 90 days after sowing while many of the crops with the poorest predictions of biomass and yield reached maturity as shown by harvest date well beyond this cut off it is probable that adverse weather conditions during the final stages of crop growth and crop maturation resulted in unpredictable crop vigour and yield loss extended periods of weather details in the input parameters may have aided in more accurate predictions of biomass and yield while feature selection and dimensionality reduction steps are warranted for the neural net based algorithms samarasinghe 2016 the purpose of this study was to compare the performance of the core approaches the investigation of optimal feature selection algorithms would constitute a research study in its own right it is worth noting that under the constructs of this study where the outputs of the simulation model are being predicted rather than trying to match real world observations all potential input factors for the mles are known albeit a very large number of them this makes the possibility of identifying a complete set of driving input factors a feasible objective based on the accuracy of predicted values the rf algorithm is the best of the three algorithms tested the accuracy of predicted output values produced by the rf emulators for the locations on which it was trained are good with the lowest accuracy being for both poddingdas and grainwt at r2 0 98 and coelm 0 91 with this level of accuracy the rf emulators could be used to predict with a high degree of confidence any of the six model outputs for any of the seven training locations for input values within the range of values observed in the training set the design of this experiment meant that one set of input factors was tested for their ability to be used to predict each of the six outputs with careful review and iterative testing it should be possible to improve the predictive accuracy for any chosen output the computational costs involved in developing or training the mles table 5 varied widely between the different algorithms one potential application of using mles to efficiently expand the output of process driven crop models is that of running sa studies such as that by zhao et al 2014 which looked at the sa of the apsim wheat model focused considerable effort on identifying a data efficient analysis method to maximise their research outcome and minimise their cost of running apsim simulations the use of mles could provide an alternative method for generating such datasets to be useful as a tool to run ia or sa as a background process on a systems model such as apsim an mle needs to be able to be rapidly developed used and discarded rather than having an iterative development and retention lifecycle this is because each analysis will be based on a different scenario and designed to test different input parameters or different ranges for input parameter values each time they are run as the mles are generated for specific sets of inputs and can only be used to predict outputs for input settings within the value ranges with which they were developed reuse of mles may be limited this would depend upon the design of the experiment at development time even where mles can be reused great care would be required to ensure that the value ranges of all input parameters were within the development limits of the mle thus avoiding covariate shift and that the mix of those inputs was of a pattern that was not dissimilar to patterns used to develop the mles while broadly applicable mles might be possible to produce a narrowly applicable mle developed for a specific application is a safer option if unpredictable outcomes are to be avoided the focus on the expected use and life timeframe of the emulator is a key feature of this study that differs from many studies into the development of ml models comparisons of development times of ml models are not readily available in the literature in this study the mars algorithm was on average almost 500 times faster to train than the rf algorithm with the ann algorithm being approximately 200 times faster than the rf algorithm it must be noted that this represents just one snapshot of specific implementations of three algorithms out of potentially dozens of alternative algorithms the code used to implement the algorithm solution the computing environment utilised to run the code and the computing hardware that the ml was run on all have the potential to significantly affect the outcomes of such a comparison advances in or reimplementation of any of these factors or the selection of alternative algorithms or environments will have effects on the outcomes for this study the outcome is clear the rf algorithm was the most accurate of the ml approaches but it came at a significant computational cost the superior results from the rf emulator are in contrast to kouadio et al 2018 who found an extreme learning machine which is an advanced form of ann algorithm superior at forecasting coffee yield obsie et al 2020 reported an extreme gradient boosting model produced better results than a rf model for blueberry yield prediction although both the gradient boosting model and the rf model performed better than a multi linear regression approach other researchers jeong et al 2016 dayal et al 2019 feng et al 2019 2020 lawes et al 2019 have chosen rf models as their preferred ml approach in studies predicting crop growth 4 2 performance with test locations a second part of this study assessed the robustness of the mle solutions by generating prediction for output values at locations which were not included in the development training and validation data sets the three ml algorithms were not as accurate in predicting the chronological development of the crop that is the emergencedas floweringdas poddingdas and maturitydas as when predicting values for locations in the training data set but predictions were not unrealistic for the ann mars and rf emulators as shown in fig 5 and associated statistical values in table 6 this demonstrates that the mles if developed with sufficiently diverse data sets are robust enough to predict outputs for any location in the production region regardless of whether that location was used in the training data set or not the floweringdas predictions with r2 values 0 98 and coelm values ranging from 0 86 to 0 88 for each of the algorithms were the most accurate of the predictions for the test locations the other statistical measures generated to test the accuracy of the emulators mb mae and rmse all followed the same relative patterns of which was the most to least accurate emulator with rf being the most accurate ann being next and mars being the least accurate with this level of accuracy the use of any of these three mles to predict flowering date as days after sowing for any location within the australian chickpea production regions would be justifiable by using test locations most of the input factors used to train the mles were able to be controlled and ensure that they fell within the ranges used to develop the mles factors that were not controlled and had the potential to fall outside the development dataset boundaries were related to the soil specifically the water holding capacity of the soil and starting soil moisture levels the predictions for above ground biomass and grain weight fig 5 and table 6 are shown to be erroneous for all three ml algorithms as noted previously these outputs reflect the sum of all the factors that influence crop growth consequently their predicted values are most likely to reveal any weakness in the robustness of the mles even though the management and genomic factors were consistent with the training data the test locations introduced different soils to the simulations for example the predictions of biomass and yield for mildura were the least accurate and most varied between the different mles mildura soil was the only sandy loam in the data set and had the lowest water holding capacity of any of the soils this soil was the most contrasting soil and the emulators performed most poorly with it this is consistent with the findings of shahhosseini et al 2021 who identified soil water parameters as key drivers of ml models used to predict corn yields some of the patterns that define the relationships between input factors and output values observed at the test locations in our study were not present in the training data so none of the ml algorithms could predict them for the new locations the situation where input data values fall outside the range of the training dataset is referred to as covariate shift and it is a known limitation of ml that predictive models are unable to handle such data variations this clearly stands as a warning about the potential use of mles for generating synthetic dataset by expanding the outputs of process driven models all patterns of input factors affecting output values must be included in the training data to develop an ml emulator that is capable of robustly predicting outputs other recent research integrating process driven models with ml has focused on the effects of climate change on crop yields feng et al 2019 leng and hall 2020 both studies have reported significant benefits in integrating the two modelling approaches but have not highlighted the dangers and limitations of supplying incomplete data sets to the ml models during development in this study the training data included all required patterns for predicting floweringdas but lacked details which determine above ground biomass and grain weight as a result the floweringdas predictors are more robust than the above ground biomass and yield predictors 5 conclusion this study has shown that emulators of crop models built on ml algorithms can be developed to predict a range of simulated crop outputs the accuracy of predictions varies based the algorithm used and the output being predicted with the rf emulator being the most consistently accurate emulator used in this study computational costs measured as the time taken to train the mles also varied by algorithm the mars emulators were the fastest emulators to be trained in this study with the rf emulators having the longest training times these findings will have implications for the choice of algorithm if this approach of utilising mles were to be used to improve the time efficiency of running very large numbers of model simulations additionally the robustness of the emulator needs to be tested for each output variable there is no set of input factors that will be suitable for predicting all outputs in all situations it is however reasonable to assume that it is possible to develop accurate predictive mles for any output as all input factors for the process driven simulation model are known so it should be possible to generate training data sets with all input factors required for the prediction of the target output a potential disadvantage of the mars algorithm is that it discards input parameters if they are found to be unimportant during its development this could limit its usefulness as a generation tool for datasets intended for ia or sa as parameters of low importance within one scope may become more important if the scope is altered by fixing some of the more influential parameters declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work has been supported by the australian government via a research training place scholarship and by the queensland government via an advance queensland phd scholarship acknowledgment is made to the apsim initiative which takes responsibility for quality assurance and a structured innovation programme for apsim s modelling software which is provided free for research and development use see www apsim info for details the author wishes to thank dr allan peake of csiro toowoomba for providing the base apsim nextgen chickpea simulation configuration and the guidelines for simulating chickpea production in each of the australian chickpea growing regions these guidelines and concepts formed the basis of the apsim experimental design used in this study 
25433,performing large scale simulation analyses using complex process driven models can be very time consuming and incur significant computational expense these analyses involve generating synthetic datasets and include processes such as impacts analysis ia and variance based sensitivity analysis sa machine learning ml provides a potential alternative path to reduce computational costs incurred when generating output from large simulation experiments we assessed the accuracy and computational efficiency of three ml based emulators mles artificial neural networks multivariate adaptive regression splines and random forest algorithms to replicate the outputs of the apsim nextgen chickpea crop model the mles were trained to predict seven outputs of the process driven model all the mles performed well r2 0 95 for predicting outputs for the training data set locations but did not perform well for previously unseen test locations these findings indicate that modellers using process driven models can benefit from using mles for efficient data generation provided suitable training data is provided keywords metamodels surrogates data availability data will be made available on request 1 introduction the agricultural and environmental science disciplines have long utilised the power of computer modelling for scientific enquiry and knowledge advancement jones et al 2016 mechanistic models have been developed for many biological and environmental processes and these models have subsequently been integrated to form whole of system simulation computing environments which are complex and computationally expensive to configure validate and run keating et al 2003 holzworth et al 2014 new developments in computer modelling are often driven by the need for cost reduction and improved efficiencies as these two concepts are integral in the functioning of most modern economies and exist as non negotiable goals for most projects as computing costs have progressively reduced over the past few decades the size and complexity of experiments and analysis based on computer modelling has grown these simulation experiments can require the running of many thousands or even millions of model runs and produce extensive amounts of data e g phelan et al 2018 and casadebaig et al 2016 a reduction in the computational costs of producing large amounts of data is one area that is a target of improved efficiency efforts machine learning ml approaches for predictive modelling are having a significant impact on many areas of society including areas of scientific research not the least of which are agricultural and environmental sciences computational efficiency in producing predicted outcomes is one benefit of ml algorithms balakrishnan and muthukumarasamy 2016 karandish and šimůnek 2016 shastry et al 2016 singh et al 2017 ryan et al 2018 feng et al 2019 niazian and niedbała 2020 much research involving ml technologies revolves around the approaches being able to take diverse data sources such as remote imaging and multiple sensor inputs and predict outcomes such as vegetation type soil water content biomass and crop health shakoor et al 2017 prasad et al 2018 lawes et al 2019 feng et al 2020 obsie et al 2020 zhang et al 2020 fajardo and whelan 2021 guo et al 2021 paudel et al 2021 while the potential computational efficiency gains have received much less attention systems modelling be it for weather environmental or agricultural systems are undertaken using complex process driven models the agricultural production systems simulator apsim nextgen holzworth et al 2018 is one such modelling system in the agricultural and environmental sciences domain while process driven modelling systems like apsim nextgen provide extensive modelling and research opportunities due to their complexity and flexible configuration they are computationally expensive this limits experimental designs where resources are insufficient to run large numbers of simulations e g casadebaig et al 2016 impacts analysis ia and sensitivity analysis sa are two examples of processes that often requires large numbers of simulations to evaluate the interactions between changes in input factor values and the effects these have on target output values while the expectations and requirements to validate models using sa continues to grow razavi et al 2021 the ability to undertake thorough sa of complex systems models is compromised by the limitations imposed by computing power this is just one example of how expanded output from crop models might be used previous studies have utilised ml emulators or meta models as they are also referred to for the computationally efficient expansion of crop modelling outputs for addressing research questions that required analysing very large datasets for example shahhosseini et al 2019 compared four ml algorithms for the prediction of maize yield and nitrate loss and generated a simulated dataset of more than three million data points results varied between which ml algorithm was best for predicting yield extreme gradient boosting algorithm versus nitrate loss random forest while the ideal size of the training dataset and the sensitivity to different input variables also varied between algorithms mandrini et al 2021 used a large synthetic dataset to compare static nitrogen recommendation tools to ml based dynamic recommendation tools the dynamic recommendation tools lacked the accuracy in predictions and were therefore found to be of less usefulness in many situations than the static recommendations there have also been studies which considered the use of emulators to improve the efficiency of performing sa on complex environmental models for example stanfill et al 2015 and ryan et al 2018 both used the statistical approach of generalised additive models to improve computational efficiency of sa applications wallach and thorburn 2017 and sexton et al 2017 discuss the relatively new approach at least in crop modelling research of utilising machine learning based emulators mles to improve computational efficiency in uncertainty analysis these studies highlight the early stage that research into the potential of using ml approaches to improve the computational efficiency of generating expanded synthetic datasets of complex process driven biophysical models is currently at more research is required to assess what range of biophysical modelling scenarios and analyses might benefit from expanded crop modelling output using ml techniques underlying these questions is the issue of whether any particular ml approach is better able to be trained to predict the outputs of complex systems models the objective of this research was to demonstrate that by using input parameters used to configure and run apsim nextgen chickpea crop simulations mles could be developed which are able to predict selected apsim model outputs if this is demonstrated then the use of these mles would allow the substitution of the apsim system model for the specific and limited purpose of generating synthetic data sets with a small and efficient predictive model that is effective for the range of input parameter variations used in the training data set these mles could then be used to generate large synthetic data sets these datasets might be suitable for undertaking a variety of analyses of the underlying modelled relationships analysis of impacts of varying input settings and potentially for aiding in developing hybrid modelling approaches which could open new areas of modelling research a further objective was to test if the mles developed were robust enough to be able to accurately predict crop outputs for all locations within the regions covered by the training data set this required the input parameters used to develop the mles to be diverse enough and contain enough variation in values used to cover the expected ranges of values for all locations of interest these objectives differ from the previous work of mandrini et al 2021 by evaluating mles ability to predict apsim outputs rather than comparing the performance of the two modelling approaches against real world observed data it also varies from the work of shahhosseini et al 2019 by evaluating mles predictive ability across different phases of a crops entire lifecycle and by including metrics to compare the computational costs of developing the mles and their statistical accuracy of predicting apsim outputs to fulfil these objectives the apsim nextgen chickpea model was configured to simulate crop production over a 120 year period at seven locations throughout the chickpea production regions in australia six model outputs were reported and further used to train emulators based on three ml algorithms 1 artificial neural network ann 2 multivariate adaptive regression splines mars and 3 a random forest rf using 24 input factors from the apsim simulations the mles were assessed for predictive accuracy input variable importance and computational effort the assessments of model performances were conducted for the locations for which the mles were trained as well as two additional locations not included in the training data set to test emulator robustness 2 methods three mles representing different ml algorithmic approaches were developed from data generated from apsim simulations of chickpea growth development and yield for seven locations in the australian chickpea production regions the mles were trained on a subset of 80 of the randomised generated data and then validated using the remaining 20 of data a bootstrap process was used to repeat this randomisation and model generation ten times to assess the consistency of the mles developed the workflow of this process is summarised by the flowchart in fig 1 goodness of fit of emulator generated data against the original apsim data for six model outputs were analysed and are presented in the results section the output targets were as follows 1 days from sowing to emergence emergencedas 2 days from sowing to flowering floweringdas 3 days from sowing to first fruiting pod poddingdas 4 days from sowing to crop maturity maturitydas 5 above ground crop biomass at harvest kg ha biomass and 6 weight of harvested grain kg ha grainwt these results cover some of the more significant chickpea model outputs for monitoring and assessing crop growth from emergence to harvest additionally two test locations within the chickpea production area but not included in the original seven locations were used to generate the ml data that was then compared against the apsim generated outputs for further benchmarking purposes 2 1 computing environment all simulations and data analyses were performed on an intel core i7 7600u cpu 2 9 ghz based computer with 16 gb ram running microsoft windows 10 operating system the apsim version used was apsim nextgen version 2020 02 05 4679 holzworth et al 2018 the apsim nextgen prototype chickpea model was used as the crop model built in features of the apsim nextgen user interface were used to configure and execute factorial simulation experiments which generated the data used for building the mles 2 2 machine learning based emulators the mles were developed and run in an installation of r version 4 0 3 2020 10 10 r core team 2020 in microsoft windows the r environment was also used for data preparation and manipulation reporting and graphics generation with the packages ggplot2 version 3 3 3 wickham 2016 and other packages from the tidyverse library version 1 3 0 wickham et al 2019 primarily used for these functions the three mles which are detailed below were nnet representing an ann earth representing a mars implementation and a random forest representing a decision tree implementation 2 2 1 artificial neural network ann artificial neural networks anns are some of the earliest ml algorithms they represent a computing paradigm which consists of a massively interconnected network of nodes acting in parallel which simulate the actions of biological neurons each network connection is characterised by a weighting factor each neuron calculates the sum of its weighted inputs and produces an activation level output value via a generally nonlinear activation function models based on anns are developed by adjusting the number of neurons number of layers of neurons topology neuron characteristics of activation functions and bias and the sensitivities to training responses lippmann 1987 they are typically characterised by the black box phenomena in ml where networks are trained on input data and automatically self calibrate to classify or predict output values the internals of the ml model generally not being able to be observed by a user of the system artificial neural networks have been used to predict outputs such as yield from biological and environmental systems shastry et al 2016 ghimire et al 2018 sanikhani et al 2018 nettleton et al 2019 shahhosseini et al 2021 and were found to be the third most used ml approach in a review of big data applications in agriculture cravero and sepúlveda 2021 in this experiment the standard r library nnet version 7 3 15 2021 01 21 based on the work of venables and ripley 2002 has been used to implement a feed forward neural network with 20 nodes in its hidden layer and utilising 100 iterations for self configuration these settings were established by trial and error as optimal for predictive accuracy the number of nodes was tested over the range of 10 nodes to 40 nodes using increments of 2 nodes the iterations for self configuration were tested over a range of 50 200 in increments of 10 default settings were utilised for all other model parameters the ann algorithm has been included in this study because of its general applicability in environmental and biological studies and its wide use as a baseline for comparative ml studies 2 2 2 multivariate adaptive regression splines mars the multivariate adaptive regression splines mars method for modelling developed by friedman 1991 and further described in friedman and roosen 1995 is a flexible regression modelling approach which has its roots in the recursive partitioning approach used in some forms of regression analysis continuous models with continuous derivatives are generated by repeatedly splitting product regression splines and introducing new basis functions for additional splines this continues until the addition of more splines fails to improve the fitting of the response curves to the sampled data friedman 1991 the method meets the criteria for a ml data analysis tool as the resulting model is automatically determined by the data used to generate the model and does not require additional programming to address the specific problem that the data relates to for this study the earth package version 5 3 0 milborrow 2020 in r was used to implement the mars algorithm the mars algorithm has been included in the ml approaches for this study as it provides an interesting comparison for computational performance and predictive accuracy with the other two pure ml based approaches 2 2 3 random forest rf random forests rf are a computing paradigm based on an ensemble of decision trees a random selection of features is used to split each node with the accuracy of prediction used to weight the strength of each tree the generalisation error for forests reduces as the number of trees increase and correlation between strong individual trees increases random forests have been shown to be quite robust with respect to outlier data points and noise within datasets breiman 2001 sexton and laake 2009 they are one of the most widely used forms of ml frameworks for both classification and regression with cravero and sepúlveda 2021 finding that they are the second most referenced technique for analysis of big data in agriculture there are many examples in agriculture of rf models being used for soil models gebauer et al 2019 hussein et al 2020 yield forecasting kouadio et al 2018 feng et al 2019 2020 obsie et al 2020 guo et al 2021 and analysis of remote sensing belgiu and drăguţ 2016 dahms et al 2016 the rf algorithm has been included in this study because of its wide applicability and use in agricultural and environmental modelling the implementation of the rf algorithm used was the randomforest package version 4 6 14 2018 03 22 liaw and wiener 2018 in the r environment default values were used for all model settings the default settings include that the number of features to be included in each decision tree is p 3 where p is the number of input parameters the default settings also specify that the algorithm calculates via its internal code the number of decision trees that are formed to optimise its predictive accuracy during its learning phase the rf algorithm has been included in this study because of its wide applicability and use in agricultural and environmental modelling 2 3 simulation configuration simulations of chickpea crops were configured in apsim nextgen for seven locations throughout the chickpea growing regions in australia fig 2 for six chickpea cultivars sown on 26 sow dates for each of 120 years 1900 2019 reports were configured in apsim to record all relevant input settings summarise weather details report the days after sowing of key crop development phases and report final above ground biomass and grain yield for each combination of year location cultivar and sowing date one apsim simulation was performed with a total of 131 040 simulations each of those simulations produced one report with summarised output each report is considered one unit of observation in our analysis in the end a large database was obtained where each row was one simulation the input settings and summarised weather details were used as the inputs to train the mles with the crop development times biomass and yield details used as the output targets for training and testing in addition to the seven locations used to train the mles two extra test locations not included in the training and testing data sets were used to test the robustness of the mles for locations outside the development data set 2 3 1 apsim simulation configuration a typical soil type for the area was selected for each location the details of these are shown in table 2 all simulations had plant available soil water reset to 70 capacity on 1st march in each simulation year sowing dates were simulated for each 5 day interval from 30 march until 5 august row spacing was consistent at 0 5 m sowing depth was 50 mm and plant population was 30 plants m2 for northern sites above 32 s and 40 plants m2 for southern sites below 32 s two chickpea genotypes desi and kabuli were sown at each location with three varieties for each genotype seamer hattrick and cica1521 for desi monarch almaz and kalkee for kabuli the genotypes differed from each other in four phenological parameters each defined in terms of thermal time shootlag vegtarget latevegtarget and floweringtarget the cultivar parameters used in apsim are shown in table 1 2 3 2 machine learning emulator inputs the mles were developed and assessed for six output targets of interest for chickpea production emergencedas floweringdas poddingdas maturitydas biomass and grainwt the models were then evaluated using data for seven production locations around australia with additional testing of the mles undertaken using two additional locations which were not included in the training and validation data set input factors table 3 used to train the mles were sourced from the reports generated by apsim nextgen weather details were summarised for each simulation for three blocks of time from the day of sowing 0 30 days 31 60 days and 61 90 days temperatures both maximum and minimum were averaged for each time block while rain and solar radiation were summed to give totals for each time block soil water was represented in two ways firstly a single value of how much plant extractable soil water mm was present at sowing sowingesw was included secondly the soil s water holding capacity measured as the plant available water capacity mm pawcmm and the sowing water content as a fractional value of this fracpawcmm were included in the input parameters these two measures are highly correlated within a soil type but variable between soil types 2 3 3 machine learning emulator targets six apsim nextgen chickpea model outputs were recorded in the apsim reports along with their corresponding input factor values to create observed data sets each of the three ml approaches was assessed on how well an emulator could predict the output values generated by the apsim nextgen simulation as well as assessing the time taken indicating computational effort required to develop each ml emulator this was undertaken on a comparative basis to assess differences between the various approaches 2 4 statistical measures for goodness of fit the goodness of fit between the apsim generated target values and those generated by the mles was assessed using the following statistical measures 1 mean bias mb 2 mean absolute error mae 3 root mean squared error rmse 4 coefficient of determination r2 and 5 coefficient of efficiency coelm also known as legates mccabe index legates and mccabe jr 1999 these metrics were used to compare the ml predicted versus apsim generated value datasets to determine the degree of match between the tested datasets mean bias mb measured in days or kg ha depending on the output 1 m b i 1 n y i x i n mean absolute error mae measured in days or kg ha depending on the output 2 m a e i 1 n y i x i n root mean squared error rmse measured in days or kg ha depending on the output 3 r m s e i 1 n y i x i 2 n coefficient of determination r2 4 r 2 n σ x y σ x σ y n σ x 2 σ x 2 n σ y 2 σ y 2 2 coefficient of efficiency coelm legates mccabe index 5 c o e l m 1 i 1 n y i x i i 1 n x i x in equation 1 through 5 n is the number of pairs of apsim generated x predicted y values where apsim generated values are the model output values generated by apsim and predicted are the ml emulator simulated value for the model output i is the output generated from the ith set of input parameters the six target outputs generated were emergencedas floweringdas poddingdas maturitydas biomass and grainwt 2 5 variable importance the contribution that each input factor table 3 has towards the value of the output target emergencedas floweringdas poddingdas maturitydas biomass or grainwt is calculated by each of the ml algorithms the values reported and presented as a heat map fig 4 have been standardised so that the most significant input is assigned an importance index value of 100 non contributing inputs are given a value of zero 0 and all other inputs are rated with index values proportionate to the most influential input each of these routines was configured to report index values rated on the reduction in the residual sum of squares rss value of generated predictions versus the actual target values when the input parameter being assessed was included in the model that is the input that resulted in the greatest reduction in the rss when it was added to the algorithm was assigned an importance index of 100 3 results 3 1 performance based on training data set results from the training data set where the mles were trained on a random subset of 80 of the data and then validated on the unused 20 of data showed that each of the three ml approaches ann mars and rf algorithms can produce mles with significant predictive accuracy for each of the six crop output targets table 4 there were no observed occurrences of any model encountering overfitting issues which would have been evidenced by the accuracy of the predictions of the validation data set being significantly lower than the accuracy for the training data sets all reported values are those for the validation data sets for each mle the accuracy of prediction the importance of input variables used to achieve these predictions and the computational effort required to develop the mles did vary between the approaches across all outputs the rf emulators showed the best and most consistent accuracy at prediction this however come at significant computational investment 3 1 1 graphical and statistical analysis of ml approaches a visual inspection of the plots of ml predicted versus apsim generated data as hexbin plots fig 3 confirmed the accuracy of the dataset of validation predictions for the six target outputs emergencedas floweringdas poddingdas maturitydas biomass and grainwt the corresponding values from the statistical analyses of the data of these graphs is presented in table 4 of note is the superiority of the rf emulators predictions for each output target all three mles produced exceptional results for predicting the start of flowering floweringdas regional variations are evident for each ml emulator with northern locations flowering after a shorter duration than locations with more southern latitudes cooler climates predictions of podding date were much less precise for each of the mles with noticeably wider variations occurring at mingenew this indicates that some crop growth factor s used within apsim which affected early pod development were possibly not included in the input parameter details one apsim parameter that was omitted and fits this profile is the phenology budding target xypairs values these values represent the budding target response curve measured in thermal time this response relationship was not converted into an equivalent single response numeric suitable of inclusion as an ml predictor variable and so was not included in the input parameter list for developing the mles while producing the most accurate predictions of podding date for most locations most noticeably for horsham the rf emulator s predictions for bongeen were slightly less accuracy than other mles there was no clear indication as to why this was the case the black box nature of ml models makes the analysis of outputs and explanation of model performance challenging the above ground crop biomass and the crop yield reported as grainwt were the least predictable outputs for each ml algorithm the mars emulators on average had the greatest tendency to under predict the output values as indicated by the negative mean bias values table 4 the rf emulators had about one quarter the amount of variance of the other two mles as shown by the mean absolute error mae values table 4 the ann and mars emulators each produced predictions with a wider distribution around the apsim predicted values than the predictions of the rf emulators the data points however are still most densely clustered along the one to one line as fig 3 shows again rf emulators did a noticeably better job of predicting each of these outputs than emulators based on the other ml algorithms further analysis of the least accurate ten percent of predictions for each mle for the outputs biomass and crop yield showed highly variable results between the three mles for the ann emulators the least accurate predictions generally resulted in significant under predictions of biomass and crop yield these results were strongly associated with late maturing crops with a mean maturitydas value of 172 days compared to an average for the rest of the simulations of 148 days a likely cause of such errors is that environmental factors that caused a decrease in the above ground crop biomass and yield in the apsim simulations occurred late in the crop lifecycle with ml weather inputs only recording meteorological data up to 90 days after sowing weather events or dry conditions late in the crop cycle would not have been considered by the ann emulators for the mars emulators the least accurate predictions also tended to result in under prediction of biomass and crop yield but these were not biased towards late maturing crops instead these simulations tended to have drier soil conditions at sowing low sowingesw and lower solar radiation levels later in the crop s life the rf emulators showed a very different pattern again with the least accurate ten percent of predicted biomass and crop yield values generally being associated with over prediction of values for the rf emulators the poor predictions were more strongly associated with elevated soil water at sowing high sowingesw higher than average rainfall beyond 60 days and lower solar radiation during the same period poor prediction of biomass was also associated with earlier sowing dates and small latevegtarget parameter values the black box nature of ml models makes detailed and accurate investigation of underlying model issues impossible in the case of the rf emulators it is worth noting that the outlier values only represent between 20 and 40 data points out of a set of 26 185 data points indicating that any visual impact of these points might have in data plots is overstating their importance this is confirmed by considering the hexbin plot of the distribution density of the data points fig 3 low numbers of data points are seen in a clearer perspective of their importance one interesting aspect to note that differs between the mles is the generation of a small number of erroneous negative values for grainwt rf did not suffer from this feature while the mars emulators showed this feature for both the crop yield and above ground biomass predictions one of the noted strengths of the rf algorithm is bootstrap aggregation also known as bagging which results in an ensemble of rf models this approach has the benefits of reducing bias and variance in the resulting prediction model and producing a more representative outcome for variable data sexton and laake 2009 biau and scornet 2016 a disadvantage of this ensemble approach is the increased computational effort required the pattern of fast emulators being the least accurate in both bias and error statistics calculated as well as the accuracy of predicted target values is observed in the data presented in table 4 this is most likely a reflection of the fact that accurate predictions are more consistently produced when greater numbers of values are processed and averaged there appears to be a generalised inverse relationship between emulator speed and accuracy of prediction 3 1 2 variable importance by comparing the influence that the input factors have on the outputs across each of the mles patterns and variations can be observed in what is driving each emulator fig 4 highlights the patterns of the index values for emergencedas all three mles were strongly influenced by the maximum and minimum temperatures during the first 30 days after sowing this is expected as emergence is primarily a temperature driven response in the chickpea model and it occurs in the first 30 days of the crop simulation the mars algorithm was shown to be significantly more sensitive to the input variable shootlag than were the other two mles for the prediction of emergencedas with the shootlag input having an input variable importance fig 4 of 93 for the mars emulator but values of only nine and six for the other emulators interestingly the ann and mars emulators had consistent values for the accuracy of emergencedas predictions table 4 with r2 values of 0 95 and coelm values of 0 79 which is a clear demonstration that different ml algorithms can use different input information to achieve similarly accurate predictions this finding is consistent with the findings of shahhosseini et al 2019 who also found that ml models differed in their sensitivities to input variables other output targets showed greater diversity in the input variables identified as most important for the ann emulators the input sowingdoy was very significant for predicting the output target floweringdas while the mars and rf emulators rated average maximum temperatures between 31 and 60 days after sowing as highly influential poddingdas and maturitydas showed something of a consist pattern between mles with sowingdoy being most important for the ann emulator while average maximum temperature for 61 to 90 das was the most significant for the mars and rf emulators the rf emulator was the only one to have an additional value over 50 that of avgmint61 90 the patterns of rating significance for both biomass and crop yield were similar in each of the mles above ground biomass and crop yield were both strongly influenced by sowingesw by all ml algorithms although rf emulators used the closely correlated fracpawcmm input instead only the rf emulator rated the latitude lat variable as a significantly important input which it did for both above ground biomass and crop yield both the mars and the rf emulators used the avgmaxt61 90 for crop yield prediction while no temperature rainfall or radiation inputs were rated above an importance of 36 by the ann emulator for crop yield 3 1 3 computational requirements the time taken to train the mles is an indicator of the computational costs associated with developing each emulator system table 5 shows that there was a great spread in the computational requirements needed to develop each type of emulator times ranged from 12 1 s for the mars algorithm to develop a predictive emulator for the output emergencedas to a high of 17 644 8 s 4hrs 54mins for the rf algorithm to produce a predictive emulator for the same output on average mars emulators were developed with least computational effort ann emulators were almost three times more costly and rf emulators were approximately 500 times more costly this observation is based solely on the performance measured for the code libraries and computing environment used for this study 3 2 performance at test locations the mles developed using data from seven locations within the australian chickpea production regions were tested using data from two additional locations also within the same production regions hexbin plots of the apsim generated values plotted against the values generated by the predictive mles are shown in fig 5 with the statistical analyses of the goodness of fit of the data values provided in table 6 for predictions of emergencedas and floweringdas the three ml algorithms ann mars and rf all performed well with consistent r2 values of 0 91 for emergencedas and of 0 98 for floweringdas the corresponding values of coelm ranged between 0 72 and 0 73 for emergencedas and between 0 86 and 0 88 for floweringdas values for each test location were equally well predicted for the three ml emulators ann mars and rf the prediction of maturitydas was the next most accurate output with r2 values of 0 95 and 0 96 and coelm values ranging from 0 72 to 0 82 this was followed by the predictions for poddingdas with r2 values ranging from 0 87 to 0 90 and coelm values ranging from 0 60 to 0 72 all three ml approaches however failed to accurately predict above ground biomass and crop yield at the unseen test locations although the ann models did come close to being acceptable the mles were incapable of making accurate predictions for the test locations based on the data from the training locations given that biomass and crop yield were both strongly influenced by soil water holding capacity and soil water content at sowing fig 4 it is most likely that insufficient soil types and soil water conditions were included in the original data set to allow the test locations to be accurately modelled the test locations effectively fell outside the parameter value ranges and effects observed at the training locations and so predicted values were nonsense this highlights a failing of the input data used not of the mles or the modelling approach 4 discussion 4 1 performance with training data set the focus of this research is on the accuracy of mles to predict apsim generated outputs and the computational costs associated with developing the mles the discussion that follows concentrates on these issues and does not attempt to review or discuss the implications of the agronomic or environmental results which would involve shifting the focus to a review of the apsim nextgen chickpea model itself the results of this study have shown that mles can be developed that can aid in expanding biophysical crop modelling systems such as apsim by providing a computationally efficient approach for the generation of very large synthetic datasets such as are required for ia and variance based sa they show that all three ml approaches reviewed are capable of being used to generate predictive regression mles for the crop model outputs tested the floweringdas prediction was the most accurate output for each of the mles indicating that the input factors included did cover all the important driving variables for this output it is revealing that the importance of the input variables fig 4 panel floweringdas was not consistent between the different algorithms for floweringdas the ann emulator was heavily reliant upon the time of sowing with no other input coming close to having as significant an impact the mars emulator relied almost entirely on mid season maximum temperatures with its next most important input time of sowing rated as only half as important the rf emulator was most strongly influenced by mid and late season maximum temperature this shows clearly that great care must be taken if interpreting the input importance values for mles as being an accurate predictor of the importance of input factors for an underlying model different algorithms can and do predict the correct answer in the majority of instances using significantly different importance weightings of input values boehmke and greenwell 2019 have previously warned that algorithms like that used in the mars approach can give misleading results for variable importance where there are closely correlated input factors this is due to the algorithms approach of selecting input factors based on their contribution to an output value and discarding additional inputs if they do not improve the prediction by some given marginal amount this can result in only one of a closely correlated set of inputs being used to predict output values with the other inputs although equally as influential on the output rated as not used or of low importance breiman 2001 and dumancas and bello 2015 indicate that the rf algorithm is well suited to cope with multico linearity of inputs and so is not subject to this limitation to the same degree as the mars algorithm for neural networks which is represented by the ann algorithm the robustness and accuracy of their predictions have been found to be adversely affected by co linearity between input factors dumancas and bello 2015 samarasinghe 2016 these authors advise that feature selection needs to be undertaken in order to remove non influential inputs and inputs that exhibit co linearity from the data set before reliable neural net models can be built for the purpose of comparing the ml algorithms based on a consistent approach this step was not undertaken in this study the greatest differences between the accuracy of predictions of the mles was for the outputs of above ground biomass and grain weight yield these two outputs are the ones in the output set most influenced by a wide range of crop environmental and management factors and represent the sum of everything the crop has experienced they are key outputs for most crop models stöckle et al 1994 2003 jones et al 2003 keating et al 2003 for these two outputs the rf emulator was clearly a superior predictor than the emulators produced by the other two ml algorithms the reasons for this difference in accuracy are not easily determined contributing factors are likely to include the inherent suitability of the underlying ml algorithm for the data being analysed and the extent to which the data set has been optimised for the ml approach one factor that was identified during analysis of this data was that the summary climate details were only recorded until 90 days after sowing while many of the crops with the poorest predictions of biomass and yield reached maturity as shown by harvest date well beyond this cut off it is probable that adverse weather conditions during the final stages of crop growth and crop maturation resulted in unpredictable crop vigour and yield loss extended periods of weather details in the input parameters may have aided in more accurate predictions of biomass and yield while feature selection and dimensionality reduction steps are warranted for the neural net based algorithms samarasinghe 2016 the purpose of this study was to compare the performance of the core approaches the investigation of optimal feature selection algorithms would constitute a research study in its own right it is worth noting that under the constructs of this study where the outputs of the simulation model are being predicted rather than trying to match real world observations all potential input factors for the mles are known albeit a very large number of them this makes the possibility of identifying a complete set of driving input factors a feasible objective based on the accuracy of predicted values the rf algorithm is the best of the three algorithms tested the accuracy of predicted output values produced by the rf emulators for the locations on which it was trained are good with the lowest accuracy being for both poddingdas and grainwt at r2 0 98 and coelm 0 91 with this level of accuracy the rf emulators could be used to predict with a high degree of confidence any of the six model outputs for any of the seven training locations for input values within the range of values observed in the training set the design of this experiment meant that one set of input factors was tested for their ability to be used to predict each of the six outputs with careful review and iterative testing it should be possible to improve the predictive accuracy for any chosen output the computational costs involved in developing or training the mles table 5 varied widely between the different algorithms one potential application of using mles to efficiently expand the output of process driven crop models is that of running sa studies such as that by zhao et al 2014 which looked at the sa of the apsim wheat model focused considerable effort on identifying a data efficient analysis method to maximise their research outcome and minimise their cost of running apsim simulations the use of mles could provide an alternative method for generating such datasets to be useful as a tool to run ia or sa as a background process on a systems model such as apsim an mle needs to be able to be rapidly developed used and discarded rather than having an iterative development and retention lifecycle this is because each analysis will be based on a different scenario and designed to test different input parameters or different ranges for input parameter values each time they are run as the mles are generated for specific sets of inputs and can only be used to predict outputs for input settings within the value ranges with which they were developed reuse of mles may be limited this would depend upon the design of the experiment at development time even where mles can be reused great care would be required to ensure that the value ranges of all input parameters were within the development limits of the mle thus avoiding covariate shift and that the mix of those inputs was of a pattern that was not dissimilar to patterns used to develop the mles while broadly applicable mles might be possible to produce a narrowly applicable mle developed for a specific application is a safer option if unpredictable outcomes are to be avoided the focus on the expected use and life timeframe of the emulator is a key feature of this study that differs from many studies into the development of ml models comparisons of development times of ml models are not readily available in the literature in this study the mars algorithm was on average almost 500 times faster to train than the rf algorithm with the ann algorithm being approximately 200 times faster than the rf algorithm it must be noted that this represents just one snapshot of specific implementations of three algorithms out of potentially dozens of alternative algorithms the code used to implement the algorithm solution the computing environment utilised to run the code and the computing hardware that the ml was run on all have the potential to significantly affect the outcomes of such a comparison advances in or reimplementation of any of these factors or the selection of alternative algorithms or environments will have effects on the outcomes for this study the outcome is clear the rf algorithm was the most accurate of the ml approaches but it came at a significant computational cost the superior results from the rf emulator are in contrast to kouadio et al 2018 who found an extreme learning machine which is an advanced form of ann algorithm superior at forecasting coffee yield obsie et al 2020 reported an extreme gradient boosting model produced better results than a rf model for blueberry yield prediction although both the gradient boosting model and the rf model performed better than a multi linear regression approach other researchers jeong et al 2016 dayal et al 2019 feng et al 2019 2020 lawes et al 2019 have chosen rf models as their preferred ml approach in studies predicting crop growth 4 2 performance with test locations a second part of this study assessed the robustness of the mle solutions by generating prediction for output values at locations which were not included in the development training and validation data sets the three ml algorithms were not as accurate in predicting the chronological development of the crop that is the emergencedas floweringdas poddingdas and maturitydas as when predicting values for locations in the training data set but predictions were not unrealistic for the ann mars and rf emulators as shown in fig 5 and associated statistical values in table 6 this demonstrates that the mles if developed with sufficiently diverse data sets are robust enough to predict outputs for any location in the production region regardless of whether that location was used in the training data set or not the floweringdas predictions with r2 values 0 98 and coelm values ranging from 0 86 to 0 88 for each of the algorithms were the most accurate of the predictions for the test locations the other statistical measures generated to test the accuracy of the emulators mb mae and rmse all followed the same relative patterns of which was the most to least accurate emulator with rf being the most accurate ann being next and mars being the least accurate with this level of accuracy the use of any of these three mles to predict flowering date as days after sowing for any location within the australian chickpea production regions would be justifiable by using test locations most of the input factors used to train the mles were able to be controlled and ensure that they fell within the ranges used to develop the mles factors that were not controlled and had the potential to fall outside the development dataset boundaries were related to the soil specifically the water holding capacity of the soil and starting soil moisture levels the predictions for above ground biomass and grain weight fig 5 and table 6 are shown to be erroneous for all three ml algorithms as noted previously these outputs reflect the sum of all the factors that influence crop growth consequently their predicted values are most likely to reveal any weakness in the robustness of the mles even though the management and genomic factors were consistent with the training data the test locations introduced different soils to the simulations for example the predictions of biomass and yield for mildura were the least accurate and most varied between the different mles mildura soil was the only sandy loam in the data set and had the lowest water holding capacity of any of the soils this soil was the most contrasting soil and the emulators performed most poorly with it this is consistent with the findings of shahhosseini et al 2021 who identified soil water parameters as key drivers of ml models used to predict corn yields some of the patterns that define the relationships between input factors and output values observed at the test locations in our study were not present in the training data so none of the ml algorithms could predict them for the new locations the situation where input data values fall outside the range of the training dataset is referred to as covariate shift and it is a known limitation of ml that predictive models are unable to handle such data variations this clearly stands as a warning about the potential use of mles for generating synthetic dataset by expanding the outputs of process driven models all patterns of input factors affecting output values must be included in the training data to develop an ml emulator that is capable of robustly predicting outputs other recent research integrating process driven models with ml has focused on the effects of climate change on crop yields feng et al 2019 leng and hall 2020 both studies have reported significant benefits in integrating the two modelling approaches but have not highlighted the dangers and limitations of supplying incomplete data sets to the ml models during development in this study the training data included all required patterns for predicting floweringdas but lacked details which determine above ground biomass and grain weight as a result the floweringdas predictors are more robust than the above ground biomass and yield predictors 5 conclusion this study has shown that emulators of crop models built on ml algorithms can be developed to predict a range of simulated crop outputs the accuracy of predictions varies based the algorithm used and the output being predicted with the rf emulator being the most consistently accurate emulator used in this study computational costs measured as the time taken to train the mles also varied by algorithm the mars emulators were the fastest emulators to be trained in this study with the rf emulators having the longest training times these findings will have implications for the choice of algorithm if this approach of utilising mles were to be used to improve the time efficiency of running very large numbers of model simulations additionally the robustness of the emulator needs to be tested for each output variable there is no set of input factors that will be suitable for predicting all outputs in all situations it is however reasonable to assume that it is possible to develop accurate predictive mles for any output as all input factors for the process driven simulation model are known so it should be possible to generate training data sets with all input factors required for the prediction of the target output a potential disadvantage of the mars algorithm is that it discards input parameters if they are found to be unimportant during its development this could limit its usefulness as a generation tool for datasets intended for ia or sa as parameters of low importance within one scope may become more important if the scope is altered by fixing some of the more influential parameters declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work has been supported by the australian government via a research training place scholarship and by the queensland government via an advance queensland phd scholarship acknowledgment is made to the apsim initiative which takes responsibility for quality assurance and a structured innovation programme for apsim s modelling software which is provided free for research and development use see www apsim info for details the author wishes to thank dr allan peake of csiro toowoomba for providing the base apsim nextgen chickpea simulation configuration and the guidelines for simulating chickpea production in each of the australian chickpea growing regions these guidelines and concepts formed the basis of the apsim experimental design used in this study 
25434,an unstructured hydrodynamic model is presented that is able to simulate 2d nearshore hydrodynamics on the wave group scale a non stationary wave driver with directional spreading with physics similar to xbeach roelvink et al 2009 is linked to an improved and extended version of the existing unstructured flow solver delft3d fm kernkamp et al 2011 martyr koller et al 2017 the model equations are discretised on meshes consisting of triangular and rectangular elements the model allows for coverage of the model domain with locally optimised resolution to accurately resolve the dominant processes yet with a smaller total number of grid cells the model also allows a larger explicit time step compared to structured models with similar functionality the model reliably reproduces measured datasets of water levels sea swell and low frequency wave heights in laboratory and field conditions and is as such widely deployable in a variety of simple and complex coastal settings to study nearshore hydrodynamics keywords numerical modelling nearshore circulation infragravity waves unstructured meshes data availability data will be made available on request software availability the software framework of the model presented in this paper is delft3d flexible mesh under development since 2008 deltares 2021 the source code is available free of charge from deltares https oss deltares nl after registration as a beta test programme participant with software deltares nl the framework is supported under windows and centos7 the computational kernel is written in fortran and the complete compiled code distribution requires 1gb of disk space 1 introduction infragravity ig waves defined as waves with periods of 25 250 s can represent a significant portion of the water level variance in the nearshore these low frequency motions modulate several coastal processes such as reef flat hydrodynamics péquignet et al 2009 pomeroy et al 2012 cheriton et al 2020 péquignet et al 2014 rip current dynamics macmahan et al 2004 reniers et al 2006 2009 wave run up and overtopping cheriton et al 2016 billson et al 2019 sediment resuspension and transport de bakker et al 2016a rosenberger et al 2020b and dune erosion and barrier breaching mccall et al 2010 baumann et al 2017 lashley et al 2019 anarde et al 2020 sources of wave energy in the ig frequency band are linked to the presence of wave groups theory longuet higgins and stewart 1962 hasselmann 1962 and observations kostense 1985 elgar et al 1992 herbers et al 1994 show that in intermediate water depths pairwise non linear difference interactions between sea swell components in a wave group force a ig wave at the group frequency that is 180 out of phase with the wave group envelope upon propagating into shallow water depths these ig waves shoal through resonant triad interactions janssen et al 2003 de bakker et al 2015 transferring energy from the peak frequency toward ig frequencies at a rate dependent on the bed slope and the group frequency battjes et al 2004 van dongeren et al 2007 moreover depth variations on the spatial scales of the wave groups cause a nett radiation stress gradient that forces free ig waves propagating both in onshore and offshore directions moura and baldock 2019 contardo et al 2021 in the outer surf zone ig waves are generated by the time varying breakpoint mechanism symonds et al 1982 contardo et al 2018 on steep normalised bed slopes battjes et al 2004 if normalised bed slopes are mild and short waves and ig waves are in the shallow water regime bound long waves are progressively released from the wave groups masselink 1995 baldock 2012 when these conditions are not met the bound wave degenerates with the breaking wave group and ig generation is minimal baldock 2012 in the inner surf zone triad interactions between ig frequencies enhance ig wave non linearity henderson et al 2006 thomson et al 2006 and can cause ig wave breaking close to the shoreline van dongeren et al 2007 de bakker et al 2016b similar triad interactions can transfer energy back to the short wave frequency band guedes et al 2013 de bakker et al 2016b merging surf bore fronts can add additional energy to ig frequencies sénéchal n dupuis et al 2001 tissier et al 2015 if the bed roughness in the surf zone is high frictional dissipation is a significant ig energy sink péquignet et al 2014 van dongeren et al 2013 upon reflection off the coastline ig waves radiate offshore as leaky waves forming a cross shore standing pattern guza and thornton 1985b or they become refractively trapped as edge waves holman and bowen 1979 herbers et al 1995 given the important role of ig waves in coastal processes substantial efforts were made over the last decades to model their generation propagation and dissipation in various coastal settings van dongeren et al 2003 provide a review of the analytical methods that have been explored to explain the generation of low frequency wave energy more recent semi analytical studies include work on ig dynamics over variable topography to explore phase lags between the short wave forcing and the associated bound long wave e g zou 2011 guérin et al 2019 zhang et al 2020 liao et al 2021 contardo et al 2021 liao et al 2022 and on the generation of free long waves in the surf zone contardo et al 2018 nowadays numerical models are the method of choice to explore ig variability on arbitrary profiles under dissipative conditions list 1992 van leeuwen 1992 and reniers et al 2002 presented linearised models incorporating the ig generation mechanisms of longuet higgins and stewart 1962 and symonds et al 1982 roelvink 1993b and karunarathna and tanimoto 1995 presented 1d versions of a model solving the depth integrated and short wave period averaged non linear conservation equations of mass and momentum on a finite difference grid the wave forcing is provided by a wave group resolving wave model reniers et al 2004 and van dongeren et al 2003 extended the approach to 2dh non linear finite difference models for ig generation and propagation again coupling a wave group resolving wave driver to a non linear shallow water solver this approach was ported to curvilinear grids by roelvink et al 2012 using a finite volume discretisation madsen and sørensen 1997 used a boussinesq type model to simulate ig wave dynamics resolving the short wave motion as well as the long wave motion a similar model paradigm was used by among others karunarathna et al 2005 cienfuegos et al 2010 nwogu and demirbilek 2010 and su et al 2015 the last few years have seen an increase in popularity of non hydrostatic models for the modelling of low frequency wave dynamics models of this type were presented by zijlema et al 2011 ma et al 2012 and de ridder et al 2021 detailed model hindcast studies of infragravity transformations in the surf zone of field sites and laboratory tests using the latter approach were reported by e g rijnsdorp et al 2015 2021 de bakker et al 2016b fiedler et al 2018 lashley et al 2018 and risandi et al 2020 in the present paper we adopt the approach of combining a 2dh non linear shallow water equations solver with a wave group resolving wave driver most models of this class discretise the model equations on a structured rectangular or orthogonal curvilinear grid e g shorecirc shi et al 2003 delft3d reniers et al 2004 xbeach roelvink et al 2009 2012 although this type of grid schematisation provides relative flexibility in efficiently covering the model domain an inherent drawback in the use of structured grids is that refinements needed for output requirements or for handling sharp spatial gradients in bathymetry or transport fields necessarily extend far beyond the part of the domain where a finer resolution is needed this can only be remedied in the structured grid approach by using nested models or through domain decomposition these latter approaches increase the computational cost of a model unnecessarily and in the case where explicit numerical schemes are used will also cause an unfavourable reduction of the maximum allowed timestep in deep areas as the courant friedrich lewy cfl condition courant et al 1967 should be met to retain stable computational results a second important drawback of structured curvilinear grids is the requirement of topological connection which precludes for example the folding and reconnection of a curvilinear grid around islands or atolls unless cyclic boundary conditions are implemented roelvink et al 2013 one approach to avoid these limitations is the use of unstructured grids which are able to provide localised resolution changes without affecting other parts of the grid this flexibility allows for grid adaptations where the grid resolution can now be optimised in function of the spatial scale of the locally dominant physical process to be modelled one example of this would be a relative coarse offshore resolution sufficient to model the propagation of wave groups combined with a finer resolution in regions where short wave dissipation or long wave reflection is important this geometrical flexibility has the added benefit that the grid resolution can also be optimised in function of the explicit time step restriction as grid resolutions ideally are nowhere finer than they need to be to accurately discretise the flow phenomenon under consideration potential reduced accuracy because of the deviation of an unstructured grid from a uniform cartesian grid can be constrained by ensuring grid orthogonality and resolution smoothness and by reducing the use of triangles as much as possible hirsch 2007 unstructured models has gained some popularity over the last decade to simulate mean i e timescales longer than wave groups nearshore circulation e g dietrich et al 2012 zheng et al 2017 wu et al 2011 and morphodynamics e g bertin et al 2009 guérin and dodet 2016 villaret et al 2013 however to date there are no unstructured numerical models that can simulate nearshore hydrodynamics on the wave group scale in this paper we present such an unstructured wave model and we test its ability to reproduce the wave measurements of a number of published datasets in field and laboratory settings in section 2 we describe the formulations and the numerical approach that are used in the wave and flow modules of our model section 3 demonstrates the practical applicability of the model on a range of spatial and temporal scales section 4 contains discussion of the results and the conclusions 2 model formulations the aim of the present model is to simulate the hydrodynamic circulation in the nearshore in response to instationary short wave forcing on the time scale of wave groups the model should be able to simulate bound free and refractively trapped long waves and it should be able to handle run up and overwash in our approach we solve the depth averaged short wave averaged non linear shallow water equations on unstructured grids that can be composed of a combination of triangular and rectangular cells short wave effects are incorporated through radiation stress gradients which drive mean surf zone circulation and low frequency motion on ig timescales moreover short wave induced stokes drift varies on the wave group time and spatial scale lastly bed shear stresses are enhanced by the presence of waves as we do not solve for the individual short waves we need a short wave driver that is forced by directionally spread narrow banded short wave spectra and that solves the time varying wave action balance in combination with a roller model over arbitrary 2d bathymetries 2 1 short wave module 2 1 1 non stationary wave model in order to calculate the propagation and dissipation of organised wave energy in the nearshore we largely follow roelvink et al 2009 the wave module solves the coupled non stationary wave action eq 1 and roller energy balance eq 8 in geographical and directional space taking into account dissipation by wave breaking and spatially varying bottom friction we assume that the incident wave field is narrow banded in frequency so we can work with a single representative wave period taken as t m 1 0 and we neglect absolute frequency shifts in the wave action and roller balance 1 a t c g a c θ a θ d w σ d b f σ where a is the time varying wave action density in geographical and directional space defined as a e w σ e w 1 8 ρ g h r m s 2 is the wave energy density t is time ρ is the water density θ the wave direction in the cartesian reference frame d w is the directionally distributed wave dissipation by breaking d b f is the directionally distributed wave energy dissipation by bottom friction and represents the spatial gradient operator i x j y with i and j the unit vectors in x and y direction respectively wind growth is not taken into account at present although it can be an important energy source on the spatial scale o 1 km at which this type of models is typically used drost et al 2019 the group velocity vector c g with which the wave action propagates in geographical space is equal to 2 c g 1 2 k h s i n h 2 k h σ k k k u l where u l represents the depth averaged flow velocity vector in the lagrangian reference frame h is the total water depth and k the wave number magnitude the refraction speed c θ is given by 3 c θ σ sinh 2 k h h n k k u n the derivative n indicates the gradient orthogonal to the local wave propagation direction θ the wave number vector k is derived from the kinematics conservation equations massel 1989 4 k i t c g j k i x j ω x i 0 i j 1 2 where ω σ k u represents the absolute radial wave frequency the intrinsic frequency σ is derived from the linear dispersion relation σ g k tanh k h wave energy is dissipated through wave breaking and bottom friction a number of wave breaking formulations is available listed in appendix a by default the formulation of roelvink 1993a is being used the total directionally integrated wave dissipation is then given by the product of the time varying breaking dissipation and the fraction of breaking waves 5 d w q b α σ 8 π ρ g h 3 h q b 1 exp h γ tanh k h k n where α is a free parameter of o 1 ρ is the water density g is gravitational acceleration γ is the breaker parameter h is the wave height varying on the timescale of wave groups and n is a shape factor for the wave breaking probability distribution γ tanh k h k represents a measure for the maximum expected wave height for the local water depth h the breaker dissipation is distributed among the directional bins according to the directional distribution of the wave energy we express the time averaged directionally integrated bottom frictional dissipation of wave energy as d b f τ b u rms 1 2 ρ f w u rms 3 where the root mean square orbital velocity magnitude u rms is approximated from the linear wave theory expression for the orbital velocity eq 6 and f w is a parameter of o 0 01 o 0 1 depending on the bottom characteristics booij et al 1999 van dongeren et al 2013 6 u rms h r m s ω 2 2 sinh k h considering the slowly varying dissipation in wave groups we can time average d b f over the representative wave period therefore we can use the monochromatic approximation for the time average of the third even velocity moment u rms 3 eq 7 to calculate d b f guza and thornton 1985a 7 u rms 3 1 20 u rms 2 3 2 0 42 u orb 3 with u orb 2 u rms the resulting bottom dissipation d b f is distributed among the directional bins in a manner analogous to the redistribution of the dissipation resulting from wave breaking processes the lack of a wind source term limits the applicability of the model to domains and applications where local wind growth is of secondary importance 2 1 2 roller model in order to account for the observed spatial lag between the start of depth induced breaking and the development of wave setup and wave related circulation bowen et al 1968 nadaoka and kondoh 1982 the roller model concept roelvink and stive 1989 nairn et al 1991 is used the breaking wave dissipation d w acts as the source term for the roller energy the roller propagates through the surf zone with wave celerity c and following the carrier wave idea where local wave and roller directions are assumed to be equal allows us to reuse the refraction velocity c θ used in the wave action balance eq 1 8 e r t c e r c θ e r θ d w d r where e r is the roller energy c is the wave celerity vector equal to σ k and d r is the directionally distributed roller energy dissipation the magnitude of which is calculated as reniers et al 2004 9 d r 2 g β r e r c in which β r is a parameter of o 0 1 representing the slope of the internal boundary between the roller and the underlying wave motions the resulting wave and roller fields feed into the radiation stress tensor components calculated using linear wave theory reniers et al 2002 10a s x x c g c 1 c o s 2 θ 1 2 e w c o s 2 θ e r d θ 10b s x y c g c c o s θ s i n θ e w c o s θ s i n θ e r d θ 10c s y y c g c 1 s i n 2 θ 1 2 e w s i n 2 θ e r d θ the wave related forcing terms in the depth averaged flow momentum equations eq 22 are then expressed in terms of the radiation stress gradients 11a f w x s x x x s x y y 11b f w y s x y x s y y y 2 1 3 numerical implementation eq 1 and 8 are discretised in geographical space on a staggered orthogonal smooth unstructured grid consisting of combinations of triangles and rectangles a user defined number of bins defines the directional grid spatial advection terms are determined from an explicit higher order discretisation scheme combined with a monotonized central flux limiter for non equidistant grids hou et al 2012 to limit numerical diffusion and to ensure total variation diminishing tvd properties of the scheme deltares 2021 all wave properties are determined in the cell centres collecting the dissipation terms in one variable d eq 1 written in terms of wave energy is discretised as follows 12 e e k i θ n 1 e e k i θ n δ t adv k i θ ref k i θ d k e k e e k i θ 0 where k is the cell number i θ the directional bin n represents the time level indicates the values after the advection and refraction step e e is the wave energy in cell k and directional bin i θ e is the directionally integrated wave energy terms adv k i θ for the spatial advection and ref k i θ for the wave refraction are elaborated on next using gauss theorem the cell centre based advection discretisation adv k i θ in eq 12 is equal to the sum of the face based advection contributions w l adv l i θ and divided by the cell area a k eq 13 13 adv k i θ 1 a k l γ w l adv l i θ where l indicates the face under consideration w l is the face width of l and γ is the cell boundary of cell k face based advection flux adv l i θ is the sum of a first order upwind contribution adv l u p w eq 14 and a higher order flux limited correction term adv l h o eq 16a the first order upwind contribution is equal to 14 adv l u p w c g l i θ ee b i θ c g l i θ 0 c g l i θ ee a i θ c g l i θ 0 0 c g l i θ 0 where c g l i θ is the cell volume weighted wave group velocity in the face normal direction for directional bin i θ calculated from the values in a and b in order to explain the construction of the higher order correction term adv l h o we assume a wave energy flux from cell b to cell a in the following fig 1 an equivalent reasoning holds for a flux from cell a to cell b the construction of the higher order correction term for the face based advection flux adv l i θ happens on a line through the cell circumcentres of cells a and b based on the wave energy in the cells adjacent to cell b these cells are called b u p w 1 and b u p w 2 in fig 1 we aim at constructing the correction at the location of the crossing b c u p w between lines a b and b u p w 1 b u p w 2 note that if either one of these cells does not exist the intersection collapses to the cell centre of b u p w 1 and the approach will reduce to a classical second order upwind correction such as for curvilinear discretisations thus the stencil applied here consists of cells a b b u p w 1 and b u p w 2 based on the values of the wave energy in the latter two cells a value adv b s l is determined at location b c u p w 15a ee b s l i θ sl b u p w 1 ee b u p w 1 i θ sl b u p w 2 ee b u p w 2 i θ 15b sl b u p w 1 d b 1 d b 1 d b 2 15c sl b u p w 2 d b 2 d b 1 d b 2 15d γ b s l dx d upw the higher order correction flux adv l h o is then 16a adv l h o α a max 1 δ t c g l i θ d x 0 ψ r c g a i θ c g b i θ 16b r ee a i θ ee b i θ ee b i θ ee b s l i θ 16c ψ r max min min r α a 1 r 2 1 α a 0 where r is the ratio of the wave energy slopes along line a b c u p w α a is the fraction represented by cell a in the volume weighting and ψ r represents the monotonized central mc difference limiter suppressing wiggles resulting from higher order advection hou et al 2012 the roller energy balance eq 8 is discretised in a similar fashion the refraction term ref k i θ is discretised in directional space with a second order upwind approach as follows 17 ref k i θ reflux k i θ reflux k i θ 1 δ θ 18 reflux k i θ c θ l i θ 1 5 ee k i θ 0 5 ee k i θ 1 c θ l i θ 0 c θ l i θ 1 5 ee k i θ 1 0 5 ee k i θ 2 c θ l i θ 0 0 c θ l i θ 0 where reflux k i θ is the wave energy flux by refraction on the interface between directional bin i θ and i θ 1 and c θ l i θ is the average refraction velocity on the same interface calculated based on eq 3 eqs 1 and 8 are solved with explicit forward euler timestepping the maximum allowed timestep δ t is determined from 19 δ t cfl min n c e l l s a k i θ n θ l γ w l c g l i θ c g l i θ 0 2 1 4 stationary wave model in addition to the non stationary wave model described in the previous section an additional wave driver is included in the code to provide the stationary wave fields used in the single direction approach of roelvink et al 2018 in this approach the stationary wave driver updates the wave field in full directional space and determines the mean wave direction in every flow node the wave groups in the non stationary model are subsequently advected along this mean direction only preserving the groupiness much better and yielding more reliable estimates of the infragravity wave heights for our present purpose the module solves the wave energy balance using an implicit first order upwind scheme with pseudo timestepping and quadrant sweeping to ensure convergence of the solution an outline of the implementation is given in appendix c 2 2 momentum and continuity equations for unsteady flow the short wave averaged wave group resolving depth averaged unsteady flow patterns are described using the non linear shallow water equations in the generalised lagrangian mean framework andrews and mcintyre 1978 nguyen et al 2021 the model equations are solved for the lagrangian velocities u l these wave period averaged water particle velocities are defined as the sum of the eulerian velocities u e the velocity field observed from a fixed point at the bed and the stokes drift u s calculated from linear wave theory 20a u l u e u s 20b u s e w 2 e r ρ h c k k where the wave energy e w the roller energy e r and the wave number vector k are obtained from the wave module we express conservation of mass as 21 ζ t h u l 0 in which ζ is the water level conservation of momentum is expressed as 22 u l t u l u l f u l g ζ ν u l u l t τ w ρ h τ b ρ h f w ρ h where the first term in the left hand side represents inertia the second term advection and the third earth rotational effects the first term in the right hand side of the equation represents the pressure gradient the second term horizontal turbulent mixing the third and fourth term the influence of wind and bottom roughness respectively and the fifth term forcing by waves as calculated by eq 11 f is the coriolis vector ν is breaker induced turbulent eddy viscosity equal to h d r ρ 1 3 battjes 1975 wind shear stress τ w is determined by 23 τ w ρ c d a u 10 u 10 with c d a a wind friction parameter and u 10 the wind velocity vector the bed shear stress τ b is calculated following the parametrisations of soulsby 1997 or ruessink et al 2001 and by using the eulerian velocity definitions u e in the bottom stress formulation the shallow water equations are discretised with a finite volume approach on an orthogonal arakawa b grid resulting in a set of facenormal velocities and cell centre water levels the momentum equations are solved conservatively using the approach of perot 2000 the cartesian advection and diffusion vector components are reconstructed in the cell centres and interpolated back to the cell faces after which they are rotated in face normal direction the advection contribution is discretised using an higher order limited upwind scheme using a similar method as described in section 2 1 3 time integration is done semi implicitly with a predictor corrector scheme first the face normal velocities are calculated explicitly this velocity field is then substituted in the continuity equation to obtain the water levels at the new time level using the θ method the updated velocities are then obtained from back substitution using the new water levels in the pressure gradient term full details of the methodology can be found in kernkamp et al 2011 martyr koller et al 2017 and deltares 2021 the explicit time discretisation to solve the momentum equation leads to a time step restriction where the flow time step is minimised based on the requirement that the ratio between the water volume in a cell and the outgoing discharges be positive within a time step as water levels are solved for implicitly the long wave celerity should not be included in the restriction deltares 2021 the global model time step is then the minimum of the flow and the wave time step derived from eq 19 drying and flooding is taken into account in the spatial discretisation by setting face normal velocities the advection contribution external force terms and the viscous fluxes to zero when the depth on a cell face is smaller than a threshold value in the temporal discretisation the drying flooding check is performed at the beginning of the time step and a second time after the water level equation is solved if the updated water levels are lower than the local bed level the cell is removed from the system of equations and the time step is repeated deltares 2021 2 3 boundary conditions the wave model is forced by spatially varying wave energy time series that are modulated on the timescale of wave groups following the procedure of van dongeren et al 2003 a single summation random phase approach is used whereby at every offshore wave boundary point the sea swell water level signal is derived from the input spectrum for a finite number of discrete frequency components and directions using linear wave theory according to the directional distribution of the spectrum these wave components are integrated over directional space and the absolute value of the hilbert transform of this signal yields the water level envelope this water level envelope is distributed back in directional space according to the directional variance distribution and converted to wave energy the bound long waves associated with the sea swell wave groups are derived from frequency difference interactions between the swell components using the equilibrium theory of hasselmann 1962 and okihiro et al 1992 lateral open boundaries that have no explicit wave signal imposed are of the neumann type causing some disturbances under obliquely incident waves the influence of these disturbances can be mitigated with an appropriate choice of boundary locations on the open flow boundaries an absorbing generating boundary condition is imposed van dongeren and svendsen 1997 that combines the slowly varying tide surge water level with the discharges associated with the bound long waves generated by the wave model the boundary absorbs perpendicularly and obliquely outgoing free waves by locally reconstructing the outgoing characteristics and subtracting them from the incoming signal lateral flow boundaries use the approach of roelvink and walstra 2004 avoiding spurious circulations by combining the specification of longshore water level gradients with switching off cross boundary velocity gradients cross shore velocity gradients along the boundary are unaffected combined with an offshore water level boundary this yields a well posed system of equations to be solved 2 4 model applicability this combination of a non linear circulation model with a linear phase averaged wave driver allows for the propagation of wave groups with their associated bound long waves from offshore to the nearshore short wave refraction and dissipation force wave group scale water motions that can non linearly interact and dissipate on ig frequency scales as we model short waves on the time scale of wave groups and not the individual waves non linear energy transfers between sea swell and ig frequency bands are not accounted for in the short wave energy balance neither can we reproduce processes like short wave bore merging which add to the ig energy content in the inner surf zone van dongeren et al 2007 tissier et al 2015 in the present approach ig waves do however influence the short wave energy propagation and dissipation by modulating the local depths in shallow water where ig wave heights can be of the same order of magnitude as the short waves pomeroy et al 2012 péquignet et al 2014 3 model verification the correctness of the implementation and some of the model s key features are demonstrated in the present section using a number of case studies the theoretical zelt case zelt 1986 özkan haller and kirby 1997 is used to check long wave propagation flooding and drying and the performance of the 2d absorbing generating boundary the boers flume study boers 1996 demonstrates short and long wave transformation over a barred profile extending to 2dh cases the coast3d test case soulsby 2001 demonstrates the capabilities of the model in reproducing non stationary wave fields in realistic field settings the delilah field testcase birkemeier et al 1997 is used to demonstrate the computational advantages of the stationary refraction approach roelvink et al 2018 in combination with unstructured grids the pilot field case clark et al 2020 applies the model to reproduce the hydrodynamics of a fringing reef on the island of guam demonstrating how local grid refinements allow an efficient computation of wave properties in a topographically complex setting this is taken one step further in the final field case which simulates the refraction of a swell field around buck island reef national monument a caribbean island sheltered from incident waves by a reef complex rosenberger et al 2020a 3 1 zelt solitary wave run up any process based surf zone model has to deal with the numerical treatment of a moving shoreline in the present model the shallow water equations are solved on a stationary mesh and the shoreline is defined as the interface of a wet and a dry flow cell whether a cell face is a shoreline is determined from the neighbouring water levels using an upwind reconstruction deltares 2021 the land water interface is moved in discrete steps with an accuracy constrained by the local grid size in order to test the accuracy of the wetting drying implementation for different grid configurations and to check the performance of the absorbing offshore boundary we replicate the tsunami run up model devised by zelt 1986 this testcase simulates the shoreward propagation of a tsunami modelled as a non linear solitary wave over a flat shelf with depth h 0 and width l s into a sinusoidal shaped bay with steep side promontories the tsunami refracts and reflects off the coast and propagates out of the domain the beach slope in the centre of the bay is 1 10 and the promontories have a maximum slope of 1 5 the wave length of the bay is 2 l y the characteristic water depth h 0 was chosen by zelt 1986 as 0 4 l y π if we locate the origin of the coordinate system on the sw corner of the domain with x increasing toward the coast fig 2 the bottom z b is defined by 24 z b h 0 if x l s h 0 0 4 x l s 3 cos π y l y if x l s at the offshore boundary we define the time varying water level ζ i n as 25 ζ i n t α h 0 s e c h 2 3 g 4 h 0 α 1 α t t 0 where α is the non linearity parameter defined as h h 0 h is the characteristic wave height of the tsunami in deep water g is gravitational acceleration and t 0 is a phase shift such that the incoming signal at t 0 is equal to 0 1 h roelvink et al 2009 as we compare our results to özkan haller and kirby 1997 α is set to their value of 0 02 this value was chosen to ensure sufficiently high run up as the characteristic length of the solitary wave and hence the slope it feels scales with 1 α zelt 1986 the model was set up to solve the non linear shallow water equations using three grid versions 1 a rectangular grid with a square net cells with a resolution of 0 125 m aligned with the x axis 2 a rectangular grid with the same resolution rotated counter clockwise by 30 to show the robustness of the solution in a framework not aligned with the coordinate axes 3 an orthogonal triangular grid with a resolution of 0 08 m to minimise shoreline staircase effects the half bay width l y was set to 8 m resulting in h 0 1 019 m and l s l y the bathymetry was then constructed using eq 24 and imposed in the flow nodes of the model grid the absorbing generating boundary was forced with eq 25 taking the first 33 5 s to send the tsunami into the domain after that only reflected signals pass the offshore boundary both lateral boundaries have a no flux condition and thus act as a sidewall the model was run without friction and viscosity following zelt 1986 and özkan haller and kirby 1997 maximum run up time series were recorded using numerical run up gauges on the unstructured grid to that end 5 polylines were added to the model fig 2 and at the start of the run an administration of crossed flow links was created per polyline the waterlevel value along each line for a model time step was then determined as the topographically highest flow link joining a wet and a dry flow node in each polyline s link administration as stated before the outcomes of runs 1 3 were compared with the model results of özkan haller and kirby 1997 fig 3 özkan haller and kirby 1997 used a collocated spectral method where solutions to the shallow water equations are determined on a temporally and spatially adaptive grid using global directionally decoupled polynomial approximations this resulted in a very high resolution in the cross shore direction in a natural way thus getting very accurate estimates for momentary coastlines at every time step fig 3 a shows the vertical run up normalised with the characteristic wave height h in function of normalised time t g h 0 l y runs 1 3 all show similar behaviour demonstrating the effectiveness of the absorbing boundary under different grid configurations the run with the rotated grid run 2 has a slight negative phase shift as compared to the run with the aligned rectangular grid showing a better comparison with özkan haller and kirby 1997 similarly to the structured models of roelvink et al 2009 and hubbard and dodd 2002 the unstructured model is only moderately capable of reproducing the secondary peak in the centre of the bay at t t 4 for profile t100 attributed by zelt 1986 to wave focusing of the headland reflections roelvink et al 2009 cites a staircase shoreline a lack of resolution and grid line oriented spatial derivatives as potential culprits for this diminished performance to test this hypothesis run 3 was included in our analysis the triangular grid has a finer resolution than the rectangular versions and moreover it has no strict directionality along grid lines in the solution process the time series corresponding to this run green line in fig 3 a show a better phasing of the vertical wave run up with the results of özkan haller and kirby 1997 as compared to the runs with rectangular grids this result hints at the interplay between local bed slope and the grid resolution as the main reason for the observed discrepancies the amplitude of the secondary water level peak is however still too small 10 difference in order to assess the grid resolution effect in more detail we performed a grid convergence test following celik et al 2008 therefore three additional runs were made with uniform resolutions of 0 04 0 08 and 0 16 m and the maximum run up value was used to evaluate the performance of the runs the apparent order of grid convergence is 1 1 showing that the discretisation method to determine the shoreline position is slightly more accurate than first order the numerical uncertainty of the maximum run up for the finest grid is about 8 the time series of points located away from the bay centre line show good correspondence with özkan haller and kirby 1997 with run 3 slightly outperforming both runs with rectangular grids table b 3 fig 3 b shows the maximum and minimum normalised vertical run up ζ α h 0 in function of the normalised longshore position y l y all three runs overpredict the run up by about 2 and the error slightly increases toward the edges of the domain all maximum shorelines show a serrated pattern away from the bay centre resulting from staircasing on the relatively steeper slope run 3 reproduces the maximum shoreline position in the central part of the bay the best although its maximum is higher than that of the reference data all runs slightly underpredict the maximum run down by 5 with run 3 performing the worst the staircasing effect is less pronounced for the run down as the bed level gradients are smaller in the deeper areas of the model overall based on the error metrics the reproduction of the run up run down patterns is judged satisfactory for all three model configurations table b 3 and comparable to results from similar modelling approaches hubbard and dodd 2002 roelvink et al 2009 it should be noted that the present case study simulates run up using the non linear flow solver only when combining the flow and the wave solver in a model setup the run up part related to the short waves cannot be modelled as we only resolve the wave group and not the individual waves 3 2 boers flume study the performance of the model in generating suitable wave boundary conditions and in simulating wave driven flow over a barred profile is tested by comparing the numerical results with data measured by boers 1996 although the case does not leverage the specific benefits of using unstructured grids it demonstrates the correct implementation of the model equations boers 1996 reproduced the conditions of the lip11d tests reported in sánchez arcilla a roelvink et al 1994 on a fixed bed in the large wave flume at delft university of technology the test conditions were froude scaled with a factor of 5 6 relative to the lip11d experiment the wave flume is 40 m long and 0 8 m wide and has a maximum depth of 1 05 m the wave generating paddle board is of the hydraulically driven piston type with second order steering and active reflection compensation to remove free reflected waves from the flume during the run the model bed was constructed from sand topped by a concrete layer to smoothen the surface and mimicked a nearshore profile with a single bar and a surf zone trough fig 4 the spatial axis of the measurements has its origin at the toe of the profile slope 4 5 m from the wave generator boundary time series were constructed by repeating a short cycle with a length of 75 times t p and applying a f p dependent high pass filter to compensate for paddle board artefacts by repeated experiments using the same boundary conditions detailed hydrodynamic measurements were made at 70 locations throughout the flume at 20 hz the experimental parameters represent different wave steepnesses related to the original goal of the lip11d tests to measure during erosive and accretive conditions during tests 1 a and 1b waves were breaking throughout the flume during test 1c the incident short waves shoaled before breaking on the bar during tests 1b and 1c low frequency lf energy increased steadily toward the shoreline test 1 a shows shoaling of the low frequency waves just before the bar and a decrease of their amplitude in the trough the measured values for h s and t p at the toe of the profile and the cycle duration per condition are listed in table 1 the numerical model was set up with a mesh of 3 by 317 square grid cells the grid resolution is 0 1 m so the model effectively covers the flume from the toe of the profile slope x 0 m to the end of the constructed profile x 31 7 m the measured bathymetry is imposed in the cell centres this bathymetry was equal for all runs on the offshore boundary corresponding to x 0 m in the flume a water level of 0 0 m was imposed the maximum still water depth in the flume was 0 75 m time series for the short waves and the associated bound long wave were derived in two ways firstly second order time series were constructed from a pierson moskowitz spectrum using the parameters listed in table 1 following the methodology outlined in section 2 3 a standard jonswap peakedness factor γ 3 3 was judged to be too large when comparing theoretical spectral shapes to the measured spectra at the toe of the profile boers 1996 secondly first order measured time series were imposed on the model boundary the associated bound long wave is computed in the model using the formulations of longuet higgins and stewart 1964 the bed shear stress was imposed using a chézy parameter of 75 that was uniform over the length of the flume the model was run for 30 minutes and output wave height h and water levels ζ were recorded with a frequency of 20 hz similar to the flume tests data acquisition from these time series the following metrics were derived setup ζ h m 0 h f h h m 0 l f 8 ζ f ζ f 2 where ζ f is the low pass filtered f p 2 f p 20 water level and h is the modelled significant wave height denotes time averaging over the run duration model results for the measured m and spectral s input wave time series are compared with the data as measured by boers 1996 in test 1 a the setup is overpredicted by the same amount by both runs fig 5 a both m and s runs capture the shoaling and subsequent dissipation of the lf wave in the surf zone fig 5 c in test 1b the m run overpredicts the incident band wave height in contrast the s run is able to reproduce the linear decrease in the zone seaward of the breaker bar fig 5 e both m and s runs underpredict the lf wave height fig 5 f in test 1c both m and s runs reproduce the setup and the short wave height accurately fig 5 g h the s run slightly overpredicts the lf wave height compared to the m results fig 5 i overall the model is capable of reliably reproducing the wave height and setup patterns table b 2 for both measured and spectral input wave time series the results are somewhat less precise for the steeper wave conditions characteristic of test 1b 3 3 delilah field experiment in this section the focus is on the 2d propagation and dissipation of directionally spread short waves and the associated forcing of surf zone circulation more specifically the implementation of the stationary refraction single direction approach of roelvink et al 2018 will be tested this approach was designed to counteract the disintegration of modelled wave groups when they are advected over longer distances in our code this is done by combining the results of two separate unstructured solvers the stationary wave module feeds the mean wave direction to the non stationary short wave driver which advects the actual wave groups section 2 1 4 to this end we constructed two models one where we propagate the wave groups using multiple directional bins mdir and one where we propagate the wave groups along the mean wave direction only sdir we use data of the delilah field experiment held in october 1990 in duck north carolina at the us army field research facility birkemeier et al 1997 for verification similarly to previous model validation studies van dongeren et al 2003 roelvink et al 2009 2018 the data run of october 13th 1990 from 16 00 to 17 00 was selected the windless swell conditions were the result of hurricane lili and were characterised by a significant wave height h s of 1 81 m and a peak period t p of 10 6 s in 8 m water depth these conditions are energetic enough to generate a significant contribution to low frequency portion of the energy spectrum f p 2 and the incident wave spectrum is sufficiently narrow banded to justify the assumptions in the generation of the wave boundary conditions fig 6 b section 2 3 the incident wave angle was 88 from n corresponding to 16 from shore normal the local wind conditions were very mild with a wind speed of 2 m s from the ne birkemeier et al 1997 3 3 1 model setup the model is set up using an unstructured grid with dimensions of 885 m cross shore by 700 m longshore the overall resolution of the grid is 10 m by 10 m with a local refinement to 5 m in the longshore and cross shore directions from the edge of the surf zone seaward of station 90 to the coastline the refinement area covers a longshore distance of 260 m centred on the measurement transect the bathymetry in the model is composed of data measured on october 12th and 13th as the october 13th survey only covered the bathymetry between the dry beach and the surf zone trough birkemeier et al 1997 the bed level data was interpolated in the flow nodes and the bathymetry was made uniform in the first three rows of cells along the offshore boundary in order to satisfy the conditions for the validity of the equilibrium theory of hasselmann 1962 a stationary offshore water level of 0 69 m ngvd was imposed on the offshore boundary the 2d absorbing generating boundary was forced with times series generated from the measured spectrum at 8 7 m water depth the directional spreading factor s of the spectrum is about 6 roelvink et al 2018 the lateral flow boundaries were forced with zero gradient neumann conditions we used the roelvink daly breaker formulation daly et al 2012 to calculate energy dissipation through wave breaking this was shown by roelvink et al 2018 to substantially improve the cross shore variation of the short wave heights and the longshore current in this test case the breaker parameter γ was set to 0 52 and a chézy bed roughness value of 65 m 0 5 s was used all other parameters were set to their default values the multiple direction run mdir fig 7 was run in standard mode with a directional resolution of 5 the single direction run sdir was set up with that same directional resolution for the stationary wave model mean wave directions obtained from the stationary calculation were fed to the surfbeat module every 10 min time series of wave characteristics and water levels were recorded at 1 hz in the model across a dense cross shore profile in order to obtain results that are compatible with the delilah dataset the demeaned water levels ζ were bandpass filtered to obtain the low frequency signal ζ l f 0 004 0 0495 hz and the high frequency signal ζ h f 0 0495 0 3 hz the corresponding rms lf wave height was then obtained by h r m s l f 8 ζ l f 2 the rms short wave height h r m s h f as h m o d e l l e d 2 8 ζ h f 2 and the longshore velocity as v with v the cell centre y component of the eulerian velocity vector as our grid is aligned with the coastline no further rotation is needed 3 3 2 results the groupiness factor gf list 1991 was calculated for the sdir fig 7 a run and the mdir fig 7 b run the groupiness factor gf is defined as 26 g f 2 σ 2 h r m s h f h r m s h f where σ denotes the standard deviation clearly the single direction run is able to maintain the wave group structure much better than the multiple directions run the wave group structure is advected with less numerical dissipation up until the edge of the surf zone with improved surf zone lf wave heights as a result fig 8 c the cross shore evolution of the hf wave height the lf wave height and the longshore current velocity obtained from the two runs were compared to the measured data fig 8 both the mdir and the sdir run capture the short wave height distribution and dissipation in the surf zone well although the shoaling process seaward of the sandbar is underestimated by both models moreover neither model captures the wave energy dissipation between stations 20 and 10 fig 8 b the lf wave height is simulated well using both sdir and mdir approaches compared to previous results of roelvink et al 2009 2018 in xbeach the sdir run reproduces the lf wave height distribution in the surf zone better only the shoreline value station 10 is overestimated but the overall pattern and magnitude match well with the data fig 8 c this finding is corroborated by the low frequency spectrum plots in fig 9 the lf energy modelled in the sdir and mdir runs generally corresponds well to the measurements except in stations 10 and 70 where the lf energy is overestimated around the peak ig frequency the peak longshore velocity is underestimated by both runs by about 30 table b 4 the longshore current distribution and magnitude proved to be extremely sensitive to the spatial and directional grid resolution of the model when using comparable settings as roelvink et al 2018 the model underpredicted the current velocity magnitude with an error of 75 overall the model is reproducing the hf and lf wave patterns well with the standard settings for the bottom roughness chézy value of 65 m 0 5 s that we have used here table b 4 regarding performance the use of unstructured grids allowed a reduction of the number of grid cells with 65 compared to a run on full 5 by 5 m resolution reducing the runtime with a factor 2 2 for the mdir setup using the stationary refraction approach this improved further to a factor 5 9 with the added benefit of better preservation of the wave group structure and slightly better lf wave height prediction in the surf zone 3 4 coast3d egmond field experiment the coast3d egmond field case aims at reproducing the hydrodynamics during a storm event in a double barred nearshore area when local ig wave generation was important the beach experiments in egmond aan zee were part of a multi institution set of field campaigns in the netherlands and the uk in 1998 1999 within the framework of the eu mast iii funded coast3d project soulsby 2001 the aim of the project was to collect a comprehensive dataset of hydrodynamic and morphological observation that would allow to test the accuracy of morphodynamic models under field conditions the meso tidal beach and nearshore at egmond feature an intertidal swash bar and two offshore breaker bars that are intersected by rip channels in this test case we use data from the main campaign in october november 1998 during this field experiment an large array consisting of 32 stations intermittently measured waves currents water level and bed dynamics throughout a number of storms ruessink 1999 breaker bar movement was tracked 22 times during the field campaign up until nap 6 m with dgps positions of a 15 m high amphibious buggy called wesp that drove cross shore tracks at alongshore intervals of about 50 m the deeper parts of the nearshore were measured once before the main campaign on september 1st 1998 using a hydrographic vessel of rijkswaterstaat klein et al 2001 hydrodynamic data is available from an array of pressure sensors located on the inner breaker bar stations 1a 1d 7b 7e and a pressure sensor on the outer bar station 2 fig 11 a the deep water wave conditions during the field experiment were measured by a directional waverider buoy station 8 located in 15 m water depth about 5 km offshore of the field site comparison with nearby operational wave measurement locations of rijkswaterstaat showed insignificant wave energy dissipation between stations deeper than 30 m and the location of station 8 and any gaps in the station 8 time series were reconstructed using these ancillary wave data vermetten et al 2001 tide levels at the field site were derived from the tide gauges at ijmuiden and den helder ruessink et al 2001 the selected depth short and infragravity wave data were recorded on october 25th 1998 between 01 00 and 13 00 gmt and cover one tidal cycle fig 10 this time frame corresponds to the first storm peak during the measurement campaign tidal water levels vary between 0 4 and 1 75 m nap the significant wave height at station 8 varied between 3 0 and 4 2 m wave peak period were between 8 7 and 9 8 s notably over the course of the tide the incident wave angle shifted from sw to nw the last available wesp bed level measurements before the storm were recorded on october 24th 1998 and were used in the model setup 3 4 1 model setup the model grid is a progressively refined unstructured grid of 2500 m in the longshore and 1600 m in the cross shore direction aligned with the local coastline orientation the grid resolution at the offshore boundary is 20 m onshore of the 8 m contour and toward the centre of the domain the resolution is 10 m and onshore of 6 m around the observation stations the resolution is further reduced to 5 m fig 11 this choice of resolution allows to accurately capture the wave group propagation and dissipation processes resulting in a grid of 32286 cells the model was set up in multi directional mode with a directional resolution of 5 to compensate for the difference in cross and longshore bed level data availability and to avoid triangulation artefacts the wesp track data of october 24th were converted to a bed level sample set using the methodology of thanh et al 2020 and subsequently interpolated on the unstructured grid cell corners bathymetry in areas not covered by the wesp measurements was interpolated from the offshore bed level data of september 1st 1998 and from the regular monitoring dataset of the dutch government the so called vaklodingen the boundary conditions were taken from vermetten et al 2001 wave boundary conditions were imposed as a time series of hourly parametrised jonswap spectra with a peakedness and cosine spreading factor of 2 4 and 4 respectively the wave buoy at station 8 malfunctioned after october 25th 2 00 and the remainder of the time series of wave bulk parameters was derived from the operational buoy network data of rijkswaterstaat by wave height class dependent interpolation vermetten et al 2001 the offshore model boundary is a 2d absorbing generating boundary forced with the tidal water level and the ig volume fluxes by calibration on the data of october 25th 1998 09 00 table b 5 the bed roughness was fixed at a global chézy roughness parameter of 65 m 0 5 s we applied the daly et al 2012 wave breaking formulation with a gamma value of 0 48 and alpha 1 1 the roller model was switched on with the roller slope set to 0 1 wave current interaction was switched off any other parameters were left to their defaults the model was then run on 6 parallel partitions for the 13 h model validation period model results are extracted as time series at the instrument locations where pressure data was available for the entire duration of the model run modelled data were saved at a frequency of 2 hz and hydrodynamic parameters are calculated from 1800 s bursts over the frequency bands reported in ruessink 1999 in order to match the field data the demeaned water levels ζ were bandpass filtered to separate the low frequency signal ζ l f 0 004 0 05 hz from the high frequency signal ζ h f 0 05 0 33 hz the corresponding rms lf wave height was then obtained by h r m s l f 8 ζ l f 2 the rms short wave height h r m s h f as h m o d e l l e d 2 8 ζ h f 2 3 4 2 results water depth and h r m s h f are well reproduced by the model in all the observation points except in station 7b fig 12 cross shore wave dissipation patterns are well modelled over the course of the tidal cycle the rms error in the estimates of both parameters is smaller than 10 of the observed value sci fig 12 a n both water depth and hf wave height are systematically overestimated all model errors are within the bounds of the measurement errors estimated at 0 05 0 15 m for the water depths and 15 for the hf wave heights in breaking wave conditions van rijn et al 2000 moreover the water depth bias is in the order of the measurement uncertainty of the bottom level data as reported by van rijn et al 2000 in station 7b the water depth is on average overestimated by 0 22 m and the wave height by 0 3 m the position of station 7b is aligned with a rip channel through the inner breaker bar which may explain part of the discrepancies between model and data the nature of the available bed level data does not allow to accurately resolve the alongshore bed level variability required to reproduce the rip channel geometry so the model bathymetry may be too deep at that location the larger water depth then allows for larger wave heights under breaking wave conditions this however would only allow an overestimation of around 0 1 m given the maximum permissible wave height over water depth ratio of 0 48 we likely underestimate the wave breaking on the outer bar onshore of station 2 yielding larger wave heights in the trough between the outer and the inner bar further evidence in this direction is that according to the data waves in station 7b are never breaking h r m s h f h 0 35 throughout the tide while the waves in the model do break h r m s h f h 0 45 throughout the tide however we have no data on the outer bar top to verify this although station 7b is close to a rip channel the lack of wave current interaction in our model does not play a role in the overestimation of the wave height as that would yield an opposite effect weir et al 2011 the comparison between modelled and measured h r m s l f values shows substantial scatter but the model error is generally expected to deviate 20 or less from the observed values sci fig 12 o u error metrics for the lf wave height in station 7b show values that are comparable with those of the other observation stations in order to explain part of this scatter we inspected the spectra of stations 2 and 1a d on october 25th 1998 07 00 fig 13 these spectra show that the ig peak period is predicted reasonably well and that the ig energy around the ig spectral peak is overpredicted the model however underpredicts the energy content for ig frequencies higher than 0 025 hz a factor playing a role in this underprediction is the absence of free long waves in the boundary forcing a bispectral analysis of the water level data in station 2 for this burst and using the method of herbers et al 1994 their eq 5 and 6 reveals that approximately 49 of the total ig energy content was directly forced by wave groups the remaining 51 is free ig energy that originates from outside the model domain or from local reflection off the coastline overall this field case demonstrates that the model can reliably reproduce the hydrodynamic conditions in a multi bar system during an energetic wave event water depths and short wave fields are simulated to within measurement accuracy while the ig wave height is mostly underpredicted due to the lack of a mechanism to transfer energy between hf and lf bands in the shoaling and outer surf zone and because of the missing free long wave contribution in the boundary forcing regarding efficiency a comparable xbeach model with the same 5 m resolution over the surf zone has a grid size of 51712 cells factor 1 6 higher and ran with a timestep of 0 11 s 45 smaller for the same cfl value of 0 7 3 5 pilot field experiment in this verification case we have tested the parallel implementation of the surfbeat model on the 2d field case of a fringing reef in guam which is a challenging study area featuring longshore bathymetry variability with very steep gradients a narrow surf zone and large bottom roughness the pilot field experiment is a long term deployment 2005 present of a number of measurement arrays of pressure sensors and current meters in varying reef environments one of the field sites is at ipan on the se coast of guam péquignet et al 2009 vetter et al 2010 péquignet et al 2014 clark et al 2020 the reef is situated between two small headlands and it is about 7 km long and 400 m wide it is incised by two cross reef channels up to 30 m deep fig 14 b which drain small watersheds the reef flat is fairly featureless consisting of algae covered coral with interspersed sandy patches the reef crest is more shallow and rougher the reef front is very steep and features spur and groove morphology péquignet pers comm clark et al 2020 the selected data was recorded on january 10th 2012 between 7 00 and 7 30 by an array of seabird sbe26 pressure sensors mt5 mt7 mt10 jc2 jc5 and aquadopp velocity profilers jc1 jc3 jc4 becker pers comm fig 14 a the offshore hm0 measured by coastal data information program cdip buoy 121 located in 200 m water depth 2 km se of the study site was 3 18 m with a t p of 10 5 s incoming from 56 n the spectrum is narrow banded so it is suitable for our approach to generate wave boundary conditions cdip 2021 the wind velocity measured in the nearby pago bay noaa station 1631428 noaa 2020 was 3 0 m s coming from 48 n the offshore tidal level was 0 3 m w r t msl section 3 5 1 3 5 1 model setup the grid and the bathymetry of the model need to fulfil several requirements for this case the area has a very shallow reef flat 0 5 m below msl fronted by a steep reef front that quickly drops to depths of more than 50 m the cross reef channels are 30 m deep the wave breaker zone is narrow but should be captured at high resolution to model the wave energy dissipation profile correctly as this dissipation forces wave setup and wave driven currents the offshore area on the other hand should be modelled with only enough detail to resolve the structure of the wave groups the fine resolution needed in the breaker zone should not affect the offshore part of the grid neither should this refinement be used around the reef channel to minimise cfl based time step restrictions after several tests the offshore grid resolution was fixed at 35 m the reef front area is refined up until 5 m and the reef flat is kept at the same 5 m resolution at the location of the cross reef channels the grid is derefined to 10 m to alleviate time step restrictions fig 14 the final unstructured model grid has 121 989 flow nodes and was run on 20 parallel domains the bathymetry used in the model was collated from 2 datasets the jalbtcx topobathy lidar dataset noaa 2018 provided data on the reef flat at 1 m resolution or better offshore areas not covered by this lidar data were filled using the guam 1 3 arc second mhw coastal digital elevation model noaa 2008 a 10 m resolution dataset for tsunami modelling both datasets were referenced to msl and interpolated to the flow nodes of the model grid the offshore depths were limited to a maximum of 30 m to reduce potential timestep restrictions as second order long wave forcing is expected to be very small in these depths we do not expect significant deterioration of the results a spatially varying field of chézy roughness parameter values was constructed based on the satellite derived benthic habitat map of guam nccos 2004 where coral patches got a roughness of 40 m 0 5 s and sand patches and algal turfs a roughness of 50 m 0 5 s this is somewhat less than the roughness derived by clark et al 2020 which would correspond to a roughness of about 31 m 0 5 s but it corresponds well with the value of 40 m 0 5 s obtained by vetter et al 2010 by doing a cross shore momentum balance during energetic conditions the wave friction factor which was shown previously to be important on reefs van dongeren et al 2013 was set to 0 45 which is somewhat less than the value of 0 60 used by van dongeren et al 2013 to model reef circulation in ningaloo australia the model was run with wave current interaction switched on as the cross reef channel outflow is likely to have an effect on the local short wave propagation initial runs forced with cdip only wave data consistently resulted in overestimation of the wave heights in the model as such wave height and period on the offshore boundary were taken from measurement location mt10 at 8 m water depth and only the wave direction from cdip buoy 121 the wave height at the buoy is typically 130 of the mt10 value in order to derive an offshore water level boundary signal a regional tide model was constructed forced with tpxo8 data egbert and erofeeva 2002 the tidal model was calibrated on tidal constituents at the pago bay station 1631428 just n of our study site and apra harbor station 1630000 noaa tide gauges noaa 2020 to explore the necessity of including potential larger scale water level forcing non tidal residuals ntr at the pago bay station were determined from the measured time series at the noaa station using a godin filter thompson 1983 the ntr s were found to correlate highly with the incident wave conditions measured at station mt10 r 2 0 66 not shown indicating the dominant influence of wave setup on the reef flat next to the tide gauge on semidiurnal and longer time scales as such the ntr s were not taken into account in constructing the offshore water level boundary conditions as wave setup setdown is resolved in the model local wind direction and wind speed were sourced from nearby noaa measurement station pago bay station 1631428 noaa 2020 3 5 2 results comparison of the model results with the measured data of clark et al 2020 on january 10th 2012 at 7 00 shows that the short wave height h m 0 h f is well reproduced in the deepest observation point mt10 indicating little energy loss in the model between the deeper part of the domain and the shoaling zone fig 15 a the overestimation of h m 0 h f at station mt7 and the underestimation at mt5 demonstrate a somewhat too large dissipation in the breaker zone the short wave energy is almost completely dissipated shoreward of the breaker zone as evidenced by stations jc2 and jc5 this is well reproduced in the numerical results the model is able to simulate the low frequency wave height h m 0 l f trend in the cross shore direction fairly well stations mt10 mt5 fig 15 b but the low frequency wave height is overestimated toward the cross reef channel in the middle of the site station jc5 and toward the northern end of the reef flat station jc2 in fig 15 c the comparison between the modelled and the measured mean water levels taken as the sum of the offshore tide level and the steady wave setup is shown both the cross shore trend stations mt10 mt5 jc4 and the longshore trend jc1 jc3 mt5 jc5 are well reproduced comparison of the water level variance in the low frequency band 0 0011 0 0400 hz between the model and the measured data shows that the model is capable of simulating the bulk low frequency wave heights h m 0 l f moreover it has some skill in reproducing the energy repartition over the frequencies in the 0 0011 0 0400 hz band in this field case fig 16 the model somewhat underestimates the variance at frequencies higher than 0 02 hz and overestimates the energy content at frequencies below that threshold especially at station jc2 as in the coast3d case it is unknown what the amount of free ig wave energy is that is entering the model domain through the boundaries and that could contribute to the model data discrepancy we observe under stationary conditions ignoring wind stress and lateral exchange of momentum the depth averaged conservation of momentum in the cross and alongshore directions on the reef can be written as a balance between water level gradient wave forces and bottom friction 27 g ζ i f w i ρ h ζ τ b o t t o m i ρ h ζ 0 i x y in which f w is the wave force h is the mean water depth τ b o t t o m represents the bottom shear stress and x and y represent the cross and longshore directions respectively the cross and longshore momentum balance of the ipan reef on january 10th 2012 07 00 was analysed based on a 30 min time average fig 17 for the modelled conditions we find strong seaward currents through the two incised stream channels that cross the reef and through local depressions in the reef crest fig 17 d gradients in wave setup are steep throughout the narrow surf zone where almost all the ss wave energy is dissipated through intense breaking fig 17 a b on the reef flat proper cross shore radiation stress gradients are virtually non existing leading to limited cross shore setup gradients that are balanced by cross shore flows as evidenced by the bed shear stress patterns fig 17 c d on the reef flat the current patterns are defined by the positions of the deep cross reef channels fig 17 h at those channels cross shore wave setup is locally reduced as there is relatively less dissipation by depth induced wave breaking fig 17 b this leads to the development of the alongshore setup gradients that were reported by clark et al 2020 as steady wave setup is relatively uniform across the reef flat toward the shore a flow develops over the reef flat toward the channels fig 17 g h in this field case we simulated the propagation of a swell field onto a fringing reef flat we were able to reproduce the wave height distribution over the reef flat the local grid resolution adaptations clearly demonstrate the advantage of the present modelling approach as opposed to a curvilinear model like xbeach roelvink et al 2009 an equivalent xbeach model needed 370 000 cells to resolve the reef flat hydrodynamics at the same resolution using a maximum time step that was 40 smaller for the same cfl number of 0 7 3 6 buck island case the final verification case explores the capabilities of the model to replicate the propagation of wave groups over larger distances in a topographically complex area using multiple mpi domains and local grid refinements with boundary conditions derived from a large scale regional model based on global wave data buck island reef national monument birnm is located on the north east coast of st croix island part of the us virgin islands in the caribbean sea fig 18 the island coastline is protected from incoming swell waves from the atlantic ocean by the sequence of a narrow barrier reef complex surrounding the island to the nw ne and a shallow fringing reef surrounding the island s coastline the reefs are separated by a 10 m deep lagoon characterised by the presence of coral bommies which locally enhance the hydraulic roughness depths in the reef system vary between 1 0 and 15 0 m two multi month deployments in 2015 and 2016 of 8 rbrsolo pressure transducers on cross shore transects over the fringing reef collected water level and wave data rosenberger et al 2020a the raw pressure data was collected in bursts of 2048 s every hour at 2 hz to verify our model results we use the data collected between january 12th 2016 18 00 and january 13th 2016 18 00 gmt the largest measured swell event during the field campaign in the winter of 2015 in observation points n1 n4 on the northern fringing reef of the island transect n fig 18 b and s1 s4 transect s fig 18 c covering the southeastern fringing reef conditions during this time frame were characterised by small tidal water level variations between 0 05 m and 0 15 m w r t msl the swell event characterised by waves of around 2 m height with a peak period varying between 10 and 12 s was impinging on the island from the ne the northern n transect was directly exposed to this swell while the wave field had to refract around the island to reach transect s 3 6 1 model setup in order to resolve the various spatial scales involved in the propagation refraction and dissipation of the wave field a grid was constructed with several degrees of resolution refinement using similar considerations as for the pilot case the offshore part where short waves do not dissipate and the propagation of long waves is important has a resolution of 30 m the same holds for areas that do not directly influence the hydrodynamics measured in the two transects fig 18 a zone a over the shallower barrier reef and the lagoon fig 18 a zone b the resolution is increased to 15 m the fringing reef area fig 18 a zone c has a resolution between 7 and 3 m in order to resolve short wave dissipation and ig generation by breakpoint forcing typical of reef lined coasts pomeroy et al 2012 péquignet et al 2014 the final number of grid cells is 50999 the directional resolution was set to 5 similarly as for the pilot case the bed levels used in the model were sourced from 2 datasets the high resolution usgs eaarl b topo bathymetric lidar dataset for st croix fredericks et al 2015 was used to interpolate the bathymetry in the fine resolution parts of the grid where large bathymetric gradients occur over limited spatial scales the grid parts not covered by the lidar dataset were filled with 1 3 arc second mhw coastal digital elevation model data for the us virgin islands love et al 2015 bed level data was reduced to msl using the tidal benchmark data of the christiansted harbor noaa station noaa 2022 the offshore depths n of the barrier reef were limited to a maximum of 30 m to reduce timestep restrictions the hydraulic roughness in the model was specified as a spatially varying field both for the flow and the wave related friction therefore a bed type classification was derived from the birnm benthic habitat data of batista 2012 in which we retained 3 classes for which we differentiate bottom friction parameters f w and chézy values following storlazzi et al 2017 a sandy patches f w 0 1 and chézy 55 m 0 5 s b patch reef with 10 coral cover f w 0 15 and chézy 30 m 0 5 s c patch reef with 50 coral cover f w 0 3 and chézy 20 m 0 5 s offshore wave data were not available at the time of the field campaign therefore a nested regional swan booij et al 1999 version 41 31 wave model was constructed to derive wave boundary conditions for the surfbeat model we included the effects of wind wave growth refraction depth induced wave breaking battjes and janssen 1978 whitecapping van der westhuysen et al 2007 and bottom friction hasselmann et al 1973 with default parameters the large swan domain covers the atlantic north of the british virgin islands until the caribbean sea south of st croix at a uniform resolution of 1500 m the finer nested domain has a resolution of 200 m and covers the north coast of st croix the swan model was forced with era5 hersbach et al 2020 wind and wave data and validated against 5 months jun 1st oct 31th 2010 of buoy data at christiansted cdip station 431 fig 18 a significant wave heights were estimated with a bias of 0 0 m and a rmse of 0 20 m and the t m01 with a bias of 0 23 s and an rmse of 1 24 s the swan model was subsequently run for the period january 11 15 2016 and hourly wave parameters were extracted at the n side of the barrier reef to construct parametric jonswap time series to force the surfbeat model fig 19 b d the water level signal to force the n absorbing generating model boundary was taken from station n1 as is fig 19 a as comparison with a tidal harmonics fit showed considerable ntr influence uncorrelated with the incident wave climate local wind was insignificant during the model period noaa 2022 and was not taken into account the model was run on 20 partitions modelled water levels and wave data were recorded continuously at 2 hz in the locations of the 8 deployment sites in order to obtain results that are compatible with the usgs dataset hourly 2048 s bursts of the demeaned water levels ζ were bandpass filtered to obtain the low frequency signal ζ l f 0 004 0 04 hz and the high frequency signal ζ h f 0 04 0 4 hz the corresponding rms lf wave height was then obtained by h r m s l f 8 ζ l f 2 the rms short wave height h r m s h f as h m o d e l l e d 2 8 ζ h f 2 3 6 2 results the mean water level variability in the observation stations is well reproduced in the model fig 20 a h the tide levels in stations n1 and s1 are simulated correctly with small biases the cross reef distribution of the sea swell wave setup values over both transects is modelled correctly as is the difference in setup magnitude between the n and the s transect sea swell ss wave heights are modelled with an absolute bias and a rmse of 0 05 m significant amounts of ss wave energy 50 are dissipated over the barrier reef and the lagoon inspection of the spatial distribution of the ss wave dissipation fields from the model output suggests that this wave height reduction occurs mainly through bottom frictional effects fig 20 i p the resulting reef face wave heights in n1 are about twice as high as those in the more sheltered s1 station significant ss wave breaking only takes place around the crest of the fringing reef for the wave heights considered in this study as a result n1 and s1 ss wave heights are tide independent but hf wave heights on the reef flat and in the fringing reef lagoon n2 4 s2 4 are largely determined by the local water depth over the reef crest modelled lf wave height error statistics are similar in magnitude as the hf ones and errors are generally smaller than 20 except for station s1 fig 20 q x inspection of the spatial distribution of the water level variance in the model domain indicates two sources of local surfbeat generation on the relatively gently sloping barrier reef ig waves are generated as a result of ss wave energy dissipation through bottom friction as wave breaking is generally absent on the fringing reef face and reef crest ig waves are generated through the breakpoint forcing mechanism normalised bed slope parameter β b battjes et al 2004 equal to 2 the model is able to reproduce the relative cross reef spatial pattern in ig wave heights on both transects including the higher ig values observed in the two stations closest to the shoreline n4 an s4 which result from long wave reflection off the steep shoreline fig 20 q x suggests a tidal dependence of the ig wave heights on transect n in the measured data this was verified not to be the case from the complete measured time series in station n1 in conclusion this test case demonstrates that the model is able to reproduce the propagation and dissipation of sea swell wave fields and the associated ig wave dynamics on larger spatial scales in a complex model domain using boundary conditions generated from a global wave and wind database as for efficiency the average achieved timestep in this unstructured surfbeat run was 0 14 s for a cfl condition of 0 7 comparison with a structured xbeach model setup for the same problem shows an improvement of the timestep with roughly 40 and a reduction in grid size by a factor of 2 9 while attaining a spatial resolution on the fringing reef that was 2 times better 4 summary and conclusions this paper presents the first numerical model defined on an unstructured mesh administration that is able to resolve hydrodynamics forced by tides wind and sea swell waves on the time scale of wave groups we have validated different functionalities in a series of six tests showing that the model has good skill in reproducing measured laboratory and field data as well as results from semi analytical model approaches the zelt test case was included to verify the correct propagation of long waves flooding and drying and the robustness of the offshore flow boundary in absorbing reflected free waves leaving the model domain under highly oblique angles additionally the implementation of the drying and wetting algorithm was compared favourably to results from a temporally and spatially adaptive grid modelling approach results proved to be robust under different grid configurations the use of a triangular grid alleviated grid influences in the phasing of the run up solution the boers test demonstrated the model s capability of generating first and second order boundary forcing the functioning of the 1d absorbing flow boundary the correct implementation of dissipation by wave breaking and of the wave forces in the momentum balance the laboratory data were in general well reproduced although the simulation of the steepest wave conditions proved more challenging as we are using linear wave theory the delilah field case served as a verification for the stationary refraction single direction approach which serves to counter the numerical diffusion of wave groups when advecting wave action in multiple directional bins over longer travel distances comparing the single direction and the multiple directions results to data measured at the duck field research facility the former method yields better results in reproducing the infragravity energy profile in the surf zone and is better at retaining the wave group structure the methods are on par in simulating the short wave distribution and the longshore current over the cross shore profile combined with the possibility of localised refinement by using unstructured grids the stationary refraction approach offers the possibility of significantly speeding up the calculations without loss of reliability the coast3d field case reproduced the hydrodynamic response of a mildly sloping double barred sandy beach to a storm mean water depths and short wave heights were well reproduced during highly non stationary conditions where incident wave heights and directions changed strongly over the course of a tide despite the fact that ig mean periods were correctly modelled the ig wave height was underestimated due to the lack of a mechanism to let free ig enter through the domain boundaries the pilot field case demonstrated the model s robustness in a challenging study area featuring longshore bathymetry variability with very steep gradients a narrow surf zone and large bottom roughness using the parallel implementation of the model the cross shore and longshore variability of the short waves the infragravity energy and the steady wave setup were skilfully resolved finally the buck island field case reproduced the propagation of an energetic swell field around an island protected by 2 reefs on both the exposed measurement sites and the sheltered ones bulk wave characteristics were correctly modelled starting from boundary conditions derived from a global reanalysis dataset demonstrating the applicability of our approach in typical engineering studies the use of this unstructured wave model allows for coverage of the same model domain with optimised resolution yet with a smaller total number of grid cells the model also allows a larger explicit average timestep for the same value of the cfl criterion compared to structured models with similar functionality the model reliably reproduces observations in laboratory and field conditions and is as such widely deployable in a variety of simple and complex coastal settings to study nearshore hydrodynamics credit authorship contribution statement johan reyns conceptualization software validation formal analysis writing original draft robert mccall software roshanka ranasinghe writing review editing ap van dongeren writing review editing funding acquisition dano roelvink conceptualization software writing review editing funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to acknowledge sander van der pijl for his assistance in the development of the software janet becker and mark merrifield provided the pilot data and are thanked for fruitful discussions on the ipan reef dynamics bart grasmeijer provided the coast3d data and reports ellen quataert substantially improved a first version of the birnm xbeach model permission to use data provided by the field research facility of the u s army engineer waterways experiment station s coastal engineering research center is appreciated funding this research was supported by the pacific community spc new caledonia under the wacop project by deltares the netherlands under strategic research programmes natural hazards and knowledge foundation software and models and by the office of naval research usa under award number n00014 17 1 2459 work on the pilot model benefited from two research visits to uh appendix a overview of implemented formulations for wave energy dissipation by breaking see table a 1 appendix b performance metrics verification runs b 1 error metrics see table b 1 b 2 boers flume tests see table b 2 b 3 zelt tsunami run up see table b 3 b 4 delilah field experiment see table b 4 b 5 coast3d egmond campaign see table b 5 b 6 pilot field experiment see table b 6 appendix c implicit stationary wave energy solver in order to determine the mean wave direction over the unstructured grid while taking into account refraction and dissipation we solve the simplified wave energy balance eq 28 in wave direction θ 28 e e t c g e e s c θ e e θ d d 0 where e e is the spectral wave energy density per node and wave direction t is time c g is the group velocity s is the distance along the wave direction under consideration c θ is the refraction velocity θ is the wave direction and d d is the local wave dissipation eq 28 is discretised with finite differences at the location of the network nodes using a forward euler scheme in time and an implicit first order upwind approach in geographical and directional space after rearranging the terms in the equation this results in a tridiagonal coefficient matrix and a right hand side that is a function of the wave energy at the previous timestep e e k n and the already updated wave energy at the upwind neighbours e e k u p w n 1 this system can be solved with standard methods such as the thomas algorithm press et al 2007 on open wave boundaries a dirichlet wave energy condition is applied while on other open non wave boundaries a zero gradient neumann wave energy condition is imposed the system of equations is solved consecutively along four orthogonal sweep directions starting along the incident wave direction on the open model boundary local wave energy values are determined from partially converged upwind node values by iterating the sweeping process and applying relaxation we can solve for the effect of the non linear dissipation term d d a user defined criterion determines when the solution process has converged after interpolation to the flow administration cell centres the mean wave direction is determined from the directional distribution of the wave energy 
25434,an unstructured hydrodynamic model is presented that is able to simulate 2d nearshore hydrodynamics on the wave group scale a non stationary wave driver with directional spreading with physics similar to xbeach roelvink et al 2009 is linked to an improved and extended version of the existing unstructured flow solver delft3d fm kernkamp et al 2011 martyr koller et al 2017 the model equations are discretised on meshes consisting of triangular and rectangular elements the model allows for coverage of the model domain with locally optimised resolution to accurately resolve the dominant processes yet with a smaller total number of grid cells the model also allows a larger explicit time step compared to structured models with similar functionality the model reliably reproduces measured datasets of water levels sea swell and low frequency wave heights in laboratory and field conditions and is as such widely deployable in a variety of simple and complex coastal settings to study nearshore hydrodynamics keywords numerical modelling nearshore circulation infragravity waves unstructured meshes data availability data will be made available on request software availability the software framework of the model presented in this paper is delft3d flexible mesh under development since 2008 deltares 2021 the source code is available free of charge from deltares https oss deltares nl after registration as a beta test programme participant with software deltares nl the framework is supported under windows and centos7 the computational kernel is written in fortran and the complete compiled code distribution requires 1gb of disk space 1 introduction infragravity ig waves defined as waves with periods of 25 250 s can represent a significant portion of the water level variance in the nearshore these low frequency motions modulate several coastal processes such as reef flat hydrodynamics péquignet et al 2009 pomeroy et al 2012 cheriton et al 2020 péquignet et al 2014 rip current dynamics macmahan et al 2004 reniers et al 2006 2009 wave run up and overtopping cheriton et al 2016 billson et al 2019 sediment resuspension and transport de bakker et al 2016a rosenberger et al 2020b and dune erosion and barrier breaching mccall et al 2010 baumann et al 2017 lashley et al 2019 anarde et al 2020 sources of wave energy in the ig frequency band are linked to the presence of wave groups theory longuet higgins and stewart 1962 hasselmann 1962 and observations kostense 1985 elgar et al 1992 herbers et al 1994 show that in intermediate water depths pairwise non linear difference interactions between sea swell components in a wave group force a ig wave at the group frequency that is 180 out of phase with the wave group envelope upon propagating into shallow water depths these ig waves shoal through resonant triad interactions janssen et al 2003 de bakker et al 2015 transferring energy from the peak frequency toward ig frequencies at a rate dependent on the bed slope and the group frequency battjes et al 2004 van dongeren et al 2007 moreover depth variations on the spatial scales of the wave groups cause a nett radiation stress gradient that forces free ig waves propagating both in onshore and offshore directions moura and baldock 2019 contardo et al 2021 in the outer surf zone ig waves are generated by the time varying breakpoint mechanism symonds et al 1982 contardo et al 2018 on steep normalised bed slopes battjes et al 2004 if normalised bed slopes are mild and short waves and ig waves are in the shallow water regime bound long waves are progressively released from the wave groups masselink 1995 baldock 2012 when these conditions are not met the bound wave degenerates with the breaking wave group and ig generation is minimal baldock 2012 in the inner surf zone triad interactions between ig frequencies enhance ig wave non linearity henderson et al 2006 thomson et al 2006 and can cause ig wave breaking close to the shoreline van dongeren et al 2007 de bakker et al 2016b similar triad interactions can transfer energy back to the short wave frequency band guedes et al 2013 de bakker et al 2016b merging surf bore fronts can add additional energy to ig frequencies sénéchal n dupuis et al 2001 tissier et al 2015 if the bed roughness in the surf zone is high frictional dissipation is a significant ig energy sink péquignet et al 2014 van dongeren et al 2013 upon reflection off the coastline ig waves radiate offshore as leaky waves forming a cross shore standing pattern guza and thornton 1985b or they become refractively trapped as edge waves holman and bowen 1979 herbers et al 1995 given the important role of ig waves in coastal processes substantial efforts were made over the last decades to model their generation propagation and dissipation in various coastal settings van dongeren et al 2003 provide a review of the analytical methods that have been explored to explain the generation of low frequency wave energy more recent semi analytical studies include work on ig dynamics over variable topography to explore phase lags between the short wave forcing and the associated bound long wave e g zou 2011 guérin et al 2019 zhang et al 2020 liao et al 2021 contardo et al 2021 liao et al 2022 and on the generation of free long waves in the surf zone contardo et al 2018 nowadays numerical models are the method of choice to explore ig variability on arbitrary profiles under dissipative conditions list 1992 van leeuwen 1992 and reniers et al 2002 presented linearised models incorporating the ig generation mechanisms of longuet higgins and stewart 1962 and symonds et al 1982 roelvink 1993b and karunarathna and tanimoto 1995 presented 1d versions of a model solving the depth integrated and short wave period averaged non linear conservation equations of mass and momentum on a finite difference grid the wave forcing is provided by a wave group resolving wave model reniers et al 2004 and van dongeren et al 2003 extended the approach to 2dh non linear finite difference models for ig generation and propagation again coupling a wave group resolving wave driver to a non linear shallow water solver this approach was ported to curvilinear grids by roelvink et al 2012 using a finite volume discretisation madsen and sørensen 1997 used a boussinesq type model to simulate ig wave dynamics resolving the short wave motion as well as the long wave motion a similar model paradigm was used by among others karunarathna et al 2005 cienfuegos et al 2010 nwogu and demirbilek 2010 and su et al 2015 the last few years have seen an increase in popularity of non hydrostatic models for the modelling of low frequency wave dynamics models of this type were presented by zijlema et al 2011 ma et al 2012 and de ridder et al 2021 detailed model hindcast studies of infragravity transformations in the surf zone of field sites and laboratory tests using the latter approach were reported by e g rijnsdorp et al 2015 2021 de bakker et al 2016b fiedler et al 2018 lashley et al 2018 and risandi et al 2020 in the present paper we adopt the approach of combining a 2dh non linear shallow water equations solver with a wave group resolving wave driver most models of this class discretise the model equations on a structured rectangular or orthogonal curvilinear grid e g shorecirc shi et al 2003 delft3d reniers et al 2004 xbeach roelvink et al 2009 2012 although this type of grid schematisation provides relative flexibility in efficiently covering the model domain an inherent drawback in the use of structured grids is that refinements needed for output requirements or for handling sharp spatial gradients in bathymetry or transport fields necessarily extend far beyond the part of the domain where a finer resolution is needed this can only be remedied in the structured grid approach by using nested models or through domain decomposition these latter approaches increase the computational cost of a model unnecessarily and in the case where explicit numerical schemes are used will also cause an unfavourable reduction of the maximum allowed timestep in deep areas as the courant friedrich lewy cfl condition courant et al 1967 should be met to retain stable computational results a second important drawback of structured curvilinear grids is the requirement of topological connection which precludes for example the folding and reconnection of a curvilinear grid around islands or atolls unless cyclic boundary conditions are implemented roelvink et al 2013 one approach to avoid these limitations is the use of unstructured grids which are able to provide localised resolution changes without affecting other parts of the grid this flexibility allows for grid adaptations where the grid resolution can now be optimised in function of the spatial scale of the locally dominant physical process to be modelled one example of this would be a relative coarse offshore resolution sufficient to model the propagation of wave groups combined with a finer resolution in regions where short wave dissipation or long wave reflection is important this geometrical flexibility has the added benefit that the grid resolution can also be optimised in function of the explicit time step restriction as grid resolutions ideally are nowhere finer than they need to be to accurately discretise the flow phenomenon under consideration potential reduced accuracy because of the deviation of an unstructured grid from a uniform cartesian grid can be constrained by ensuring grid orthogonality and resolution smoothness and by reducing the use of triangles as much as possible hirsch 2007 unstructured models has gained some popularity over the last decade to simulate mean i e timescales longer than wave groups nearshore circulation e g dietrich et al 2012 zheng et al 2017 wu et al 2011 and morphodynamics e g bertin et al 2009 guérin and dodet 2016 villaret et al 2013 however to date there are no unstructured numerical models that can simulate nearshore hydrodynamics on the wave group scale in this paper we present such an unstructured wave model and we test its ability to reproduce the wave measurements of a number of published datasets in field and laboratory settings in section 2 we describe the formulations and the numerical approach that are used in the wave and flow modules of our model section 3 demonstrates the practical applicability of the model on a range of spatial and temporal scales section 4 contains discussion of the results and the conclusions 2 model formulations the aim of the present model is to simulate the hydrodynamic circulation in the nearshore in response to instationary short wave forcing on the time scale of wave groups the model should be able to simulate bound free and refractively trapped long waves and it should be able to handle run up and overwash in our approach we solve the depth averaged short wave averaged non linear shallow water equations on unstructured grids that can be composed of a combination of triangular and rectangular cells short wave effects are incorporated through radiation stress gradients which drive mean surf zone circulation and low frequency motion on ig timescales moreover short wave induced stokes drift varies on the wave group time and spatial scale lastly bed shear stresses are enhanced by the presence of waves as we do not solve for the individual short waves we need a short wave driver that is forced by directionally spread narrow banded short wave spectra and that solves the time varying wave action balance in combination with a roller model over arbitrary 2d bathymetries 2 1 short wave module 2 1 1 non stationary wave model in order to calculate the propagation and dissipation of organised wave energy in the nearshore we largely follow roelvink et al 2009 the wave module solves the coupled non stationary wave action eq 1 and roller energy balance eq 8 in geographical and directional space taking into account dissipation by wave breaking and spatially varying bottom friction we assume that the incident wave field is narrow banded in frequency so we can work with a single representative wave period taken as t m 1 0 and we neglect absolute frequency shifts in the wave action and roller balance 1 a t c g a c θ a θ d w σ d b f σ where a is the time varying wave action density in geographical and directional space defined as a e w σ e w 1 8 ρ g h r m s 2 is the wave energy density t is time ρ is the water density θ the wave direction in the cartesian reference frame d w is the directionally distributed wave dissipation by breaking d b f is the directionally distributed wave energy dissipation by bottom friction and represents the spatial gradient operator i x j y with i and j the unit vectors in x and y direction respectively wind growth is not taken into account at present although it can be an important energy source on the spatial scale o 1 km at which this type of models is typically used drost et al 2019 the group velocity vector c g with which the wave action propagates in geographical space is equal to 2 c g 1 2 k h s i n h 2 k h σ k k k u l where u l represents the depth averaged flow velocity vector in the lagrangian reference frame h is the total water depth and k the wave number magnitude the refraction speed c θ is given by 3 c θ σ sinh 2 k h h n k k u n the derivative n indicates the gradient orthogonal to the local wave propagation direction θ the wave number vector k is derived from the kinematics conservation equations massel 1989 4 k i t c g j k i x j ω x i 0 i j 1 2 where ω σ k u represents the absolute radial wave frequency the intrinsic frequency σ is derived from the linear dispersion relation σ g k tanh k h wave energy is dissipated through wave breaking and bottom friction a number of wave breaking formulations is available listed in appendix a by default the formulation of roelvink 1993a is being used the total directionally integrated wave dissipation is then given by the product of the time varying breaking dissipation and the fraction of breaking waves 5 d w q b α σ 8 π ρ g h 3 h q b 1 exp h γ tanh k h k n where α is a free parameter of o 1 ρ is the water density g is gravitational acceleration γ is the breaker parameter h is the wave height varying on the timescale of wave groups and n is a shape factor for the wave breaking probability distribution γ tanh k h k represents a measure for the maximum expected wave height for the local water depth h the breaker dissipation is distributed among the directional bins according to the directional distribution of the wave energy we express the time averaged directionally integrated bottom frictional dissipation of wave energy as d b f τ b u rms 1 2 ρ f w u rms 3 where the root mean square orbital velocity magnitude u rms is approximated from the linear wave theory expression for the orbital velocity eq 6 and f w is a parameter of o 0 01 o 0 1 depending on the bottom characteristics booij et al 1999 van dongeren et al 2013 6 u rms h r m s ω 2 2 sinh k h considering the slowly varying dissipation in wave groups we can time average d b f over the representative wave period therefore we can use the monochromatic approximation for the time average of the third even velocity moment u rms 3 eq 7 to calculate d b f guza and thornton 1985a 7 u rms 3 1 20 u rms 2 3 2 0 42 u orb 3 with u orb 2 u rms the resulting bottom dissipation d b f is distributed among the directional bins in a manner analogous to the redistribution of the dissipation resulting from wave breaking processes the lack of a wind source term limits the applicability of the model to domains and applications where local wind growth is of secondary importance 2 1 2 roller model in order to account for the observed spatial lag between the start of depth induced breaking and the development of wave setup and wave related circulation bowen et al 1968 nadaoka and kondoh 1982 the roller model concept roelvink and stive 1989 nairn et al 1991 is used the breaking wave dissipation d w acts as the source term for the roller energy the roller propagates through the surf zone with wave celerity c and following the carrier wave idea where local wave and roller directions are assumed to be equal allows us to reuse the refraction velocity c θ used in the wave action balance eq 1 8 e r t c e r c θ e r θ d w d r where e r is the roller energy c is the wave celerity vector equal to σ k and d r is the directionally distributed roller energy dissipation the magnitude of which is calculated as reniers et al 2004 9 d r 2 g β r e r c in which β r is a parameter of o 0 1 representing the slope of the internal boundary between the roller and the underlying wave motions the resulting wave and roller fields feed into the radiation stress tensor components calculated using linear wave theory reniers et al 2002 10a s x x c g c 1 c o s 2 θ 1 2 e w c o s 2 θ e r d θ 10b s x y c g c c o s θ s i n θ e w c o s θ s i n θ e r d θ 10c s y y c g c 1 s i n 2 θ 1 2 e w s i n 2 θ e r d θ the wave related forcing terms in the depth averaged flow momentum equations eq 22 are then expressed in terms of the radiation stress gradients 11a f w x s x x x s x y y 11b f w y s x y x s y y y 2 1 3 numerical implementation eq 1 and 8 are discretised in geographical space on a staggered orthogonal smooth unstructured grid consisting of combinations of triangles and rectangles a user defined number of bins defines the directional grid spatial advection terms are determined from an explicit higher order discretisation scheme combined with a monotonized central flux limiter for non equidistant grids hou et al 2012 to limit numerical diffusion and to ensure total variation diminishing tvd properties of the scheme deltares 2021 all wave properties are determined in the cell centres collecting the dissipation terms in one variable d eq 1 written in terms of wave energy is discretised as follows 12 e e k i θ n 1 e e k i θ n δ t adv k i θ ref k i θ d k e k e e k i θ 0 where k is the cell number i θ the directional bin n represents the time level indicates the values after the advection and refraction step e e is the wave energy in cell k and directional bin i θ e is the directionally integrated wave energy terms adv k i θ for the spatial advection and ref k i θ for the wave refraction are elaborated on next using gauss theorem the cell centre based advection discretisation adv k i θ in eq 12 is equal to the sum of the face based advection contributions w l adv l i θ and divided by the cell area a k eq 13 13 adv k i θ 1 a k l γ w l adv l i θ where l indicates the face under consideration w l is the face width of l and γ is the cell boundary of cell k face based advection flux adv l i θ is the sum of a first order upwind contribution adv l u p w eq 14 and a higher order flux limited correction term adv l h o eq 16a the first order upwind contribution is equal to 14 adv l u p w c g l i θ ee b i θ c g l i θ 0 c g l i θ ee a i θ c g l i θ 0 0 c g l i θ 0 where c g l i θ is the cell volume weighted wave group velocity in the face normal direction for directional bin i θ calculated from the values in a and b in order to explain the construction of the higher order correction term adv l h o we assume a wave energy flux from cell b to cell a in the following fig 1 an equivalent reasoning holds for a flux from cell a to cell b the construction of the higher order correction term for the face based advection flux adv l i θ happens on a line through the cell circumcentres of cells a and b based on the wave energy in the cells adjacent to cell b these cells are called b u p w 1 and b u p w 2 in fig 1 we aim at constructing the correction at the location of the crossing b c u p w between lines a b and b u p w 1 b u p w 2 note that if either one of these cells does not exist the intersection collapses to the cell centre of b u p w 1 and the approach will reduce to a classical second order upwind correction such as for curvilinear discretisations thus the stencil applied here consists of cells a b b u p w 1 and b u p w 2 based on the values of the wave energy in the latter two cells a value adv b s l is determined at location b c u p w 15a ee b s l i θ sl b u p w 1 ee b u p w 1 i θ sl b u p w 2 ee b u p w 2 i θ 15b sl b u p w 1 d b 1 d b 1 d b 2 15c sl b u p w 2 d b 2 d b 1 d b 2 15d γ b s l dx d upw the higher order correction flux adv l h o is then 16a adv l h o α a max 1 δ t c g l i θ d x 0 ψ r c g a i θ c g b i θ 16b r ee a i θ ee b i θ ee b i θ ee b s l i θ 16c ψ r max min min r α a 1 r 2 1 α a 0 where r is the ratio of the wave energy slopes along line a b c u p w α a is the fraction represented by cell a in the volume weighting and ψ r represents the monotonized central mc difference limiter suppressing wiggles resulting from higher order advection hou et al 2012 the roller energy balance eq 8 is discretised in a similar fashion the refraction term ref k i θ is discretised in directional space with a second order upwind approach as follows 17 ref k i θ reflux k i θ reflux k i θ 1 δ θ 18 reflux k i θ c θ l i θ 1 5 ee k i θ 0 5 ee k i θ 1 c θ l i θ 0 c θ l i θ 1 5 ee k i θ 1 0 5 ee k i θ 2 c θ l i θ 0 0 c θ l i θ 0 where reflux k i θ is the wave energy flux by refraction on the interface between directional bin i θ and i θ 1 and c θ l i θ is the average refraction velocity on the same interface calculated based on eq 3 eqs 1 and 8 are solved with explicit forward euler timestepping the maximum allowed timestep δ t is determined from 19 δ t cfl min n c e l l s a k i θ n θ l γ w l c g l i θ c g l i θ 0 2 1 4 stationary wave model in addition to the non stationary wave model described in the previous section an additional wave driver is included in the code to provide the stationary wave fields used in the single direction approach of roelvink et al 2018 in this approach the stationary wave driver updates the wave field in full directional space and determines the mean wave direction in every flow node the wave groups in the non stationary model are subsequently advected along this mean direction only preserving the groupiness much better and yielding more reliable estimates of the infragravity wave heights for our present purpose the module solves the wave energy balance using an implicit first order upwind scheme with pseudo timestepping and quadrant sweeping to ensure convergence of the solution an outline of the implementation is given in appendix c 2 2 momentum and continuity equations for unsteady flow the short wave averaged wave group resolving depth averaged unsteady flow patterns are described using the non linear shallow water equations in the generalised lagrangian mean framework andrews and mcintyre 1978 nguyen et al 2021 the model equations are solved for the lagrangian velocities u l these wave period averaged water particle velocities are defined as the sum of the eulerian velocities u e the velocity field observed from a fixed point at the bed and the stokes drift u s calculated from linear wave theory 20a u l u e u s 20b u s e w 2 e r ρ h c k k where the wave energy e w the roller energy e r and the wave number vector k are obtained from the wave module we express conservation of mass as 21 ζ t h u l 0 in which ζ is the water level conservation of momentum is expressed as 22 u l t u l u l f u l g ζ ν u l u l t τ w ρ h τ b ρ h f w ρ h where the first term in the left hand side represents inertia the second term advection and the third earth rotational effects the first term in the right hand side of the equation represents the pressure gradient the second term horizontal turbulent mixing the third and fourth term the influence of wind and bottom roughness respectively and the fifth term forcing by waves as calculated by eq 11 f is the coriolis vector ν is breaker induced turbulent eddy viscosity equal to h d r ρ 1 3 battjes 1975 wind shear stress τ w is determined by 23 τ w ρ c d a u 10 u 10 with c d a a wind friction parameter and u 10 the wind velocity vector the bed shear stress τ b is calculated following the parametrisations of soulsby 1997 or ruessink et al 2001 and by using the eulerian velocity definitions u e in the bottom stress formulation the shallow water equations are discretised with a finite volume approach on an orthogonal arakawa b grid resulting in a set of facenormal velocities and cell centre water levels the momentum equations are solved conservatively using the approach of perot 2000 the cartesian advection and diffusion vector components are reconstructed in the cell centres and interpolated back to the cell faces after which they are rotated in face normal direction the advection contribution is discretised using an higher order limited upwind scheme using a similar method as described in section 2 1 3 time integration is done semi implicitly with a predictor corrector scheme first the face normal velocities are calculated explicitly this velocity field is then substituted in the continuity equation to obtain the water levels at the new time level using the θ method the updated velocities are then obtained from back substitution using the new water levels in the pressure gradient term full details of the methodology can be found in kernkamp et al 2011 martyr koller et al 2017 and deltares 2021 the explicit time discretisation to solve the momentum equation leads to a time step restriction where the flow time step is minimised based on the requirement that the ratio between the water volume in a cell and the outgoing discharges be positive within a time step as water levels are solved for implicitly the long wave celerity should not be included in the restriction deltares 2021 the global model time step is then the minimum of the flow and the wave time step derived from eq 19 drying and flooding is taken into account in the spatial discretisation by setting face normal velocities the advection contribution external force terms and the viscous fluxes to zero when the depth on a cell face is smaller than a threshold value in the temporal discretisation the drying flooding check is performed at the beginning of the time step and a second time after the water level equation is solved if the updated water levels are lower than the local bed level the cell is removed from the system of equations and the time step is repeated deltares 2021 2 3 boundary conditions the wave model is forced by spatially varying wave energy time series that are modulated on the timescale of wave groups following the procedure of van dongeren et al 2003 a single summation random phase approach is used whereby at every offshore wave boundary point the sea swell water level signal is derived from the input spectrum for a finite number of discrete frequency components and directions using linear wave theory according to the directional distribution of the spectrum these wave components are integrated over directional space and the absolute value of the hilbert transform of this signal yields the water level envelope this water level envelope is distributed back in directional space according to the directional variance distribution and converted to wave energy the bound long waves associated with the sea swell wave groups are derived from frequency difference interactions between the swell components using the equilibrium theory of hasselmann 1962 and okihiro et al 1992 lateral open boundaries that have no explicit wave signal imposed are of the neumann type causing some disturbances under obliquely incident waves the influence of these disturbances can be mitigated with an appropriate choice of boundary locations on the open flow boundaries an absorbing generating boundary condition is imposed van dongeren and svendsen 1997 that combines the slowly varying tide surge water level with the discharges associated with the bound long waves generated by the wave model the boundary absorbs perpendicularly and obliquely outgoing free waves by locally reconstructing the outgoing characteristics and subtracting them from the incoming signal lateral flow boundaries use the approach of roelvink and walstra 2004 avoiding spurious circulations by combining the specification of longshore water level gradients with switching off cross boundary velocity gradients cross shore velocity gradients along the boundary are unaffected combined with an offshore water level boundary this yields a well posed system of equations to be solved 2 4 model applicability this combination of a non linear circulation model with a linear phase averaged wave driver allows for the propagation of wave groups with their associated bound long waves from offshore to the nearshore short wave refraction and dissipation force wave group scale water motions that can non linearly interact and dissipate on ig frequency scales as we model short waves on the time scale of wave groups and not the individual waves non linear energy transfers between sea swell and ig frequency bands are not accounted for in the short wave energy balance neither can we reproduce processes like short wave bore merging which add to the ig energy content in the inner surf zone van dongeren et al 2007 tissier et al 2015 in the present approach ig waves do however influence the short wave energy propagation and dissipation by modulating the local depths in shallow water where ig wave heights can be of the same order of magnitude as the short waves pomeroy et al 2012 péquignet et al 2014 3 model verification the correctness of the implementation and some of the model s key features are demonstrated in the present section using a number of case studies the theoretical zelt case zelt 1986 özkan haller and kirby 1997 is used to check long wave propagation flooding and drying and the performance of the 2d absorbing generating boundary the boers flume study boers 1996 demonstrates short and long wave transformation over a barred profile extending to 2dh cases the coast3d test case soulsby 2001 demonstrates the capabilities of the model in reproducing non stationary wave fields in realistic field settings the delilah field testcase birkemeier et al 1997 is used to demonstrate the computational advantages of the stationary refraction approach roelvink et al 2018 in combination with unstructured grids the pilot field case clark et al 2020 applies the model to reproduce the hydrodynamics of a fringing reef on the island of guam demonstrating how local grid refinements allow an efficient computation of wave properties in a topographically complex setting this is taken one step further in the final field case which simulates the refraction of a swell field around buck island reef national monument a caribbean island sheltered from incident waves by a reef complex rosenberger et al 2020a 3 1 zelt solitary wave run up any process based surf zone model has to deal with the numerical treatment of a moving shoreline in the present model the shallow water equations are solved on a stationary mesh and the shoreline is defined as the interface of a wet and a dry flow cell whether a cell face is a shoreline is determined from the neighbouring water levels using an upwind reconstruction deltares 2021 the land water interface is moved in discrete steps with an accuracy constrained by the local grid size in order to test the accuracy of the wetting drying implementation for different grid configurations and to check the performance of the absorbing offshore boundary we replicate the tsunami run up model devised by zelt 1986 this testcase simulates the shoreward propagation of a tsunami modelled as a non linear solitary wave over a flat shelf with depth h 0 and width l s into a sinusoidal shaped bay with steep side promontories the tsunami refracts and reflects off the coast and propagates out of the domain the beach slope in the centre of the bay is 1 10 and the promontories have a maximum slope of 1 5 the wave length of the bay is 2 l y the characteristic water depth h 0 was chosen by zelt 1986 as 0 4 l y π if we locate the origin of the coordinate system on the sw corner of the domain with x increasing toward the coast fig 2 the bottom z b is defined by 24 z b h 0 if x l s h 0 0 4 x l s 3 cos π y l y if x l s at the offshore boundary we define the time varying water level ζ i n as 25 ζ i n t α h 0 s e c h 2 3 g 4 h 0 α 1 α t t 0 where α is the non linearity parameter defined as h h 0 h is the characteristic wave height of the tsunami in deep water g is gravitational acceleration and t 0 is a phase shift such that the incoming signal at t 0 is equal to 0 1 h roelvink et al 2009 as we compare our results to özkan haller and kirby 1997 α is set to their value of 0 02 this value was chosen to ensure sufficiently high run up as the characteristic length of the solitary wave and hence the slope it feels scales with 1 α zelt 1986 the model was set up to solve the non linear shallow water equations using three grid versions 1 a rectangular grid with a square net cells with a resolution of 0 125 m aligned with the x axis 2 a rectangular grid with the same resolution rotated counter clockwise by 30 to show the robustness of the solution in a framework not aligned with the coordinate axes 3 an orthogonal triangular grid with a resolution of 0 08 m to minimise shoreline staircase effects the half bay width l y was set to 8 m resulting in h 0 1 019 m and l s l y the bathymetry was then constructed using eq 24 and imposed in the flow nodes of the model grid the absorbing generating boundary was forced with eq 25 taking the first 33 5 s to send the tsunami into the domain after that only reflected signals pass the offshore boundary both lateral boundaries have a no flux condition and thus act as a sidewall the model was run without friction and viscosity following zelt 1986 and özkan haller and kirby 1997 maximum run up time series were recorded using numerical run up gauges on the unstructured grid to that end 5 polylines were added to the model fig 2 and at the start of the run an administration of crossed flow links was created per polyline the waterlevel value along each line for a model time step was then determined as the topographically highest flow link joining a wet and a dry flow node in each polyline s link administration as stated before the outcomes of runs 1 3 were compared with the model results of özkan haller and kirby 1997 fig 3 özkan haller and kirby 1997 used a collocated spectral method where solutions to the shallow water equations are determined on a temporally and spatially adaptive grid using global directionally decoupled polynomial approximations this resulted in a very high resolution in the cross shore direction in a natural way thus getting very accurate estimates for momentary coastlines at every time step fig 3 a shows the vertical run up normalised with the characteristic wave height h in function of normalised time t g h 0 l y runs 1 3 all show similar behaviour demonstrating the effectiveness of the absorbing boundary under different grid configurations the run with the rotated grid run 2 has a slight negative phase shift as compared to the run with the aligned rectangular grid showing a better comparison with özkan haller and kirby 1997 similarly to the structured models of roelvink et al 2009 and hubbard and dodd 2002 the unstructured model is only moderately capable of reproducing the secondary peak in the centre of the bay at t t 4 for profile t100 attributed by zelt 1986 to wave focusing of the headland reflections roelvink et al 2009 cites a staircase shoreline a lack of resolution and grid line oriented spatial derivatives as potential culprits for this diminished performance to test this hypothesis run 3 was included in our analysis the triangular grid has a finer resolution than the rectangular versions and moreover it has no strict directionality along grid lines in the solution process the time series corresponding to this run green line in fig 3 a show a better phasing of the vertical wave run up with the results of özkan haller and kirby 1997 as compared to the runs with rectangular grids this result hints at the interplay between local bed slope and the grid resolution as the main reason for the observed discrepancies the amplitude of the secondary water level peak is however still too small 10 difference in order to assess the grid resolution effect in more detail we performed a grid convergence test following celik et al 2008 therefore three additional runs were made with uniform resolutions of 0 04 0 08 and 0 16 m and the maximum run up value was used to evaluate the performance of the runs the apparent order of grid convergence is 1 1 showing that the discretisation method to determine the shoreline position is slightly more accurate than first order the numerical uncertainty of the maximum run up for the finest grid is about 8 the time series of points located away from the bay centre line show good correspondence with özkan haller and kirby 1997 with run 3 slightly outperforming both runs with rectangular grids table b 3 fig 3 b shows the maximum and minimum normalised vertical run up ζ α h 0 in function of the normalised longshore position y l y all three runs overpredict the run up by about 2 and the error slightly increases toward the edges of the domain all maximum shorelines show a serrated pattern away from the bay centre resulting from staircasing on the relatively steeper slope run 3 reproduces the maximum shoreline position in the central part of the bay the best although its maximum is higher than that of the reference data all runs slightly underpredict the maximum run down by 5 with run 3 performing the worst the staircasing effect is less pronounced for the run down as the bed level gradients are smaller in the deeper areas of the model overall based on the error metrics the reproduction of the run up run down patterns is judged satisfactory for all three model configurations table b 3 and comparable to results from similar modelling approaches hubbard and dodd 2002 roelvink et al 2009 it should be noted that the present case study simulates run up using the non linear flow solver only when combining the flow and the wave solver in a model setup the run up part related to the short waves cannot be modelled as we only resolve the wave group and not the individual waves 3 2 boers flume study the performance of the model in generating suitable wave boundary conditions and in simulating wave driven flow over a barred profile is tested by comparing the numerical results with data measured by boers 1996 although the case does not leverage the specific benefits of using unstructured grids it demonstrates the correct implementation of the model equations boers 1996 reproduced the conditions of the lip11d tests reported in sánchez arcilla a roelvink et al 1994 on a fixed bed in the large wave flume at delft university of technology the test conditions were froude scaled with a factor of 5 6 relative to the lip11d experiment the wave flume is 40 m long and 0 8 m wide and has a maximum depth of 1 05 m the wave generating paddle board is of the hydraulically driven piston type with second order steering and active reflection compensation to remove free reflected waves from the flume during the run the model bed was constructed from sand topped by a concrete layer to smoothen the surface and mimicked a nearshore profile with a single bar and a surf zone trough fig 4 the spatial axis of the measurements has its origin at the toe of the profile slope 4 5 m from the wave generator boundary time series were constructed by repeating a short cycle with a length of 75 times t p and applying a f p dependent high pass filter to compensate for paddle board artefacts by repeated experiments using the same boundary conditions detailed hydrodynamic measurements were made at 70 locations throughout the flume at 20 hz the experimental parameters represent different wave steepnesses related to the original goal of the lip11d tests to measure during erosive and accretive conditions during tests 1 a and 1b waves were breaking throughout the flume during test 1c the incident short waves shoaled before breaking on the bar during tests 1b and 1c low frequency lf energy increased steadily toward the shoreline test 1 a shows shoaling of the low frequency waves just before the bar and a decrease of their amplitude in the trough the measured values for h s and t p at the toe of the profile and the cycle duration per condition are listed in table 1 the numerical model was set up with a mesh of 3 by 317 square grid cells the grid resolution is 0 1 m so the model effectively covers the flume from the toe of the profile slope x 0 m to the end of the constructed profile x 31 7 m the measured bathymetry is imposed in the cell centres this bathymetry was equal for all runs on the offshore boundary corresponding to x 0 m in the flume a water level of 0 0 m was imposed the maximum still water depth in the flume was 0 75 m time series for the short waves and the associated bound long wave were derived in two ways firstly second order time series were constructed from a pierson moskowitz spectrum using the parameters listed in table 1 following the methodology outlined in section 2 3 a standard jonswap peakedness factor γ 3 3 was judged to be too large when comparing theoretical spectral shapes to the measured spectra at the toe of the profile boers 1996 secondly first order measured time series were imposed on the model boundary the associated bound long wave is computed in the model using the formulations of longuet higgins and stewart 1964 the bed shear stress was imposed using a chézy parameter of 75 that was uniform over the length of the flume the model was run for 30 minutes and output wave height h and water levels ζ were recorded with a frequency of 20 hz similar to the flume tests data acquisition from these time series the following metrics were derived setup ζ h m 0 h f h h m 0 l f 8 ζ f ζ f 2 where ζ f is the low pass filtered f p 2 f p 20 water level and h is the modelled significant wave height denotes time averaging over the run duration model results for the measured m and spectral s input wave time series are compared with the data as measured by boers 1996 in test 1 a the setup is overpredicted by the same amount by both runs fig 5 a both m and s runs capture the shoaling and subsequent dissipation of the lf wave in the surf zone fig 5 c in test 1b the m run overpredicts the incident band wave height in contrast the s run is able to reproduce the linear decrease in the zone seaward of the breaker bar fig 5 e both m and s runs underpredict the lf wave height fig 5 f in test 1c both m and s runs reproduce the setup and the short wave height accurately fig 5 g h the s run slightly overpredicts the lf wave height compared to the m results fig 5 i overall the model is capable of reliably reproducing the wave height and setup patterns table b 2 for both measured and spectral input wave time series the results are somewhat less precise for the steeper wave conditions characteristic of test 1b 3 3 delilah field experiment in this section the focus is on the 2d propagation and dissipation of directionally spread short waves and the associated forcing of surf zone circulation more specifically the implementation of the stationary refraction single direction approach of roelvink et al 2018 will be tested this approach was designed to counteract the disintegration of modelled wave groups when they are advected over longer distances in our code this is done by combining the results of two separate unstructured solvers the stationary wave module feeds the mean wave direction to the non stationary short wave driver which advects the actual wave groups section 2 1 4 to this end we constructed two models one where we propagate the wave groups using multiple directional bins mdir and one where we propagate the wave groups along the mean wave direction only sdir we use data of the delilah field experiment held in october 1990 in duck north carolina at the us army field research facility birkemeier et al 1997 for verification similarly to previous model validation studies van dongeren et al 2003 roelvink et al 2009 2018 the data run of october 13th 1990 from 16 00 to 17 00 was selected the windless swell conditions were the result of hurricane lili and were characterised by a significant wave height h s of 1 81 m and a peak period t p of 10 6 s in 8 m water depth these conditions are energetic enough to generate a significant contribution to low frequency portion of the energy spectrum f p 2 and the incident wave spectrum is sufficiently narrow banded to justify the assumptions in the generation of the wave boundary conditions fig 6 b section 2 3 the incident wave angle was 88 from n corresponding to 16 from shore normal the local wind conditions were very mild with a wind speed of 2 m s from the ne birkemeier et al 1997 3 3 1 model setup the model is set up using an unstructured grid with dimensions of 885 m cross shore by 700 m longshore the overall resolution of the grid is 10 m by 10 m with a local refinement to 5 m in the longshore and cross shore directions from the edge of the surf zone seaward of station 90 to the coastline the refinement area covers a longshore distance of 260 m centred on the measurement transect the bathymetry in the model is composed of data measured on october 12th and 13th as the october 13th survey only covered the bathymetry between the dry beach and the surf zone trough birkemeier et al 1997 the bed level data was interpolated in the flow nodes and the bathymetry was made uniform in the first three rows of cells along the offshore boundary in order to satisfy the conditions for the validity of the equilibrium theory of hasselmann 1962 a stationary offshore water level of 0 69 m ngvd was imposed on the offshore boundary the 2d absorbing generating boundary was forced with times series generated from the measured spectrum at 8 7 m water depth the directional spreading factor s of the spectrum is about 6 roelvink et al 2018 the lateral flow boundaries were forced with zero gradient neumann conditions we used the roelvink daly breaker formulation daly et al 2012 to calculate energy dissipation through wave breaking this was shown by roelvink et al 2018 to substantially improve the cross shore variation of the short wave heights and the longshore current in this test case the breaker parameter γ was set to 0 52 and a chézy bed roughness value of 65 m 0 5 s was used all other parameters were set to their default values the multiple direction run mdir fig 7 was run in standard mode with a directional resolution of 5 the single direction run sdir was set up with that same directional resolution for the stationary wave model mean wave directions obtained from the stationary calculation were fed to the surfbeat module every 10 min time series of wave characteristics and water levels were recorded at 1 hz in the model across a dense cross shore profile in order to obtain results that are compatible with the delilah dataset the demeaned water levels ζ were bandpass filtered to obtain the low frequency signal ζ l f 0 004 0 0495 hz and the high frequency signal ζ h f 0 0495 0 3 hz the corresponding rms lf wave height was then obtained by h r m s l f 8 ζ l f 2 the rms short wave height h r m s h f as h m o d e l l e d 2 8 ζ h f 2 and the longshore velocity as v with v the cell centre y component of the eulerian velocity vector as our grid is aligned with the coastline no further rotation is needed 3 3 2 results the groupiness factor gf list 1991 was calculated for the sdir fig 7 a run and the mdir fig 7 b run the groupiness factor gf is defined as 26 g f 2 σ 2 h r m s h f h r m s h f where σ denotes the standard deviation clearly the single direction run is able to maintain the wave group structure much better than the multiple directions run the wave group structure is advected with less numerical dissipation up until the edge of the surf zone with improved surf zone lf wave heights as a result fig 8 c the cross shore evolution of the hf wave height the lf wave height and the longshore current velocity obtained from the two runs were compared to the measured data fig 8 both the mdir and the sdir run capture the short wave height distribution and dissipation in the surf zone well although the shoaling process seaward of the sandbar is underestimated by both models moreover neither model captures the wave energy dissipation between stations 20 and 10 fig 8 b the lf wave height is simulated well using both sdir and mdir approaches compared to previous results of roelvink et al 2009 2018 in xbeach the sdir run reproduces the lf wave height distribution in the surf zone better only the shoreline value station 10 is overestimated but the overall pattern and magnitude match well with the data fig 8 c this finding is corroborated by the low frequency spectrum plots in fig 9 the lf energy modelled in the sdir and mdir runs generally corresponds well to the measurements except in stations 10 and 70 where the lf energy is overestimated around the peak ig frequency the peak longshore velocity is underestimated by both runs by about 30 table b 4 the longshore current distribution and magnitude proved to be extremely sensitive to the spatial and directional grid resolution of the model when using comparable settings as roelvink et al 2018 the model underpredicted the current velocity magnitude with an error of 75 overall the model is reproducing the hf and lf wave patterns well with the standard settings for the bottom roughness chézy value of 65 m 0 5 s that we have used here table b 4 regarding performance the use of unstructured grids allowed a reduction of the number of grid cells with 65 compared to a run on full 5 by 5 m resolution reducing the runtime with a factor 2 2 for the mdir setup using the stationary refraction approach this improved further to a factor 5 9 with the added benefit of better preservation of the wave group structure and slightly better lf wave height prediction in the surf zone 3 4 coast3d egmond field experiment the coast3d egmond field case aims at reproducing the hydrodynamics during a storm event in a double barred nearshore area when local ig wave generation was important the beach experiments in egmond aan zee were part of a multi institution set of field campaigns in the netherlands and the uk in 1998 1999 within the framework of the eu mast iii funded coast3d project soulsby 2001 the aim of the project was to collect a comprehensive dataset of hydrodynamic and morphological observation that would allow to test the accuracy of morphodynamic models under field conditions the meso tidal beach and nearshore at egmond feature an intertidal swash bar and two offshore breaker bars that are intersected by rip channels in this test case we use data from the main campaign in october november 1998 during this field experiment an large array consisting of 32 stations intermittently measured waves currents water level and bed dynamics throughout a number of storms ruessink 1999 breaker bar movement was tracked 22 times during the field campaign up until nap 6 m with dgps positions of a 15 m high amphibious buggy called wesp that drove cross shore tracks at alongshore intervals of about 50 m the deeper parts of the nearshore were measured once before the main campaign on september 1st 1998 using a hydrographic vessel of rijkswaterstaat klein et al 2001 hydrodynamic data is available from an array of pressure sensors located on the inner breaker bar stations 1a 1d 7b 7e and a pressure sensor on the outer bar station 2 fig 11 a the deep water wave conditions during the field experiment were measured by a directional waverider buoy station 8 located in 15 m water depth about 5 km offshore of the field site comparison with nearby operational wave measurement locations of rijkswaterstaat showed insignificant wave energy dissipation between stations deeper than 30 m and the location of station 8 and any gaps in the station 8 time series were reconstructed using these ancillary wave data vermetten et al 2001 tide levels at the field site were derived from the tide gauges at ijmuiden and den helder ruessink et al 2001 the selected depth short and infragravity wave data were recorded on october 25th 1998 between 01 00 and 13 00 gmt and cover one tidal cycle fig 10 this time frame corresponds to the first storm peak during the measurement campaign tidal water levels vary between 0 4 and 1 75 m nap the significant wave height at station 8 varied between 3 0 and 4 2 m wave peak period were between 8 7 and 9 8 s notably over the course of the tide the incident wave angle shifted from sw to nw the last available wesp bed level measurements before the storm were recorded on october 24th 1998 and were used in the model setup 3 4 1 model setup the model grid is a progressively refined unstructured grid of 2500 m in the longshore and 1600 m in the cross shore direction aligned with the local coastline orientation the grid resolution at the offshore boundary is 20 m onshore of the 8 m contour and toward the centre of the domain the resolution is 10 m and onshore of 6 m around the observation stations the resolution is further reduced to 5 m fig 11 this choice of resolution allows to accurately capture the wave group propagation and dissipation processes resulting in a grid of 32286 cells the model was set up in multi directional mode with a directional resolution of 5 to compensate for the difference in cross and longshore bed level data availability and to avoid triangulation artefacts the wesp track data of october 24th were converted to a bed level sample set using the methodology of thanh et al 2020 and subsequently interpolated on the unstructured grid cell corners bathymetry in areas not covered by the wesp measurements was interpolated from the offshore bed level data of september 1st 1998 and from the regular monitoring dataset of the dutch government the so called vaklodingen the boundary conditions were taken from vermetten et al 2001 wave boundary conditions were imposed as a time series of hourly parametrised jonswap spectra with a peakedness and cosine spreading factor of 2 4 and 4 respectively the wave buoy at station 8 malfunctioned after october 25th 2 00 and the remainder of the time series of wave bulk parameters was derived from the operational buoy network data of rijkswaterstaat by wave height class dependent interpolation vermetten et al 2001 the offshore model boundary is a 2d absorbing generating boundary forced with the tidal water level and the ig volume fluxes by calibration on the data of october 25th 1998 09 00 table b 5 the bed roughness was fixed at a global chézy roughness parameter of 65 m 0 5 s we applied the daly et al 2012 wave breaking formulation with a gamma value of 0 48 and alpha 1 1 the roller model was switched on with the roller slope set to 0 1 wave current interaction was switched off any other parameters were left to their defaults the model was then run on 6 parallel partitions for the 13 h model validation period model results are extracted as time series at the instrument locations where pressure data was available for the entire duration of the model run modelled data were saved at a frequency of 2 hz and hydrodynamic parameters are calculated from 1800 s bursts over the frequency bands reported in ruessink 1999 in order to match the field data the demeaned water levels ζ were bandpass filtered to separate the low frequency signal ζ l f 0 004 0 05 hz from the high frequency signal ζ h f 0 05 0 33 hz the corresponding rms lf wave height was then obtained by h r m s l f 8 ζ l f 2 the rms short wave height h r m s h f as h m o d e l l e d 2 8 ζ h f 2 3 4 2 results water depth and h r m s h f are well reproduced by the model in all the observation points except in station 7b fig 12 cross shore wave dissipation patterns are well modelled over the course of the tidal cycle the rms error in the estimates of both parameters is smaller than 10 of the observed value sci fig 12 a n both water depth and hf wave height are systematically overestimated all model errors are within the bounds of the measurement errors estimated at 0 05 0 15 m for the water depths and 15 for the hf wave heights in breaking wave conditions van rijn et al 2000 moreover the water depth bias is in the order of the measurement uncertainty of the bottom level data as reported by van rijn et al 2000 in station 7b the water depth is on average overestimated by 0 22 m and the wave height by 0 3 m the position of station 7b is aligned with a rip channel through the inner breaker bar which may explain part of the discrepancies between model and data the nature of the available bed level data does not allow to accurately resolve the alongshore bed level variability required to reproduce the rip channel geometry so the model bathymetry may be too deep at that location the larger water depth then allows for larger wave heights under breaking wave conditions this however would only allow an overestimation of around 0 1 m given the maximum permissible wave height over water depth ratio of 0 48 we likely underestimate the wave breaking on the outer bar onshore of station 2 yielding larger wave heights in the trough between the outer and the inner bar further evidence in this direction is that according to the data waves in station 7b are never breaking h r m s h f h 0 35 throughout the tide while the waves in the model do break h r m s h f h 0 45 throughout the tide however we have no data on the outer bar top to verify this although station 7b is close to a rip channel the lack of wave current interaction in our model does not play a role in the overestimation of the wave height as that would yield an opposite effect weir et al 2011 the comparison between modelled and measured h r m s l f values shows substantial scatter but the model error is generally expected to deviate 20 or less from the observed values sci fig 12 o u error metrics for the lf wave height in station 7b show values that are comparable with those of the other observation stations in order to explain part of this scatter we inspected the spectra of stations 2 and 1a d on october 25th 1998 07 00 fig 13 these spectra show that the ig peak period is predicted reasonably well and that the ig energy around the ig spectral peak is overpredicted the model however underpredicts the energy content for ig frequencies higher than 0 025 hz a factor playing a role in this underprediction is the absence of free long waves in the boundary forcing a bispectral analysis of the water level data in station 2 for this burst and using the method of herbers et al 1994 their eq 5 and 6 reveals that approximately 49 of the total ig energy content was directly forced by wave groups the remaining 51 is free ig energy that originates from outside the model domain or from local reflection off the coastline overall this field case demonstrates that the model can reliably reproduce the hydrodynamic conditions in a multi bar system during an energetic wave event water depths and short wave fields are simulated to within measurement accuracy while the ig wave height is mostly underpredicted due to the lack of a mechanism to transfer energy between hf and lf bands in the shoaling and outer surf zone and because of the missing free long wave contribution in the boundary forcing regarding efficiency a comparable xbeach model with the same 5 m resolution over the surf zone has a grid size of 51712 cells factor 1 6 higher and ran with a timestep of 0 11 s 45 smaller for the same cfl value of 0 7 3 5 pilot field experiment in this verification case we have tested the parallel implementation of the surfbeat model on the 2d field case of a fringing reef in guam which is a challenging study area featuring longshore bathymetry variability with very steep gradients a narrow surf zone and large bottom roughness the pilot field experiment is a long term deployment 2005 present of a number of measurement arrays of pressure sensors and current meters in varying reef environments one of the field sites is at ipan on the se coast of guam péquignet et al 2009 vetter et al 2010 péquignet et al 2014 clark et al 2020 the reef is situated between two small headlands and it is about 7 km long and 400 m wide it is incised by two cross reef channels up to 30 m deep fig 14 b which drain small watersheds the reef flat is fairly featureless consisting of algae covered coral with interspersed sandy patches the reef crest is more shallow and rougher the reef front is very steep and features spur and groove morphology péquignet pers comm clark et al 2020 the selected data was recorded on january 10th 2012 between 7 00 and 7 30 by an array of seabird sbe26 pressure sensors mt5 mt7 mt10 jc2 jc5 and aquadopp velocity profilers jc1 jc3 jc4 becker pers comm fig 14 a the offshore hm0 measured by coastal data information program cdip buoy 121 located in 200 m water depth 2 km se of the study site was 3 18 m with a t p of 10 5 s incoming from 56 n the spectrum is narrow banded so it is suitable for our approach to generate wave boundary conditions cdip 2021 the wind velocity measured in the nearby pago bay noaa station 1631428 noaa 2020 was 3 0 m s coming from 48 n the offshore tidal level was 0 3 m w r t msl section 3 5 1 3 5 1 model setup the grid and the bathymetry of the model need to fulfil several requirements for this case the area has a very shallow reef flat 0 5 m below msl fronted by a steep reef front that quickly drops to depths of more than 50 m the cross reef channels are 30 m deep the wave breaker zone is narrow but should be captured at high resolution to model the wave energy dissipation profile correctly as this dissipation forces wave setup and wave driven currents the offshore area on the other hand should be modelled with only enough detail to resolve the structure of the wave groups the fine resolution needed in the breaker zone should not affect the offshore part of the grid neither should this refinement be used around the reef channel to minimise cfl based time step restrictions after several tests the offshore grid resolution was fixed at 35 m the reef front area is refined up until 5 m and the reef flat is kept at the same 5 m resolution at the location of the cross reef channels the grid is derefined to 10 m to alleviate time step restrictions fig 14 the final unstructured model grid has 121 989 flow nodes and was run on 20 parallel domains the bathymetry used in the model was collated from 2 datasets the jalbtcx topobathy lidar dataset noaa 2018 provided data on the reef flat at 1 m resolution or better offshore areas not covered by this lidar data were filled using the guam 1 3 arc second mhw coastal digital elevation model noaa 2008 a 10 m resolution dataset for tsunami modelling both datasets were referenced to msl and interpolated to the flow nodes of the model grid the offshore depths were limited to a maximum of 30 m to reduce potential timestep restrictions as second order long wave forcing is expected to be very small in these depths we do not expect significant deterioration of the results a spatially varying field of chézy roughness parameter values was constructed based on the satellite derived benthic habitat map of guam nccos 2004 where coral patches got a roughness of 40 m 0 5 s and sand patches and algal turfs a roughness of 50 m 0 5 s this is somewhat less than the roughness derived by clark et al 2020 which would correspond to a roughness of about 31 m 0 5 s but it corresponds well with the value of 40 m 0 5 s obtained by vetter et al 2010 by doing a cross shore momentum balance during energetic conditions the wave friction factor which was shown previously to be important on reefs van dongeren et al 2013 was set to 0 45 which is somewhat less than the value of 0 60 used by van dongeren et al 2013 to model reef circulation in ningaloo australia the model was run with wave current interaction switched on as the cross reef channel outflow is likely to have an effect on the local short wave propagation initial runs forced with cdip only wave data consistently resulted in overestimation of the wave heights in the model as such wave height and period on the offshore boundary were taken from measurement location mt10 at 8 m water depth and only the wave direction from cdip buoy 121 the wave height at the buoy is typically 130 of the mt10 value in order to derive an offshore water level boundary signal a regional tide model was constructed forced with tpxo8 data egbert and erofeeva 2002 the tidal model was calibrated on tidal constituents at the pago bay station 1631428 just n of our study site and apra harbor station 1630000 noaa tide gauges noaa 2020 to explore the necessity of including potential larger scale water level forcing non tidal residuals ntr at the pago bay station were determined from the measured time series at the noaa station using a godin filter thompson 1983 the ntr s were found to correlate highly with the incident wave conditions measured at station mt10 r 2 0 66 not shown indicating the dominant influence of wave setup on the reef flat next to the tide gauge on semidiurnal and longer time scales as such the ntr s were not taken into account in constructing the offshore water level boundary conditions as wave setup setdown is resolved in the model local wind direction and wind speed were sourced from nearby noaa measurement station pago bay station 1631428 noaa 2020 3 5 2 results comparison of the model results with the measured data of clark et al 2020 on january 10th 2012 at 7 00 shows that the short wave height h m 0 h f is well reproduced in the deepest observation point mt10 indicating little energy loss in the model between the deeper part of the domain and the shoaling zone fig 15 a the overestimation of h m 0 h f at station mt7 and the underestimation at mt5 demonstrate a somewhat too large dissipation in the breaker zone the short wave energy is almost completely dissipated shoreward of the breaker zone as evidenced by stations jc2 and jc5 this is well reproduced in the numerical results the model is able to simulate the low frequency wave height h m 0 l f trend in the cross shore direction fairly well stations mt10 mt5 fig 15 b but the low frequency wave height is overestimated toward the cross reef channel in the middle of the site station jc5 and toward the northern end of the reef flat station jc2 in fig 15 c the comparison between the modelled and the measured mean water levels taken as the sum of the offshore tide level and the steady wave setup is shown both the cross shore trend stations mt10 mt5 jc4 and the longshore trend jc1 jc3 mt5 jc5 are well reproduced comparison of the water level variance in the low frequency band 0 0011 0 0400 hz between the model and the measured data shows that the model is capable of simulating the bulk low frequency wave heights h m 0 l f moreover it has some skill in reproducing the energy repartition over the frequencies in the 0 0011 0 0400 hz band in this field case fig 16 the model somewhat underestimates the variance at frequencies higher than 0 02 hz and overestimates the energy content at frequencies below that threshold especially at station jc2 as in the coast3d case it is unknown what the amount of free ig wave energy is that is entering the model domain through the boundaries and that could contribute to the model data discrepancy we observe under stationary conditions ignoring wind stress and lateral exchange of momentum the depth averaged conservation of momentum in the cross and alongshore directions on the reef can be written as a balance between water level gradient wave forces and bottom friction 27 g ζ i f w i ρ h ζ τ b o t t o m i ρ h ζ 0 i x y in which f w is the wave force h is the mean water depth τ b o t t o m represents the bottom shear stress and x and y represent the cross and longshore directions respectively the cross and longshore momentum balance of the ipan reef on january 10th 2012 07 00 was analysed based on a 30 min time average fig 17 for the modelled conditions we find strong seaward currents through the two incised stream channels that cross the reef and through local depressions in the reef crest fig 17 d gradients in wave setup are steep throughout the narrow surf zone where almost all the ss wave energy is dissipated through intense breaking fig 17 a b on the reef flat proper cross shore radiation stress gradients are virtually non existing leading to limited cross shore setup gradients that are balanced by cross shore flows as evidenced by the bed shear stress patterns fig 17 c d on the reef flat the current patterns are defined by the positions of the deep cross reef channels fig 17 h at those channels cross shore wave setup is locally reduced as there is relatively less dissipation by depth induced wave breaking fig 17 b this leads to the development of the alongshore setup gradients that were reported by clark et al 2020 as steady wave setup is relatively uniform across the reef flat toward the shore a flow develops over the reef flat toward the channels fig 17 g h in this field case we simulated the propagation of a swell field onto a fringing reef flat we were able to reproduce the wave height distribution over the reef flat the local grid resolution adaptations clearly demonstrate the advantage of the present modelling approach as opposed to a curvilinear model like xbeach roelvink et al 2009 an equivalent xbeach model needed 370 000 cells to resolve the reef flat hydrodynamics at the same resolution using a maximum time step that was 40 smaller for the same cfl number of 0 7 3 6 buck island case the final verification case explores the capabilities of the model to replicate the propagation of wave groups over larger distances in a topographically complex area using multiple mpi domains and local grid refinements with boundary conditions derived from a large scale regional model based on global wave data buck island reef national monument birnm is located on the north east coast of st croix island part of the us virgin islands in the caribbean sea fig 18 the island coastline is protected from incoming swell waves from the atlantic ocean by the sequence of a narrow barrier reef complex surrounding the island to the nw ne and a shallow fringing reef surrounding the island s coastline the reefs are separated by a 10 m deep lagoon characterised by the presence of coral bommies which locally enhance the hydraulic roughness depths in the reef system vary between 1 0 and 15 0 m two multi month deployments in 2015 and 2016 of 8 rbrsolo pressure transducers on cross shore transects over the fringing reef collected water level and wave data rosenberger et al 2020a the raw pressure data was collected in bursts of 2048 s every hour at 2 hz to verify our model results we use the data collected between january 12th 2016 18 00 and january 13th 2016 18 00 gmt the largest measured swell event during the field campaign in the winter of 2015 in observation points n1 n4 on the northern fringing reef of the island transect n fig 18 b and s1 s4 transect s fig 18 c covering the southeastern fringing reef conditions during this time frame were characterised by small tidal water level variations between 0 05 m and 0 15 m w r t msl the swell event characterised by waves of around 2 m height with a peak period varying between 10 and 12 s was impinging on the island from the ne the northern n transect was directly exposed to this swell while the wave field had to refract around the island to reach transect s 3 6 1 model setup in order to resolve the various spatial scales involved in the propagation refraction and dissipation of the wave field a grid was constructed with several degrees of resolution refinement using similar considerations as for the pilot case the offshore part where short waves do not dissipate and the propagation of long waves is important has a resolution of 30 m the same holds for areas that do not directly influence the hydrodynamics measured in the two transects fig 18 a zone a over the shallower barrier reef and the lagoon fig 18 a zone b the resolution is increased to 15 m the fringing reef area fig 18 a zone c has a resolution between 7 and 3 m in order to resolve short wave dissipation and ig generation by breakpoint forcing typical of reef lined coasts pomeroy et al 2012 péquignet et al 2014 the final number of grid cells is 50999 the directional resolution was set to 5 similarly as for the pilot case the bed levels used in the model were sourced from 2 datasets the high resolution usgs eaarl b topo bathymetric lidar dataset for st croix fredericks et al 2015 was used to interpolate the bathymetry in the fine resolution parts of the grid where large bathymetric gradients occur over limited spatial scales the grid parts not covered by the lidar dataset were filled with 1 3 arc second mhw coastal digital elevation model data for the us virgin islands love et al 2015 bed level data was reduced to msl using the tidal benchmark data of the christiansted harbor noaa station noaa 2022 the offshore depths n of the barrier reef were limited to a maximum of 30 m to reduce timestep restrictions the hydraulic roughness in the model was specified as a spatially varying field both for the flow and the wave related friction therefore a bed type classification was derived from the birnm benthic habitat data of batista 2012 in which we retained 3 classes for which we differentiate bottom friction parameters f w and chézy values following storlazzi et al 2017 a sandy patches f w 0 1 and chézy 55 m 0 5 s b patch reef with 10 coral cover f w 0 15 and chézy 30 m 0 5 s c patch reef with 50 coral cover f w 0 3 and chézy 20 m 0 5 s offshore wave data were not available at the time of the field campaign therefore a nested regional swan booij et al 1999 version 41 31 wave model was constructed to derive wave boundary conditions for the surfbeat model we included the effects of wind wave growth refraction depth induced wave breaking battjes and janssen 1978 whitecapping van der westhuysen et al 2007 and bottom friction hasselmann et al 1973 with default parameters the large swan domain covers the atlantic north of the british virgin islands until the caribbean sea south of st croix at a uniform resolution of 1500 m the finer nested domain has a resolution of 200 m and covers the north coast of st croix the swan model was forced with era5 hersbach et al 2020 wind and wave data and validated against 5 months jun 1st oct 31th 2010 of buoy data at christiansted cdip station 431 fig 18 a significant wave heights were estimated with a bias of 0 0 m and a rmse of 0 20 m and the t m01 with a bias of 0 23 s and an rmse of 1 24 s the swan model was subsequently run for the period january 11 15 2016 and hourly wave parameters were extracted at the n side of the barrier reef to construct parametric jonswap time series to force the surfbeat model fig 19 b d the water level signal to force the n absorbing generating model boundary was taken from station n1 as is fig 19 a as comparison with a tidal harmonics fit showed considerable ntr influence uncorrelated with the incident wave climate local wind was insignificant during the model period noaa 2022 and was not taken into account the model was run on 20 partitions modelled water levels and wave data were recorded continuously at 2 hz in the locations of the 8 deployment sites in order to obtain results that are compatible with the usgs dataset hourly 2048 s bursts of the demeaned water levels ζ were bandpass filtered to obtain the low frequency signal ζ l f 0 004 0 04 hz and the high frequency signal ζ h f 0 04 0 4 hz the corresponding rms lf wave height was then obtained by h r m s l f 8 ζ l f 2 the rms short wave height h r m s h f as h m o d e l l e d 2 8 ζ h f 2 3 6 2 results the mean water level variability in the observation stations is well reproduced in the model fig 20 a h the tide levels in stations n1 and s1 are simulated correctly with small biases the cross reef distribution of the sea swell wave setup values over both transects is modelled correctly as is the difference in setup magnitude between the n and the s transect sea swell ss wave heights are modelled with an absolute bias and a rmse of 0 05 m significant amounts of ss wave energy 50 are dissipated over the barrier reef and the lagoon inspection of the spatial distribution of the ss wave dissipation fields from the model output suggests that this wave height reduction occurs mainly through bottom frictional effects fig 20 i p the resulting reef face wave heights in n1 are about twice as high as those in the more sheltered s1 station significant ss wave breaking only takes place around the crest of the fringing reef for the wave heights considered in this study as a result n1 and s1 ss wave heights are tide independent but hf wave heights on the reef flat and in the fringing reef lagoon n2 4 s2 4 are largely determined by the local water depth over the reef crest modelled lf wave height error statistics are similar in magnitude as the hf ones and errors are generally smaller than 20 except for station s1 fig 20 q x inspection of the spatial distribution of the water level variance in the model domain indicates two sources of local surfbeat generation on the relatively gently sloping barrier reef ig waves are generated as a result of ss wave energy dissipation through bottom friction as wave breaking is generally absent on the fringing reef face and reef crest ig waves are generated through the breakpoint forcing mechanism normalised bed slope parameter β b battjes et al 2004 equal to 2 the model is able to reproduce the relative cross reef spatial pattern in ig wave heights on both transects including the higher ig values observed in the two stations closest to the shoreline n4 an s4 which result from long wave reflection off the steep shoreline fig 20 q x suggests a tidal dependence of the ig wave heights on transect n in the measured data this was verified not to be the case from the complete measured time series in station n1 in conclusion this test case demonstrates that the model is able to reproduce the propagation and dissipation of sea swell wave fields and the associated ig wave dynamics on larger spatial scales in a complex model domain using boundary conditions generated from a global wave and wind database as for efficiency the average achieved timestep in this unstructured surfbeat run was 0 14 s for a cfl condition of 0 7 comparison with a structured xbeach model setup for the same problem shows an improvement of the timestep with roughly 40 and a reduction in grid size by a factor of 2 9 while attaining a spatial resolution on the fringing reef that was 2 times better 4 summary and conclusions this paper presents the first numerical model defined on an unstructured mesh administration that is able to resolve hydrodynamics forced by tides wind and sea swell waves on the time scale of wave groups we have validated different functionalities in a series of six tests showing that the model has good skill in reproducing measured laboratory and field data as well as results from semi analytical model approaches the zelt test case was included to verify the correct propagation of long waves flooding and drying and the robustness of the offshore flow boundary in absorbing reflected free waves leaving the model domain under highly oblique angles additionally the implementation of the drying and wetting algorithm was compared favourably to results from a temporally and spatially adaptive grid modelling approach results proved to be robust under different grid configurations the use of a triangular grid alleviated grid influences in the phasing of the run up solution the boers test demonstrated the model s capability of generating first and second order boundary forcing the functioning of the 1d absorbing flow boundary the correct implementation of dissipation by wave breaking and of the wave forces in the momentum balance the laboratory data were in general well reproduced although the simulation of the steepest wave conditions proved more challenging as we are using linear wave theory the delilah field case served as a verification for the stationary refraction single direction approach which serves to counter the numerical diffusion of wave groups when advecting wave action in multiple directional bins over longer travel distances comparing the single direction and the multiple directions results to data measured at the duck field research facility the former method yields better results in reproducing the infragravity energy profile in the surf zone and is better at retaining the wave group structure the methods are on par in simulating the short wave distribution and the longshore current over the cross shore profile combined with the possibility of localised refinement by using unstructured grids the stationary refraction approach offers the possibility of significantly speeding up the calculations without loss of reliability the coast3d field case reproduced the hydrodynamic response of a mildly sloping double barred sandy beach to a storm mean water depths and short wave heights were well reproduced during highly non stationary conditions where incident wave heights and directions changed strongly over the course of a tide despite the fact that ig mean periods were correctly modelled the ig wave height was underestimated due to the lack of a mechanism to let free ig enter through the domain boundaries the pilot field case demonstrated the model s robustness in a challenging study area featuring longshore bathymetry variability with very steep gradients a narrow surf zone and large bottom roughness using the parallel implementation of the model the cross shore and longshore variability of the short waves the infragravity energy and the steady wave setup were skilfully resolved finally the buck island field case reproduced the propagation of an energetic swell field around an island protected by 2 reefs on both the exposed measurement sites and the sheltered ones bulk wave characteristics were correctly modelled starting from boundary conditions derived from a global reanalysis dataset demonstrating the applicability of our approach in typical engineering studies the use of this unstructured wave model allows for coverage of the same model domain with optimised resolution yet with a smaller total number of grid cells the model also allows a larger explicit average timestep for the same value of the cfl criterion compared to structured models with similar functionality the model reliably reproduces observations in laboratory and field conditions and is as such widely deployable in a variety of simple and complex coastal settings to study nearshore hydrodynamics credit authorship contribution statement johan reyns conceptualization software validation formal analysis writing original draft robert mccall software roshanka ranasinghe writing review editing ap van dongeren writing review editing funding acquisition dano roelvink conceptualization software writing review editing funding acquisition declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to acknowledge sander van der pijl for his assistance in the development of the software janet becker and mark merrifield provided the pilot data and are thanked for fruitful discussions on the ipan reef dynamics bart grasmeijer provided the coast3d data and reports ellen quataert substantially improved a first version of the birnm xbeach model permission to use data provided by the field research facility of the u s army engineer waterways experiment station s coastal engineering research center is appreciated funding this research was supported by the pacific community spc new caledonia under the wacop project by deltares the netherlands under strategic research programmes natural hazards and knowledge foundation software and models and by the office of naval research usa under award number n00014 17 1 2459 work on the pilot model benefited from two research visits to uh appendix a overview of implemented formulations for wave energy dissipation by breaking see table a 1 appendix b performance metrics verification runs b 1 error metrics see table b 1 b 2 boers flume tests see table b 2 b 3 zelt tsunami run up see table b 3 b 4 delilah field experiment see table b 4 b 5 coast3d egmond campaign see table b 5 b 6 pilot field experiment see table b 6 appendix c implicit stationary wave energy solver in order to determine the mean wave direction over the unstructured grid while taking into account refraction and dissipation we solve the simplified wave energy balance eq 28 in wave direction θ 28 e e t c g e e s c θ e e θ d d 0 where e e is the spectral wave energy density per node and wave direction t is time c g is the group velocity s is the distance along the wave direction under consideration c θ is the refraction velocity θ is the wave direction and d d is the local wave dissipation eq 28 is discretised with finite differences at the location of the network nodes using a forward euler scheme in time and an implicit first order upwind approach in geographical and directional space after rearranging the terms in the equation this results in a tridiagonal coefficient matrix and a right hand side that is a function of the wave energy at the previous timestep e e k n and the already updated wave energy at the upwind neighbours e e k u p w n 1 this system can be solved with standard methods such as the thomas algorithm press et al 2007 on open wave boundaries a dirichlet wave energy condition is applied while on other open non wave boundaries a zero gradient neumann wave energy condition is imposed the system of equations is solved consecutively along four orthogonal sweep directions starting along the incident wave direction on the open model boundary local wave energy values are determined from partially converged upwind node values by iterating the sweeping process and applying relaxation we can solve for the effect of the non linear dissipation term d d a user defined criterion determines when the solution process has converged after interpolation to the flow administration cell centres the mean wave direction is determined from the directional distribution of the wave energy 
