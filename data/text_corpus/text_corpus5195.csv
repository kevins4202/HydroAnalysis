index,text
25975,with the rapid development of the internet of things iot and big data infrastructure crowdsourcing techniques have emerged to facilitate data processing and problem solving particularly for flood emergences purposes a flood analytics information system fais has been developed as a python web application to gather big data from multiple servers and analyze flooding impacts during historical and real time events the application is smartly designed to integrate crowd intelligence machine learning ml and natural language processing of tweets to provide flood warning with the aim to improve situational awareness for flood risk management fais a national scale prototype combines flood peak rates and river level information with geotagged tweets to identify a dynamic set of at risk locations to flooding the prototype was successfully tested in real time during hurricane dorian flooding as well as for historical event hurricanes florence across the carolinas usa where the storm made extensive disruption to infrastructure and communities keywords internet of things flood analytics information system machine learning big data flood situational awareness the carolinas 1 introduction the south and southeast united states us are subjected to a series of intense storms throughout the year as well as deadly atlantic hurricane events during hurricane season june november these events can happen in quick succession 2 weeks apart and produce catastrophic flooding in wide geographic areas 1000 km swath and within short time spans less than a 48 h period as a consequence of these successive events many lives were lost and numerous critical infrastructure and communities were vastly disrupted to reduce the risk of damages accurate and real time flood assessment is critical for emergency management and to improve two way communication and understanding of potential impacts at present national weather service nws and national hurricane center nhc provide river and storm path forecasts and issue early warning system for potential areas of flooding however it is acknowledged that these forecasts are large scale and have less skills with respect to detecting localized floods and identifying specific areas at risk of flooding samaniego et al 2017 adams and dymond 2019 flooding in the south and southeast regions are often highly localized and intense which cause inundation in low lying roads and poor drainage areas and the level of individual properties e g philips et al 2018 furthermore providing geographically targeted early flood warnings in time is hampered by a lack of data and real time information for stakeholders and residents to take protective actions for themselves their property and livestock improved data collection and real time assessment of at risk locations allow more efficient mutual aid in the operational theater for warnings and evacuations and more effective search and rescue plans while enabling automatic dispatching of relief resources and evacuation plans further development validation and implementation of viable and accurate flood warning systems requires a step change in the methodologies used for data collection and analysis with the rapid development of earth observation technology and ground based monitoring systems that produce time lapse videos and images high spatial and temporal big data have been recently tapped into flood early warning assessment e g barker and macleod 2019 although these images require innovative enabling technologies to improve the integration retrieval analysis and presentation of large amounts of information grolinger et al 2013 nativi et al 2013 smart technologies such as internet of things iots image processing and machine learning ml can provide the intelligence to analyze real time data and alleviate information overload for flood early warning system e g rao et al 2017 iot is one of the fastest developing fields in the history of computing with an estimated 50 billion devices by the end of 2020 al garadi et al 2018 that can integrate billions of smart devices to communicate with one another with minimal human intervention the connectivity in iots and high speed data transfer capabilities can be used to implement real time image processing and ml analytics system for flood risk studies both image processing and ml algorithms are powerful methods of data exploration for real time monitoring and learning about normal and abnormal condition of a watershed system recently image processing and ml algorithms have been demonstrated to label time laps camera imagery crowdsourcing tabular data and user generated texts and photos to extract road flooding inundation extend and depth de albuquerque et al 2015 starkey et al 2017 feng and sester 2018 barker and macleod 2019 feng and sester 2018 2018 erfani and samadi 2019 in addition analysis of social geodata during floods can provide actionable intelligence to assist first responders to identify at risk communities stakeholders need place based geotagged and crowd sourced information about flooding being analyzed rapidly in real time accessibility to voluntarily generated and often publicly published content on social networking and social media provides a strong draw for disaster related research crowdsourced social media data particularly twitter is increasingly used to improve situational awareness and two way communication during hurricane and flood events e g kryvasheyeu et al 2016 barker and macleod 2019 social crowdsourcing data can help with the identification of flood extend especially during pluvial and fluvial flooding in urban settings e g smith et al 2015 eilander et al 2016 arthur et al 2018 developing a pipeline to gather the data and identify tweets relevant to flooding proved to be useful to assess real time flooding impacts and damage in sao paulo brazil de assis et al 2016 jakarta indonesia eilander et al 2016 the river elbe germany herfort et al 2014 and across great britain barker and macleod 2019 as needs for real time flood impact assessment increase stakeholders are facing fragmented data environments and warehouses with multiple technologies often on multiple web services there is a need to automate big data and crowd sourced information collection in real time and create a map based dashboard to better determine at risk locations and flood situations indeed with the new advancement in technologies there is an opportunity to gather and combine social media data with ground based observations and imagery and translate this information into a web based application to monitor and assess flooding hazards and to communicate this information with citizens in real time this paper introduces flood analytics information system fais as a data engineering and analytics pipeline based on real time flood warnings and river level information natural language processing of tweets and river and traffic web cameras imagery fais allows the user to directly download flood related data from usgs and visualize the data in real time the outcome of the river measurement imagery and tabular data is displayed in a web based remote dashboard and the information can be plotted in real time a twitter application programming interface api and a bot software were developed and incorporated into the prototype as part of the real time crowd intelligence for twitter data gathering the developed twitter bot allows user to monitor every tweet being tweeted and can automate all or part of twitter activities indeed our developed pipeline allows the user to query tweets from twitter by a specific user and or keyword using both search and streaming apis a search api gathers historical geotag data while a stream api monitors real time geotagged tweets with shortlisting at risk areas based on provided keywords fais system can be used equally efficiently by stakeholders as a pervasive early warning system to take smart action such as warning and evacuation deployment of emergency assets search and rescue and planning the prototype was tested for hurricanes florence and dorian driven flood situational assessment across the carolinas this paper is organized as follows in section 2 the research questions and motivation of this research work are explained the procedures algorithms and the functionality of fais application are introduced and discussed in section 3 section 4 discusses the implementation and case studies conclusions and future works and limitation of the prototype are provided respectively in sections 5 and 6 2 research questions and motivation developing fais required addressing three research questions which are 1 how to programmatically and automatically identify areas at risk of flooding based on crowdsourced data real time flood peak rates and river level information the first research question was whether we could compute programmatically the areas of at risk to flooding using various data sources vieweg et al 2014 and barker and macleod 2019 indicated that such an automatic task is difficult to implement in real time due to the volume of data to identify relevant information for decision making process however streaming apis proved to be useful for prioritizing a list of at risk locations barker and macleod 2019 2 how to spatially display the retrieved data and implement this information for alert and warning system the second research question was to investigate the viability of displaying the retrieved data in a timely and continuous way previous studies showed how cross referencing tweets can be used for prioritizing at risk locations to flooding middleton et al 2014 barker and macleod 2019 as well as arranging location based queries during floods using georeferencing geotag tweets laylavi et al 2016 3 how to seamlessly retrieve data from various sources and how to use this information for making actionable decision the third research question investigated the viability of automated retrieval of data and images from ground based monitoring gauges as well as live traffic and river webcams data previous studies highlighted that apis are particularly helpful in gathering various big datasets text tabular and images and could filter social media messages during flooding events spielhofer et al 2016 barker and macleod 2019 internationally there are an increasing number of data sources with a data service apis that can be integrated with any software application our aim was to develop and test a pipeline integrated with historical and real time information based on these three research questions and visualize at risk locations during a series of flooding events across the carolinas the authors also discussed the design of the prototype with federal and state stakeholders to more proficiently develop and implement the workflow during several visits to sc emergency management division scemd as well as virtual discussions with federal agencies such as usgs and federal emergency management agency fema we demonstrated the need for a national scale pipeline that i combines historical and real time river level information with crowdsourcing data ii automates big data gathering and information collection in real time and iii creates a map based dashboard to better determine at risk locations and flood situations across the united states these needs and discussion along with deficiencies in existing big data pipelines provided comprehensive roadmap tasks for performing this research 3 prototype design and development fais is initially designed as a python package targeting two sources of data i e usgs and twitter the package was then transferred to a web python platform to collect the data during historical and real time events and visualize impacted areas the pipeline uses iots apis and machine learning for transmitting processing and loading big data through which the application gathers information from various data servers and replicates it to a data warehouses for use with crowd intelligence approaches fais filters flood relevant tweets using location filtering and word embedding of tweets user can stream or search for tweets using proper keywords for any region in us fais provides both custom data and analytics as a service offerings to help users gain insights about flood situation data environment and start driving informed decisions the prototype also performs flood frequency analysis ffa to assist engineers in designing safe structures below we systematically describe a series of major design components and algorithms designed within the fais application 3 1 machine learning and image processing approaches this study used google vision api to detect objects in time lapse images google vision api uses image processing and machine learning approaches to detect and extract information about objects and entities in an image across a broad group of categories this tool encapsulates machine learning models in an api approach that allows developers to use computer vision technology for classifying images into thousands of categories and assign them sensible labels and scores vision api detects objects in the images using i multiple objects including the location of each object within the image and ii fast high accuracy models to classify images or detect objects at the edge and trigger real time actions based on local data fais allows the user to use the vision api directly or use automl vision to train machine learning model for image annotation and label images the application detects the objects in the image using google cloud vision as a python package to deal with the api we included google cloud sdk along with gsutil tools in the fais algorithm to easily upload large dataset of images to a google bucket the tool then creates a bucket in google cloud storage and user can upload image folder from the local desktop to google bucket the api then utilizes machine learning tools to perform label detection on a request image and sends the result back to the fais application the tool can detect individual objects and pieces of text and information within an image directly from the application analyze images and build custom models using the api to accommodate more flexibility for particular use case fais uses label detection to annotates an image with a label or tag based on the image content and then name them for example a picture of a flooded road may produce a label of flood road or some other similar annotation label detection determines broader category contexts in different ways for example an image can be labeled as flood water river floodplain etc that cover broader categories of water resources objects to create high quality training datasets of annotated images 100 200 or more flood occurrences across all images is required to train the vision model and label the objects the more occurrences of an object such as flood in time lapse images the better the model trains and performs after the user creates the labels fais api calls to create an object detection dataset and populates the images and labels them in the json format the labels constantly store on the mangodb database and display on the presented image 3 2 flood frequency analysis ffa is a technique used by hydrologists and engineers to predict flow values corresponding to specific return periods or probabilities along a river the tool uses dataretrieval decicco et al 2018 and xts jeffrey et al 2020 libraries to retrieve annual peak flow rates for provided years and calculates statistical information such as mean standard deviation and skewness which are further used to create frequency distribution graphs the tool currently fits gumbel distribution to the annual maximum flood data and plots frequency curves these graphs are then used to estimate the design flow values corresponding to specific return periods which can be used for designing structures such as dams bridges culverts levees highways sewage disposal plants waterworks and industrial buildings gumbel is a proper distribution if i the river system is less regulated with less significant reservoir operations diversions or urbanization effects ii flow data are homogeneous and independent lack of fluctuations and long term trends philips et al 2018 iii peak flow data cover relatively long records 10 years and iv no major tributary exists whose inflow may affect the flood peak rates e g raynal and salas 1986 gumbel distribution and the procedure with a return period t is given as 1 x t x k σ x where σ x represents standard deviation of the sample time series k denotes frequency factor which is formulated as k y t y n s n in which y t is reduced variate y t l n l n t t 1 the values of y n and s n are selected from gumbel s extreme value distribution that depends on the sample size e g raynal and salas 1986 it should be noted that the theoretical definition of return period is the inverse of the probability that an event will be exceeded in a given year for example a 10 year return period corresponds to a flood that an exceedance probability of 0 10 or a 10 chance that the flow will exceed in one year 3 3 development of twitter apis during recent hurricane events many citizens in the carolinas used twitter to share flood information such as local damage road closure and shelter information the government agencies such as nws nhc sc department of transportation scdot and usgs also used twitter to provide updates about the storm path environmental condition damaged infrastructure emergency situations evacuation route and resources in a continuous and timely manner during and after the event a tweet can provide a variety of information such as text images videos audio and additional links in addition there is also a significant amount of metadata that is attached to each tweet this metadata includes information regarding geolocation either a place name or coordinates the author name a defined location a timestamp of the moment the tweet was sent or retweeted the number of retweets the number of favorites a list of hashtags a list of links etc this information is valuable and has the potential to provide intelligence when attempting to extract information for use in crisis response twitter apis also offer a varying number of filters and filtering capabilities including additional filter operators and tweet enhancements e g profile location and un shortened urls the twitter platform provides various apis for searching tweets including i the standard twitter apis consisting of rest apis and streaming apis and ii the enterprise apis including filtered firehose historical search and engagement apis for deeper data analytics fais uses the standard twitter apis because it is free and less challenging to gather twitter flow of information and hashtag driven topics standard api provides an endpoint to return time series counts of tweets matching user query the interested geotagged data that can be gathered using the standard api are images videos text and numeric e g flood depth from citizen inputs for privacy issues and other security concerns regarding personal information our developed twitter apis do not employ any user related features such as number of followers on twitter rather focused on the message related attributes the apis also calculates sentiment of the tweet and the identified categories sentiment is a text tweet analysis that is used to categorize and classify the opinions and sentiments expressed in text three classes of sentiment were implemented using twitter apis including i positive a positive sentiment has been expressed ii negative a negative sentiment has been expressed and iii neutral a neutral or no reaction sentiment has been expressed 3 3 1 twitter streaming api we used tweepy package and integrated it with fais as an easy to use python library for accessing the twitter data twitter developer account was used to access token token secret consumer key and consumer secret to manipulate twitter functionalities to protect the credential the authors decided to develop a twitter streaming bot functions on both ios and mac and deployed it at heroku cloud platform outside of the application access which can be controlled by the heroku user interface heroku is a cloud platform as a service paas that enables system level supervision and coordination of twitter apis crowd sourced data and tweets our developed streaming twitter bot automated all twitter data gathering and continuously watched all twitter activities during real time implementation to be able to watch twitter activity in real time the bot gets notified when new content such as tweets that matches certain criteria such as dorian floods is created this is particularly important when dealing a vast amount of real time tweets we created a reusable python module a module config containing the logic common to the bot functionalities this module reads the authentication credentials from environment variables and creates the tweepy api object by reading the credentials from environment variables user avoids hard coding them into the source code making it much more secure the bot reads the credentials from four environment variables including consumer key consumer secret access token and access token secret after reading the environment variables the bot creates the tweetpy authentication object that eventually uses to create api object the bot uses the logging python module to inform errors and information messages that help user debug them if any issue arises the tweets save constantly in the mangodb database when the bot is in operational use the administrator can choose to activate or deactivate the bot and change the keywords for streaming services our developed twitter bot contains three components fig 1 notably 1 twitter client this component talks to the twitter api and authenticates the connection to use its functionality this also hosts a function called tweets listener which will continuously stream tweets and listen for the matched keywords once it finds the match it will then talk to the other two components 2 tweet analyzer it analyzes the tweets and gives it a score after a match is found 3 twitter streamer this module streams tweets from pre specified keywords analyzes the data and organizes them into a data frame the collected tweets will then store in a mongodb database waiting to be extracted a conceptual process about how to gather real time tweets using the developed streaming twitter bot is shown in fig 1 due to the size of queried data the twitter bot filters the data and only keeps text location author and date of tweets which are eliminated for over 95 of uninterested data fais application has access to mongodb cloud database without having access to twitter bot this allows the user to see the result of our services while protecting fais twitter account privacy and information allowing the users to have access to database resources whenever needed using twitter streaming bot fais was able to identify at risk locations to flooding the application first cycles through a set of usgs web addresses for river gauge height readings parsing these flat files using python web scraping technique and obtain all the latest river levels each river level reading is compared with its respective long term cached average level to identify the highest relative river levels in real time the highest river level then intersects with watershed polygons as well as geotagged tweets to identify at risk locations to flooding geotagged tweets coordinates considered as a center point for approximately 16 km wide square boxes this size is arbitrarily chosen to cover the areas nearby to each flood gauge the retrieved tweets are constantly stored in a mangodb database during operational use which provide an ideal open source database to store json format files 3 3 2 twitter search api twitter search api is a functionality to search past tweets that match the search criteria twitter contains the search functionality but has a limited amount of search result search frequency and time constrained to upgrade the functionality the user requires to pay for the upgraded twitter account with the goals of keeping fais application free and open source an alternative search method was implemented which was using the rest service freely from twitter search function twitter search operation allows the users to look for tweets based on interested user account keywords language and time period but this is not an ideal procedure for the user to go through the website and gather all the interested tweets that they need therefore this project used urllib python library for universal resource locator url handling modules to send specific request to twitter search web service and to gather the tweets the url can be then modified based on the user search criteria the search api continuously sends request url to twitter and waits for the json response from twitter after receiving the json file the search api extracts interested tweets including username text retweets likes date id permalink user id media url and the sentiment the response json file contains a min position data file which can be used to move iteration forward the iteration will keep running until the response exceeds maximum number of tweets limit set by the user note that the date limit is set to the request url sent to twitter twitter outcomes such as the sentiment analysis can be performed for a tweet the sentiment shows whether or not a tweet has positive 1 neutral 0 or negative tones 1 this information is important because it can be used by social scientists to study the impacts of flooding on the citizens and how the residents responded to the flooding and damage 3 4 development of usgs and 511 traffic data apis usgs collects and stores multiple river system data across the united states these are river flow data flood streamflow gauge heights water quality ground water levels and precipitation at defined gauging stations which are strategically placed at the outlets of rivers and lakes these placements allow usgs to correctly monitor and collect the data and compute several statistical indices related to the river flow usgs server provides two different types of flow data including real time and historical records based on datetime fais gathers usgs discharge data daily streamflow and flood data as well as gauge heights and river web camera images the stream data is value of flow rate in cubic feet per second cfs and water level gauge height in feet fais first gathers both historical and real time usgs data for any state in us it then analyzes the information and plots the historical and real time data these data are recorded based on data recording time step such as every 15 min to 1 h intervals fais made usgs data collection seamless and straightforward by providing access to the station s information including station name id name latitude longitude and url to usgs server in addition fais gathers usgs real time cameras and 511 traffic images dot in each state provides traffic data in real time to the users through 511 web application the cameras are strategically placed on the bridges and roads and along the interstate allowing operators to continuously monitor road conditions they also monitor rising and falling water stage over critical infrastructures such as roads and bridges in addition usgs established real time web cameras for critical rivers where there is high chance for flooding and inundation issues fais currently collects six usgs sevillian cameras images across south carolina sc the application uses a dynamic mapping interface to allow the user to select specific traffic camera and access the data in real time indeed there are embedded urls at both usgs and 511 traffic websites that were used to gather both usgs and traffic images in real time images constantly store at mangodb database when the application is in operational use 3 5 web development platform with the use of python in the fais api tools it was straightforward to use the same language and develop a web application the seamless interaction between the api and python motivated this study to use django as a web framework django is a rapid development platform with clean and pragmatic design that provides a widest range of libraries including twitter apis and machine learning tools the key point of making it an ideal development framework is due to its focus on automation as much as possible and adhering to the don t repeat yourself dry principle aiming at reducing repetition of software patterns and avoid redundancy django allows the user to develop a web application using python element along with a classic web development language like html javascript jquery etc this means that developers are not stuck with limited approaches of solving the problem this interaction can also be a downside of this development framework as it requires the knowledge of different languages to debug like ruby on rails fais is version controlled using github since git is a widely used version control system and an open source repository hosting service fig 2 shows the overall workflow of the fais pipeline development the workflow includes collecting the data from various resources analyzing the potential at risk areas and providing actionable results to users stakeholders as illustrated fais combines multiple apis along with machine learning and image processing algorithms google vision api and ffa script written in r to provide both historical and real time information about flood risk incidents our developed prototype offers an end to end open source web based pipeline architecture to address the crucial issue of how first responders and decision makers can be smartly informed and acted in emergency situations to achieve our aim and answer three research questions stated above we tested fais operationally during hurricane dorian flooding event september 04 06 2019 as well as during historical event hurricane florence flooding september 13 16 2018 across the carolinas 4 results and discussion 4 1 retrieval of social geotagged tweets using twitter apis this section addresses the first and second research questions how to programmatically and automatically identify areas at risk of flooding based on crowdsourced data real time flood peak rates and river level information and how to spatially display the retrieved data and implement this information for alert and warning system to address these questions two options i e the search and the streaming apis were included to the fais application for data gathering we used tweepy library as a search api and developed a streaming api using twitter bot search api was more suited to singular and specific queries for tweets whereas the streaming api provided a real time stream of tweets there were widespread flooding events across the carolinas when we designed and beta tested fais application these events caused localized and major flooding as well as above average river level for most basins in the carolinas fais was tested for historical hurricane driven flood events such as hurricane florence september 13 16 2018 across the carolinas in addition the prototype was operationally tested during hurricane dorian event september 04 06 2019 and georeferenced tweets were gathered in real time to identify at risk locations an example from each hurricane driven floods is presented with intersecting at risk locations and geotagged tweets a web based console and a visualization tool geojsonlint13 were used to view results and inspect the polygons 4 1 1 hurricane dorian case study we monitored georeferenced tweets filtered by keywords and queries across the shortlisted areas in the carolinas during hurricane dorian event september 04 06 2019 to identify at risk locations in real time a shell script in python ran on a local computer server the script was reset every 3 h in order to update areas at risk of flooding from the latest national and environmental data sources as well as twitter feeds a period of 15 min was initially chosen as intended trade off between tracking the latest at risk area forecasts api updates varied between 15 min to several hours but based on real time testing the period extend to 3 h to allow some reaction time from impacted citizens on twitter however the choice of time period depends solely on the project requirements as well as the severity and impacts of flooding events a period of 6 12 h was chosen during hurricane dorian flooding event as an intended trade off between tracking the latest at risk areas to flooding api updates varied between 15 min and 6 h depending on flood data time step and that allowed capturing reaction from those at risk areas in real time the collected tweets provided a real time dataset which explored further and used to prioritize at risk locations for dorian flood simulation results not shown here this provided a general indication of the proportion of potentially relevant tweets that could be used to identify flooded areas and improve flood situational awareness for first responders fais filtered georeferenced tweets returned from the streaming api with keywords and a blend of geographic data sources geolocations to show areas affected by dorian floods at a regional scale during hurricane dorian we constantly ran the twitter bot to collect tweets and to determine at risk locations over time the stored tweets provided a real time dataset which then explored and used to prioritize at risk locations to dorian flooding this provided a general indication of the proportion of potentially relevant tweets that could be used to identify flooded areas and improve flood situational awareness data retrieval of dorian flooding was explored via basic text query searches such as hurricane dorian floods and centroids were added to tweet s geotag for creation of map based visualizations currently the size of bounding boxes of tweets are 16 km from the centroid that determines the number of tweets intersect with the bounding boxes to yield and visualize at risk areas the size for 16 km boxes is arbitrarily chosen to cover the areas nearby to each usgs gauge in the coastal carolinas and may differ location by location based on the proximity of the tweets to the nearest flood gauge previous studies showed that messages within 10 km of severely flooded areas had a much higher likelihood of being related to such events see de albuquerque et al 2015 fig 3 illustrated collected geotagged tweets intersected with watershed boundaries for south carolina during hurricane dorian flooding delineating at risk areas could be further improved by populating critical infrastructure bridges roads and flood defense structures levees dams reservoir which are out of the scope of this research table 1 presents several real time tweets along with their geolocation information twitter account etc which were retrieved during hurricane dorian flooding in south carolina september 04 06 2019 the classifier was developed by an annotator the first author manually labeled a subset of 800 tweets from september 04 06 2020 as either relevant or irrelevant tweets 4 1 2 hurricane florence case study we implemented fais to extract geospatial footprint of hurricane florence flooding event using georeferenced tweets the user has an option to select a proper twitter account for the historical tweets gathering we chose nws twitter account nws to gather hurricane florence historical tweets and map the geospatial footprint map view wasnot shown here fais was able to successfully identify a dynamic set of at risk areas using twitter search api for hurricane florence event at risk areas to flooding were identified by intersecting the geotagged tweets with watershed polygons and river gage heights search api that is designed within the fais application was able to collect the tweets and visualize them in on a leaflet map via folium python api fig 4 showed the fais design and outcomes for twitter search api implementation of hurricane florence flooding event the search api provided the source of the tweet image the sentiment and map view of the tweets if the user changes the keywords the twitter search api will then filter the tweets and save the data in a mangodb database we used national geographic base map tiles and geolocated each tweet based on its geographic coordinates each tweet is represented by a clickable marker which provided a pop up box of the tweet information locations coordinates time etc alternative interactive base map can be included to the prototype such as openstreetmap imagery topographic etc fais can be applied nationally across the us running for both real time and post flood event tweet gathering and assessment table 2 displays several tweets gathered using nws account for hurricane florence flooding in sc the tweets were retrieved via location filter that varied in terms of spatial precision less or no exact place metadata or coordinates location filtering reduced mismatch of off topic tweets irrelevant which is an issue for keyword based retrieval de albuquerque et al 2015 although the word flood and its translations were also frequently used figuratively or in a transferred sense e g i am flooded with many tasks we performed the filtering mostly by location filter capabilities designed within twitter apis that seemed to be an effective filtering approach to improve collecting the relevant tweets and reducing manual labor overall we collected over 800 location filtered tweets between september 13 16 2018 across the carolinas 4 1 3 usgs data collection this section addresses the third research question how to seamlessly retrieve data from various sources and how to use this information for making actionable decision fais collects and displays usgs data that include the date discharge cfs and gage height ft along with their associated plots gathering usgs historical data involves selecting the target state the interested station and the date after query criteria is entered fais creates a request url and sends it to the usgs server for collecting the data the prototype displays the data as table view fig 5 a as well as map view fig 5 b and plots the results fig 5 c the user can also upload a csv format file of all the collected data that contains station name and id latitude longitude discharge gauge height and usgs original url fig 5 d in addition fais collects six usgs sevillian cameras images across south carolina these cameras are located at rocky creek near wade hampton rocky branch at whaley st columbia peedee river near florence lake monltrie trailrace canal at moncks corner sc tearcoat brach at i 95 near manning sc and pocotaligo river at i 95 above manning sc the image contains the meta data such as when and where the image was captured fais also collects north carolina nc 511 images in real time several cameras that are located in the coastal region of nc were selected to record road flooding conditions in real time these images are crucial for monitoring of water level and providing early warning to the local community in case the water level increases above a predefined threshold value both usgs and 511 traffic images cane be stored at mangodb database when the application is in operational use readers are referred to fais web server https floodanalytics clemson edu for more detailed information and to stream and store the camera images 4 2 image processing and label detection we integrated google vision api with fais application for object detection of flooded and non flooded images the tool first trains the automated ml vision model and then labeled the datasets this provides custom label detection data with scores we used flooded and non flooded images of hurricane florence to train the google vision api and detect the objects in the images we extracted high quality still images from hurricane florence videos overall we analyzed a range of 100 240 time lapse images to detect the labels and sore them the higher quality images user delivers and the better the design of the model user uses the smarter outcome will be produced fig 6 shows the detected objects for the person street bridge cape fear river nc flood 91 land lot 91 and asphalt 89 are detected as major labels in flooded images while the major labels and scores are given to sport venue 89 and residential area 85 in non flooded images fig 7 showed the score and the number of images that we used to detect the label for this location as illustrated the algorithm was able to detect water flood 0 97 during and after hurricane event the algorithm also detected bridge infrastructure 0 92 for pre event while distinguished it as a reservoir 0 9 during post flooding event due to overtopping issue see fig 8 a number associated with every returned label annotation representing the vision api s assessment of the label s accuracy scores range from 0 no confidence to 1 very high confidence fig 9 overall google vision api detected 11 labels for the flooded and non flooded images at the new bern nc while captured 17 labels for the person street bridge flooding event at the cape fear river nc despite the lower number of labels 92 6 of vision s labels turned out to be relevant 8 errors it would be worth mentioning that google vision api could further improved by cluster equivalent labels together flood water and water resources river whenever a water being is detected in the image by collapsing such labels into one the number of detected labels will definitely decrease and it may also have some impacts on relevance scores our analysis suggests that google vision api has detection problems whenever flooded areas are too small below 50px partially out of the image or occluded by other obstacles such as vehicles and trees this might improve over time with a more specialized pattern recognition layer and approaches such as including an image segmentation to the tool based on watershed algorithm that builds barriers in the locations where water merges these barriers can provide segmentation results that could be used to estimate inundation extend and flood level using a reference such as a bridge pier or a car it should be noted that we didn t focus on other accuracy parameters such as location direction and special traits vision doesn t provide such data further work and a considerable dataset expansion may provide useful insight into flood location and direction accuracy although the difference of a few pixels is usually negligible for most applications the best scoring fragments for a given label matched well with visual appearance of the object in the image indeed the pairs of flooded and non flooded images should have the same view angle and geometry although image processing and machine learning algorithms designed within google vision api yielded successful results as indicated by the scoring metrics there are several challenges in detecting the flooded or inundated areas for example this study found discrepancies in labeling dry flooded image pairs specifically for differentiating flood floodplain waterway and road the algorithm detected some labels that weren t existed in the image such as roof and floodplain to prevent this issue the model can be validated on a simple dataset where a single object e g flood occupied most of the image a wide variety of images with annotated objects that co occur in the same images can be used to test the accuracy of the tool further the algorithm can be enhanced by including in painting procedure to efficiently remove unwanted objects such as the sidewalk and streetlight to detect the inundation border as studied elsewhere witherow et al 2019 although differences in image resolution and lighting and environmental conditions have significant impacts on annotating an image with a reliable label and score flood detection provides important information to stakeholders because the results can be proactively used for flood emergency management indeed the capability to detect temporal changes in image sequences is crucial and this information can be combined with other datasets such as usgs flood peak rates and rainfall radar data to develop an automated image based flood alarm system as a disaster monitoring application image based flood warning information can facilitate proactive monitoring and damage assessment and early warning to rising water levels and associated inundation areas in real time however current deficiencies in google vision api and overall image processing algorithms explained above may limit vision application for image based early warning system 4 3 flood frequency analysis of two usgs gauges fais provides flood frequency analysis to estimate flood quantiles that combines elements of observational analysis stochastic probability distribution and design return periods fais currently uses gumbel distribution to compute ffa for any given flood gauging station in us fig 10 shows ffa for the usgs 02147500 rocky creek at great falls sc sc as well as the usgs 02196000 stevens creek near modoc sc as illustrated annual flood peak of 15000 cfs for the rocky creek represents a design return period of 25 year while the same flood peak shows a design return period of 5 years for the stevens creek gauge the difference is related to the size of the drainage system on small watersheds a 25 year rainfall event may produce a 25 year flood event on large watersheds however the 25 year flow event may be produced by a series of smaller rainfall events this distinction should particularly be kept in mind by the practitioner while dealing with design projects in large watersheds the likelihood of a 100 year flood 50 annual chance occurring at both gauging stations ranging between 32000 to 35000 cfs both gauging stations appeared to experince large flooding events particularly usgs 02196000 this gauge is part of the savannah basin where frequent flooding is a major threat for the residents ffa for this location proved that high peak values made critical contributions to the upper tail of the gumbel probability distribution function this analysis is useful in providing a measurement parameter to assess the damage corresponding to specific flow during flooding event along with civil infrastructure design flood frequency analysis can be used in flood insurance and flood zoning activities accurate estimation of flood frequency not only helps engineers in designing safe infrastructure but also in protection against economic losses due to maintenance of structures however the accuracy of ffa estimates may vary using different probability distributions such as pearson type iii gamma and normal distributions we recommend using gumbel function for a river system with less regulation and significant reservoir operations diversions or urbanization effects efforts are underway to include other distribution functions to the prototype therefore care should be given when the current function uses for any structure design purposes 5 conclusions in this paper we developed a prototype for flood data analytics and assessment using iot apis crowd intelligence and big data gathering approaches our study presented the first step towards identifying at risk areas to flooding in real time and defining the geospatial footprint of a flood event using georeferenced tweets we aimed to use iot apis and collect environmental data river levels and discharge at the national scale in combination with twitter apis to identify the areas affected by a flood the application also uses image processing and machine learning to detect label and scores objects in time lapse images ffa algorithm was also developed in r and embedded with the fais to perform flood design metrics and peak rates that could be combined with twitter and image processing results for studying the design metrics of overtopped structures as well as economic assessment overall our proposed pipeline proved to be robust and user friendly tool for both real time and post event analysis of flooding at regional scale that could help stakeholders for rapid assessment of the situation and damages gathering the data and analysis of flood situation take on average few minutes to select the data periods and set the designed algorithm to run thereby it reveals to be promising for meeting first responders needs during emergencies during real time event the time between a tweet appears online and visually plots for a stakeholder as being potentially relevant in terms of location and content would be in the order of few seconds to minutes thereby this rapid analysis can provide an early information channel for asset allocation and rescue the application proved to be proficient for real time flood assessment as it was tested during a 2 day hurricane dorian event across the carolinas with over 15 000 geotagged tweets collected from 38 dynamic potentially at risk areas using streaming api during hurricane dorian tweets were collected and labeled as relevant because they were related to ongoing hurricane and flood event streaming api assigned a tweet as relevant class when a user tweeted about ongoing dorian floods at the time of posting this includes for example tweets referring to response rescue road closure and failure of critical infrastructure water supply all other tweets were assigned being irrelevant examples include historic flood event commemorations the use of the word flood in figurative or transferred sense etc it is interesting to note that the streaming api showed a great performance to maximizing the retrieval of geotagged tweets via location filtering as well as using more detailed key words and this finding is in agreement with other studies see barker and macleod 2019 ekta et al 2017 tsou et al 2017 morstatter et al 2013 the alternative search api preferentially relied on a tweet s geotag presence and therefore reverts to the more general and less spatially accurate user profile location the user profile location is manually set by the user and maybe different to the specific tweet s actual location furthermore tweets are often contained noisy and redundant data and that cleaning and geoparsing might be difficult and present a time consuming task most importantly the search api permits just a single location per query and is not suitable for spatiotemporal assessment of flooding event in real time our developed streaming api and a twitter bot allowed simultaneous monitoring of multiple locations necessary for real time national scale flood assessment in addition we integrated google vision api to parcel images into labels and scores them as discussed image processing and machine learning is not perfect it often splits objects into separate labels labels objects that are not presented in the image or includes multiple objects in the same label however over a large dataset of time lapse images we found that image processing approaches perform better for many object categories we showed improved object discovery for a more complicated set of flooded and non flooded images for recent hurricane driven floods in the carolinas our research revealed that images and image sequences videos make up about 80 percent of all corporate and public unstructured big data for flood related studies providing an excellent data sources for flood analytics research although the result described herein is encouraging for time lapse images videos analysis the following open issues still exist in image recognition i how do we effectively improve recognition and label detection approaches to cope with numerous object categories that exist and are recognizable by humans and ii how can we apply image processing and machine learning approaches to motion and actions in video and label them we hope that the techniques developed in this study will offer a good starting point in addressing these issues and developing more intelligent and proficient image segmentation and processing methods for flood studies the authors believe that data analytics applications in the field of flood risk management should adopt a modular view moving from a component based to a national scale at present data analytics research remains in its developing phase into existing workflows and practices there appears a major gap particularly in seamless integration of different data sources as number of datasets keep increasing over time although the main issue in integrating these datasets is to ensure data consistency accuracy and completeness for informed decision making processes as data collection through various heterogeneous sources in real time is highly susceptible to noise and uncertainty to this end security as well as privacy issues in data transmission and analytics and storage also need to be under constant control to ensure the authenticity of data and citizen based crowd sourced information while keeping the confidentiality of people s sensitive information in this study we set out the major design choices and decisions made by acknowledging the inputs from many decision makers and first responders we appreciate the discussions with scemd and believe fais can be a more transparent and efficient tool than many other similar pipelines we acknowledge there is a need to evaluate the pipeline in other case studies and real time events also integrating other approaches such as deep learning techniques for word embeddings natural language processing in deriving vector representations for non english tweets this would be crucial since we gathered tweets in spanish and other languages when we tested the prototype operationally during dorian flooding 6 limitation of fais prototype and future work this national scale big data analytics pipeline could be further improved in several ways in the ffa section we recommend computing nonstationary flood frequency models by incorporating external covariates lópez and francés 2013 philips et al 2018 su and chen 2019 meteorological modes such as el niño southern oscillation enso and or physical properties reservoir index are key covariates that could be taken into account for non stationary ffa assessment in addition compound flood calculation can be included to compute ffa based on multivariate extreme variables see renard et al 2013 for the coastal region in addition fais workflow and design for identifying at risk areas to flooding can be further improved the size of bounding boxes of tweets currently 16 km from the centroid requires further attention and evaluation given the boundary effect on the number of tweets intersect bounding boxes and yielding a match as well as considering the density of usgs flood gauges including ungauged areas in an urban setting increasing the size of bounding boxes would likely include more significant tweets thereby more accuracy on the chosen proximity the selected tweets can be also overlaid with other data layers such as census data population density rainfall radar data and federal emergency management agency fema flood hazard map in addition fais image processing and machine learning approaches need further attension google cloud vision is a mature detection tool and comes with more flexible api conventions multiple image formats and a native batch support its object detection functionality generates much more relevant labels and its label detection currently seems mature enough as well although it s not quite perfect yet the biggest issue of this tool seems to be rotational invariance although it might be transparently added to the deep learning model in the future vision has a wide margin of improvement regarding batch video support and more advanced features such as image search object localization and object tracking in video being able to fetch external images e g by url might help speed up api adoption while improving the quality of flood detection features will inspire greater trust from users for further fais development a linking approach could be proposed to increase system autonomy during real time flood risk assessment and data retrieval tasks this automation would help satisfy the need for timeliness and reliability during emergency responses and management efforts are currently underway to combine iot sensor rainfall and water level information to the prototype 7 software and data availability flood analytics information system fais various scripts as they apply to pipeline development description data gathering application designed for flood and twitter data analyses developer nattapon donratanapat contact pleuk5667 gmail com software access https pypi org manage project fais releases year first available 2019 hardware required windows linux macos hardware required intel i3 or mid performance pc with multicore processor and ssd main drive 4 gb memory recommended cost free software and source code are released under the massachusetts institute of technology license software availability all source code can be found at github public repository as well as at the python package index pypi https github com vidyasamadi flood analytics information system fais https pypi org project fais https floodanalytics clemson edu declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 7 acknowledgements this work was supported by the u s national science foundation nsf directorate for engineering under grant cbet 1901646 any opinions findings and discussions expressed in this study are those of the authors and do not necessarily reflect the views of the nsf the authors acknowledge the early and continuous discussion with south carolina stakeholders first responders and decision makers fais prototype has been developed using cluster computing at the university of south carolina as well as clemson university therefore their support and assistant are gratefully acknowledged fais python package is publicly available at the hhr hydosystem and hydroinformatics research github account https github com hhrclemson the prototype is currently running on the ibm cloud computing service https floodanalytics clemson edu appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104828 
25975,with the rapid development of the internet of things iot and big data infrastructure crowdsourcing techniques have emerged to facilitate data processing and problem solving particularly for flood emergences purposes a flood analytics information system fais has been developed as a python web application to gather big data from multiple servers and analyze flooding impacts during historical and real time events the application is smartly designed to integrate crowd intelligence machine learning ml and natural language processing of tweets to provide flood warning with the aim to improve situational awareness for flood risk management fais a national scale prototype combines flood peak rates and river level information with geotagged tweets to identify a dynamic set of at risk locations to flooding the prototype was successfully tested in real time during hurricane dorian flooding as well as for historical event hurricanes florence across the carolinas usa where the storm made extensive disruption to infrastructure and communities keywords internet of things flood analytics information system machine learning big data flood situational awareness the carolinas 1 introduction the south and southeast united states us are subjected to a series of intense storms throughout the year as well as deadly atlantic hurricane events during hurricane season june november these events can happen in quick succession 2 weeks apart and produce catastrophic flooding in wide geographic areas 1000 km swath and within short time spans less than a 48 h period as a consequence of these successive events many lives were lost and numerous critical infrastructure and communities were vastly disrupted to reduce the risk of damages accurate and real time flood assessment is critical for emergency management and to improve two way communication and understanding of potential impacts at present national weather service nws and national hurricane center nhc provide river and storm path forecasts and issue early warning system for potential areas of flooding however it is acknowledged that these forecasts are large scale and have less skills with respect to detecting localized floods and identifying specific areas at risk of flooding samaniego et al 2017 adams and dymond 2019 flooding in the south and southeast regions are often highly localized and intense which cause inundation in low lying roads and poor drainage areas and the level of individual properties e g philips et al 2018 furthermore providing geographically targeted early flood warnings in time is hampered by a lack of data and real time information for stakeholders and residents to take protective actions for themselves their property and livestock improved data collection and real time assessment of at risk locations allow more efficient mutual aid in the operational theater for warnings and evacuations and more effective search and rescue plans while enabling automatic dispatching of relief resources and evacuation plans further development validation and implementation of viable and accurate flood warning systems requires a step change in the methodologies used for data collection and analysis with the rapid development of earth observation technology and ground based monitoring systems that produce time lapse videos and images high spatial and temporal big data have been recently tapped into flood early warning assessment e g barker and macleod 2019 although these images require innovative enabling technologies to improve the integration retrieval analysis and presentation of large amounts of information grolinger et al 2013 nativi et al 2013 smart technologies such as internet of things iots image processing and machine learning ml can provide the intelligence to analyze real time data and alleviate information overload for flood early warning system e g rao et al 2017 iot is one of the fastest developing fields in the history of computing with an estimated 50 billion devices by the end of 2020 al garadi et al 2018 that can integrate billions of smart devices to communicate with one another with minimal human intervention the connectivity in iots and high speed data transfer capabilities can be used to implement real time image processing and ml analytics system for flood risk studies both image processing and ml algorithms are powerful methods of data exploration for real time monitoring and learning about normal and abnormal condition of a watershed system recently image processing and ml algorithms have been demonstrated to label time laps camera imagery crowdsourcing tabular data and user generated texts and photos to extract road flooding inundation extend and depth de albuquerque et al 2015 starkey et al 2017 feng and sester 2018 barker and macleod 2019 feng and sester 2018 2018 erfani and samadi 2019 in addition analysis of social geodata during floods can provide actionable intelligence to assist first responders to identify at risk communities stakeholders need place based geotagged and crowd sourced information about flooding being analyzed rapidly in real time accessibility to voluntarily generated and often publicly published content on social networking and social media provides a strong draw for disaster related research crowdsourced social media data particularly twitter is increasingly used to improve situational awareness and two way communication during hurricane and flood events e g kryvasheyeu et al 2016 barker and macleod 2019 social crowdsourcing data can help with the identification of flood extend especially during pluvial and fluvial flooding in urban settings e g smith et al 2015 eilander et al 2016 arthur et al 2018 developing a pipeline to gather the data and identify tweets relevant to flooding proved to be useful to assess real time flooding impacts and damage in sao paulo brazil de assis et al 2016 jakarta indonesia eilander et al 2016 the river elbe germany herfort et al 2014 and across great britain barker and macleod 2019 as needs for real time flood impact assessment increase stakeholders are facing fragmented data environments and warehouses with multiple technologies often on multiple web services there is a need to automate big data and crowd sourced information collection in real time and create a map based dashboard to better determine at risk locations and flood situations indeed with the new advancement in technologies there is an opportunity to gather and combine social media data with ground based observations and imagery and translate this information into a web based application to monitor and assess flooding hazards and to communicate this information with citizens in real time this paper introduces flood analytics information system fais as a data engineering and analytics pipeline based on real time flood warnings and river level information natural language processing of tweets and river and traffic web cameras imagery fais allows the user to directly download flood related data from usgs and visualize the data in real time the outcome of the river measurement imagery and tabular data is displayed in a web based remote dashboard and the information can be plotted in real time a twitter application programming interface api and a bot software were developed and incorporated into the prototype as part of the real time crowd intelligence for twitter data gathering the developed twitter bot allows user to monitor every tweet being tweeted and can automate all or part of twitter activities indeed our developed pipeline allows the user to query tweets from twitter by a specific user and or keyword using both search and streaming apis a search api gathers historical geotag data while a stream api monitors real time geotagged tweets with shortlisting at risk areas based on provided keywords fais system can be used equally efficiently by stakeholders as a pervasive early warning system to take smart action such as warning and evacuation deployment of emergency assets search and rescue and planning the prototype was tested for hurricanes florence and dorian driven flood situational assessment across the carolinas this paper is organized as follows in section 2 the research questions and motivation of this research work are explained the procedures algorithms and the functionality of fais application are introduced and discussed in section 3 section 4 discusses the implementation and case studies conclusions and future works and limitation of the prototype are provided respectively in sections 5 and 6 2 research questions and motivation developing fais required addressing three research questions which are 1 how to programmatically and automatically identify areas at risk of flooding based on crowdsourced data real time flood peak rates and river level information the first research question was whether we could compute programmatically the areas of at risk to flooding using various data sources vieweg et al 2014 and barker and macleod 2019 indicated that such an automatic task is difficult to implement in real time due to the volume of data to identify relevant information for decision making process however streaming apis proved to be useful for prioritizing a list of at risk locations barker and macleod 2019 2 how to spatially display the retrieved data and implement this information for alert and warning system the second research question was to investigate the viability of displaying the retrieved data in a timely and continuous way previous studies showed how cross referencing tweets can be used for prioritizing at risk locations to flooding middleton et al 2014 barker and macleod 2019 as well as arranging location based queries during floods using georeferencing geotag tweets laylavi et al 2016 3 how to seamlessly retrieve data from various sources and how to use this information for making actionable decision the third research question investigated the viability of automated retrieval of data and images from ground based monitoring gauges as well as live traffic and river webcams data previous studies highlighted that apis are particularly helpful in gathering various big datasets text tabular and images and could filter social media messages during flooding events spielhofer et al 2016 barker and macleod 2019 internationally there are an increasing number of data sources with a data service apis that can be integrated with any software application our aim was to develop and test a pipeline integrated with historical and real time information based on these three research questions and visualize at risk locations during a series of flooding events across the carolinas the authors also discussed the design of the prototype with federal and state stakeholders to more proficiently develop and implement the workflow during several visits to sc emergency management division scemd as well as virtual discussions with federal agencies such as usgs and federal emergency management agency fema we demonstrated the need for a national scale pipeline that i combines historical and real time river level information with crowdsourcing data ii automates big data gathering and information collection in real time and iii creates a map based dashboard to better determine at risk locations and flood situations across the united states these needs and discussion along with deficiencies in existing big data pipelines provided comprehensive roadmap tasks for performing this research 3 prototype design and development fais is initially designed as a python package targeting two sources of data i e usgs and twitter the package was then transferred to a web python platform to collect the data during historical and real time events and visualize impacted areas the pipeline uses iots apis and machine learning for transmitting processing and loading big data through which the application gathers information from various data servers and replicates it to a data warehouses for use with crowd intelligence approaches fais filters flood relevant tweets using location filtering and word embedding of tweets user can stream or search for tweets using proper keywords for any region in us fais provides both custom data and analytics as a service offerings to help users gain insights about flood situation data environment and start driving informed decisions the prototype also performs flood frequency analysis ffa to assist engineers in designing safe structures below we systematically describe a series of major design components and algorithms designed within the fais application 3 1 machine learning and image processing approaches this study used google vision api to detect objects in time lapse images google vision api uses image processing and machine learning approaches to detect and extract information about objects and entities in an image across a broad group of categories this tool encapsulates machine learning models in an api approach that allows developers to use computer vision technology for classifying images into thousands of categories and assign them sensible labels and scores vision api detects objects in the images using i multiple objects including the location of each object within the image and ii fast high accuracy models to classify images or detect objects at the edge and trigger real time actions based on local data fais allows the user to use the vision api directly or use automl vision to train machine learning model for image annotation and label images the application detects the objects in the image using google cloud vision as a python package to deal with the api we included google cloud sdk along with gsutil tools in the fais algorithm to easily upload large dataset of images to a google bucket the tool then creates a bucket in google cloud storage and user can upload image folder from the local desktop to google bucket the api then utilizes machine learning tools to perform label detection on a request image and sends the result back to the fais application the tool can detect individual objects and pieces of text and information within an image directly from the application analyze images and build custom models using the api to accommodate more flexibility for particular use case fais uses label detection to annotates an image with a label or tag based on the image content and then name them for example a picture of a flooded road may produce a label of flood road or some other similar annotation label detection determines broader category contexts in different ways for example an image can be labeled as flood water river floodplain etc that cover broader categories of water resources objects to create high quality training datasets of annotated images 100 200 or more flood occurrences across all images is required to train the vision model and label the objects the more occurrences of an object such as flood in time lapse images the better the model trains and performs after the user creates the labels fais api calls to create an object detection dataset and populates the images and labels them in the json format the labels constantly store on the mangodb database and display on the presented image 3 2 flood frequency analysis ffa is a technique used by hydrologists and engineers to predict flow values corresponding to specific return periods or probabilities along a river the tool uses dataretrieval decicco et al 2018 and xts jeffrey et al 2020 libraries to retrieve annual peak flow rates for provided years and calculates statistical information such as mean standard deviation and skewness which are further used to create frequency distribution graphs the tool currently fits gumbel distribution to the annual maximum flood data and plots frequency curves these graphs are then used to estimate the design flow values corresponding to specific return periods which can be used for designing structures such as dams bridges culverts levees highways sewage disposal plants waterworks and industrial buildings gumbel is a proper distribution if i the river system is less regulated with less significant reservoir operations diversions or urbanization effects ii flow data are homogeneous and independent lack of fluctuations and long term trends philips et al 2018 iii peak flow data cover relatively long records 10 years and iv no major tributary exists whose inflow may affect the flood peak rates e g raynal and salas 1986 gumbel distribution and the procedure with a return period t is given as 1 x t x k σ x where σ x represents standard deviation of the sample time series k denotes frequency factor which is formulated as k y t y n s n in which y t is reduced variate y t l n l n t t 1 the values of y n and s n are selected from gumbel s extreme value distribution that depends on the sample size e g raynal and salas 1986 it should be noted that the theoretical definition of return period is the inverse of the probability that an event will be exceeded in a given year for example a 10 year return period corresponds to a flood that an exceedance probability of 0 10 or a 10 chance that the flow will exceed in one year 3 3 development of twitter apis during recent hurricane events many citizens in the carolinas used twitter to share flood information such as local damage road closure and shelter information the government agencies such as nws nhc sc department of transportation scdot and usgs also used twitter to provide updates about the storm path environmental condition damaged infrastructure emergency situations evacuation route and resources in a continuous and timely manner during and after the event a tweet can provide a variety of information such as text images videos audio and additional links in addition there is also a significant amount of metadata that is attached to each tweet this metadata includes information regarding geolocation either a place name or coordinates the author name a defined location a timestamp of the moment the tweet was sent or retweeted the number of retweets the number of favorites a list of hashtags a list of links etc this information is valuable and has the potential to provide intelligence when attempting to extract information for use in crisis response twitter apis also offer a varying number of filters and filtering capabilities including additional filter operators and tweet enhancements e g profile location and un shortened urls the twitter platform provides various apis for searching tweets including i the standard twitter apis consisting of rest apis and streaming apis and ii the enterprise apis including filtered firehose historical search and engagement apis for deeper data analytics fais uses the standard twitter apis because it is free and less challenging to gather twitter flow of information and hashtag driven topics standard api provides an endpoint to return time series counts of tweets matching user query the interested geotagged data that can be gathered using the standard api are images videos text and numeric e g flood depth from citizen inputs for privacy issues and other security concerns regarding personal information our developed twitter apis do not employ any user related features such as number of followers on twitter rather focused on the message related attributes the apis also calculates sentiment of the tweet and the identified categories sentiment is a text tweet analysis that is used to categorize and classify the opinions and sentiments expressed in text three classes of sentiment were implemented using twitter apis including i positive a positive sentiment has been expressed ii negative a negative sentiment has been expressed and iii neutral a neutral or no reaction sentiment has been expressed 3 3 1 twitter streaming api we used tweepy package and integrated it with fais as an easy to use python library for accessing the twitter data twitter developer account was used to access token token secret consumer key and consumer secret to manipulate twitter functionalities to protect the credential the authors decided to develop a twitter streaming bot functions on both ios and mac and deployed it at heroku cloud platform outside of the application access which can be controlled by the heroku user interface heroku is a cloud platform as a service paas that enables system level supervision and coordination of twitter apis crowd sourced data and tweets our developed streaming twitter bot automated all twitter data gathering and continuously watched all twitter activities during real time implementation to be able to watch twitter activity in real time the bot gets notified when new content such as tweets that matches certain criteria such as dorian floods is created this is particularly important when dealing a vast amount of real time tweets we created a reusable python module a module config containing the logic common to the bot functionalities this module reads the authentication credentials from environment variables and creates the tweepy api object by reading the credentials from environment variables user avoids hard coding them into the source code making it much more secure the bot reads the credentials from four environment variables including consumer key consumer secret access token and access token secret after reading the environment variables the bot creates the tweetpy authentication object that eventually uses to create api object the bot uses the logging python module to inform errors and information messages that help user debug them if any issue arises the tweets save constantly in the mangodb database when the bot is in operational use the administrator can choose to activate or deactivate the bot and change the keywords for streaming services our developed twitter bot contains three components fig 1 notably 1 twitter client this component talks to the twitter api and authenticates the connection to use its functionality this also hosts a function called tweets listener which will continuously stream tweets and listen for the matched keywords once it finds the match it will then talk to the other two components 2 tweet analyzer it analyzes the tweets and gives it a score after a match is found 3 twitter streamer this module streams tweets from pre specified keywords analyzes the data and organizes them into a data frame the collected tweets will then store in a mongodb database waiting to be extracted a conceptual process about how to gather real time tweets using the developed streaming twitter bot is shown in fig 1 due to the size of queried data the twitter bot filters the data and only keeps text location author and date of tweets which are eliminated for over 95 of uninterested data fais application has access to mongodb cloud database without having access to twitter bot this allows the user to see the result of our services while protecting fais twitter account privacy and information allowing the users to have access to database resources whenever needed using twitter streaming bot fais was able to identify at risk locations to flooding the application first cycles through a set of usgs web addresses for river gauge height readings parsing these flat files using python web scraping technique and obtain all the latest river levels each river level reading is compared with its respective long term cached average level to identify the highest relative river levels in real time the highest river level then intersects with watershed polygons as well as geotagged tweets to identify at risk locations to flooding geotagged tweets coordinates considered as a center point for approximately 16 km wide square boxes this size is arbitrarily chosen to cover the areas nearby to each flood gauge the retrieved tweets are constantly stored in a mangodb database during operational use which provide an ideal open source database to store json format files 3 3 2 twitter search api twitter search api is a functionality to search past tweets that match the search criteria twitter contains the search functionality but has a limited amount of search result search frequency and time constrained to upgrade the functionality the user requires to pay for the upgraded twitter account with the goals of keeping fais application free and open source an alternative search method was implemented which was using the rest service freely from twitter search function twitter search operation allows the users to look for tweets based on interested user account keywords language and time period but this is not an ideal procedure for the user to go through the website and gather all the interested tweets that they need therefore this project used urllib python library for universal resource locator url handling modules to send specific request to twitter search web service and to gather the tweets the url can be then modified based on the user search criteria the search api continuously sends request url to twitter and waits for the json response from twitter after receiving the json file the search api extracts interested tweets including username text retweets likes date id permalink user id media url and the sentiment the response json file contains a min position data file which can be used to move iteration forward the iteration will keep running until the response exceeds maximum number of tweets limit set by the user note that the date limit is set to the request url sent to twitter twitter outcomes such as the sentiment analysis can be performed for a tweet the sentiment shows whether or not a tweet has positive 1 neutral 0 or negative tones 1 this information is important because it can be used by social scientists to study the impacts of flooding on the citizens and how the residents responded to the flooding and damage 3 4 development of usgs and 511 traffic data apis usgs collects and stores multiple river system data across the united states these are river flow data flood streamflow gauge heights water quality ground water levels and precipitation at defined gauging stations which are strategically placed at the outlets of rivers and lakes these placements allow usgs to correctly monitor and collect the data and compute several statistical indices related to the river flow usgs server provides two different types of flow data including real time and historical records based on datetime fais gathers usgs discharge data daily streamflow and flood data as well as gauge heights and river web camera images the stream data is value of flow rate in cubic feet per second cfs and water level gauge height in feet fais first gathers both historical and real time usgs data for any state in us it then analyzes the information and plots the historical and real time data these data are recorded based on data recording time step such as every 15 min to 1 h intervals fais made usgs data collection seamless and straightforward by providing access to the station s information including station name id name latitude longitude and url to usgs server in addition fais gathers usgs real time cameras and 511 traffic images dot in each state provides traffic data in real time to the users through 511 web application the cameras are strategically placed on the bridges and roads and along the interstate allowing operators to continuously monitor road conditions they also monitor rising and falling water stage over critical infrastructures such as roads and bridges in addition usgs established real time web cameras for critical rivers where there is high chance for flooding and inundation issues fais currently collects six usgs sevillian cameras images across south carolina sc the application uses a dynamic mapping interface to allow the user to select specific traffic camera and access the data in real time indeed there are embedded urls at both usgs and 511 traffic websites that were used to gather both usgs and traffic images in real time images constantly store at mangodb database when the application is in operational use 3 5 web development platform with the use of python in the fais api tools it was straightforward to use the same language and develop a web application the seamless interaction between the api and python motivated this study to use django as a web framework django is a rapid development platform with clean and pragmatic design that provides a widest range of libraries including twitter apis and machine learning tools the key point of making it an ideal development framework is due to its focus on automation as much as possible and adhering to the don t repeat yourself dry principle aiming at reducing repetition of software patterns and avoid redundancy django allows the user to develop a web application using python element along with a classic web development language like html javascript jquery etc this means that developers are not stuck with limited approaches of solving the problem this interaction can also be a downside of this development framework as it requires the knowledge of different languages to debug like ruby on rails fais is version controlled using github since git is a widely used version control system and an open source repository hosting service fig 2 shows the overall workflow of the fais pipeline development the workflow includes collecting the data from various resources analyzing the potential at risk areas and providing actionable results to users stakeholders as illustrated fais combines multiple apis along with machine learning and image processing algorithms google vision api and ffa script written in r to provide both historical and real time information about flood risk incidents our developed prototype offers an end to end open source web based pipeline architecture to address the crucial issue of how first responders and decision makers can be smartly informed and acted in emergency situations to achieve our aim and answer three research questions stated above we tested fais operationally during hurricane dorian flooding event september 04 06 2019 as well as during historical event hurricane florence flooding september 13 16 2018 across the carolinas 4 results and discussion 4 1 retrieval of social geotagged tweets using twitter apis this section addresses the first and second research questions how to programmatically and automatically identify areas at risk of flooding based on crowdsourced data real time flood peak rates and river level information and how to spatially display the retrieved data and implement this information for alert and warning system to address these questions two options i e the search and the streaming apis were included to the fais application for data gathering we used tweepy library as a search api and developed a streaming api using twitter bot search api was more suited to singular and specific queries for tweets whereas the streaming api provided a real time stream of tweets there were widespread flooding events across the carolinas when we designed and beta tested fais application these events caused localized and major flooding as well as above average river level for most basins in the carolinas fais was tested for historical hurricane driven flood events such as hurricane florence september 13 16 2018 across the carolinas in addition the prototype was operationally tested during hurricane dorian event september 04 06 2019 and georeferenced tweets were gathered in real time to identify at risk locations an example from each hurricane driven floods is presented with intersecting at risk locations and geotagged tweets a web based console and a visualization tool geojsonlint13 were used to view results and inspect the polygons 4 1 1 hurricane dorian case study we monitored georeferenced tweets filtered by keywords and queries across the shortlisted areas in the carolinas during hurricane dorian event september 04 06 2019 to identify at risk locations in real time a shell script in python ran on a local computer server the script was reset every 3 h in order to update areas at risk of flooding from the latest national and environmental data sources as well as twitter feeds a period of 15 min was initially chosen as intended trade off between tracking the latest at risk area forecasts api updates varied between 15 min to several hours but based on real time testing the period extend to 3 h to allow some reaction time from impacted citizens on twitter however the choice of time period depends solely on the project requirements as well as the severity and impacts of flooding events a period of 6 12 h was chosen during hurricane dorian flooding event as an intended trade off between tracking the latest at risk areas to flooding api updates varied between 15 min and 6 h depending on flood data time step and that allowed capturing reaction from those at risk areas in real time the collected tweets provided a real time dataset which explored further and used to prioritize at risk locations for dorian flood simulation results not shown here this provided a general indication of the proportion of potentially relevant tweets that could be used to identify flooded areas and improve flood situational awareness for first responders fais filtered georeferenced tweets returned from the streaming api with keywords and a blend of geographic data sources geolocations to show areas affected by dorian floods at a regional scale during hurricane dorian we constantly ran the twitter bot to collect tweets and to determine at risk locations over time the stored tweets provided a real time dataset which then explored and used to prioritize at risk locations to dorian flooding this provided a general indication of the proportion of potentially relevant tweets that could be used to identify flooded areas and improve flood situational awareness data retrieval of dorian flooding was explored via basic text query searches such as hurricane dorian floods and centroids were added to tweet s geotag for creation of map based visualizations currently the size of bounding boxes of tweets are 16 km from the centroid that determines the number of tweets intersect with the bounding boxes to yield and visualize at risk areas the size for 16 km boxes is arbitrarily chosen to cover the areas nearby to each usgs gauge in the coastal carolinas and may differ location by location based on the proximity of the tweets to the nearest flood gauge previous studies showed that messages within 10 km of severely flooded areas had a much higher likelihood of being related to such events see de albuquerque et al 2015 fig 3 illustrated collected geotagged tweets intersected with watershed boundaries for south carolina during hurricane dorian flooding delineating at risk areas could be further improved by populating critical infrastructure bridges roads and flood defense structures levees dams reservoir which are out of the scope of this research table 1 presents several real time tweets along with their geolocation information twitter account etc which were retrieved during hurricane dorian flooding in south carolina september 04 06 2019 the classifier was developed by an annotator the first author manually labeled a subset of 800 tweets from september 04 06 2020 as either relevant or irrelevant tweets 4 1 2 hurricane florence case study we implemented fais to extract geospatial footprint of hurricane florence flooding event using georeferenced tweets the user has an option to select a proper twitter account for the historical tweets gathering we chose nws twitter account nws to gather hurricane florence historical tweets and map the geospatial footprint map view wasnot shown here fais was able to successfully identify a dynamic set of at risk areas using twitter search api for hurricane florence event at risk areas to flooding were identified by intersecting the geotagged tweets with watershed polygons and river gage heights search api that is designed within the fais application was able to collect the tweets and visualize them in on a leaflet map via folium python api fig 4 showed the fais design and outcomes for twitter search api implementation of hurricane florence flooding event the search api provided the source of the tweet image the sentiment and map view of the tweets if the user changes the keywords the twitter search api will then filter the tweets and save the data in a mangodb database we used national geographic base map tiles and geolocated each tweet based on its geographic coordinates each tweet is represented by a clickable marker which provided a pop up box of the tweet information locations coordinates time etc alternative interactive base map can be included to the prototype such as openstreetmap imagery topographic etc fais can be applied nationally across the us running for both real time and post flood event tweet gathering and assessment table 2 displays several tweets gathered using nws account for hurricane florence flooding in sc the tweets were retrieved via location filter that varied in terms of spatial precision less or no exact place metadata or coordinates location filtering reduced mismatch of off topic tweets irrelevant which is an issue for keyword based retrieval de albuquerque et al 2015 although the word flood and its translations were also frequently used figuratively or in a transferred sense e g i am flooded with many tasks we performed the filtering mostly by location filter capabilities designed within twitter apis that seemed to be an effective filtering approach to improve collecting the relevant tweets and reducing manual labor overall we collected over 800 location filtered tweets between september 13 16 2018 across the carolinas 4 1 3 usgs data collection this section addresses the third research question how to seamlessly retrieve data from various sources and how to use this information for making actionable decision fais collects and displays usgs data that include the date discharge cfs and gage height ft along with their associated plots gathering usgs historical data involves selecting the target state the interested station and the date after query criteria is entered fais creates a request url and sends it to the usgs server for collecting the data the prototype displays the data as table view fig 5 a as well as map view fig 5 b and plots the results fig 5 c the user can also upload a csv format file of all the collected data that contains station name and id latitude longitude discharge gauge height and usgs original url fig 5 d in addition fais collects six usgs sevillian cameras images across south carolina these cameras are located at rocky creek near wade hampton rocky branch at whaley st columbia peedee river near florence lake monltrie trailrace canal at moncks corner sc tearcoat brach at i 95 near manning sc and pocotaligo river at i 95 above manning sc the image contains the meta data such as when and where the image was captured fais also collects north carolina nc 511 images in real time several cameras that are located in the coastal region of nc were selected to record road flooding conditions in real time these images are crucial for monitoring of water level and providing early warning to the local community in case the water level increases above a predefined threshold value both usgs and 511 traffic images cane be stored at mangodb database when the application is in operational use readers are referred to fais web server https floodanalytics clemson edu for more detailed information and to stream and store the camera images 4 2 image processing and label detection we integrated google vision api with fais application for object detection of flooded and non flooded images the tool first trains the automated ml vision model and then labeled the datasets this provides custom label detection data with scores we used flooded and non flooded images of hurricane florence to train the google vision api and detect the objects in the images we extracted high quality still images from hurricane florence videos overall we analyzed a range of 100 240 time lapse images to detect the labels and sore them the higher quality images user delivers and the better the design of the model user uses the smarter outcome will be produced fig 6 shows the detected objects for the person street bridge cape fear river nc flood 91 land lot 91 and asphalt 89 are detected as major labels in flooded images while the major labels and scores are given to sport venue 89 and residential area 85 in non flooded images fig 7 showed the score and the number of images that we used to detect the label for this location as illustrated the algorithm was able to detect water flood 0 97 during and after hurricane event the algorithm also detected bridge infrastructure 0 92 for pre event while distinguished it as a reservoir 0 9 during post flooding event due to overtopping issue see fig 8 a number associated with every returned label annotation representing the vision api s assessment of the label s accuracy scores range from 0 no confidence to 1 very high confidence fig 9 overall google vision api detected 11 labels for the flooded and non flooded images at the new bern nc while captured 17 labels for the person street bridge flooding event at the cape fear river nc despite the lower number of labels 92 6 of vision s labels turned out to be relevant 8 errors it would be worth mentioning that google vision api could further improved by cluster equivalent labels together flood water and water resources river whenever a water being is detected in the image by collapsing such labels into one the number of detected labels will definitely decrease and it may also have some impacts on relevance scores our analysis suggests that google vision api has detection problems whenever flooded areas are too small below 50px partially out of the image or occluded by other obstacles such as vehicles and trees this might improve over time with a more specialized pattern recognition layer and approaches such as including an image segmentation to the tool based on watershed algorithm that builds barriers in the locations where water merges these barriers can provide segmentation results that could be used to estimate inundation extend and flood level using a reference such as a bridge pier or a car it should be noted that we didn t focus on other accuracy parameters such as location direction and special traits vision doesn t provide such data further work and a considerable dataset expansion may provide useful insight into flood location and direction accuracy although the difference of a few pixels is usually negligible for most applications the best scoring fragments for a given label matched well with visual appearance of the object in the image indeed the pairs of flooded and non flooded images should have the same view angle and geometry although image processing and machine learning algorithms designed within google vision api yielded successful results as indicated by the scoring metrics there are several challenges in detecting the flooded or inundated areas for example this study found discrepancies in labeling dry flooded image pairs specifically for differentiating flood floodplain waterway and road the algorithm detected some labels that weren t existed in the image such as roof and floodplain to prevent this issue the model can be validated on a simple dataset where a single object e g flood occupied most of the image a wide variety of images with annotated objects that co occur in the same images can be used to test the accuracy of the tool further the algorithm can be enhanced by including in painting procedure to efficiently remove unwanted objects such as the sidewalk and streetlight to detect the inundation border as studied elsewhere witherow et al 2019 although differences in image resolution and lighting and environmental conditions have significant impacts on annotating an image with a reliable label and score flood detection provides important information to stakeholders because the results can be proactively used for flood emergency management indeed the capability to detect temporal changes in image sequences is crucial and this information can be combined with other datasets such as usgs flood peak rates and rainfall radar data to develop an automated image based flood alarm system as a disaster monitoring application image based flood warning information can facilitate proactive monitoring and damage assessment and early warning to rising water levels and associated inundation areas in real time however current deficiencies in google vision api and overall image processing algorithms explained above may limit vision application for image based early warning system 4 3 flood frequency analysis of two usgs gauges fais provides flood frequency analysis to estimate flood quantiles that combines elements of observational analysis stochastic probability distribution and design return periods fais currently uses gumbel distribution to compute ffa for any given flood gauging station in us fig 10 shows ffa for the usgs 02147500 rocky creek at great falls sc sc as well as the usgs 02196000 stevens creek near modoc sc as illustrated annual flood peak of 15000 cfs for the rocky creek represents a design return period of 25 year while the same flood peak shows a design return period of 5 years for the stevens creek gauge the difference is related to the size of the drainage system on small watersheds a 25 year rainfall event may produce a 25 year flood event on large watersheds however the 25 year flow event may be produced by a series of smaller rainfall events this distinction should particularly be kept in mind by the practitioner while dealing with design projects in large watersheds the likelihood of a 100 year flood 50 annual chance occurring at both gauging stations ranging between 32000 to 35000 cfs both gauging stations appeared to experince large flooding events particularly usgs 02196000 this gauge is part of the savannah basin where frequent flooding is a major threat for the residents ffa for this location proved that high peak values made critical contributions to the upper tail of the gumbel probability distribution function this analysis is useful in providing a measurement parameter to assess the damage corresponding to specific flow during flooding event along with civil infrastructure design flood frequency analysis can be used in flood insurance and flood zoning activities accurate estimation of flood frequency not only helps engineers in designing safe infrastructure but also in protection against economic losses due to maintenance of structures however the accuracy of ffa estimates may vary using different probability distributions such as pearson type iii gamma and normal distributions we recommend using gumbel function for a river system with less regulation and significant reservoir operations diversions or urbanization effects efforts are underway to include other distribution functions to the prototype therefore care should be given when the current function uses for any structure design purposes 5 conclusions in this paper we developed a prototype for flood data analytics and assessment using iot apis crowd intelligence and big data gathering approaches our study presented the first step towards identifying at risk areas to flooding in real time and defining the geospatial footprint of a flood event using georeferenced tweets we aimed to use iot apis and collect environmental data river levels and discharge at the national scale in combination with twitter apis to identify the areas affected by a flood the application also uses image processing and machine learning to detect label and scores objects in time lapse images ffa algorithm was also developed in r and embedded with the fais to perform flood design metrics and peak rates that could be combined with twitter and image processing results for studying the design metrics of overtopped structures as well as economic assessment overall our proposed pipeline proved to be robust and user friendly tool for both real time and post event analysis of flooding at regional scale that could help stakeholders for rapid assessment of the situation and damages gathering the data and analysis of flood situation take on average few minutes to select the data periods and set the designed algorithm to run thereby it reveals to be promising for meeting first responders needs during emergencies during real time event the time between a tweet appears online and visually plots for a stakeholder as being potentially relevant in terms of location and content would be in the order of few seconds to minutes thereby this rapid analysis can provide an early information channel for asset allocation and rescue the application proved to be proficient for real time flood assessment as it was tested during a 2 day hurricane dorian event across the carolinas with over 15 000 geotagged tweets collected from 38 dynamic potentially at risk areas using streaming api during hurricane dorian tweets were collected and labeled as relevant because they were related to ongoing hurricane and flood event streaming api assigned a tweet as relevant class when a user tweeted about ongoing dorian floods at the time of posting this includes for example tweets referring to response rescue road closure and failure of critical infrastructure water supply all other tweets were assigned being irrelevant examples include historic flood event commemorations the use of the word flood in figurative or transferred sense etc it is interesting to note that the streaming api showed a great performance to maximizing the retrieval of geotagged tweets via location filtering as well as using more detailed key words and this finding is in agreement with other studies see barker and macleod 2019 ekta et al 2017 tsou et al 2017 morstatter et al 2013 the alternative search api preferentially relied on a tweet s geotag presence and therefore reverts to the more general and less spatially accurate user profile location the user profile location is manually set by the user and maybe different to the specific tweet s actual location furthermore tweets are often contained noisy and redundant data and that cleaning and geoparsing might be difficult and present a time consuming task most importantly the search api permits just a single location per query and is not suitable for spatiotemporal assessment of flooding event in real time our developed streaming api and a twitter bot allowed simultaneous monitoring of multiple locations necessary for real time national scale flood assessment in addition we integrated google vision api to parcel images into labels and scores them as discussed image processing and machine learning is not perfect it often splits objects into separate labels labels objects that are not presented in the image or includes multiple objects in the same label however over a large dataset of time lapse images we found that image processing approaches perform better for many object categories we showed improved object discovery for a more complicated set of flooded and non flooded images for recent hurricane driven floods in the carolinas our research revealed that images and image sequences videos make up about 80 percent of all corporate and public unstructured big data for flood related studies providing an excellent data sources for flood analytics research although the result described herein is encouraging for time lapse images videos analysis the following open issues still exist in image recognition i how do we effectively improve recognition and label detection approaches to cope with numerous object categories that exist and are recognizable by humans and ii how can we apply image processing and machine learning approaches to motion and actions in video and label them we hope that the techniques developed in this study will offer a good starting point in addressing these issues and developing more intelligent and proficient image segmentation and processing methods for flood studies the authors believe that data analytics applications in the field of flood risk management should adopt a modular view moving from a component based to a national scale at present data analytics research remains in its developing phase into existing workflows and practices there appears a major gap particularly in seamless integration of different data sources as number of datasets keep increasing over time although the main issue in integrating these datasets is to ensure data consistency accuracy and completeness for informed decision making processes as data collection through various heterogeneous sources in real time is highly susceptible to noise and uncertainty to this end security as well as privacy issues in data transmission and analytics and storage also need to be under constant control to ensure the authenticity of data and citizen based crowd sourced information while keeping the confidentiality of people s sensitive information in this study we set out the major design choices and decisions made by acknowledging the inputs from many decision makers and first responders we appreciate the discussions with scemd and believe fais can be a more transparent and efficient tool than many other similar pipelines we acknowledge there is a need to evaluate the pipeline in other case studies and real time events also integrating other approaches such as deep learning techniques for word embeddings natural language processing in deriving vector representations for non english tweets this would be crucial since we gathered tweets in spanish and other languages when we tested the prototype operationally during dorian flooding 6 limitation of fais prototype and future work this national scale big data analytics pipeline could be further improved in several ways in the ffa section we recommend computing nonstationary flood frequency models by incorporating external covariates lópez and francés 2013 philips et al 2018 su and chen 2019 meteorological modes such as el niño southern oscillation enso and or physical properties reservoir index are key covariates that could be taken into account for non stationary ffa assessment in addition compound flood calculation can be included to compute ffa based on multivariate extreme variables see renard et al 2013 for the coastal region in addition fais workflow and design for identifying at risk areas to flooding can be further improved the size of bounding boxes of tweets currently 16 km from the centroid requires further attention and evaluation given the boundary effect on the number of tweets intersect bounding boxes and yielding a match as well as considering the density of usgs flood gauges including ungauged areas in an urban setting increasing the size of bounding boxes would likely include more significant tweets thereby more accuracy on the chosen proximity the selected tweets can be also overlaid with other data layers such as census data population density rainfall radar data and federal emergency management agency fema flood hazard map in addition fais image processing and machine learning approaches need further attension google cloud vision is a mature detection tool and comes with more flexible api conventions multiple image formats and a native batch support its object detection functionality generates much more relevant labels and its label detection currently seems mature enough as well although it s not quite perfect yet the biggest issue of this tool seems to be rotational invariance although it might be transparently added to the deep learning model in the future vision has a wide margin of improvement regarding batch video support and more advanced features such as image search object localization and object tracking in video being able to fetch external images e g by url might help speed up api adoption while improving the quality of flood detection features will inspire greater trust from users for further fais development a linking approach could be proposed to increase system autonomy during real time flood risk assessment and data retrieval tasks this automation would help satisfy the need for timeliness and reliability during emergency responses and management efforts are currently underway to combine iot sensor rainfall and water level information to the prototype 7 software and data availability flood analytics information system fais various scripts as they apply to pipeline development description data gathering application designed for flood and twitter data analyses developer nattapon donratanapat contact pleuk5667 gmail com software access https pypi org manage project fais releases year first available 2019 hardware required windows linux macos hardware required intel i3 or mid performance pc with multicore processor and ssd main drive 4 gb memory recommended cost free software and source code are released under the massachusetts institute of technology license software availability all source code can be found at github public repository as well as at the python package index pypi https github com vidyasamadi flood analytics information system fais https pypi org project fais https floodanalytics clemson edu declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 7 acknowledgements this work was supported by the u s national science foundation nsf directorate for engineering under grant cbet 1901646 any opinions findings and discussions expressed in this study are those of the authors and do not necessarily reflect the views of the nsf the authors acknowledge the early and continuous discussion with south carolina stakeholders first responders and decision makers fais prototype has been developed using cluster computing at the university of south carolina as well as clemson university therefore their support and assistant are gratefully acknowledged fais python package is publicly available at the hhr hydosystem and hydroinformatics research github account https github com hhrclemson the prototype is currently running on the ibm cloud computing service https floodanalytics clemson edu appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104828 
25976,numerical lake models are useful tools to study hydrodynamics in lakes and are increasingly applied to extreme weather events however little is known about the accuracy of such models during these short term events we used high frequency data from three lakes to test the performance of three one dimensional 1d hydrodynamic models simstrat gotm glm during storms and heatwaves models reproduced the overall direction and magnitude of changes during the extreme events with accurate timing and little bias changes in volume averaged and surface temperatures and schmidt stability were simulated more accurately than changes in bottom temperature maximum buoyancy frequency or mixed layer depth however in most cases the model error was higher 30 100 during extreme events compared to reference periods as a consequence while 1d lake models can be used to study effects of extreme weather events the increased uncertainty in the simulations should be taken into account when interpreting results keywords storm heatwave model validation simstrat gotm general lake model 1 introduction over the last few years limnologists have devoted increased attention to extreme weather events e g bertani et al 2016 kasprzak et al 2017 andersen et al 2020 chen et al 2020 these are predicted to become more frequent and intense with climate change ipcc 2014 bailey and pol 2016 and can have profound effects on lake ecosystems extreme weather events such as storms and heatwaves have a direct effect on lake physics wind storms can induce mixing events cooling of the surface layer resuspension of sediments and deepening of the thermocline jennings et al 2012 andersen et al 2020 heatwaves have effects that are largely opposite to storms as these cause heating of the water column and strengthen thermal stratification jankowski et al 2006 huber et al 2012 the disturbance in the thermal profile and inflow conditions caused by these events often mediates further changes in nutrients oxygen and phytoplankton community huber et al 2010 klug et al 2012 kasprzak et al 2017 physical disturbances in the water column due to an extreme event are often short lived wilhelm and adrian 2008 jennings et al 2012 kuha et al 2016 stockwell et al 2020 although they can also have a longer effect huber et al 2012 andersen et al 2020 depending on the time of year when they occur mi et al 2018 or whether water transparency is affected as part of the event e g dissolved organic carbon loading or suspended particles klug et al 2012 de eyto et al 2016 perga et al 2018 moreover a short physical disturbance does not automatically imply a short lived effect on biogeochemistry and ecology for example short term mixing events can be a major factor affecting the transport of nutrients stimulating phytoplankton growth soranno et al 1997 crockford et al 2015 numerical lake models are useful tools for understanding aquatic processes disentangling causal factors and for estimating future trajectories of the system forecasting climate scenarios recently several studies have applied one dimensional 1d lake models to study the effects of extreme weather conditions on lake thermal structure or lake ecology bueche et al 2017 mi et al 2018 perga et al 2018 soares et al 2019 chen et al 2020 however there is still a lack of understanding on how accurately lake models actually simulate observed conditions during these short term events it is common practice to assess models on the basis of their goodness of fit e g root mean square error or mean absolute error over the whole calibration and or validation period but these long timescales obscure any potential errors during disturbance by and recovery from short term events evaluating event specific errors will help to understand and minimise the uncertainty in model studies concerning ecosystem effects of extreme weather events showing that a model is capable to accurately simulate system changes caused by extreme weather events will increase our confidence in their capability to provide reliable estimates for future effects of climate change this advanced model testing is an important step in a multi level model assessment hipsey et al 2020 in the present validation study we used more than 10 years of hourly and sub hourly in situ measurements of meteorological variables and water temperature from three lakes of varying depths and mixing dynamics to assess model performance during short term extreme wind and temperature events the analyses were done with three 1d hydrodynamic models simstrat goudsmit et al 2002 gaudard et al 2019 gotm general ocean turbulence model umlauf et al 2005 and glm general lake model hipsey et al 2019 these three models differ in turbulence schemes calibration procedures forcing variables and parameterisations additionally we report on observed changes in lake thermal metrics during storms and heatwaves in lakes with different morphology and mixing regimes we focus our analysis on lake temperature full profile volume averaged surface and bottom temperatures and stratification metrics schmidt stability maximum buoyancy frequency and mixed layer depth based on high frequency temperature profile observations and simulations changes in these thermal metrics can translate into further changes in water transparency and distribution and transport of oxygen and nutrients with repercussions on biological processes we assessed whether the models could reproduce the direction magnitude and timing of change during an event what the accuracy of the models was during extreme events compared to standard conditions and if there was a consistent tendency of the models to over or underestimate changes during an event following this we draw conclusions on the implications of our findings for applying 1d hydrodynamic models to short term extreme wind and temperature events in different types of studies 2 methods 2 1 observational data meteorological and water temperature profile data from lough feeagh ireland lake erken sweden and müggelsee germany were used for this study long term records of sub hourly water temperature profile and surface meteorological data were available for the period 2004 2017 at müggelsee and between 2005 and 2017 for lough feeagh and lake erken measurements of air temperature wind speed and wind direction relative humidity incoming solar radiation and air pressure were collected at each lake cloud cover was available at hourly intervals in the database generated by moras et al 2019 for lake erken and from the airport berlin schönefeld for müggelsee 10 km distance from the lake and daily observations on site were made for lough feeagh lough feeagh 53 56 21 n 9 34 33 w mean depth 14 5 m maximum depth 46 m is a monomictic lake located on the west coast of ireland it experiences high rainfall and wind speeds has no winter ice cover and is rich in dissolved organic carbon doc as a result of drainage from surrounding peatlands de eyto et al 2016 meteorological records and lake temperature profile data are available with measurement frequencies of 1 and 2 min respectively meteorological data were collected on the shore of the lake met éireann 2018 the water temperature profiles were measured by an automated monitoring buoy above the deepest point of the lake with temperature sensors at 0 9 2 5 5 8 11 14 16 18 20 22 27 32 and 42 m below the surface de eyto et al 2020 lake erken 59 50 37 n 18 35 38 e mean depth 9 m maximum depth 21 m is a dimictic lake in the eastern part of sweden it has a surface area of 24 km2 and experiences ice cover in winter and stratification in summer persson and jones 2008 high frequency 30 min lake temperature data from 2005 onwards were used for this study meteorological forcing data with a 5 min frequency are available from july 2008 onwards and hourly forcing data were used before this point in time meteorological data were collected from a station on a small island 500 m off shore while water temperature was measured at a monitoring buoy that was located a further 500 m from the island at 15 m depth water temperature measurements were made at 0 5 m depth intervals prior to 2016 and at 0 25 m intervals after 2016 müggelsee 52 26 24 n 13 38 58 e mean depth 4 9 m maximum depth 8 m located in berlin germany is the shallowest lake in this study it has a surface area of 7 3 km2 is wind exposed and classifies as a polymictic lake wilhelm and adrian 2008 it experiences ice cover in most winters and stratifies during summer at high air temperatures and moderate wind conditions lake temperature data were collected by a floating monitoring station anchored 300 m from the northern lake shore at a depth of 5 5 m water temperature at 2 m depth was measured every 5 min and a profile with 0 5 m depth intervals from the surface up to 5 m depth was measured every hour meteorological data were available every 5 min measured at the monitoring station for more details on used sensors and methodology see wilhelm et al 2006 precipitation and in and outflows were not included in this study and the water level was kept constant in the simulations the reason for this is that these data were available at lower frequencies than the forcing and model time steps and potentially at lower frequencies than the effects of the investigated events additionally annual water level fluctuations are generally less than 1 m in lough feeagh and lake erken moras et al 2019 kelly et al 2020 and only around 0 25 m in müggelsee driescher et al 1993 so water level was assumed to be of minor importance for thermal stratification patterns water transparency was also kept constant in all three models the light attenuation coefficient was calculated from the average secchi depth s d observed over the simulated period with equations specific for the conditions in each lake attenuation coefficient 2 7 s d feeagh koenings and edmundson 1991 2 4 s d lake erken based on observed secchi depths and light profiles unpublished data and 1 3611 s d 0 7105 müggelsee hilt et al 2010 for information on gap filling procedures see suppl mat a 2 2 lake thermal metrics model fit was assessed for the following thermal metrics lake temperature full profile volume averaged temperature surface temperature 1 m depth bottom temperature deepest observation schmidt stability schmidt 1928 idso 1973 maximum buoyancy frequency squared n 2 hereafter referred to as maximum buoyancy frequency and mixed layer depth the r package rlakeanalyzer winslow et al 2019 was used to calculate volume averaged temperature schmidt stability and maximum buoyancy frequency see read et al 2011 for formulas the mixed layer depth was defined using an absolute density difference from the surface following de boyer montégut et al 2004 wilson et al submitted a threshold of 0 15 kg m3 was chosen which gave robust estimates of the depth of stratification for all three lakes if the density of the deepest measured temperature was within this density threshold the water column was assumed to be completely mixed and mixed layer depth was set to the deepest measurement the relation between water temperature and density by martin and mccutcheon 1999 was used 2 3 lake models three 1d hydrodynamic lake models were used in this study simstrat gotm and glm the models take into account lake morphology in turbulence equations but otherwise assume horizontal homogeneity these models all simulate the vertical thermal lake structure and are forced by the same meteorological input but are different in their code structure processes included such as seiche induced mixing or different wavelengths of light and parameterizations for surface fluxes and turbulence so that each model could result in potentially different outcomes a full description of the governing equations used by each of these open source models can be found in goudsmit et al 2002 for simstrat umlauf et al 2005 for gotm and hipsey et al 2019 for glm in addition to manuals and support on the respective websites simstrat https github com eawag appliedsystemanalysis simstrat gotm https gotm net glm http aed see uwa edu au research models glm last access 2020 08 20 specific settings for each model used in this study are provided in the suppl mat b the main differences between the models are mentioned below simstrat and gotm have a fixed layer structure resolving turbulent kinetic energy production and diffusion between layers of fixed thickness layers in glm can vary in thickness or merge depending on the degree of turbulent kinetic energy simstrat was forced with wind direction as an additional input variable this is used to resolve mixing caused by seiches ice cover modules are present in simstrat and glm while there is no ice module in the version of gotm used in this study air pressure is a constant value in simstrat and glm and the average value over the simulated period was used in this study while measured air pressure was used as input in gotm additionally the used version of glm could not be run with sub hourly forcing due to the inherent structure of the code while a forcing frequency of 10 min was used for simstrat and gotm to account for this difference additional runs with hourly forcing were performed for simstrat and gotm whenever these hourly forcing runs were used instead of the ones with 10 min forcing this is specifically mentioned 2 4 calibration a period of one year was used for model spin up and calibration automatic calibration procedures were applied to minimise the error in water temperature at all depths the standard calibration procedures available for each model were different simstrat applied the pest model independent parameter estimation and uncertainty analysis software to minimise the sum of squares of the error doherty 2015 the parsac python package was used for gotm it maximises the log likelihood using a differential evolution method glm was calibrated with the nloptr r package johnson 2014 using the nelder mead simplex algorithm nelder and mead 1965 to minimise the root mean square error model parameters and calibration ranges can be found in suppl mat c the remainder of the data series was used as validation period and to identify extreme events 2 5 storm and heatwave events model performance during extreme weather events was assessed on a selection of ten storms and ten heatwaves per lake the storm events were defined using 10 min wind speed observations for the purpose of identifying storms missing data were not filled suppl mat a so that only actual measured data were used the period april october was used due to the frequent absence of winter profile data in lake erken and müggelsee due to ice cover we chose to base the events on the turbulent wind energy flux at 10 m above the surface p 10 w m 2 instead of wind speed because it is a more direct measure of the amount of energy transfer to the lake and thus a more direct measure of the atmospheric impact on thermal stratification p 10 was calculated as p 10 ρ a i r c d u 10 3 using a fixed drag coefficient c d of 0 9 10 3 wüest et al 2000 where ρ air is air density kg m 3 and u 10 is wind speed at 10 m above the surface m s 1 the top 5 of daily sums of p 10 were selected and days within this selection were considered as a single event if they occurred within two days from each other events with less than 10 h of measured water temperature data or no prior thermal stratification lough feeagh and lake erken only were excluded the exact timing of the start and end of an event were defined when the 8 h moving average of wind speed passed the 75th percentile of all observed wind speed data lastly p 10 was recalculated for the whole duration of an event but the 75th percentile of all p 10 data was subtracted to attach value only to the periods with extremely high wind speeds the events were then ordered by the summed p 10 and the top 10 events were selected the heatwave events were defined using air temperature data to select warm spells relative to the time of the year that is also outside of the middle of summer the two warmest three day degree day periods for each month in the period april august were taken always in two separate years if the temperature on the days before and after this three day period was above the 95th percentile of that month these days were also included in the event events that had insufficient water temperature data or that were within one week of another heatwave event were excluded in that case the next warmest period was chosen until an event with enough lake data was found for lake erken only one event of the four warmest degree day periods in april had enough data instead of picking a colder period in april an extra event in august was selected in order to compare the response of the models during extreme events with average weather conditions ten reference wind and temperature periods were defined the selection methods and time periods were identical to the methods and periods used for the extreme events but instead of selecting events with the highest daily sums of p 10 or highest three day summed temperature periods with values closest to the median were chosen reference events could not be within one week of an extreme event and the duration was fixed to 24 h for wind periods and three days for temperature periods fig 1 for lake erken reference temperature periods were shifted one month may september due to frequently missing data in mid april because of ice cover simulations were initialised one week before each event this initialisation was done to minimise model error and differences between models at the onset of an event but at the same time to allow spin up time of the simulation restricting the simulation period before the extreme event allowed for direct quantification of model performance during extreme weather conditions and isolation of the effects of the event avoiding the effects of accumulated model error during pre event normal weather conditions 2 6 assessment of model performance model performance was evaluated by comparing measured and simulated temperature profiles and the lake thermal metrics calculated from them using mean absolute error mae as a measure for goodness of model fit mae was first calculated for the calibration and validation periods then the mae of the water temperature profile was compared between extreme and reference events with a t test or a wilcoxon rank sum test in case of non normality or outliers for each lake and storms and heatwaves separately to see if different lakes and event types had a different effect on model fit a two way anova on the mae during extreme events only was performed a post hoc tukey test was done to compare lakes with each other a one way anova on the mae was done to compare the performance of the different models during extreme events followed by a post hoc tukey test in addition the difference in thermal metrics between the two pre event days and the two post event days was defined as the change in a metric during an event this change for each metric in observations was tested for significance with a t test or with a wilcoxon sign test in case of non normal data assessed by qq plots and shapiro wilk tests or outliers the performance of the models in simulating the change in a metric during events was assessed by inspecting plots and by calculating the concordance correlation coefficient ccc lin 1989 between the simulated and observed change in metric the ccc is similar to pearson s correlation coefficient but penalises for a deviation from the 1 1 line and was therefore deemed a more accurate statistic for model comparison to test for consistent bias in model simulations during events a one way anova was performed on the change during an extreme event for each lake event type storm heatwave and metric a post hoc tukey test was used to compare models with observations in case the data was non normally distributed assessed by shapiro wilk tests a kruskal wallis test and post hoc dunn test were performed instead using the dunn test r package dinno 2017 to evaluate model accuracy in simulating the timing of events a temporal cross correlation analysis was performed on the simulated and observed datasets for each event and each metric the cross correlation analysis temporally shifted the two datasets relative to each other and the time lag with the highest cross correlation coefficient was taken as the time lag in the simulation data gaps up to 2 h were linearly interpolated larger gaps were considered exclusion criteria for the cross correlation analysis also if the maximum cross correlation coefficient between simulation and observations was below 0 3 the simulation was deemed too inaccurate to determine a time lag all analyses were done with the software r version 3 6 2 r core team 2019 in those cases where the p value of a statistical test was used to distinguish between significant and non significant an alpha of 0 05 was used 3 results 3 1 model performance for the whole simulation period the models successfully reproduced the seasonal cycles of temperature and stratification suppl mat e all models performed reasonably well although glm showed a poorer performance compared to the other two models based on mae during the calibration and validation periods fig 2 3 2 observations during events the observed data confirmed the opposite effects of storms and heatwaves on surface temperature volume averaged temperature schmidt stability and maximum buoyancy frequency fig 3 suppl mat f differences between lakes could be observed in the two deeper lakes of this study lough feeagh and lake erken schmidt stability decreased and the mixed layer deepened during extreme wind events volume averaged temperature was not strongly affected but surface temperature decreased and bottom temperatures increased indicating mixing between top and bottom waters in müggelsee complete mixing occurred during all studied storm events and the water column was often well mixed already before the start of the actual event due to the lake s shallow depth data not shown for four out of the ten storms in müggelsee stratification formed again within a few days after the end of an event which caused no change in schmidt stability or mixed layer depth compared to before the event cooling of all water layers occurred during all ten storm events in müggelsee during high temperature events schmidt stability tended to increase in lough feeagh and lake erken fig 3 there was no change in the mixed layer depth during these events water temperatures at all depths increased but the increase was stronger near the surface than near the bottom after heatwave events in müggelsee temperature in all water layers had increased to a similar extent because effects of the heatwaves on stratification dissipated soon after the end of the events stratification occurred during nine of the ten events but increases in schmidt stability and mixed layer depth did not remain significant after the events in all lakes and during both storm and heatwave events changes in maximum buoyancy frequency tended to follow the same trend as schmidt stability but were not significantly different from zero 3 3 model performance during events generally models performed better during the reference events compared to the extreme events only the simulations for lough feeagh had significantly higher mae during storm events compared to reference wind events while mae during storm events in müggelsee was significantly lower than the reference fig 4 in all lakes the mae of the water temperature profile was higher during the heatwave events compared to the mae during the reference temperature events a two way anova on the mae during extreme events showed that different lakes f 18 58 p 0 001 different event types storm heatwave f 6 54 p 0 01 and the interaction between the two f 6 91 p 0 001 had significant effects on mae lough feeagh had the lowest mae s during storm and heatwave events compared to the other lakes 0 3 0 4 c lower tukey test p 0 001 and mae s were slightly higher during the heatwave events compared to the storm events 0 16 c higher tukey test p 0 01 during the extreme events simstrat and gotm had a similar mae mean of 0 62 c while the mae for glm was 0 24 c 39 higher one way anova f 5 32 p 0 005 tukey test p 0 05 suppl mat g despite these increases in model error the direction and magnitude of change during extreme events was often reproduced by the models figs 5 6 during storms the changes in surface and volume averaged temperature were accurately reproduced by all models concordance correlation coefficient ccc 0 7 fig 6 while the bottom temperature was reproduced with less accuracy simstrat and gotm reproduced changes in schmidt stability and buoyancy frequency during storms better than glm fig 6 the change in mixed layer depth during storms was reproduced with an average ccc of 0 5 for all models the simulated changes during heatwaves had slightly lower performance for surface and volume averaged temperature than during storms fig 6 during the heatwaves simstrat and gotm performed better than glm for all metrics except for bottom temperature where gotm and simstrat performed poorly simstrat and gotm simulated the change in mixed layer depth better during heatwaves compared to storm events on average changes in surface temperature volume average temperature and schmidt stability were simulated more accurately than bottom temperature maximum buoyancy frequency and mixed layer depth simstrat and glm underestimated the increases in bottom temperature during heatwaves in lough feeagh kruskal wallis test chi sq 21 1 p 0 001 glm overestimated the increases in both surface kruskal wallis test chi sq 8 2 p 0 04 and bottom temperatures one way anova f 9 1 p 0 001 during heatwaves in müggelsee none of the other extreme events showed a statistically significant difference in the mean change during the event between models and observations for any metric however glm underestimated the change in bottom temperature during reference wind events as well kruskal wallis test chi sq 15 5 p 0 001 this was likely due to glm showing very little heating of deep water layers during the reference wind events in lough feeagh resulting in low variance and a significant difference with observations some heating of bottom layers is expected even under non extreme conditions for example as a result of vertical turbulent diffusion livingstone 1997 temporal cross correlation could be performed for more than 80 of the events for schmidt stability and volume averaged and surface temperature to calculate the simulation lag where the lags were calculated for these metrics they were less than 1 h in more than 80 of the events suppl mat i for bottom temperature maximum buoyancy frequency and mixed layer depth lags could only be calculated for about half of the events due to inaccurate simulations see material methods about 70 maximum buoyancy frequency and 80 bottom temperature and mixed layer depth of the calculated lags were below 1 h glm was slightly worse in reproducing the timing of the simulations compared to the other models but still had more than 50 schmidt stability maximum buoyancy frequency mixed layer depth or more than 80 temperature metrics of the lags at or below 1 h differences between lakes varied per metric but the timing of the simulations was not consistently better in any of the lakes in suppl mat j we show the temperature profiles and the corresponding detailed model simulations for three example events 4 discussion in the present study we tested the model performance of three 1d models to capture responses to two kinds of extreme weather events in three different lakes firstly we assessed the model performance during the generic validation period the model fit for the full validation period was comparable to other studies rmse ranging from 0 5 to 2 0 c e g fang et al 2012 stepanenko et al 2013 bruce et al 2018 moras et al 2019 schwefel et al 2019 all models performed within the margins commonly found in literature although gotm and simstrat performed better than glm a potential reason for this could have been a consequence of forcing glm with hourly data as compared to 10 min data for gotm and simstrat however this was found not to be the reason for the lower performance of glm because when gotm and simstrat were calibrated and run with hourly data model errors were still about 40 lower than for glm suppl mat e model validation studies like this are valuable to better understand in which systems the models perform well and where they may have limitations it could be that the different layer structure is beneficial for gotm and simstrat in this case of short term extreme events whereas glm with adaptive layers may perform better in water bodies with fluctuating water levels the different calibration routines between the models might also have influenced the model fit more studies of this type are required to understand structural uncertainty in lake models frassl et al 2019 in agreement with previous studies jennings et al 2012 kasprzak et al 2017 andersen et al 2020 wind events caused reduced schmidt stability deepened mixed layers and cooled surface waters while the bottom water warmed in the two deep lakes lough feeagh and lake erken the shallow müggelsee was always completely mixed during the storm events heatwaves are associated with increased surface water temperatures and stronger stratification jankowski et al 2006 jöhnk et al 2008 in this study temperatures in all water layers increased during the high temperature events in lough feeagh and lake erken the surface temperature increase was stronger than near the bottom and stratification strengthened in müggelsee stratification occurred during most of the heatwave events in line with the findings of wilhelm and adrian 2008 however within two days after the heatwave events stratification had reached levels similar to before the event this caused the temperature increase between two days before and two days after the events to be more or less uniform with depth in general all models were able to reproduce the overall trends during either heating or wind events changes in surface and volume averaged temperature and schmidt stability were simulated most accurately while changes in bottom temperatures especially during heatwaves were simulated less well also the simulations of changes in maximum buoyancy frequency during storms and heatwaves and of changes in mixed layer depth during heatwaves were less accurate the present study is amongst the first to look at model performance during short term events in the scenario study by mi et al 2018 glm also simulated credible changes in hypolimnetic temperature mixed layer depth and schmidt stability after a wind perturbation although a comparison with observations during wind events was not performed in addition to reproducing the general trends only in a few cases did models consistently over or underestimate a change during events increases in bottom temperatures were underestimated during heatwaves in lough feeagh by simstrat and glm which suggests that these models fail to adequately simulate increases in bottom temperatures in deep lakes at least over the short time intervals evaluated here however the increases in lough feeagh bottom temperatures during heatwaves were only around 0 2 c glm overestimated temperature increase in the whole water column during heatwaves in müggelsee often by more than 1 c while not showing such a bias over the full validation period we have not explored further why only glm showed this overestimation during heatwaves it may be related to the combination of glm s flexible grid structure and the depth of the lake with müggelsee being a shallow lake the positive bias to warmer temperatures during a heatwave was not observed in the glm simulations of lough feeagh and lake erken this aligns with a glm simulation of lake ammersee mean depth 38 6 m where surface temperature was also not overestimated during a heatwave year bueche et al 2017 as with the overall model performance in this study glm displayed higher model errors than simstrat and gotm during extreme events like the performance during the calibration and validation periods we found that even when simstrat and gotm were forced with hourly inputs these models still showed lower errors than glm supp mat g the example results show that the surface heat fluxes had different values for each model suppl mat j this is partially the result of different calibration outcomes the heat fluxes in the different models followed the same pattern except for the longwave heat flux which was notably different in glm than in the other two models this was likely due to a different parameterisation of the incoming longwave radiation the behaviour of simstrat and gotm under extreme weather conditions was more similar to each other than to glm e g suppl mat j this similarity is likely the result of a similar model structure as both are k epsilon turbulence models rodi 1980 while glm calculates mixing based on energy and density gradients see hipsey et al 2019 the reason for using multiple models in this study was to ascertain if certain models performed significantly better than others but also to provide results that are representative of 1d models in general rather than any one particular model because all three models despite their differences tended to simulate the same general trends but showed a higher mae during extreme weather events we can assume that strengths and weakness in event simulations found here are likely to occur to a similar extent in other 1d hydrodynamic lake models as well most simulations captured the observed timing of the extreme events that is most of the effects were simulated within 1 h of the observations and more than 90 of the modelled events had lags of less than 4 h for all metrics it should be noted however that we could only determine the lag if a reasonable model fit after the cross correlation analysis was obtained cross correlation coefficient of 0 3 or higher so there is a bias towards events that were simulated well for schmidt stability volume averaged and surface temperature lags could be determined in 80 90 of the cases but for the other metrics only in 40 60 of the cases to our knowledge accuracy of timing of short term events in hydrodynamic lake models has rarely been tested yet it is a crucial aspect of model performance especially for forecasting purposes in studies aimed at forecasting phytoplankton blooms timing is sometimes included in model assessment gurkan et al 2006 page et al 2018 and changes in hydrodynamics can be an important driver in phytoplankton dynamics wilhelm and adrian 2008 kasprzak et al 2017 despite the reproduction of the overall trends the low degree of bias and the accurate timing of simulations model error increased during extreme events compared to the reference periods by roughly 30 during storm events in lough feeagh and during heatwaves by 30 lough feeagh lake erken to 100 müggelsee this lower performance shows that predictions made by hydrodynamic models during extreme weather events should be treated with additional caution notable exceptions were the storm events in müggelsee where the model error was 40 lower than during the reference periods this likely has to do with the shallow depth of müggelsee and might be systematic for shallow lakes in general the selected storm events were some of the most extreme in a 14 year period and as a result this shallow lake mixed completely this was correctly simulated by the models and errors estimating these isothermal conditions tended to be lower than the errors than during the reference periods when stratification sometimes occurred the larger errors during the storms in the deep lakes and during heatwaves can have multiple causes firstly many of the models parameterizations are nonlinear and thus the magnitude of energy and turbulence fluxes might increase faster than linearly under more extreme conditions by using high frequency driving data averaging errors relating to removing high frequency variation in meteorological forcing data were reduced however it is still possible that the values assigned to model coefficients during long term calibration may not be appropriate for the extreme conditions of specific events and this would then automatically cause a larger error secondly the assumption of one dimensionality in the models holds less well during extreme events during storms the leeside of a lake and bays experience notably less wind forcing internal waves can form and wave breaking creates turbulence on underwater slopes wüest et al 2000 macintyre and jellison 2001 shallow areas tend to stratify earlier and warm faster than deep areas woolway and merchant 2018 potentially creating more horizontal heterogeneity during heatwaves these three dimensional processes are not included in 1d models and these sources of error may be accentuated during extreme events lastly extreme events could also increase the importance of processes that were not included or kept constant in this study such as precipitation inflow or turbidity we found that extreme weather generally resulted in momentarily less accurate simulation of lake conditions even with high frequency forcing data collected on site and with all three models but to what extent is this a problem numerical process based lake models are still amongst the best tools we have to simulate thermal dynamics in lakes during extreme weather events and the fact that uncertainty increases during these conditions does not invalidate their usefulness in flood and hurricane forecasting it is acknowledged that numerical models have large uncertainty during extreme weather conditions todini 2004 heming et al 2019 the uncertainty connected to these forecasts is an important aspect of the output that is included when informing decision makers and the public in the case of extreme events in lakes uncertainty can be taken into account partially by simply being aware of it for example since the timing of event impacts was simulated accurately for some purposes of modelling it might be sufficient to take the timing of the event as information and knowing that the magnitude of the impact could differ from the simulations however to quantify the uncertainty during extreme events a potential pathway would be ensemble modelling with forcing scenarios of varying intensity because we found little consistent bias model runs with higher and lower wind speeds or temperatures could provide an uncertainty band during extreme weather events more research would be needed to determine what methods would be best suited to quantify uncertainty during extreme events the models in this study captured the overall trends and the range of error during the extreme events mae 0 4 1 2 c is similar to the level of uncertainty found in other lake modelling studies during regular conditions e g soulignac et al 2018 moras et al 2019 larger model uncertainty during extreme events is to a certain extent expected because of greater spatial variations in lake thermal structure larger energy fluxes and more rapid changes in thermal gradients in the water column at small temporal scale compared to non extreme circumstances it depends on the objective of the modeller if this reduced accuracy poses a problem larger error during extreme events might not pose a problem for long term climate forecasting as model fit during these short periods is generally not of interest for this type of studies an exception to this statement would be if there are long term consequences of extreme events as in the case of tipping points scheffer et al 2001 for short term forecasting however extreme events are amongst the most important events to capture this study shows that 1d lake models can be used to simulate these events but the short term predictions may be less precise than would occur under more normal conditions this should be kept in mind when interpreting the forecasts the results in the present study suggest that forecasts for temperature data and schmidt stability will be more precise than for maximum buoyancy frequency and mixed layer depth for scenario studies as in mi et al 2018 the increased uncertainty during events is likely not a major issue the absolute magnitude of the effect of an event might differ from observations but the overall response is simulated coupling of physical models and biogeochemical models involves a risk of error propagation a wrong estimation of water temperature could lead to wrong growth rates or a too shallow mixing event results in less nutrient upwelling than in reality because of this it is likely that uncertainty during extreme events also increases for biogeochemical models 5 conclusion extreme weather events are projected to increase in magnitude and frequency and can have large and diverse effects on lake ecosystems one dimensional hydrodynamic lake models could help in elucidating their impacts on lakes but so far no studies have investigated how well these models perform during such events in this study simstrat gotm and glm were run during multiple selected storms and heatwaves in three lakes in order to assess model performance the overall effects of extreme weather on lake temperature and stratification metrics were captured by the models with correct timing and little bias but the precision of the model output was reduced compared to non extreme conditions as with the model fit during calibration and validation simstrat and gotm performed better during extreme events than glm the implications of these findings ultimately depend on a modeller s objectives but we are convinced that the findings in this paper can help to elucidate the uncertainty of model predictions during extreme weather events this would lead to a more responsible use of 1d lake models as uncertainty is an important part of model simulations we propose that 1d lake models can be adequate tools to evaluate changes in hydrodynamics during extreme weather events provided that the increased uncertainty during these events is kept in mind when interpreting the results declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors express their thanks to the marine institute the swedish infrastructure for ecosystem science sites and uppsala university and the leibniz institute of freshwater ecology and inland fisheries igb for collecting and sharing the water temperature data of lough feeagh lake erken and müggelsee j p m a i a and j a a s were funded by the mantel itn management of climatic extreme events in lakes and reservoirs for the protection of ecosystem services through the european union s horizon 2020 research and innovation programme under the marie skłodowska curie grant agreement no 722518 r a acknowledges support by the limnoscenes project ad 91 22 1 within the biodiversa and the belmont forum programme appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104852 software availability instructions to download and install the models used in this study can be found at simstrat version 2 1 https github com eawag appliedsystemanalysis simstrat tree v2 1 gotm version 5 1 https gotm net glm version 3 0 4 https aed see uwa edu au research models glm index html last access 2020 08 20 
25976,numerical lake models are useful tools to study hydrodynamics in lakes and are increasingly applied to extreme weather events however little is known about the accuracy of such models during these short term events we used high frequency data from three lakes to test the performance of three one dimensional 1d hydrodynamic models simstrat gotm glm during storms and heatwaves models reproduced the overall direction and magnitude of changes during the extreme events with accurate timing and little bias changes in volume averaged and surface temperatures and schmidt stability were simulated more accurately than changes in bottom temperature maximum buoyancy frequency or mixed layer depth however in most cases the model error was higher 30 100 during extreme events compared to reference periods as a consequence while 1d lake models can be used to study effects of extreme weather events the increased uncertainty in the simulations should be taken into account when interpreting results keywords storm heatwave model validation simstrat gotm general lake model 1 introduction over the last few years limnologists have devoted increased attention to extreme weather events e g bertani et al 2016 kasprzak et al 2017 andersen et al 2020 chen et al 2020 these are predicted to become more frequent and intense with climate change ipcc 2014 bailey and pol 2016 and can have profound effects on lake ecosystems extreme weather events such as storms and heatwaves have a direct effect on lake physics wind storms can induce mixing events cooling of the surface layer resuspension of sediments and deepening of the thermocline jennings et al 2012 andersen et al 2020 heatwaves have effects that are largely opposite to storms as these cause heating of the water column and strengthen thermal stratification jankowski et al 2006 huber et al 2012 the disturbance in the thermal profile and inflow conditions caused by these events often mediates further changes in nutrients oxygen and phytoplankton community huber et al 2010 klug et al 2012 kasprzak et al 2017 physical disturbances in the water column due to an extreme event are often short lived wilhelm and adrian 2008 jennings et al 2012 kuha et al 2016 stockwell et al 2020 although they can also have a longer effect huber et al 2012 andersen et al 2020 depending on the time of year when they occur mi et al 2018 or whether water transparency is affected as part of the event e g dissolved organic carbon loading or suspended particles klug et al 2012 de eyto et al 2016 perga et al 2018 moreover a short physical disturbance does not automatically imply a short lived effect on biogeochemistry and ecology for example short term mixing events can be a major factor affecting the transport of nutrients stimulating phytoplankton growth soranno et al 1997 crockford et al 2015 numerical lake models are useful tools for understanding aquatic processes disentangling causal factors and for estimating future trajectories of the system forecasting climate scenarios recently several studies have applied one dimensional 1d lake models to study the effects of extreme weather conditions on lake thermal structure or lake ecology bueche et al 2017 mi et al 2018 perga et al 2018 soares et al 2019 chen et al 2020 however there is still a lack of understanding on how accurately lake models actually simulate observed conditions during these short term events it is common practice to assess models on the basis of their goodness of fit e g root mean square error or mean absolute error over the whole calibration and or validation period but these long timescales obscure any potential errors during disturbance by and recovery from short term events evaluating event specific errors will help to understand and minimise the uncertainty in model studies concerning ecosystem effects of extreme weather events showing that a model is capable to accurately simulate system changes caused by extreme weather events will increase our confidence in their capability to provide reliable estimates for future effects of climate change this advanced model testing is an important step in a multi level model assessment hipsey et al 2020 in the present validation study we used more than 10 years of hourly and sub hourly in situ measurements of meteorological variables and water temperature from three lakes of varying depths and mixing dynamics to assess model performance during short term extreme wind and temperature events the analyses were done with three 1d hydrodynamic models simstrat goudsmit et al 2002 gaudard et al 2019 gotm general ocean turbulence model umlauf et al 2005 and glm general lake model hipsey et al 2019 these three models differ in turbulence schemes calibration procedures forcing variables and parameterisations additionally we report on observed changes in lake thermal metrics during storms and heatwaves in lakes with different morphology and mixing regimes we focus our analysis on lake temperature full profile volume averaged surface and bottom temperatures and stratification metrics schmidt stability maximum buoyancy frequency and mixed layer depth based on high frequency temperature profile observations and simulations changes in these thermal metrics can translate into further changes in water transparency and distribution and transport of oxygen and nutrients with repercussions on biological processes we assessed whether the models could reproduce the direction magnitude and timing of change during an event what the accuracy of the models was during extreme events compared to standard conditions and if there was a consistent tendency of the models to over or underestimate changes during an event following this we draw conclusions on the implications of our findings for applying 1d hydrodynamic models to short term extreme wind and temperature events in different types of studies 2 methods 2 1 observational data meteorological and water temperature profile data from lough feeagh ireland lake erken sweden and müggelsee germany were used for this study long term records of sub hourly water temperature profile and surface meteorological data were available for the period 2004 2017 at müggelsee and between 2005 and 2017 for lough feeagh and lake erken measurements of air temperature wind speed and wind direction relative humidity incoming solar radiation and air pressure were collected at each lake cloud cover was available at hourly intervals in the database generated by moras et al 2019 for lake erken and from the airport berlin schönefeld for müggelsee 10 km distance from the lake and daily observations on site were made for lough feeagh lough feeagh 53 56 21 n 9 34 33 w mean depth 14 5 m maximum depth 46 m is a monomictic lake located on the west coast of ireland it experiences high rainfall and wind speeds has no winter ice cover and is rich in dissolved organic carbon doc as a result of drainage from surrounding peatlands de eyto et al 2016 meteorological records and lake temperature profile data are available with measurement frequencies of 1 and 2 min respectively meteorological data were collected on the shore of the lake met éireann 2018 the water temperature profiles were measured by an automated monitoring buoy above the deepest point of the lake with temperature sensors at 0 9 2 5 5 8 11 14 16 18 20 22 27 32 and 42 m below the surface de eyto et al 2020 lake erken 59 50 37 n 18 35 38 e mean depth 9 m maximum depth 21 m is a dimictic lake in the eastern part of sweden it has a surface area of 24 km2 and experiences ice cover in winter and stratification in summer persson and jones 2008 high frequency 30 min lake temperature data from 2005 onwards were used for this study meteorological forcing data with a 5 min frequency are available from july 2008 onwards and hourly forcing data were used before this point in time meteorological data were collected from a station on a small island 500 m off shore while water temperature was measured at a monitoring buoy that was located a further 500 m from the island at 15 m depth water temperature measurements were made at 0 5 m depth intervals prior to 2016 and at 0 25 m intervals after 2016 müggelsee 52 26 24 n 13 38 58 e mean depth 4 9 m maximum depth 8 m located in berlin germany is the shallowest lake in this study it has a surface area of 7 3 km2 is wind exposed and classifies as a polymictic lake wilhelm and adrian 2008 it experiences ice cover in most winters and stratifies during summer at high air temperatures and moderate wind conditions lake temperature data were collected by a floating monitoring station anchored 300 m from the northern lake shore at a depth of 5 5 m water temperature at 2 m depth was measured every 5 min and a profile with 0 5 m depth intervals from the surface up to 5 m depth was measured every hour meteorological data were available every 5 min measured at the monitoring station for more details on used sensors and methodology see wilhelm et al 2006 precipitation and in and outflows were not included in this study and the water level was kept constant in the simulations the reason for this is that these data were available at lower frequencies than the forcing and model time steps and potentially at lower frequencies than the effects of the investigated events additionally annual water level fluctuations are generally less than 1 m in lough feeagh and lake erken moras et al 2019 kelly et al 2020 and only around 0 25 m in müggelsee driescher et al 1993 so water level was assumed to be of minor importance for thermal stratification patterns water transparency was also kept constant in all three models the light attenuation coefficient was calculated from the average secchi depth s d observed over the simulated period with equations specific for the conditions in each lake attenuation coefficient 2 7 s d feeagh koenings and edmundson 1991 2 4 s d lake erken based on observed secchi depths and light profiles unpublished data and 1 3611 s d 0 7105 müggelsee hilt et al 2010 for information on gap filling procedures see suppl mat a 2 2 lake thermal metrics model fit was assessed for the following thermal metrics lake temperature full profile volume averaged temperature surface temperature 1 m depth bottom temperature deepest observation schmidt stability schmidt 1928 idso 1973 maximum buoyancy frequency squared n 2 hereafter referred to as maximum buoyancy frequency and mixed layer depth the r package rlakeanalyzer winslow et al 2019 was used to calculate volume averaged temperature schmidt stability and maximum buoyancy frequency see read et al 2011 for formulas the mixed layer depth was defined using an absolute density difference from the surface following de boyer montégut et al 2004 wilson et al submitted a threshold of 0 15 kg m3 was chosen which gave robust estimates of the depth of stratification for all three lakes if the density of the deepest measured temperature was within this density threshold the water column was assumed to be completely mixed and mixed layer depth was set to the deepest measurement the relation between water temperature and density by martin and mccutcheon 1999 was used 2 3 lake models three 1d hydrodynamic lake models were used in this study simstrat gotm and glm the models take into account lake morphology in turbulence equations but otherwise assume horizontal homogeneity these models all simulate the vertical thermal lake structure and are forced by the same meteorological input but are different in their code structure processes included such as seiche induced mixing or different wavelengths of light and parameterizations for surface fluxes and turbulence so that each model could result in potentially different outcomes a full description of the governing equations used by each of these open source models can be found in goudsmit et al 2002 for simstrat umlauf et al 2005 for gotm and hipsey et al 2019 for glm in addition to manuals and support on the respective websites simstrat https github com eawag appliedsystemanalysis simstrat gotm https gotm net glm http aed see uwa edu au research models glm last access 2020 08 20 specific settings for each model used in this study are provided in the suppl mat b the main differences between the models are mentioned below simstrat and gotm have a fixed layer structure resolving turbulent kinetic energy production and diffusion between layers of fixed thickness layers in glm can vary in thickness or merge depending on the degree of turbulent kinetic energy simstrat was forced with wind direction as an additional input variable this is used to resolve mixing caused by seiches ice cover modules are present in simstrat and glm while there is no ice module in the version of gotm used in this study air pressure is a constant value in simstrat and glm and the average value over the simulated period was used in this study while measured air pressure was used as input in gotm additionally the used version of glm could not be run with sub hourly forcing due to the inherent structure of the code while a forcing frequency of 10 min was used for simstrat and gotm to account for this difference additional runs with hourly forcing were performed for simstrat and gotm whenever these hourly forcing runs were used instead of the ones with 10 min forcing this is specifically mentioned 2 4 calibration a period of one year was used for model spin up and calibration automatic calibration procedures were applied to minimise the error in water temperature at all depths the standard calibration procedures available for each model were different simstrat applied the pest model independent parameter estimation and uncertainty analysis software to minimise the sum of squares of the error doherty 2015 the parsac python package was used for gotm it maximises the log likelihood using a differential evolution method glm was calibrated with the nloptr r package johnson 2014 using the nelder mead simplex algorithm nelder and mead 1965 to minimise the root mean square error model parameters and calibration ranges can be found in suppl mat c the remainder of the data series was used as validation period and to identify extreme events 2 5 storm and heatwave events model performance during extreme weather events was assessed on a selection of ten storms and ten heatwaves per lake the storm events were defined using 10 min wind speed observations for the purpose of identifying storms missing data were not filled suppl mat a so that only actual measured data were used the period april october was used due to the frequent absence of winter profile data in lake erken and müggelsee due to ice cover we chose to base the events on the turbulent wind energy flux at 10 m above the surface p 10 w m 2 instead of wind speed because it is a more direct measure of the amount of energy transfer to the lake and thus a more direct measure of the atmospheric impact on thermal stratification p 10 was calculated as p 10 ρ a i r c d u 10 3 using a fixed drag coefficient c d of 0 9 10 3 wüest et al 2000 where ρ air is air density kg m 3 and u 10 is wind speed at 10 m above the surface m s 1 the top 5 of daily sums of p 10 were selected and days within this selection were considered as a single event if they occurred within two days from each other events with less than 10 h of measured water temperature data or no prior thermal stratification lough feeagh and lake erken only were excluded the exact timing of the start and end of an event were defined when the 8 h moving average of wind speed passed the 75th percentile of all observed wind speed data lastly p 10 was recalculated for the whole duration of an event but the 75th percentile of all p 10 data was subtracted to attach value only to the periods with extremely high wind speeds the events were then ordered by the summed p 10 and the top 10 events were selected the heatwave events were defined using air temperature data to select warm spells relative to the time of the year that is also outside of the middle of summer the two warmest three day degree day periods for each month in the period april august were taken always in two separate years if the temperature on the days before and after this three day period was above the 95th percentile of that month these days were also included in the event events that had insufficient water temperature data or that were within one week of another heatwave event were excluded in that case the next warmest period was chosen until an event with enough lake data was found for lake erken only one event of the four warmest degree day periods in april had enough data instead of picking a colder period in april an extra event in august was selected in order to compare the response of the models during extreme events with average weather conditions ten reference wind and temperature periods were defined the selection methods and time periods were identical to the methods and periods used for the extreme events but instead of selecting events with the highest daily sums of p 10 or highest three day summed temperature periods with values closest to the median were chosen reference events could not be within one week of an extreme event and the duration was fixed to 24 h for wind periods and three days for temperature periods fig 1 for lake erken reference temperature periods were shifted one month may september due to frequently missing data in mid april because of ice cover simulations were initialised one week before each event this initialisation was done to minimise model error and differences between models at the onset of an event but at the same time to allow spin up time of the simulation restricting the simulation period before the extreme event allowed for direct quantification of model performance during extreme weather conditions and isolation of the effects of the event avoiding the effects of accumulated model error during pre event normal weather conditions 2 6 assessment of model performance model performance was evaluated by comparing measured and simulated temperature profiles and the lake thermal metrics calculated from them using mean absolute error mae as a measure for goodness of model fit mae was first calculated for the calibration and validation periods then the mae of the water temperature profile was compared between extreme and reference events with a t test or a wilcoxon rank sum test in case of non normality or outliers for each lake and storms and heatwaves separately to see if different lakes and event types had a different effect on model fit a two way anova on the mae during extreme events only was performed a post hoc tukey test was done to compare lakes with each other a one way anova on the mae was done to compare the performance of the different models during extreme events followed by a post hoc tukey test in addition the difference in thermal metrics between the two pre event days and the two post event days was defined as the change in a metric during an event this change for each metric in observations was tested for significance with a t test or with a wilcoxon sign test in case of non normal data assessed by qq plots and shapiro wilk tests or outliers the performance of the models in simulating the change in a metric during events was assessed by inspecting plots and by calculating the concordance correlation coefficient ccc lin 1989 between the simulated and observed change in metric the ccc is similar to pearson s correlation coefficient but penalises for a deviation from the 1 1 line and was therefore deemed a more accurate statistic for model comparison to test for consistent bias in model simulations during events a one way anova was performed on the change during an extreme event for each lake event type storm heatwave and metric a post hoc tukey test was used to compare models with observations in case the data was non normally distributed assessed by shapiro wilk tests a kruskal wallis test and post hoc dunn test were performed instead using the dunn test r package dinno 2017 to evaluate model accuracy in simulating the timing of events a temporal cross correlation analysis was performed on the simulated and observed datasets for each event and each metric the cross correlation analysis temporally shifted the two datasets relative to each other and the time lag with the highest cross correlation coefficient was taken as the time lag in the simulation data gaps up to 2 h were linearly interpolated larger gaps were considered exclusion criteria for the cross correlation analysis also if the maximum cross correlation coefficient between simulation and observations was below 0 3 the simulation was deemed too inaccurate to determine a time lag all analyses were done with the software r version 3 6 2 r core team 2019 in those cases where the p value of a statistical test was used to distinguish between significant and non significant an alpha of 0 05 was used 3 results 3 1 model performance for the whole simulation period the models successfully reproduced the seasonal cycles of temperature and stratification suppl mat e all models performed reasonably well although glm showed a poorer performance compared to the other two models based on mae during the calibration and validation periods fig 2 3 2 observations during events the observed data confirmed the opposite effects of storms and heatwaves on surface temperature volume averaged temperature schmidt stability and maximum buoyancy frequency fig 3 suppl mat f differences between lakes could be observed in the two deeper lakes of this study lough feeagh and lake erken schmidt stability decreased and the mixed layer deepened during extreme wind events volume averaged temperature was not strongly affected but surface temperature decreased and bottom temperatures increased indicating mixing between top and bottom waters in müggelsee complete mixing occurred during all studied storm events and the water column was often well mixed already before the start of the actual event due to the lake s shallow depth data not shown for four out of the ten storms in müggelsee stratification formed again within a few days after the end of an event which caused no change in schmidt stability or mixed layer depth compared to before the event cooling of all water layers occurred during all ten storm events in müggelsee during high temperature events schmidt stability tended to increase in lough feeagh and lake erken fig 3 there was no change in the mixed layer depth during these events water temperatures at all depths increased but the increase was stronger near the surface than near the bottom after heatwave events in müggelsee temperature in all water layers had increased to a similar extent because effects of the heatwaves on stratification dissipated soon after the end of the events stratification occurred during nine of the ten events but increases in schmidt stability and mixed layer depth did not remain significant after the events in all lakes and during both storm and heatwave events changes in maximum buoyancy frequency tended to follow the same trend as schmidt stability but were not significantly different from zero 3 3 model performance during events generally models performed better during the reference events compared to the extreme events only the simulations for lough feeagh had significantly higher mae during storm events compared to reference wind events while mae during storm events in müggelsee was significantly lower than the reference fig 4 in all lakes the mae of the water temperature profile was higher during the heatwave events compared to the mae during the reference temperature events a two way anova on the mae during extreme events showed that different lakes f 18 58 p 0 001 different event types storm heatwave f 6 54 p 0 01 and the interaction between the two f 6 91 p 0 001 had significant effects on mae lough feeagh had the lowest mae s during storm and heatwave events compared to the other lakes 0 3 0 4 c lower tukey test p 0 001 and mae s were slightly higher during the heatwave events compared to the storm events 0 16 c higher tukey test p 0 01 during the extreme events simstrat and gotm had a similar mae mean of 0 62 c while the mae for glm was 0 24 c 39 higher one way anova f 5 32 p 0 005 tukey test p 0 05 suppl mat g despite these increases in model error the direction and magnitude of change during extreme events was often reproduced by the models figs 5 6 during storms the changes in surface and volume averaged temperature were accurately reproduced by all models concordance correlation coefficient ccc 0 7 fig 6 while the bottom temperature was reproduced with less accuracy simstrat and gotm reproduced changes in schmidt stability and buoyancy frequency during storms better than glm fig 6 the change in mixed layer depth during storms was reproduced with an average ccc of 0 5 for all models the simulated changes during heatwaves had slightly lower performance for surface and volume averaged temperature than during storms fig 6 during the heatwaves simstrat and gotm performed better than glm for all metrics except for bottom temperature where gotm and simstrat performed poorly simstrat and gotm simulated the change in mixed layer depth better during heatwaves compared to storm events on average changes in surface temperature volume average temperature and schmidt stability were simulated more accurately than bottom temperature maximum buoyancy frequency and mixed layer depth simstrat and glm underestimated the increases in bottom temperature during heatwaves in lough feeagh kruskal wallis test chi sq 21 1 p 0 001 glm overestimated the increases in both surface kruskal wallis test chi sq 8 2 p 0 04 and bottom temperatures one way anova f 9 1 p 0 001 during heatwaves in müggelsee none of the other extreme events showed a statistically significant difference in the mean change during the event between models and observations for any metric however glm underestimated the change in bottom temperature during reference wind events as well kruskal wallis test chi sq 15 5 p 0 001 this was likely due to glm showing very little heating of deep water layers during the reference wind events in lough feeagh resulting in low variance and a significant difference with observations some heating of bottom layers is expected even under non extreme conditions for example as a result of vertical turbulent diffusion livingstone 1997 temporal cross correlation could be performed for more than 80 of the events for schmidt stability and volume averaged and surface temperature to calculate the simulation lag where the lags were calculated for these metrics they were less than 1 h in more than 80 of the events suppl mat i for bottom temperature maximum buoyancy frequency and mixed layer depth lags could only be calculated for about half of the events due to inaccurate simulations see material methods about 70 maximum buoyancy frequency and 80 bottom temperature and mixed layer depth of the calculated lags were below 1 h glm was slightly worse in reproducing the timing of the simulations compared to the other models but still had more than 50 schmidt stability maximum buoyancy frequency mixed layer depth or more than 80 temperature metrics of the lags at or below 1 h differences between lakes varied per metric but the timing of the simulations was not consistently better in any of the lakes in suppl mat j we show the temperature profiles and the corresponding detailed model simulations for three example events 4 discussion in the present study we tested the model performance of three 1d models to capture responses to two kinds of extreme weather events in three different lakes firstly we assessed the model performance during the generic validation period the model fit for the full validation period was comparable to other studies rmse ranging from 0 5 to 2 0 c e g fang et al 2012 stepanenko et al 2013 bruce et al 2018 moras et al 2019 schwefel et al 2019 all models performed within the margins commonly found in literature although gotm and simstrat performed better than glm a potential reason for this could have been a consequence of forcing glm with hourly data as compared to 10 min data for gotm and simstrat however this was found not to be the reason for the lower performance of glm because when gotm and simstrat were calibrated and run with hourly data model errors were still about 40 lower than for glm suppl mat e model validation studies like this are valuable to better understand in which systems the models perform well and where they may have limitations it could be that the different layer structure is beneficial for gotm and simstrat in this case of short term extreme events whereas glm with adaptive layers may perform better in water bodies with fluctuating water levels the different calibration routines between the models might also have influenced the model fit more studies of this type are required to understand structural uncertainty in lake models frassl et al 2019 in agreement with previous studies jennings et al 2012 kasprzak et al 2017 andersen et al 2020 wind events caused reduced schmidt stability deepened mixed layers and cooled surface waters while the bottom water warmed in the two deep lakes lough feeagh and lake erken the shallow müggelsee was always completely mixed during the storm events heatwaves are associated with increased surface water temperatures and stronger stratification jankowski et al 2006 jöhnk et al 2008 in this study temperatures in all water layers increased during the high temperature events in lough feeagh and lake erken the surface temperature increase was stronger than near the bottom and stratification strengthened in müggelsee stratification occurred during most of the heatwave events in line with the findings of wilhelm and adrian 2008 however within two days after the heatwave events stratification had reached levels similar to before the event this caused the temperature increase between two days before and two days after the events to be more or less uniform with depth in general all models were able to reproduce the overall trends during either heating or wind events changes in surface and volume averaged temperature and schmidt stability were simulated most accurately while changes in bottom temperatures especially during heatwaves were simulated less well also the simulations of changes in maximum buoyancy frequency during storms and heatwaves and of changes in mixed layer depth during heatwaves were less accurate the present study is amongst the first to look at model performance during short term events in the scenario study by mi et al 2018 glm also simulated credible changes in hypolimnetic temperature mixed layer depth and schmidt stability after a wind perturbation although a comparison with observations during wind events was not performed in addition to reproducing the general trends only in a few cases did models consistently over or underestimate a change during events increases in bottom temperatures were underestimated during heatwaves in lough feeagh by simstrat and glm which suggests that these models fail to adequately simulate increases in bottom temperatures in deep lakes at least over the short time intervals evaluated here however the increases in lough feeagh bottom temperatures during heatwaves were only around 0 2 c glm overestimated temperature increase in the whole water column during heatwaves in müggelsee often by more than 1 c while not showing such a bias over the full validation period we have not explored further why only glm showed this overestimation during heatwaves it may be related to the combination of glm s flexible grid structure and the depth of the lake with müggelsee being a shallow lake the positive bias to warmer temperatures during a heatwave was not observed in the glm simulations of lough feeagh and lake erken this aligns with a glm simulation of lake ammersee mean depth 38 6 m where surface temperature was also not overestimated during a heatwave year bueche et al 2017 as with the overall model performance in this study glm displayed higher model errors than simstrat and gotm during extreme events like the performance during the calibration and validation periods we found that even when simstrat and gotm were forced with hourly inputs these models still showed lower errors than glm supp mat g the example results show that the surface heat fluxes had different values for each model suppl mat j this is partially the result of different calibration outcomes the heat fluxes in the different models followed the same pattern except for the longwave heat flux which was notably different in glm than in the other two models this was likely due to a different parameterisation of the incoming longwave radiation the behaviour of simstrat and gotm under extreme weather conditions was more similar to each other than to glm e g suppl mat j this similarity is likely the result of a similar model structure as both are k epsilon turbulence models rodi 1980 while glm calculates mixing based on energy and density gradients see hipsey et al 2019 the reason for using multiple models in this study was to ascertain if certain models performed significantly better than others but also to provide results that are representative of 1d models in general rather than any one particular model because all three models despite their differences tended to simulate the same general trends but showed a higher mae during extreme weather events we can assume that strengths and weakness in event simulations found here are likely to occur to a similar extent in other 1d hydrodynamic lake models as well most simulations captured the observed timing of the extreme events that is most of the effects were simulated within 1 h of the observations and more than 90 of the modelled events had lags of less than 4 h for all metrics it should be noted however that we could only determine the lag if a reasonable model fit after the cross correlation analysis was obtained cross correlation coefficient of 0 3 or higher so there is a bias towards events that were simulated well for schmidt stability volume averaged and surface temperature lags could be determined in 80 90 of the cases but for the other metrics only in 40 60 of the cases to our knowledge accuracy of timing of short term events in hydrodynamic lake models has rarely been tested yet it is a crucial aspect of model performance especially for forecasting purposes in studies aimed at forecasting phytoplankton blooms timing is sometimes included in model assessment gurkan et al 2006 page et al 2018 and changes in hydrodynamics can be an important driver in phytoplankton dynamics wilhelm and adrian 2008 kasprzak et al 2017 despite the reproduction of the overall trends the low degree of bias and the accurate timing of simulations model error increased during extreme events compared to the reference periods by roughly 30 during storm events in lough feeagh and during heatwaves by 30 lough feeagh lake erken to 100 müggelsee this lower performance shows that predictions made by hydrodynamic models during extreme weather events should be treated with additional caution notable exceptions were the storm events in müggelsee where the model error was 40 lower than during the reference periods this likely has to do with the shallow depth of müggelsee and might be systematic for shallow lakes in general the selected storm events were some of the most extreme in a 14 year period and as a result this shallow lake mixed completely this was correctly simulated by the models and errors estimating these isothermal conditions tended to be lower than the errors than during the reference periods when stratification sometimes occurred the larger errors during the storms in the deep lakes and during heatwaves can have multiple causes firstly many of the models parameterizations are nonlinear and thus the magnitude of energy and turbulence fluxes might increase faster than linearly under more extreme conditions by using high frequency driving data averaging errors relating to removing high frequency variation in meteorological forcing data were reduced however it is still possible that the values assigned to model coefficients during long term calibration may not be appropriate for the extreme conditions of specific events and this would then automatically cause a larger error secondly the assumption of one dimensionality in the models holds less well during extreme events during storms the leeside of a lake and bays experience notably less wind forcing internal waves can form and wave breaking creates turbulence on underwater slopes wüest et al 2000 macintyre and jellison 2001 shallow areas tend to stratify earlier and warm faster than deep areas woolway and merchant 2018 potentially creating more horizontal heterogeneity during heatwaves these three dimensional processes are not included in 1d models and these sources of error may be accentuated during extreme events lastly extreme events could also increase the importance of processes that were not included or kept constant in this study such as precipitation inflow or turbidity we found that extreme weather generally resulted in momentarily less accurate simulation of lake conditions even with high frequency forcing data collected on site and with all three models but to what extent is this a problem numerical process based lake models are still amongst the best tools we have to simulate thermal dynamics in lakes during extreme weather events and the fact that uncertainty increases during these conditions does not invalidate their usefulness in flood and hurricane forecasting it is acknowledged that numerical models have large uncertainty during extreme weather conditions todini 2004 heming et al 2019 the uncertainty connected to these forecasts is an important aspect of the output that is included when informing decision makers and the public in the case of extreme events in lakes uncertainty can be taken into account partially by simply being aware of it for example since the timing of event impacts was simulated accurately for some purposes of modelling it might be sufficient to take the timing of the event as information and knowing that the magnitude of the impact could differ from the simulations however to quantify the uncertainty during extreme events a potential pathway would be ensemble modelling with forcing scenarios of varying intensity because we found little consistent bias model runs with higher and lower wind speeds or temperatures could provide an uncertainty band during extreme weather events more research would be needed to determine what methods would be best suited to quantify uncertainty during extreme events the models in this study captured the overall trends and the range of error during the extreme events mae 0 4 1 2 c is similar to the level of uncertainty found in other lake modelling studies during regular conditions e g soulignac et al 2018 moras et al 2019 larger model uncertainty during extreme events is to a certain extent expected because of greater spatial variations in lake thermal structure larger energy fluxes and more rapid changes in thermal gradients in the water column at small temporal scale compared to non extreme circumstances it depends on the objective of the modeller if this reduced accuracy poses a problem larger error during extreme events might not pose a problem for long term climate forecasting as model fit during these short periods is generally not of interest for this type of studies an exception to this statement would be if there are long term consequences of extreme events as in the case of tipping points scheffer et al 2001 for short term forecasting however extreme events are amongst the most important events to capture this study shows that 1d lake models can be used to simulate these events but the short term predictions may be less precise than would occur under more normal conditions this should be kept in mind when interpreting the forecasts the results in the present study suggest that forecasts for temperature data and schmidt stability will be more precise than for maximum buoyancy frequency and mixed layer depth for scenario studies as in mi et al 2018 the increased uncertainty during events is likely not a major issue the absolute magnitude of the effect of an event might differ from observations but the overall response is simulated coupling of physical models and biogeochemical models involves a risk of error propagation a wrong estimation of water temperature could lead to wrong growth rates or a too shallow mixing event results in less nutrient upwelling than in reality because of this it is likely that uncertainty during extreme events also increases for biogeochemical models 5 conclusion extreme weather events are projected to increase in magnitude and frequency and can have large and diverse effects on lake ecosystems one dimensional hydrodynamic lake models could help in elucidating their impacts on lakes but so far no studies have investigated how well these models perform during such events in this study simstrat gotm and glm were run during multiple selected storms and heatwaves in three lakes in order to assess model performance the overall effects of extreme weather on lake temperature and stratification metrics were captured by the models with correct timing and little bias but the precision of the model output was reduced compared to non extreme conditions as with the model fit during calibration and validation simstrat and gotm performed better during extreme events than glm the implications of these findings ultimately depend on a modeller s objectives but we are convinced that the findings in this paper can help to elucidate the uncertainty of model predictions during extreme weather events this would lead to a more responsible use of 1d lake models as uncertainty is an important part of model simulations we propose that 1d lake models can be adequate tools to evaluate changes in hydrodynamics during extreme weather events provided that the increased uncertainty during these events is kept in mind when interpreting the results declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors express their thanks to the marine institute the swedish infrastructure for ecosystem science sites and uppsala university and the leibniz institute of freshwater ecology and inland fisheries igb for collecting and sharing the water temperature data of lough feeagh lake erken and müggelsee j p m a i a and j a a s were funded by the mantel itn management of climatic extreme events in lakes and reservoirs for the protection of ecosystem services through the european union s horizon 2020 research and innovation programme under the marie skłodowska curie grant agreement no 722518 r a acknowledges support by the limnoscenes project ad 91 22 1 within the biodiversa and the belmont forum programme appendix a supplementary data the following are the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104852 software availability instructions to download and install the models used in this study can be found at simstrat version 2 1 https github com eawag appliedsystemanalysis simstrat tree v2 1 gotm version 5 1 https gotm net glm version 3 0 4 https aed see uwa edu au research models glm index html last access 2020 08 20 
25977,freshwater ecosystems particularly those in agricultural areas remain at risk of eutrophication due to anthropogenic inputs of nutrients while community based monitoring has helped improve awareness and spur action to mitigate nutrient loads monitoring is challenging due to the reliance on expensive laboratory technology poor data management time lags between measurement and availability of results and risk of sample degradation during transport or storage in this study an easy to use smartphone based application the nutrient app was developed to estimate no 3 and po 4 concentrations through the image processing of on site qualitative colorimetric based results obtained via cheap commercially available instantaneous test kits the app was tested in rivers wetlands and lakes across canada and relative errors between 30 filtered samples and 70 unfiltered samples were obtained for both no 3 and po 4 the app can be used to identify sources and hotspots of contamination which can empower communities to take immediate remedial action to reduce nutrient pollution keywords community based monitoring smartphone mobile application low cost nitrate phosphate 1 introduction the biomass of plant and algae communities in freshwater ecosystems as well as the frequency of the development of disruptive algal blooms are dependent on the load of nutrients available for uptake particularly nitrogen n and phosphorus p smith et al 2006 an increase in nutrient concentrations can lead to an increase in primary production which can have detrimental effects on the chemical physical and ecological conditions of freshwater ecosystems the costs of algal blooms resulting from excess nutrient loading to aquatic ecosystems and climatic change are estimated to range in the hundreds of millions of dollars per year across north america australia and the european union smith et al 2019 atech 2000 in canada these problems have spurred increased government expenditures for remediation of the adverse effects of excess nutrient loading that include loss of habitat and biodiversity and water quality chambers et al 2001 current estimates suggest algal blooms on lake erie may cost canada 5 3 billion over the next 30 years an impact that could be reduced to 2 8 billion if algae blooms are reduced smith et al 2019 the monitoring of n and p levels in freshwater systems can identify problem areas and nutrient sources these areas can then be targeted for remedial action such as the implementation of agricultural beneficial management practices bmps or the treatment of a point source smith et al 2006 increasing public awareness of environmental issues and the need to promote acceptance of bmps has led to developments in community based monitoring cbm conrad 2007 weyhenmeyer et al 2017 such initiatives aim at actively engaging the general public in collective learning and remediation while reducing some of the costs and logistics surrounding environmental monitoring research has shown that cbm can be as reliable as government or academic led monitoring in a variety of activities such as the collection of water samples or the measurement of ph and dissolved oxygen in water bodies canfield et al 2002 castleden 2015 for instance the establishment of organizations such as the indigenous observation network ion by a partnership between the us geological survey and the yukon river inter tribal watershed council has shown through analysis of data accuracy that volunteer collected scientific data can closely mirror professionally collected data and therefore be highly valuable herman mercer et al 2018 the clean annapolis river project carp and the sackville rivers association sra are two examples of volunteer based monitoring and restoration initiatives by ngos that have been operating for over 20 years in nova scotia to increase community involvement and public education of water quality conrad 2007 support from government institutions such as environment and climate change canada as well as ngos across canada with the development of protocols and guidelines and the establishment of monitoring networks such as the community based environmental monitoring network cbemn shows that there is an increasing demand for cbm carlson et al 2017 however a lack of stable funding can result in irregular sampling and discontinuity in monitoring programs and while useful high quality data can be garnered there can also be problems of uncertainty in the quality of the collected data and incompatibility of data storage and archiving across different platforms stanley et al 2019 this ichrompedes the functionality of cbm and its ability to provide support to remedial solutions carlson et al 2017 cbm through the use of mobile phone applications may act as a solution to these data management issues and the lack of continuity in community engagement and it has shown to be especially promising due to its potentially large user base and built in location tracking technology aitkenhead et al 2014 simultaneously since such mobile applications can be used on site they could potentially solve the well known problem with sample degradation from the sensitivity of p fractions to ambient conditions after sampling jarvie et al 2002 during sampling transport and storage p concentrations can be affected by processes such as sorption precipitation complexation hydrolysis and microbiological uptake therefore concentrations at the time of laboratory analysis may not be representative of the water body at the time of collection jarvie et al 2002 maher and woo 1998 some smartphone applications have been developed to estimate water quality parameters e g kılıç et al 2018 leeuw and boss 2018 rozemeijer et al 2016 leeuw and boss 2018 however many of these applications have focused on parameters associated with particulate suspended matter such as turbidity or total suspended solids leeuw and boss 2018 and chlorophyll goddijn murphy et al 2009 where concentrations produce a visible change in the colour of water that can be captured with a photo conversely smartphone applications focused on the measurement of dissolved nutrients in water such as nitrate and phosphate are much less common and have limitations in kılıç et al 2018 for instance the authors developed an application that can estimate both no 3 and po 4 concentrations but tests were only performed in controlled lab light conditions using laboratory standards which do not reflect fields conditions the validation tests performed were also limited to a few lab standards that did not include low concentration values which are extremely challenging to measure but often observed in the environment including in many parts of canada 0 1 mg l other important challenging aspects related to the use of the app in the field were also neglected such as human errors and the weather e g light that can strongly affect the quality of the photos and concentration estimates the deltares nitrate app rozemeijer et al 2016 is another example of a mobile based technology aimed towards community based test strips in this case the authors tested the app in the field although the available information on its performance is limited the app gives the user the option to upload the results to an online database where measurements shared by others are displayed on a global map deltares 2019 this application was created as a tool for farmers and water quality researchers for detecting hotspots and sources of no 3 contamination in water bodies unfortunately the app is not suitable for po 4 measurements and no detailed information is available regarding the calculation methods algorithms used and sensitivity to ambient conditions such as sunlight intensity air temperature and temperature of the sample and turbility in this study the suitability of this no 3 measuring app was evaluated and as a result of evidence of biased results in our study region a new mobile app named nutrient app was developed tested and extended to po 4 the new app uses colorimetric analysis based test kits that are commercially available to provide nutrient concentration estimates based on an automated color identification algorithm the algorithm used which outperformed others also tested in this study was evaluated by comparing laboratory based and app based no 3 and po 4 concentration estimates under both controlled laboratory and field conditions this manuscript focuses mainly on the testing and validation of the color identification algorithms used in the app which are the key app components performing the concentration estimates 2 materials and methods 2 1 development of the new mobile application 2 1 1 overall development and testing approach the development of the nutrient app is composed of two main parts 1 selection and testing of suitable commercially available instantaneous colorimetric analysis based test kits for no 3 and po 4 such as the hach s test strips or the api phosphate test kit that relies on the mixing of the sampled water with reagents in the vial and 2 development and testing of color identification algorithms to provide concentration estimates from photos of the results provided by those test kits different commercial available test kits were tested but criteria for selection of suitable candidates included 1 preference to test strips as opposed to more complicated reagent mixing vial based tests to ease the use in the field and 2 suitability for low concentrations by means of providing reference cards with sufficient concentration color reference colors at low concentrations 2 mg no 3 n l and 2 mg po 4 l for no 3 the test kits tested were hach s test strips and the red sea nitrate pro test kit for po 4 the test kits tested were the api phosphate test kit the salifert phosphate profi test kit the nyos reefer test kit and the seachem laboratories multi test for po 4 however only certain test kits were subject to the extensive performance analyses detailed below namely the hach s test strips for no 3 fig 1 a and the api test kits for po 4 panels b1 to b4 in fig 1 these test kits were selected based on preliminary tests that considered both the quality and precision of the results as well as suitability for phone photography and the two considerations noted above in the case of the api test kit for po 4 which relies on the change of the color of the water sample inside a vial after the mixing of two reagents additional testing was needed to identify the imaging protocol that could maximize accuracy and precision in essence this component of the work consisted of testing options that could maximize homogeneity in the color of the region of the image containing the water sample inside the vial in ways that avoided light reflection and shadows a total of four different imaging options were tested fig 1 differing in the positioning and angle of the camera with respect to the reference card and vial as well the region in the image of the vial that is used to select the reference color pixel for the measurement that is used by the color identification algorithms to estimate the concentration 2 1 2 algorithms for color identification and interpolation the algorithms used are based on the delta e de method which measures the change in the visual perception of two colors e g brainard 2003 sharma et al 2005 in the case of the nutrient app this is used to measure the color distance between the reference color boxes in the reference card which have reference concentrations associated and the measurement color which is obtained from the image of the test strip in the case of no 3 or the image of the water sample in the vial in the case of po 4 by quantifying all of these color distances a concentration estimate is provided by linear interpolation of colors the method has been used across different research fields industries and applications e g textile and graphic arts to quantify differences between colors as perceived by the human eye gómez polo et al 2017 there are several variations of the method that have been proposed three of which were evaluated in this study for suitability to the nutrient app namely the delta e 1994 delta e 2000 and delta e lab versions of the method all methods account for differences in lightness chroma and hue values but the delta e 2000 and delta e lab methods have a more accurate calculation of lightness via a better weighting of saturation these algorithms were first programmed in matlab for the initial testing during the development phase but were subsequently converted into c for integration into the smartphone application both versions can be accessed and downloaded from https github com diogocostapt nutrient app git 2 2 testing and validation of the new app 2 2 1 general procedure the development and testing of the nutrient app were performed in three steps first the performance of an existing similar app the deltares nitrate app was evaluated this app also uses colorimetric analysis based on the same hach s nitrate instantaneous test strips selected for use with the new nutrient app this step was important to identify the strengths and weaknesses of existing technology that could inform the algorithms used for color identification see the paragraph below for more details about the testing of the existing app second the new deltae based algorithms were developed and tested using laboratory standards and for different light conditions this step was critical to evaluate the performance of the algorithms before incorporation into an app which was developed specifically for this purpose third the nutrient app was developed for two operating systems on smartphones and deployed in the field to test its performance with natural unfiltered freshwater samples from rivers wetlands and lakes in agricultural and urban drainages in saskatchewan canada water samples were analyzed in the laboratory for both no 3 epa method 353 2 and po 4 epa 365 1 using a discrete analyzer system smartchem 170 autoanalyzer westco scientific instruments inc brookfield ct the selection of the sampling locations aimed at testing the app for different types of water bodies concentration ranges and organic carbon content the latter which was suspected to be particularly important because it can change the background color of the samples different algorithms based on variations of the delta e were developed and tested namely the delta e 1994 delta e 2000 and delta e laboratory versions 2 2 2 comparison with an existing colorimetric analysis based mobile app the existing nitrate app tested was the deltares nitrate app laboratory standards with no 3 n concentrations of 0 5 2 5 10 and 15 mg l were used for this test which follows a similar validation procedure used for the nutrient app developed in this study the mobile app was tested on these solutions following the recommendations of deltares use of hach s no 3 test strips and concentration color reference cards the tests focused on three main factors and their effects on the no 3 concentration estimates temperature 20 c and 2 c light conditions during image capturing indoors bright outdoors sunny outdoors cloudy outdoors shade and camera software and hardware iphone xs max ios and lg g7 android with 12 mp and 16 mp rear cameras respectively three replicates of each of the laboratory standards were tested at 20 c to simulate summer conditions while the other three were placed in a cold room to be tested at 2 c to simulate spring snowmelt temperatures subsequently the methodology was evaluated for po 4 using test strips from the same supplier hach the reader is referred to the official webpage of the deltares nutrient app for more information about the nitrate app https www deltares nl en software nitrate app 3 results 3 1 app performance in controlled test conditions 3 1 1 evaluation of an existing nitrate estimating mobile app results of tests with the deltares nitrate app show that the app tends to overpredict no 3 n concentrations fig 2 the temperature of the water had little impact on the performance of the app although the biases in the measurements appear to be slightly higher at 20 c than at 2 c in order to investigate the impact of different hardware e g camera software e g built in camera app and operating systems further fig 3 compares the results obtained with an iphone 5 ios and an lg g7 android for different light conditions indoors outdoors sunny outdoors cloudy and outdoor shade the results suggest that the app is more accurate at low concentrations particularly in the 0 2 mg l no 3 n range for concentrations higher than 5 mg l no 3 n the precision and accuracy of the results drop although significant differences can be observed in the readings obtained for different light conditions the two phones provided similar results at this stage of the tests since the deltares nitrate app also uses the delta e method for color interpolation although there is no mention about the version of the method used at this stage these results suggest that no major difference in performance is expected to arise from the use of different mobile hardware and software technologies unfortunately the deltares nitrate app is proprietary and uses closed source software and unspecified methods and therefore the color identification algorithms implemented were not accessible and could not be improved or bias corrected which led to the need to develop a new app and test alternative algorithms 3 1 2 determination of global bias correction factors for the new nutrient app the tests of the deltares nitrate app that is also based on the delta e method indicated that the concentration estimations calculated using this method for color identification tend to be linearly biased as a result there was a need to determine global bias corrections factors for the new app for each nutrient species fig 4 compares laboratory and app concentration estimates for no 3 n panel a and po 4 panel b calculated using the new algorithms based on different delta e methods see section 2 1 the different versions of the delta e method were tested for no 3 n panel a and the results indicate that similarly to the deltares nitrate app the deltae based algorithms tend to overpredict concentrations this is likely due to a combination of errors associated with both the coloring technology of the test strips and the color identification methods used the results show significant biases in the estimated concentrations although the trend lines for the lab and 1994 versions of the method lie closest to the correct standard concentration trend line higher r2 the correction factors calculated from these analyses table 1 were hardcoded in the app all methods account for differences in lightness chroma and hue values but the delta e 2000 and delta e lab methods have a more accurate calculation of lightness via a better weighting of saturation however the delta e 1994 method showed surprisingly better prediction capacity likely because it ignores differences in the lightness values across the image this is because the photos of the test kit results may be unevenly non uniformly exposed to light intensifying the color distances between the different color concentration color reference boxes in the reference card and the measurement color as a result in the case of po 4 only the delta e 1994 method was tested but the analysis was here extended to assess the effect of different light conditions when photographing the test results the results obtained fig 4b show that the api test kit enables more accurate concentration estimates than the hach test strips the results obtained with the test strips could not be bias corrected because of low precision and were not used further in this study however the results obtained with the api test kit showed promising results black markers fig 5 b right and the calculated bias correction factors substantially improved the accuracy and precision of the results red markers fig 5b right the results also indicate that the app performs better for po 4 concentrations between 0 mg l and 2 mg l the correction factors calculated from these analyses for both no 3 and po 4 are presented in table 1 these corrections factors were hardcoded in the app 3 1 2 1 evaluation of performance for no 3 controlled test conditions fig 5 compares the laboratory and app no 3 n concentration estimates obtained for different light conditions and versions of the delta e method for each of these methods both the bias corrected and uncorrected concentration estimates are shown the bias correction was performed based on the global correction parameters estimated in fig 4 and provided in table 1 see section 3 2 1 the limits of the x axis and y axis of the panels in fig 5 are based on the concentration range of the concentration color reference card which is between 0 and 50 mg l but a log log scale is used to enable the evaluation of the performance of the app within the range of concentrations that are more commonly found in natural waters in canada i e between 0 and 2 mg l results show that the different delta e methods tested perform better when biases were corrected red dots particularly in the presence of good light conditions indoor or outdoor sunny and within the 0 2 mg l concentration range the methods are all able to successfully identify the correct upper and lower color concentration boxes in the reference card see section 4 4 as well as to interpolate within these boxes for estimation of test kit result concentration the light conditions had a strong effect on the accuracy of the app with bright indoor and outdoor conditions showing the best results this is because light exposure affects the quality of the photos of the test results used by the color identification algorithms the relative errors re obtained with the various delta e methods under different light conditions is shown in fig 6 also here the results highlight the importance of good light exposure when imaging the test results superior concentration estimates were obtained with indoor and outdoor sunny conditions and similarly to our preliminary tests with the deltares nitrate app the software hardware had no observable effect on the results under the controlled conditions tested all methods showed similar res which worsened with poorer light conditions i e outdoor cloudy and outdoor shady an average 30 re can be achieved with bright conditions indoor and outdoor bright whereas in outdoor cloudy or shady conditions the accuracy drops to about 50 re 3 1 3 evaluation of performance for po 4 controlled test conditions fig 7 compares the results obtained with the api phosphate test kit for different delta e methods and imaging approaches see section 2 1 development of the new mobile application overall development and testing approach similarly to the no 3 tests the concentration estimates obtained via the delta e method were bias corrected using the global correction coefficients estimated from fig 4 and provided in table 1 also here the x axis and y axis limits of the panels in fig 7 are based on the range of concentrations contemplated in the concentration color reference card which is between 0 and 10 mg l but the log log scale used allows to examine also the performance of the app within the range of concentrations that are most common in natural waters in canada i e between 0 and 2 mg l results show that the top view imaging approach see fig 1 was the least accurate of all the approaches tested and was deemed unsuccessful and not pursued further this was largely due to excessive light reflections in the region of the image containing the sample solution see panel b2 in fig 1 which created difficulties in identifying a consistent and replicable criterium for the selection of the image pixel representative of the sample the remaining imaging approaches showed promising results with the side view with angle and central pixel selection see panel b4 in fig 1 central red dot providing better results across the test kit s detection range 0 50 mg l these methods were able to identify correctly the upper and lower concentration color boxes in the reference card see fig 1 but they were also successful in linearly interpolating colors to provide a good concentration estimate this method was also considered more suitable for field conditions because it enables users to more easily and consistently generate good quality images where homogeneous colored areas are maximized which is of critical importance to increase precision and accuracy the res obtained for the different delta e methods and imaging approaches are shown in fig 8 results show that unlike with the no 3 tests figs 7 and 8 the performance of the app was greatly improved with the lg g7 android device suggesting that high resolution cameras may be needed to improve accuracy in the results the lg g7 smartphone is equipped with a higher resolution camera 16 mp than the iphone xs max 12 mp similarly to the no 3 tests the delta e 1994 method is also best for po 4 fig 7b bar j 3 2 app performance in field conditions the accuracy of the app decreased in field conditions for both no 3 and po 4 fig 9 the app was successful in identifying the magnitude of the concentrations given the large concentration range contemplated in the concentration color reference cards for no 3 n i e 0 50 mg l see main left panel and po 4 i e 0 10 mg l see main right panel but it showed a decline in the precision and accuracy within the range of concentrations of greater relevance to natural waters in canada the results revealed two main challenges 1 the accuracy depended on the resolution of the camera featured in the smartphone i e iphone xs max ios with a 12 mp camera black square marker performed worse than lg g7 android with a 16 mp camera red star marker and 2 both phones were unable to accurately estimate concentrations below 0 05 mg l where a significant decrease in both precision and accuracy were observed the results improved when a higher resolution camera was used lg g7 android but that was still insufficient to avoid gross overprediction of low concentrations i e 0 05 mg l see outliers in fig 9b the best performing approaches used for imaging the po 4 test results i e full side view panel b1 in fig 1 and side view with the image taken at an angle panel b4 in fig 1 were also tested in field conditions using the iphone 12 mp camera fig 10 similarly to the tests conducted under controlled conditions see fig 7 the side view images taken at an angle showed better performances and a slight increase in precision it should be noted that these tests were conducted with a relatively poorly performing smartphone camera iphone xs max ios because the importance of the camera resolution was still unknown at the time of these tests 4 discussion 4 1 precision and accuracy summary the app showed better accuracy and precision when used under bright light conditions see table 2 and figs 5 9 however the results indicate that the accuracy and precision increased when the app was used with laboratory standards re 0 3 for both no 3 n and po 4 when compared to natural unfiltered water samples measured in the field under field conditions the accuracy decreased substantially but remained useful for concentrations above 0 05 mg l re between 0 71 and 1 21 for no 3 n and between 0 72 and 1 31 for po 4 the lower re values i e higher accuracy better performance were obtained when using a higher resolution camera i e lg g7 smartphone with a 16 mp resolution camera outperformed the iphone xs max with a 12 mp camera particularly for po 4 see fig 10 the testing of the app using laboratory standards and under controlled light conditions which allowed the optimization of the colorimetric analysis based algorithms see section 2 1 showed that the app is more accurate at concentrations below 2 mg l for no 3 n see section 3 1 the results indicate that this is because the reference cards of both no 3 n and po 4 provide higher resolution at low concentrations see fig 1 i e 1 mg l steps within the 0 2 mg l range i e 0 1 and 2 mg l and 5 30 mg l steps within the 5 50 mg l range reference i e 5 10 20 and 50 mg l in the case of po 4 the concentration estimates are better in the 0 1 mg l range but the app still provides reasonable results for the full concentration range contemplated in the concentration color reference card i e 0 10 mg l see fig 1 this higher accuracy of results between 0 and 1 mg l is as noted for no 3 n due to higher color concentration resolution in reference cards within this range which provides more reference concentration color points for the color identification algorithm delta e method 4 2 potential fields of application and users the app showed a level of accuracy and precision that can be useful for community based science see table 2 particularly when used under bright light conditions see figs 5 8 however the lowest re re 0 3 was obtained when the app was used with laboratory standards which suggests that the water samples may need filtering to decrease turbidity and improve the accuracy and precision of the results or that further post processing of the images to remove background color may be necessary e g due to the presence of dissolved organic matter future research could help to evaluate these issues and improve the results to levels that are closer to conventional laboratory analysis inexpensive and easily deployed mobile phone technologies could be the key to reduce costly laboratory analysis and increase the spatial and temporal coverage of monitoring campaigns they are of special interest also because they have the potential to solve the problem of the transport and storage of water samples that can affect concentrations when samples are shipped to centralized laboratories maher and woo 1998 jarvie et al 2002 e g via processes such as sorption precipitation complexation hydrolysis and microbiological uptake since many inexpensive commercially available colorometric test kits exist for do ph and chlorophyll a this technology can potentially be extended to other water quality indicators in field conditions one of the main challenges is consistently taking high quality photos of the test kit results which may be challenging because it requires the reference card and test results to have uniform exposure to sunlight in laboratory conditions many of these challenges can be overcome by using a high resolution dslr camera and by creating controlled laboratory light conditions e g placing the test kit result inside a black box future research work could focus on the design and testing of such a laboratory apparatus additionally problems related to human error particulates and other impurities in the water changing the background color of the water samples and differences between test kits due to manufacturing are sources of uncertainty to our knowledge the nutrient app together with the deltares nitrate app rozemeijer et al 2016 are the first attempts to provide free and easy to use tools for community based monitoring cbm that can instantaneously measure no 3 concentrations in the field with the nutrient app developed in this study supporting also the measurement of po 4 however comprehensive performance analyses such as presented in this study are of critical importance towards exploring the full potential of such cheap technologies cost per sample for both no 3 and po 4 is less than 1 usd this is an important step to identify the potential applications of such tools and the target audiences as well as provide some insight into future research directions e g canfield et al 2002 castleden 2014 4 3 challenges for field application the use of the app and test kits in the field is straightforward the user is requested to take a photo of the test kit results and then follow the instructions provided in the app these instructions aim to help the users take the best quality photos possible and select the regions in the photos to be used in the calculations however the accuracy of the results depends heavily on the resolution of the smartphone camera and adherence to the measurement protocol provided in the app and supporting documents the key steps in this procedure are 1 obtaining a good quality image of the test results by avoiding shadows reflections and stripes in the concentration color reference card test strip and vial fig 11 2 correct selection of representative pixels of the different color boxes in the reference card and measurement i e tip of the strip in the case of no 3 see panel b4 in fig 1 and the inner cone region in the vial containing the measuring sample in the case of po 4 see panel b4 in fig 1 however complying with all these conditions in the field can be challenging depending on the weather conditions our tests showed that the imaging approach needed to be slightly adjusted for every new test in other to comply with the measurement protocol for instance the optimal camera angle and distance to the test results that avoid shadows or stripes in the image i e uniform light exposure differs from measurement to measurement but excessive distances may cause insufficient image resolution in the case of po 4 additional challenges arose in the field particularly with identifying the right camera angle and distance to the test result that would reveal the color pattern in the vial that was required panels b1 b2 in fig 11 additionally the measurement of po 4 requires the mixing of the water sample with two reagents in a vial as well as a 3 min waiting time afterwards for the color in the water sample to fully develop it is also important to note that one of the reagents in the po 4 test requires proper handling as it is composed of 45 weight of sulfuric acid despite the challenges the nutrient app is an important step towards empowering citizens and communities to contribute to science and the monitoring of water quality as well as increasing public awareness of environmental issues and the need for preventative efforts and remedial action for nutrient reduction e g conrad 2007 carlson et al 2017 the nutrient app was purposely developed for use with cheap commercially available test kits and stores all the geo referenced measurements taken by the users in a database that can be made freely accessible to the public or have access limited as required by the user this approach aimed at enabling the public and organizations to easily and cheaply use the app without the need to acquire specialized equipment devices or sensors which have resulted in inconsistent monitoring in past due to lack of funding and incompatibility of collected data across different platforms resulting in a decrease in the functionality of community based monitoring and its application to remedial solutions carlson et al 2017 4 4 mobile application interface ios and android devices once the methodology was validated a mobile application was developed for both ios and android mobile devices and was equipped with a built in map that displays all the measurements taken by the users this work was developed in coordination with push interactions a canadian provider of mobile app development services design and analysis the users are asked to share their results which are stored in a postgressql database server hosted and managed by the university of saskatchewan the geo referenced measurements can be seen both directly via the nutrient mobile app or the nutrient app webpage https gwf usask ca projects facilities nutrient app php the measurement protocols are accessible also through the app and the official app s webpage instructions feedback from pilot users across canada that includes universities ngos and school programs was critical to support the design and development of the app fig 12 shows snapshots of different screens of the nutrient app the implementation of the deltae in matlab and c version integrated into the app can be downloaded from https github com diogocostapt nutrient app git 4 5 future research directions the study results indicate that the resolution of the camera could be the most important factor determining the precision and accuracy of the results with the lg g7 smartphone with a camera resolution of 16 mp showing the best results for both no 3 and po 4 under both controlled and field conditions future research work could focus on exploring further the potential of the color identification method delta e using dslr cameras with a higher image resolution e g camera resolution higher than 30 mp this may allow reducing some of the uncertainties associated with the quality of the image e g pixelization for example the test kits results may be photographed inside a black box using an optimized light exposure for which the delta e is calibrated if the accuracy can be improved by increasing the camera resolution and controlling the light conditions the color identification method used in this app could be considered as a cheaper alternative or complementary method for laboratory analysis for such applications where practical constrains of field applications do not apply other test kits for no 3 such as the one produced by red sea could also be explored to help further improve the precision and accuracy of the results future research efforts focusing on further enhancing the app for use by the public in field conditions should be put into identifying alternative test kits that are safer to use one of the disadvantages of the api test used in this study for po 4 is the requirement to mix the water samples with two reagents in the field which raises practicality issues the alternative red sea test for no 3 also has this requirement additionally one other important issue with these types of approaches is that they use sulfuric acid in one of the reagents 10 25 weight in the case of the redsea test and 45 weight in the case of the api po 4 test creating safety concerns and waste disposal challenges 5 conclusions a new mobile application was developed to measure no 3 and po 4 concentrations in the field based on cheap commercially available and colorimetric analysis based test kits the approach is based on the delta e method for comparison and interpolation of reference colors displayed in a reference card with the colors that develop in a test strip in the case of no 3 or water sample in a vial in the case of po 4 that are the test kit results different test kits variations of the delta e interpolation method hardware and software including the camera resolution light conditions and imaging procedures were tested the results showed that a relative error re of approximately 0 3 can be achieved for both no 3 n and po 4 under controlled laboratory conditions however the resolution of the camera played a key role in the precision and accuracy of the results particularly in field conditions cloudy and shady light conditions during the imaging of the test results decreased the precision of the app in field conditions where light conditions cannot be controlled and it can be challenging to obtain good quality images of the results the lg g7 with a 16mp camera outperformed the iphone xs max with a 12 mp camera while the res increased substantially in both cases during field conditions the lg g7 16 mp camera achieved res of about 0 71 for no 3 n and po 4 concentrations above 0 05 mg l which is roughly half of those achieved with the iphone xs max 12 mp camera unfortunately the app was unable to provide acceptable estimates of no 3 n and po 4 concentrations below 0 05 mg l when used in field conditions a problem that should be further investigated in future research efforts e g exploring the potential of artificial intelligence for this application it is expected that improved smartphone camera image processing quality and resolution may resolve some of these problems in the near future software availabilty apple store https apps apple com us app nutrient app id1479779114 ls 1 android store https play google com store apps details id com pi nutrientapp website https gwf usask ca projects facilities nutrient app php declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the lake winnipeg basin program lwbp of environment and climate change canada the global water futures program lead by the university of saskatchewan canada and the canada research chairs program the global water futures program was developed in part to provide solutions to water users to help them reduce their risk exposure and to encourage sustainable management of water ecosystems https www globalwaterfutures ca the authors would like to extend their gratitude to kimberly gilmour and cameron hoggarth for kindly running the laboratory tests presented in this study 
25977,freshwater ecosystems particularly those in agricultural areas remain at risk of eutrophication due to anthropogenic inputs of nutrients while community based monitoring has helped improve awareness and spur action to mitigate nutrient loads monitoring is challenging due to the reliance on expensive laboratory technology poor data management time lags between measurement and availability of results and risk of sample degradation during transport or storage in this study an easy to use smartphone based application the nutrient app was developed to estimate no 3 and po 4 concentrations through the image processing of on site qualitative colorimetric based results obtained via cheap commercially available instantaneous test kits the app was tested in rivers wetlands and lakes across canada and relative errors between 30 filtered samples and 70 unfiltered samples were obtained for both no 3 and po 4 the app can be used to identify sources and hotspots of contamination which can empower communities to take immediate remedial action to reduce nutrient pollution keywords community based monitoring smartphone mobile application low cost nitrate phosphate 1 introduction the biomass of plant and algae communities in freshwater ecosystems as well as the frequency of the development of disruptive algal blooms are dependent on the load of nutrients available for uptake particularly nitrogen n and phosphorus p smith et al 2006 an increase in nutrient concentrations can lead to an increase in primary production which can have detrimental effects on the chemical physical and ecological conditions of freshwater ecosystems the costs of algal blooms resulting from excess nutrient loading to aquatic ecosystems and climatic change are estimated to range in the hundreds of millions of dollars per year across north america australia and the european union smith et al 2019 atech 2000 in canada these problems have spurred increased government expenditures for remediation of the adverse effects of excess nutrient loading that include loss of habitat and biodiversity and water quality chambers et al 2001 current estimates suggest algal blooms on lake erie may cost canada 5 3 billion over the next 30 years an impact that could be reduced to 2 8 billion if algae blooms are reduced smith et al 2019 the monitoring of n and p levels in freshwater systems can identify problem areas and nutrient sources these areas can then be targeted for remedial action such as the implementation of agricultural beneficial management practices bmps or the treatment of a point source smith et al 2006 increasing public awareness of environmental issues and the need to promote acceptance of bmps has led to developments in community based monitoring cbm conrad 2007 weyhenmeyer et al 2017 such initiatives aim at actively engaging the general public in collective learning and remediation while reducing some of the costs and logistics surrounding environmental monitoring research has shown that cbm can be as reliable as government or academic led monitoring in a variety of activities such as the collection of water samples or the measurement of ph and dissolved oxygen in water bodies canfield et al 2002 castleden 2015 for instance the establishment of organizations such as the indigenous observation network ion by a partnership between the us geological survey and the yukon river inter tribal watershed council has shown through analysis of data accuracy that volunteer collected scientific data can closely mirror professionally collected data and therefore be highly valuable herman mercer et al 2018 the clean annapolis river project carp and the sackville rivers association sra are two examples of volunteer based monitoring and restoration initiatives by ngos that have been operating for over 20 years in nova scotia to increase community involvement and public education of water quality conrad 2007 support from government institutions such as environment and climate change canada as well as ngos across canada with the development of protocols and guidelines and the establishment of monitoring networks such as the community based environmental monitoring network cbemn shows that there is an increasing demand for cbm carlson et al 2017 however a lack of stable funding can result in irregular sampling and discontinuity in monitoring programs and while useful high quality data can be garnered there can also be problems of uncertainty in the quality of the collected data and incompatibility of data storage and archiving across different platforms stanley et al 2019 this ichrompedes the functionality of cbm and its ability to provide support to remedial solutions carlson et al 2017 cbm through the use of mobile phone applications may act as a solution to these data management issues and the lack of continuity in community engagement and it has shown to be especially promising due to its potentially large user base and built in location tracking technology aitkenhead et al 2014 simultaneously since such mobile applications can be used on site they could potentially solve the well known problem with sample degradation from the sensitivity of p fractions to ambient conditions after sampling jarvie et al 2002 during sampling transport and storage p concentrations can be affected by processes such as sorption precipitation complexation hydrolysis and microbiological uptake therefore concentrations at the time of laboratory analysis may not be representative of the water body at the time of collection jarvie et al 2002 maher and woo 1998 some smartphone applications have been developed to estimate water quality parameters e g kılıç et al 2018 leeuw and boss 2018 rozemeijer et al 2016 leeuw and boss 2018 however many of these applications have focused on parameters associated with particulate suspended matter such as turbidity or total suspended solids leeuw and boss 2018 and chlorophyll goddijn murphy et al 2009 where concentrations produce a visible change in the colour of water that can be captured with a photo conversely smartphone applications focused on the measurement of dissolved nutrients in water such as nitrate and phosphate are much less common and have limitations in kılıç et al 2018 for instance the authors developed an application that can estimate both no 3 and po 4 concentrations but tests were only performed in controlled lab light conditions using laboratory standards which do not reflect fields conditions the validation tests performed were also limited to a few lab standards that did not include low concentration values which are extremely challenging to measure but often observed in the environment including in many parts of canada 0 1 mg l other important challenging aspects related to the use of the app in the field were also neglected such as human errors and the weather e g light that can strongly affect the quality of the photos and concentration estimates the deltares nitrate app rozemeijer et al 2016 is another example of a mobile based technology aimed towards community based test strips in this case the authors tested the app in the field although the available information on its performance is limited the app gives the user the option to upload the results to an online database where measurements shared by others are displayed on a global map deltares 2019 this application was created as a tool for farmers and water quality researchers for detecting hotspots and sources of no 3 contamination in water bodies unfortunately the app is not suitable for po 4 measurements and no detailed information is available regarding the calculation methods algorithms used and sensitivity to ambient conditions such as sunlight intensity air temperature and temperature of the sample and turbility in this study the suitability of this no 3 measuring app was evaluated and as a result of evidence of biased results in our study region a new mobile app named nutrient app was developed tested and extended to po 4 the new app uses colorimetric analysis based test kits that are commercially available to provide nutrient concentration estimates based on an automated color identification algorithm the algorithm used which outperformed others also tested in this study was evaluated by comparing laboratory based and app based no 3 and po 4 concentration estimates under both controlled laboratory and field conditions this manuscript focuses mainly on the testing and validation of the color identification algorithms used in the app which are the key app components performing the concentration estimates 2 materials and methods 2 1 development of the new mobile application 2 1 1 overall development and testing approach the development of the nutrient app is composed of two main parts 1 selection and testing of suitable commercially available instantaneous colorimetric analysis based test kits for no 3 and po 4 such as the hach s test strips or the api phosphate test kit that relies on the mixing of the sampled water with reagents in the vial and 2 development and testing of color identification algorithms to provide concentration estimates from photos of the results provided by those test kits different commercial available test kits were tested but criteria for selection of suitable candidates included 1 preference to test strips as opposed to more complicated reagent mixing vial based tests to ease the use in the field and 2 suitability for low concentrations by means of providing reference cards with sufficient concentration color reference colors at low concentrations 2 mg no 3 n l and 2 mg po 4 l for no 3 the test kits tested were hach s test strips and the red sea nitrate pro test kit for po 4 the test kits tested were the api phosphate test kit the salifert phosphate profi test kit the nyos reefer test kit and the seachem laboratories multi test for po 4 however only certain test kits were subject to the extensive performance analyses detailed below namely the hach s test strips for no 3 fig 1 a and the api test kits for po 4 panels b1 to b4 in fig 1 these test kits were selected based on preliminary tests that considered both the quality and precision of the results as well as suitability for phone photography and the two considerations noted above in the case of the api test kit for po 4 which relies on the change of the color of the water sample inside a vial after the mixing of two reagents additional testing was needed to identify the imaging protocol that could maximize accuracy and precision in essence this component of the work consisted of testing options that could maximize homogeneity in the color of the region of the image containing the water sample inside the vial in ways that avoided light reflection and shadows a total of four different imaging options were tested fig 1 differing in the positioning and angle of the camera with respect to the reference card and vial as well the region in the image of the vial that is used to select the reference color pixel for the measurement that is used by the color identification algorithms to estimate the concentration 2 1 2 algorithms for color identification and interpolation the algorithms used are based on the delta e de method which measures the change in the visual perception of two colors e g brainard 2003 sharma et al 2005 in the case of the nutrient app this is used to measure the color distance between the reference color boxes in the reference card which have reference concentrations associated and the measurement color which is obtained from the image of the test strip in the case of no 3 or the image of the water sample in the vial in the case of po 4 by quantifying all of these color distances a concentration estimate is provided by linear interpolation of colors the method has been used across different research fields industries and applications e g textile and graphic arts to quantify differences between colors as perceived by the human eye gómez polo et al 2017 there are several variations of the method that have been proposed three of which were evaluated in this study for suitability to the nutrient app namely the delta e 1994 delta e 2000 and delta e lab versions of the method all methods account for differences in lightness chroma and hue values but the delta e 2000 and delta e lab methods have a more accurate calculation of lightness via a better weighting of saturation these algorithms were first programmed in matlab for the initial testing during the development phase but were subsequently converted into c for integration into the smartphone application both versions can be accessed and downloaded from https github com diogocostapt nutrient app git 2 2 testing and validation of the new app 2 2 1 general procedure the development and testing of the nutrient app were performed in three steps first the performance of an existing similar app the deltares nitrate app was evaluated this app also uses colorimetric analysis based on the same hach s nitrate instantaneous test strips selected for use with the new nutrient app this step was important to identify the strengths and weaknesses of existing technology that could inform the algorithms used for color identification see the paragraph below for more details about the testing of the existing app second the new deltae based algorithms were developed and tested using laboratory standards and for different light conditions this step was critical to evaluate the performance of the algorithms before incorporation into an app which was developed specifically for this purpose third the nutrient app was developed for two operating systems on smartphones and deployed in the field to test its performance with natural unfiltered freshwater samples from rivers wetlands and lakes in agricultural and urban drainages in saskatchewan canada water samples were analyzed in the laboratory for both no 3 epa method 353 2 and po 4 epa 365 1 using a discrete analyzer system smartchem 170 autoanalyzer westco scientific instruments inc brookfield ct the selection of the sampling locations aimed at testing the app for different types of water bodies concentration ranges and organic carbon content the latter which was suspected to be particularly important because it can change the background color of the samples different algorithms based on variations of the delta e were developed and tested namely the delta e 1994 delta e 2000 and delta e laboratory versions 2 2 2 comparison with an existing colorimetric analysis based mobile app the existing nitrate app tested was the deltares nitrate app laboratory standards with no 3 n concentrations of 0 5 2 5 10 and 15 mg l were used for this test which follows a similar validation procedure used for the nutrient app developed in this study the mobile app was tested on these solutions following the recommendations of deltares use of hach s no 3 test strips and concentration color reference cards the tests focused on three main factors and their effects on the no 3 concentration estimates temperature 20 c and 2 c light conditions during image capturing indoors bright outdoors sunny outdoors cloudy outdoors shade and camera software and hardware iphone xs max ios and lg g7 android with 12 mp and 16 mp rear cameras respectively three replicates of each of the laboratory standards were tested at 20 c to simulate summer conditions while the other three were placed in a cold room to be tested at 2 c to simulate spring snowmelt temperatures subsequently the methodology was evaluated for po 4 using test strips from the same supplier hach the reader is referred to the official webpage of the deltares nutrient app for more information about the nitrate app https www deltares nl en software nitrate app 3 results 3 1 app performance in controlled test conditions 3 1 1 evaluation of an existing nitrate estimating mobile app results of tests with the deltares nitrate app show that the app tends to overpredict no 3 n concentrations fig 2 the temperature of the water had little impact on the performance of the app although the biases in the measurements appear to be slightly higher at 20 c than at 2 c in order to investigate the impact of different hardware e g camera software e g built in camera app and operating systems further fig 3 compares the results obtained with an iphone 5 ios and an lg g7 android for different light conditions indoors outdoors sunny outdoors cloudy and outdoor shade the results suggest that the app is more accurate at low concentrations particularly in the 0 2 mg l no 3 n range for concentrations higher than 5 mg l no 3 n the precision and accuracy of the results drop although significant differences can be observed in the readings obtained for different light conditions the two phones provided similar results at this stage of the tests since the deltares nitrate app also uses the delta e method for color interpolation although there is no mention about the version of the method used at this stage these results suggest that no major difference in performance is expected to arise from the use of different mobile hardware and software technologies unfortunately the deltares nitrate app is proprietary and uses closed source software and unspecified methods and therefore the color identification algorithms implemented were not accessible and could not be improved or bias corrected which led to the need to develop a new app and test alternative algorithms 3 1 2 determination of global bias correction factors for the new nutrient app the tests of the deltares nitrate app that is also based on the delta e method indicated that the concentration estimations calculated using this method for color identification tend to be linearly biased as a result there was a need to determine global bias corrections factors for the new app for each nutrient species fig 4 compares laboratory and app concentration estimates for no 3 n panel a and po 4 panel b calculated using the new algorithms based on different delta e methods see section 2 1 the different versions of the delta e method were tested for no 3 n panel a and the results indicate that similarly to the deltares nitrate app the deltae based algorithms tend to overpredict concentrations this is likely due to a combination of errors associated with both the coloring technology of the test strips and the color identification methods used the results show significant biases in the estimated concentrations although the trend lines for the lab and 1994 versions of the method lie closest to the correct standard concentration trend line higher r2 the correction factors calculated from these analyses table 1 were hardcoded in the app all methods account for differences in lightness chroma and hue values but the delta e 2000 and delta e lab methods have a more accurate calculation of lightness via a better weighting of saturation however the delta e 1994 method showed surprisingly better prediction capacity likely because it ignores differences in the lightness values across the image this is because the photos of the test kit results may be unevenly non uniformly exposed to light intensifying the color distances between the different color concentration color reference boxes in the reference card and the measurement color as a result in the case of po 4 only the delta e 1994 method was tested but the analysis was here extended to assess the effect of different light conditions when photographing the test results the results obtained fig 4b show that the api test kit enables more accurate concentration estimates than the hach test strips the results obtained with the test strips could not be bias corrected because of low precision and were not used further in this study however the results obtained with the api test kit showed promising results black markers fig 5 b right and the calculated bias correction factors substantially improved the accuracy and precision of the results red markers fig 5b right the results also indicate that the app performs better for po 4 concentrations between 0 mg l and 2 mg l the correction factors calculated from these analyses for both no 3 and po 4 are presented in table 1 these corrections factors were hardcoded in the app 3 1 2 1 evaluation of performance for no 3 controlled test conditions fig 5 compares the laboratory and app no 3 n concentration estimates obtained for different light conditions and versions of the delta e method for each of these methods both the bias corrected and uncorrected concentration estimates are shown the bias correction was performed based on the global correction parameters estimated in fig 4 and provided in table 1 see section 3 2 1 the limits of the x axis and y axis of the panels in fig 5 are based on the concentration range of the concentration color reference card which is between 0 and 50 mg l but a log log scale is used to enable the evaluation of the performance of the app within the range of concentrations that are more commonly found in natural waters in canada i e between 0 and 2 mg l results show that the different delta e methods tested perform better when biases were corrected red dots particularly in the presence of good light conditions indoor or outdoor sunny and within the 0 2 mg l concentration range the methods are all able to successfully identify the correct upper and lower color concentration boxes in the reference card see section 4 4 as well as to interpolate within these boxes for estimation of test kit result concentration the light conditions had a strong effect on the accuracy of the app with bright indoor and outdoor conditions showing the best results this is because light exposure affects the quality of the photos of the test results used by the color identification algorithms the relative errors re obtained with the various delta e methods under different light conditions is shown in fig 6 also here the results highlight the importance of good light exposure when imaging the test results superior concentration estimates were obtained with indoor and outdoor sunny conditions and similarly to our preliminary tests with the deltares nitrate app the software hardware had no observable effect on the results under the controlled conditions tested all methods showed similar res which worsened with poorer light conditions i e outdoor cloudy and outdoor shady an average 30 re can be achieved with bright conditions indoor and outdoor bright whereas in outdoor cloudy or shady conditions the accuracy drops to about 50 re 3 1 3 evaluation of performance for po 4 controlled test conditions fig 7 compares the results obtained with the api phosphate test kit for different delta e methods and imaging approaches see section 2 1 development of the new mobile application overall development and testing approach similarly to the no 3 tests the concentration estimates obtained via the delta e method were bias corrected using the global correction coefficients estimated from fig 4 and provided in table 1 also here the x axis and y axis limits of the panels in fig 7 are based on the range of concentrations contemplated in the concentration color reference card which is between 0 and 10 mg l but the log log scale used allows to examine also the performance of the app within the range of concentrations that are most common in natural waters in canada i e between 0 and 2 mg l results show that the top view imaging approach see fig 1 was the least accurate of all the approaches tested and was deemed unsuccessful and not pursued further this was largely due to excessive light reflections in the region of the image containing the sample solution see panel b2 in fig 1 which created difficulties in identifying a consistent and replicable criterium for the selection of the image pixel representative of the sample the remaining imaging approaches showed promising results with the side view with angle and central pixel selection see panel b4 in fig 1 central red dot providing better results across the test kit s detection range 0 50 mg l these methods were able to identify correctly the upper and lower concentration color boxes in the reference card see fig 1 but they were also successful in linearly interpolating colors to provide a good concentration estimate this method was also considered more suitable for field conditions because it enables users to more easily and consistently generate good quality images where homogeneous colored areas are maximized which is of critical importance to increase precision and accuracy the res obtained for the different delta e methods and imaging approaches are shown in fig 8 results show that unlike with the no 3 tests figs 7 and 8 the performance of the app was greatly improved with the lg g7 android device suggesting that high resolution cameras may be needed to improve accuracy in the results the lg g7 smartphone is equipped with a higher resolution camera 16 mp than the iphone xs max 12 mp similarly to the no 3 tests the delta e 1994 method is also best for po 4 fig 7b bar j 3 2 app performance in field conditions the accuracy of the app decreased in field conditions for both no 3 and po 4 fig 9 the app was successful in identifying the magnitude of the concentrations given the large concentration range contemplated in the concentration color reference cards for no 3 n i e 0 50 mg l see main left panel and po 4 i e 0 10 mg l see main right panel but it showed a decline in the precision and accuracy within the range of concentrations of greater relevance to natural waters in canada the results revealed two main challenges 1 the accuracy depended on the resolution of the camera featured in the smartphone i e iphone xs max ios with a 12 mp camera black square marker performed worse than lg g7 android with a 16 mp camera red star marker and 2 both phones were unable to accurately estimate concentrations below 0 05 mg l where a significant decrease in both precision and accuracy were observed the results improved when a higher resolution camera was used lg g7 android but that was still insufficient to avoid gross overprediction of low concentrations i e 0 05 mg l see outliers in fig 9b the best performing approaches used for imaging the po 4 test results i e full side view panel b1 in fig 1 and side view with the image taken at an angle panel b4 in fig 1 were also tested in field conditions using the iphone 12 mp camera fig 10 similarly to the tests conducted under controlled conditions see fig 7 the side view images taken at an angle showed better performances and a slight increase in precision it should be noted that these tests were conducted with a relatively poorly performing smartphone camera iphone xs max ios because the importance of the camera resolution was still unknown at the time of these tests 4 discussion 4 1 precision and accuracy summary the app showed better accuracy and precision when used under bright light conditions see table 2 and figs 5 9 however the results indicate that the accuracy and precision increased when the app was used with laboratory standards re 0 3 for both no 3 n and po 4 when compared to natural unfiltered water samples measured in the field under field conditions the accuracy decreased substantially but remained useful for concentrations above 0 05 mg l re between 0 71 and 1 21 for no 3 n and between 0 72 and 1 31 for po 4 the lower re values i e higher accuracy better performance were obtained when using a higher resolution camera i e lg g7 smartphone with a 16 mp resolution camera outperformed the iphone xs max with a 12 mp camera particularly for po 4 see fig 10 the testing of the app using laboratory standards and under controlled light conditions which allowed the optimization of the colorimetric analysis based algorithms see section 2 1 showed that the app is more accurate at concentrations below 2 mg l for no 3 n see section 3 1 the results indicate that this is because the reference cards of both no 3 n and po 4 provide higher resolution at low concentrations see fig 1 i e 1 mg l steps within the 0 2 mg l range i e 0 1 and 2 mg l and 5 30 mg l steps within the 5 50 mg l range reference i e 5 10 20 and 50 mg l in the case of po 4 the concentration estimates are better in the 0 1 mg l range but the app still provides reasonable results for the full concentration range contemplated in the concentration color reference card i e 0 10 mg l see fig 1 this higher accuracy of results between 0 and 1 mg l is as noted for no 3 n due to higher color concentration resolution in reference cards within this range which provides more reference concentration color points for the color identification algorithm delta e method 4 2 potential fields of application and users the app showed a level of accuracy and precision that can be useful for community based science see table 2 particularly when used under bright light conditions see figs 5 8 however the lowest re re 0 3 was obtained when the app was used with laboratory standards which suggests that the water samples may need filtering to decrease turbidity and improve the accuracy and precision of the results or that further post processing of the images to remove background color may be necessary e g due to the presence of dissolved organic matter future research could help to evaluate these issues and improve the results to levels that are closer to conventional laboratory analysis inexpensive and easily deployed mobile phone technologies could be the key to reduce costly laboratory analysis and increase the spatial and temporal coverage of monitoring campaigns they are of special interest also because they have the potential to solve the problem of the transport and storage of water samples that can affect concentrations when samples are shipped to centralized laboratories maher and woo 1998 jarvie et al 2002 e g via processes such as sorption precipitation complexation hydrolysis and microbiological uptake since many inexpensive commercially available colorometric test kits exist for do ph and chlorophyll a this technology can potentially be extended to other water quality indicators in field conditions one of the main challenges is consistently taking high quality photos of the test kit results which may be challenging because it requires the reference card and test results to have uniform exposure to sunlight in laboratory conditions many of these challenges can be overcome by using a high resolution dslr camera and by creating controlled laboratory light conditions e g placing the test kit result inside a black box future research work could focus on the design and testing of such a laboratory apparatus additionally problems related to human error particulates and other impurities in the water changing the background color of the water samples and differences between test kits due to manufacturing are sources of uncertainty to our knowledge the nutrient app together with the deltares nitrate app rozemeijer et al 2016 are the first attempts to provide free and easy to use tools for community based monitoring cbm that can instantaneously measure no 3 concentrations in the field with the nutrient app developed in this study supporting also the measurement of po 4 however comprehensive performance analyses such as presented in this study are of critical importance towards exploring the full potential of such cheap technologies cost per sample for both no 3 and po 4 is less than 1 usd this is an important step to identify the potential applications of such tools and the target audiences as well as provide some insight into future research directions e g canfield et al 2002 castleden 2014 4 3 challenges for field application the use of the app and test kits in the field is straightforward the user is requested to take a photo of the test kit results and then follow the instructions provided in the app these instructions aim to help the users take the best quality photos possible and select the regions in the photos to be used in the calculations however the accuracy of the results depends heavily on the resolution of the smartphone camera and adherence to the measurement protocol provided in the app and supporting documents the key steps in this procedure are 1 obtaining a good quality image of the test results by avoiding shadows reflections and stripes in the concentration color reference card test strip and vial fig 11 2 correct selection of representative pixels of the different color boxes in the reference card and measurement i e tip of the strip in the case of no 3 see panel b4 in fig 1 and the inner cone region in the vial containing the measuring sample in the case of po 4 see panel b4 in fig 1 however complying with all these conditions in the field can be challenging depending on the weather conditions our tests showed that the imaging approach needed to be slightly adjusted for every new test in other to comply with the measurement protocol for instance the optimal camera angle and distance to the test results that avoid shadows or stripes in the image i e uniform light exposure differs from measurement to measurement but excessive distances may cause insufficient image resolution in the case of po 4 additional challenges arose in the field particularly with identifying the right camera angle and distance to the test result that would reveal the color pattern in the vial that was required panels b1 b2 in fig 11 additionally the measurement of po 4 requires the mixing of the water sample with two reagents in a vial as well as a 3 min waiting time afterwards for the color in the water sample to fully develop it is also important to note that one of the reagents in the po 4 test requires proper handling as it is composed of 45 weight of sulfuric acid despite the challenges the nutrient app is an important step towards empowering citizens and communities to contribute to science and the monitoring of water quality as well as increasing public awareness of environmental issues and the need for preventative efforts and remedial action for nutrient reduction e g conrad 2007 carlson et al 2017 the nutrient app was purposely developed for use with cheap commercially available test kits and stores all the geo referenced measurements taken by the users in a database that can be made freely accessible to the public or have access limited as required by the user this approach aimed at enabling the public and organizations to easily and cheaply use the app without the need to acquire specialized equipment devices or sensors which have resulted in inconsistent monitoring in past due to lack of funding and incompatibility of collected data across different platforms resulting in a decrease in the functionality of community based monitoring and its application to remedial solutions carlson et al 2017 4 4 mobile application interface ios and android devices once the methodology was validated a mobile application was developed for both ios and android mobile devices and was equipped with a built in map that displays all the measurements taken by the users this work was developed in coordination with push interactions a canadian provider of mobile app development services design and analysis the users are asked to share their results which are stored in a postgressql database server hosted and managed by the university of saskatchewan the geo referenced measurements can be seen both directly via the nutrient mobile app or the nutrient app webpage https gwf usask ca projects facilities nutrient app php the measurement protocols are accessible also through the app and the official app s webpage instructions feedback from pilot users across canada that includes universities ngos and school programs was critical to support the design and development of the app fig 12 shows snapshots of different screens of the nutrient app the implementation of the deltae in matlab and c version integrated into the app can be downloaded from https github com diogocostapt nutrient app git 4 5 future research directions the study results indicate that the resolution of the camera could be the most important factor determining the precision and accuracy of the results with the lg g7 smartphone with a camera resolution of 16 mp showing the best results for both no 3 and po 4 under both controlled and field conditions future research work could focus on exploring further the potential of the color identification method delta e using dslr cameras with a higher image resolution e g camera resolution higher than 30 mp this may allow reducing some of the uncertainties associated with the quality of the image e g pixelization for example the test kits results may be photographed inside a black box using an optimized light exposure for which the delta e is calibrated if the accuracy can be improved by increasing the camera resolution and controlling the light conditions the color identification method used in this app could be considered as a cheaper alternative or complementary method for laboratory analysis for such applications where practical constrains of field applications do not apply other test kits for no 3 such as the one produced by red sea could also be explored to help further improve the precision and accuracy of the results future research efforts focusing on further enhancing the app for use by the public in field conditions should be put into identifying alternative test kits that are safer to use one of the disadvantages of the api test used in this study for po 4 is the requirement to mix the water samples with two reagents in the field which raises practicality issues the alternative red sea test for no 3 also has this requirement additionally one other important issue with these types of approaches is that they use sulfuric acid in one of the reagents 10 25 weight in the case of the redsea test and 45 weight in the case of the api po 4 test creating safety concerns and waste disposal challenges 5 conclusions a new mobile application was developed to measure no 3 and po 4 concentrations in the field based on cheap commercially available and colorimetric analysis based test kits the approach is based on the delta e method for comparison and interpolation of reference colors displayed in a reference card with the colors that develop in a test strip in the case of no 3 or water sample in a vial in the case of po 4 that are the test kit results different test kits variations of the delta e interpolation method hardware and software including the camera resolution light conditions and imaging procedures were tested the results showed that a relative error re of approximately 0 3 can be achieved for both no 3 n and po 4 under controlled laboratory conditions however the resolution of the camera played a key role in the precision and accuracy of the results particularly in field conditions cloudy and shady light conditions during the imaging of the test results decreased the precision of the app in field conditions where light conditions cannot be controlled and it can be challenging to obtain good quality images of the results the lg g7 with a 16mp camera outperformed the iphone xs max with a 12 mp camera while the res increased substantially in both cases during field conditions the lg g7 16 mp camera achieved res of about 0 71 for no 3 n and po 4 concentrations above 0 05 mg l which is roughly half of those achieved with the iphone xs max 12 mp camera unfortunately the app was unable to provide acceptable estimates of no 3 n and po 4 concentrations below 0 05 mg l when used in field conditions a problem that should be further investigated in future research efforts e g exploring the potential of artificial intelligence for this application it is expected that improved smartphone camera image processing quality and resolution may resolve some of these problems in the near future software availabilty apple store https apps apple com us app nutrient app id1479779114 ls 1 android store https play google com store apps details id com pi nutrientapp website https gwf usask ca projects facilities nutrient app php declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the lake winnipeg basin program lwbp of environment and climate change canada the global water futures program lead by the university of saskatchewan canada and the canada research chairs program the global water futures program was developed in part to provide solutions to water users to help them reduce their risk exposure and to encourage sustainable management of water ecosystems https www globalwaterfutures ca the authors would like to extend their gratitude to kimberly gilmour and cameron hoggarth for kindly running the laboratory tests presented in this study 
25978,the surface water quality monitoring network wqmn is crucial for effective water environment management how to design an optimal monitoring network is an important scientific and engineering problem that presents a special challenge in the smart city era this comprehensive review provides a timely and systematic overview and analysis on quantitative design approaches bibliometric analysis shows the chronological pattern journal distribution authorship citation and country pattern administration types of water bodies and design methods are classified the flexibility characteristics of four types of direct design methods and optimization objectives are systematically summarized and conclusions are drawn from experiences with wqmn parameters station locations and sampling frequency and water quality indicators this paper concludes by identifying four main future directions that should be pursued by the research community this review sheds light on how to better design and construct wqmns keywords monitoring network design optimization sampling frequency station location surface water quality smart city 1 introduction surface water such as streams rivers wetlands lakes estuaries and coasts is the important source of water for human life and industry production and also the most accessible and polluted in many countries monitoring activities can help understanding protecting and improving aquatic habitats and water quality data analysis help to quantify environmental changes and develop best management practices for informed decisions ysi 2020 therefore the water quality monitoring network wqmn is a key element for managing and protecting water environment as it captures information about the states of water systems wqmn design and deployment involves not only scientific but also economic legal and technical aspects a wqmn usually needs to meet different administration requirements such as regulation for violation and emergency monitoring of incidents although the earliest monitoring activity started in 1960s sanders et al 1983 wqmn design remains a critical challenge in both developed and developing countries nguyen et al 2019 scientists and practitioners have made many efforts to improve the design of wqmn they have to balance management requirements against many constrains and influential factors including budget monitoring sites sampling frequency technology administrative purpose and representativeness behmel et al 2016 reported methods mainly concentrate on allocation of water quality indicators sampling locations frequencies and durations however few official guidelines are available for quantitative design methods in practice shi et al 2018 in the guidelines officially published by the who and environmental protection agencies of different countries e g usepa euepa and china epa monitoring strategies mainly focus on how to organize monitoring activities bartram and balance 1996 behmel et al 2016 epa 2015 loo et al 2012 watkinson 2000 zhang et al 2010 recently rapid development of water quality monitoring technology and instruments provide more alternatives however this leads to higher flexibility of network design and more complexity in network implementation to meet different administration requirements it has evolved from traditional field sampling with lab analysis to online monitoring with wet chemistry approaches and advanced in situ sensors moreover proxy surrogate monitoring technologies which combine mathematical models with in situ sensors are emerging recently jones et al 2011 viviano et al 2014 water quality monitoring has expanded from direct stoichiometric analysis to spectrometry based on optical reflection scattering or absorption and approaches such as laser radar remote sensing and uv vis ultraviolet visible spectrometers new carriers for sensors have also become increasingly popular and have spread rapidly in practice such as unmanned vehicles e g drones boats buoys monitoring cars the development and implementation of the smart city concept globally chapman 2019 introduce new requirements to the network design where water related problems are amongst those of concern and smart urban environmental protection is an important frontier in smart city construction alavi et al 2018 butler et al 2014 ramaswami et al 2016 under this circumstance monitoring infrastructure serves as perception neurons and plays a fundamental role in smartness as part of the smart city platform or city brain chen and han 2018 reis et al 2015 the wqmn is thereby one of the important infrastructure components of smart city and closely linked with environmental system models another important component of smart cities with timely processing and response administrative departments such as epa are facing more challenges than ever before in the optimal design of wqmns in the smart city era chen and han 2018 traditional non quantitative design approaches including expert panels and brainstorming based on general guidelines are not enough to find an optimal balance chapman et al 2016 there is an urgent need to conduct a systematically literature review to provide a timely academic and practice reference on the wave of wqmn constructions around the world nguyen et al 2019 conducted a review of wqmn design for rivers which mainly highlights the influence of the scale of the study area i e watershed size and water quality indicators for routine regulatory networks behmel et al 2016 provided a review and perspective on the management of monitoring strategy not focused the designing method itself however these reviews offer limited guidance on the perspective of management requirements regional differences design method evaluation and flexibility experiences and recommendations new trends and opportunities and linkage between monitoring and modelling this paper aims to provide a comprehensive review on the published design approaches for wqmns of natural surface water bodies it covers the following aspects 1 holistically analysing the methods and cases proposed in the literature 2 identifying challenges future trends and opportunities and 3 providing most important experiences and guidelines for decision makers public services managers and other stakeholders in practice section 2 illustrates the review methodology section 3 presents the characteristics of related research based on a bibliometric analysis focusing on publishing history areas countries affiliations technical classifications and data availability section 4 summarizes the characteristics advantages and disadvantages of reported design methods including topology information entropy geostatistics multivariate statistics and different optimization approaches section 5 analyses experiences in the design of monitoring locations monitoring frequency and water quality indicators section 6 analyses the linkage between monitoring and water quality modelling section 7 discuss specific concerns outlines important implications learned from the review and recommends future research directions 2 review methodology 2 1 wqmn categories and design parameters a logical structure of design optimization of wqmns is shown in fig 1 management requirements of wqmns refer to different administration types i e monitoring purpose and associated measurement resolution of the water bodies monitored chacon hurtado et al 2017 and its constrains include financial resources data availability monitoring technologies accessibility of locations and administrative and legal considerations all these drivers of network design can be linked to three basic design parameters of wqmn monitoring locations monitoring frequency and water quality indicators which were first identified by sanders et al 1983 and now have been well accepted this review mainly focuses on these three key parameters of network design and optimization generally the wqmns can be categorized by three administration types the first kind of wqmn is for regulation monitoring the most fundamental function of wqmns which is used to monitor the water environment status and regulatory compliance wqmns can be used for pollution event emergency management and it usually includes two different functions early warning forecast monitoring type 2 and source identification monitoring type 3 in practice many established regulation wqmns can be updated to emergency use therefore it can be difficult to distinguish these three administration types 2 2 literature search and selection criteria a comprehensive literature search through november 2019 was conducted of studies using quantitative methods to design river wqmns the review was mainly based on research published in international journals or conference proceedings cetrulo et al 2019 we searched for literature in google scholar and web of science using the different combinations of the following boolean search strings for paper theme water quality and monitoring network or sample and design or optimize we then manually check the searched targets one by one and applied the following rules to condense the final paper sample set inclusion of studies focusing on rivers lakes watersheds estuaries or coasts exclusion of studies on groundwater exclusion of studies on urban water supply systems and drainage systems exclusion of studies on hydrological monitoring such as water discharges water levels and speeds exclusion of studies that do not include quantitative results exclusion of studies in languages other than english ninety articles were selected most of which were published in the last 20 years 2 3 review strategy and analysis approaches the diagram of review strategy is shown in fig 2 the bibliometric characteristics were firstly conduced chronological pattern was analysed to show the development of the research topic and trends outlook the distribution of countries studied was analysed to illustrate how the leading countries varied at different stage and their potential to applying wqmn design methods journal distribution was analysed to illustrate the research outputs in different disciplines academic societies and specific areas authors and affiliations were analysed to identify the active scholars wide attention and collaborations citation was analysed to reflect the spread of the knowledge and whether the wqmn design is a hot topic or not the underlying reasons drivers were also discussed to provide valuable references for practitioners and policy makers the review on the design methods and associated network parameters is highlighted in this work the design methods were classified method principles objective functions and optimization criteria were systematically summarized and the advantages and flexibility in different scenarios were identified then the experiences with design network parameters were summed up and the research gaps in literatures were identified relative issues were reviewed and discussed including modern water quality sensors relationship and implications to surface water quality modelling data scarcity and associated uncertainty more complex network architecture comparison with other water system etc these are emerging issues recently and important for the application of network design method in practices future research directions were finally comprehensively provided from aspects of methodology monitoring technologies and management practices 3 bibliometric characteristics 3 1 chronological pattern fig 3 shows the temporal distribution of publications on the given topic as illustrated in this figure the study history can be divided into three stages analogous to plant growth i sprouting stage before 2005 ii seedling stage 2005 2013 and iii growing stage after 2013 very few studies were published before 2004 the first study found was sharp 1971 in water resource research who used the topological centroid concept to develop a uniform sampling plan for a river in south carolina usa in the sprouting stage the majority of study areas were in developed countries or areas and usa scholars conducted pioneering work on this topic accounting for 40 of studies in the 2000s stable publication status was reached after 2004 with three papers per year on average in the seedling stage it is interesting to find the area of taiwan 18 became predominant developing counties began to focus on wqmn design e g such as iran 14 after 2013 up to 9 5 papers on average were published per year iran dominated this field with an 18 share followed by mainland china and the usa this changing distribution implies that wqmn design is receiving increasing attention due to the development of monitoring technology and increased environmental infrastructure construction around the world 3 2 countries studied a total of 22 countries and regions were covered by the retrieved wqmn design studies as shown in fig 4 usa iran taiwan and mainland china have conducted the most extensive research with 16 14 12 and 10 studies respectively as reported in fig 3 and section 3 1 the study of wqmn design has undergone an obvious shift from developed countries or regions to developing countries or regions due to the more challenging environmental problems in the latter the usa was at the forefront in the initial two stages before 2010 due to the overwhelming water quality monitoring campaign after the clean water act amendment in 1972 and the implementation of pollution control programmes in recent years many developing countries or regions such as china iran turkey india and brazil have made effective attempts which may be partly attributed to the context of growing environmental pressure over water 3 3 journal distribution table 1 shows the distribution of publications among journals 87 papers the remaining 3 are conference papers were published in 42 different journals of which 39 92 are indexed in the thompson database i e sci journals this profile indicates that the design of surface water wqmns has a broad readership and is well received in many journals the journal environmental monitoring and assessment published 23 of all studies three journals have four publications each and 6 journals have three publications each the top 10 journals account for 57 of all publications 3 4 authors and affiliations in total 266 researchers contributed to 90 articles which denotes the reviewed specific topic has received wide attentions only 4 5 of the authors had three or more publications kerachian r with 5 publications karmakar s and nikoo m r with 4 publications 9 authors had 3 publications including aral et al and 14 3 of authors published two articles figure 5 presents the distribution of the affiliations of the co authors universities clearly led the studies and were involved in a total of 84 papers 93 independent research institutions and governments participated in 18 and 12 papers respectively mostly in collaboration with universities there is still much room for improving cooperation between research institutes and administrative departments of local governments 3 5 citation analysis here we analyse the statistics on the citations of reviewed papers as shown in fig 6 the citation distribution presented a power law noting the logarithmic coordinates of y axis in fig 6 i e a few papers contribute most of the citations gupta et al 2005 redner 1998 the top 10 papers contributed more than 50 of citations while the second half the 45 least cited papers constituted 7 1 the h index of all reviewed papers is 26 which means the reviewed scientific topic is hot on water environment modelling and management banks 2006 nine of the 10 most cited papers were published before 2010 in addition ouyang 2005 has been cited the most among all 90 articles up to 432 times and the citations mostly come from other fields instead of only wqmn related papers because the author presented a comprehensive possibility of multivariate statistics by contrast limited by the methodology the earliest paper in this research sharp 1971 has 113 citations mostly in related fields 3 6 administration types and water bodies as shown in table 2 78 studies considered re design for regulation monitoring with far fewer studies on the other two types of use see fig 1 in the reported studies the authors did not usually emphasize the administration type unless the article was focused on emergency monitoring shi et al 2018 other network functions can be embodied in the design requirements or constraints of the reviewed cases 45 articles i e 50 designed wqmns for the whole watershed while 20 focused on a single river reach table 2 and 13 focused on lake or reservoir monitoring a few studies were conducted on bays and estuaries the topological characteristics of different bodies of water require different design approaches for example stream order approaches beveridge et al 2012 sanders et al 1983 can be easily used to design a stream wqmn and the kriging method is straightforward for lake wqmn design beveridge et al 2012 the majority of papers focus on the design of monitoring locations which affects the number of stations and further impacts the final cost greatly as stated before advancements in monitoring technology have weakened the importance of design optimization for monitoring frequency and monitoring items among data types it is clear that water quality data are most important whether historical data from existing wqmns 63 cases or simulation data from hydrological models 25 cases as shown in table 3 natural conditions include all natural factors in a study area such as climate topography land use and even river structure social conditions include all anthropogenic factors such as factories population density gdp etc both natural and social conditions are mainly used with optimization methods based on multiple criteria and the specific data types in a certain study area are determined by the criteria 4 basic principles and flexibility of design methods 4 1 categories of design methods the reported design methods can be divided into two categories direct design methods without optimization and optimization methods as shown in fig 7 the former category includes five major sub classes topology multivariable statistics geostatistics information entropy and other optimization methods can be divided into two sub classes according to the data inputs single criterion which usually concerns how the network represents the nature of water quality changes and requires only water quality data for optimization and multiple criteria optimization which refers to the social values of the water body such as drinking water source irrigation etc optimization methods generally depend on the four fundamental direct design methods to handle original data besides special and uncommon used methods are labelled as others such as matter element analysis chen et al 2012 the concept of the station ratio keum and kaluarachchi 2015 etc the classification and study numbers are summarized in fig 7 except the geostatistics approach the proportions of the other 5 methods are similar the sum of all numbers is greater than 90 the number of reviewed papers due to the abovementioned compatibility of the methods the following presents a summary on the six sub types of quantitative design methods referring to the fundamental theory design process flexibility and limitations 4 2 topological methods river topology based methods are amongst the earliest wqmn design methods proposed in the literature the sanders approach is a typical example dixon et al 1999 sanders and adrian 1978 sanders et al 1983 it named after emeritus professor thomas g sanders of colorado state university who published a book in 1983 that was long a standard reference for monitoring programme design sanders et al 1983 the sanders approach is derived from sharp s sampling method in sharp 1971 which is based on the basic topological identification of river systems by shreve 1967 details on the shreve approach can easily be found in many hydrology textbooks the concept of a centroid was used by sharp to divide the river network into approximately equal halves and the centroid link is simply the link whose weight as upper tributary number is closest to half the weight of the outlet sharp 1971 that is m c m i for which m i m o 1 2 is a minimum where m c is weight of the centroid m o is the weight of the outlet m i is the weight of the ith interior link is the absolute value and is integer then the first order potential sampling station can be set in the first order centroid link and is usually at the downstream of the link by default in the second order river networks divided from the original river network second order centroid links and corresponding sampling stations can be found in the same way a similar procedure is used for the remaining networks sanders sanders et al 1983 later modified this approach by adding pollution loadings and number of outfalls which is equivalent to the number of tributaries to the calculation of weights in this perspective the weight can be the sum of the length of upstream reaches or the area of the upstream basin dixon et al 1999 moreover the pollution loadings of the whole basin can be simulated and then used as a weight to determine the centroid of the river network do et al 2011 2012 a notable benefit of topological approaches is that the monitoring network can be used to identify potential pollution sources effectively as shown in fig 8 if a river network has a single pollution source that is detectable at the outlet then a sequential search is carried out by locating the successive centroids of the network and sampling their outlets the pollution source is located by noting the presence or absence of the pollutant at the successive sampling sites a b c and d and eliminating all portions of the network where the pollutant is absent dashed lines topological based design algorithms are also easily implemented on geographic information systems gis obviously topological methods are not applicable for wqmns on lakes reservoirs or coastal water 4 3 multivariate statistics many multivariate statistics methods can be easily adopted for wqmn design and optimization principal component analysis pca principal factor analysis pfa and clustering analysis ca are most commonly used calazans et al 2018b mavukkandy et al 2014 varekar et al 2016 pca and pfa are similar multivariate statistical techniques that are widely used to identify principal important components or factors that explain most of the variance of a system these methods are supposed to reduce the number of variables to a small number of indices i e principal components or factors while attempting to preserve the relationships present in the original data ca groups a set of objects in such a way that objects in the same group called a cluster are more similar in some sense to each other than to those in other groups clusters therefore these multivariate statistics approaches are mainly used to remove redundant monitoring locations and unnecessary water quality indicators based on historical monitored datasets these approaches do not work when a new network must be designed based on watershed characteristics varekar et al 2016 compared the sanders and multivariate statistic approaches under the effect of seasonal variation and a limited water quality data scenario fa pca was shown to be applicable if adequate water quality data are available while the sanders approach is ideal if water quality data are limited but considerable watershed information is available 4 4 geostatistical methods by incorporating spatial correlations geostatistical methods provide another useful alternative for wqmn design kriging and moran s i are the two typical methods proposed beveridge et al 2012 ou et al 2012 geostatistical methods are also data driven approaches that require large spatial scale datasets however long term records of water quality observations are not necessary kriging krige 1951 matheron 1963 is a widely used geospatial interpolation method that utilizes the observed data from nearby locations to predict the value of a single variable at an unmeasured location the kriging estimator at a given point is the best linear unbiased estimator blue of mean parameters spatial correlation is expressed using a semivariogram which is a graphical representation of how the similarity between values varies as a function of the spatial or temporal distance and direction kriging also provides an estimation of variance unlike variance in linear regression models the kriging variance at an unsampled point is not a measure of the local estimation accuracy of the variable but is a useful statistic that allows for comparison of network configuration as it is solely dependent on the overall covariance structure which is a function of inter station distance and kriging weights deutsch and journel 1992 moran s i is an important cluster and outlier analysis method in spatial statistics local moran s i was proposed by anselin 1995 and identifies clusters of points that are similar or different from their neighbours therefore it can be used to estimate the importance of sites for wqmn design kriging is normally used in two ways 1 to evaluate errors associated with the removal of sampling stations beveridge et al 2012 and 2 to evaluate the variance as uncertainty with the addition of sampling stations chen et al 2016 sabzipour et al 2017 it can also be used for sampling frequency design moran s i has been used to identify clusters of redundant stations that can be removed while minimizing the loss of information e g in association with the z score beveridge et al 2012 4 5 information entropy recently information entropy based design methods have received attention for example four related articles were published in 2018 information entropy is a core concept in information theory in hydrology entropy is a measure of the degree of uncertainty of random hydrological processes singh 2015 it is also a quantitative measure of the information content of time series the dispersion degree of uncertainty in a random variable x can be measured by information entropy the larger the dispersion degree of the random variable the greater the information entropy the marginal entropy h x can be defined as potential information of the variable and can be calculated as h x i 1 n p x i ln p x i where x i i 1 2 n are the values of the discrete variable x p x i is the discrete probability of occurrence and ln p x i is the information content if the state x i x the information transport index iti has been widely used in wqmn design it is a better index of dependence and is defined after normalizing transformation iti indicates the transfer of standardized information from one variable to another and provides a direct and effective means of assessing the dependence of two random variables mogheir et al 2004 for long term monitoring data series information entropy is a good index to evaluate the information and redundancies in wqmns 4 6 optimization methods according to the difference in design drivers i e input information for design optimization methods proposed in studies can be divided into two classes one group considers only the representativeness of water quality monitoring and the other is based on multiple criteria that take natural and social conditions into account fig 7 fuzzy optimization ning and chang 2004 genetic algorithm icaga 2005 artificial bee colony pérez et al 2017 and other algorithms have been used in these studies the optimization objective for water quality data driven approach includes minimizing errors in detected and simulated data maximizing coverage covering highly contaminated areas park et al 2014 minimizing the detection probability for lower compliance areas and minimizing redundant information among monitoring stations ning and chang 2004 proposed many specific objectives with clear mathematic expressions minimizing the cost of wqmns is also a consideration in many studies this goal is sometimes quantified as multi objective functions as a constraint of functions or as a reference for the selection of the final optimization result optimization approaches can also be combined with direct design methods to set up the objective function wqmns for regulation monitoring require tools such as information entropy nikoo et al 2016 to minimize system redundancy while emergency use wqmns often consider minimizing detection time and maximizing system reliability tables 4 and 5 summarize optimization objectives and criteria proposed in the literature 4 7 flexibility of design methods the adequacy or flexibility of the four main direct design methods is summarized in table 6 these methods have specific advantages for suitable water bodies network parameters functionality and data requirements it is important to select the design method according to the objective requirements and available information in addition many applications involving combinations of different types of design methods have been reported for example ou et al 2012 combined the geostatistical method with pca as a pre treatment and fuzzy optimization to design two wq mns in a lake in canada memarzadeh et al 2013 coupled dynamic factor analysis dfa and entropy methods to evaluate the station locations of river wqmns to some extent the combination approaches facilitate the holistic design of a network but more historical data are required which raises the technical threshold in practice optimization methods based on multiple criteria may be the best approach to design a new wqmn without any historical wq data there are more studies on the optimization of location than on the optimization of frequency and water quality indicators topology methods are recommended to help pre process stations or sub basins geostatistics methods are recommended for specific wqmns with high spatial correlation statistical analysis i e information entropy and multivariate statistics of existing wq data is only useful for developing a contraction strategy however hydrological models combined with optimization methods can facilitate statistical analysis to propose new stations managers need to carefully select a quantitative design approach when setting up or revising a wqmn according to the flexibility of the technology 5 experiences with design network parameters among the wqmn design variables location is the most important frequency is more flexible and water quality indicators are mostly dependent on administrative requirements and the need for localization this section summarizes the issues on design those variables 5 1 design station locations limited financial resources require as few stations as possible but on the other hand the monitoring network has to cover a large enough area that the monitoring data are representative of the water body and merit interpretation and presentation early monitoring practices relied on manual sampling and laboratory analysis the easy accessibility of sites was the primary consideration over time advancements in monitoring techniques have allowed an increasing number of wireless sensor monitoring stations to supervise the water quality status at points of interest such as areas with point pollution sources areas close to water intakes or points located upstream and downstream of highly industrialized and populated areas a number of approaches have been proposed to select both the number and location of monitoring stations almost all reviewed studies are related to monitoring location design with 84 of 90 studies related to station location design the methods summarized in table 4 are all appropriate for location design 1 topological methods as mentioned in section 3 1 topological methods were the earliest semi quantitative design method sharp 1971 for monitoring locations and continue to be actively used the most recent paper using the sanders approach is varekar et al 2016 previous work has shown that the sharp or sanders approach is suitable to carry out the hierarchical analysis of centroids for large scale river basins with a number of tributaries as a result this approach does not work in mainstreams without tributaries lakes reservoirs or dense river networks with loops the sanders approach has been proposed for early warning and source identification to cope with sudden water pollution in practice this approach has also proved useful for regulation monitoring to design a new wqmn in a watershed the sanders approach can be a good choice because of its simplicity and low demand for historical water quality data however this approach is not recommended for the evaluation or redesign of existing wqmns 2 multivariate statistics multivariate statistical methods are the most widely used approach for wqmn design as shown in fig 7 perhaps because these methods analyse water quality data without other types of data input it is easy to understand that this method can only be used as part of a reduction strategy however the adaptability to data which makes multivariate statistical methods feasible for various data scenarios and various water bodies 3 geostatistics there are relatively few cases of the use of geostatistics methods to design station locations the basic requirement of the geostatistical method is spatial correlation of the data which is rarely observed in networks consisting of a small number of stations compared with kriging the demand of moran s i for the number of stations is greater ou et al 2012 although the calculation is less complicated however existing geostatistical analysis tools i e arcgis can effectively aid the calculation moreover geostatistical methods do not require data over a certain length of time and analyses can be performed with only one sample of data these characteristics make geostatistical methods more feasible in areas lacking historical water quality data but also create a limitation it is difficult for monitoring networks designed using this method to cope with time variations of water quality based on the features stated above this approach is recommended for lakes or reservoirs for both the design and optimization of wqmns 4 information entropy it has been three decades since the application of information entropy to wqmn design was first reported by harmancioglu and alpaslan 1992 the general idea is to minimize the redundancy of information namely to remove stations that share substantial mutual entropy with others to make the system efficient in many cases entropy indexes such as iti and value of information voi are combined with multi objective functions alameddine et al 2013 proposed a maximum entropy based hierarchical spatiotemporal bayesian model three entropy based criteria were used dissolved oxygen standard violation entropy total system entropy and chlorophyll a standard violation entropy in nikoo et al 2016 the entropy was calculated by using simulation data produced by the ce qual w2 model to cope with the shortage of historical data in addition to the design of a routine regulatory wqmn shi et al 2018 was the first to use iti to design an emergency monitoring network 5 optimization framework single criterion or multi criteria optimization methods provide general frameworks and the detailed objectives are summarized in tables 4 and 5 these approaches require a number of types of data the composition of the multi objective function in addition to the water quality data usually requires various geographic data population data and pollution source data in the basin the methods for quantifying these data as part of the objective function are usually different but some common methods can be summarized as follows 1 f 1 c s c where f 1 is an objective function c is the concentration of pollution in the station and s is the standard value the maximization of f 1 helps to monitor highly polluted areas liyanage et al 2016 ning and chang 2002 2004 2005 park et al 2006 pérez et al 2017 2 f 2 1 d where d is the distance between a station and key points such as water intakes confluence or even roads with the consideration of the accessibility of stations the maximization of the objective function f 2 helps to enhance the control of important points bastidas et al 2017 liyanage et al 2016 ning and chang 2002 2004 2005 park et al 2006 3 f 3 p where p is the total population near the monitoring stations usually taking a radius of 10 km in the evaluation of sub basins p can also indicate the population in the sub basins maximization of f 3 helps to monitor areas of high population density as much as possible liyanage et al 2016 ning and chang 2002 2004 2005 pérez et al 2017 however overall different data types and quantification methods remain a problem the weight selection of different objectives is also an important issue usually after the transformation of the data by min max or z value the weights of each objective can be considered equally by default chang et al 2014 icaga 2005 park et al 2006 furthermore expert scoring can also be used to determine the weights bastidas et al 2017 chang and lin 2014b liyanage et al 2016 ning and chang 2002 and fuzzy theory can be used to analyse the weights ning and chang 2004 the search space where the multi objective function is applied that is all possible monitoring locations is theoretically spread throughout the river however some potential monitoring stations are usually selected using the sanders approach to reduce the computational pressure of subsequent optimization alilou et al 2018 icaga 2005 letternmaier et al 1984 park et al 2006 there is no such problem with the evaluation of sub basins because the number of sub basins is usually small all objectives and criteria are optional depending on the management requirements and data constraints thus an optimization method based on multiple criteria is a very convenient and feasible approach in different information scenarios to provide a complete solution for the design evaluation or optimization of wqmns 6 method selection previous studies have provided the most experience in monitoring station design however how to select the best design method is unknown nguyen et al 2019 summarized that the river size and extent of wqmn do not seem to influence the selection of the design method therefore comparisons or combinations of different design methods are recommended for example a topological approach can be used for pre allocation of the network and then optimization approaches based on different criteria can obtain a finer solution 5 2 design sampling frequency quantitatively the design of monitoring frequency is quite different from the design of network locations in terms of methodology only 22 cases refer to the design of sampling frequency table 3 of which 5 studies specifically focus on sampling frequency liu et al 2014 naddeo et al 2007 2013 sanders and adrian 1978 vilmin et al 2018 the first study was conducted as early as 1978 by sanders and adrian 1978 they used confident interval ci approaches to define the most suitable sampling frequencies considering river flow as random variability the goal is to select the appropriate frequency so that the monitoring can estimate the mean value of the water quality data within a certain confidence interval lo et al 1996 the most recent work was conducted by vilmin et al 2018 they used a hydro biogeochemical modelling approach to design the sampling frequencies for six major water quality indicators defined by the european water framework directive in a large human impact river the optimal frequency depends on station location and water quality indicators another typical method is information entropy which was first used by harmancioglu and alpaslan 1992 in this method the optimal sampling interval is determined after introducing the entropy concept to determine the monitoring locations the application approach for information entropy is also very simple and straightforward for a station the more intensive the monitoring the richer the water quality information that can be obtained and the greater the marginal entropy of the time series corresponding to a water quality indicator if the monitoring frequency is continuously reduced the water quality information and marginal entropy will subsequently decrease and the mutual entropy with the original full frequency data will also decrease that is the impact of monitoring frequency changes on water quality information acquisition can be measured quantitatively karamouz et al 2009a ozkul et al 2000 shi et al 2018 for the time series of water quality data analysed in the information entropy method the station number and frequency are the only two labels and there is no substantial difference therefore considering the time frequency and spatial distribution we can find the best combination of locations and frequency to capture the water quality information harmancioglu and alpaslan 1992 karamouz et al 2009b furthermore through the bridge of information entropy the optimization method can also be applied to frequency design mahjouri and kerachian 2011 maymandi et al 2018 pourshahabi et al 2018 in addition to information entropy various statistical methods in a broad sense are used for the selection of sampling frequency these methods can be divided into two classes one reduces the frequency while the other increases the frequency the basic idea of the former is to reduce the frequency until the water quality data obtained is not as representative as the original frequency scenario this representativeness can be measured by the confidence interval lo et al 1996 or a self defined index e g water pollution index wpi by liu et al 2014 in addition analysis of variance anova guigues et al 2013 and trend analysis naddeo et al 2007 2013 can be used to check the representativeness some studies have also incorporated a water quality model for frequency design hunt et al 2008 used existing monitoring data to model and analyse the trends of dissolved oxygen and chlorophyll and found that an appropriate reduction in the monitoring frequency had little effect on the statistical model accuracy results for research on increasing frequency are generally qualitative cluster and discriminant analysis ccda can cope with temporal changes in water quality and provide some shallow advice the basic idea is to strengthen the monitoring frequency in periods with more obvious changes identified by statistical approaches such as spring and autumn tanos et al 2015 or the rainy season calazans et al 2018b nguyen et al 2019 provided good insights on the difference between low frequency sampling and high frequency sampling a sub daily frequency can be defined as high frequency monitoring for sensors a response ranging from 15 min to 5 min intervals is the maximum frequency they can stably provide however few studies have focused on the design of high frequency monitoring 5 3 select water quality indicators the design of water quality indicators is a semi structured problem and is not as complex as the design of the other two network parameters therefore we use the term select here consistent with the opinion of sanders et al 1983 only 8 studies refer to the selection of water quality indicators table 3 and 7 studies are after the year 2013 water quality indicators thus far mainly focus on general physicochemical parameters and organic pollutant indicators nguyen et al 2019 summarized the most frequently reported water quality indicators in rivers the top five are bod do nitrate ph and conductivity the most used approach is multivariable statistics such as pca and pfa where the primary purpose is to reduce the number of water quality indicators for instance ouyang 2005 was the first to use pca and pfa to evaluate 20 water quality indicators in the wqmn of the lower st johns river lsjr and identified several key water quality indicators that contributed most significantly calazans et al 2018a 2018b used ca to divide the stations into multiple groups and then used the pca pfa method to study the main factors and major pollutants in each group interestingly villas boas et al 2017 used a nonlinear principal component analysis nlpca based on an autoassociative neural network to evaluate the redundancy of water quality indicators in the piabanha river brazil guigues et al 2013 recognized three very different behaviours of water quality variability indicators with high temporal variability and low spatial variability e g suspended solids indicators with high spatial variability and average temporal variability e g calcium and finally indicators with both high temporal and spatial variability e g nitrate thus indicators cannot be reduced beyond these three basic categories existing studies are all focused on rivers or watersheds other water bodies such as lakes reservoirs and estuaries obviously have different characteristics of water quality changes and more studies are needed 6 wqmn design linked to water quality modelling 6 1 surface water quality modelling helps wqmn design surface water management often involves the monitoring and modelling of water quality and quantity water quality models can be used in areas or periods that monitoring is not feasible or accessible as well as used to assess and predict water quality status resulting from different management strategies fu et al 2019 reis et al 2015 the design of monitoring network is also closely related to water quality models and modelling as shown in fig 9 on one hand wqmn outputs measurement data based on designed network parameters for calibration validation and training the water quality models zheng et al 2018 on the other hand water quality models in turn improve the network design water quality model is able to extent monitoring data for data scarce scenarios shi et al 2018 and the model performance can be used as the optimization objectives of network representativeness and reliability in the investigated studies water quality models whether physical process based models nikoo et al 2016 or data driven models hunt et al 2008 are recently combined with network design methods such as chen et al 2012 do et al 2011 hunt et al 2008 nikoo et al 2016 puri et al 2017 shi et al 2018 6 2 wqmn deployment helps water quality modelling an effective water quality modelling platform needs a well deployed wqmn in turn the development of online multi parametric water sensors improve the performance both of data driven models and process based models a new trend is integrated modelling and smart sensors under the big data paradigm zheng et al 2018 provided an in depth analysis on how the crowdsourcing data acquisition as a largely distributed monitoring network impacts and improves geophysical modelling it is promising to develop a real time monitoring and early warning system on hydrology and water quality with a bespoke network of wireless water sensors combined with machine learning data assimilation technique supported with real time monitoring has been widely used to improve forecasting performance of process based models cooper et al 2018 park et al 2020 assimilating high frequency water quality data actualize the identification of the multiple sources of uncertainty involving model parameters model structure hydrology hydraulics water quality future forcing e g rainfall temperature wind speed and solar radiation and observations which challenge the validation and application of water quality models cooper et al 2018 kim et al 2014 research has shown real time monitoring of river water quality can be used to improve the control of urban wastewater systems and thus comply downstream river water quality requirements with reduced energy consumption for wastewater treatment meng et al 2017 2020 thus the design of wqmns for supporting water quality modelling should be practically considered in the future research reis et al 2015 proposed a data to information transformation by the intergradation of monitoring and smart sensors chacon hurtado et al 2017 demonstrated the roles of measurements in rainfall runoff modelling and classified the model free and model based approaches for network design more investigations are needed to discuss the appropriate combination of water quality models for various scenarios fu et al 2019 such as lakes and reservoirs different non point source pollution scenarios and different spatial temporal scales 7 emerging issues and future directions 7 1 modern water quality sensors and wqmn design relative to smart cities in situ sensor observation has become increasingly popular with the spread of internet of things applications reis et al 2015 automatic high frequency monitoring ahfm based water environment management is emerging and a few studies in our review of the literature have discussed this trend horsburgh et al 2010 jácome et al 2018 nam and aral 2007 the design of sensor based wqmns involve communication data storage power management and other special factors particular concern should be paid to this topic monitoring devices in smart cities tend to be miniaturized intelligent and multifunctional and their portability is greatly enhanced these devices do not need to be fixed in one place for a long time such as a traditional monitoring station therefore the optimal monitoring layout will become increasingly important and the dynamic layout optimization of the monitoring network can be adjusted at any time information collection technologies in smart cities are expanding and now include passive and active remote sensing using radars and satellites microwave links crowdsourcing and citizen observatories zheng et al 2018 this unconventional information can supplement the limitations of traditional networks and new monitoring network design methods are needed to build a unified heterogeneous sensor network chacon hurtado 2019 the papers reviewed here are mainly focused on the design of large scale monitoring projects including watersheds lakes and bays and the methods used perform well at this scale in the foreseeable future it will be difficult and unnecessary to carry out the detailed design of monitoring networks in smart cities as mentioned on a large scale as mentioned above therefore traditional in basin monitoring projects provide large scale background support for water quality management for smart city construction the development of new design frameworks that can provide refined management of urban water bodies while complementing each other is urgently needed 7 2 similarity and nexus with other monitoring networks comparison with water quality monitoring network design on other water systems is helpful they typically involve underground water and artificial water bodies by municipal engineering such as water supply network bragalli et al 2019 he et al 2018 drainage system casal campos et al 2018 and channels chen and han 2018 in contrast to surface water quality monitoring the design of groundwater monitoring networks usually places greater emphasis on identifying pollution sources amirabdollahian and datta 2013 loaiciga et al 1992 considering the three dimensional diffusion of pollutants in groundwater the monitoring design will be more complicated because various types of underground data are difficult to obtain models are also crucial for network design there are fewer literature reports on the quantitative design of urban water distribution networks and sewer systems he et al 2018 compared with open water bodies the impact of these clear cut systems is relatively controllable and various statistical methods may be more suitable for designing monitoring networks yazdi 2018 the design and optimization of the hydrological monitoring network are similar to that of surface water quality i e considering the acquisition of information reducing redundant information and reducing the uncertainty of other points therefore they share the same methodology chacon hurtado et al 2017 monitoring of hydrological parameters e g rainfall and stream flow in many circumstances can be integrated with monitoring of water quality indicators but the combined design considering their interactions is rarely reported in literature nevertheless hydrological models including rainfall and runoff modelling are more well developed than water quality models and the combination of monitoring networks and models can provide more reliable and accurate results in addition various modern monitoring approaches such as remote sensing microwave and crowdsourcing are also easier to incorporate into hydrological monitoring campaigns and can complement traditional monitoring networks chacon hurtado 2019 as a result traditional monitoring networks must be updated to allow the assimilation of such heterogeneous dynamic data similar trends are apparent for wqmns for smart cities as depicted in section 5 1 7 3 additional relative issues other issues warrant specific discussion 1 data scarcity in reality and associated uncertainty the majority of studies have been conducted under the condition or assumption of sufficient data for design however in reality the available data are often scarce especially for setting up a new wqmn in ungauged water bodies alilou et al 2019 noted this limitation a combined approach coupling an analytic network process fuzzy logic and river mixing length was proposed and finally identified the six most appropriate locations and four candidate locations in a watershed in northwest iran how to treat the uncertainty associated with limited data availability during design is an important question an alternative to solve the dilemma of data scarcity is coupling with a water quality model 2 aftermath evaluation of re designed wqmns few studies have investigated the performance of re designed networks how to evaluate the performance of updated networks is an important question for good practices 3 adapt to more complex network architecture some studies have proposed station locations with different levels of priority alilou et al 2019 chang and lin 2014a chang et al 2014 which is usually a natural output of the optimization process such an approach is a good way to balance financial limitations and network functions by setting up a more complex network architecture like computer storage hierarchy of registers in a cpu which includes l1 l3 caches main memory local secondary storage and remote secondary storage distributed file system web services berger 2005 a hierarchy for wqmn is proposed in fig 10 this pathway can also incorporate monitoring network construction such as phase i stations and phase ii stations 7 4 future research directions based on the outcomes of this review and emerging trends in water environmental management the following four research directions with ten specific questions or issues have been identified for the research community and professionals working on surface water wqmn construction fig 11 1 innovate design patterns and methods a meta analysis to find new patterns when suitable cases are available in the literature meta analysis will provide new insights on the influencing factors of the network such as water body size and the extent of human activity impacts and the performance of design methods huang and han 2014 zhuo et al 2015 b novel design methods novel design methods are still desired particularly for water bodies linked with complex social and economy activities e g wqmns for urban receiving water connected to drainage systems researchers from alibaba business college utilized complex network theory to design a wqmn for an urban water environment xiang et al 2016 this approach makes good use of topical characteristics and water quality records 2 embrace emerging monitoring technologies a surfing the wave of automatic high frequency monitoring the recent rise of high frequency monitoring has promoted innovation in water quality management kunz et al 2017 marcé et al 2016 rode et al 2016 marcé et al 2016 argued that ahfm maximizes the provision of ecosystem services by lakes and reservoirs and is conductive for reporting lake status to management agencies it also uncovers new patterns such as concentration relationships bouchez et al 2017 moatar et al 2017 storm event responses blaen et al 2017 and water chemistry kunz et al 2017 how to optimally design different sampling frequencies and select water quality indicators under high time resolution observations is an open question b surrogate monitoring and soft measurement surrogate monitoring is a peer to ahfm and has been used in water management practices horsburgh et al 2010 jones et al 2011 it improves the variability of the selection of water quality indicators machine learning technology can be used in water quality monitoring by linking external variables such as rainfall temperature and solar radiation with observations from in situ water quality monitoring sensors to provide better water quality estimation c sensor network the use of sensors is increasingly popular and particular concern should be paid to this topic some discussion has been provided in section 5 1 d design towards manifold measurement instruments as mentioned in section 5 1 options for monitoring measurements are increasing a modern monitoring network usually combines various monitoring instruments and several functions the quantitative design of an optimal network under a complex architecture is very challenging some have recognized this issue in smart city construction chen and han 2018 demonstrated how to construct water quality monitoring infrastructures for a smart city 3 meet new administration requirements a network for non point source management non point source nps management including nutrients in surface water is an important topic only six studies in the literature focused on wqmn design for watershed nps management greater consideration of this topic is needed b network for pollution source identification since the first paper on wqmn design the purpose of identifying pollution sources has been associated with wqmns designing an effective network to meet the environmental forensics requirements of water agencies by holistically considering the monitoring location frequency and indicators is a significant issue that remains challenging for urban water management in developing countries due to the complexity of point source release and transport processes 4 toward better practices a practical guidelines for wqmn design a systemic assessment of design methods and the establishment of practical guidelines or frameworks for wqmn design under typical scenarios are anticipated b developing gis based network design tools a computer aided design tool based on the gis platform is in high demand by managers but has not yet been developed 8 conclusion the design of an appropriate water monitoring network is a fundamental aspect of water management as it is the first step in providing a representative and reliable estimation of the quality of surface waters for all stakeholders keeping in mind that the best monitoring network is a fit for purpose and cost effective one great care should be dedicated to the process of re designing such a network guigues et al 2013 the following major lessons can be learned from this critical review the quantitative design of wqmns is currently in a stage of rapid development and several successful methods have been proposed in the literature topology multivariate statistics geostatistics information entropy and single and multiple criteria optimization are typical categories among the network parameters the station location is of much investigated while studies on sampling frequency and water quality indicators are relative less the pros and cons of these methods for difference network parameters have been summarized in this work the chronological changes journal distribution authors and affiliations citations study areas of reviewed literatures are all present interesting and meaningful patterns it seems to be a tendency in developing countries to develop more wqmn construction practices from the aspect of country distribution pattern of publications the purpose of wqmn design varies from country to country and from decade to decade due to the diversity and succession of environmental problems hence studies on wqmn design will keep active for a long run accounting for the development of monitoring technology and continuous investment in the smart city era surface water wqmns present new characteristics such as dynamic heterogeneous coupling with other urban monitoring infrastructures such as urban flood control transportation and security moreover the spatial scale of a city is sometimes inconsistent with the natural boundaries of natural water bodies and the time scale or observation frequency requirements for precise urban environment management may not match those of traditional wqmns those challenges demand smart solutions for network design the summarized design methods lay the foundation for success under these more complex management conditions large gaps in knowledge or methods imply opportunities for example how can the design of sampling frequency and water quality indicators be improved in the age of high frequency water quality management how should appropriate optimization objectives and the representativeness and stability of the network under different conditions or restrictions be defined how should the results provided by different methods be evaluated how should wqmns be designed under uncertainty new design methods are still very much needed particularly for non point source management emergency monitoring mobile monitoring and pollution source identification furthermore the international hydrology community proposed 23 open unsolved problems in hydrology in 2019 blöschl et al 2019 precisely designing a monitoring network will definitely play a fundamental role in solving these problems there is still a long road ahead before mature official standardized design guidelines can be issued for industrial utilization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the key area research and development program of guangdong province grant no 2019b110205005 national natural science foundation of china grant no 51979136 and innovation project of universities in guangdong province natural science grant no 2018ktscx201 we thank prof ke sheng cheng from taiwan university and dr abdou khouakhi from cranfield university for helpful communication yang gu for the help with the literature search and statistics and anonymous reviewers for constructive suggestions appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104792 
25978,the surface water quality monitoring network wqmn is crucial for effective water environment management how to design an optimal monitoring network is an important scientific and engineering problem that presents a special challenge in the smart city era this comprehensive review provides a timely and systematic overview and analysis on quantitative design approaches bibliometric analysis shows the chronological pattern journal distribution authorship citation and country pattern administration types of water bodies and design methods are classified the flexibility characteristics of four types of direct design methods and optimization objectives are systematically summarized and conclusions are drawn from experiences with wqmn parameters station locations and sampling frequency and water quality indicators this paper concludes by identifying four main future directions that should be pursued by the research community this review sheds light on how to better design and construct wqmns keywords monitoring network design optimization sampling frequency station location surface water quality smart city 1 introduction surface water such as streams rivers wetlands lakes estuaries and coasts is the important source of water for human life and industry production and also the most accessible and polluted in many countries monitoring activities can help understanding protecting and improving aquatic habitats and water quality data analysis help to quantify environmental changes and develop best management practices for informed decisions ysi 2020 therefore the water quality monitoring network wqmn is a key element for managing and protecting water environment as it captures information about the states of water systems wqmn design and deployment involves not only scientific but also economic legal and technical aspects a wqmn usually needs to meet different administration requirements such as regulation for violation and emergency monitoring of incidents although the earliest monitoring activity started in 1960s sanders et al 1983 wqmn design remains a critical challenge in both developed and developing countries nguyen et al 2019 scientists and practitioners have made many efforts to improve the design of wqmn they have to balance management requirements against many constrains and influential factors including budget monitoring sites sampling frequency technology administrative purpose and representativeness behmel et al 2016 reported methods mainly concentrate on allocation of water quality indicators sampling locations frequencies and durations however few official guidelines are available for quantitative design methods in practice shi et al 2018 in the guidelines officially published by the who and environmental protection agencies of different countries e g usepa euepa and china epa monitoring strategies mainly focus on how to organize monitoring activities bartram and balance 1996 behmel et al 2016 epa 2015 loo et al 2012 watkinson 2000 zhang et al 2010 recently rapid development of water quality monitoring technology and instruments provide more alternatives however this leads to higher flexibility of network design and more complexity in network implementation to meet different administration requirements it has evolved from traditional field sampling with lab analysis to online monitoring with wet chemistry approaches and advanced in situ sensors moreover proxy surrogate monitoring technologies which combine mathematical models with in situ sensors are emerging recently jones et al 2011 viviano et al 2014 water quality monitoring has expanded from direct stoichiometric analysis to spectrometry based on optical reflection scattering or absorption and approaches such as laser radar remote sensing and uv vis ultraviolet visible spectrometers new carriers for sensors have also become increasingly popular and have spread rapidly in practice such as unmanned vehicles e g drones boats buoys monitoring cars the development and implementation of the smart city concept globally chapman 2019 introduce new requirements to the network design where water related problems are amongst those of concern and smart urban environmental protection is an important frontier in smart city construction alavi et al 2018 butler et al 2014 ramaswami et al 2016 under this circumstance monitoring infrastructure serves as perception neurons and plays a fundamental role in smartness as part of the smart city platform or city brain chen and han 2018 reis et al 2015 the wqmn is thereby one of the important infrastructure components of smart city and closely linked with environmental system models another important component of smart cities with timely processing and response administrative departments such as epa are facing more challenges than ever before in the optimal design of wqmns in the smart city era chen and han 2018 traditional non quantitative design approaches including expert panels and brainstorming based on general guidelines are not enough to find an optimal balance chapman et al 2016 there is an urgent need to conduct a systematically literature review to provide a timely academic and practice reference on the wave of wqmn constructions around the world nguyen et al 2019 conducted a review of wqmn design for rivers which mainly highlights the influence of the scale of the study area i e watershed size and water quality indicators for routine regulatory networks behmel et al 2016 provided a review and perspective on the management of monitoring strategy not focused the designing method itself however these reviews offer limited guidance on the perspective of management requirements regional differences design method evaluation and flexibility experiences and recommendations new trends and opportunities and linkage between monitoring and modelling this paper aims to provide a comprehensive review on the published design approaches for wqmns of natural surface water bodies it covers the following aspects 1 holistically analysing the methods and cases proposed in the literature 2 identifying challenges future trends and opportunities and 3 providing most important experiences and guidelines for decision makers public services managers and other stakeholders in practice section 2 illustrates the review methodology section 3 presents the characteristics of related research based on a bibliometric analysis focusing on publishing history areas countries affiliations technical classifications and data availability section 4 summarizes the characteristics advantages and disadvantages of reported design methods including topology information entropy geostatistics multivariate statistics and different optimization approaches section 5 analyses experiences in the design of monitoring locations monitoring frequency and water quality indicators section 6 analyses the linkage between monitoring and water quality modelling section 7 discuss specific concerns outlines important implications learned from the review and recommends future research directions 2 review methodology 2 1 wqmn categories and design parameters a logical structure of design optimization of wqmns is shown in fig 1 management requirements of wqmns refer to different administration types i e monitoring purpose and associated measurement resolution of the water bodies monitored chacon hurtado et al 2017 and its constrains include financial resources data availability monitoring technologies accessibility of locations and administrative and legal considerations all these drivers of network design can be linked to three basic design parameters of wqmn monitoring locations monitoring frequency and water quality indicators which were first identified by sanders et al 1983 and now have been well accepted this review mainly focuses on these three key parameters of network design and optimization generally the wqmns can be categorized by three administration types the first kind of wqmn is for regulation monitoring the most fundamental function of wqmns which is used to monitor the water environment status and regulatory compliance wqmns can be used for pollution event emergency management and it usually includes two different functions early warning forecast monitoring type 2 and source identification monitoring type 3 in practice many established regulation wqmns can be updated to emergency use therefore it can be difficult to distinguish these three administration types 2 2 literature search and selection criteria a comprehensive literature search through november 2019 was conducted of studies using quantitative methods to design river wqmns the review was mainly based on research published in international journals or conference proceedings cetrulo et al 2019 we searched for literature in google scholar and web of science using the different combinations of the following boolean search strings for paper theme water quality and monitoring network or sample and design or optimize we then manually check the searched targets one by one and applied the following rules to condense the final paper sample set inclusion of studies focusing on rivers lakes watersheds estuaries or coasts exclusion of studies on groundwater exclusion of studies on urban water supply systems and drainage systems exclusion of studies on hydrological monitoring such as water discharges water levels and speeds exclusion of studies that do not include quantitative results exclusion of studies in languages other than english ninety articles were selected most of which were published in the last 20 years 2 3 review strategy and analysis approaches the diagram of review strategy is shown in fig 2 the bibliometric characteristics were firstly conduced chronological pattern was analysed to show the development of the research topic and trends outlook the distribution of countries studied was analysed to illustrate how the leading countries varied at different stage and their potential to applying wqmn design methods journal distribution was analysed to illustrate the research outputs in different disciplines academic societies and specific areas authors and affiliations were analysed to identify the active scholars wide attention and collaborations citation was analysed to reflect the spread of the knowledge and whether the wqmn design is a hot topic or not the underlying reasons drivers were also discussed to provide valuable references for practitioners and policy makers the review on the design methods and associated network parameters is highlighted in this work the design methods were classified method principles objective functions and optimization criteria were systematically summarized and the advantages and flexibility in different scenarios were identified then the experiences with design network parameters were summed up and the research gaps in literatures were identified relative issues were reviewed and discussed including modern water quality sensors relationship and implications to surface water quality modelling data scarcity and associated uncertainty more complex network architecture comparison with other water system etc these are emerging issues recently and important for the application of network design method in practices future research directions were finally comprehensively provided from aspects of methodology monitoring technologies and management practices 3 bibliometric characteristics 3 1 chronological pattern fig 3 shows the temporal distribution of publications on the given topic as illustrated in this figure the study history can be divided into three stages analogous to plant growth i sprouting stage before 2005 ii seedling stage 2005 2013 and iii growing stage after 2013 very few studies were published before 2004 the first study found was sharp 1971 in water resource research who used the topological centroid concept to develop a uniform sampling plan for a river in south carolina usa in the sprouting stage the majority of study areas were in developed countries or areas and usa scholars conducted pioneering work on this topic accounting for 40 of studies in the 2000s stable publication status was reached after 2004 with three papers per year on average in the seedling stage it is interesting to find the area of taiwan 18 became predominant developing counties began to focus on wqmn design e g such as iran 14 after 2013 up to 9 5 papers on average were published per year iran dominated this field with an 18 share followed by mainland china and the usa this changing distribution implies that wqmn design is receiving increasing attention due to the development of monitoring technology and increased environmental infrastructure construction around the world 3 2 countries studied a total of 22 countries and regions were covered by the retrieved wqmn design studies as shown in fig 4 usa iran taiwan and mainland china have conducted the most extensive research with 16 14 12 and 10 studies respectively as reported in fig 3 and section 3 1 the study of wqmn design has undergone an obvious shift from developed countries or regions to developing countries or regions due to the more challenging environmental problems in the latter the usa was at the forefront in the initial two stages before 2010 due to the overwhelming water quality monitoring campaign after the clean water act amendment in 1972 and the implementation of pollution control programmes in recent years many developing countries or regions such as china iran turkey india and brazil have made effective attempts which may be partly attributed to the context of growing environmental pressure over water 3 3 journal distribution table 1 shows the distribution of publications among journals 87 papers the remaining 3 are conference papers were published in 42 different journals of which 39 92 are indexed in the thompson database i e sci journals this profile indicates that the design of surface water wqmns has a broad readership and is well received in many journals the journal environmental monitoring and assessment published 23 of all studies three journals have four publications each and 6 journals have three publications each the top 10 journals account for 57 of all publications 3 4 authors and affiliations in total 266 researchers contributed to 90 articles which denotes the reviewed specific topic has received wide attentions only 4 5 of the authors had three or more publications kerachian r with 5 publications karmakar s and nikoo m r with 4 publications 9 authors had 3 publications including aral et al and 14 3 of authors published two articles figure 5 presents the distribution of the affiliations of the co authors universities clearly led the studies and were involved in a total of 84 papers 93 independent research institutions and governments participated in 18 and 12 papers respectively mostly in collaboration with universities there is still much room for improving cooperation between research institutes and administrative departments of local governments 3 5 citation analysis here we analyse the statistics on the citations of reviewed papers as shown in fig 6 the citation distribution presented a power law noting the logarithmic coordinates of y axis in fig 6 i e a few papers contribute most of the citations gupta et al 2005 redner 1998 the top 10 papers contributed more than 50 of citations while the second half the 45 least cited papers constituted 7 1 the h index of all reviewed papers is 26 which means the reviewed scientific topic is hot on water environment modelling and management banks 2006 nine of the 10 most cited papers were published before 2010 in addition ouyang 2005 has been cited the most among all 90 articles up to 432 times and the citations mostly come from other fields instead of only wqmn related papers because the author presented a comprehensive possibility of multivariate statistics by contrast limited by the methodology the earliest paper in this research sharp 1971 has 113 citations mostly in related fields 3 6 administration types and water bodies as shown in table 2 78 studies considered re design for regulation monitoring with far fewer studies on the other two types of use see fig 1 in the reported studies the authors did not usually emphasize the administration type unless the article was focused on emergency monitoring shi et al 2018 other network functions can be embodied in the design requirements or constraints of the reviewed cases 45 articles i e 50 designed wqmns for the whole watershed while 20 focused on a single river reach table 2 and 13 focused on lake or reservoir monitoring a few studies were conducted on bays and estuaries the topological characteristics of different bodies of water require different design approaches for example stream order approaches beveridge et al 2012 sanders et al 1983 can be easily used to design a stream wqmn and the kriging method is straightforward for lake wqmn design beveridge et al 2012 the majority of papers focus on the design of monitoring locations which affects the number of stations and further impacts the final cost greatly as stated before advancements in monitoring technology have weakened the importance of design optimization for monitoring frequency and monitoring items among data types it is clear that water quality data are most important whether historical data from existing wqmns 63 cases or simulation data from hydrological models 25 cases as shown in table 3 natural conditions include all natural factors in a study area such as climate topography land use and even river structure social conditions include all anthropogenic factors such as factories population density gdp etc both natural and social conditions are mainly used with optimization methods based on multiple criteria and the specific data types in a certain study area are determined by the criteria 4 basic principles and flexibility of design methods 4 1 categories of design methods the reported design methods can be divided into two categories direct design methods without optimization and optimization methods as shown in fig 7 the former category includes five major sub classes topology multivariable statistics geostatistics information entropy and other optimization methods can be divided into two sub classes according to the data inputs single criterion which usually concerns how the network represents the nature of water quality changes and requires only water quality data for optimization and multiple criteria optimization which refers to the social values of the water body such as drinking water source irrigation etc optimization methods generally depend on the four fundamental direct design methods to handle original data besides special and uncommon used methods are labelled as others such as matter element analysis chen et al 2012 the concept of the station ratio keum and kaluarachchi 2015 etc the classification and study numbers are summarized in fig 7 except the geostatistics approach the proportions of the other 5 methods are similar the sum of all numbers is greater than 90 the number of reviewed papers due to the abovementioned compatibility of the methods the following presents a summary on the six sub types of quantitative design methods referring to the fundamental theory design process flexibility and limitations 4 2 topological methods river topology based methods are amongst the earliest wqmn design methods proposed in the literature the sanders approach is a typical example dixon et al 1999 sanders and adrian 1978 sanders et al 1983 it named after emeritus professor thomas g sanders of colorado state university who published a book in 1983 that was long a standard reference for monitoring programme design sanders et al 1983 the sanders approach is derived from sharp s sampling method in sharp 1971 which is based on the basic topological identification of river systems by shreve 1967 details on the shreve approach can easily be found in many hydrology textbooks the concept of a centroid was used by sharp to divide the river network into approximately equal halves and the centroid link is simply the link whose weight as upper tributary number is closest to half the weight of the outlet sharp 1971 that is m c m i for which m i m o 1 2 is a minimum where m c is weight of the centroid m o is the weight of the outlet m i is the weight of the ith interior link is the absolute value and is integer then the first order potential sampling station can be set in the first order centroid link and is usually at the downstream of the link by default in the second order river networks divided from the original river network second order centroid links and corresponding sampling stations can be found in the same way a similar procedure is used for the remaining networks sanders sanders et al 1983 later modified this approach by adding pollution loadings and number of outfalls which is equivalent to the number of tributaries to the calculation of weights in this perspective the weight can be the sum of the length of upstream reaches or the area of the upstream basin dixon et al 1999 moreover the pollution loadings of the whole basin can be simulated and then used as a weight to determine the centroid of the river network do et al 2011 2012 a notable benefit of topological approaches is that the monitoring network can be used to identify potential pollution sources effectively as shown in fig 8 if a river network has a single pollution source that is detectable at the outlet then a sequential search is carried out by locating the successive centroids of the network and sampling their outlets the pollution source is located by noting the presence or absence of the pollutant at the successive sampling sites a b c and d and eliminating all portions of the network where the pollutant is absent dashed lines topological based design algorithms are also easily implemented on geographic information systems gis obviously topological methods are not applicable for wqmns on lakes reservoirs or coastal water 4 3 multivariate statistics many multivariate statistics methods can be easily adopted for wqmn design and optimization principal component analysis pca principal factor analysis pfa and clustering analysis ca are most commonly used calazans et al 2018b mavukkandy et al 2014 varekar et al 2016 pca and pfa are similar multivariate statistical techniques that are widely used to identify principal important components or factors that explain most of the variance of a system these methods are supposed to reduce the number of variables to a small number of indices i e principal components or factors while attempting to preserve the relationships present in the original data ca groups a set of objects in such a way that objects in the same group called a cluster are more similar in some sense to each other than to those in other groups clusters therefore these multivariate statistics approaches are mainly used to remove redundant monitoring locations and unnecessary water quality indicators based on historical monitored datasets these approaches do not work when a new network must be designed based on watershed characteristics varekar et al 2016 compared the sanders and multivariate statistic approaches under the effect of seasonal variation and a limited water quality data scenario fa pca was shown to be applicable if adequate water quality data are available while the sanders approach is ideal if water quality data are limited but considerable watershed information is available 4 4 geostatistical methods by incorporating spatial correlations geostatistical methods provide another useful alternative for wqmn design kriging and moran s i are the two typical methods proposed beveridge et al 2012 ou et al 2012 geostatistical methods are also data driven approaches that require large spatial scale datasets however long term records of water quality observations are not necessary kriging krige 1951 matheron 1963 is a widely used geospatial interpolation method that utilizes the observed data from nearby locations to predict the value of a single variable at an unmeasured location the kriging estimator at a given point is the best linear unbiased estimator blue of mean parameters spatial correlation is expressed using a semivariogram which is a graphical representation of how the similarity between values varies as a function of the spatial or temporal distance and direction kriging also provides an estimation of variance unlike variance in linear regression models the kriging variance at an unsampled point is not a measure of the local estimation accuracy of the variable but is a useful statistic that allows for comparison of network configuration as it is solely dependent on the overall covariance structure which is a function of inter station distance and kriging weights deutsch and journel 1992 moran s i is an important cluster and outlier analysis method in spatial statistics local moran s i was proposed by anselin 1995 and identifies clusters of points that are similar or different from their neighbours therefore it can be used to estimate the importance of sites for wqmn design kriging is normally used in two ways 1 to evaluate errors associated with the removal of sampling stations beveridge et al 2012 and 2 to evaluate the variance as uncertainty with the addition of sampling stations chen et al 2016 sabzipour et al 2017 it can also be used for sampling frequency design moran s i has been used to identify clusters of redundant stations that can be removed while minimizing the loss of information e g in association with the z score beveridge et al 2012 4 5 information entropy recently information entropy based design methods have received attention for example four related articles were published in 2018 information entropy is a core concept in information theory in hydrology entropy is a measure of the degree of uncertainty of random hydrological processes singh 2015 it is also a quantitative measure of the information content of time series the dispersion degree of uncertainty in a random variable x can be measured by information entropy the larger the dispersion degree of the random variable the greater the information entropy the marginal entropy h x can be defined as potential information of the variable and can be calculated as h x i 1 n p x i ln p x i where x i i 1 2 n are the values of the discrete variable x p x i is the discrete probability of occurrence and ln p x i is the information content if the state x i x the information transport index iti has been widely used in wqmn design it is a better index of dependence and is defined after normalizing transformation iti indicates the transfer of standardized information from one variable to another and provides a direct and effective means of assessing the dependence of two random variables mogheir et al 2004 for long term monitoring data series information entropy is a good index to evaluate the information and redundancies in wqmns 4 6 optimization methods according to the difference in design drivers i e input information for design optimization methods proposed in studies can be divided into two classes one group considers only the representativeness of water quality monitoring and the other is based on multiple criteria that take natural and social conditions into account fig 7 fuzzy optimization ning and chang 2004 genetic algorithm icaga 2005 artificial bee colony pérez et al 2017 and other algorithms have been used in these studies the optimization objective for water quality data driven approach includes minimizing errors in detected and simulated data maximizing coverage covering highly contaminated areas park et al 2014 minimizing the detection probability for lower compliance areas and minimizing redundant information among monitoring stations ning and chang 2004 proposed many specific objectives with clear mathematic expressions minimizing the cost of wqmns is also a consideration in many studies this goal is sometimes quantified as multi objective functions as a constraint of functions or as a reference for the selection of the final optimization result optimization approaches can also be combined with direct design methods to set up the objective function wqmns for regulation monitoring require tools such as information entropy nikoo et al 2016 to minimize system redundancy while emergency use wqmns often consider minimizing detection time and maximizing system reliability tables 4 and 5 summarize optimization objectives and criteria proposed in the literature 4 7 flexibility of design methods the adequacy or flexibility of the four main direct design methods is summarized in table 6 these methods have specific advantages for suitable water bodies network parameters functionality and data requirements it is important to select the design method according to the objective requirements and available information in addition many applications involving combinations of different types of design methods have been reported for example ou et al 2012 combined the geostatistical method with pca as a pre treatment and fuzzy optimization to design two wq mns in a lake in canada memarzadeh et al 2013 coupled dynamic factor analysis dfa and entropy methods to evaluate the station locations of river wqmns to some extent the combination approaches facilitate the holistic design of a network but more historical data are required which raises the technical threshold in practice optimization methods based on multiple criteria may be the best approach to design a new wqmn without any historical wq data there are more studies on the optimization of location than on the optimization of frequency and water quality indicators topology methods are recommended to help pre process stations or sub basins geostatistics methods are recommended for specific wqmns with high spatial correlation statistical analysis i e information entropy and multivariate statistics of existing wq data is only useful for developing a contraction strategy however hydrological models combined with optimization methods can facilitate statistical analysis to propose new stations managers need to carefully select a quantitative design approach when setting up or revising a wqmn according to the flexibility of the technology 5 experiences with design network parameters among the wqmn design variables location is the most important frequency is more flexible and water quality indicators are mostly dependent on administrative requirements and the need for localization this section summarizes the issues on design those variables 5 1 design station locations limited financial resources require as few stations as possible but on the other hand the monitoring network has to cover a large enough area that the monitoring data are representative of the water body and merit interpretation and presentation early monitoring practices relied on manual sampling and laboratory analysis the easy accessibility of sites was the primary consideration over time advancements in monitoring techniques have allowed an increasing number of wireless sensor monitoring stations to supervise the water quality status at points of interest such as areas with point pollution sources areas close to water intakes or points located upstream and downstream of highly industrialized and populated areas a number of approaches have been proposed to select both the number and location of monitoring stations almost all reviewed studies are related to monitoring location design with 84 of 90 studies related to station location design the methods summarized in table 4 are all appropriate for location design 1 topological methods as mentioned in section 3 1 topological methods were the earliest semi quantitative design method sharp 1971 for monitoring locations and continue to be actively used the most recent paper using the sanders approach is varekar et al 2016 previous work has shown that the sharp or sanders approach is suitable to carry out the hierarchical analysis of centroids for large scale river basins with a number of tributaries as a result this approach does not work in mainstreams without tributaries lakes reservoirs or dense river networks with loops the sanders approach has been proposed for early warning and source identification to cope with sudden water pollution in practice this approach has also proved useful for regulation monitoring to design a new wqmn in a watershed the sanders approach can be a good choice because of its simplicity and low demand for historical water quality data however this approach is not recommended for the evaluation or redesign of existing wqmns 2 multivariate statistics multivariate statistical methods are the most widely used approach for wqmn design as shown in fig 7 perhaps because these methods analyse water quality data without other types of data input it is easy to understand that this method can only be used as part of a reduction strategy however the adaptability to data which makes multivariate statistical methods feasible for various data scenarios and various water bodies 3 geostatistics there are relatively few cases of the use of geostatistics methods to design station locations the basic requirement of the geostatistical method is spatial correlation of the data which is rarely observed in networks consisting of a small number of stations compared with kriging the demand of moran s i for the number of stations is greater ou et al 2012 although the calculation is less complicated however existing geostatistical analysis tools i e arcgis can effectively aid the calculation moreover geostatistical methods do not require data over a certain length of time and analyses can be performed with only one sample of data these characteristics make geostatistical methods more feasible in areas lacking historical water quality data but also create a limitation it is difficult for monitoring networks designed using this method to cope with time variations of water quality based on the features stated above this approach is recommended for lakes or reservoirs for both the design and optimization of wqmns 4 information entropy it has been three decades since the application of information entropy to wqmn design was first reported by harmancioglu and alpaslan 1992 the general idea is to minimize the redundancy of information namely to remove stations that share substantial mutual entropy with others to make the system efficient in many cases entropy indexes such as iti and value of information voi are combined with multi objective functions alameddine et al 2013 proposed a maximum entropy based hierarchical spatiotemporal bayesian model three entropy based criteria were used dissolved oxygen standard violation entropy total system entropy and chlorophyll a standard violation entropy in nikoo et al 2016 the entropy was calculated by using simulation data produced by the ce qual w2 model to cope with the shortage of historical data in addition to the design of a routine regulatory wqmn shi et al 2018 was the first to use iti to design an emergency monitoring network 5 optimization framework single criterion or multi criteria optimization methods provide general frameworks and the detailed objectives are summarized in tables 4 and 5 these approaches require a number of types of data the composition of the multi objective function in addition to the water quality data usually requires various geographic data population data and pollution source data in the basin the methods for quantifying these data as part of the objective function are usually different but some common methods can be summarized as follows 1 f 1 c s c where f 1 is an objective function c is the concentration of pollution in the station and s is the standard value the maximization of f 1 helps to monitor highly polluted areas liyanage et al 2016 ning and chang 2002 2004 2005 park et al 2006 pérez et al 2017 2 f 2 1 d where d is the distance between a station and key points such as water intakes confluence or even roads with the consideration of the accessibility of stations the maximization of the objective function f 2 helps to enhance the control of important points bastidas et al 2017 liyanage et al 2016 ning and chang 2002 2004 2005 park et al 2006 3 f 3 p where p is the total population near the monitoring stations usually taking a radius of 10 km in the evaluation of sub basins p can also indicate the population in the sub basins maximization of f 3 helps to monitor areas of high population density as much as possible liyanage et al 2016 ning and chang 2002 2004 2005 pérez et al 2017 however overall different data types and quantification methods remain a problem the weight selection of different objectives is also an important issue usually after the transformation of the data by min max or z value the weights of each objective can be considered equally by default chang et al 2014 icaga 2005 park et al 2006 furthermore expert scoring can also be used to determine the weights bastidas et al 2017 chang and lin 2014b liyanage et al 2016 ning and chang 2002 and fuzzy theory can be used to analyse the weights ning and chang 2004 the search space where the multi objective function is applied that is all possible monitoring locations is theoretically spread throughout the river however some potential monitoring stations are usually selected using the sanders approach to reduce the computational pressure of subsequent optimization alilou et al 2018 icaga 2005 letternmaier et al 1984 park et al 2006 there is no such problem with the evaluation of sub basins because the number of sub basins is usually small all objectives and criteria are optional depending on the management requirements and data constraints thus an optimization method based on multiple criteria is a very convenient and feasible approach in different information scenarios to provide a complete solution for the design evaluation or optimization of wqmns 6 method selection previous studies have provided the most experience in monitoring station design however how to select the best design method is unknown nguyen et al 2019 summarized that the river size and extent of wqmn do not seem to influence the selection of the design method therefore comparisons or combinations of different design methods are recommended for example a topological approach can be used for pre allocation of the network and then optimization approaches based on different criteria can obtain a finer solution 5 2 design sampling frequency quantitatively the design of monitoring frequency is quite different from the design of network locations in terms of methodology only 22 cases refer to the design of sampling frequency table 3 of which 5 studies specifically focus on sampling frequency liu et al 2014 naddeo et al 2007 2013 sanders and adrian 1978 vilmin et al 2018 the first study was conducted as early as 1978 by sanders and adrian 1978 they used confident interval ci approaches to define the most suitable sampling frequencies considering river flow as random variability the goal is to select the appropriate frequency so that the monitoring can estimate the mean value of the water quality data within a certain confidence interval lo et al 1996 the most recent work was conducted by vilmin et al 2018 they used a hydro biogeochemical modelling approach to design the sampling frequencies for six major water quality indicators defined by the european water framework directive in a large human impact river the optimal frequency depends on station location and water quality indicators another typical method is information entropy which was first used by harmancioglu and alpaslan 1992 in this method the optimal sampling interval is determined after introducing the entropy concept to determine the monitoring locations the application approach for information entropy is also very simple and straightforward for a station the more intensive the monitoring the richer the water quality information that can be obtained and the greater the marginal entropy of the time series corresponding to a water quality indicator if the monitoring frequency is continuously reduced the water quality information and marginal entropy will subsequently decrease and the mutual entropy with the original full frequency data will also decrease that is the impact of monitoring frequency changes on water quality information acquisition can be measured quantitatively karamouz et al 2009a ozkul et al 2000 shi et al 2018 for the time series of water quality data analysed in the information entropy method the station number and frequency are the only two labels and there is no substantial difference therefore considering the time frequency and spatial distribution we can find the best combination of locations and frequency to capture the water quality information harmancioglu and alpaslan 1992 karamouz et al 2009b furthermore through the bridge of information entropy the optimization method can also be applied to frequency design mahjouri and kerachian 2011 maymandi et al 2018 pourshahabi et al 2018 in addition to information entropy various statistical methods in a broad sense are used for the selection of sampling frequency these methods can be divided into two classes one reduces the frequency while the other increases the frequency the basic idea of the former is to reduce the frequency until the water quality data obtained is not as representative as the original frequency scenario this representativeness can be measured by the confidence interval lo et al 1996 or a self defined index e g water pollution index wpi by liu et al 2014 in addition analysis of variance anova guigues et al 2013 and trend analysis naddeo et al 2007 2013 can be used to check the representativeness some studies have also incorporated a water quality model for frequency design hunt et al 2008 used existing monitoring data to model and analyse the trends of dissolved oxygen and chlorophyll and found that an appropriate reduction in the monitoring frequency had little effect on the statistical model accuracy results for research on increasing frequency are generally qualitative cluster and discriminant analysis ccda can cope with temporal changes in water quality and provide some shallow advice the basic idea is to strengthen the monitoring frequency in periods with more obvious changes identified by statistical approaches such as spring and autumn tanos et al 2015 or the rainy season calazans et al 2018b nguyen et al 2019 provided good insights on the difference between low frequency sampling and high frequency sampling a sub daily frequency can be defined as high frequency monitoring for sensors a response ranging from 15 min to 5 min intervals is the maximum frequency they can stably provide however few studies have focused on the design of high frequency monitoring 5 3 select water quality indicators the design of water quality indicators is a semi structured problem and is not as complex as the design of the other two network parameters therefore we use the term select here consistent with the opinion of sanders et al 1983 only 8 studies refer to the selection of water quality indicators table 3 and 7 studies are after the year 2013 water quality indicators thus far mainly focus on general physicochemical parameters and organic pollutant indicators nguyen et al 2019 summarized the most frequently reported water quality indicators in rivers the top five are bod do nitrate ph and conductivity the most used approach is multivariable statistics such as pca and pfa where the primary purpose is to reduce the number of water quality indicators for instance ouyang 2005 was the first to use pca and pfa to evaluate 20 water quality indicators in the wqmn of the lower st johns river lsjr and identified several key water quality indicators that contributed most significantly calazans et al 2018a 2018b used ca to divide the stations into multiple groups and then used the pca pfa method to study the main factors and major pollutants in each group interestingly villas boas et al 2017 used a nonlinear principal component analysis nlpca based on an autoassociative neural network to evaluate the redundancy of water quality indicators in the piabanha river brazil guigues et al 2013 recognized three very different behaviours of water quality variability indicators with high temporal variability and low spatial variability e g suspended solids indicators with high spatial variability and average temporal variability e g calcium and finally indicators with both high temporal and spatial variability e g nitrate thus indicators cannot be reduced beyond these three basic categories existing studies are all focused on rivers or watersheds other water bodies such as lakes reservoirs and estuaries obviously have different characteristics of water quality changes and more studies are needed 6 wqmn design linked to water quality modelling 6 1 surface water quality modelling helps wqmn design surface water management often involves the monitoring and modelling of water quality and quantity water quality models can be used in areas or periods that monitoring is not feasible or accessible as well as used to assess and predict water quality status resulting from different management strategies fu et al 2019 reis et al 2015 the design of monitoring network is also closely related to water quality models and modelling as shown in fig 9 on one hand wqmn outputs measurement data based on designed network parameters for calibration validation and training the water quality models zheng et al 2018 on the other hand water quality models in turn improve the network design water quality model is able to extent monitoring data for data scarce scenarios shi et al 2018 and the model performance can be used as the optimization objectives of network representativeness and reliability in the investigated studies water quality models whether physical process based models nikoo et al 2016 or data driven models hunt et al 2008 are recently combined with network design methods such as chen et al 2012 do et al 2011 hunt et al 2008 nikoo et al 2016 puri et al 2017 shi et al 2018 6 2 wqmn deployment helps water quality modelling an effective water quality modelling platform needs a well deployed wqmn in turn the development of online multi parametric water sensors improve the performance both of data driven models and process based models a new trend is integrated modelling and smart sensors under the big data paradigm zheng et al 2018 provided an in depth analysis on how the crowdsourcing data acquisition as a largely distributed monitoring network impacts and improves geophysical modelling it is promising to develop a real time monitoring and early warning system on hydrology and water quality with a bespoke network of wireless water sensors combined with machine learning data assimilation technique supported with real time monitoring has been widely used to improve forecasting performance of process based models cooper et al 2018 park et al 2020 assimilating high frequency water quality data actualize the identification of the multiple sources of uncertainty involving model parameters model structure hydrology hydraulics water quality future forcing e g rainfall temperature wind speed and solar radiation and observations which challenge the validation and application of water quality models cooper et al 2018 kim et al 2014 research has shown real time monitoring of river water quality can be used to improve the control of urban wastewater systems and thus comply downstream river water quality requirements with reduced energy consumption for wastewater treatment meng et al 2017 2020 thus the design of wqmns for supporting water quality modelling should be practically considered in the future research reis et al 2015 proposed a data to information transformation by the intergradation of monitoring and smart sensors chacon hurtado et al 2017 demonstrated the roles of measurements in rainfall runoff modelling and classified the model free and model based approaches for network design more investigations are needed to discuss the appropriate combination of water quality models for various scenarios fu et al 2019 such as lakes and reservoirs different non point source pollution scenarios and different spatial temporal scales 7 emerging issues and future directions 7 1 modern water quality sensors and wqmn design relative to smart cities in situ sensor observation has become increasingly popular with the spread of internet of things applications reis et al 2015 automatic high frequency monitoring ahfm based water environment management is emerging and a few studies in our review of the literature have discussed this trend horsburgh et al 2010 jácome et al 2018 nam and aral 2007 the design of sensor based wqmns involve communication data storage power management and other special factors particular concern should be paid to this topic monitoring devices in smart cities tend to be miniaturized intelligent and multifunctional and their portability is greatly enhanced these devices do not need to be fixed in one place for a long time such as a traditional monitoring station therefore the optimal monitoring layout will become increasingly important and the dynamic layout optimization of the monitoring network can be adjusted at any time information collection technologies in smart cities are expanding and now include passive and active remote sensing using radars and satellites microwave links crowdsourcing and citizen observatories zheng et al 2018 this unconventional information can supplement the limitations of traditional networks and new monitoring network design methods are needed to build a unified heterogeneous sensor network chacon hurtado 2019 the papers reviewed here are mainly focused on the design of large scale monitoring projects including watersheds lakes and bays and the methods used perform well at this scale in the foreseeable future it will be difficult and unnecessary to carry out the detailed design of monitoring networks in smart cities as mentioned on a large scale as mentioned above therefore traditional in basin monitoring projects provide large scale background support for water quality management for smart city construction the development of new design frameworks that can provide refined management of urban water bodies while complementing each other is urgently needed 7 2 similarity and nexus with other monitoring networks comparison with water quality monitoring network design on other water systems is helpful they typically involve underground water and artificial water bodies by municipal engineering such as water supply network bragalli et al 2019 he et al 2018 drainage system casal campos et al 2018 and channels chen and han 2018 in contrast to surface water quality monitoring the design of groundwater monitoring networks usually places greater emphasis on identifying pollution sources amirabdollahian and datta 2013 loaiciga et al 1992 considering the three dimensional diffusion of pollutants in groundwater the monitoring design will be more complicated because various types of underground data are difficult to obtain models are also crucial for network design there are fewer literature reports on the quantitative design of urban water distribution networks and sewer systems he et al 2018 compared with open water bodies the impact of these clear cut systems is relatively controllable and various statistical methods may be more suitable for designing monitoring networks yazdi 2018 the design and optimization of the hydrological monitoring network are similar to that of surface water quality i e considering the acquisition of information reducing redundant information and reducing the uncertainty of other points therefore they share the same methodology chacon hurtado et al 2017 monitoring of hydrological parameters e g rainfall and stream flow in many circumstances can be integrated with monitoring of water quality indicators but the combined design considering their interactions is rarely reported in literature nevertheless hydrological models including rainfall and runoff modelling are more well developed than water quality models and the combination of monitoring networks and models can provide more reliable and accurate results in addition various modern monitoring approaches such as remote sensing microwave and crowdsourcing are also easier to incorporate into hydrological monitoring campaigns and can complement traditional monitoring networks chacon hurtado 2019 as a result traditional monitoring networks must be updated to allow the assimilation of such heterogeneous dynamic data similar trends are apparent for wqmns for smart cities as depicted in section 5 1 7 3 additional relative issues other issues warrant specific discussion 1 data scarcity in reality and associated uncertainty the majority of studies have been conducted under the condition or assumption of sufficient data for design however in reality the available data are often scarce especially for setting up a new wqmn in ungauged water bodies alilou et al 2019 noted this limitation a combined approach coupling an analytic network process fuzzy logic and river mixing length was proposed and finally identified the six most appropriate locations and four candidate locations in a watershed in northwest iran how to treat the uncertainty associated with limited data availability during design is an important question an alternative to solve the dilemma of data scarcity is coupling with a water quality model 2 aftermath evaluation of re designed wqmns few studies have investigated the performance of re designed networks how to evaluate the performance of updated networks is an important question for good practices 3 adapt to more complex network architecture some studies have proposed station locations with different levels of priority alilou et al 2019 chang and lin 2014a chang et al 2014 which is usually a natural output of the optimization process such an approach is a good way to balance financial limitations and network functions by setting up a more complex network architecture like computer storage hierarchy of registers in a cpu which includes l1 l3 caches main memory local secondary storage and remote secondary storage distributed file system web services berger 2005 a hierarchy for wqmn is proposed in fig 10 this pathway can also incorporate monitoring network construction such as phase i stations and phase ii stations 7 4 future research directions based on the outcomes of this review and emerging trends in water environmental management the following four research directions with ten specific questions or issues have been identified for the research community and professionals working on surface water wqmn construction fig 11 1 innovate design patterns and methods a meta analysis to find new patterns when suitable cases are available in the literature meta analysis will provide new insights on the influencing factors of the network such as water body size and the extent of human activity impacts and the performance of design methods huang and han 2014 zhuo et al 2015 b novel design methods novel design methods are still desired particularly for water bodies linked with complex social and economy activities e g wqmns for urban receiving water connected to drainage systems researchers from alibaba business college utilized complex network theory to design a wqmn for an urban water environment xiang et al 2016 this approach makes good use of topical characteristics and water quality records 2 embrace emerging monitoring technologies a surfing the wave of automatic high frequency monitoring the recent rise of high frequency monitoring has promoted innovation in water quality management kunz et al 2017 marcé et al 2016 rode et al 2016 marcé et al 2016 argued that ahfm maximizes the provision of ecosystem services by lakes and reservoirs and is conductive for reporting lake status to management agencies it also uncovers new patterns such as concentration relationships bouchez et al 2017 moatar et al 2017 storm event responses blaen et al 2017 and water chemistry kunz et al 2017 how to optimally design different sampling frequencies and select water quality indicators under high time resolution observations is an open question b surrogate monitoring and soft measurement surrogate monitoring is a peer to ahfm and has been used in water management practices horsburgh et al 2010 jones et al 2011 it improves the variability of the selection of water quality indicators machine learning technology can be used in water quality monitoring by linking external variables such as rainfall temperature and solar radiation with observations from in situ water quality monitoring sensors to provide better water quality estimation c sensor network the use of sensors is increasingly popular and particular concern should be paid to this topic some discussion has been provided in section 5 1 d design towards manifold measurement instruments as mentioned in section 5 1 options for monitoring measurements are increasing a modern monitoring network usually combines various monitoring instruments and several functions the quantitative design of an optimal network under a complex architecture is very challenging some have recognized this issue in smart city construction chen and han 2018 demonstrated how to construct water quality monitoring infrastructures for a smart city 3 meet new administration requirements a network for non point source management non point source nps management including nutrients in surface water is an important topic only six studies in the literature focused on wqmn design for watershed nps management greater consideration of this topic is needed b network for pollution source identification since the first paper on wqmn design the purpose of identifying pollution sources has been associated with wqmns designing an effective network to meet the environmental forensics requirements of water agencies by holistically considering the monitoring location frequency and indicators is a significant issue that remains challenging for urban water management in developing countries due to the complexity of point source release and transport processes 4 toward better practices a practical guidelines for wqmn design a systemic assessment of design methods and the establishment of practical guidelines or frameworks for wqmn design under typical scenarios are anticipated b developing gis based network design tools a computer aided design tool based on the gis platform is in high demand by managers but has not yet been developed 8 conclusion the design of an appropriate water monitoring network is a fundamental aspect of water management as it is the first step in providing a representative and reliable estimation of the quality of surface waters for all stakeholders keeping in mind that the best monitoring network is a fit for purpose and cost effective one great care should be dedicated to the process of re designing such a network guigues et al 2013 the following major lessons can be learned from this critical review the quantitative design of wqmns is currently in a stage of rapid development and several successful methods have been proposed in the literature topology multivariate statistics geostatistics information entropy and single and multiple criteria optimization are typical categories among the network parameters the station location is of much investigated while studies on sampling frequency and water quality indicators are relative less the pros and cons of these methods for difference network parameters have been summarized in this work the chronological changes journal distribution authors and affiliations citations study areas of reviewed literatures are all present interesting and meaningful patterns it seems to be a tendency in developing countries to develop more wqmn construction practices from the aspect of country distribution pattern of publications the purpose of wqmn design varies from country to country and from decade to decade due to the diversity and succession of environmental problems hence studies on wqmn design will keep active for a long run accounting for the development of monitoring technology and continuous investment in the smart city era surface water wqmns present new characteristics such as dynamic heterogeneous coupling with other urban monitoring infrastructures such as urban flood control transportation and security moreover the spatial scale of a city is sometimes inconsistent with the natural boundaries of natural water bodies and the time scale or observation frequency requirements for precise urban environment management may not match those of traditional wqmns those challenges demand smart solutions for network design the summarized design methods lay the foundation for success under these more complex management conditions large gaps in knowledge or methods imply opportunities for example how can the design of sampling frequency and water quality indicators be improved in the age of high frequency water quality management how should appropriate optimization objectives and the representativeness and stability of the network under different conditions or restrictions be defined how should the results provided by different methods be evaluated how should wqmns be designed under uncertainty new design methods are still very much needed particularly for non point source management emergency monitoring mobile monitoring and pollution source identification furthermore the international hydrology community proposed 23 open unsolved problems in hydrology in 2019 blöschl et al 2019 precisely designing a monitoring network will definitely play a fundamental role in solving these problems there is still a long road ahead before mature official standardized design guidelines can be issued for industrial utilization declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work was supported by the key area research and development program of guangdong province grant no 2019b110205005 national natural science foundation of china grant no 51979136 and innovation project of universities in guangdong province natural science grant no 2018ktscx201 we thank prof ke sheng cheng from taiwan university and dr abdou khouakhi from cranfield university for helpful communication yang gu for the help with the literature search and statistics and anonymous reviewers for constructive suggestions appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2020 104792 
25979,environmental decision support systems edss have drawn an increasing attention among scientists to tackle with the growing complexity of environmental problems and to support policy makers yet many edss reported in literature are case specific fragmented in development strategies and under reporting server setup thus impeding edss development and knowledge sharing among the scientific community in this work we introduce an edss development framework mainly based on r language which is popular among environmental scientists and docker related software to lower technical hurdles for deployment in a web based context using two examples we demonstrate that the framework is able to deliver a unified and cost effective solution for setting up prototypes of modern web based edss without compromising usability a public repository is created to promote access to more examples from literature which users can adapt for their own studies keywords web based edss free and open source software foss r shiny docker groundwater model crop model software availability product name open source development framework for edss developer dr yu li software required docker r license mit license availability https github com cocomice edss dep framework 1 introduction decision support systems dss usually refer to computer aided programs that help decision makers to solve unstructured or semi structured information using data and models morton 1971 they are widely used in different fields nowadays among them environmental decision support systems edss specialize in tackling environmental problems and have attracted a growing attention from researchers e g matthies et al 2007 hadded et al 2013 mcdonald et al 2019 whateley et al 2015 shao et al 2017 the popularity of edss on one hand is due to the increasing difficulties in solving environmental problems which are often entangled with human society turning them into complex coupled human nature systems liu et al 2007 the complexity is further amplified by global change which imposes extra uncertainties concerning the future state of the world pachauri et al 2014 walker et al 2013 milly et al 2008 on the other hand the improvement of computer and monitoring technologies have enabled researchers to produce access vast amounts of information and to develop more advanced analytic tools however those trends not only increase the potential of edss but also demand modern edss implementation to be more democratic user friendly and flexible mir and quadri 2009 loucks 1995 hewitt and macleod 2017 zulkafli et al 2017 the democratisation implies the participation of stakeholders in data collection development of edss as well as their final operation the user friendliness aims to lower technical barriers so that policy makers lacking specific knowledge can still apply those complex analytic tools for their decision making the flexibility requires edss to be able to include new information functionalities with ease but also to allow integration with legacy models which were built with state of the art science but not designed for the internet context kumar et al 2015 web based applications have become a popular solution that overcomes many of the challenges such as accessibility users can easily visit applications on their computers or smart devices swain et al 2015 the benefit can be further leveraged via cloud computing to remove computation limits and to provide on demand load balancing in the face of a high numbers of simultaneous users yet most edss reported in literature are case specific they are implemented in different programming languages and the programs sometimes are closed source making them difficult to be transferred adapted to other study regions this partly explains why many case studies end up with their edss being only experimental small scale and underused in practice matthies et al 2007 moreover the development of a web based edss is not limited to the software implementation but requires a number of indispensable steps to make the entire system operational in a web context including testing deployment on servers and maintenance over time etc murugesan et al 2001 which are often under reported in literature regardless they represent additional if not significant burdens on developers whose primary background is environmental modelling to address all those steps usually requires specific knowledge of web engineering or outsourcing to third party services with non trivial cost usually unaffordable for small research projects last but not least development of edss requires active engagement between decision makers and developers which is a recurring theme during any project loucks 1995 hewitt and macleod 2017 zulkafli et al 2017 this requirement becomes even more prominent if a large number of stakeholders is involved while lack of sufficient participation leads to poor acceptance of the edss by end users volk et al 2010 being able to prototyping edss that assemble the user interface ui and the core functions will allow the participation of stakeholders at an early stage so they can evaluate the systems and provide feedback from their experience smith 1991 as a result it increases the likelihood of successful use of developed edss therefore having an effective development framework for fast prototyping edss or even providing a production ready high quality web based application will be extremely valuable particularly for research teams with limited resources allowing them to engage less in technical exercises and spend their efforts in more crucial research tasks such a development framework in particular should favor free and open source software foss to promote knowledge sharing and adaptation cover complete stages in developing web based edss with minimal interventions and most importantly be capable of producing state of the art edss as seen in literature surprisingly few studies reported that kind of frameworks in a recent paper by swain et al 2016 the authors presented an open source software called tethys platform for developing and hosting web based models for environmental researchers the software implemented in python language van rossum et al 2007 aims to lower technical barriers for researchers in shipping environmental web apps into operational context and has sparked several applications with new tools and software infrastructure being continuously introduced in recent years new opportunities have also emerged for further reducing the aforementioned technical hurdles for environmental researchers who may be acquainted with programming languages other than python in this paper we aim to contribute the toolkits for developing edss by proposing a framework based on the r shiny package docker and other open source software the contribution to the edss community is twofold 1 we introduce a development framework that can quickly produce prototypes of modern edss powered by web accessibility interactive visualization and cloud computation 2 since the edss developed with the framework are portable and can be easily reproduced on major computer platforms we further set up a public repository collecting examples from literature to promote knowledge sharing and to encourage the adaptation of existing edss note that the framework is designed for researchers with modest experience in programming models in r or similar languages as well as in using command line tools the following sections are organized as follows section 2 describes the detail features of the related software and the proposed development framework section 3 demonstrates the potential of the proposed framework in implementing modern web based edss by showcasing two edss apps for groundwater management and crop planning respectively the final concluding remarks are given in section 4 2 software components and development framework 2 1 r shiny since its birth the programming language r has quickly become one of the top open source languages in the scientific computation community rank 5 according to ieee spectrum 2019 the advantage of using r in environmental studies has been well discussed in literature e g slater et al 2019 andrews et al 2011 and the most cited features are its ease of use support of vast libraries independence of operating system as well as detailed software documentation originally designed for statistical analysis r is now widely applied in various fields of research and is particularly popular among environmental scientists e g lai et al 2019 tippmann 2015 among those packages shiny chang et al 2017 is the one providing a framework for building interactive web applications using that package researchers can easily turn their r based models into edss as web based apps targeting stakeholders a shiny app requires two files namely ui r and server r where the former sketches the graphical user interface i e front end while the latter implements routines to process user inputs i e backend the package ships with a number of application programming interfaces apis that encompass common ui elements such as sliders buttons file upload download etc for accepting user inputs and for creating output panels in addition thanks to the community support other packages have been continuously introduced to embrace state of the art visualization techniques and software related to geographic information systems gis as an example table 1 lists a number of packages for implementing common functionalities in modern edss some literature already reported the use of r shiny for edss development in their case studies whateley et al 2015 developed a shiny app for performing climate risk evaluations of small scale water utilities in the northeastern u s hewitt and macleod 2017 discussed several criteria for implementing user oriented edss and presented a shiny app as a qualified example ye et al 2018 proposed to use shiny for developing cloud based water resource analysis tools with a demonstration of a case study in china despite the fact that shiny apps are easy to implement for r users deploying them in a web based operational context is more cumbersome and technical a task that is often beyond the specialties of environmental scientists what is worse native shiny apps do not allow multiple users to access the same app simultaneously which is clearly in conflict with the democratic use of edss among stakeholders one solution to circumvent those issues is to use shinyapps io a web provider specific for hosting shiny applications with a subscription cost fortunately alternatives are available and are implemented in the proposed framework which allow the deployment of shiny apps either on a private server or on the cloud using docker 2 2 docker docker is an open source virtualization software which is very popular among developers in recent years bernstein 2014 merkel 2014 it allows applications to be encapsulated as a docker image a file including the application its dependencies and fundamentals of an operating system os for running apps when launched in docker the image is incarnated into a running process in an isolated environment called a container the isolation also provides security benefit since it can protect host machine from being affected by mulfunctioning apps in containers one notable difference between docker and a virtual machine vm is that docker does not require a virtual os as vm does see fig 1 making it more lightweight i e better scalability while still keeping apps in isolation i e portability and security as a consequence developers can use existing hardware more efficiently in addition upscaling docker based applications is straightforward since it only needs duplication of containers based on users demand all those features imply that docker based applications can easily fit into a production context such as an enterprise cloud or a private server last but not least to deploy an application into the docker platform only requires preparing an instruction file named dockerfile without modifying underlying infrastructure of an application in addition to shiny and docker another two open source software products are used to construct a complete framework namely shinyproxy reese 2008 and nginx verbeke and michielssen 2016 the shinyproxy is used for communicating between shiny apps and docker and it also provides additional features such as user authorization while the nginx is a web serving program for managing web apps easily and securely both shinyproxy and nginx are provided as docker images which can directly be run on the docker platform the following section proceeds with a detailed explanation of the proposed integrated framework 2 3 the infrastructural design of the framework the proposed framework is shown in fig 2 specifically for a web based application a browser serves as front end to present graphic interface to end users applications can be accessed on the internet via a uniform resource identifier uri that locates resources on a server running edss on the server side the containerized software i e shiny apps shinyproxy nginx are deployed inside the docker environment where nginx acts as a reverse proxy that directs users requests to shinyproxy which then launches the required shiny applications the shinyproxy organizes and manages shiny applications in the sense that when multiple users try to access the same shiny app it will automatically duplicate the containerized shiny app to match the demand deleting them when the task is finished therefore it resolves the restriction of single user access in native shiny apps as mentioned before data encryption has also been addressed automatically to provide a secured internet traffic compatible with modern https standard the bottom part of our infrastructure are containerized shiny apps it should be mentioned that a containerized program is ephemeral meaning that intermediates e g the model outputs user inputs etc generated from a running app will be destroyed after users disconnection therefore permanent information must not be packed with apps into containers in docker this problem can be solved by having a database external to a containerized program as shown in fig 2 besides the database the configuration file of shinyproxy is also located externally to the docker environment so developers can adapt it when necessary 2 4 development workflow the workflow of developing a web based edss using our framework can be summarized in the following four steps step 1 developing shiny applications step 2 preparing docker image files for the app step 3 adapting the configuration file for shinyproxy step 4 deploying the software on docker platform among them step 1 is the part which requires main effort from researchers it involves coupling models and optionally databases or external databases to apps for models that have been written in r researchers only need to wrap them with the shiny apis existing models programmed with other languages i e legacy models can be loosely coupled by implementing in r the processing routines of input output files to those models see the example in section 3 1 the framework also comes with a built in mysql database dubois and foreword by widenius 1999 and an interface for manipulating tables in it which can be useful for cases when data should be stored permanently outside the containerized apps see the example in section 3 2 step 2 packs the developed shiny app into a docker image which can then be run as docker container s during deployment the image can be shared among researchers so others can reproduce the application on their system in step 3 developers have to adapt two configuration files namely the application yml and docker compose yml the first file is used to configure the shinyproxy so it can automatically manage all shiny apps as well as user groups while the second file is used for setting up a server on the internet as shown in fig 3 a minimal adaptation requires adding the image name of developed shiny apps creating different user groups and specifying a server s domain name specifically the user specification block defines legitimate users with their authorization details therefore only the users in this list can access apps the shiny app specification defines the parameters of each shiny app integrated within the framework so that they can be correctly displayed and executed in the domain specification users have to specify a server s domain name such as www example com detailed instructions can be found on our github repository https github com cocomice edss dep framework git in the end the entire edss system can be launched by running docker compose run d end users can visit the applications on the internet by browsing the domain specified in the configuration file the framework has a built in portal to manage all shiny apps hosted in the system as shown in fig 4 the first page fig 4 a is the login screen from which legitimate users can then enter the navigation page fig 4 b to access the shiny apps available to him her then each app can be opened individually for use behind the scenes resides the automatic server setup which allows the renewal of https certificates tracking application status and easy maintenance of the database the docker based containerization technique allows multiple users to open the same app simultaneously therefore an edss developed within the proposed framework can be easily fitted into operational context in addition by distributing working folders others can reproduce the same system on their machine by repeating steps 2 to 4 3 example applications 3 1 on line groundwater modelling platform in this section we present an edss for conjunctive water management by coupling a calibrated groundwater model in a shiny app the shiny package is easy to use and effective for building a user friendly front end interface while computational routines in the back end are essentially models that can either be transformed from researchers own r scripts or be integrated with legacy models the integration not only saves development effort but more importantly can take advantage of a program written in low level programming languages e g c c fortran which are less human interpretable but more computationally efficient compared to r thanks to the versatility of r language and package support legacy models can be loosely integrated with shiny applications by communicating only inputs outputs between the shiny apps and the models in this example we showed an application which directly integrates a shiny app with modflow harbaugh et al 2000 a widely used groundwater modelling program written in fortran the model has been calibrated for a shallow aquifer in the middle reach of heihe river basin from our previous work li and wolfgang kinzelbach 2020 fig 5 shows the interface with three highlighted parts the first one is the input panel where users can specify scenarios to drive the groundwater model which includes the irrigation water use i e planning decisions and boundary conditions the interaction is done via moving of sliders or uploading files the second and third parts are output windows where the top panel shows an interactive map of the heihe mid reach for displaying spatially varying hydraulic heads the bottom panel shows a time series plot for head change at a user chosen location and overall statistics of water use all figures are created as interactive plots using leaflet cheng et al 2019 and ramcharts thieurmel et al 2019 packages so users can further inspect simulated results in the spirit of a data driven decision making process provost and fawcett 2013 the ui language can be switched between chinese and english by clicking the radio button at the top left corner on the back end the app processes users inputs and triggers simulations of the groundwater model it can easily become an overwhelming task if one wants to implement a modflow model in r instead in this case the shiny app only prepares input files compatible with modflow calls the native modflow executable compiled in fortran and then retrieves outputs for producing plots the integration between shiny app and modflow model is thus a loose coupling as shown in fig 6 similar edss can follow this manner to include other legacy models for implementing real time simulation routines however one should note that for large scale complex models the latency between users inputs and output display can be large due to longer simulation time therefore instead of explicitly integrating a model in an edss one can store results for a sizable number of input scenarios to build a lookup table then the output from a given input scenario of a user can be retrieved rapidly 3 2 dashboard for crop irrigation management crop irrigation planning is another common subject for applying edss in this example we show a shiny app which includes a crop model and a data portal for monitoring meteorological conditions the data and materials are based on a pilot region in china named guantao county located in north china plain where the subsurface groundwater resource is jeopardized by overpumping practices e g liu et al 2011 li et al 2019 the ui is implemented as a dashboard consisting of three tabs see fig 7 a c the home tab provides background information about the study region the irrigation calculator tab contains an interface to a crop model for estimating crop water deficit the tool is implemented following fao s aquacrop routines steduto et al 2009 with user specified inputs i e crop type soil type planting date on the left panel the model calculates expected water deficit for wet normal dry years based on observed meteorological data from the database in the data portal tab users can visualize those meteorological data or download them for demonstration purposes in this example only synthetic precipitation data is used it should be mentioned that by the design of the containerization technique the groundwater modelling app from the first example will be reset automatically to its initial state whenever users close the application therefore any data generated from the system is only available during users access instead the database used in the crop water management app is external to the containerized shiny app stored in a database server i e mysql see fig 7 d and lives permanently on a disk regardless of users access in an operational context with data monitoring infrastructure this decoupling between a database and a containerized shiny app allows the data to evolve with real time data transmission while tools built in a shiny app can always access the latest data for decision support in summary the two examples showed the capability of the framework to produce simple and user friendly interfaces without compromising common functionalities e g gis interactive visualization file upload download seen in a modern edss it is also possible to integrate with external database or legacy models for more practical uses the authors also set up a repository where other edss from literature were adapted according the proposed framework the goal is to reuse materials from existing case studies and to further promote knowledge sharing 4 conclusions and outlook in this work we introduced an integrated software framework for developing edss the framework is based on r shiny package docker and a few other open source software products which allows researchers to benefit from the versatility of r for developing web based apps while lowering technical hurdles in deploying maintaining edss on servers to sum up the following features of the framework have been identified 1 easy implementation of user friendly and interactive web based applications 2 high flexibility for integrating legacy modelling tools 3 low technical barrier to setup the system on a server for production ready context those features were demonstrated using two examples one based on groundwater modelling for conjunctive water planning and the other for crop irrigation management these and more examples are included in a public repository to promote knowledge sharing and replication of case studies from literature due to the popularity of r language and its wide use in the scientific community the proposed framework can also be used in an educational context for teaching edss declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is supported by the swiss agency for development and cooperation under the project rehabilitation and management strategy for over pumped aquifers under a changing climate we thank dr beatrice marti from hydrosolution ltd for contributing the crop model mr gianni pedrazzini from holinger ltd for contributing the groundwater model and dr wolfgang kinzelbach as the leader of the project we also thank the developers of open source shinyproxy and other docker based software used in this work 
25979,environmental decision support systems edss have drawn an increasing attention among scientists to tackle with the growing complexity of environmental problems and to support policy makers yet many edss reported in literature are case specific fragmented in development strategies and under reporting server setup thus impeding edss development and knowledge sharing among the scientific community in this work we introduce an edss development framework mainly based on r language which is popular among environmental scientists and docker related software to lower technical hurdles for deployment in a web based context using two examples we demonstrate that the framework is able to deliver a unified and cost effective solution for setting up prototypes of modern web based edss without compromising usability a public repository is created to promote access to more examples from literature which users can adapt for their own studies keywords web based edss free and open source software foss r shiny docker groundwater model crop model software availability product name open source development framework for edss developer dr yu li software required docker r license mit license availability https github com cocomice edss dep framework 1 introduction decision support systems dss usually refer to computer aided programs that help decision makers to solve unstructured or semi structured information using data and models morton 1971 they are widely used in different fields nowadays among them environmental decision support systems edss specialize in tackling environmental problems and have attracted a growing attention from researchers e g matthies et al 2007 hadded et al 2013 mcdonald et al 2019 whateley et al 2015 shao et al 2017 the popularity of edss on one hand is due to the increasing difficulties in solving environmental problems which are often entangled with human society turning them into complex coupled human nature systems liu et al 2007 the complexity is further amplified by global change which imposes extra uncertainties concerning the future state of the world pachauri et al 2014 walker et al 2013 milly et al 2008 on the other hand the improvement of computer and monitoring technologies have enabled researchers to produce access vast amounts of information and to develop more advanced analytic tools however those trends not only increase the potential of edss but also demand modern edss implementation to be more democratic user friendly and flexible mir and quadri 2009 loucks 1995 hewitt and macleod 2017 zulkafli et al 2017 the democratisation implies the participation of stakeholders in data collection development of edss as well as their final operation the user friendliness aims to lower technical barriers so that policy makers lacking specific knowledge can still apply those complex analytic tools for their decision making the flexibility requires edss to be able to include new information functionalities with ease but also to allow integration with legacy models which were built with state of the art science but not designed for the internet context kumar et al 2015 web based applications have become a popular solution that overcomes many of the challenges such as accessibility users can easily visit applications on their computers or smart devices swain et al 2015 the benefit can be further leveraged via cloud computing to remove computation limits and to provide on demand load balancing in the face of a high numbers of simultaneous users yet most edss reported in literature are case specific they are implemented in different programming languages and the programs sometimes are closed source making them difficult to be transferred adapted to other study regions this partly explains why many case studies end up with their edss being only experimental small scale and underused in practice matthies et al 2007 moreover the development of a web based edss is not limited to the software implementation but requires a number of indispensable steps to make the entire system operational in a web context including testing deployment on servers and maintenance over time etc murugesan et al 2001 which are often under reported in literature regardless they represent additional if not significant burdens on developers whose primary background is environmental modelling to address all those steps usually requires specific knowledge of web engineering or outsourcing to third party services with non trivial cost usually unaffordable for small research projects last but not least development of edss requires active engagement between decision makers and developers which is a recurring theme during any project loucks 1995 hewitt and macleod 2017 zulkafli et al 2017 this requirement becomes even more prominent if a large number of stakeholders is involved while lack of sufficient participation leads to poor acceptance of the edss by end users volk et al 2010 being able to prototyping edss that assemble the user interface ui and the core functions will allow the participation of stakeholders at an early stage so they can evaluate the systems and provide feedback from their experience smith 1991 as a result it increases the likelihood of successful use of developed edss therefore having an effective development framework for fast prototyping edss or even providing a production ready high quality web based application will be extremely valuable particularly for research teams with limited resources allowing them to engage less in technical exercises and spend their efforts in more crucial research tasks such a development framework in particular should favor free and open source software foss to promote knowledge sharing and adaptation cover complete stages in developing web based edss with minimal interventions and most importantly be capable of producing state of the art edss as seen in literature surprisingly few studies reported that kind of frameworks in a recent paper by swain et al 2016 the authors presented an open source software called tethys platform for developing and hosting web based models for environmental researchers the software implemented in python language van rossum et al 2007 aims to lower technical barriers for researchers in shipping environmental web apps into operational context and has sparked several applications with new tools and software infrastructure being continuously introduced in recent years new opportunities have also emerged for further reducing the aforementioned technical hurdles for environmental researchers who may be acquainted with programming languages other than python in this paper we aim to contribute the toolkits for developing edss by proposing a framework based on the r shiny package docker and other open source software the contribution to the edss community is twofold 1 we introduce a development framework that can quickly produce prototypes of modern edss powered by web accessibility interactive visualization and cloud computation 2 since the edss developed with the framework are portable and can be easily reproduced on major computer platforms we further set up a public repository collecting examples from literature to promote knowledge sharing and to encourage the adaptation of existing edss note that the framework is designed for researchers with modest experience in programming models in r or similar languages as well as in using command line tools the following sections are organized as follows section 2 describes the detail features of the related software and the proposed development framework section 3 demonstrates the potential of the proposed framework in implementing modern web based edss by showcasing two edss apps for groundwater management and crop planning respectively the final concluding remarks are given in section 4 2 software components and development framework 2 1 r shiny since its birth the programming language r has quickly become one of the top open source languages in the scientific computation community rank 5 according to ieee spectrum 2019 the advantage of using r in environmental studies has been well discussed in literature e g slater et al 2019 andrews et al 2011 and the most cited features are its ease of use support of vast libraries independence of operating system as well as detailed software documentation originally designed for statistical analysis r is now widely applied in various fields of research and is particularly popular among environmental scientists e g lai et al 2019 tippmann 2015 among those packages shiny chang et al 2017 is the one providing a framework for building interactive web applications using that package researchers can easily turn their r based models into edss as web based apps targeting stakeholders a shiny app requires two files namely ui r and server r where the former sketches the graphical user interface i e front end while the latter implements routines to process user inputs i e backend the package ships with a number of application programming interfaces apis that encompass common ui elements such as sliders buttons file upload download etc for accepting user inputs and for creating output panels in addition thanks to the community support other packages have been continuously introduced to embrace state of the art visualization techniques and software related to geographic information systems gis as an example table 1 lists a number of packages for implementing common functionalities in modern edss some literature already reported the use of r shiny for edss development in their case studies whateley et al 2015 developed a shiny app for performing climate risk evaluations of small scale water utilities in the northeastern u s hewitt and macleod 2017 discussed several criteria for implementing user oriented edss and presented a shiny app as a qualified example ye et al 2018 proposed to use shiny for developing cloud based water resource analysis tools with a demonstration of a case study in china despite the fact that shiny apps are easy to implement for r users deploying them in a web based operational context is more cumbersome and technical a task that is often beyond the specialties of environmental scientists what is worse native shiny apps do not allow multiple users to access the same app simultaneously which is clearly in conflict with the democratic use of edss among stakeholders one solution to circumvent those issues is to use shinyapps io a web provider specific for hosting shiny applications with a subscription cost fortunately alternatives are available and are implemented in the proposed framework which allow the deployment of shiny apps either on a private server or on the cloud using docker 2 2 docker docker is an open source virtualization software which is very popular among developers in recent years bernstein 2014 merkel 2014 it allows applications to be encapsulated as a docker image a file including the application its dependencies and fundamentals of an operating system os for running apps when launched in docker the image is incarnated into a running process in an isolated environment called a container the isolation also provides security benefit since it can protect host machine from being affected by mulfunctioning apps in containers one notable difference between docker and a virtual machine vm is that docker does not require a virtual os as vm does see fig 1 making it more lightweight i e better scalability while still keeping apps in isolation i e portability and security as a consequence developers can use existing hardware more efficiently in addition upscaling docker based applications is straightforward since it only needs duplication of containers based on users demand all those features imply that docker based applications can easily fit into a production context such as an enterprise cloud or a private server last but not least to deploy an application into the docker platform only requires preparing an instruction file named dockerfile without modifying underlying infrastructure of an application in addition to shiny and docker another two open source software products are used to construct a complete framework namely shinyproxy reese 2008 and nginx verbeke and michielssen 2016 the shinyproxy is used for communicating between shiny apps and docker and it also provides additional features such as user authorization while the nginx is a web serving program for managing web apps easily and securely both shinyproxy and nginx are provided as docker images which can directly be run on the docker platform the following section proceeds with a detailed explanation of the proposed integrated framework 2 3 the infrastructural design of the framework the proposed framework is shown in fig 2 specifically for a web based application a browser serves as front end to present graphic interface to end users applications can be accessed on the internet via a uniform resource identifier uri that locates resources on a server running edss on the server side the containerized software i e shiny apps shinyproxy nginx are deployed inside the docker environment where nginx acts as a reverse proxy that directs users requests to shinyproxy which then launches the required shiny applications the shinyproxy organizes and manages shiny applications in the sense that when multiple users try to access the same shiny app it will automatically duplicate the containerized shiny app to match the demand deleting them when the task is finished therefore it resolves the restriction of single user access in native shiny apps as mentioned before data encryption has also been addressed automatically to provide a secured internet traffic compatible with modern https standard the bottom part of our infrastructure are containerized shiny apps it should be mentioned that a containerized program is ephemeral meaning that intermediates e g the model outputs user inputs etc generated from a running app will be destroyed after users disconnection therefore permanent information must not be packed with apps into containers in docker this problem can be solved by having a database external to a containerized program as shown in fig 2 besides the database the configuration file of shinyproxy is also located externally to the docker environment so developers can adapt it when necessary 2 4 development workflow the workflow of developing a web based edss using our framework can be summarized in the following four steps step 1 developing shiny applications step 2 preparing docker image files for the app step 3 adapting the configuration file for shinyproxy step 4 deploying the software on docker platform among them step 1 is the part which requires main effort from researchers it involves coupling models and optionally databases or external databases to apps for models that have been written in r researchers only need to wrap them with the shiny apis existing models programmed with other languages i e legacy models can be loosely coupled by implementing in r the processing routines of input output files to those models see the example in section 3 1 the framework also comes with a built in mysql database dubois and foreword by widenius 1999 and an interface for manipulating tables in it which can be useful for cases when data should be stored permanently outside the containerized apps see the example in section 3 2 step 2 packs the developed shiny app into a docker image which can then be run as docker container s during deployment the image can be shared among researchers so others can reproduce the application on their system in step 3 developers have to adapt two configuration files namely the application yml and docker compose yml the first file is used to configure the shinyproxy so it can automatically manage all shiny apps as well as user groups while the second file is used for setting up a server on the internet as shown in fig 3 a minimal adaptation requires adding the image name of developed shiny apps creating different user groups and specifying a server s domain name specifically the user specification block defines legitimate users with their authorization details therefore only the users in this list can access apps the shiny app specification defines the parameters of each shiny app integrated within the framework so that they can be correctly displayed and executed in the domain specification users have to specify a server s domain name such as www example com detailed instructions can be found on our github repository https github com cocomice edss dep framework git in the end the entire edss system can be launched by running docker compose run d end users can visit the applications on the internet by browsing the domain specified in the configuration file the framework has a built in portal to manage all shiny apps hosted in the system as shown in fig 4 the first page fig 4 a is the login screen from which legitimate users can then enter the navigation page fig 4 b to access the shiny apps available to him her then each app can be opened individually for use behind the scenes resides the automatic server setup which allows the renewal of https certificates tracking application status and easy maintenance of the database the docker based containerization technique allows multiple users to open the same app simultaneously therefore an edss developed within the proposed framework can be easily fitted into operational context in addition by distributing working folders others can reproduce the same system on their machine by repeating steps 2 to 4 3 example applications 3 1 on line groundwater modelling platform in this section we present an edss for conjunctive water management by coupling a calibrated groundwater model in a shiny app the shiny package is easy to use and effective for building a user friendly front end interface while computational routines in the back end are essentially models that can either be transformed from researchers own r scripts or be integrated with legacy models the integration not only saves development effort but more importantly can take advantage of a program written in low level programming languages e g c c fortran which are less human interpretable but more computationally efficient compared to r thanks to the versatility of r language and package support legacy models can be loosely integrated with shiny applications by communicating only inputs outputs between the shiny apps and the models in this example we showed an application which directly integrates a shiny app with modflow harbaugh et al 2000 a widely used groundwater modelling program written in fortran the model has been calibrated for a shallow aquifer in the middle reach of heihe river basin from our previous work li and wolfgang kinzelbach 2020 fig 5 shows the interface with three highlighted parts the first one is the input panel where users can specify scenarios to drive the groundwater model which includes the irrigation water use i e planning decisions and boundary conditions the interaction is done via moving of sliders or uploading files the second and third parts are output windows where the top panel shows an interactive map of the heihe mid reach for displaying spatially varying hydraulic heads the bottom panel shows a time series plot for head change at a user chosen location and overall statistics of water use all figures are created as interactive plots using leaflet cheng et al 2019 and ramcharts thieurmel et al 2019 packages so users can further inspect simulated results in the spirit of a data driven decision making process provost and fawcett 2013 the ui language can be switched between chinese and english by clicking the radio button at the top left corner on the back end the app processes users inputs and triggers simulations of the groundwater model it can easily become an overwhelming task if one wants to implement a modflow model in r instead in this case the shiny app only prepares input files compatible with modflow calls the native modflow executable compiled in fortran and then retrieves outputs for producing plots the integration between shiny app and modflow model is thus a loose coupling as shown in fig 6 similar edss can follow this manner to include other legacy models for implementing real time simulation routines however one should note that for large scale complex models the latency between users inputs and output display can be large due to longer simulation time therefore instead of explicitly integrating a model in an edss one can store results for a sizable number of input scenarios to build a lookup table then the output from a given input scenario of a user can be retrieved rapidly 3 2 dashboard for crop irrigation management crop irrigation planning is another common subject for applying edss in this example we show a shiny app which includes a crop model and a data portal for monitoring meteorological conditions the data and materials are based on a pilot region in china named guantao county located in north china plain where the subsurface groundwater resource is jeopardized by overpumping practices e g liu et al 2011 li et al 2019 the ui is implemented as a dashboard consisting of three tabs see fig 7 a c the home tab provides background information about the study region the irrigation calculator tab contains an interface to a crop model for estimating crop water deficit the tool is implemented following fao s aquacrop routines steduto et al 2009 with user specified inputs i e crop type soil type planting date on the left panel the model calculates expected water deficit for wet normal dry years based on observed meteorological data from the database in the data portal tab users can visualize those meteorological data or download them for demonstration purposes in this example only synthetic precipitation data is used it should be mentioned that by the design of the containerization technique the groundwater modelling app from the first example will be reset automatically to its initial state whenever users close the application therefore any data generated from the system is only available during users access instead the database used in the crop water management app is external to the containerized shiny app stored in a database server i e mysql see fig 7 d and lives permanently on a disk regardless of users access in an operational context with data monitoring infrastructure this decoupling between a database and a containerized shiny app allows the data to evolve with real time data transmission while tools built in a shiny app can always access the latest data for decision support in summary the two examples showed the capability of the framework to produce simple and user friendly interfaces without compromising common functionalities e g gis interactive visualization file upload download seen in a modern edss it is also possible to integrate with external database or legacy models for more practical uses the authors also set up a repository where other edss from literature were adapted according the proposed framework the goal is to reuse materials from existing case studies and to further promote knowledge sharing 4 conclusions and outlook in this work we introduced an integrated software framework for developing edss the framework is based on r shiny package docker and a few other open source software products which allows researchers to benefit from the versatility of r for developing web based apps while lowering technical hurdles in deploying maintaining edss on servers to sum up the following features of the framework have been identified 1 easy implementation of user friendly and interactive web based applications 2 high flexibility for integrating legacy modelling tools 3 low technical barrier to setup the system on a server for production ready context those features were demonstrated using two examples one based on groundwater modelling for conjunctive water planning and the other for crop irrigation management these and more examples are included in a public repository to promote knowledge sharing and replication of case studies from literature due to the popularity of r language and its wide use in the scientific community the proposed framework can also be used in an educational context for teaching edss declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is supported by the swiss agency for development and cooperation under the project rehabilitation and management strategy for over pumped aquifers under a changing climate we thank dr beatrice marti from hydrosolution ltd for contributing the crop model mr gianni pedrazzini from holinger ltd for contributing the groundwater model and dr wolfgang kinzelbach as the leader of the project we also thank the developers of open source shinyproxy and other docker based software used in this work 
