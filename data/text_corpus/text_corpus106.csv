index,text
530,this study analyzes daily mean streamflow records from 5 311 u s geological survey stream gages in the continental united states and develops a metastatistical extreme value distribution mevd tailored for flood frequency analysis we compare the new tool with the generalized extreme value gev and log pearson type iii lp3 distributions and investigate the role of el niño southern oscillation enso in the generation of floods hence we formulate the mevd in terms of mixture of distributions to describe the occurrence of flood peaks generated under different enso phases we find that the mevd outperforms gev and lp3 distributions respectively in about 76 and 86 of the stations with a significant improvement in the accuracy of quantiles corresponding to return periods much larger than the calibration sample size the enso signature detected in the distributions of the daily peak flows does not necessarily improve the estimation of high return period flow values 1 introduction globally in the period 1998 2017 floods have been the most frequent disaster 43 4 of the natural disasters and have caused more than 140 000 deaths representing 11 of the fatalities due to natural disasters of all types wallemacq and house 2018 within this global context during the 20th century floods in the united states were the number one natural disaster in terms of the number of lives lost and property damage perry 2000 the costliest miller et al 2008 and affected the largest number of people stromberg 2007 they are also the second weather related hazard in terms of fatalities in the united states with 4586 reported deaths between 1959 and 2005 mainly due to flash floods caused by heavy precipitation ashley and ashley 2008 reliable flood frequency estimation methods are the basis to devise and implement strategies for the mitigation of these societal and economic impacts with applications in a number of fields from the design of hydraulic structures to environmental management and planning to flood insurance a key concept is the design flood peak value typically set in national regulations by specifying an average recurrence interval or return period t associated with a probability of being exceeded in each year equal to p 1 t the t year flood in turn is estimated based on the analysis of past floods requiring the selection of a probability distribution to perform this inference using a sample with size s t benson 1962 flood frequency analyses are commonly performed as a part of engineering and planning projects but they too often represent a mere statistical fitting exercise and do not attempt to incorporate a representation of the underlying physical processes as noted by klemeš 1988 1993 the standard approach of extreme event probability estimation is moving towards a higher mathematical abstraction renouncing any leveraging or understanding of the different flood generating mechanisms at play the hydrological process information in the observations is thus often neglected and the selection of an optimal statistical model through a goodness of fit metric remains the main focus of these types of analyses the approach proposed here does not belong to the standard methodology on which flood frequency analyses are usually based three points of departure can indeed be highlighted first it is not a mere fit of a distribution on annual maxima but by dealing with all the observations see section data and methodology for further details the approach described here is closer to the raw observations second the approach we propose for flood frequency analysis is not a mere goodness of fit exercise which is what klemeš 1988 1993 was stigmatizing one of the important points highlighted here is that we need to address predictive uncertainty through a cross validation procedure rather than just use unexplained variance as a measure of uncertainty third but not less important the proposed approach does not just consider peak flows as random numbers but attempts to base the statistical modeling on physical drivers the u s federal guidelines themselves bulletin 17 b iacwd 1982 and its updated version 17 c england et al 2018 recognize that the assumption under which stream gage records are generated by one single flood generating mechanism may not always be realistic they highlight the need to understand and more accurately identify these physical mechanisms and list the identification and treatment of mixed distributions to represent their diversity as a research and application priority even though the idea of a more process driven flood frequency analysis is not necessarily new e g hirschboeck 1987 there has been a renewed interest in recent years in process based formulations of extreme flood distributions e g alila and mtiraoui 2002 smith et al 2011 villarini and slater 2017 barth et al 2019 flood generating processes can quite naturally be analyzed using mixed distributions e g alila and mtiraoui 2002 but the determination of which flood peaks result from the different processes and of when the use of mixed distributions is beneficial remains an open problem e g villarini and slater 2017 there are several different hydrological mechanisms that can drive the occurrence of flood events including snowmelt frontal systems local convective processes monsoons and intense tropical cyclones see villarini 2016 zhang et al 2017 slater et al 2015 compared hydrologic and geomorphic drivers in flood hazard while berghuijs et al 2016 analyzed the dominant flood generating mechanisms across the united states barth et al 2017 2019 investigated the role of atmospheric rivers in the generation of flood peaks across the western united states and suggested a weighted mixed population approach to perform a process driven flood frequency analysis to reflect the differences in flood agents within the context of mixed distributions this study formulates a novel flood frequency distribution and uses it to investigate the role of el niño southern oscillation enso in the generation of floods and the detectability of its signature in observed records enso is a major mode of variability of the coupled atmosphere ocean system associated with episodes of above normal el niño and below normal la niña sea surface temperature in the tropical pacific ocean with impacts on seasonal winds rainfall and temperature across the globe we propose here the use of a novel approach the metastatistical extreme value distribution mevd which can naturally incorporate mixed distributions to represent flood magnitudes generated by different mechanisms the mevd has been introduced by marani and ignaccolo 2015 and has been applied mostly to rainfall zorzetto et al 2016 marra et al 2018 zorzetto and marani 2019 for which it was shown to provide significantly smaller estimation uncertainty when compared to traditional approaches especially when considering return periods that are larger than the sample size used for distribution estimation currently the mevd has yet to be applied to flood magnitudes and flood frequency analysis here we provide the first such application and ask the following relevant questions does the mevd outperform the traditional generalized extreme value gev distribution in flood frequency analysis does the incorporation of mixed probability distributions representing different types of flood events associated with different enso phases improve the estimation of event magnitudes with high return periods to answer these questions we apply the mevd approach to daily records from stream gage stations across the continental united states conus examining the role played by mixtures of distributions associated with different enso phases results are compared and contrasted against those from the gev distribution providing qualitative and quantitative evaluations of their relative predictive performance the gev distribution is the natural reference benchmark for this comparison as it stems from the widely applied traditional extreme value theory smith 1987 katz et al 2002 however other methods are widely applied in the flood frequency analysis practice we thus extended the comparison of cross validation performances to the log pearson type iii lp3 the distribution extensively used in flood frequency analyses in the us uswrc 1976 the paper is organized as follows the data and methodology used in the study are described in section 2 followed by a section 3 detailing the results of the analyses section 4 summarizes the main points of this study and concludes the paper 2 data and methodology we analyzed daily records from 5311 u s geological survey usgs stream gages across the continental united states fig 1 panel a we focus on water years defined to run between october 1 and september 30 and select only sites where at least 30 complete i e with more than 330 daily observations year years of observation exist and where no statistically significant trends are found at the 5 level based on the mann kendall test mann 1945 kendall 1975 the historical time series selected cover the period 1916 2017 with record lengths between 31 and 101 years fig 1 panels b and c consult supplementary fig 1 for an overview of the spatial distribution of the lengths of the historical time series flood frequency analysis requires the identification of independent events here we identify the largest flood peaks within blocks of length equal to t 10 days log a where a is the drainage area in square miles lang et al 1999 additionally we discard the smallest discharge peak within any pair of consecutive peaks if the minimum flow between them does not drop below a threshold equal to 75 of the lower of the two water resources council uswrc 1976 this additional condition is necessary to eliminate secondary peaks occurring during recession periods of previous floods the entire set of peak discharge values resulting from this selection process of uncorrelated events is here called the set of ordinary events to denote that it contains all the independent events that have occurred in the record irrespective of their magnitudes 2 1 mevd approach the metastatistical extreme value distribution originally introduced by marani and ignaccolo 2015 is explicitly formulated on the basis of the probability distribution s of the ordinary values from which the distribution of extremes annual maxima is then derived hence the expression of the mevd describing the extreme events is identified by estimating its parameters using the entire set of observed ordinary events this is quite different from the assumptions at the basis of the traditional extreme value theory evt which focuses on fitting a distribution to the annual maxima or to relatively few values above a high threshold the mevd approach treats as realizations of stochastic variables both the parameters of the distributions describing the ordinary events in each year f x θ where θ is the parameter vector and the number n of yearly event occurrences under these premises the mevd of yearly maxima can be defined as 1 ζ x n 1 ω θ f x θ n g n θ d θ where g n θ is the joint probability distribution of the random variables n and θ discrete in n and continuous in θ and ω θ is the population of the parameters values the ensemble average can be approximated by the sample average computed over all the years of the historical series m becoming 2 ζ x 1 m j 1 m f x θ j n j where f x θ j is the cumulative distribution of ordinary values and nj is the number of events in year j here we apply the mevd to peak discharges and modify this approach to account for ordinary values belonging to different populations corresponding in the present case to different enso phases the cumulative distribution function ζ x of the mixed mevd can be written as follows 3 ζ x 1 m j 1 m p 1 n p h f p x θ j n j p where nph is the number of phases that induce statistically different distributions of the ordinary events and should therefore be considered separately fp is the yearly or time window when the low number of events year requires parameter estimation to be performed of multi year windows cumulative distribution of the ordinary values in phase p nj p is the original yearly number of the peaks in phase p and year j m is the number of years for which observations are available eq 3 reduces to the original formulation in marani and ignaccolo 2015 when only one phase is present the mixed mevd formulation in eq 3 is the same as the approach proposed in marra et al 2019 here instead of using the further simplification smev marra et al 2019 that removes the inter annual variability in the statistics of the ordinary events i e there is no dependence on j in eq 2 we preserve the time variability of the distributions the first step in the application of the mevd approach is identifying a suitable parametric distribution to represent the ordinary events we evaluate three candidate distributions for the f x θ j in eq 2 weibull generalized pareto and gamma distributions we select the most suitable distribution on the basis of the skill score see section on the evaluation metrics for its definition comparing the mevd estimated quantiles to the observed maxima in the present analyses the gamma distribution was the best performing one see section 3 for further details because the average number of flood events in a year is small for many stations e g in 1243 of the 5311 analyzed stations the average number of peaks year is smaller than or equal to 10 see supplementary fig 2 we explore estimating the parameters of the distribution using either five year windows or the entire sample eq 3 can be hence expressed in terms of the windows used for parameter estimation 4 ζ x 1 m j 1 m p 1 n p h f p x θ k j n j p where k j is the index value identifying the window containing year j θ k j is the parameter vector in the k th window p is the number of statistically different phases and m is the number of years on which the sample average is computed to approximate the ensemble average in eq 1 when considering window sizes longer than one year k years in general 5 and s years in the present analyses the parameters of the ordinary distributions are the same for the k years used as calibration sample unlike the application of the mevd to the analysis of daily rainfall zorzetto et al 2016 the use of a yearly estimation window is not considered here due to the potential inaccuracies in the estimation of the parameters when few values are available this is because the autocorrelation in discharge is much larger than in precipitation leading to the need to use an inhibition window to identify independent flood events as previously described this problem is further exacerbated when we stratify the data into different components of the mixture of distributions the use of relatively long data windows either 5 year windows or the whole calibration sample for parameter fitting is thus necessary to make sure that in periods in which multiple enso phases are present there is a sufficient number of flood events in each phase to ensure a robust parameter estimation after evaluating the number of peaks year in stream gages across the conus see supplementary fig 2 we selected 5 years as the minimum window size because a smaller one would have led to very few peaks window in the driest areas especially when enso phases are considered 2 2 fitting procedure and cross validation 2 2 1 fitting procedure gev fitting is performed on annual maxima using l moments hosking 1990 zorzetto et al 2016 found that the cross validation performance of the peak over threshold gev fits is indistinguishable from the performance of maximum likelihood or l moment gev fits on annual maxima hence we only present here results from the application of the latter the parameters of the yearly gamma distributions in the mevd eq 2 are estimated on independent peaks from either 5 year windows or the whole sample via l moments hosking 1990 lp3 fitting is performed on annual maxima using the method of moments griffis and stedinger 2007 low floods have not been removed because they still convey information about the frequency of events 2 2 2 evaluation metrics to identify the possible signature of enso phases in the distributions of ordinary flood peaks we assign each event to one of the three enso phases based on the extended multivariate enso index https www esrl noaa gov psd enso past events html the phases are defined with a monthly time span by means of an index 1 for el niño 1 for la niña and 0 for the neutral phase we then test whether the distributions of ordinary flood peaks for each phase are different from one another using the kolmogorov smirnov test with the bonferroni correction bonferroni 1936 to account for multiple hypotheses testing the three possible combinations among the phases in this case if the distributions of the peak magnitudes belonging to two separate enso phases are not statistically different at the 5 level we combine all discharge peak values from both phases hence we classify each time series in the dataset depending on whether 1 three separate enso phases are distinguishable in the empirical distribution of ordinary flood peaks 2 two separate enso phases are distinguishable i e peaks from two of the phases were merged 3 no enso phases are statistically different from each other in the set of ordinary events we estimate the values of the empirical cumulative frequency associated with observed peak discharge in the test sub sample using the weibull plotting position fk k l 1 where k denotes the kth peak discharge value qk in an ascending order ranking the estimated quantiles corresponding to each value fk are computed using the three ev distributions by solving dist qdist k fk where dist indicates the mev gev and lp3 distributions we use two metrics to evaluate goodness of fit and estimation accuracy 1 we compare estimated quantiles with observed ones through the computation of the skill score ss 1 murphy and winkler 1992 hashino et al 2006 which provides a global metric of estimation accuracy 5 ss q est q obs ρ q est q obs 2 ρ q est q obs σ q est σ q obs 2 μ q est μ q obs σ q obs 2 where ρ q e s t q o b s is the correlation between the estimated values q est and the observations qobs σ q e s t and σ q o b s μ q e s t and μ q o b s represent the standard deviation mean of the observations and estimations respectively the ss accounts for the potential skill i e coefficient of determination as well as conditional and unconditional biases the ss is used both in the context of ordinary values fitting and of extreme values estimation evaluation in the latter case to provide a measure of the estimation of high quantiles we compute the terms in the skill score definition eq 5 only on quantiles with return period tk 1 fk 1 s i e greater than the length of the dataset used for calibration this reflects application needs which target the estimation of extremes with return period much greater than the length of the observational time series available estimation of quantiles with tk s can be performed empirically without the need to assume a specific probability distribution 2 we compute the non dimensional estimation error 6 ε j s t q e s t j s t q o b s j s t q o b s j s t for which we estimate a whole frequency distribution based on the nr 1000 monte carlo realizations the monte carlo experiment allows us to eliminate any non stationarity in the observational records while at the same time it preserves the distribution of the flood peak values and their occurrence over these realizations we finally compute the fractional standard error 7 fse s t 1 n r j 1 n r ε j s t 2 1 2 2 2 3 cross validation we quantify the uncertainty in estimating high quantiles associated with the use of the mevd in its single or multi phase versions and of the gev and lp3 distributions by means of a cross validation procedure involving monte carlo simulations with nr 1000 realizations for each station as follows 1 we randomly reshuffle the observational years on record keeping all the observations in their original year to preserve their yearly frequency distributions and the distribution of the number of events year to generate a realization without any systematic variability 2 we divide the observational sample into two sub samples obtained by randomly selecting s years from the original time series of length ltot this sub sample is used for parameter estimation while data in the remaining l ltot s years are used for testing 3 in every realization we compute both the ss and the fse between the estimated and observed quantiles as described in the section about the estimation metrics 4 the whole procedure above is performed for different calibration sample sizes s 10 20 and 30 years to evaluate how estimation uncertainty varies jointly with return period and calibration sample size 3 results 3 1 ordinary values we start by selecting the most appropriate parametric distribution for ordinary peak discharge values based on the ss computed for yearly maxima the gamma distribution provides the highest ss for most of the observational records analyzed 71 of the sites supplementary fig 3 this is consistent with other studies in the literature e g hann 1977 palynchk and guo 2008 villarini and strong 2014 slater and villarini 2017 hence analyses were performed using the gamma distribution at all conus sites including the minority of sites where it was not the most skillful this choice was made to obtain a method that would be simpler to apply and more homogeneous comparisons across the conus another necessary step for the application of the mevd towards the evaluation of the potential improvements associated with the use of a mixed enso based mevd is to identify whether the distribution of ordinary flow peak values associated with different enso phases are different for most of the stations 3718 or 70 of the total we did not detect statistically different distributions among the peaks occurred under different enso phases at 883 sites about 17 the three phases are all different from one another for the remaining 13 of the stations we find common distributions to be shared by el niño and the neutral phases or by la niña and the neutral phase fig 1a in most of the analyzed cases different enso phases are detectable in ordinary peak discharge values in areas located in the eastern and southern united states which are known to be more strongly affected by enso e g emerton et al 2017 mallakpour and villarini 2017 we found detectable phases also in a group of stations in the u s midwest which are not usually areas that are affected by the effects of enso but other processes might play a role 3 2 extreme values analysis we now turn to the question of evaluating the predictive performance of the mevd gamma formulations and compare the predictive performances of the two mevd approaches for the first time applied to flood frequency analysis with those from the traditionally used gev distribution and the log pearson type iii distribution the optimal mevd gamma formulation compared below to traditional flood frequency analysis approaches is for each station obtained by selecting the window size and the approach single component or multi phase that maximizes the skill score value we will then quantify the potential benefits of including enso phases in extreme flood estimation when comparing the optimal mevd i e the mevd formulation based on the number of phases that yielded the maximum ss value computed on yearly maxima we find that mevd outperforms the gev applied to yearly maxima in 78 of the stations based on the ss metric fig 2a shows the results from a s 10 years calibration period in terms of the relative difference between the ss from the optimal mevd and the ss from the gev distribution divided by the absolute value of the ss for the gev distribution here assumed as a reference regarding the comparison with the log pearson type iii fig 2b shows that the mevd outperforms the lp3 at a large number of stations 86 of the stations the same results being confirmed when looking at the fse computed on the quantile corresponding to the maximum return period i e the length of the test subsample 1 year supplementary fig 4 a calibration sample size of 10 years does not always allow a reliable the calculation of the third moment skewness when estimating lp3 parameters hence leading to a low parameter estimation accuracy based on these multiple comparisons and on the lower performance of lp3 in the following we will focus on further comparative analyses between the predictive performances of the mevd and gev distribution we identify some areas where the performance of the gev distribution is generally higher they are characterized by a small number of independent events and thus of uncorrelated flow peaks year e g the rocky mountains and southern california where the number of independent peaks year is frequently less than 10 see supplementary fig 2 usually in combination with short historical time series the low number of flood events in these areas leads to a limited number of ordinary peaks that can be included in the mevd in addition to yearly maxima moreover previous findings showed that the gev distribution does outperform the mevd when the return period of interest is comparable with the calibration sample size zorzetto et al 2016 in addition to characterize one method s predictive performance globally we are also interested in focusing on the prediction accuracy for high return periods being this the case for most practical applications the return period associated with the maximum value in each test sub series is estimated as tmax ltot s 1 where ltot represents the length of the historical series it is variable among the analyzed stations and ranges between 22 and 92 years with reference to the range of the historical records available here 31 101 years of observations the highest quantile for which the estimation error can be quantified using a calibration sample size of 10 years corresponds to a return period t 31 101 10 22 92 years when we look at fse s 10 years tmax computed for the highest return period from the mevd and gev approaches the mevd outperforms the gev distribution in about 76 of the analyzed stations fig 3 the information provided by fig 3 is complemented for all return periods in fig 4 where the fse is plotted as a function of the ratio between return period and calibration sample size s 10 years for small values of the ratio between the return period t and the calibration sample size s the errors in the estimations computed with the two ev approaches are comparable both in terms of average value and uncertainty when higher values of t s are considered the estimates provided by the traditional gev distribution are less accurate than those provided by the mevd approach which shows a 30 improvement with respect to gev estimates many engineering applications are almost exclusively focused on high return periods i e return periods much larger than the span of observational time series the uncertainty of the gev based estimates shows a steadily increasing trend of the fse with increasing return period while the mevd estimation error stabilizes around a value of about 0 32 for high values of t s this result suggests that when estimating quantiles corresponding to return periods much larger than values that have been observed the estimation errors of the traditional evt approach will become very large and much larger than for the mevd based estimates the results of the fse computed with calibration sample sizes of 20 and 30 years are consistent with those from s 10 years yet limited to smaller ratios of t over s for both the gev and lp3 distributions second and third row in fig 4 the average number of peaks year exhibits a large variation across space from 3 to 31 suggesting that different analysis approaches may be differently effective in dry areas where extremely few flood events are observed and more humid areas where the larger number of events year makes available larger quantities of data also in consideration of the spatial pattern identified in fig 2 it is thus interesting to analyze the possible dependence of the estimation performance associated with different ev approaches with respect to the number of floods year fig 5 shows the fse plotted as a function of t s for two groups of stations representing two end member cases sites with less than 10 events year and sites with more than 17 events year limits are defined in such a way that both groups include about 1000 stations the advantage in the use of the mevd approach instead of the traditional one is limited to higher values of t s when few peaks are selected while it always outperforms the gev distribution when a greater number of peaks is available this is linked to the fact that for the same t s having a small number of peaks is not adding much information to the distribution of maxima like it does when the number of peaks increases however the robustness of the mevd with respect to the gev distribution is confirmed for high ratios of return period over sample size we now focus on answering the second question i e whether it is beneficial to adopt a mixed distribution mevd approach accounting for the different enso phases when estimating extreme flood magnitudes in fig 6 we show the performance of the optimal mevd when the single component approach and the mixed one are compared on the ss basis the performance of the two mevd approaches is presented as the relative difference between the ss from the single component mevd and the one from the mixed approach divided by the absolute value of the ss for the mixed mevd here assumed as a reference we find that including mixtures of distributions does not significantly improve the estimation the number of sites where extremes are best described by a mixed mevd is approximately equal to the number of sites where a single population mevd performs the best furthermore in most cases the difference in skill scores between single population and mixed mevd estimates is negligible this is consistent with the results obtained for rainfall by marra et al 2019 who found that introducing two populations to represent different rainfall generating mechanisms in a relatively arid mediterranean area did not yield improvements in the estimation of extremes moreover whenever the mixed mevd is selected the value of the ss is generally comparable to what obtained using a single mevd the signal that we detected in the ordinary distributions is confirmed by the use of a mixed distribution for the estimation of extremes only along the eastern and south eastern united states which are known to be more strongly affected by enso these conclusions are corroborated by the analyses of the fse values obtained from multi phase and single phase mevd approaches supplementary fig 7 which show negligible improvement in the estimation accuracy when adopting a mixed distribution mevd based on multiple enso phases with a more focused attention on the areas most influenced by enso i e eastern and south eastern us and on those stations in which statistically different distributions were detected for the ordinary peaks we look at the performance of the two mevd approaches in absolute terms to provide concrete indications of the estimation uncertainty at play mevd estimates slightly under estimate high return period quantiles see fig 7 and generally the two approaches perform very similarly consistently with the skill score values shown in fig 6 however in several cases the mixed mevd helps mitigating issues related to under estimations of the quantiles compared to the observations fig 7 4 discussion and conclusions in this paper we applied for the first time an adapted formulation of the metastatistical extreme value distribution mevd to flood peaks observed at more than 5000 usgs stream gages across the continental united states and proposed an alternative procedure to the standard flood frequency analysis the major differences between the proposed and the standard flood frequency approach does not only lie in the distribution chosen to describe flood peaks but also in the use of independent test samples to evaluate predictive performance within a monte carlo cross validation approach goodness of fit evaluations in fact merely consist in the evaluation of the suitability of a distribution to describe a record of observations without quantifying the skill of a method when presented with yet unoccurred extremes a cross validation approach instead allowed us to properly quantify the predictive uncertainty of mevd based estimates both comparatively with respect to traditional approaches and in absolute terms furthermore we leveraged the ability of the mevd of making use of all available independent observations to explore the potential benefits of a mixed distribution approach in which different ordinary value distributions are used to describe different flood generation mechanisms processes with specific reference to enso phases two fundamental steps were needed to develop a methodology tailored for flood frequency analysis first it was necessary to identify a robust method for the automatic and objective selection of independent flood peaks unlike previous mevd applications focused mainly on rainfall at the daily and sub daily scale the correlation in streamflow records is very significant over large range of time scales this circumstance required discarding potentially correlated flood peaks and as a consequence the reduction of the number of events year the proposed approach performs analyses on data windows with length 5 years or higher to compensate for this effect a particularly strong constraint when stratifying observed peaks based on different enso phases second a preliminary screening of candidate ordinary value distributions was necessary mevd analyses of rainfall are based on the weibull distribution wilson and toumi 2005 marani and ignaccolo 2015 which was not found to be ideal in the case of flood peaks here we compared the performance of three candidate distributions eventually selecting the gamma distribution as the best statistical model of flood peaks at the largest number of stations across the conus this choice leads to a simpler mevd gamma formulation though applications seeking to optimize performance even further may be based on an at site selection of the optimal ordinary value distribution the mevd gamma approach outperforms the traditional gev analysis of extreme flood peaks at about 76 of the stations especially in the presence of records that are short with respect to the return period of interest in fact the mevd displays the smallest fractional standard error for small calibration sample sizes and high return periods the case of greatest practical interest we found similar results when comparing the mevd with the widely applied log pearson type iii mevd based estimates of extreme peak flows outperform lp3 estimates at 86 of the stations across the conus when the size of the calibration sample is increased from s 10 years to s 20 and to s 30 years the three distributions considered here show comparable results for low values of t s the demonstrated superior predictive skill of the mevd gamma approach to flood frequency analysis becomes especially relevant for engineering applications where the estimation of exceptionally high design events is often required in the basis of short observational time series in a minority of cases the traditional gev distribution shows a reduced estimation uncertainty in the presence of a low number of peaks year and of short time series in these stations little additional information is available for the mevd to exploit additionally to yearly maxima considering enso as a factor potentially identifying different populations of flood peaks we found that the estimation of high return period flows does not necessarily improve even though the enso signature in the distributions of ordinary flood peaks was identified as statistically significant at several stream gages we conclude that either the uncertainty intrinsic to extreme value estimation overwhelms the information contributed by enso phases or that just one of the enso phases detected in the distribution of the ordinary events effectively dominates the shape of the distributional tail and of the extreme values a final comment is in order even though the conclusion regarding the information that can be extracted from enso phases about extreme streamflows is negative the ability of accounting for mixtures of distributions in the flood peak mevd formulation still has practical potential in fact several physical flood drivers can be identified e g snowmelt rain on snow atmospheric rivers whose role can be studied using the proposed approach the introduction and formalization of a mixed distribution mevd for flood frequency analysis thus remains important because it lends itself to applications to a variety of contexts in which different physical drivers can be defined such as the north atlantic oscillation in western europe marani and zanetti 2015 or the arctic oscillation in north eastern europe bartolini et al 2009 regarding enso its detection in the distributions of the ordinary peaks is also still valuable the detection or prediction of the occurrence of an enso phase justifies the use of mevd parameters conditional to the known occurrence of that specific enso phase with potential improvements over flood frequency analyses neglecting this information credit authorship contribution statement arianna miniussi visualization formal analysis writing original draft writing review editing marco marani visualization writing original draft writing review editing gabriele villarini visualization writing original draft writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments a m acknowledges the foundation cassa di risparmio di padova e rovigo for funding her phd under the project extreme hydrologic events and the foundation aldo gini for the grant in support of the period abroad at iihr hydroscience engineering university of iowa m m acknowledges support by the venice water authority venice 2021 project line 1 3 modellazione numerica integrata del sistema bacino scolante laguna mare g v acknowledges financial support by the usace institute for water resources and the national science foundation under career grant ags 1349827 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 103498 appendix supplementary materials image application 1 
530,this study analyzes daily mean streamflow records from 5 311 u s geological survey stream gages in the continental united states and develops a metastatistical extreme value distribution mevd tailored for flood frequency analysis we compare the new tool with the generalized extreme value gev and log pearson type iii lp3 distributions and investigate the role of el niño southern oscillation enso in the generation of floods hence we formulate the mevd in terms of mixture of distributions to describe the occurrence of flood peaks generated under different enso phases we find that the mevd outperforms gev and lp3 distributions respectively in about 76 and 86 of the stations with a significant improvement in the accuracy of quantiles corresponding to return periods much larger than the calibration sample size the enso signature detected in the distributions of the daily peak flows does not necessarily improve the estimation of high return period flow values 1 introduction globally in the period 1998 2017 floods have been the most frequent disaster 43 4 of the natural disasters and have caused more than 140 000 deaths representing 11 of the fatalities due to natural disasters of all types wallemacq and house 2018 within this global context during the 20th century floods in the united states were the number one natural disaster in terms of the number of lives lost and property damage perry 2000 the costliest miller et al 2008 and affected the largest number of people stromberg 2007 they are also the second weather related hazard in terms of fatalities in the united states with 4586 reported deaths between 1959 and 2005 mainly due to flash floods caused by heavy precipitation ashley and ashley 2008 reliable flood frequency estimation methods are the basis to devise and implement strategies for the mitigation of these societal and economic impacts with applications in a number of fields from the design of hydraulic structures to environmental management and planning to flood insurance a key concept is the design flood peak value typically set in national regulations by specifying an average recurrence interval or return period t associated with a probability of being exceeded in each year equal to p 1 t the t year flood in turn is estimated based on the analysis of past floods requiring the selection of a probability distribution to perform this inference using a sample with size s t benson 1962 flood frequency analyses are commonly performed as a part of engineering and planning projects but they too often represent a mere statistical fitting exercise and do not attempt to incorporate a representation of the underlying physical processes as noted by klemeš 1988 1993 the standard approach of extreme event probability estimation is moving towards a higher mathematical abstraction renouncing any leveraging or understanding of the different flood generating mechanisms at play the hydrological process information in the observations is thus often neglected and the selection of an optimal statistical model through a goodness of fit metric remains the main focus of these types of analyses the approach proposed here does not belong to the standard methodology on which flood frequency analyses are usually based three points of departure can indeed be highlighted first it is not a mere fit of a distribution on annual maxima but by dealing with all the observations see section data and methodology for further details the approach described here is closer to the raw observations second the approach we propose for flood frequency analysis is not a mere goodness of fit exercise which is what klemeš 1988 1993 was stigmatizing one of the important points highlighted here is that we need to address predictive uncertainty through a cross validation procedure rather than just use unexplained variance as a measure of uncertainty third but not less important the proposed approach does not just consider peak flows as random numbers but attempts to base the statistical modeling on physical drivers the u s federal guidelines themselves bulletin 17 b iacwd 1982 and its updated version 17 c england et al 2018 recognize that the assumption under which stream gage records are generated by one single flood generating mechanism may not always be realistic they highlight the need to understand and more accurately identify these physical mechanisms and list the identification and treatment of mixed distributions to represent their diversity as a research and application priority even though the idea of a more process driven flood frequency analysis is not necessarily new e g hirschboeck 1987 there has been a renewed interest in recent years in process based formulations of extreme flood distributions e g alila and mtiraoui 2002 smith et al 2011 villarini and slater 2017 barth et al 2019 flood generating processes can quite naturally be analyzed using mixed distributions e g alila and mtiraoui 2002 but the determination of which flood peaks result from the different processes and of when the use of mixed distributions is beneficial remains an open problem e g villarini and slater 2017 there are several different hydrological mechanisms that can drive the occurrence of flood events including snowmelt frontal systems local convective processes monsoons and intense tropical cyclones see villarini 2016 zhang et al 2017 slater et al 2015 compared hydrologic and geomorphic drivers in flood hazard while berghuijs et al 2016 analyzed the dominant flood generating mechanisms across the united states barth et al 2017 2019 investigated the role of atmospheric rivers in the generation of flood peaks across the western united states and suggested a weighted mixed population approach to perform a process driven flood frequency analysis to reflect the differences in flood agents within the context of mixed distributions this study formulates a novel flood frequency distribution and uses it to investigate the role of el niño southern oscillation enso in the generation of floods and the detectability of its signature in observed records enso is a major mode of variability of the coupled atmosphere ocean system associated with episodes of above normal el niño and below normal la niña sea surface temperature in the tropical pacific ocean with impacts on seasonal winds rainfall and temperature across the globe we propose here the use of a novel approach the metastatistical extreme value distribution mevd which can naturally incorporate mixed distributions to represent flood magnitudes generated by different mechanisms the mevd has been introduced by marani and ignaccolo 2015 and has been applied mostly to rainfall zorzetto et al 2016 marra et al 2018 zorzetto and marani 2019 for which it was shown to provide significantly smaller estimation uncertainty when compared to traditional approaches especially when considering return periods that are larger than the sample size used for distribution estimation currently the mevd has yet to be applied to flood magnitudes and flood frequency analysis here we provide the first such application and ask the following relevant questions does the mevd outperform the traditional generalized extreme value gev distribution in flood frequency analysis does the incorporation of mixed probability distributions representing different types of flood events associated with different enso phases improve the estimation of event magnitudes with high return periods to answer these questions we apply the mevd approach to daily records from stream gage stations across the continental united states conus examining the role played by mixtures of distributions associated with different enso phases results are compared and contrasted against those from the gev distribution providing qualitative and quantitative evaluations of their relative predictive performance the gev distribution is the natural reference benchmark for this comparison as it stems from the widely applied traditional extreme value theory smith 1987 katz et al 2002 however other methods are widely applied in the flood frequency analysis practice we thus extended the comparison of cross validation performances to the log pearson type iii lp3 the distribution extensively used in flood frequency analyses in the us uswrc 1976 the paper is organized as follows the data and methodology used in the study are described in section 2 followed by a section 3 detailing the results of the analyses section 4 summarizes the main points of this study and concludes the paper 2 data and methodology we analyzed daily records from 5311 u s geological survey usgs stream gages across the continental united states fig 1 panel a we focus on water years defined to run between october 1 and september 30 and select only sites where at least 30 complete i e with more than 330 daily observations year years of observation exist and where no statistically significant trends are found at the 5 level based on the mann kendall test mann 1945 kendall 1975 the historical time series selected cover the period 1916 2017 with record lengths between 31 and 101 years fig 1 panels b and c consult supplementary fig 1 for an overview of the spatial distribution of the lengths of the historical time series flood frequency analysis requires the identification of independent events here we identify the largest flood peaks within blocks of length equal to t 10 days log a where a is the drainage area in square miles lang et al 1999 additionally we discard the smallest discharge peak within any pair of consecutive peaks if the minimum flow between them does not drop below a threshold equal to 75 of the lower of the two water resources council uswrc 1976 this additional condition is necessary to eliminate secondary peaks occurring during recession periods of previous floods the entire set of peak discharge values resulting from this selection process of uncorrelated events is here called the set of ordinary events to denote that it contains all the independent events that have occurred in the record irrespective of their magnitudes 2 1 mevd approach the metastatistical extreme value distribution originally introduced by marani and ignaccolo 2015 is explicitly formulated on the basis of the probability distribution s of the ordinary values from which the distribution of extremes annual maxima is then derived hence the expression of the mevd describing the extreme events is identified by estimating its parameters using the entire set of observed ordinary events this is quite different from the assumptions at the basis of the traditional extreme value theory evt which focuses on fitting a distribution to the annual maxima or to relatively few values above a high threshold the mevd approach treats as realizations of stochastic variables both the parameters of the distributions describing the ordinary events in each year f x θ where θ is the parameter vector and the number n of yearly event occurrences under these premises the mevd of yearly maxima can be defined as 1 ζ x n 1 ω θ f x θ n g n θ d θ where g n θ is the joint probability distribution of the random variables n and θ discrete in n and continuous in θ and ω θ is the population of the parameters values the ensemble average can be approximated by the sample average computed over all the years of the historical series m becoming 2 ζ x 1 m j 1 m f x θ j n j where f x θ j is the cumulative distribution of ordinary values and nj is the number of events in year j here we apply the mevd to peak discharges and modify this approach to account for ordinary values belonging to different populations corresponding in the present case to different enso phases the cumulative distribution function ζ x of the mixed mevd can be written as follows 3 ζ x 1 m j 1 m p 1 n p h f p x θ j n j p where nph is the number of phases that induce statistically different distributions of the ordinary events and should therefore be considered separately fp is the yearly or time window when the low number of events year requires parameter estimation to be performed of multi year windows cumulative distribution of the ordinary values in phase p nj p is the original yearly number of the peaks in phase p and year j m is the number of years for which observations are available eq 3 reduces to the original formulation in marani and ignaccolo 2015 when only one phase is present the mixed mevd formulation in eq 3 is the same as the approach proposed in marra et al 2019 here instead of using the further simplification smev marra et al 2019 that removes the inter annual variability in the statistics of the ordinary events i e there is no dependence on j in eq 2 we preserve the time variability of the distributions the first step in the application of the mevd approach is identifying a suitable parametric distribution to represent the ordinary events we evaluate three candidate distributions for the f x θ j in eq 2 weibull generalized pareto and gamma distributions we select the most suitable distribution on the basis of the skill score see section on the evaluation metrics for its definition comparing the mevd estimated quantiles to the observed maxima in the present analyses the gamma distribution was the best performing one see section 3 for further details because the average number of flood events in a year is small for many stations e g in 1243 of the 5311 analyzed stations the average number of peaks year is smaller than or equal to 10 see supplementary fig 2 we explore estimating the parameters of the distribution using either five year windows or the entire sample eq 3 can be hence expressed in terms of the windows used for parameter estimation 4 ζ x 1 m j 1 m p 1 n p h f p x θ k j n j p where k j is the index value identifying the window containing year j θ k j is the parameter vector in the k th window p is the number of statistically different phases and m is the number of years on which the sample average is computed to approximate the ensemble average in eq 1 when considering window sizes longer than one year k years in general 5 and s years in the present analyses the parameters of the ordinary distributions are the same for the k years used as calibration sample unlike the application of the mevd to the analysis of daily rainfall zorzetto et al 2016 the use of a yearly estimation window is not considered here due to the potential inaccuracies in the estimation of the parameters when few values are available this is because the autocorrelation in discharge is much larger than in precipitation leading to the need to use an inhibition window to identify independent flood events as previously described this problem is further exacerbated when we stratify the data into different components of the mixture of distributions the use of relatively long data windows either 5 year windows or the whole calibration sample for parameter fitting is thus necessary to make sure that in periods in which multiple enso phases are present there is a sufficient number of flood events in each phase to ensure a robust parameter estimation after evaluating the number of peaks year in stream gages across the conus see supplementary fig 2 we selected 5 years as the minimum window size because a smaller one would have led to very few peaks window in the driest areas especially when enso phases are considered 2 2 fitting procedure and cross validation 2 2 1 fitting procedure gev fitting is performed on annual maxima using l moments hosking 1990 zorzetto et al 2016 found that the cross validation performance of the peak over threshold gev fits is indistinguishable from the performance of maximum likelihood or l moment gev fits on annual maxima hence we only present here results from the application of the latter the parameters of the yearly gamma distributions in the mevd eq 2 are estimated on independent peaks from either 5 year windows or the whole sample via l moments hosking 1990 lp3 fitting is performed on annual maxima using the method of moments griffis and stedinger 2007 low floods have not been removed because they still convey information about the frequency of events 2 2 2 evaluation metrics to identify the possible signature of enso phases in the distributions of ordinary flood peaks we assign each event to one of the three enso phases based on the extended multivariate enso index https www esrl noaa gov psd enso past events html the phases are defined with a monthly time span by means of an index 1 for el niño 1 for la niña and 0 for the neutral phase we then test whether the distributions of ordinary flood peaks for each phase are different from one another using the kolmogorov smirnov test with the bonferroni correction bonferroni 1936 to account for multiple hypotheses testing the three possible combinations among the phases in this case if the distributions of the peak magnitudes belonging to two separate enso phases are not statistically different at the 5 level we combine all discharge peak values from both phases hence we classify each time series in the dataset depending on whether 1 three separate enso phases are distinguishable in the empirical distribution of ordinary flood peaks 2 two separate enso phases are distinguishable i e peaks from two of the phases were merged 3 no enso phases are statistically different from each other in the set of ordinary events we estimate the values of the empirical cumulative frequency associated with observed peak discharge in the test sub sample using the weibull plotting position fk k l 1 where k denotes the kth peak discharge value qk in an ascending order ranking the estimated quantiles corresponding to each value fk are computed using the three ev distributions by solving dist qdist k fk where dist indicates the mev gev and lp3 distributions we use two metrics to evaluate goodness of fit and estimation accuracy 1 we compare estimated quantiles with observed ones through the computation of the skill score ss 1 murphy and winkler 1992 hashino et al 2006 which provides a global metric of estimation accuracy 5 ss q est q obs ρ q est q obs 2 ρ q est q obs σ q est σ q obs 2 μ q est μ q obs σ q obs 2 where ρ q e s t q o b s is the correlation between the estimated values q est and the observations qobs σ q e s t and σ q o b s μ q e s t and μ q o b s represent the standard deviation mean of the observations and estimations respectively the ss accounts for the potential skill i e coefficient of determination as well as conditional and unconditional biases the ss is used both in the context of ordinary values fitting and of extreme values estimation evaluation in the latter case to provide a measure of the estimation of high quantiles we compute the terms in the skill score definition eq 5 only on quantiles with return period tk 1 fk 1 s i e greater than the length of the dataset used for calibration this reflects application needs which target the estimation of extremes with return period much greater than the length of the observational time series available estimation of quantiles with tk s can be performed empirically without the need to assume a specific probability distribution 2 we compute the non dimensional estimation error 6 ε j s t q e s t j s t q o b s j s t q o b s j s t for which we estimate a whole frequency distribution based on the nr 1000 monte carlo realizations the monte carlo experiment allows us to eliminate any non stationarity in the observational records while at the same time it preserves the distribution of the flood peak values and their occurrence over these realizations we finally compute the fractional standard error 7 fse s t 1 n r j 1 n r ε j s t 2 1 2 2 2 3 cross validation we quantify the uncertainty in estimating high quantiles associated with the use of the mevd in its single or multi phase versions and of the gev and lp3 distributions by means of a cross validation procedure involving monte carlo simulations with nr 1000 realizations for each station as follows 1 we randomly reshuffle the observational years on record keeping all the observations in their original year to preserve their yearly frequency distributions and the distribution of the number of events year to generate a realization without any systematic variability 2 we divide the observational sample into two sub samples obtained by randomly selecting s years from the original time series of length ltot this sub sample is used for parameter estimation while data in the remaining l ltot s years are used for testing 3 in every realization we compute both the ss and the fse between the estimated and observed quantiles as described in the section about the estimation metrics 4 the whole procedure above is performed for different calibration sample sizes s 10 20 and 30 years to evaluate how estimation uncertainty varies jointly with return period and calibration sample size 3 results 3 1 ordinary values we start by selecting the most appropriate parametric distribution for ordinary peak discharge values based on the ss computed for yearly maxima the gamma distribution provides the highest ss for most of the observational records analyzed 71 of the sites supplementary fig 3 this is consistent with other studies in the literature e g hann 1977 palynchk and guo 2008 villarini and strong 2014 slater and villarini 2017 hence analyses were performed using the gamma distribution at all conus sites including the minority of sites where it was not the most skillful this choice was made to obtain a method that would be simpler to apply and more homogeneous comparisons across the conus another necessary step for the application of the mevd towards the evaluation of the potential improvements associated with the use of a mixed enso based mevd is to identify whether the distribution of ordinary flow peak values associated with different enso phases are different for most of the stations 3718 or 70 of the total we did not detect statistically different distributions among the peaks occurred under different enso phases at 883 sites about 17 the three phases are all different from one another for the remaining 13 of the stations we find common distributions to be shared by el niño and the neutral phases or by la niña and the neutral phase fig 1a in most of the analyzed cases different enso phases are detectable in ordinary peak discharge values in areas located in the eastern and southern united states which are known to be more strongly affected by enso e g emerton et al 2017 mallakpour and villarini 2017 we found detectable phases also in a group of stations in the u s midwest which are not usually areas that are affected by the effects of enso but other processes might play a role 3 2 extreme values analysis we now turn to the question of evaluating the predictive performance of the mevd gamma formulations and compare the predictive performances of the two mevd approaches for the first time applied to flood frequency analysis with those from the traditionally used gev distribution and the log pearson type iii distribution the optimal mevd gamma formulation compared below to traditional flood frequency analysis approaches is for each station obtained by selecting the window size and the approach single component or multi phase that maximizes the skill score value we will then quantify the potential benefits of including enso phases in extreme flood estimation when comparing the optimal mevd i e the mevd formulation based on the number of phases that yielded the maximum ss value computed on yearly maxima we find that mevd outperforms the gev applied to yearly maxima in 78 of the stations based on the ss metric fig 2a shows the results from a s 10 years calibration period in terms of the relative difference between the ss from the optimal mevd and the ss from the gev distribution divided by the absolute value of the ss for the gev distribution here assumed as a reference regarding the comparison with the log pearson type iii fig 2b shows that the mevd outperforms the lp3 at a large number of stations 86 of the stations the same results being confirmed when looking at the fse computed on the quantile corresponding to the maximum return period i e the length of the test subsample 1 year supplementary fig 4 a calibration sample size of 10 years does not always allow a reliable the calculation of the third moment skewness when estimating lp3 parameters hence leading to a low parameter estimation accuracy based on these multiple comparisons and on the lower performance of lp3 in the following we will focus on further comparative analyses between the predictive performances of the mevd and gev distribution we identify some areas where the performance of the gev distribution is generally higher they are characterized by a small number of independent events and thus of uncorrelated flow peaks year e g the rocky mountains and southern california where the number of independent peaks year is frequently less than 10 see supplementary fig 2 usually in combination with short historical time series the low number of flood events in these areas leads to a limited number of ordinary peaks that can be included in the mevd in addition to yearly maxima moreover previous findings showed that the gev distribution does outperform the mevd when the return period of interest is comparable with the calibration sample size zorzetto et al 2016 in addition to characterize one method s predictive performance globally we are also interested in focusing on the prediction accuracy for high return periods being this the case for most practical applications the return period associated with the maximum value in each test sub series is estimated as tmax ltot s 1 where ltot represents the length of the historical series it is variable among the analyzed stations and ranges between 22 and 92 years with reference to the range of the historical records available here 31 101 years of observations the highest quantile for which the estimation error can be quantified using a calibration sample size of 10 years corresponds to a return period t 31 101 10 22 92 years when we look at fse s 10 years tmax computed for the highest return period from the mevd and gev approaches the mevd outperforms the gev distribution in about 76 of the analyzed stations fig 3 the information provided by fig 3 is complemented for all return periods in fig 4 where the fse is plotted as a function of the ratio between return period and calibration sample size s 10 years for small values of the ratio between the return period t and the calibration sample size s the errors in the estimations computed with the two ev approaches are comparable both in terms of average value and uncertainty when higher values of t s are considered the estimates provided by the traditional gev distribution are less accurate than those provided by the mevd approach which shows a 30 improvement with respect to gev estimates many engineering applications are almost exclusively focused on high return periods i e return periods much larger than the span of observational time series the uncertainty of the gev based estimates shows a steadily increasing trend of the fse with increasing return period while the mevd estimation error stabilizes around a value of about 0 32 for high values of t s this result suggests that when estimating quantiles corresponding to return periods much larger than values that have been observed the estimation errors of the traditional evt approach will become very large and much larger than for the mevd based estimates the results of the fse computed with calibration sample sizes of 20 and 30 years are consistent with those from s 10 years yet limited to smaller ratios of t over s for both the gev and lp3 distributions second and third row in fig 4 the average number of peaks year exhibits a large variation across space from 3 to 31 suggesting that different analysis approaches may be differently effective in dry areas where extremely few flood events are observed and more humid areas where the larger number of events year makes available larger quantities of data also in consideration of the spatial pattern identified in fig 2 it is thus interesting to analyze the possible dependence of the estimation performance associated with different ev approaches with respect to the number of floods year fig 5 shows the fse plotted as a function of t s for two groups of stations representing two end member cases sites with less than 10 events year and sites with more than 17 events year limits are defined in such a way that both groups include about 1000 stations the advantage in the use of the mevd approach instead of the traditional one is limited to higher values of t s when few peaks are selected while it always outperforms the gev distribution when a greater number of peaks is available this is linked to the fact that for the same t s having a small number of peaks is not adding much information to the distribution of maxima like it does when the number of peaks increases however the robustness of the mevd with respect to the gev distribution is confirmed for high ratios of return period over sample size we now focus on answering the second question i e whether it is beneficial to adopt a mixed distribution mevd approach accounting for the different enso phases when estimating extreme flood magnitudes in fig 6 we show the performance of the optimal mevd when the single component approach and the mixed one are compared on the ss basis the performance of the two mevd approaches is presented as the relative difference between the ss from the single component mevd and the one from the mixed approach divided by the absolute value of the ss for the mixed mevd here assumed as a reference we find that including mixtures of distributions does not significantly improve the estimation the number of sites where extremes are best described by a mixed mevd is approximately equal to the number of sites where a single population mevd performs the best furthermore in most cases the difference in skill scores between single population and mixed mevd estimates is negligible this is consistent with the results obtained for rainfall by marra et al 2019 who found that introducing two populations to represent different rainfall generating mechanisms in a relatively arid mediterranean area did not yield improvements in the estimation of extremes moreover whenever the mixed mevd is selected the value of the ss is generally comparable to what obtained using a single mevd the signal that we detected in the ordinary distributions is confirmed by the use of a mixed distribution for the estimation of extremes only along the eastern and south eastern united states which are known to be more strongly affected by enso these conclusions are corroborated by the analyses of the fse values obtained from multi phase and single phase mevd approaches supplementary fig 7 which show negligible improvement in the estimation accuracy when adopting a mixed distribution mevd based on multiple enso phases with a more focused attention on the areas most influenced by enso i e eastern and south eastern us and on those stations in which statistically different distributions were detected for the ordinary peaks we look at the performance of the two mevd approaches in absolute terms to provide concrete indications of the estimation uncertainty at play mevd estimates slightly under estimate high return period quantiles see fig 7 and generally the two approaches perform very similarly consistently with the skill score values shown in fig 6 however in several cases the mixed mevd helps mitigating issues related to under estimations of the quantiles compared to the observations fig 7 4 discussion and conclusions in this paper we applied for the first time an adapted formulation of the metastatistical extreme value distribution mevd to flood peaks observed at more than 5000 usgs stream gages across the continental united states and proposed an alternative procedure to the standard flood frequency analysis the major differences between the proposed and the standard flood frequency approach does not only lie in the distribution chosen to describe flood peaks but also in the use of independent test samples to evaluate predictive performance within a monte carlo cross validation approach goodness of fit evaluations in fact merely consist in the evaluation of the suitability of a distribution to describe a record of observations without quantifying the skill of a method when presented with yet unoccurred extremes a cross validation approach instead allowed us to properly quantify the predictive uncertainty of mevd based estimates both comparatively with respect to traditional approaches and in absolute terms furthermore we leveraged the ability of the mevd of making use of all available independent observations to explore the potential benefits of a mixed distribution approach in which different ordinary value distributions are used to describe different flood generation mechanisms processes with specific reference to enso phases two fundamental steps were needed to develop a methodology tailored for flood frequency analysis first it was necessary to identify a robust method for the automatic and objective selection of independent flood peaks unlike previous mevd applications focused mainly on rainfall at the daily and sub daily scale the correlation in streamflow records is very significant over large range of time scales this circumstance required discarding potentially correlated flood peaks and as a consequence the reduction of the number of events year the proposed approach performs analyses on data windows with length 5 years or higher to compensate for this effect a particularly strong constraint when stratifying observed peaks based on different enso phases second a preliminary screening of candidate ordinary value distributions was necessary mevd analyses of rainfall are based on the weibull distribution wilson and toumi 2005 marani and ignaccolo 2015 which was not found to be ideal in the case of flood peaks here we compared the performance of three candidate distributions eventually selecting the gamma distribution as the best statistical model of flood peaks at the largest number of stations across the conus this choice leads to a simpler mevd gamma formulation though applications seeking to optimize performance even further may be based on an at site selection of the optimal ordinary value distribution the mevd gamma approach outperforms the traditional gev analysis of extreme flood peaks at about 76 of the stations especially in the presence of records that are short with respect to the return period of interest in fact the mevd displays the smallest fractional standard error for small calibration sample sizes and high return periods the case of greatest practical interest we found similar results when comparing the mevd with the widely applied log pearson type iii mevd based estimates of extreme peak flows outperform lp3 estimates at 86 of the stations across the conus when the size of the calibration sample is increased from s 10 years to s 20 and to s 30 years the three distributions considered here show comparable results for low values of t s the demonstrated superior predictive skill of the mevd gamma approach to flood frequency analysis becomes especially relevant for engineering applications where the estimation of exceptionally high design events is often required in the basis of short observational time series in a minority of cases the traditional gev distribution shows a reduced estimation uncertainty in the presence of a low number of peaks year and of short time series in these stations little additional information is available for the mevd to exploit additionally to yearly maxima considering enso as a factor potentially identifying different populations of flood peaks we found that the estimation of high return period flows does not necessarily improve even though the enso signature in the distributions of ordinary flood peaks was identified as statistically significant at several stream gages we conclude that either the uncertainty intrinsic to extreme value estimation overwhelms the information contributed by enso phases or that just one of the enso phases detected in the distribution of the ordinary events effectively dominates the shape of the distributional tail and of the extreme values a final comment is in order even though the conclusion regarding the information that can be extracted from enso phases about extreme streamflows is negative the ability of accounting for mixtures of distributions in the flood peak mevd formulation still has practical potential in fact several physical flood drivers can be identified e g snowmelt rain on snow atmospheric rivers whose role can be studied using the proposed approach the introduction and formalization of a mixed distribution mevd for flood frequency analysis thus remains important because it lends itself to applications to a variety of contexts in which different physical drivers can be defined such as the north atlantic oscillation in western europe marani and zanetti 2015 or the arctic oscillation in north eastern europe bartolini et al 2009 regarding enso its detection in the distributions of the ordinary peaks is also still valuable the detection or prediction of the occurrence of an enso phase justifies the use of mevd parameters conditional to the known occurrence of that specific enso phase with potential improvements over flood frequency analyses neglecting this information credit authorship contribution statement arianna miniussi visualization formal analysis writing original draft writing review editing marco marani visualization writing original draft writing review editing gabriele villarini visualization writing original draft writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments a m acknowledges the foundation cassa di risparmio di padova e rovigo for funding her phd under the project extreme hydrologic events and the foundation aldo gini for the grant in support of the period abroad at iihr hydroscience engineering university of iowa m m acknowledges support by the venice water authority venice 2021 project line 1 3 modellazione numerica integrata del sistema bacino scolante laguna mare g v acknowledges financial support by the usace institute for water resources and the national science foundation under career grant ags 1349827 supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 103498 appendix supplementary materials image application 1 
531,in sparse fracture systems flow and transport patterns are often dominated by main flowpaths and characterized by a strong complexity induced by the fractures surface roughness hierarchical arrangement in networks and from the interaction of the fractures with the surrounding rocks the accurate characterization of the hydraulic properties and connectivity of major fracture zones is essential to model flow solute and heat transport and fluid pressure propagation in fractured media in this study we present a novel deterministic inverse modeling method for imaging the connectivity and spatial variability of transmissivity and storativity of a network of fractures the method is based on a numerical model that simulates fluid flow in a simple three dimensional 3 d discrete fracture network dfn with a fixed fracture network structure the forward model is coupled to an inverse algorithm to match observed pressure transients obtained from sequential hydraulic cross hole tests this method has been successfully tested for constant rate injection tests carried out at the grimsel test site gts in switzerland cross hole injection tests were conducted in a tomographic configuration with hydraulic responses monitored at four observation intervals at various depths in two different boreholes a discrete fracture network approach with a simple parametrization is developed to estimate log transformed transmissivity t and storativity s values of hydraulically active fractures between boreholes by inverting the pressure transients under the hypothesis that t and s are independent variables we identified several permeable fractures and their connectivity without attempting to represent explicitly the true fracture network geometrical properties length orientation dip focusing instead on the calibration of a simple model capturing the observed pressure main behavior the identified inter borehole connectivity structure agrees well with independent information including additional data from a cross borehole step rate injection and hydrogeophysical data hence the proposed tomography approach appears to be a promising approach for characterizing the connectivity structure and hydraulic properties of the main flowpaths in sparsely fractured rock keywords fractured media inverse modelling discrete fractured network hydraulic tomography fracture connectivity 1 introduction knowledge about the distribution of hydraulic properties in fractured rocks is of particular importance for water resources management geothermal energy production oil and gas industry nuclear waste disposal or co2 sequestration numerous field observations gave evidence that flow transport and pressure propagation in fractured media are controlled by strongly heterogeneous flowpath distributions and often dominated by a limited number of flow channels e g abelin et al 1991a 1991b read et al 2013 roques et al 2016 tsang and neretnieks 1998 the accurate characterization of the hydraulic properties of natural fracture networks including the hydraulic conductivity k and specific storage ss and their connectivity is therefore of primary importance however the cost intensive nature of data collection and spatial scarcity of data generally available from boreholes imply that the information content of the measured data is rather limited compared to the complexity of fractured networks this strongly restricts our ability to model and forecast the aforementioned environmental processes and industrial applications over the last decades a broad range of new experimental techniques supported by different interpretation frameworks underpinned by different conceptual models has emerged to improve the characterization of heterogeneous flowpath in fractured aquifers this includes i borehole flow measurements le borgne et al 2007 klepikova et al 2013 and periodic hydraulic tests cardiff et al 2013a guiltinan and becker 2015 to characterize the hydraulic properties of fractures and fractured rock mass ii fiber optic borehole temperature measurements combined with heating experiments read et al 2013 bense et al 2016 to investigate ground water flow and heat transport iii ground penetrating radar gpr measurements to image tracer displacement within the fracture network dorn et al 2012 shakas et al 2016 and iv multi scale tracer transport experiments to understand the emergence of anomalous transport in fractures de la bernardie et al 2018 guihéneuf et al 2017 kang et al 2015 klepikova et al 2016 however despite the technical efforts and cost intensive field implementation hydraulic packer tests and especially their application under tomographic configuration are recognized to provide the most detailed and reliable information about spatial heterogeneity of the hydraulic properties of the aquifers illman et al 2009 zha et al 2015 at its core hydraulic tomography ht aims to jointly analyse multiple sets of cross borehole packer test data integrating thereby more information contained in the measurements compared to a classical testing configuration butler jr et al 1999 yeh and liu 2000 originally proposed for imaging of hydraulic heterogeneity in porous media liu et al 2002 li et al 2008 cardiff et al 2013a jiménez et al 2013 the development and validation of ht in fractured media as a proven method remains at an early stage a strong point of debate for instance is the choice of appropriate inverse models to interpret hydraulic tomography data sets in fractured media the majority of existing interpretation frameworks conceptualize fractured rock as an equivalent porous media this includes travel time based inversion scheme brauchler et al 2003 trajectory based traveltime approach he et al 2006 sequential successive linear estimator hao et al 2008 illman et al 2009 yeh and liu 2000 the pilot points method lavenue and de marsily 2001 vesselinov et al 2001 castagna et al 2011 and a stochastic inversion approach constrained by a priori inter borehole connection information day lewis et al 2000 wang et al 2017 to avoid resolving extreme spatial contrasts in hydraulic properties from fractures and the surrounding rock these methods assign instead average hydraulic properties across bulk regions of the aquifer this often results in smooth parameter fields that do not reflect the discrete nature of spatial variability in hydraulic properties of fractured rocks recent laboratory tests on fractured rock samples demonstrated that the resolution of the estimated fracture pattern could be significantly improved by increasing the number of pumping observation intervals sharmeen et al 2012 illman 2014 in the field however the monitoring interval density restricted by the limited number of observation boreholes is generally sparse making it impossible to capture the discrete nature of flow in fractured geologic systems in contrast recent studies have proposed new ht interpretation frameworks based on discrete fracture network dfn models le goc et al 2010 klepikova et al 2013 2014 borghi et al 2016 somogyvari et al 2017 fischer et al 2018b the main challenge associated with this approach is to constrain the geometrical complexity of the model fracture internal heterogeneity fracture location orientation length density and fracture interconnections based on the information available from field measurements in order to avoid under and over parameterization e g borghi et al 2016 such complex task imposes some conceptual simplifications or requires that additional data such as hydrogeophysical measurements or tracer test data be considered le goc et al 2010 for example proposed an algorithm to identify both geometrical and hydraulic characteristics of a limited number of most probable highly localized flow channels in fractured media the methodology was established on 2 d synthetic flow channel structures using synthetic steady state hydraulic heads supplemented by geometrical data i e the distance between a well and the nearest flow channel furthermore based on a simplified fracture network model which did not take the fracture geometry explicitly into account a framework was proposed for inverting borehole flow measurements klepikova et al 2013 and borehole temperature profiles klepikova et al 2014 to infer the connectivity and transmissivity range of the main fracture interconnections between a borehole pair more recently other methods were developed to invert the 2 d network of fractures from tracer tomography experiments somogyvari et al 2017 and from ht data fischer et al 2018b whatever the choice of interpretation framework the spatial variability of storativity is rarely considered in the analysis of ht data castagna et al 2011 fischer et al 2018 illman et al 2009 as it is often thought to be minor compared to the spatial variability of transmissivity meier et al 1998 the characterization of storativity further implies conditioning numerical models to transient pressure data franssen et al 1999 and thus significantly increases inversion run times e g cardiff et al 2013 nevertheless both t and s should be accounted for as they control the pressure time response to a natural or induced stimulus e g cappa et al 2008 an improved estimate of fracture storativity implies that fracture transmissivities can be determined more accurately and reasonable fracture hydraulic properties could be obtained from pressure transient tests in this paper we discuss a new field study of transient ht carried out at the grimsel test site in switzerland the dataset was obtained through extensive cross borehole hydraulic packer testing sequential injection tests in crystalline rock performed during the pre stimulation characterization phase of a decameter scale in situ stimulation and circulation isc experiment we perform inversion of ht data collected in a fracture bearing brittle ductile shear zone characterized as a multiple fault core system to infer the full connectivity structure as well as the effective hydraulic properties t s of hydraulically connected flowpaths the underlying conceptual model of fracture flow and connectivity was first proposed by klepikova et al 2013 and has been demonstrated to be effective in inversion data from flow and temperature tomography experiments klepikova et al 2013 2014 the main originality behind our approach is to focus on the characterization of preferential flowpaths and their connectivity rather than identifying the equivalent porous media properties a major advancement in the inversion method described here is that we use transient pressure data to jointly estimate both t and s under the hypothesis that both parameters are spatially variable whereas s has been considered to be uniform in previous simplified dfn approaches le goc et al 2010 klepikova et al 2013 2014 fischer et al 2018b in the first part of the manuscript we review the conceptual model of flow and connectivity for fractured media adopted in this study we then present the methods used for the inversion procedure in the third part we describe the experimental site the borehole instrumentation and the implementation of the field ht experiment conducted finally we present and discuss the results of the application of the inverse approach to the transient ht data set lastly we validate the calibrated connectivity pattern against an independent dataset 2 background and methodology proposed in this work we follow the approach developed by klepikova et al 2013 and extend it for the inversion of transient ht data in sparsely fractured rocks in this section we briefly provide an overview of the framework present the numerical forward model used to simulate the tests performed and the inversion algorithm used to fit the observed pressure data 2 1 hydraulic tomography approach and conceptual discrete fracture network model the ht field approach consists of a series of injection tests in which the position of the active interval isolated with packers is varied between tests to i yield a larger amount of data compared to traditional hydraulic tests and ii explore the local effects in highly heterogeneous media an example of a fracture network intersected by two boreholes is shown in fig 1 a note that pumping tests can be also performed but for simplicity we illustrate this approach using injection tests considering a borehole pair with two packed off intervals in each borehole the pressure signal induced by fluid injection in the active test interval in the fig 1a water is injected in i2 1 interval depends on local fracture hydraulic properties at larger scale injection induces hydraulic head variations along flowpaths which in turn intersect the observation intervals in fig 1 only fractures packed off in intervals i1 1 i1 2 and i2 1 form an organized inter connected network at the inter borehole scale fractures highlighted in blue fig 1b illustrates a typical example of transient pressure signals collected in such an experimental setting during fluid injection in one of the intervals and the following pressure recovery period the induced pressure responses in the observation intervals depend on both local hydraulic properties at the injection and observation points and hydraulic properties and connectivity of fractures linking injection and observation intervals i e fractures that interconnect fractures connected to the intervals in order to accurately represent the hydraulic properties and connectivity of a fracture network fig 1a a large number of parameters including fracture location density orientation length and hydraulic properties of fractures would be required hydraulic data alone are insufficient to constrain such a degree of complexity le goc et al 2010 day lewis et al 2000 therefore the method that we propose here aims instead at generating a simplified representation of a fracture network with an effective fracture pattern connectivity structure and effective property values in order to focus on the connectivity existing within this network given the prevalent influence of preferential flowpaths in fractured media similarly to the conceptual fracture connectivity model presented in klepikova et al 2013 we aim to represent cross borehole scale features comprising a number of connected permeable fractures than the details of individual fractures in this simplified fracture network model each interval is intersected by a horizontal rectangular fracture their connectivity is controlled by a vertical rectangular fracture set at mid distance from the two boreholes in this way the proposed fracture network model introduces a level of complexity that matches the information content of ht data in the example considered we rely on a field setup with 4 intervals from fig 1 where the inter borehole fracture connectivity can be described through a total of 4 horizontal fractures and 1 vertical rectangular fracture consisting of 3 sections fig 1c d uniform transmissivity and storativity are assigned to each fracture since we assume that all fractures have the same aperture transmissivity here represents a measure of the effective connectivity of the network de dreuzy et al 2001 hence attributing different t and s values to the different sections of the vertical connecting fracture allows to modulate the connectivity of to the horizontal fractures directly intersecting the 4 borehole intervals a 3 step ht approach is proposed in this study 1 estimation of local apparent transmissivities of fracture zones intersecting the intervals t 1 1 t 1 2 t 2 1 and t 2 2 in fig 1d through interpretation of pressure transients from active injection intervals 2 defining simplified fracture network model based on a number of involved packed off intervals from each borehole their connectivity is determined by a combination of the total number of fractures from each borehole and the spatial arrangement of the intervals relative to each other in some situation the adjustment of fracture connectivity pattern could be required see fig 6 as an example 3 estimation of the transmissivities and storativities of connecting fractures between the intervals t 1 t 2 and t 3 in fig 1d through inversion of pressure transients from observation intervals until simulated pressure transients provide a satisfactory match of the field data 2 2 forward model we study the hydraulic responses of connected fractures under cross borehole injection conditions by developing a 3 d numerical model with 2 d flow in each fracture that simulates flow in the simplified fracture network the model considers fractures surrounded by the rock matrix time dependent fluid flow in the matrix block is governed by the continuity equation associated to darcy s law assuming laminar flow 1 ρ s sm p t ρ κ m μ p 0 ω matr ix block and the linearized storage model 2 s sm χ f ϕ m χ r 1 ϕ m ω matr ix block where p is the fluid pressure ϕm is the porosity of the matrix domain χf and χr are the compressibility of the fluid and solid respectively κm is the matrix permeability ρ is the fluid density and μ is its dynamic viscosity the fractures in the model are represented as interior boundaries with flow simulated along these interior boundaries fractures 3 ρ s s d p t t ρ κ μ d t p 0 ω frac ture where ss is the fracture storage coefficient κ describes the fracture s permeability d is the fracture s thickness and t denotes the gradient operator restricted to the fracture s tangential plane a no flux boundary condition on all the faces of the rock matrix applies 4 n u 0 ω block face where u κ μ p is the darcy velocity each fracture is characterized by a single transmissivity t value given by t d κ ρ g μ and a storativity s value given by s d ρ g s s where g is the acceleration due to gravity and d is the fracture aperture we apply no flux boundary on all the faces of the rock matrix an example of the pressure propagation in a simplified fracture network is given in fig 1c 2 3 inverse problem we next characterize the spatial variability of hydraulic parameters t and s by adjusting the transmissivities and storativities of fractures intersecting the intervals and of the vertical connecting fractures through inversion of ht data the misfit function fo which evaluates the difference between direct model simulations and pressure measurements is given by the classical least squares objective function with a linear time sampling 5 f o 1 σ 2 1 n 1 n p o b s p m o d 2 where pobs are transient pressure observations σ is the data error for pressure n is the number of observations pmod are pressure predicted by the model we choose a linear time sampling to reduce the weight of early time pressure data influenced by local heterogeneities and measurement errors giving instead more weight to late time data since early times represent a smaller number of data points the optimization problem is solved by minimizing the fo value using the nelder mead simplex algorithm a nonlinear fast local search method as implemented in the matlab function fminsearch klepikova et al 2013 lagarias et al 1998 this optimization routine requires a starting point for the inversion and permits the parameter search intervals to be restricted if desired to start model iteration an initial guess is supplied consisting of a starting model using an appropriate set of transmissivity and storativity values as the result provided by the simplex search method is not necessarily the absolute minimum of the objective function we improve this direct search method by generating a number of random starting points a local solver is then used to find the optima in the basins of attraction of the starting points then the final result corresponds to a set of transmissivity and storativity values leading to the lowest objective function value such a deterministic inversion method i e with a predefined pattern of spatial variability of hydraulic parameters allows to characterize the local hydraulic properties of fracture zones intersecting the observation intervals and represented by and represented by horizontal fractures and fracture network effective connectivity which is controlled by hydraulic properties of the different sections of the vertical fracture 3 experimental setting 3 1 test facility the ht experiment described in the present study was carried out as part of a unique decameter scale in situ stimulation and circulation isc experiment aiming at a better understanding of hydro seismo mechanical coupled processes that are associated with a high pressure fluid stimulation in a crystalline rock mass the isc experiment was conducted in a well characterized moderately fractured crystalline rock at the grimsel test site gts which has been extensively characterized hydraulically e g illman and tartakovsky 2006 keusen et al 1989 martinez landa and carrera 2005 wenning et al 2018 the gts is an underground rock laboratory owned and operated by the swiss national cooperative for the disposal of radioactive waste nagra situated in the southern part of the central aar massif in the swiss alps approximately 480 m below ground surface fig 2 a the isc experiment is located at the southern end of the gts in a small cavern au cavern at the end of the au gallery fig 2b as part of the isc experiment before high pressure stimulation a total of 15 boreholes of 30 50 m depth were drilled from or nearby the au cavern this includes two injection boreholes inj1 and inj2 shown by blue lines in fig 2c as well as boreholes for pressure strain temperature and geophysical monitoring shown by grey lines in fig 2c a general overview of the experimental site as well as the main concepts and the design of the isc experiment can be found in amann et al 2018 3 2 hydrogeological conditions numerous investigations have been previously performed to study the hydraulic properties of the grimsel granodiorite based on analytical illman and tartakovsky 2006 martinez landa and carrera 2005 hoehn and fierz 1990 and numerical martinez landa and carrera 2006 interpretation of single and cross borehole hydraulic tests carried out in the full scale engineered barrier experiment febex gallery and at the migration experiment location locations are shown in fig 2b the average transmissivity of the intact rock was estimated to be in the range of 10 14 10 13 m 2 s whereas the transmissivity of fault related fractures is in the order of 10 12 10 5 m 2 s the storage coefficient of fractures was found to vary by three orders of magnitude from 1 10 8 to 1 5 10 5 martinez landa and carrera 2005 2006 at the isc rock laboratory the rock mass aar granite and grimsel granodiorite is intersected by two sets of fracture bearing shear zones a set of reactivated ductile s1 shear zones 142 77 depicted by red surfaces in fig 2c and a set of brittle ductile s3 shear zones 183 65 depicted by green surfaces in fig 2c on a smaller scale the support volume studied contains a significant number of fractures krietsch et al 2018 the degree of fracturing is variable across the site the mean fracture density measured from core and borehole logging was estimated to be low 1 m 1 in the intact rock and increasing up to 2 4 and 3 m 1 in the s1 and s3 shear zones respectively note that the internal structure of these shear zones fits the description of most crustal fault zones chester and logan 1986 including a fault core damage zone and unperturbed host rock field experiments associated to the isc experiment relied on different techniques including hydraulic packer testing jalali et al 2018 solute kittilä et al 2019 and heat tracer tests brixel et al 2017 to characterize the hydraulic and transport properties of the support volume studied the distribution of hydraulic properties estimates inferred from high resolution discrete pulse tests fig 3 a and cross hole hydraulic packer tests fig 3b c are synthesized in fig 3 all the measurements shown here were collected between the s3 1 and s3 2 shear zones in boreholes drilled for the isc experiment including other boreholes not used for the analysis presented in this paper and shown by grey lines in fig 2c transmissivity estimates from hydraulic pressure pulse tests were obtained from generally discrete 2 to 4 m long tests intervals and fitted to type curves from standard analytical solutions such transmissivity values are representative of hydraulic conditions in the direct vicinity of boreholes at cross hole scale late time portion of the cross hole pressure responses induced from constant rate injection tests were analyzed using the straight line approximation following the cooper jacob method cooper et al 1967 key results may be summarized as follows compared to pulse test results cross borehole injection tests transmissivity estimates present a small variability generally within one and a half order of magnitude fig 3b the values obtained from pulse testing tend to be much lower the average value is almost two orders of magnitude smaller than for cross borehole estimates this difference could be explained by scale effects generally occurring in crystalline rocks e g clauser 1992 le borgne et al 2006 illman 2006 and in particular already observed at the gts martinez landa and carrera 2006 thus single borehole pulse testing allows to measure the response of packed off fractures for a very broad range of local property range while during cross borehole tests only the most transmissive flowpaths may carry flow and therefore show pressure responses as inter borehole hydraulic connectivity is primarily controlled by the spatial integration of high transmissivity zones as the spatial footprint of a test grows in time le borgne et al 2006 in fig 3c storativity estimates from cross borehole injection tests are presented the estimates of storativity obtained for measurements collected between s3 1 and s3 2 shear zones range from 1 5 10 7 to 2 10 4 which is one order of magnitude higher than values estimated in previous studies illman and tartakovsky 2006 martinez landa and carrera 2005 2006 here storativity values were calculated following the cooper jacob method described in the above i e obtained from the intersection of the slope superimposed on late time data with the time axis where s 2 25 t t r 2 where t denotes time and r is the radial euclidean distance from injection the storativity estimates vary over three orders of magnitude highlighting potential oversimplifications in assuming a spatially homogeneous storativity field 3 3 pressure tomography experiment test design and field instrumentation the ht field experiments analyzed in the present study are part of the hydraulic characterization of the support volume targeted for high pressure stimulation which was completed before the drilling of boreholes for pressure temperature and geophysical monitoring shown by grey lines in fig 2c the two injection boreholes inj1 and inj2 used in this study were instrumented with deployable multi packer systems providing in total four packed off intervals fig 4 these four intervals were selected because they are located in between the s3 shear zones identified as the most permeable regions a summary of the setup including interval depths and lengths as well as the injection rate applied to the interval is provided in table 1 fluid pressures in packed off testing intervals were measured uphole using piezoresistive pressure transducers keller paa 23sy connected to a 2 m m id access line reporting to a downhole open port the pressure data resolution achieved under most test conditions was in the order of 0 5 kpa pressure sensors were connected to a multi channel data acquisition system to yield a continuous measurement record with a scan rate of 2 sec during each injection test a constant rate was maintained by bronkhorst cori flow mass flow controllers mounted in parallel on a flow board covering a total flow range of 0 03 to 667 ml min with a stated accuracy of 0 2 of the flow rate in order to carry out constant rate tests tap water sourced from the facility main water line was injected through one of the bronkhorst flow controllers all three controllers were connected through a rs232 flow bus interface to a main computer unit allowing to start control and stop fluid injection remotely 3 3 1 test implementation and field observations a total of four cross hole injection tests were conducted in a tomographic configuration with fluid injection taking place and pressure responses monitored successively at four intervals described in table 1 the intervals and the contours depicting shear zones are both displayed in fig 4 preliminary injection tests were conducted to verify that there is no leakage between the packers and the borehole wall each test consisted of injection a constant flow rate q table 1 in one interval while monitoring the pressure response in all the other intervals after 2 3 hours of monitoring the pressure buildup the injection was switched off and pressure recovery was monitored for an additional 2 4 hours prior to starting the next injection test the pressure was allowed to recover for each experiment note that the injection pressure applied during the tests presented in this study are much below the minimum principal stress 8 mpa i e the hydromechanical effects are likely minor field data are shown in fig 5 the full data set consists of 16 pressure transients i e responses of four intervals from four injection tests with injection pressures shown on the left y axes and cross hole pressure responses from observation intervals shown relatively on the right y axes for the data recorded a moving average smoothing with a span of 5 of the total number of data points was applied to remove the static sensor noise since the data were collected at a high frequency 0 5 hz the application of this filter did not significantly impact the shape of the pressure curve we note that even in the range where response is on the order of 1 kpa the noise does not completely mask the main tendencies for the application in this work data used in the inversion consist of hundred pressure responses measurements two hundreds for 20 000 s records chosen automatically per pressure curve 4 results in this section we present results of the proposed deterministic inversion method to ht field data first we simplify the fracture network geometry once the fracture network geometry has been simplified we apply the inverse modelling framework to estimate the transmissivities and storativities of hydraulically active fractures between and around the borehole pair 4 1 simplified discrete fracture network model using estimates of fracture strike and dip from geophysical logs and projecting the intersection of permeable packed off fractures with other boreholes we have found that the vast majority of discrete fracture traces are not continuous between boreholes this suggests that the permeable fractures in one borehole are either closed and do not intersect other boreholes or have been shifted hydraulic connections between boreholes are thus likely formed through complex three dimensional patterns of fractures to model pressure propagation between testing intervals the fracture network geometry has been simplified as described in section 2 1 a vertical fracture equally distanced from both boreholes and divided into three sections by horizontal fractures controls the degree of hydraulic connections between intervals depending on the relative position of the intervals different simplified geometries involving six fracture connectivity patterns could be proposed as a basis of the forward model fig 6 4 2 forward model the rectangular domain selected for the ht analysis has x y z dimensions of 100m 100m 20m considered to be much larger than the radius of influence of the injection tests based on results from hydraulic tests conducted in intact rock we assume that matrix is impermeable note however that the approach can be easily extended to fractured porous rock additional simulations not shown here indicate that assigning a uniform value of transmissivitiy within the matrix in a range 10 15 10 12 m 2 s induces no remarkable effects on the pressure propagation the model includes four horizontal rectangular fracture planes intersected by a vertical rectangular fracture plane the relative positions of horizontal fractures is fixed according to the tested connectivity pattern fig 6 note that boreholes are not explicitly modelled and testing intervals correspond to four points lying on horizontal fracture planes where fluxes q into the fracture were successively specified the points in the model are separated from one another by distances corresponding to distances in between the centers of packed off intervals 11 2 m distance for shallow intervals and 12 3 m distance for deeper intervals governing equations are solved with the finite element code comsol multiphysics 5 4 with a fine tetrahedral meshing the domain was discretized into 280 000 to 300 000 elements depending on the configuration the element size varied from 0 2 m to 1 5 m the finer elements are located around the fluid injection observation points and fractures and the coarser elements are located near the faces of the rock matrix various tests were performed to ensure mesh independent results we apply no flux boundary on all the faces of the rock matrix initial conditions were set by assuming hydrostatic conditions prior to the start of each cross borehole test we sequentially impose the injection rates q as point fluid injections table 1 and simulate transient pressure responses induced therefrom finally we use this forward model to determine fracture hydraulic properties by fitting the pressure transients using inverse analysis described in the section 2 3 4 3 inverse model to match the observed pressure transients a total of seven transmissivity and seven storativity estimates four horizontal fractures and three sections in the vertical fracture are adjusted for each fracture connection pattern fig 6 as part of the inversion procedure to this end we minimize the sum of squares misfit eq 5 between the measured data and model predictions using different values for the fracture hydraulic properties the values of hydraulic properties of the connecting fractures are permitted to vary within the ranges determined based on prior hydrogeological knowledge illman and tartakovsky 2006 martinez landa and carrera 2005 2006 hoehn and fierz 1990 for the logarithm of the transmissivity over the range l o g t 12 5 and for the logarithm of the storativity over the range l o g s 8 4 8 a set of 20 starting models is generated for each fracture connectivity pattern fig 6 to search for a global minimum of the objective functions yielding 20 estimates of the fracture hydraulic properties the final result corresponds to a set of hydraulic properties values leading to the lowest objective function value the starting points were selected randomly from a uniform distribution for the log transformed transmissivity and storativity over the same range as parameters note that the total number of starting points was limited by computing time for these model runs about 7 min for one direct simulation on a 2 6 ghz intel core i7 processor 8th generation with 16 gb of ddr4 ram while the solution converges generally after a 30 100 iterations first we impose the fracture network connectivity pattern shown in fig 6a and assume that s is spatially homogeneous while t is assumed to be spatially heterogeneous the cross section of fracture network model geometry with the distribution of log transmissivity values obtained after inversion is presented in fig 7 the results show that overall transmissivities of fractures connecting the inj1 inj2 borehole pair decrease with depth l o g t 1 log t 2 log t 3 7 7 8 8 8 in other words model captures the stronger hydraulic connectivity of shallow transmissive intervals inj1 i4 and inj2 i4 while the deeper intervals inj1 i3 and inj2 i3 are found to be less transmissive and less well connected for this case l o g s 6 was estimated to be the best fit value the solution obtained yields the best fit to pressure transients fig 8 compares the observed pressure responses after filtering solid lines and calibrated records of pressure dashed lines versus time for four injection tests the majority of the simulated responses in the injection intervals capture reasonably well the observed pressure behaviour while the exact temporal scaling of the pressure curves is not necessarily reproduced altogether the matches in the observation intervals are of intermediate quality and some of the matches are poor rmse values are provided in table 2 in particular the simulated cross borehole pressure responses are overestimated by a factor of 3 responses in inj2 i3 and inj1 4 in fig 8a response in inj1 i3 in fig 8b and c furthermore all responses observed when injecting in inj2 i4 fig 8d are underestimated by a factor of 4 we presume that the main reason for the inaccurate prediction of pressure transients in observation intervals is that we assumed in these simulations that s is homogeneous while field data indicates that it is spatially variable another possible reason could be related to the assumption about the connectivity pattern for example for the fracture connectivity pattern imposed here fig 6a the inj1 i4 interval cannot be connected to inj2 i3 interval without being connected to the inj1 i3 interval this contrasts with the pressure transients observed in inj1 i3 and in inj2 i3 in response to injecting in inj1 i4 fig 8b thus this connectivity pattern imposes geometrical constraints on fracture connections which are not necessarily compatible with the observed pressure trends to improve the match between simulated and measured pressure records we then in a second step allow the spatial variability of the log transmissivity and log storativity values under the hypothesis that t and s are independent variables we let the storativity of the fracture connections vary in a range given by s n 10 8 10 4 moreover we test different connectivity patterns as shown in fig 6 among these six connectivity patterns the connectivity pattern shown in fig 6b yields the best fit to pressure transients the corresponding distributions of the log transmissivity and log storativity values is given in fig 9 a and b respectively compared to the first case assuming log storativity to be spatially constant fig 7 the estimated t field is spatially more heterogeneous the results in fig 9a demonstrate that the most permeable fracture connection is the one in between inj1 i4 and inj2 i4 intervals while deeper fractures corresponding to s3 2 shear zone are found to be less permeable and less well connected these findings were also further confirmed through dna labeled nanoparticle and solute tracer tests conducted on site kittilä et al 2019 a comparison with tracer tests results is presented in section 5 furthermore the inversion procedure produces high storativity values for shallower fractures l o g s i n j 2 i 4 5 l o g s i n j 1 i 4 5 3 l o g s 1 5 higher storativity values imply slower pressure time responses of shallower fractures that our inverse approach compensates for by an increase in transmissivity of the vertical fractures connecting shallow intervals l o g t 1 log t 2 log t 3 5 7 6 2 8 9 a comparison of simulated versus observed pressure curves for all intervals for four injection tests is shown in fig 10 the model reproduces nearly accurately the pressure transients observed in response to injecting in intervals inj1 i3 and inj2 i3 fig 10a and c respectively fig 10b reveals that discrepancies occur between simulated and observed records in interval inj1 i3 finally although the model fits presented in fig 10d were improved pressure responses observed when injecting in inj2 i4 are still underestimated by a factor of 3 overall comparing these plots to fig 8 clearly shows a significant improvement in the model fit for purposes of comparison we used the root mean square error rmse given in table 2 the rmse values range from 0 15 to 6 2 kpa which are 1 2 18 times smaller than the model considering spatially homogeneous s and a different fracture network connectivity pattern for the same respective pressure responses to further assess the quality of data fit we provide a scatterplot of the measured versus computed pressure measurements in fig 11 data from different injection tests are represented with symbols of different colors a solid black line indicating a perfect 1 1 correlation is plotted and the dashed red line represents the best fit of observed and simulated values in general the points cluster along the 1 1 line with a minor bias the outliers plotting off the diagonal mainly correspond to the cross borehole responses induced by injection in the inj2 i4 interval cyan markers the inability of the proposed approach to match all pressure transients may indicate that i the chosen geometry is over simplified we acknowledge that this is probably the main source of uncertainty and or ii that the measured pressure response is influenced by boundary effects this is especially true for longer duration of the test and for high transmissivity fracture zones such as one intersecting inj2 i4 interval and or iii that the measured pressure response is influenced by effects of fracture dead ends fracture aperture variations and increase in contact area inducing non darcy flow in fractures e g quinn et al 2011 and or iv that the measured pressure response is may also be influenced by earth tides and ambient fluctuations of hydraulic heads which the proposed model does not take into account given these factors and potential measurement errors such as instrument noise of at least 0 5 kpa imperfect measurement modeling of injection flow rates etc we believe that this represents a good fit to the measurements furthermore we evaluate our model results by simulating an injection test which has not been utilized for the estimation of the t and s tomograms and compare the simulated and observed pressure records for this test note that the location of the injection used for model validation is the inj1 i3 interval i e the same as one of the constant injection tests included in the inverse model however during the test used for model validation water was injected as a step function step rate test q 20 40 and 60 ml min simulated and the observed pressure records are shown in fig 12 the rmse of the simulations of the validation step rate test is 0 2 kpa overall most of the simulated pressure variations capture the general trends of the observed variations demonstrating that t and s tomograms do a good job in predicting the pressure responses in these intervals 5 discussion and conclusions we present here a novel deterministic inversion framework to infer the connectivity and spatial distribution of log transmissivity and storativity of preferential permeable flowpaths in sparse fracture systems constrained by a series of hydraulic cross borehole tests the originality of the proposed approach is to focus on preferential flowpaths and characterize their connectivity based on discrete fracture network concepts rather than on equivalent media properties this permits to capture the effect of flow channeling thus leading to a realistic fracture pattern and connectivity the method proposed is successfully applied to hydraulic tomography data set obtained from fractured crystalline rocks at the grimsel test site in switzerland this study allowed to identify the key controlling fracture network connectivity pattern and hydraulic properties of the main flowpaths including the transmissivity and the storativity distribution in agreement with a previous study of castagna et al 2011 our results showed that keeping log storativity spatially constant results in an overall change of the inferred log transmissivity field and thus affects the degree of hydraulic connection between observation intervals we evaluated the quality of the inferred t and s estimates and connectivity distributions by comparing observed and calibrated transient pressure records and found that model predictions of pressure transients were generally good moreover we used additional data from a cross borehole step rate injection test to evaluate the inferred distribution of the fracture network transmissivity and storativity most calibrated responses accurately capture the observed pressure behaviors providing thus additional confidence in the validity of the inversion results even though the fracture network geometry is highly simplified our estimates of fracture connectivity are generally consistent with fracture connectivity analyses reported by other studies implemented as part of the isc experiment thus both ht field experiments and tracer tests kittilä et al 2019 confirm the existence of a network of connected fractures in between inj1 i4 and inj2 i4 intervals moreover no plausible breakthrough curves were detected in monitoring intervals in borehole inj1 from the tracer injected into interval inj2 i3 thus revealing that this interval is poorly connected to inj1 intervals this observation agrees well with the parameter distribution derived from our analysis and illustrated in fig 10 the connectivity pattern used in this study is found to agree relatively well with the backbone structure identified from a joint analysis of geological and hydrogeological data the two sets of horizontal fractures may be linked with the two subparallel s3 shear zones while the vertical linking fracture could represent the extension fractures identified from pressure pulse testing through extensive cross hole testing such fractures were found to allow fluids to migrate from one shear zone to another forming therefore a key linkage between the two major shear zones overall hydrogeophysical data provide a strong field evidence of channeling effects in solute and heat transport at the test site for the isc experiment in particular we carried out cross hole heat injection experiments with the injection taking place in inj2 i4 interval combined with temperature measurements along fiber optic cables installed in six boreholes brixel et al 2017 observed temperature breakthrough curves revealed that only few depth discrete well connected fractures contribute to heat transport giertzuch et al 2018 used single hole ground penetrating radar gpr surveys to monitor salt tracer plume propagating between the s3 shear zones using borehole antennas in single hole reflection mode a tracer injected in the inj2 borehole was monitored over time by exploiting difference imaging techniques the obtained gpr images revealed a tracer branching into two different flowpaths between the s3 shear zones nevertheless a detailed comparison of the delineated flowpaths is not possible at the current stage because i tracer transport flowpaths can differ from the major flowpaths in fractured rocks dorn et al 2012 ii our approach does not image the true fracture network geometry iii gpr images correspond to omni directional fracture projections these results highlight the importance of the characterization of the preferential flow paths rather than equivalent media properties or details of individual fractures as discussed above for some fracture patterns our simplified conceptual model introduces geometrical constrains on fracture connections a major restriction of the tested geometries is a limited number of independent possible paths in between observation intervals a data fusion approach combining geometrical and hydraulic information is expected to provide new insights into the complex structure of channeled flows in fractured media future works will set up a 3 d dfn model of the testing volume at the gts conditioned to hydraulic solute tracer tests kittilä et al 2019 data high resolution breakthrough curves from cross borehole thermal tracer tests conducted on site brixel et al 2017 and gpr reflection data giertzuch et al 2018 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the isc is a project of the deep underground laboratory at eth zurich established by the swiss competence center for energy research supply of electricity sccer soe with the support of the swiss commission for technology and innovation cti funding for the isc project was provided by the eth foundation with grants from shell and ewz and by the swiss federal office of energy through a p d grant the grimsel test site is operated by nagra the national cooperative for the disposal of radioactive waste we are indebted to nagra for hosting the isc experiment in their gts facility and to the nagra technical staff for onsite support the authors thank rené dorrer and reto seifert for their help in the field finally we thank five anonymous reviewers for their helpful comments and suggestions that improved the quality of the manuscript supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 103500 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
531,in sparse fracture systems flow and transport patterns are often dominated by main flowpaths and characterized by a strong complexity induced by the fractures surface roughness hierarchical arrangement in networks and from the interaction of the fractures with the surrounding rocks the accurate characterization of the hydraulic properties and connectivity of major fracture zones is essential to model flow solute and heat transport and fluid pressure propagation in fractured media in this study we present a novel deterministic inverse modeling method for imaging the connectivity and spatial variability of transmissivity and storativity of a network of fractures the method is based on a numerical model that simulates fluid flow in a simple three dimensional 3 d discrete fracture network dfn with a fixed fracture network structure the forward model is coupled to an inverse algorithm to match observed pressure transients obtained from sequential hydraulic cross hole tests this method has been successfully tested for constant rate injection tests carried out at the grimsel test site gts in switzerland cross hole injection tests were conducted in a tomographic configuration with hydraulic responses monitored at four observation intervals at various depths in two different boreholes a discrete fracture network approach with a simple parametrization is developed to estimate log transformed transmissivity t and storativity s values of hydraulically active fractures between boreholes by inverting the pressure transients under the hypothesis that t and s are independent variables we identified several permeable fractures and their connectivity without attempting to represent explicitly the true fracture network geometrical properties length orientation dip focusing instead on the calibration of a simple model capturing the observed pressure main behavior the identified inter borehole connectivity structure agrees well with independent information including additional data from a cross borehole step rate injection and hydrogeophysical data hence the proposed tomography approach appears to be a promising approach for characterizing the connectivity structure and hydraulic properties of the main flowpaths in sparsely fractured rock keywords fractured media inverse modelling discrete fractured network hydraulic tomography fracture connectivity 1 introduction knowledge about the distribution of hydraulic properties in fractured rocks is of particular importance for water resources management geothermal energy production oil and gas industry nuclear waste disposal or co2 sequestration numerous field observations gave evidence that flow transport and pressure propagation in fractured media are controlled by strongly heterogeneous flowpath distributions and often dominated by a limited number of flow channels e g abelin et al 1991a 1991b read et al 2013 roques et al 2016 tsang and neretnieks 1998 the accurate characterization of the hydraulic properties of natural fracture networks including the hydraulic conductivity k and specific storage ss and their connectivity is therefore of primary importance however the cost intensive nature of data collection and spatial scarcity of data generally available from boreholes imply that the information content of the measured data is rather limited compared to the complexity of fractured networks this strongly restricts our ability to model and forecast the aforementioned environmental processes and industrial applications over the last decades a broad range of new experimental techniques supported by different interpretation frameworks underpinned by different conceptual models has emerged to improve the characterization of heterogeneous flowpath in fractured aquifers this includes i borehole flow measurements le borgne et al 2007 klepikova et al 2013 and periodic hydraulic tests cardiff et al 2013a guiltinan and becker 2015 to characterize the hydraulic properties of fractures and fractured rock mass ii fiber optic borehole temperature measurements combined with heating experiments read et al 2013 bense et al 2016 to investigate ground water flow and heat transport iii ground penetrating radar gpr measurements to image tracer displacement within the fracture network dorn et al 2012 shakas et al 2016 and iv multi scale tracer transport experiments to understand the emergence of anomalous transport in fractures de la bernardie et al 2018 guihéneuf et al 2017 kang et al 2015 klepikova et al 2016 however despite the technical efforts and cost intensive field implementation hydraulic packer tests and especially their application under tomographic configuration are recognized to provide the most detailed and reliable information about spatial heterogeneity of the hydraulic properties of the aquifers illman et al 2009 zha et al 2015 at its core hydraulic tomography ht aims to jointly analyse multiple sets of cross borehole packer test data integrating thereby more information contained in the measurements compared to a classical testing configuration butler jr et al 1999 yeh and liu 2000 originally proposed for imaging of hydraulic heterogeneity in porous media liu et al 2002 li et al 2008 cardiff et al 2013a jiménez et al 2013 the development and validation of ht in fractured media as a proven method remains at an early stage a strong point of debate for instance is the choice of appropriate inverse models to interpret hydraulic tomography data sets in fractured media the majority of existing interpretation frameworks conceptualize fractured rock as an equivalent porous media this includes travel time based inversion scheme brauchler et al 2003 trajectory based traveltime approach he et al 2006 sequential successive linear estimator hao et al 2008 illman et al 2009 yeh and liu 2000 the pilot points method lavenue and de marsily 2001 vesselinov et al 2001 castagna et al 2011 and a stochastic inversion approach constrained by a priori inter borehole connection information day lewis et al 2000 wang et al 2017 to avoid resolving extreme spatial contrasts in hydraulic properties from fractures and the surrounding rock these methods assign instead average hydraulic properties across bulk regions of the aquifer this often results in smooth parameter fields that do not reflect the discrete nature of spatial variability in hydraulic properties of fractured rocks recent laboratory tests on fractured rock samples demonstrated that the resolution of the estimated fracture pattern could be significantly improved by increasing the number of pumping observation intervals sharmeen et al 2012 illman 2014 in the field however the monitoring interval density restricted by the limited number of observation boreholes is generally sparse making it impossible to capture the discrete nature of flow in fractured geologic systems in contrast recent studies have proposed new ht interpretation frameworks based on discrete fracture network dfn models le goc et al 2010 klepikova et al 2013 2014 borghi et al 2016 somogyvari et al 2017 fischer et al 2018b the main challenge associated with this approach is to constrain the geometrical complexity of the model fracture internal heterogeneity fracture location orientation length density and fracture interconnections based on the information available from field measurements in order to avoid under and over parameterization e g borghi et al 2016 such complex task imposes some conceptual simplifications or requires that additional data such as hydrogeophysical measurements or tracer test data be considered le goc et al 2010 for example proposed an algorithm to identify both geometrical and hydraulic characteristics of a limited number of most probable highly localized flow channels in fractured media the methodology was established on 2 d synthetic flow channel structures using synthetic steady state hydraulic heads supplemented by geometrical data i e the distance between a well and the nearest flow channel furthermore based on a simplified fracture network model which did not take the fracture geometry explicitly into account a framework was proposed for inverting borehole flow measurements klepikova et al 2013 and borehole temperature profiles klepikova et al 2014 to infer the connectivity and transmissivity range of the main fracture interconnections between a borehole pair more recently other methods were developed to invert the 2 d network of fractures from tracer tomography experiments somogyvari et al 2017 and from ht data fischer et al 2018b whatever the choice of interpretation framework the spatial variability of storativity is rarely considered in the analysis of ht data castagna et al 2011 fischer et al 2018 illman et al 2009 as it is often thought to be minor compared to the spatial variability of transmissivity meier et al 1998 the characterization of storativity further implies conditioning numerical models to transient pressure data franssen et al 1999 and thus significantly increases inversion run times e g cardiff et al 2013 nevertheless both t and s should be accounted for as they control the pressure time response to a natural or induced stimulus e g cappa et al 2008 an improved estimate of fracture storativity implies that fracture transmissivities can be determined more accurately and reasonable fracture hydraulic properties could be obtained from pressure transient tests in this paper we discuss a new field study of transient ht carried out at the grimsel test site in switzerland the dataset was obtained through extensive cross borehole hydraulic packer testing sequential injection tests in crystalline rock performed during the pre stimulation characterization phase of a decameter scale in situ stimulation and circulation isc experiment we perform inversion of ht data collected in a fracture bearing brittle ductile shear zone characterized as a multiple fault core system to infer the full connectivity structure as well as the effective hydraulic properties t s of hydraulically connected flowpaths the underlying conceptual model of fracture flow and connectivity was first proposed by klepikova et al 2013 and has been demonstrated to be effective in inversion data from flow and temperature tomography experiments klepikova et al 2013 2014 the main originality behind our approach is to focus on the characterization of preferential flowpaths and their connectivity rather than identifying the equivalent porous media properties a major advancement in the inversion method described here is that we use transient pressure data to jointly estimate both t and s under the hypothesis that both parameters are spatially variable whereas s has been considered to be uniform in previous simplified dfn approaches le goc et al 2010 klepikova et al 2013 2014 fischer et al 2018b in the first part of the manuscript we review the conceptual model of flow and connectivity for fractured media adopted in this study we then present the methods used for the inversion procedure in the third part we describe the experimental site the borehole instrumentation and the implementation of the field ht experiment conducted finally we present and discuss the results of the application of the inverse approach to the transient ht data set lastly we validate the calibrated connectivity pattern against an independent dataset 2 background and methodology proposed in this work we follow the approach developed by klepikova et al 2013 and extend it for the inversion of transient ht data in sparsely fractured rocks in this section we briefly provide an overview of the framework present the numerical forward model used to simulate the tests performed and the inversion algorithm used to fit the observed pressure data 2 1 hydraulic tomography approach and conceptual discrete fracture network model the ht field approach consists of a series of injection tests in which the position of the active interval isolated with packers is varied between tests to i yield a larger amount of data compared to traditional hydraulic tests and ii explore the local effects in highly heterogeneous media an example of a fracture network intersected by two boreholes is shown in fig 1 a note that pumping tests can be also performed but for simplicity we illustrate this approach using injection tests considering a borehole pair with two packed off intervals in each borehole the pressure signal induced by fluid injection in the active test interval in the fig 1a water is injected in i2 1 interval depends on local fracture hydraulic properties at larger scale injection induces hydraulic head variations along flowpaths which in turn intersect the observation intervals in fig 1 only fractures packed off in intervals i1 1 i1 2 and i2 1 form an organized inter connected network at the inter borehole scale fractures highlighted in blue fig 1b illustrates a typical example of transient pressure signals collected in such an experimental setting during fluid injection in one of the intervals and the following pressure recovery period the induced pressure responses in the observation intervals depend on both local hydraulic properties at the injection and observation points and hydraulic properties and connectivity of fractures linking injection and observation intervals i e fractures that interconnect fractures connected to the intervals in order to accurately represent the hydraulic properties and connectivity of a fracture network fig 1a a large number of parameters including fracture location density orientation length and hydraulic properties of fractures would be required hydraulic data alone are insufficient to constrain such a degree of complexity le goc et al 2010 day lewis et al 2000 therefore the method that we propose here aims instead at generating a simplified representation of a fracture network with an effective fracture pattern connectivity structure and effective property values in order to focus on the connectivity existing within this network given the prevalent influence of preferential flowpaths in fractured media similarly to the conceptual fracture connectivity model presented in klepikova et al 2013 we aim to represent cross borehole scale features comprising a number of connected permeable fractures than the details of individual fractures in this simplified fracture network model each interval is intersected by a horizontal rectangular fracture their connectivity is controlled by a vertical rectangular fracture set at mid distance from the two boreholes in this way the proposed fracture network model introduces a level of complexity that matches the information content of ht data in the example considered we rely on a field setup with 4 intervals from fig 1 where the inter borehole fracture connectivity can be described through a total of 4 horizontal fractures and 1 vertical rectangular fracture consisting of 3 sections fig 1c d uniform transmissivity and storativity are assigned to each fracture since we assume that all fractures have the same aperture transmissivity here represents a measure of the effective connectivity of the network de dreuzy et al 2001 hence attributing different t and s values to the different sections of the vertical connecting fracture allows to modulate the connectivity of to the horizontal fractures directly intersecting the 4 borehole intervals a 3 step ht approach is proposed in this study 1 estimation of local apparent transmissivities of fracture zones intersecting the intervals t 1 1 t 1 2 t 2 1 and t 2 2 in fig 1d through interpretation of pressure transients from active injection intervals 2 defining simplified fracture network model based on a number of involved packed off intervals from each borehole their connectivity is determined by a combination of the total number of fractures from each borehole and the spatial arrangement of the intervals relative to each other in some situation the adjustment of fracture connectivity pattern could be required see fig 6 as an example 3 estimation of the transmissivities and storativities of connecting fractures between the intervals t 1 t 2 and t 3 in fig 1d through inversion of pressure transients from observation intervals until simulated pressure transients provide a satisfactory match of the field data 2 2 forward model we study the hydraulic responses of connected fractures under cross borehole injection conditions by developing a 3 d numerical model with 2 d flow in each fracture that simulates flow in the simplified fracture network the model considers fractures surrounded by the rock matrix time dependent fluid flow in the matrix block is governed by the continuity equation associated to darcy s law assuming laminar flow 1 ρ s sm p t ρ κ m μ p 0 ω matr ix block and the linearized storage model 2 s sm χ f ϕ m χ r 1 ϕ m ω matr ix block where p is the fluid pressure ϕm is the porosity of the matrix domain χf and χr are the compressibility of the fluid and solid respectively κm is the matrix permeability ρ is the fluid density and μ is its dynamic viscosity the fractures in the model are represented as interior boundaries with flow simulated along these interior boundaries fractures 3 ρ s s d p t t ρ κ μ d t p 0 ω frac ture where ss is the fracture storage coefficient κ describes the fracture s permeability d is the fracture s thickness and t denotes the gradient operator restricted to the fracture s tangential plane a no flux boundary condition on all the faces of the rock matrix applies 4 n u 0 ω block face where u κ μ p is the darcy velocity each fracture is characterized by a single transmissivity t value given by t d κ ρ g μ and a storativity s value given by s d ρ g s s where g is the acceleration due to gravity and d is the fracture aperture we apply no flux boundary on all the faces of the rock matrix an example of the pressure propagation in a simplified fracture network is given in fig 1c 2 3 inverse problem we next characterize the spatial variability of hydraulic parameters t and s by adjusting the transmissivities and storativities of fractures intersecting the intervals and of the vertical connecting fractures through inversion of ht data the misfit function fo which evaluates the difference between direct model simulations and pressure measurements is given by the classical least squares objective function with a linear time sampling 5 f o 1 σ 2 1 n 1 n p o b s p m o d 2 where pobs are transient pressure observations σ is the data error for pressure n is the number of observations pmod are pressure predicted by the model we choose a linear time sampling to reduce the weight of early time pressure data influenced by local heterogeneities and measurement errors giving instead more weight to late time data since early times represent a smaller number of data points the optimization problem is solved by minimizing the fo value using the nelder mead simplex algorithm a nonlinear fast local search method as implemented in the matlab function fminsearch klepikova et al 2013 lagarias et al 1998 this optimization routine requires a starting point for the inversion and permits the parameter search intervals to be restricted if desired to start model iteration an initial guess is supplied consisting of a starting model using an appropriate set of transmissivity and storativity values as the result provided by the simplex search method is not necessarily the absolute minimum of the objective function we improve this direct search method by generating a number of random starting points a local solver is then used to find the optima in the basins of attraction of the starting points then the final result corresponds to a set of transmissivity and storativity values leading to the lowest objective function value such a deterministic inversion method i e with a predefined pattern of spatial variability of hydraulic parameters allows to characterize the local hydraulic properties of fracture zones intersecting the observation intervals and represented by and represented by horizontal fractures and fracture network effective connectivity which is controlled by hydraulic properties of the different sections of the vertical fracture 3 experimental setting 3 1 test facility the ht experiment described in the present study was carried out as part of a unique decameter scale in situ stimulation and circulation isc experiment aiming at a better understanding of hydro seismo mechanical coupled processes that are associated with a high pressure fluid stimulation in a crystalline rock mass the isc experiment was conducted in a well characterized moderately fractured crystalline rock at the grimsel test site gts which has been extensively characterized hydraulically e g illman and tartakovsky 2006 keusen et al 1989 martinez landa and carrera 2005 wenning et al 2018 the gts is an underground rock laboratory owned and operated by the swiss national cooperative for the disposal of radioactive waste nagra situated in the southern part of the central aar massif in the swiss alps approximately 480 m below ground surface fig 2 a the isc experiment is located at the southern end of the gts in a small cavern au cavern at the end of the au gallery fig 2b as part of the isc experiment before high pressure stimulation a total of 15 boreholes of 30 50 m depth were drilled from or nearby the au cavern this includes two injection boreholes inj1 and inj2 shown by blue lines in fig 2c as well as boreholes for pressure strain temperature and geophysical monitoring shown by grey lines in fig 2c a general overview of the experimental site as well as the main concepts and the design of the isc experiment can be found in amann et al 2018 3 2 hydrogeological conditions numerous investigations have been previously performed to study the hydraulic properties of the grimsel granodiorite based on analytical illman and tartakovsky 2006 martinez landa and carrera 2005 hoehn and fierz 1990 and numerical martinez landa and carrera 2006 interpretation of single and cross borehole hydraulic tests carried out in the full scale engineered barrier experiment febex gallery and at the migration experiment location locations are shown in fig 2b the average transmissivity of the intact rock was estimated to be in the range of 10 14 10 13 m 2 s whereas the transmissivity of fault related fractures is in the order of 10 12 10 5 m 2 s the storage coefficient of fractures was found to vary by three orders of magnitude from 1 10 8 to 1 5 10 5 martinez landa and carrera 2005 2006 at the isc rock laboratory the rock mass aar granite and grimsel granodiorite is intersected by two sets of fracture bearing shear zones a set of reactivated ductile s1 shear zones 142 77 depicted by red surfaces in fig 2c and a set of brittle ductile s3 shear zones 183 65 depicted by green surfaces in fig 2c on a smaller scale the support volume studied contains a significant number of fractures krietsch et al 2018 the degree of fracturing is variable across the site the mean fracture density measured from core and borehole logging was estimated to be low 1 m 1 in the intact rock and increasing up to 2 4 and 3 m 1 in the s1 and s3 shear zones respectively note that the internal structure of these shear zones fits the description of most crustal fault zones chester and logan 1986 including a fault core damage zone and unperturbed host rock field experiments associated to the isc experiment relied on different techniques including hydraulic packer testing jalali et al 2018 solute kittilä et al 2019 and heat tracer tests brixel et al 2017 to characterize the hydraulic and transport properties of the support volume studied the distribution of hydraulic properties estimates inferred from high resolution discrete pulse tests fig 3 a and cross hole hydraulic packer tests fig 3b c are synthesized in fig 3 all the measurements shown here were collected between the s3 1 and s3 2 shear zones in boreholes drilled for the isc experiment including other boreholes not used for the analysis presented in this paper and shown by grey lines in fig 2c transmissivity estimates from hydraulic pressure pulse tests were obtained from generally discrete 2 to 4 m long tests intervals and fitted to type curves from standard analytical solutions such transmissivity values are representative of hydraulic conditions in the direct vicinity of boreholes at cross hole scale late time portion of the cross hole pressure responses induced from constant rate injection tests were analyzed using the straight line approximation following the cooper jacob method cooper et al 1967 key results may be summarized as follows compared to pulse test results cross borehole injection tests transmissivity estimates present a small variability generally within one and a half order of magnitude fig 3b the values obtained from pulse testing tend to be much lower the average value is almost two orders of magnitude smaller than for cross borehole estimates this difference could be explained by scale effects generally occurring in crystalline rocks e g clauser 1992 le borgne et al 2006 illman 2006 and in particular already observed at the gts martinez landa and carrera 2006 thus single borehole pulse testing allows to measure the response of packed off fractures for a very broad range of local property range while during cross borehole tests only the most transmissive flowpaths may carry flow and therefore show pressure responses as inter borehole hydraulic connectivity is primarily controlled by the spatial integration of high transmissivity zones as the spatial footprint of a test grows in time le borgne et al 2006 in fig 3c storativity estimates from cross borehole injection tests are presented the estimates of storativity obtained for measurements collected between s3 1 and s3 2 shear zones range from 1 5 10 7 to 2 10 4 which is one order of magnitude higher than values estimated in previous studies illman and tartakovsky 2006 martinez landa and carrera 2005 2006 here storativity values were calculated following the cooper jacob method described in the above i e obtained from the intersection of the slope superimposed on late time data with the time axis where s 2 25 t t r 2 where t denotes time and r is the radial euclidean distance from injection the storativity estimates vary over three orders of magnitude highlighting potential oversimplifications in assuming a spatially homogeneous storativity field 3 3 pressure tomography experiment test design and field instrumentation the ht field experiments analyzed in the present study are part of the hydraulic characterization of the support volume targeted for high pressure stimulation which was completed before the drilling of boreholes for pressure temperature and geophysical monitoring shown by grey lines in fig 2c the two injection boreholes inj1 and inj2 used in this study were instrumented with deployable multi packer systems providing in total four packed off intervals fig 4 these four intervals were selected because they are located in between the s3 shear zones identified as the most permeable regions a summary of the setup including interval depths and lengths as well as the injection rate applied to the interval is provided in table 1 fluid pressures in packed off testing intervals were measured uphole using piezoresistive pressure transducers keller paa 23sy connected to a 2 m m id access line reporting to a downhole open port the pressure data resolution achieved under most test conditions was in the order of 0 5 kpa pressure sensors were connected to a multi channel data acquisition system to yield a continuous measurement record with a scan rate of 2 sec during each injection test a constant rate was maintained by bronkhorst cori flow mass flow controllers mounted in parallel on a flow board covering a total flow range of 0 03 to 667 ml min with a stated accuracy of 0 2 of the flow rate in order to carry out constant rate tests tap water sourced from the facility main water line was injected through one of the bronkhorst flow controllers all three controllers were connected through a rs232 flow bus interface to a main computer unit allowing to start control and stop fluid injection remotely 3 3 1 test implementation and field observations a total of four cross hole injection tests were conducted in a tomographic configuration with fluid injection taking place and pressure responses monitored successively at four intervals described in table 1 the intervals and the contours depicting shear zones are both displayed in fig 4 preliminary injection tests were conducted to verify that there is no leakage between the packers and the borehole wall each test consisted of injection a constant flow rate q table 1 in one interval while monitoring the pressure response in all the other intervals after 2 3 hours of monitoring the pressure buildup the injection was switched off and pressure recovery was monitored for an additional 2 4 hours prior to starting the next injection test the pressure was allowed to recover for each experiment note that the injection pressure applied during the tests presented in this study are much below the minimum principal stress 8 mpa i e the hydromechanical effects are likely minor field data are shown in fig 5 the full data set consists of 16 pressure transients i e responses of four intervals from four injection tests with injection pressures shown on the left y axes and cross hole pressure responses from observation intervals shown relatively on the right y axes for the data recorded a moving average smoothing with a span of 5 of the total number of data points was applied to remove the static sensor noise since the data were collected at a high frequency 0 5 hz the application of this filter did not significantly impact the shape of the pressure curve we note that even in the range where response is on the order of 1 kpa the noise does not completely mask the main tendencies for the application in this work data used in the inversion consist of hundred pressure responses measurements two hundreds for 20 000 s records chosen automatically per pressure curve 4 results in this section we present results of the proposed deterministic inversion method to ht field data first we simplify the fracture network geometry once the fracture network geometry has been simplified we apply the inverse modelling framework to estimate the transmissivities and storativities of hydraulically active fractures between and around the borehole pair 4 1 simplified discrete fracture network model using estimates of fracture strike and dip from geophysical logs and projecting the intersection of permeable packed off fractures with other boreholes we have found that the vast majority of discrete fracture traces are not continuous between boreholes this suggests that the permeable fractures in one borehole are either closed and do not intersect other boreholes or have been shifted hydraulic connections between boreholes are thus likely formed through complex three dimensional patterns of fractures to model pressure propagation between testing intervals the fracture network geometry has been simplified as described in section 2 1 a vertical fracture equally distanced from both boreholes and divided into three sections by horizontal fractures controls the degree of hydraulic connections between intervals depending on the relative position of the intervals different simplified geometries involving six fracture connectivity patterns could be proposed as a basis of the forward model fig 6 4 2 forward model the rectangular domain selected for the ht analysis has x y z dimensions of 100m 100m 20m considered to be much larger than the radius of influence of the injection tests based on results from hydraulic tests conducted in intact rock we assume that matrix is impermeable note however that the approach can be easily extended to fractured porous rock additional simulations not shown here indicate that assigning a uniform value of transmissivitiy within the matrix in a range 10 15 10 12 m 2 s induces no remarkable effects on the pressure propagation the model includes four horizontal rectangular fracture planes intersected by a vertical rectangular fracture plane the relative positions of horizontal fractures is fixed according to the tested connectivity pattern fig 6 note that boreholes are not explicitly modelled and testing intervals correspond to four points lying on horizontal fracture planes where fluxes q into the fracture were successively specified the points in the model are separated from one another by distances corresponding to distances in between the centers of packed off intervals 11 2 m distance for shallow intervals and 12 3 m distance for deeper intervals governing equations are solved with the finite element code comsol multiphysics 5 4 with a fine tetrahedral meshing the domain was discretized into 280 000 to 300 000 elements depending on the configuration the element size varied from 0 2 m to 1 5 m the finer elements are located around the fluid injection observation points and fractures and the coarser elements are located near the faces of the rock matrix various tests were performed to ensure mesh independent results we apply no flux boundary on all the faces of the rock matrix initial conditions were set by assuming hydrostatic conditions prior to the start of each cross borehole test we sequentially impose the injection rates q as point fluid injections table 1 and simulate transient pressure responses induced therefrom finally we use this forward model to determine fracture hydraulic properties by fitting the pressure transients using inverse analysis described in the section 2 3 4 3 inverse model to match the observed pressure transients a total of seven transmissivity and seven storativity estimates four horizontal fractures and three sections in the vertical fracture are adjusted for each fracture connection pattern fig 6 as part of the inversion procedure to this end we minimize the sum of squares misfit eq 5 between the measured data and model predictions using different values for the fracture hydraulic properties the values of hydraulic properties of the connecting fractures are permitted to vary within the ranges determined based on prior hydrogeological knowledge illman and tartakovsky 2006 martinez landa and carrera 2005 2006 hoehn and fierz 1990 for the logarithm of the transmissivity over the range l o g t 12 5 and for the logarithm of the storativity over the range l o g s 8 4 8 a set of 20 starting models is generated for each fracture connectivity pattern fig 6 to search for a global minimum of the objective functions yielding 20 estimates of the fracture hydraulic properties the final result corresponds to a set of hydraulic properties values leading to the lowest objective function value the starting points were selected randomly from a uniform distribution for the log transformed transmissivity and storativity over the same range as parameters note that the total number of starting points was limited by computing time for these model runs about 7 min for one direct simulation on a 2 6 ghz intel core i7 processor 8th generation with 16 gb of ddr4 ram while the solution converges generally after a 30 100 iterations first we impose the fracture network connectivity pattern shown in fig 6a and assume that s is spatially homogeneous while t is assumed to be spatially heterogeneous the cross section of fracture network model geometry with the distribution of log transmissivity values obtained after inversion is presented in fig 7 the results show that overall transmissivities of fractures connecting the inj1 inj2 borehole pair decrease with depth l o g t 1 log t 2 log t 3 7 7 8 8 8 in other words model captures the stronger hydraulic connectivity of shallow transmissive intervals inj1 i4 and inj2 i4 while the deeper intervals inj1 i3 and inj2 i3 are found to be less transmissive and less well connected for this case l o g s 6 was estimated to be the best fit value the solution obtained yields the best fit to pressure transients fig 8 compares the observed pressure responses after filtering solid lines and calibrated records of pressure dashed lines versus time for four injection tests the majority of the simulated responses in the injection intervals capture reasonably well the observed pressure behaviour while the exact temporal scaling of the pressure curves is not necessarily reproduced altogether the matches in the observation intervals are of intermediate quality and some of the matches are poor rmse values are provided in table 2 in particular the simulated cross borehole pressure responses are overestimated by a factor of 3 responses in inj2 i3 and inj1 4 in fig 8a response in inj1 i3 in fig 8b and c furthermore all responses observed when injecting in inj2 i4 fig 8d are underestimated by a factor of 4 we presume that the main reason for the inaccurate prediction of pressure transients in observation intervals is that we assumed in these simulations that s is homogeneous while field data indicates that it is spatially variable another possible reason could be related to the assumption about the connectivity pattern for example for the fracture connectivity pattern imposed here fig 6a the inj1 i4 interval cannot be connected to inj2 i3 interval without being connected to the inj1 i3 interval this contrasts with the pressure transients observed in inj1 i3 and in inj2 i3 in response to injecting in inj1 i4 fig 8b thus this connectivity pattern imposes geometrical constraints on fracture connections which are not necessarily compatible with the observed pressure trends to improve the match between simulated and measured pressure records we then in a second step allow the spatial variability of the log transmissivity and log storativity values under the hypothesis that t and s are independent variables we let the storativity of the fracture connections vary in a range given by s n 10 8 10 4 moreover we test different connectivity patterns as shown in fig 6 among these six connectivity patterns the connectivity pattern shown in fig 6b yields the best fit to pressure transients the corresponding distributions of the log transmissivity and log storativity values is given in fig 9 a and b respectively compared to the first case assuming log storativity to be spatially constant fig 7 the estimated t field is spatially more heterogeneous the results in fig 9a demonstrate that the most permeable fracture connection is the one in between inj1 i4 and inj2 i4 intervals while deeper fractures corresponding to s3 2 shear zone are found to be less permeable and less well connected these findings were also further confirmed through dna labeled nanoparticle and solute tracer tests conducted on site kittilä et al 2019 a comparison with tracer tests results is presented in section 5 furthermore the inversion procedure produces high storativity values for shallower fractures l o g s i n j 2 i 4 5 l o g s i n j 1 i 4 5 3 l o g s 1 5 higher storativity values imply slower pressure time responses of shallower fractures that our inverse approach compensates for by an increase in transmissivity of the vertical fractures connecting shallow intervals l o g t 1 log t 2 log t 3 5 7 6 2 8 9 a comparison of simulated versus observed pressure curves for all intervals for four injection tests is shown in fig 10 the model reproduces nearly accurately the pressure transients observed in response to injecting in intervals inj1 i3 and inj2 i3 fig 10a and c respectively fig 10b reveals that discrepancies occur between simulated and observed records in interval inj1 i3 finally although the model fits presented in fig 10d were improved pressure responses observed when injecting in inj2 i4 are still underestimated by a factor of 3 overall comparing these plots to fig 8 clearly shows a significant improvement in the model fit for purposes of comparison we used the root mean square error rmse given in table 2 the rmse values range from 0 15 to 6 2 kpa which are 1 2 18 times smaller than the model considering spatially homogeneous s and a different fracture network connectivity pattern for the same respective pressure responses to further assess the quality of data fit we provide a scatterplot of the measured versus computed pressure measurements in fig 11 data from different injection tests are represented with symbols of different colors a solid black line indicating a perfect 1 1 correlation is plotted and the dashed red line represents the best fit of observed and simulated values in general the points cluster along the 1 1 line with a minor bias the outliers plotting off the diagonal mainly correspond to the cross borehole responses induced by injection in the inj2 i4 interval cyan markers the inability of the proposed approach to match all pressure transients may indicate that i the chosen geometry is over simplified we acknowledge that this is probably the main source of uncertainty and or ii that the measured pressure response is influenced by boundary effects this is especially true for longer duration of the test and for high transmissivity fracture zones such as one intersecting inj2 i4 interval and or iii that the measured pressure response is influenced by effects of fracture dead ends fracture aperture variations and increase in contact area inducing non darcy flow in fractures e g quinn et al 2011 and or iv that the measured pressure response is may also be influenced by earth tides and ambient fluctuations of hydraulic heads which the proposed model does not take into account given these factors and potential measurement errors such as instrument noise of at least 0 5 kpa imperfect measurement modeling of injection flow rates etc we believe that this represents a good fit to the measurements furthermore we evaluate our model results by simulating an injection test which has not been utilized for the estimation of the t and s tomograms and compare the simulated and observed pressure records for this test note that the location of the injection used for model validation is the inj1 i3 interval i e the same as one of the constant injection tests included in the inverse model however during the test used for model validation water was injected as a step function step rate test q 20 40 and 60 ml min simulated and the observed pressure records are shown in fig 12 the rmse of the simulations of the validation step rate test is 0 2 kpa overall most of the simulated pressure variations capture the general trends of the observed variations demonstrating that t and s tomograms do a good job in predicting the pressure responses in these intervals 5 discussion and conclusions we present here a novel deterministic inversion framework to infer the connectivity and spatial distribution of log transmissivity and storativity of preferential permeable flowpaths in sparse fracture systems constrained by a series of hydraulic cross borehole tests the originality of the proposed approach is to focus on preferential flowpaths and characterize their connectivity based on discrete fracture network concepts rather than on equivalent media properties this permits to capture the effect of flow channeling thus leading to a realistic fracture pattern and connectivity the method proposed is successfully applied to hydraulic tomography data set obtained from fractured crystalline rocks at the grimsel test site in switzerland this study allowed to identify the key controlling fracture network connectivity pattern and hydraulic properties of the main flowpaths including the transmissivity and the storativity distribution in agreement with a previous study of castagna et al 2011 our results showed that keeping log storativity spatially constant results in an overall change of the inferred log transmissivity field and thus affects the degree of hydraulic connection between observation intervals we evaluated the quality of the inferred t and s estimates and connectivity distributions by comparing observed and calibrated transient pressure records and found that model predictions of pressure transients were generally good moreover we used additional data from a cross borehole step rate injection test to evaluate the inferred distribution of the fracture network transmissivity and storativity most calibrated responses accurately capture the observed pressure behaviors providing thus additional confidence in the validity of the inversion results even though the fracture network geometry is highly simplified our estimates of fracture connectivity are generally consistent with fracture connectivity analyses reported by other studies implemented as part of the isc experiment thus both ht field experiments and tracer tests kittilä et al 2019 confirm the existence of a network of connected fractures in between inj1 i4 and inj2 i4 intervals moreover no plausible breakthrough curves were detected in monitoring intervals in borehole inj1 from the tracer injected into interval inj2 i3 thus revealing that this interval is poorly connected to inj1 intervals this observation agrees well with the parameter distribution derived from our analysis and illustrated in fig 10 the connectivity pattern used in this study is found to agree relatively well with the backbone structure identified from a joint analysis of geological and hydrogeological data the two sets of horizontal fractures may be linked with the two subparallel s3 shear zones while the vertical linking fracture could represent the extension fractures identified from pressure pulse testing through extensive cross hole testing such fractures were found to allow fluids to migrate from one shear zone to another forming therefore a key linkage between the two major shear zones overall hydrogeophysical data provide a strong field evidence of channeling effects in solute and heat transport at the test site for the isc experiment in particular we carried out cross hole heat injection experiments with the injection taking place in inj2 i4 interval combined with temperature measurements along fiber optic cables installed in six boreholes brixel et al 2017 observed temperature breakthrough curves revealed that only few depth discrete well connected fractures contribute to heat transport giertzuch et al 2018 used single hole ground penetrating radar gpr surveys to monitor salt tracer plume propagating between the s3 shear zones using borehole antennas in single hole reflection mode a tracer injected in the inj2 borehole was monitored over time by exploiting difference imaging techniques the obtained gpr images revealed a tracer branching into two different flowpaths between the s3 shear zones nevertheless a detailed comparison of the delineated flowpaths is not possible at the current stage because i tracer transport flowpaths can differ from the major flowpaths in fractured rocks dorn et al 2012 ii our approach does not image the true fracture network geometry iii gpr images correspond to omni directional fracture projections these results highlight the importance of the characterization of the preferential flow paths rather than equivalent media properties or details of individual fractures as discussed above for some fracture patterns our simplified conceptual model introduces geometrical constrains on fracture connections a major restriction of the tested geometries is a limited number of independent possible paths in between observation intervals a data fusion approach combining geometrical and hydraulic information is expected to provide new insights into the complex structure of channeled flows in fractured media future works will set up a 3 d dfn model of the testing volume at the gts conditioned to hydraulic solute tracer tests kittilä et al 2019 data high resolution breakthrough curves from cross borehole thermal tracer tests conducted on site brixel et al 2017 and gpr reflection data giertzuch et al 2018 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the isc is a project of the deep underground laboratory at eth zurich established by the swiss competence center for energy research supply of electricity sccer soe with the support of the swiss commission for technology and innovation cti funding for the isc project was provided by the eth foundation with grants from shell and ewz and by the swiss federal office of energy through a p d grant the grimsel test site is operated by nagra the national cooperative for the disposal of radioactive waste we are indebted to nagra for hosting the isc experiment in their gts facility and to the nagra technical staff for onsite support the authors thank rené dorrer and reto seifert for their help in the field finally we thank five anonymous reviewers for their helpful comments and suggestions that improved the quality of the manuscript supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 103500 appendix a supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 
532,sediment transport in mountain and gravel bed rivers is characterized by bedload transport of a wide range of grain sizes when the bed is moving dynamic void openings permit downward infiltration of the smaller particles this process termed here kinetic sieving has been studied in industrial contexts but more rarely in fluvial sediment transport we present an experimental study of two size mixtures of coarse spherical glass beads entrained by turbulent and supercritical steady water flows down a steep channel with a mobile bed the particle diameters were 4 mm and 6 mm and the channel inclination 10 the spatial and temporal evolution of the segregating smaller 4 mm diameter particles was studied through the introduction of the smaller particles at a low constant rate into the large particle bedload flow at transport equilibrium particle flows were filmed from the side by a high speed camera using original particle tracking algorithms the position and velocity of both small and large particles were determined results include the time evolution of the layer of segregating smaller beads assessment of segregation velocity and particle depth profiles segregation resulted in the progressive establishment of a quasi continuous region of small particles reaching a steady state penetration depth the segregation dynamics showed a logarithmic time decreasing trend this evolution was demonstrated to be dependent on the particle streamwise shear rate which decays downwards exponentially this result is comparable to theories initially developed for dry granular flows keywords sediment transport bedload experimental segregation two phase flow granular flow particle tracking 1 introduction flooding hazards reproduction of salmonids earth landscapes are all heavily impacted by sediment transported by rivers yet after more than a century of research duboys 1879 gilbert 1914 there is no satisfactory theory available for sediment transport especially for bedload the coarser sediment load transported in contact with the stream bed empirical sediment rate formulas often poorly compare with field measurements by sometimes orders of magnitude gomez and church 1989 hinton et al 2018 an important factor impairing our ability to predict sediment transport is the wide range of grain sizes within bedload leading to size segregation also known as grain size sorting this phenomenon drastically modifies fluxes and results in patterns that can be seen ubiquitously in nature such as armoring bathurst 2007 this is particularly the case in mountain streams where steep slopes drive an intense transport of a wide range of grain sizes in this paper focus lies on vertical size segregation with controlled laboratory experiments using idealized spherical particles within a steep narrow flume two distinct size segregation phenomena occur both relevant to bedload flows whether the coarsest fractions of the bed are immobile or not spontaneous percolation and kinetic sieving respectively spontaneous percolation the size segregation by infiltration of fine sediment into an immobile gravel or boulder bed bridgwater and ingram 1971 has been extensively studied in fluvial geomorphology there have been diverse motivations to study the phenomena including the intrusion of fines into salmonid spawning beds placer mineral concentration and stratigraphical interpretation research has been undertaken experimentally beschta and jackson 1979 dermisis and papanicolaou 2014 dudill et al 2017 einstein 1968 gibson et al 2011 in the field frostick et al 1984 lisle 1989 and using models cui et al 2008 spontaneous percolation essentially by the effect of gravity can only occur for sufficiently high size ratios large to fine diameter depending predominantly on the grain size distribution and on the shape of the material however when the coarse bed is moving another type of segregation can take place even with a size ratio close to unity the moving bed acts as a sieve in which smaller particles are more likely to fall into gaps opened by shearing the typical net result is a downward flux of the smaller particles and an upward flux of larger particles causing vertical reverse segregation this gravity and shear driven phenomenon has been variously termed kinetic sieving middleton 1970 interparticle percolation drahun and bridgwater 1983 or gravity driven segregation gray 2018 it has been significantly studied in the powder and grain community concerned with industrial applications a landmark paper savage and lun 1988 proposed a model resulting from the combination of a random fluctuating sieve mechanism and squeeze expulsion whereby particles are squeezed upwards out of a layer the term kinetic sieving sometimes seems to be used somewhat restrictively to refer only to the model by savage and lun 1988 we will however use this colorful term to describe the entire shear driven reverse segregation phenomenon instead of a specific model kinetic sieving in stream channels has rarely been highlighted despite its morphodynamic relevance though to the best of our knowledge this term was coined by a geologist middleton 1970 note however that parker and klingeman 1982 described the process as vertical winnowing whereby small grains fall into holes created by the movement of the larger grains in the armored river bed surface accordingly we focus on kinetic sieving in the present contribution kinetic sieving has been studied experimentally in dry granular inclined chute flows savage and lun 1988 van der vaart et al 2015 wiederseiner et al 2011 the theory savage and lun 1988 evidenced a dependency of the segregation rate on the shear rate other theories rely on the kinetic theory of gasses fan and hill 2011 and mixture models have been proposed gajjar et al 2016 thornton et al 2006 tunuguntla et al 2014 although the effect of interstitial fluid on granular flow properties has received attention jain et al 2004 thornton et al 2006 segregation in bedload transport which is a granular flow shear driven by a free surface water flow has rarely been investigated ferdowsi et al 2017 in the laboratory at irstea a very efficient vertical and longitudinal sorting process was evidenced with bed sample measurements during bedload transport experiments on steep slopes with natural material mixtures bacchi et al 2014 recking et al 2009b subsurface fining occurred which was attributed to kinetic sieving dudill et al 2017 2018 dudill et al 2020 also performed experiments with spherical glass beads with a large range of grain size ratios ratio of large to fine diameter they found that below a ratio of 1 7 no spontaneous percolation occurred only kinetic sieving furthermore kinetic sieving excluding spontaneous percolation was first qualitatively investigated at irstea by frey and church 2009 and hergault et al 2010 in the same quasi 2d flume that will be used and described herein considering the importance of kinetic sieving for morphodynamics poor characterization and understanding of the underlying physical processes the aim of the present research is to investigate size segregation in turbulent bedload transport with the help of high resolution experiments thanks to innovative quantitative image analysis measurements the granular depth structure can be analyzed frey 2014 revil baudard et al 2015 allowing for a better process understanding exploring in particular the link between segregation and the shear rate such an unique dataset is also of high importance for the validation of refined numerical models either in the continuum chauchat 2018 cheng et al 2018 or in the discrete framework maurin et al 2015 2 experimental methods 2 1 experimental arrangement experiments were carried out in the narrow two meter long sediment feed flume hosted at the national research institute of science and technology for environment and agriculture irstea located in the university grenoble alpes campus france fig 1 the width of the channel was 6 5 mm and the slope s 10 5 7 uncertainty of the slope was 0 1 a steady water flow rate was ensured by a constant head reservoir and controlled with an electromagnetic flow meter less than 1 uncertainty two types of spherical glass particles were used each with a density of 2500 kg m3 the larger particles were black with a diameter d 6 0 3 mm provided by sigmund lindner gmbh germany and the smaller ones were transparent with a diameter d 4 0 3 mm provided by cimap france as the flume is only slightly wider than the diameter d of the large beads it allows for a quasi 2d movement to occur and greatly facilitates particle tracking numerical simulations have shown that the granular flow structure and fluid particle coupling observed in such a quasi 2d configuration exhibit the same mechanisms and signatures to that observed in full 3d configurations maurin et al 2015 this supports the relevance of the experimental setup in particular for the present analysis focusing on the granular behavior to prevent crystallographic type arrangements of spheres within the bed the flume has a steel bottom made of cylinders of the same diameters as the large beads but located at random heights a 40 mm high obstacle was positioned at the downstream end of the flume so as to permit bed formation of about 6 7 layers of large beads the origin of the elevation y was taken at the top of this obstacle fig 1 and the elevation normalized by the large diameter d will be denoted y the procedure to obtain transport equilibrium with the large beads followed several steps first the large glass beads were injected at the desired rate from a reservoir with a motorized wheel equipped with 20 hollows on its circumference the feed rate was constant within 4 the water supply was subsequently adjusted to make the bed line parallel to the channel base finally we ensured that transport equilibrium was reached by transport equilibrium we mean that the outgoing sediment rate was equal to the input rate and that there was no more aggradation nor degradation i e no positive or negative change of the bed slope over a sufficiently long time interval the outflowing sediment rate was monitored for approximately 20 min to ensure that the transport equilibrium condition was respected the outgoing sediment rate was measured during 1 min at least three times during the 20 min period if these three values were consistently equal to the ingoing sediment rate within 8 transport equilibrium was assumed we focused in this study on both a low and a high flow discharge experiment with a feed rate of approximately 6 and 20 large beads s 0 10l s m and 0 35l s m these experiments will hereafter be defined as s6 and s20 when the above procedure was successfully completed the smaller transparent beads were input upstream of the recorded image field at a very low rate of one bead every two seconds 2 58 10 3 liter s m to study the vertical segregation process the sediment rate of small beads was chosen sufficiently low only 2 5 of the rate of large beads for s6 and less than 1 for s20 to preserve transport equilibrium no significant change of the slope was subsequently observed nor adjustment of the flow rate necessary 2 2 experimental parameters we give in table 1 all relevant hydrodynamic and sedimentological parameters the sediment transport rate imposed at the inlet is given both in absolute experimental values n 0 in beads s and per unit width qs in m2 s qw is the flow rate per unit width the water depth h was obtained from the water free surface and bed surface detections by image processing see details in next subsection the water depth fluctuated significantly over the experiment duration and along the main stream direction for data calculated over a duration of 0 2 s the standard deviation was 0 7 mm in table 1 the water depth is averaged over time and space all subsequent parameters in table 1 are calculated from the water depth u f q w h is the mean fluid velocity the froude number is defined as f r u f g h where g denotes gravity acceleration using the so called einstein 1942 sidewall correction flow dissipation by the smooth sidewalls was taken into account by calculating a bottom hydraulic radius rhb the method is explained in frey et al 2006 this allowed us to assess a more consistent shields number θ b r h b s s 1 d than if it were calculated with the water depth s ρs ρ 2 5 is the ratio of sediment to water density the flow was turbulent reynolds number above 4000 and hydraulically rough the steep slope of the flume meant that the flow was supercritical froude number of 1 2 1 3 and that the submergence h d the ratio of flow depth to the particle diameter was low at 1 6 and 2 8 despite these unusual conditions the water flow in this flume has been shown still to behave classically as in larger flumes in particular with the presence of a logarithmic layer frey and reboud 2001 moreover one size sediment rates have been shown to be in good agreement dudill et al 2017 with classic semi empirical bedload formulae such as meyer peter and müller 1948 the shields number ranged from 0 081 to 0 115 this is about two times the classical incipient motion value as all beads moved frequently with only some periods of rest the flow can be classified in stage 3 warburton 1992 or termed full mobility bedload frey and church 2011 however the flow was well below the intense bedload range depicted by capart and fraccarollo 2011 2 3 image acquisition all experiments were recorded using a baumer hxc13 high speed camera the camera was positioned perpendicular to the sidewalls approximately 90 cm upstream from the channel outlet and inclined at the same angle as the channel fig 1 a high bright white led backlight panel was positioned behind the channel and provided stable and uniform lighting the camera recorded at a rate of 130 frames per second and the image resolution was 1280 320 pixels which corresponds approximately to a window of 260 mm by 65 mm up to 500 000 images were recorded over one hour images are shown in fig 3 preliminary test runs showed that a duration of about one hour was sufficient for s6 and 45 min for s20 to obtain a steady state segregation pattern characterized by a quasi continuous region of small beads fig 3 f and l display an image of each run at a time near the end of the experiment when the steady state segregation pattern had been reached see also phenomenological descriptions in section 3 in preliminary tests the smaller beads were input exactly at the upstream corner of the recorded image field of view beads then began to saltate and roll the majority of the introduced beads never infiltrated into the bed and rapidly left the field of view some beads however were caught in the bed and infiltrated immediately downstream of the feeder position at steady state a region with only a very low number of infiltrated small beads could be identified between the feed location and the upstream position of the quasi continuous layer the length between this position which fluctuated somewhat with time and the feed location could be estimated to approximately 10d for s6 see fig 3 f and 35d for s20 for context the total length of the image field of view is 44d as a result in run s20 the quasi continuous region had a length of only 9d which meant insufficient small beads in the field of view to perform any consistent statistics therefore for the higher rate run s20 we decided to input the smaller beads upstream of the upper left corner at a distance of 44d which yielded a quasi continuous small bead region over the entire image see fig 3 l this was not necessary for run s6 since the quasi continuous region was larger with a sufficient number of infiltrated small beads see fig 3 f 2 4 image processing thanks to a sufficient resolution and frame rate acquired sequences of images contain enough information to extract the trajectories of individual beads the algorithm to obtain the individual trajectory of each object relies on a two step deterministic approach 1 object detection 2 object tracking from frame to frame hergault et al 2010 lafaye de micheaux et al 2018 in parallel characteristic lines such as the water free surface and bed surface can be automatically detected thanks to segmentation methods with a dedicated image processing chain lafaye de micheaux et al 2018 2015 all the algorithms coded in matlab are included in a package called beadtracking available on https github com hugolafaye beadtracking moreover a ground truth consisting of a 1000 frame experimental image sequence was used for validation and is available on zenodo https doi org 10 5281 zenodo 3454628 the entire trajectory dataset for both runs s6 and s20 is available on https doi org 10 5281 zenodo 3407127 in addition 18 sequences of raw images corresponding to data used in fig 5 are also included 9 sequences for run s6 and 9 sequences for run s20 2 4 1 object detection for each image the object detection phase aims at detecting all beads fig 2 a by image processing first black large beads are detected by thresholding the image to keep only black pixels fig 2b eroding to isolate black beads fig 2c and then identifying the center of connected components fig 2d this detector is very efficient since black beads are not overlapping and are easily distinguishable from other kind of beads 6 mm diameter beads in a channel width of 6 5 mm it returns the set of black bead centers then black beads are erased by replacing their gray levels by those of the background fig 2e the background is reconstructed through a morphological closing with a disk shaped structuring element radius just greater than bead radius this makes transparent beads more discernible secondly since transparent beads appear as faint dark rings of different shapes because of their neighboring configurations 4 mm diameter beads can be partially overlapped by others we use a specific chain of morphological operations to detect them initially developed by hergault et al 2010 we apply a hconvex operator soille 1999 on the image obtained at the previous step fig 2f then a normalized cross correlation with a ring shaped model fig 2g local maxima extraction and maxima selection according to an adjusted threshold fig 2h this returns the set of transparent bead centers 2 4 2 object tracking to assign at most one detection to at most one tracker i e each physical trajectory is estimated by an individual internal state called tracker a detection tracker association process is needed due to the high number of detections and the long time series the association was done deterministically with a greedy algorithm wu and nevatia 2007 given the matching cost between all possible detection tracker pairs the greedy algorithm iteratively selects the best candidate and removes the corresponding concurrent associations the matching cost between a detection and a tracker consists of a combination of two factors one based on the distance from the detection position to the predicted position of the tracker the other based on the tracker velocity at the previous time step the distance term promotes the detections closest to the prediction with the velocity term we promote low velocities as it allows better associations in case of bead collisions indeed when a bead bounces its predicted position assuming a constant velocity model can fall very close to another bead leading to the same distance term for the corresponding trackers in this case adding a velocity term to the matching cost increases the cost of higher velocities much more than lower ones and so favors choosing lower velocities first during the greedy data association to limit the number of possible detection tracker pairs the set of possible detections associated to a given tracker is limited to the detections located inside a circular region centered at the predicted position given by this tracker this predicted position is determined assuming a constant velocity model in the end the tracking algorithm returns the set of estimated trajectories over the sequence of images examples of trajectories are given on fig 2b and f 2 4 3 water surface and bed line detection both water free surface and bed surface detections are based on a combination of image analysis techniques such as morphological operations gradient calculations and watershed transformations beucher and lantuejoul 1979 vincent and soille 1991 the water free surface is detected by first removing detected beads then amplifying it by bottom hat filtering and then applying a marker controlled watershed to finally return the water free surface as a thin line the bed surface is detected by first computing the average image over n consecutive images removed from their detected water free surface then computing a gradient magnitude of the obtained image and then applying a marker controlled watershed to finally return the bed surface as a thin line these detected lines allowed the derivation of water depth as the difference between the water free surface and the bed line 2 5 post processing results of both the center location and the velocity of each bead are used to compute the depth profiles we partition each image into equally sized bins parallel to the x streamwise direction comprising the entire length of the image and the thickness of n pixels in the normal direction the procedure is similar to that of hill and fan 2008 and maurin et al 2015 for each elevation the volumetric solid concentration c is computed as the sum of all volume portions of beads present in the bin vs divided by the total volume of the bin v vs is simple to calculate since the volume of a slice of a sphere can be evaluated analytically the average particle velocity up is computed similarly with the velocity of each bead weighted by its portion present in the bin averaging is made over all images within a considered temporal sequence using a bin of thickness 1 pixel resulted in too noisy profiles testing with larger bins from 30 pixels down to 1 pixel showed that a bin of 15 pixels half the diameter of a large bead ensured a readable and still converging profile particular attention was paid to ensure that the spatial gradients were still preserved the particle transport rate per unit width at each elevation was defined as qs upx c m2 s m we defined the bedload layer as all elevations exhibiting a particle transport rate higher than 2 of its maximum value we seek to follow the temporal evolution of the segregation pattern to achieve this we need to choose temporal sequences comprising a sufficient number of images this allows us to reach statistical consistency but the sequence should not be too long to ensure a small variation of variables within the sequence temporal sequences were chosen with a number of images between 8000 and 10 000 corresponding to a duration of 61 s to 77 s in the legend of figures the median time is used to label the temporal sequences the initial time 0 is defined as the time when the first small bead was introduced 3 results 3 1 phenomenology before giving quantitative results we will describe the qualitative evolution of both runs s6 and s20 with the help of sequences of images fig 3 while the majority of introduced small beads saltate and leave the downstream end of the image some beads infiltrate through the large beads by kinetic sieving they tend to form clusters very rapidly though some rare beads remain isolated see fig 3b and h for both runs at 138 s the number and the size of these clusters increase see fig 3c and i for both runs at 600 s before progressively merging to form a quasi continuous layer of small beads see run s6 at 1522s and 2509s fig 2d and e and run s20 at 1200 and 1800s fig 2j and k the evolution is then slower before reaching a dynamic steady state final segregation pattern see fig 3f and l for s6 and s20 in neither run did any bead reach the steel bottom at the final stage the elevation of the bottom of the small bead layer does not seem to vary any more this elevation that we will call penetration depth is clearly identifiable and depends on the run considered a higher penetration depth is observed for s20 with a larger sediment rate corresponding to a higher shields number we will analyze below this important feature with the help of concentration profiles once infiltrated in the bed most small beads remained trapped during the entire experiment however the shear driven general movement meant that some beads in the upper layers could be re caught in the bedload layer and finally disappear from the image shearing also meant that several beads could progressively disappear or appear through the downstream or upstream edge of the image field 3 2 initial one size depth structure we recall here the principal characteristics of the depth structure once transport equilibrium is established for flows composed only of one size of the large beads more details can be found in frey et al 2014 fig 4 gives the depth profiles of the particle velocity the volumetric solid concentration and the particle rate for both s6 and s20 the velocity profile is characterized from bottom to top by an exponential decay a linear part and a logarithmic like region the velocity of s20 is always higher than that of s6 at all elevations in the bed the solid concentration is maximal with a mean value of about 0 53 some oscillations are reminiscent of a layered structure above y 0 i e above the top of the downstream obstacle there is a dramatic decrease up to the free surface from y 0 to 1 the concentration is smaller for s20 than for s6 above y 1 it is the reverse due to a higher number of beads in saltation for s20 the sediment rate is the product of velocity and concentration as defined in postprocessing in our case when the concentration is low the velocity is high and vice versa the sediment rate peaks around y 1 for both runs and has an approximate gaussian shape though skewed downwards the bedload layer as defined above with values of sediment rate larger than 2 of its maximum value ranged from y 1 to 2 5 for run s6 and from 2 to 2 8 for run s20 the region below the bedload layer is characterized by low particle velocities hence the use of the term quasi static flow or creeping flow houssais et al 2015 since particle velocities are small in this region it does not practically contribute to the bedload rate however the properties of this region are of utmost importance for the evolution of vertical segregation of particular interest is the exponential decay of the particle velocity see inset of fig 4 and hence of the shear rate gradient of the velocity the consequences of this interesting property will be used in the discussion section where a link between segregation and shear rate will be evidenced 3 3 concentration profiles of small particles fig 5 shows the time evolution of small bead concentration profiles for both runs s6 and s20 the concentration x axis scale is the same and the y axis depicts a range of 5 5 diameters below y 1 for run s6 and below 0 for run s20 same range with a translation of 1d the same shape can be observed for both runs the concentration profiles are quasi gaussian though skewed towards the bottom right from the beginning and at all times the profile of s20 peaks at a lower elevation than s6 at the final stage the profile of s20 is slightly thicker than s6 approximately 4d vs 3 5d looking at the time evolution on fig 5 see also fig 3 with the images there is a decrease of the peak vertical position relatively sharp for s20 more diffuse for s6 there is clearly a rapid increase of concentration in the lowest layers in both runs it is remarkable that the lowest limbs approximately collapse onto a single curve this behavior related to the penetration depth will now be analyzed in more details with the help of concentration isolines 3 4 concentration isolines fig 6 shows for the two runs the concentration isolines in the elevation time space each isoline corresponds to a constant concentration value chosen as 0 01 0 02 0 05 and 0 1 for each concentration value there is a lower and an upper isoline hence 2 series of 4 curves on each graph in both runs the lower isolines decrease with time first sharply then more gradually for finally reaching a steady penetration depth by contrast the upper isolines are more or less horizontal showing no trend comparing globally the upper isolines it is observed that in run s20 they are located 1 diameter below those of run s6 and for the lower isolines this difference is approximately 1 5 diameter at final steady state for instance at a concentration of 0 02 the upper and lower isolines are around y 0 and 3 25 for s6 and around y 1 and 5 for s20 which results in a thickness of respectively 3 25d and 4d for a concentration of 0 05 the upper and lower isolines are around y 0 5 and 3 for s6 and around y 1 5 and 4 5 for s20 which makes a thickness of respectively 2 5d and 3d so these figures also indicate a slightly larger thickness of the small bead region for s20 clearly the evolution of the lower isolines is very dynamic at the beginning fig 7 shows a semi log plot of the temporal evolution of each lower isoline in run s20 a logarithmic time decreasing trend is observed for all lower isolines up to a time between 1000 and 2000 s it is particularly obvious for c 0 01 and 0 02 in run s6 the logarithmic time decreasing trend is also observed but less obviously more quantitatively we have fitted the experimental isolines with logarithmic curves for the 3 cases c 0 01 0 02 0 05 a being the absolute slope value of the fitted line and b the y intercept 1 y c a log t b table 2 gives the fitted values for each concentration isoline and for both runs together with the coefficient of determination run s20 with a higher shields number displays larger absolute slope values ranging from 1 95 to 2 13 denoting a faster segregation rate than run s6 ranging from 1 21 to 1 37 for both runs regardless of the concentration c 0 01 0 02 and 0 05 the slope of the isolines is very similar which is remarkable 3 5 normal small particle velocity image analysis allows us to track each bead and thus to compute the normal small particle velocity i e orthogonal to the bed slope which is on average negative since kinetic sieving implies an average downward movement in fig 8 where the absolute downward velocity upy is plotted we focus on the dynamic evolution taking place until about 500 600 s see fig 3 c and i for all elevations the downward velocity of run s20 is noticeably higher than for run s6 for both runs s6 and s20 and for all time sequences the downward velocity decreases sharply in the dynamic layer between y 1 and about y 0 then more gradually in the region between y 0 and y 1 5 at initial times the downward velocity is significant in the region around y 0 s6 and 1 s20 then it approaches zero at later times more specifically in run s20 the mean downward velocity decreases to 1 5 10 4 m s at an elevation of y 1 5 at time 138 s brown curve whereas the same velocity is reached at a higher elevation of about 0 5 at the later time of 576 s magenta curve in run s6 this time evolution can also be observed though less clearly for example at y 0 the velocity has a mean value of 3 10 4 m s at 64 s dropping to 0 15 10 4 m s at 495 s the intermediate profiles display a value of about 10 4 m s essentially the velocity tends to approach 0 at a higher elevation as time increases and as the quasi continuous small bead layer is building see fig 3 displaying the temporal evolution of the images an assumption is that this behavior is controlled by the dynamic conditions prevailing at the bottom of this quasi continuous layer notably by the shear rate behavior since the normal motion of the bottom of the evolving quasi continuous small bead region progressively decreases the normal movement of beads located above becomes hindered and hence the downward small particle velocity in the upper layers tends to decrease as evidenced in fig 8 we will explore this assumption further in the following discussion section 4 discussion 4 1 shear rate dependence to discuss the time evolution of the concentration isolines we will define the segregation rate as d y c d t with yc the elevation corresponding to the isoconcentration c a number of theories may et al 2010 savage and lun 1988 have proposed a linear dependence of the segregation rate to the shear rate defined as γ d u p x d y 2 d y c d t γ recall that the streamwise velocity profile is characterized by an exponential decay u p x e y λ with λ the exponential decay constant see semi log plot in fig 9 this means that the shear rate derivative of the velocity also follows an exponential decay γ c 0 e y λ with c0 the shear rate at y 0 3 hence d y c dt c 0 e y c λ separating variables and integrating from time t 0 and initial position gives y c λ log t λ c 0 c 1 with c 1 a constant of integration when t 1 c 1 is negligible and hence 4 y c λ log t c 2 we have seen that the experimental small particle concentration isolines decrease as a logarithmic function of time see fitting in fig 7 and eq 1 yc alog t b therefore if the segregation rate actually linearly depends on the shear rate the absolute value of the slope of the isolines a should be equal to λ the slope of the velocity profile let us test this assumption on our data on one hand recall table 2 that fitting on the lower isolines c 0 01 0 02 and 0 05 yielded a slope value ranging from 1 21 to 1 37 mean of 1 27 in run s6 and from 1 95 to 2 13 mean of 2 06 in run s20 on the other hand streamwise velocity profiles were fitted over the exponential domain fig 9 in the same y region where the three isolines evolve c 0 01 0 02 and 0 05 in run s6 for y between 3 and 0 5 the exponential decay constant λ equals 1 37 coefficient of determination r2 of 0 995 in run s20 for y between 5 and 2 this constant equals 2 11 r2 of 0 986 therefore the agreement between the exponential decay constant λ derived from the velocity profile and the time constant derived from the logarithmic decay of the concentration isolines is good with an average difference of only 8 in run s6 λ 1 37 vs a 1 27 and 2 in run s20 λ 2 11 vs a 2 06 to conclude the present analysis shows that the particle shear rate drives vertical segregation at first order in turbulent bedload transport this result is similar to what has been obtained in dry granular flows may et al 2010 savage and lun 1988 it suggests that the mechanisms observed in turbulent bedload transport are not different from the ones observed in dry granular flows this size segregation mechanism results from the relative displacement of grains between the granular layers following savage and lun 1988 the creation of holes in the granular media due to the shear rate induces a net downward small particle flux linked with a higher probability for a small particle to fall into a hole than for a large one in addition the parallel with dry granular flows suggests that the models designed for such flows could be used in the future to model size segregation in turbulent bedload transport thornton et al 2006 tunuguntla et al 2014 similarly the shear rate dependence could be included in the segregation flux as described by may et al 2010 4 2 penetration depth an important question is whether there is a cut off shear rate under which no kinetic sieving is possible this would explain the constant values reached by the lower isolines and hence the possibility to define the penetration depth our experiments showed that no evolution of the penetration depth was observed after about 40 min in run s6 and 30 min in run s20 at this point examination of the isolines gives a penetration depth of about 3d for s6 and 5d for s20 this observation may be put into perspective with experimental or field measurements of infiltration resulting in maximum values of 2 to 5d90 with d90 the diameter at which 90 of the coarse bed particle is finer however in field measurements it is nearly impossible to distinguish between the evolution of the bedload layer and scour and fill phenomena due to bedform formation and travelling notably bars dunes and antidunes recking et al 2009 the depth of buried tracers is indeed the result of both phenomena a discussion as how to distinguish and simply name both phenomena was provided by church and haschenburger 2017 therefore most investigations have been made in hydraulic flumes and moreover with stable gravel beds for example beschta and jackson 1979 reported infiltration down to 2 5 5d90 similar values were reported by gibson et al 2009 who underlined that the seal called bridging did not occur at a very definite elevation but that the fine content decayed exponentially with the depth whilst most field studies have reported infiltration due to the scour and fill process some field experiments by lisle 1989 were conducted over a relatively plane moving bed and resulted in a seal thickness of 2 6 3 6 d90 to conclude although the experimental conditions of our idealized experiments could at first sight be considered quite different from field conditions the reported penetration depths have the same relative values however since the velocity and the shear rate profiles are exponential and if the proportionality assumption between the segregation rate and the shear rate still holds in the lowermost layers simple theories such as savage and lun 1988 predict that the penetration depth should continue to evolve with no cutoff value we recently conducted preliminary coupled fluid discrete simulations frey et al 2017 that indeed showed no cutoff value for the penetration depth to summarize we have two series of seemingly contradictory results on the one hand observations in the laboratory including our idealized experiments as well as in the field tend to show a maximal penetration depth on the other hand preliminary coupled fluid discrete simulations and simple theories show no cutoff value of the penetration depth with the system therefore never reaching a steady state further comments can be made to reconcile these two observations since the granular shear rate exhibits an exponential decrease in the bed the expected downward segregation motion of the small particles should scale with the logarithm of time therefore the experimental duration might be too short to observe further motion in the present case roughly speaking the time needed for a small particle to move down is ten times greater every length of the large diameter it is therefore possible that our one hour long experiment needs to be run for considerably longer durations ten hours would be needed to possibly see an added one large diameter long descent and hundred hours for two diameters at present experiments of this duration would be very difficult or even impossible to arrange with respect to field observations it should be kept in mind here that the system is idealized considering only steady and uniform configurations in a bi disperse granular bed the large grain size distribution and the non uniform conditions bacchi et al 2014 observed in the field could have important effects further work is required to understand physical mechanisms in idealized bedload transport and extend the analysis to more complex and realistic cases 4 3 high resolution dataset for model validation bedload transport is essentially modeled with engineering oriented shallow water equations together with the exner mass conservation equation and with a semi empirical excess shear stress transport rate formula e g formula of meyer peter and müller 1948 or smart and jaeggi 1983 however in this framework there is no information on the normal vertical direction which is crucial for the study of vertical size segregation more recently lagrangian discrete element models coupled with fluid solvers have been applied to bedload maurin et al 2018 2015 likewise continuous eulerian two phase flow models have been developed with the use of an appropriate granular rheology cheng et al 2018 multiphase continuum models and multi sized fluid coupled discrete element models are now under development it is therefore crucial to have high resolution spatio temporal datasets to be able to validate those models the dataset described in this paper can fulfill this goal 5 conclusion we have analyzed the temporal evolution of new kinetic sieving experiments following the introduction of a low rate of smaller particles in a larger particle bedload flow at transport equilibrium segregation results in the progressive development of a quasi continuous region of small particles reaching a steady state penetration depth with time image processed concentration depth profiles and isolines showed a logarithmic time decrease we demonstrated that this evolution was dependent at first order on the streamwise shear rate at the bottom of the small particle region which decays downwards exponentially this result is in line with several theories originally developed for dry granular flows however this theory does not allow the system to reach a maximal constant penetration depth as evidenced by our experimental data as well by other field or experimental data further research is therefore necessary to reconcile these two observations finally this high resolution spatio temporal dataset could be used to develop and validate multi size fluid coupled discrete element models as well as eulerian multiphase models declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the work was funded by agence nationale de la recherche project segsed anr 16 ce01 0005 the work was also partly funded by the french institut national des sciences de l univers program ec2co biohefect dril and labex osug 2020 investissements d avenir anr10 labx56 the authors acknowledge the support of irstea formerly cemagref h lafaye de micheaux also acknowledges funding provided by grants from la région rhône alpes this work would not have been possible without the technical support of f ousset in the laboratory at irstea supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 103478 appendix supplementary materials image application 1 
532,sediment transport in mountain and gravel bed rivers is characterized by bedload transport of a wide range of grain sizes when the bed is moving dynamic void openings permit downward infiltration of the smaller particles this process termed here kinetic sieving has been studied in industrial contexts but more rarely in fluvial sediment transport we present an experimental study of two size mixtures of coarse spherical glass beads entrained by turbulent and supercritical steady water flows down a steep channel with a mobile bed the particle diameters were 4 mm and 6 mm and the channel inclination 10 the spatial and temporal evolution of the segregating smaller 4 mm diameter particles was studied through the introduction of the smaller particles at a low constant rate into the large particle bedload flow at transport equilibrium particle flows were filmed from the side by a high speed camera using original particle tracking algorithms the position and velocity of both small and large particles were determined results include the time evolution of the layer of segregating smaller beads assessment of segregation velocity and particle depth profiles segregation resulted in the progressive establishment of a quasi continuous region of small particles reaching a steady state penetration depth the segregation dynamics showed a logarithmic time decreasing trend this evolution was demonstrated to be dependent on the particle streamwise shear rate which decays downwards exponentially this result is comparable to theories initially developed for dry granular flows keywords sediment transport bedload experimental segregation two phase flow granular flow particle tracking 1 introduction flooding hazards reproduction of salmonids earth landscapes are all heavily impacted by sediment transported by rivers yet after more than a century of research duboys 1879 gilbert 1914 there is no satisfactory theory available for sediment transport especially for bedload the coarser sediment load transported in contact with the stream bed empirical sediment rate formulas often poorly compare with field measurements by sometimes orders of magnitude gomez and church 1989 hinton et al 2018 an important factor impairing our ability to predict sediment transport is the wide range of grain sizes within bedload leading to size segregation also known as grain size sorting this phenomenon drastically modifies fluxes and results in patterns that can be seen ubiquitously in nature such as armoring bathurst 2007 this is particularly the case in mountain streams where steep slopes drive an intense transport of a wide range of grain sizes in this paper focus lies on vertical size segregation with controlled laboratory experiments using idealized spherical particles within a steep narrow flume two distinct size segregation phenomena occur both relevant to bedload flows whether the coarsest fractions of the bed are immobile or not spontaneous percolation and kinetic sieving respectively spontaneous percolation the size segregation by infiltration of fine sediment into an immobile gravel or boulder bed bridgwater and ingram 1971 has been extensively studied in fluvial geomorphology there have been diverse motivations to study the phenomena including the intrusion of fines into salmonid spawning beds placer mineral concentration and stratigraphical interpretation research has been undertaken experimentally beschta and jackson 1979 dermisis and papanicolaou 2014 dudill et al 2017 einstein 1968 gibson et al 2011 in the field frostick et al 1984 lisle 1989 and using models cui et al 2008 spontaneous percolation essentially by the effect of gravity can only occur for sufficiently high size ratios large to fine diameter depending predominantly on the grain size distribution and on the shape of the material however when the coarse bed is moving another type of segregation can take place even with a size ratio close to unity the moving bed acts as a sieve in which smaller particles are more likely to fall into gaps opened by shearing the typical net result is a downward flux of the smaller particles and an upward flux of larger particles causing vertical reverse segregation this gravity and shear driven phenomenon has been variously termed kinetic sieving middleton 1970 interparticle percolation drahun and bridgwater 1983 or gravity driven segregation gray 2018 it has been significantly studied in the powder and grain community concerned with industrial applications a landmark paper savage and lun 1988 proposed a model resulting from the combination of a random fluctuating sieve mechanism and squeeze expulsion whereby particles are squeezed upwards out of a layer the term kinetic sieving sometimes seems to be used somewhat restrictively to refer only to the model by savage and lun 1988 we will however use this colorful term to describe the entire shear driven reverse segregation phenomenon instead of a specific model kinetic sieving in stream channels has rarely been highlighted despite its morphodynamic relevance though to the best of our knowledge this term was coined by a geologist middleton 1970 note however that parker and klingeman 1982 described the process as vertical winnowing whereby small grains fall into holes created by the movement of the larger grains in the armored river bed surface accordingly we focus on kinetic sieving in the present contribution kinetic sieving has been studied experimentally in dry granular inclined chute flows savage and lun 1988 van der vaart et al 2015 wiederseiner et al 2011 the theory savage and lun 1988 evidenced a dependency of the segregation rate on the shear rate other theories rely on the kinetic theory of gasses fan and hill 2011 and mixture models have been proposed gajjar et al 2016 thornton et al 2006 tunuguntla et al 2014 although the effect of interstitial fluid on granular flow properties has received attention jain et al 2004 thornton et al 2006 segregation in bedload transport which is a granular flow shear driven by a free surface water flow has rarely been investigated ferdowsi et al 2017 in the laboratory at irstea a very efficient vertical and longitudinal sorting process was evidenced with bed sample measurements during bedload transport experiments on steep slopes with natural material mixtures bacchi et al 2014 recking et al 2009b subsurface fining occurred which was attributed to kinetic sieving dudill et al 2017 2018 dudill et al 2020 also performed experiments with spherical glass beads with a large range of grain size ratios ratio of large to fine diameter they found that below a ratio of 1 7 no spontaneous percolation occurred only kinetic sieving furthermore kinetic sieving excluding spontaneous percolation was first qualitatively investigated at irstea by frey and church 2009 and hergault et al 2010 in the same quasi 2d flume that will be used and described herein considering the importance of kinetic sieving for morphodynamics poor characterization and understanding of the underlying physical processes the aim of the present research is to investigate size segregation in turbulent bedload transport with the help of high resolution experiments thanks to innovative quantitative image analysis measurements the granular depth structure can be analyzed frey 2014 revil baudard et al 2015 allowing for a better process understanding exploring in particular the link between segregation and the shear rate such an unique dataset is also of high importance for the validation of refined numerical models either in the continuum chauchat 2018 cheng et al 2018 or in the discrete framework maurin et al 2015 2 experimental methods 2 1 experimental arrangement experiments were carried out in the narrow two meter long sediment feed flume hosted at the national research institute of science and technology for environment and agriculture irstea located in the university grenoble alpes campus france fig 1 the width of the channel was 6 5 mm and the slope s 10 5 7 uncertainty of the slope was 0 1 a steady water flow rate was ensured by a constant head reservoir and controlled with an electromagnetic flow meter less than 1 uncertainty two types of spherical glass particles were used each with a density of 2500 kg m3 the larger particles were black with a diameter d 6 0 3 mm provided by sigmund lindner gmbh germany and the smaller ones were transparent with a diameter d 4 0 3 mm provided by cimap france as the flume is only slightly wider than the diameter d of the large beads it allows for a quasi 2d movement to occur and greatly facilitates particle tracking numerical simulations have shown that the granular flow structure and fluid particle coupling observed in such a quasi 2d configuration exhibit the same mechanisms and signatures to that observed in full 3d configurations maurin et al 2015 this supports the relevance of the experimental setup in particular for the present analysis focusing on the granular behavior to prevent crystallographic type arrangements of spheres within the bed the flume has a steel bottom made of cylinders of the same diameters as the large beads but located at random heights a 40 mm high obstacle was positioned at the downstream end of the flume so as to permit bed formation of about 6 7 layers of large beads the origin of the elevation y was taken at the top of this obstacle fig 1 and the elevation normalized by the large diameter d will be denoted y the procedure to obtain transport equilibrium with the large beads followed several steps first the large glass beads were injected at the desired rate from a reservoir with a motorized wheel equipped with 20 hollows on its circumference the feed rate was constant within 4 the water supply was subsequently adjusted to make the bed line parallel to the channel base finally we ensured that transport equilibrium was reached by transport equilibrium we mean that the outgoing sediment rate was equal to the input rate and that there was no more aggradation nor degradation i e no positive or negative change of the bed slope over a sufficiently long time interval the outflowing sediment rate was monitored for approximately 20 min to ensure that the transport equilibrium condition was respected the outgoing sediment rate was measured during 1 min at least three times during the 20 min period if these three values were consistently equal to the ingoing sediment rate within 8 transport equilibrium was assumed we focused in this study on both a low and a high flow discharge experiment with a feed rate of approximately 6 and 20 large beads s 0 10l s m and 0 35l s m these experiments will hereafter be defined as s6 and s20 when the above procedure was successfully completed the smaller transparent beads were input upstream of the recorded image field at a very low rate of one bead every two seconds 2 58 10 3 liter s m to study the vertical segregation process the sediment rate of small beads was chosen sufficiently low only 2 5 of the rate of large beads for s6 and less than 1 for s20 to preserve transport equilibrium no significant change of the slope was subsequently observed nor adjustment of the flow rate necessary 2 2 experimental parameters we give in table 1 all relevant hydrodynamic and sedimentological parameters the sediment transport rate imposed at the inlet is given both in absolute experimental values n 0 in beads s and per unit width qs in m2 s qw is the flow rate per unit width the water depth h was obtained from the water free surface and bed surface detections by image processing see details in next subsection the water depth fluctuated significantly over the experiment duration and along the main stream direction for data calculated over a duration of 0 2 s the standard deviation was 0 7 mm in table 1 the water depth is averaged over time and space all subsequent parameters in table 1 are calculated from the water depth u f q w h is the mean fluid velocity the froude number is defined as f r u f g h where g denotes gravity acceleration using the so called einstein 1942 sidewall correction flow dissipation by the smooth sidewalls was taken into account by calculating a bottom hydraulic radius rhb the method is explained in frey et al 2006 this allowed us to assess a more consistent shields number θ b r h b s s 1 d than if it were calculated with the water depth s ρs ρ 2 5 is the ratio of sediment to water density the flow was turbulent reynolds number above 4000 and hydraulically rough the steep slope of the flume meant that the flow was supercritical froude number of 1 2 1 3 and that the submergence h d the ratio of flow depth to the particle diameter was low at 1 6 and 2 8 despite these unusual conditions the water flow in this flume has been shown still to behave classically as in larger flumes in particular with the presence of a logarithmic layer frey and reboud 2001 moreover one size sediment rates have been shown to be in good agreement dudill et al 2017 with classic semi empirical bedload formulae such as meyer peter and müller 1948 the shields number ranged from 0 081 to 0 115 this is about two times the classical incipient motion value as all beads moved frequently with only some periods of rest the flow can be classified in stage 3 warburton 1992 or termed full mobility bedload frey and church 2011 however the flow was well below the intense bedload range depicted by capart and fraccarollo 2011 2 3 image acquisition all experiments were recorded using a baumer hxc13 high speed camera the camera was positioned perpendicular to the sidewalls approximately 90 cm upstream from the channel outlet and inclined at the same angle as the channel fig 1 a high bright white led backlight panel was positioned behind the channel and provided stable and uniform lighting the camera recorded at a rate of 130 frames per second and the image resolution was 1280 320 pixels which corresponds approximately to a window of 260 mm by 65 mm up to 500 000 images were recorded over one hour images are shown in fig 3 preliminary test runs showed that a duration of about one hour was sufficient for s6 and 45 min for s20 to obtain a steady state segregation pattern characterized by a quasi continuous region of small beads fig 3 f and l display an image of each run at a time near the end of the experiment when the steady state segregation pattern had been reached see also phenomenological descriptions in section 3 in preliminary tests the smaller beads were input exactly at the upstream corner of the recorded image field of view beads then began to saltate and roll the majority of the introduced beads never infiltrated into the bed and rapidly left the field of view some beads however were caught in the bed and infiltrated immediately downstream of the feeder position at steady state a region with only a very low number of infiltrated small beads could be identified between the feed location and the upstream position of the quasi continuous layer the length between this position which fluctuated somewhat with time and the feed location could be estimated to approximately 10d for s6 see fig 3 f and 35d for s20 for context the total length of the image field of view is 44d as a result in run s20 the quasi continuous region had a length of only 9d which meant insufficient small beads in the field of view to perform any consistent statistics therefore for the higher rate run s20 we decided to input the smaller beads upstream of the upper left corner at a distance of 44d which yielded a quasi continuous small bead region over the entire image see fig 3 l this was not necessary for run s6 since the quasi continuous region was larger with a sufficient number of infiltrated small beads see fig 3 f 2 4 image processing thanks to a sufficient resolution and frame rate acquired sequences of images contain enough information to extract the trajectories of individual beads the algorithm to obtain the individual trajectory of each object relies on a two step deterministic approach 1 object detection 2 object tracking from frame to frame hergault et al 2010 lafaye de micheaux et al 2018 in parallel characteristic lines such as the water free surface and bed surface can be automatically detected thanks to segmentation methods with a dedicated image processing chain lafaye de micheaux et al 2018 2015 all the algorithms coded in matlab are included in a package called beadtracking available on https github com hugolafaye beadtracking moreover a ground truth consisting of a 1000 frame experimental image sequence was used for validation and is available on zenodo https doi org 10 5281 zenodo 3454628 the entire trajectory dataset for both runs s6 and s20 is available on https doi org 10 5281 zenodo 3407127 in addition 18 sequences of raw images corresponding to data used in fig 5 are also included 9 sequences for run s6 and 9 sequences for run s20 2 4 1 object detection for each image the object detection phase aims at detecting all beads fig 2 a by image processing first black large beads are detected by thresholding the image to keep only black pixels fig 2b eroding to isolate black beads fig 2c and then identifying the center of connected components fig 2d this detector is very efficient since black beads are not overlapping and are easily distinguishable from other kind of beads 6 mm diameter beads in a channel width of 6 5 mm it returns the set of black bead centers then black beads are erased by replacing their gray levels by those of the background fig 2e the background is reconstructed through a morphological closing with a disk shaped structuring element radius just greater than bead radius this makes transparent beads more discernible secondly since transparent beads appear as faint dark rings of different shapes because of their neighboring configurations 4 mm diameter beads can be partially overlapped by others we use a specific chain of morphological operations to detect them initially developed by hergault et al 2010 we apply a hconvex operator soille 1999 on the image obtained at the previous step fig 2f then a normalized cross correlation with a ring shaped model fig 2g local maxima extraction and maxima selection according to an adjusted threshold fig 2h this returns the set of transparent bead centers 2 4 2 object tracking to assign at most one detection to at most one tracker i e each physical trajectory is estimated by an individual internal state called tracker a detection tracker association process is needed due to the high number of detections and the long time series the association was done deterministically with a greedy algorithm wu and nevatia 2007 given the matching cost between all possible detection tracker pairs the greedy algorithm iteratively selects the best candidate and removes the corresponding concurrent associations the matching cost between a detection and a tracker consists of a combination of two factors one based on the distance from the detection position to the predicted position of the tracker the other based on the tracker velocity at the previous time step the distance term promotes the detections closest to the prediction with the velocity term we promote low velocities as it allows better associations in case of bead collisions indeed when a bead bounces its predicted position assuming a constant velocity model can fall very close to another bead leading to the same distance term for the corresponding trackers in this case adding a velocity term to the matching cost increases the cost of higher velocities much more than lower ones and so favors choosing lower velocities first during the greedy data association to limit the number of possible detection tracker pairs the set of possible detections associated to a given tracker is limited to the detections located inside a circular region centered at the predicted position given by this tracker this predicted position is determined assuming a constant velocity model in the end the tracking algorithm returns the set of estimated trajectories over the sequence of images examples of trajectories are given on fig 2b and f 2 4 3 water surface and bed line detection both water free surface and bed surface detections are based on a combination of image analysis techniques such as morphological operations gradient calculations and watershed transformations beucher and lantuejoul 1979 vincent and soille 1991 the water free surface is detected by first removing detected beads then amplifying it by bottom hat filtering and then applying a marker controlled watershed to finally return the water free surface as a thin line the bed surface is detected by first computing the average image over n consecutive images removed from their detected water free surface then computing a gradient magnitude of the obtained image and then applying a marker controlled watershed to finally return the bed surface as a thin line these detected lines allowed the derivation of water depth as the difference between the water free surface and the bed line 2 5 post processing results of both the center location and the velocity of each bead are used to compute the depth profiles we partition each image into equally sized bins parallel to the x streamwise direction comprising the entire length of the image and the thickness of n pixels in the normal direction the procedure is similar to that of hill and fan 2008 and maurin et al 2015 for each elevation the volumetric solid concentration c is computed as the sum of all volume portions of beads present in the bin vs divided by the total volume of the bin v vs is simple to calculate since the volume of a slice of a sphere can be evaluated analytically the average particle velocity up is computed similarly with the velocity of each bead weighted by its portion present in the bin averaging is made over all images within a considered temporal sequence using a bin of thickness 1 pixel resulted in too noisy profiles testing with larger bins from 30 pixels down to 1 pixel showed that a bin of 15 pixels half the diameter of a large bead ensured a readable and still converging profile particular attention was paid to ensure that the spatial gradients were still preserved the particle transport rate per unit width at each elevation was defined as qs upx c m2 s m we defined the bedload layer as all elevations exhibiting a particle transport rate higher than 2 of its maximum value we seek to follow the temporal evolution of the segregation pattern to achieve this we need to choose temporal sequences comprising a sufficient number of images this allows us to reach statistical consistency but the sequence should not be too long to ensure a small variation of variables within the sequence temporal sequences were chosen with a number of images between 8000 and 10 000 corresponding to a duration of 61 s to 77 s in the legend of figures the median time is used to label the temporal sequences the initial time 0 is defined as the time when the first small bead was introduced 3 results 3 1 phenomenology before giving quantitative results we will describe the qualitative evolution of both runs s6 and s20 with the help of sequences of images fig 3 while the majority of introduced small beads saltate and leave the downstream end of the image some beads infiltrate through the large beads by kinetic sieving they tend to form clusters very rapidly though some rare beads remain isolated see fig 3b and h for both runs at 138 s the number and the size of these clusters increase see fig 3c and i for both runs at 600 s before progressively merging to form a quasi continuous layer of small beads see run s6 at 1522s and 2509s fig 2d and e and run s20 at 1200 and 1800s fig 2j and k the evolution is then slower before reaching a dynamic steady state final segregation pattern see fig 3f and l for s6 and s20 in neither run did any bead reach the steel bottom at the final stage the elevation of the bottom of the small bead layer does not seem to vary any more this elevation that we will call penetration depth is clearly identifiable and depends on the run considered a higher penetration depth is observed for s20 with a larger sediment rate corresponding to a higher shields number we will analyze below this important feature with the help of concentration profiles once infiltrated in the bed most small beads remained trapped during the entire experiment however the shear driven general movement meant that some beads in the upper layers could be re caught in the bedload layer and finally disappear from the image shearing also meant that several beads could progressively disappear or appear through the downstream or upstream edge of the image field 3 2 initial one size depth structure we recall here the principal characteristics of the depth structure once transport equilibrium is established for flows composed only of one size of the large beads more details can be found in frey et al 2014 fig 4 gives the depth profiles of the particle velocity the volumetric solid concentration and the particle rate for both s6 and s20 the velocity profile is characterized from bottom to top by an exponential decay a linear part and a logarithmic like region the velocity of s20 is always higher than that of s6 at all elevations in the bed the solid concentration is maximal with a mean value of about 0 53 some oscillations are reminiscent of a layered structure above y 0 i e above the top of the downstream obstacle there is a dramatic decrease up to the free surface from y 0 to 1 the concentration is smaller for s20 than for s6 above y 1 it is the reverse due to a higher number of beads in saltation for s20 the sediment rate is the product of velocity and concentration as defined in postprocessing in our case when the concentration is low the velocity is high and vice versa the sediment rate peaks around y 1 for both runs and has an approximate gaussian shape though skewed downwards the bedload layer as defined above with values of sediment rate larger than 2 of its maximum value ranged from y 1 to 2 5 for run s6 and from 2 to 2 8 for run s20 the region below the bedload layer is characterized by low particle velocities hence the use of the term quasi static flow or creeping flow houssais et al 2015 since particle velocities are small in this region it does not practically contribute to the bedload rate however the properties of this region are of utmost importance for the evolution of vertical segregation of particular interest is the exponential decay of the particle velocity see inset of fig 4 and hence of the shear rate gradient of the velocity the consequences of this interesting property will be used in the discussion section where a link between segregation and shear rate will be evidenced 3 3 concentration profiles of small particles fig 5 shows the time evolution of small bead concentration profiles for both runs s6 and s20 the concentration x axis scale is the same and the y axis depicts a range of 5 5 diameters below y 1 for run s6 and below 0 for run s20 same range with a translation of 1d the same shape can be observed for both runs the concentration profiles are quasi gaussian though skewed towards the bottom right from the beginning and at all times the profile of s20 peaks at a lower elevation than s6 at the final stage the profile of s20 is slightly thicker than s6 approximately 4d vs 3 5d looking at the time evolution on fig 5 see also fig 3 with the images there is a decrease of the peak vertical position relatively sharp for s20 more diffuse for s6 there is clearly a rapid increase of concentration in the lowest layers in both runs it is remarkable that the lowest limbs approximately collapse onto a single curve this behavior related to the penetration depth will now be analyzed in more details with the help of concentration isolines 3 4 concentration isolines fig 6 shows for the two runs the concentration isolines in the elevation time space each isoline corresponds to a constant concentration value chosen as 0 01 0 02 0 05 and 0 1 for each concentration value there is a lower and an upper isoline hence 2 series of 4 curves on each graph in both runs the lower isolines decrease with time first sharply then more gradually for finally reaching a steady penetration depth by contrast the upper isolines are more or less horizontal showing no trend comparing globally the upper isolines it is observed that in run s20 they are located 1 diameter below those of run s6 and for the lower isolines this difference is approximately 1 5 diameter at final steady state for instance at a concentration of 0 02 the upper and lower isolines are around y 0 and 3 25 for s6 and around y 1 and 5 for s20 which results in a thickness of respectively 3 25d and 4d for a concentration of 0 05 the upper and lower isolines are around y 0 5 and 3 for s6 and around y 1 5 and 4 5 for s20 which makes a thickness of respectively 2 5d and 3d so these figures also indicate a slightly larger thickness of the small bead region for s20 clearly the evolution of the lower isolines is very dynamic at the beginning fig 7 shows a semi log plot of the temporal evolution of each lower isoline in run s20 a logarithmic time decreasing trend is observed for all lower isolines up to a time between 1000 and 2000 s it is particularly obvious for c 0 01 and 0 02 in run s6 the logarithmic time decreasing trend is also observed but less obviously more quantitatively we have fitted the experimental isolines with logarithmic curves for the 3 cases c 0 01 0 02 0 05 a being the absolute slope value of the fitted line and b the y intercept 1 y c a log t b table 2 gives the fitted values for each concentration isoline and for both runs together with the coefficient of determination run s20 with a higher shields number displays larger absolute slope values ranging from 1 95 to 2 13 denoting a faster segregation rate than run s6 ranging from 1 21 to 1 37 for both runs regardless of the concentration c 0 01 0 02 and 0 05 the slope of the isolines is very similar which is remarkable 3 5 normal small particle velocity image analysis allows us to track each bead and thus to compute the normal small particle velocity i e orthogonal to the bed slope which is on average negative since kinetic sieving implies an average downward movement in fig 8 where the absolute downward velocity upy is plotted we focus on the dynamic evolution taking place until about 500 600 s see fig 3 c and i for all elevations the downward velocity of run s20 is noticeably higher than for run s6 for both runs s6 and s20 and for all time sequences the downward velocity decreases sharply in the dynamic layer between y 1 and about y 0 then more gradually in the region between y 0 and y 1 5 at initial times the downward velocity is significant in the region around y 0 s6 and 1 s20 then it approaches zero at later times more specifically in run s20 the mean downward velocity decreases to 1 5 10 4 m s at an elevation of y 1 5 at time 138 s brown curve whereas the same velocity is reached at a higher elevation of about 0 5 at the later time of 576 s magenta curve in run s6 this time evolution can also be observed though less clearly for example at y 0 the velocity has a mean value of 3 10 4 m s at 64 s dropping to 0 15 10 4 m s at 495 s the intermediate profiles display a value of about 10 4 m s essentially the velocity tends to approach 0 at a higher elevation as time increases and as the quasi continuous small bead layer is building see fig 3 displaying the temporal evolution of the images an assumption is that this behavior is controlled by the dynamic conditions prevailing at the bottom of this quasi continuous layer notably by the shear rate behavior since the normal motion of the bottom of the evolving quasi continuous small bead region progressively decreases the normal movement of beads located above becomes hindered and hence the downward small particle velocity in the upper layers tends to decrease as evidenced in fig 8 we will explore this assumption further in the following discussion section 4 discussion 4 1 shear rate dependence to discuss the time evolution of the concentration isolines we will define the segregation rate as d y c d t with yc the elevation corresponding to the isoconcentration c a number of theories may et al 2010 savage and lun 1988 have proposed a linear dependence of the segregation rate to the shear rate defined as γ d u p x d y 2 d y c d t γ recall that the streamwise velocity profile is characterized by an exponential decay u p x e y λ with λ the exponential decay constant see semi log plot in fig 9 this means that the shear rate derivative of the velocity also follows an exponential decay γ c 0 e y λ with c0 the shear rate at y 0 3 hence d y c dt c 0 e y c λ separating variables and integrating from time t 0 and initial position gives y c λ log t λ c 0 c 1 with c 1 a constant of integration when t 1 c 1 is negligible and hence 4 y c λ log t c 2 we have seen that the experimental small particle concentration isolines decrease as a logarithmic function of time see fitting in fig 7 and eq 1 yc alog t b therefore if the segregation rate actually linearly depends on the shear rate the absolute value of the slope of the isolines a should be equal to λ the slope of the velocity profile let us test this assumption on our data on one hand recall table 2 that fitting on the lower isolines c 0 01 0 02 and 0 05 yielded a slope value ranging from 1 21 to 1 37 mean of 1 27 in run s6 and from 1 95 to 2 13 mean of 2 06 in run s20 on the other hand streamwise velocity profiles were fitted over the exponential domain fig 9 in the same y region where the three isolines evolve c 0 01 0 02 and 0 05 in run s6 for y between 3 and 0 5 the exponential decay constant λ equals 1 37 coefficient of determination r2 of 0 995 in run s20 for y between 5 and 2 this constant equals 2 11 r2 of 0 986 therefore the agreement between the exponential decay constant λ derived from the velocity profile and the time constant derived from the logarithmic decay of the concentration isolines is good with an average difference of only 8 in run s6 λ 1 37 vs a 1 27 and 2 in run s20 λ 2 11 vs a 2 06 to conclude the present analysis shows that the particle shear rate drives vertical segregation at first order in turbulent bedload transport this result is similar to what has been obtained in dry granular flows may et al 2010 savage and lun 1988 it suggests that the mechanisms observed in turbulent bedload transport are not different from the ones observed in dry granular flows this size segregation mechanism results from the relative displacement of grains between the granular layers following savage and lun 1988 the creation of holes in the granular media due to the shear rate induces a net downward small particle flux linked with a higher probability for a small particle to fall into a hole than for a large one in addition the parallel with dry granular flows suggests that the models designed for such flows could be used in the future to model size segregation in turbulent bedload transport thornton et al 2006 tunuguntla et al 2014 similarly the shear rate dependence could be included in the segregation flux as described by may et al 2010 4 2 penetration depth an important question is whether there is a cut off shear rate under which no kinetic sieving is possible this would explain the constant values reached by the lower isolines and hence the possibility to define the penetration depth our experiments showed that no evolution of the penetration depth was observed after about 40 min in run s6 and 30 min in run s20 at this point examination of the isolines gives a penetration depth of about 3d for s6 and 5d for s20 this observation may be put into perspective with experimental or field measurements of infiltration resulting in maximum values of 2 to 5d90 with d90 the diameter at which 90 of the coarse bed particle is finer however in field measurements it is nearly impossible to distinguish between the evolution of the bedload layer and scour and fill phenomena due to bedform formation and travelling notably bars dunes and antidunes recking et al 2009 the depth of buried tracers is indeed the result of both phenomena a discussion as how to distinguish and simply name both phenomena was provided by church and haschenburger 2017 therefore most investigations have been made in hydraulic flumes and moreover with stable gravel beds for example beschta and jackson 1979 reported infiltration down to 2 5 5d90 similar values were reported by gibson et al 2009 who underlined that the seal called bridging did not occur at a very definite elevation but that the fine content decayed exponentially with the depth whilst most field studies have reported infiltration due to the scour and fill process some field experiments by lisle 1989 were conducted over a relatively plane moving bed and resulted in a seal thickness of 2 6 3 6 d90 to conclude although the experimental conditions of our idealized experiments could at first sight be considered quite different from field conditions the reported penetration depths have the same relative values however since the velocity and the shear rate profiles are exponential and if the proportionality assumption between the segregation rate and the shear rate still holds in the lowermost layers simple theories such as savage and lun 1988 predict that the penetration depth should continue to evolve with no cutoff value we recently conducted preliminary coupled fluid discrete simulations frey et al 2017 that indeed showed no cutoff value for the penetration depth to summarize we have two series of seemingly contradictory results on the one hand observations in the laboratory including our idealized experiments as well as in the field tend to show a maximal penetration depth on the other hand preliminary coupled fluid discrete simulations and simple theories show no cutoff value of the penetration depth with the system therefore never reaching a steady state further comments can be made to reconcile these two observations since the granular shear rate exhibits an exponential decrease in the bed the expected downward segregation motion of the small particles should scale with the logarithm of time therefore the experimental duration might be too short to observe further motion in the present case roughly speaking the time needed for a small particle to move down is ten times greater every length of the large diameter it is therefore possible that our one hour long experiment needs to be run for considerably longer durations ten hours would be needed to possibly see an added one large diameter long descent and hundred hours for two diameters at present experiments of this duration would be very difficult or even impossible to arrange with respect to field observations it should be kept in mind here that the system is idealized considering only steady and uniform configurations in a bi disperse granular bed the large grain size distribution and the non uniform conditions bacchi et al 2014 observed in the field could have important effects further work is required to understand physical mechanisms in idealized bedload transport and extend the analysis to more complex and realistic cases 4 3 high resolution dataset for model validation bedload transport is essentially modeled with engineering oriented shallow water equations together with the exner mass conservation equation and with a semi empirical excess shear stress transport rate formula e g formula of meyer peter and müller 1948 or smart and jaeggi 1983 however in this framework there is no information on the normal vertical direction which is crucial for the study of vertical size segregation more recently lagrangian discrete element models coupled with fluid solvers have been applied to bedload maurin et al 2018 2015 likewise continuous eulerian two phase flow models have been developed with the use of an appropriate granular rheology cheng et al 2018 multiphase continuum models and multi sized fluid coupled discrete element models are now under development it is therefore crucial to have high resolution spatio temporal datasets to be able to validate those models the dataset described in this paper can fulfill this goal 5 conclusion we have analyzed the temporal evolution of new kinetic sieving experiments following the introduction of a low rate of smaller particles in a larger particle bedload flow at transport equilibrium segregation results in the progressive development of a quasi continuous region of small particles reaching a steady state penetration depth with time image processed concentration depth profiles and isolines showed a logarithmic time decrease we demonstrated that this evolution was dependent at first order on the streamwise shear rate at the bottom of the small particle region which decays downwards exponentially this result is in line with several theories originally developed for dry granular flows however this theory does not allow the system to reach a maximal constant penetration depth as evidenced by our experimental data as well by other field or experimental data further research is therefore necessary to reconcile these two observations finally this high resolution spatio temporal dataset could be used to develop and validate multi size fluid coupled discrete element models as well as eulerian multiphase models declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the work was funded by agence nationale de la recherche project segsed anr 16 ce01 0005 the work was also partly funded by the french institut national des sciences de l univers program ec2co biohefect dril and labex osug 2020 investissements d avenir anr10 labx56 the authors acknowledge the support of irstea formerly cemagref h lafaye de micheaux also acknowledges funding provided by grants from la région rhône alpes this work would not have been possible without the technical support of f ousset in the laboratory at irstea supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 103478 appendix supplementary materials image application 1 
533,we present a new approach for estimating the frequency of sub hourly rainfall extremes in a warming climate with simulation by conditioning bartlett lewis rectangular pulse blrp rainfall model parameters on the mean monthly near surface air temperature we use a censored modelling approach with multivariate regression to capture the sensitivity of the full set of blrp parameter estimators to temperature enabling the parameter estimators to be updated the downscaling framework incorporates uncertainty in climate model projections for moderate and severe carbon forcing scenarios by using an ensemble of climate model outputs linear regression on the logarithm of blrp parameter estimators offers a robust model for parameter estimation with uncertainty the approach is tested with 5 min rainfall data from bochum in germany and atherstone in the united kingdom we find that the approach is highly effective at estimating rainfall extremes in the present climate and the estimation of future rainfall extremes appears highly plausible keywords mechanistic stochastic extremes rainfall climate change impacts k nearest neighbour multivariate regression 1 introduction drainage system design requires the robust estimation of rainfall extremes at fine spatial and temporal scales for decades this has been achieved with historical gauge records however the growing consensus that anthropogenic climate change is accelerating challenges the premise that climate stationarity can be assumed over the lifetime of civil infrastructure this not only impacts new build projects but has direct consequences for existing environments increasingly utility providers are required to assess the resilience of their assets to natural hazards while businesses and individuals alike seek greater insurance against perils not previously perceived our principal tools for projecting future climate feedbacks are global circulation gcm and earth system esm models however the coarse resolution of these models makes their outputs unsuitable for climate impact assessment at the urban scale burlando and rosso 1991 sunyer et al 2012 downscaling techniques are therefore required to quantify the effect of global scale warming on catchment processes numerous methods exist including resampling thorndahl et al 2017 scaling lenderink and attema 2015 method of fragments srikanthan and mcmahon 2001 disaggregation hay et al 1992 gyasi agyei 2011 adjusting procedures koutsoyiannis and onof 2001 yusop et al 2014 abdellatif et al 2013 debele et al 2007 and multifractal cascade models schmitt 2014 verrier et al 2011 however a popular approach is to use stochastic weather generators to synthesise long sequences of environmental variables at the catchment scale with perturbed parameters to reflect future changes at the climate scale these can be used to estimate changes to important properties such as variance and extremes or as inputs into hydrological models to investigate catchment response significant progress has been made in this respect since the early 1990s and is described in numerous reviews and comparison articles fowler et al 2007 sunyer and madsen 2009 maraun et al 2010 arnbjerg nielsen 2011 sunyer et al 2012 willems et al 2012 ailliot et al 2014 westra et al 2014 sunyer et al 2015 the purpose of this research is to investigate if temperature projections from an ensemble of cmip5 global climate models can be used to downscale 5 min rainfall extremes using stochastic simulation while there are many other environmental indicators of rainfall temperature is chosen based on its known high correlation with rainfall and the relatively high skill of climate models in estimating temperature compared with other variables furthermore several studies have investigated the relative importance of different environmental variables as indicators of rainfall with many concluding that temperature is a key indicator the bartlett lewis rectangular pulse blrp model is chosen to enhance the physical basis of extreme rainfall estimation in a changing climate mechanistic stochastic rainfall models such as the blrp model emulate the theory of rain cell clustering in storms consequently extremes are constructed from the superposition of rain cells making this class of model particularly appealing for extreme rainfall estimation taking the censored rainfall modelling approach introduced in cross et al 2018 in which blrp rainfall models are used to simulate the intense rainfall profile over a low censor we further develop the fitting methodology to allow for climate non stationarity motivated by the simple linear regression on summary statistics of wasko and sharma 2017 and the change of conditioning variable in kaczmarska et al 2015 the censored modelling approach is extended into a downscaling framework following kaczmarska et al 2015 we change the conditioning variable from calendar month to mean monthly near surface air temperature but use k nearest neighbour sampling to identify the training data for model parameter estimation to enable simulation under hypothesised warming scenarios we investigate the sensitivity of censored blrp model parameters to temperature using multivariate linear regression which preserves the known dependence between parameters the relation between rainfall model parameters and temperature is established with historical observations using an ensemble of mean monthly temperature projections from the cmip5 inter comparison experiment we estimate the change in rainfall extremes for the far future time window 2070 2100 under moderate and severe representative concentration pathways rcp4 5 and 8 5 respectively because the perturbation of model parameters is underpinned by a physical atmospheric process this downscaling approach is preferable to other established methods in particular change factors a review of the principal developments in rainfall downscaling is presented in section 2 in section 3 we set out the downscaling methodology for extreme rainfall simulation with censoring including blrp model fitting and parameter regression rainfall and climate model data are introduced in section 4 followed by the calibration and validation of the models in section 5 extreme rainfall estimation for present and future climates is presented in section 6 discussion and conclusions are given in sections 7 and 8 2 developments in rainfall downscaling 2 1 established methods early attempts at downscaling climate model outputs for hydrological analysis focussed on weather typing bárdossy and plate 1992 goodess and palutikof 1998 trigo and dacamara 2000 fowler et al 2000 bárdossy et al 2002 qian et al 2002 fowler et al 2005 in which stochastic weather generators are conditioned on homogeneous mesoscale circulation patterns this approach is attractive because circulation patterns describe the prevailing weather conditions on a given day and provide discrete integrated series of otherwise continuous non linear processes however because weather types are derived from pressure fields alone they are unlikely to fully capture the influence of thermodynamic effects on rainfall extremes maraun et al 2010 which are known to have a significant impact this approach has lost favour in the recent decade with greater emphasis being placed on factor of change methods factor of change methods also referred to as delta change or perturbation methods fowler et al 2007 apply simple scaling to historical summary statistics according to the change in control and future gcm outputs change factors cf are used to perturb weather generator parameters so that environmental variables may be simulated at the local scale but which are representative of changed future climates while they can be applied to any type of weather generator cfs have been used extensively with those based on the neyman scott rectangular pulse nsrp rainfall model following the development of several weather and rainfall simulation software tools including earwig kilsby et al 2007 awe gen fatichi et al 2011 and rainsim burton et al 2008 the environment agency rainfall and weather impacts generator earwig is a single site daily weather generator developed for the uk climate impacts programme ukcip02 by kilsby et al 2007 this downscaling tool comprises a two part stochastic weather generator which first simulates daily rainfall using the nsrp model and then daily means of temperature temperature range vapour pressure wind speed and sunshine duration using regressions on the rainfall observations cfs are calculated for the daily mean variance and skewness of rainfall and to a logit transformation of the proportion of dry days using ukcip02 future scenarios hulme et al 2002 the uk climate projections were updated in 2009 ukcp09 murphy et al 2009 following publication of the ipcc intergovernmental panel on climate change fourth assessment report ar4 correspondingly the earwig methodology was updated to provide climate change simulations with probabilistic projections from an ensemble of regional climate model rcm runs subsequently referred to as the ukcp09 weather generator ukcp09 wg this approach has formed the basis of significant research into catchment scale climate impacts including studies by manning et al 2009 burton et al 2010 and honti et al 2014 the advanced weather generator awe gen fatichi et al 2011 is a single site hourly weather generator and downscaling tool it is similar to the ukcp09 wg in structure although change factors are assigned probability distributions for a range of climate statistics and applied directly to the rainfall statistics used to fit the embedded nsrp model numerous climate impact studies have been undertaken using awe gen francipane et al 2015 liuzzo et al 2015 pumo et al 2017 to generate hourly rainfall time series for future climates a two dimensional simulator awe gen 2d was later developed by peleg et al 2017 in which the point nsrp model was replaced with the space time realizations of areal precipitation streap model paschalis et al 2013 for spatiotemporal rainfall simulation despite the significant structural and numerical developments in this model change factors are still used to estimate the future change in weather variables rainsim is an hourly spatiotemporal nsrp rainfall field generator developed by burton et al 2008 and is closely related to earwig and the ukcp09 wg it has been used in several climate impact studies using novel downscaling methods goderniaux et al 2011 scale nsrp change factors in rainsim proportionally to global temperature changes to investigate transient climate change impacts on groundwater resources in a two part publication bordoy and burlando 2014a b apply scaling laws to future daily summary statistics perturbed using change factors calculated at the daily scale the rescaled future summary statistics are used to reparameterise the space time rainsim nsrp model to simulate future sub daily rainfall in the swiss alps sørup et al 2016 use the space time rainsim model to investigate the climate change signal for extreme precipitation over a densely urbanized region of north eastern denmark for use in urban drainage design further applications of the change factor approach to downscaling can be found in the literature using similar stochastic rainfall models khazaei et al 2012 use the original nsrp model as the basis of a new weather generator for the estimation of minimum and maximum temperatures onof and arnbjerg nielsen 2009 use the random parameter bartlett lewis rectangular pulse blrp model with truncated gamma distribution to downscale rcm aerial rainfall to sub hourly point estimates using a combination of change factors and disaggregation using a semi markov chain model of rainfall events sørup et al 2017 apply change factors for system states which categorise events in the historical record change factors have been used extensively in the recent decades for climate downscaling with rainfall simulators because the method is conceptually tractable and computationally simple however it relies on a certain assumptions which are the matter of debate primarily factor of change methods assume that the scaling relationships of rainfall statistics or model parameters are stationary between present and future climates bordoy and burlando 2014a sunyer and madsen 2009 this may be appropriate in locations where there is no reason to believe that the mechanisms governing the local climate will alter significantly although there is no way to test this assumption for hypothesised future climates furthermore the relationship between mesoscale circulation and point precipitation is assumed to be valid forsythe et al 2014 on the basis that the phenomenology of rainfall generation in storms is scale invariant in each case there is no physical underpinning for these assumptions because the scaling relationships are not explained by physical processes or parameters 2 2 linking climate to rainfall trenberth et al 2003 set out the link between global warming and the intensification of rainfall extremes broadly concluding that as the climate warms atmospheric water vapour content will increase leading to more intense downpours explained by the clausius clapeyron c c relationship in which water saturation vapour pressure increases with temperature at a rate of 7 k 1 several studies have identified similar scaling relationships between high rainfall quantiles and air temperature hardwick jones et al 2010 shaw et al 2011 utsumi et al 2011 fujibe 2013 molnar et al 2015 supporting the hypothesis that rainfall extremes will intensify with increasing global temperatures further evidence is provided by westra et al 2013 who identify increasing dependence between maximum annual daily precipitation and globally averaged near surface temperature of between 5 9 and 7 7 k 1 for two thirds of over 8000 stations globally despite this local geography and climate feedbacks can dominate the precipitation temperature scaling ali and mishra 2017 identify negative scaling between surface air temperature and extreme rainfall in india because of the cooling effect of monsoon events hardwick jones et al 2010 report negative scaling for australia above 26 c while shaw et al 2011 fujibe 2013 and molnar et al 2015 find for the usa japan and switzerland respectively that scaling rates vary significantly depending on the region season temporal scale and rainfall type despite the mixed signals for the c c relationship it is physically appealing to identify dependence in precipitation at the local scale with exogenous variables at the mesoscale which capture thermodynamic effects such an approach has formed the basis of several studies in the recent decade using a modified markov model with kernel density estimator for rainfall occurrence and amounts mehrotra and sharma 2010 present a multisite stochastic downscaling framework in which both occurrence and amount models are conditioned on atmospheric variables the conditioning variables are chosen to capture atmospheric circulation moisture and short term rainfall persistence from rain gauge and ncep reanalysis data using climate projections of the exogenous conditioning variables from cisro mk3 climate model the model is used to evaluate changes in rainfall at the daily seasonal and annual scales between the current and future climates centred on 2070 the modelling framework presented by mehrotra and sharma 2010 offers a tool for climate impact assessment on water resources but urban scale impacts require rainfall simulation at shorter temporal scales mechanistic stochastic rainfall models offer this capability although as already highlighted downscaling with these models has largely focussed on change factor approaches recognising the demand for non stationary rainfall simulation in a changing climate and the limited development of transient model frameworks evin and favre 2013 reformulate the original nsrp model to allow the storm arrival process λ to vary with time with one additional model parameter representing the evolution in storm arrivals ϵ the new transient nsrp model has a total of 6 parameters a potential criticism of this approach is that the non stationarity captured in the temporal variation in λ is only representative of the historical period with global warming the rate of change may be different and could vary between the different carbon forcing scenarios set out in the ipcc s fifth assessment reports ar5 the reader is referred to the synthesis report by pachauri et al 2014 which combines the findings of the three contributing ipcc working groups wasko and sharma 2017 use linear regression to establish relationships between monthly rainfall summary statistics and mean monthly air temperature to obtain perturbed fitting statistics for the original nsrp model representative of warmer climates the simplicity of this approach makes it more pragmatic and potentially more informative than that of evin and favre 2013 because historical temperature changes implicit in evin and favre 2013 will not necessarily be representative of the future however a possible criticism is that by applying linear regression separately to each summary statistic the dependence structure between statistics may be lost taking a different approach to both evin and favre 2013 and wasko and sharma 2017 kaczmarska et al 2015 develop a new method to calibrate the original blrp model using continuous environmental covariates instead of calendar month using a nonparametric form of local linear regression blrp model parameters are estimated using monthly rainfall summary statistics coincident with monthly mean environmental variables the neighbourhood of summary statistics is weighted by a normalised gaussian distribution over all data points with a user defined bandwidth specifying the standard deviation of the distribution with this approach parameter estimation beyond the range of the conditioning covariate is highly uncertain because all conditioning months fall within the tails of the normal distribution which are down weighted this limits the usefulness of local regression for climate impact assessment for the most severe climate forcing scenarios and far future estimation 3 downscaling methodology the downscaling methodology set out here extends the censored rainfall modelling approach for the estimation of fine scale extremes introduced in cross et al 2018 to assess climate change impacts on intense rainfall in this paper bartlett lewis rectangular pulse models are used to mechanistically simulate short duration intense rainfall from which extremes may be estimated the method was motivated by the desire to improve the physical realism of extreme rainfall estimation for intense cloudburst type events to supplement frequency techniques and increase the amount of data used for estimation mechanistic stochastic rainfall models which include both the bartlett lewis and neyman scott varieties simulate rainfall by emulating the rainfall generating mechanism with poisson cluster processes in this mechanism rainfall observed on the ground arises from the clustering of rain cells in storms storm arrivals are simulated with a primary poisson process and rain cell arrivals with a secondary poisson process rain cells are typically assumed to be rectangular in shape describing their intensity and duration and storms are terminated according to an exponential distribution the summation of cells forms the unobserved continuous time rainfall which is aggregated to form discrete rainfall time series at any temporal scale because of the tendency for mechanistic stochastic rainfall models to under estimate rainfall extremes at fine temporal scales a censored approach was developed to simulate the intense rainfall profile in this approach a low censor is applied to the whole rainfall time series observations below the censor are replaced with zero rainfall and observations above the censor are reduced by the censor amount using three variants of the bartlett lewis rectangular pulse model the approach was applied to two rainfall records in the uk and germany for 5 and 15 min accumulations and shown to significantly improve the estimation of extremes sections 3 1 3 4 describe the downscaling methodology presented in this paper to aid understanding of the procedure a workflow of the steps involved is presented in fig 1 3 1 rainfall model calibration the first step in the downscaling methodology is to estimate model parameters that are optimal given the monthly near surface air temperature to this end we have to select periods within the historical rainfall record which are representative of a given temperature the standard method for fitting mechanistic rainfall models essentially uses seasonality as the conditioning covariate represented with non overlapping calendar months this is analogous to applying uniform weight to all target months in the historical record where the target month is the calendar month for which the model is being fitted the result of this is to obtain seasonally varying parameter estimators for each month of the year for the downscaling of rainfall extremes in this study we propose to change the conditioning covariate for censored rainfall models to mean monthly near surface air temperature for incremental changes in temperature we select the calibration data by applying uniform weights to months with mean monthly temperatures within a defined bandwidth in selecting the bandwidth there is a trade off between making it large enough to ensure optimised parameters are well identified while keeping it small enough to generate parameter sets which capture the variability of the rainfall generating process there are two obvious approaches for specifying the bandwidth for temperature this can be any fixed width in degrees celsius or any fixed number of months as per the standard fitting procedure because of the potential subjectivity of fixing a user specified bandwidth the latter is preferred a fixed number of months will result in different bandwidths measured in degrees celsius because of the variability in mean monthly temperature throughout the year selecting a fixed number of months is analogous to using the k nearest neighbour knn algorithm to classify the data in only one dimension with k equal to the selected number of months observations y are selected according to their euclidean distance d from a chosen value x using eq 1 1 d x y x 1 y 1 2 x 1 y 1 where x 1 and y 1 are the target and observed values for the single predictor the training data for model fitting at target value x is therefore based on the k nearest observations or the top k months taken from the rank of all calculated distances parameter estimation is performed using a generalised method of moments gmm with a weighted least squares objective function s θ t i 1 k ω i t i τ i θ 2 where θ θ 1 θ p t is the vector of p model parameters t t 1 t k t is the vector of k observed summary statistics τ θ τ 1 θ τ k θ t is the vector of k expected summary statistics and ωi is a weighting applied to the i th summary statistic the gmm minimisation routine employed in this research is performed using an updated version of the momfit software developed by chandler et al 2010 this software uses asymptotic theory to provide parameter estimators which are multivariate normally distributed mvn this captures the known dependence between blrp model parameters and enables parameter uncertainty to be propagated forward into the estimation of rainfall summary statistics and extremes the mean of the mvn distribution is the optimal parameter set and the variance covariance matrix is estimated from the least squares objective function s θ t chandler et al 2010 for consistency with the analysis and results presented in cross et al 2018 the downscaling approach is tested using the same three variants of the blrp rainfall models the original model bl0 by rodriguez iturbe et al 1987 the modified model bl1 by rodriguez iturbe et al 1988 in which the rain cell duration parameter η is randomized and the modified model with rain cell intensity μx also randomised bl1m by kaczmarska et al 2014 table a 1 in appendix compares the parameters of the three blrp model variants and provides the units for each parameter following the standard fitting procedure for seasonally varying parameters rainfall summary statistics are calculated for each month of the historical record however to fit the blrp models using the gmm it is necessary to select rainfall summary statistics coincident with selected covariate target values this is described in the following section 3 2 covariate target range knn sampling can be applied to any covariate target value within or outside the observed covariate range however for target values close to the extremes of the observed covariate range samples of historical months become increasingly unrepresentative of the target value because of the increasing scarcity of unique historical records because the downscaling methodology involves regression on the parameter estimators it is necessary to ensure that the selection of fitting months is representative of the covariate target values this gives rise to a covariate target range which is inevitably narrower than the observed range as k increases the covariate target range used for model fitting reduces this is shown graphically in fig 2 for bochum using the historical temperature projections from the ipsl cm5a mr climate model see section 4 2 for a description of the climate model data with k fixed at 52 104 and 156 months see section 4 1 for a description of the rain gauge data at bochum the mean monthly near surface air temperature ranges from 6 8 to 22 5 c for covariate target values incremented at 1 c an initial target range would span 6 to 22 c as shown by the series of box and whisker plots in fig 2 it can be seen in fig 2 that most selections closely follow the 1 1 line with both mean and median temperatures within the selection very close to the target value however this is not true at the extremes of the covariate range with several target values comprising identical selections with mean temperatures which deviate significantly from the target value grey box and whisker plots identical selections would result in identical blrp parameter estimators the covariate target range is therefore refined to remove these identical selections and those where the mean of the selection magenta line deviates by more than 1 c from the target value for k 52 these include 6 5 4 3 2 1 21 and 22 c see fig 2 therefore for k 52 months the covariate target range is 0 to 20 c shown by the series of blue box and whisker plots in fig 2 assuming that the selection bandwidth is defined by the total extent of the whiskers the bandwidths for k 52 range between approximately 1 and 5 c 3 3 model parameter regression we hypothesize that mechanistic stochastic rainfall model parameter estimators may be approximated by regression without significant detriment to rainfall simulation and the estimation of extremes therefore upon the calibration of model parameters for the full range of target values section 3 2 the relationship between parameter set estimators and temperature is approximated using multivariate linear regression in this framework we have multiple dependent response variables the set of blrp parameter estimators and one independent predictor variable our chosen covariate the mean monthly near surface air temperature the multivariate regression framework enables us to preserve the dependence between model parameters and is performed on the natural logarithm of the parameter estimators for consistency with blrp fitting methodology using the regression model we can estimate parameters within and beyond the range of present day monthly mean temperatures enabling estimation of rainfall extremes under hypothesised warmer climates for n observations m response variables and p predictor variables the multivariate regression model takes the form given in eq 2 2 y n m x n p 1 β p 1 m ϵ n m where y is the n m matrix of response variables x is the n p 1 design matrix for predictor variables β is the p 1 m matrix of regression coefficients and ϵ is the n m error matrix for our regression model n is the number of covariate target values m is the number of blrp model parameters and p 1 for the single predictor covariate the mean monthly near surface air temperature the matrix expansion of eq 2 is given in eq 3 3 y 1 1 y 1 m y n 1 y n m 1 x 1 p 1 1 x n p 1 b 0 1 b 0 m b p 1 1 b p 1 m e 1 1 e 1 m e n 1 e n m where the subscript p 1 refers to the single predictor variable here the mean monthly near surface air temperature upon fitting the regression model we obtain the matrix of regression coefficients and their error matrix which describes the regression uncertainty with these we can approximate the set of blrp parameter estimators for new observations within and beyond the covariate target range used to fit the regression for the simulation of rainfall extremes using the blrp models it is desirable to specify the multivariate normal distribution of parameter estimators so that realistic simulation bands may be produced incorporating parameter uncertainty 1 1 the blrp parameter estimators are asymptotically normally distributed this requires the mean of the distribution which is the optimal parameter set and its corresponding variance covariance matrix which describes the rainfall model uncertainty in the downscaling approach the mean of the distribution is replaced by the regression estimates at the target values however the rainfall model uncertainty needs to be updated to incorporate the regression uncertainty to obtain the combined temperature conditioned model uncertainty the regression uncertainty comprises two sources of error which arise from predicting the response variables for new observations from the regression model the first describes the positions of the regression lines at the prediction value and the second the variance around this assuming yi is the predicted response vector for the ith observation xi the variance of the expectation of response variables e yi which describes the position of the regression lines is var β t x i β t x i where β x i t x i 1 x i t y i is the least squares estimate of β the variance of the response variables around the regression lines is var y i β t x i setting σ var ϵ the prediction variance is σ 1 h where h x x t x 1 x t predicted response variables are assumed to be multivariate normally distributed hence we have the distribution of response variables given in eq 4 4 y β t x mvn x β σ 1 h the regression error variance σ is unknown therefore the unbiased estimate of the error variance for covariate xi is σ i sscp ϵ i n p 1 where sscp ϵ i ϵ i t ϵ i is the sum of squares and cross products of the residuals therefore the estimated prediction variance of response variables yi the regression uncertainty is var y i β t x i σ i 1 h this is added to the variance of the blrp parameter estimators the rainfall model uncertainty to obtain the updated variance for the blrp parameters the temperature conditioned model uncertainty for predicted blrp parameter estimators beyond the range of observations either the variance for the nearest blrp parameter estimator can be used or the mean of the blrp parameter variances therefore the multivariate normal distribution of predicted blrp parameter estimators from the regression model y blrp is given in eq 5 where σ bl is the mean rainfall model uncertainty 5 y blrp mvn x β σ 1 h σ bl 3 4 simulation and weighting of ensemble extremes when the conditioning covariate is changed to mean monthly air temperature the number of parameter sets used to capture the annual variation in rainfall depends on the increments over which the blrp model is fitted and the covariate target range the estimated blrp parameters then become the data used to fit the regression model which has only 10 regression parameters the intercept b 0 and slope bp of the separate blrp parameter regressions example regression parameters for the ncep conditioned blrp models are provided in table a 3 in appendix despite this rainfall simulation is performed in the standard way with 12 parameter sets or blrp models one for each calendar month of the year in the downscaling framework the monthly blrp parameter sets are estimated from the regression model rainfall simulation follows the same procedure as that set out in cross et al 2018 by randomly sampling n times from the distribution of blrp parameter estimators to generate n realisations of simulated rainfall above the predefined censor by sampling from the updated multivariate normal distribution of blrp parameter estimators given in eq 5 the spread of realisations generates simulations bands which capture the different sources of error in the model these include the rainfall model uncertainty the regression uncertainty and the sampling uncertainty which arises in simulation for the estimation of rainfall extremes mean monthly near surface air temperatures are selected from climate model outputs using an ensemble of cmip5 models the rainfall models are initially calibrated using the historical climate model run before applying the regression model to blrp parameter estimators then present and future climate extremes are estimated using mean monthly air temperature sampled from the historical and future climate model simulations respectively for the estimation of future extremes temperature projections are obtained corresponding to a chosen representative concentration pathway rcp see moss et al 2010 and vuuren et al 2011 and time slice the latter may represent any future period typically 30 years in duration up to 2100 the temporal extent of most climate model runs hence estimation for a high concentration pathway in the far future would use the climate model projection for rcp8 5 spanning 2070 2100 from this 30 year time slice the mean monthly air temperature for each calendar month is calculated and used as a new observation in the regression model to generate the distribution of blrp parameter estimators for simulation the benefit of using an ensemble of climate model runs in the estimation of rainfall extremes is to reduce the bias of individual climate models however it is recognised that some climate conditioned rainfall models will perform better at estimating rainfall extremes than others therefore a simple weighting is applied to each model according to its ability to reproduce 5 min rainfall extremes estimated from reanalysis conditioned rainfall models for the present climate extremes are estimated for a range of return periods using simulated annual maxima a weighted root mean square error rmse score is calculated for each climate conditioned model which applies greater weight to the ability of the models to reproduce higher return period extremes this approach was chosen to ensure that all extremes were used in the weighting while placing greater importance on the larger extremes while it is recognised that larger extremes have larger variance these extremes are more likely to be representative of convective summer storms than the more frequent ones the weighted rmse gives relative scores for each climate conditioned model enabling them to be ranked according to their extreme value performance this ranking is used to weight the quantiles of the extreme value estimates across all realisations thereby generating weighted median and 95 simulation bands 4 data 4 1 rain gauge data the downscaling of fine scale rainfall extremes with censoring for hypothetical warming climates using the methodology set out in section 3 is performed for the two rainfall datasets used to present the censored approach for the estimation of fine scale extremes in cross et al 2018 bochum in germany and atherstone in the uk both datasets were chosen for their quality and length 69 and 48 years because the methods of recording is different hellmann floating pen and tipping bucket gauges respectively and because of their different locations the bochum data are aggregated to the 5 min temporal resolution while tip time data can be aggregated to any scale guided by the results presented in cross et al 2018 the downscaling approach is performed for 5 min accumulations using a constant censor of 0 5 mm at both sites for bochum downscaling is based on a subset of the rainfall record from 1948 to 1999 to coincide with the data range of the ncep reanalysis data section 4 3 which is used for climate model weighting and blrp model validation section 5 the atherstone dataset commences in 1967 after the start of the ncep reanalysis data on january 1st 1948 and extends to 2015 4 2 climate model data in this study both present and future climate extremes are estimated using an ensemble of long term historical and future rcp climate simulations from phase 5 of the climate model intercomparison project cmip5 taylor et al 2012 the cmip5 experiments provide climate projections for a range of common scenarios in a multi model context they are differentiated by their starting point in the pre industrial control period realisation their initialisation method and physics physical constraints or parameterisation and are identified by the rip nomenclature r ealisation i nitialisation p hysics to test the downscaling methodology only long term simulations relating to ensemble number r1i1p1 were used with two climate forcing scenarios rcp4 5 and 8 5 chosen to be indicative of moderate to severe climate change a total of 29 models within the suite of cmip5 climate models were identified with the historical and two future scenarios available these are listed in table a 2 in appendix the output variable from these climate models used for analysis is the mean monthly near surface air temperature with the common abbreviation tas historical and future time series were sampled for the closest climate model grid squares to the gauge latitude and longitude locations 51 49 7 22 and 52 58 358 47 for bochum and atherstone respectively 4 3 climate reanalysis data the reanalysis data used for climate model weighting and blrp model validation is the ncep ncar national centers for environmental prediction reanalysis 1 model output from the national oceanic and atmospheric administration noaa earth systems research laboratory kalnay et al 1996 outputs for the mean monthly near surface air temperature are used for grid squares closest to the gauge latitude and longitude locations 51 49 7 22 and 52 58 358 47 for bochum and atherstone respectively which are available from january 1st 1948 5 calibration and validation 5 1 model fitting and simulation the estimation of fine scale rainfall extremes in a changing climate with censoring is first performed for the present climate both censored and uncensored extremes extremes obtained with and without censoring applied are estimated to confirm that similar improvement in estimation to that observed in cross et al 2018 is achieved when the conditioning variable is changed to mean monthly near surface air temperature annual maxima are sampled directly from simulations and the estimation of extremes is based on the weighted 50th quantile wq50 with 95 simulation bands given by weighted 2 5th and 97 5th quantiles wq2 5 and wq97 5 respectively the calibration methodology and conditioning information is set out in the remainder of this section section 5 2 presents the results of the model calibration and regression and section 5 3 the results of the validation the choice of fitting statistics at each gauge location is summarised in table 1 with timescales given in brackets because the gamma shape parameter α in the two randomised models bl1 and bl1m is insensitive and can take a wide range of values without significant impact on the estimation of extremes cross et al 2018 it is held fixed at 100 and 5 respectively the ratio of the standard deviation to mean of the cell depth r σ x μ x is fixed at 1 for all three models thereby specifying the exponential distribution for the mean cell intensity by holding these parameters fixed optimisation is only required for 5 parameters in each model bl0 λ β μx η γ bl1 λ κ μx ν ϕ and bl1m λ κ ι ν ϕ for each of the climate conditioned blrp rainfall models 40 time series realizations are simulated with duration equivalent to the duration of the observed record 47 years at atherstone and 52 years at bochum this number of realisations is large enough to give a robust estimation of the median extreme rainfall required for individual model weighting but because the total ensemble of climate conditioned models is large with 29 members the resulting number of realisations for estimation of the wq50 is very large 1160 realisations each realization is generated using a different parameter set sampled from the multivariate normal distribution of model parameter estimators given in eq 5 hereafter referred to as mvn realizations the fitting window is based on k equal to the duration of the rainfall records in years 47 and 52 months for atherstone and bochum respectively by analogy with the standard fitting methodology this is equivalent to using one calendar month albeit months may be selected from anywhere in the rainfall record according to their historical mean monthly air temperature initial model fitting using this approach resulted in some parameter sets being poorly identified in these cases k is increased by including additional months from the selections for adjacent target values this procedure is repeated until the upper limit on parameter uncertainty for all model parameters is reduced to below an arbitrary threshold of 250 units vary for each paramter see table a 1 in appendix or for a maximum of 5 iterations by including additional months from adjacent selections in this way the fitting window is effectively widened locally with k for subsequent target value parameterisations returning to the original selections 5 2 bartlett lewis parameter estimators fig 3 shows the full range of censored blrp model parameter estimators conditioned on near surface air temperature using outputs for the historical periods for each of the 29 cmip5 climate models and the ncep reanalysis data the plots reveal a clear dependence between temperature and several of the blrp model parameters at both locations including increasing relations in the storm arrival rate λ mean cell depth μx or ratio of mean cell depth to cell duration ι and decreasing relations in the cell duration parameter η or the ratio of the gamma shape and scale parameters α ν for the randomised models the remaining parameters cell arrival rate β storm duration γ and the ratios of cell arrival rate and storm duration to cell duration κ and ϕ respectively show less obvious relations broadly the relations appear to follow the same pattern at both sites and the spread of parameter estimators across all 29 cmip5 climate models and the ncep reanalysis is small indicating high consistency in the fitting it is also noted that the ncep conditioned parameter estimators sit inside the range of the 29 cmip5 climate model conditioned estimators as would be expected for reanalysis data by means of an example the fitted regression relationships with associated uncertainties for the ncep conditioned rainfall models are shown in fig 4 the plots show the rainfall model uncertainty magenta polygons ncep cond 95 bl estimator cis regression uncertainty blue polygons blrp 95 regression pis and the combined climate conditioned blrp model uncertainty blue dashed lines blrp 95 regression uncertainty as noted in section 3 3 the regression uncertainty comprises two additive error sources which when combined comprise the prediction intervals pis for the regression model multivariate analysis of variance manova is used to simultaneously test the response of the blrp parameter estimators to the ncep mean monthly near surface air temperatures the primary test statistics pillai s trace the approximate f statistic which determines the significance level of the regression and the p value associated with the f statistic are provided in table 2 for all three models at both locations the p value is very small indicating that the f statistic is highly significant and consequently the null hypothesis that temperature has no effect on the mean blpr parameter estimators is rejected the high pillai trace values demonstrate that temperature is highly informative for each of the fitted regressions while the p values in table 2 indicate that we can reject the null hypothesis of no relationship for the multivariate regressions testing each variable individually with a univariate analysis of variance anova appendix table a 3 shows that for some blrp parameter estimators such as the cell arrival rate β and storm duration γ in the bl0 model there are little or no differences in their means at the 5 significance level p values 0 05 correspondingly we see that the coefficient of determination r 2 for those parameters is low indicating that only a small percentage of the variance in those parameters is explained by their individual regressions however their inclusion in the multivariate regression is important for the estimation of the variance covariance of model parameters the regression parameter estimators b 0 and bp in eq 2 are also shown in appendix table a 3 because the regressions are performed on the natural logarithm of the rainfall model parameters the units of b 0 are the natural logarithm of the rainfall model parameter units and the units of bp are same but per kelvin the plotted regressions are shown extending beyond the range of observations used for fitting to indicate the nature of the regressions when extrapolating where the parameter response is broadly flat cell arrival rate β storm duration γ and the ratios of cell arrival rate and storm duration to cell duration κ and ϕ respectively the regression uncertainties grow much more quickly this is likely to result from one or several blrp parameter estimators lying further away from the best fit regression line thereby increasing the error variance of the regression model greater error variance is likely to arise in parameters which are relatively to the other model parameters less sensitive to the training data this may also be indicative of multiple fitting regions in the blrp parameter space resulting in larger deviations in the fitted parameters between some covariate target values because all fitted blrp parameter estimators are well identified they are assumed to be robust estimators for inclusion in the regression model however it is recognised that the rapid growth in regression uncertainty may be an issue when sampling from the mvn distribution of parameter estimators in simulation if the estimation of extremes is sensitive to large deviations in these parameters from their mean the regression uncertainty blue polygons in fig 4 show the 95 pis for the multivariate regression a visual inspection of the regression plots shows that in most instances the pis bracket most of the fitted blrp parameter estimators exceptions include bochum bl0 β bl1 λ κ and bl1m λ ι κ where only a few parameters fall just outside the pis none of the estimators for atherstone appear to fall outside the 95 pis this level of accuracy can reasonably be expected and indicates that the regression fitting is successful the updated climate conditioned blrp uncertainty blue dashed lines blrp 95 regression uncertainty in fig 4 are consistently wider than the regression uncertainty and do bracket all blrp parameter estimators and much more of the rainfall model uncertainty it is important to capture as much of the rainfall model uncertainty as possible to ensure that range over which blrp parameter estimators are randomly sampled in simulation is characteristic of the original rainfall model fitting 5 3 replication of summary statistics it is important to verify that the sampling of regressed parameters from the multivariate normal distribution given in eq 5 provides blrp parameter sets which satisfactorily reproduce the statistical profile of the data to which they are fitted given that it is neither possible to validate larger extremes in the current climate nor extremes in any hypothesised changed climate validation of the process upon which these extremes are generated is essential for the extreme value estimates to be credible validation is performed in the standard way for mechanistic stochastic rainfall models first we verify that the parameter estimators can reproduce the summary statistics used to derive them while it seems intuitive that they would this is not always true if the parameters are poorly identified or the objective function value is significantly different from zero next we check how well the blrp parameter estimators reproduce other summary statistics not used in the fitting because of the two stage fitting process used in this study validation statistics are presented for the blrp parameter estimators used to fit the regression model and blrp parameter estimators derived from the regression model at the target covariate values while there are many models in the cmip5 ensemble validation statistics are only calculated for the ncep conditioned blrp model because the bias of reanalysis outputs is marginal and the ncep conditioned extreme value estimates are used to weight the performance of the 29 cmip5 conditioned blrp models fig 5 shows the variation in censored 5 min rainfall summary statistics used to fit the blrp models mean coefficient of variation and lag 1 autocorrelation see table 1 with mean monthly near surface air temperature for validation of the fitting statistics the mean rainfall is re scaled from 1 h to 5 min the top 6 panels show the variation in summary statistics for fitted model parameters at both locations and the bottom 6 panels the equivalent statistics for estimated parameters using the regression model in eq 5 fig 6 shows the ability of the models to reproduce a selection of validation statistics not used in the fitting the variance lag 1 autocovariance and proportion of wet periods skewness has been left out of this selection because as highlighted in cross et al 2018 the blrp models are not expected to reproduce the skewness well for censored data at bochum the target covariate values for the ncep reanalysis data range from 1 to 18 c and at atherstone from 4 to 17 c in each case at 1 c increments summary statistics are estimated under each blrp model variant using 100 parameter sets randomly sampled from the mvn distribution of parameter estimators and used to generate simulation bands from the 2 5th and 97 5th quantiles the observed summary statistics are calculated from the selection of rainfall months corresponding to the separate target covariate values summary statistics relating to the mean of the distribution are also calculated using the optimal parameter sets all three models perform equally well in replicating both the fitting and validation statistics with little variation in the statistics estimated by each of the models for all target covariate values the only exception to this is the performance of the bl1 and bl1m models at bochum for the target value of 12 c where the width of the simulation bands on the estimated summary statistics diverges this divergence occurs because the parameter set for 12 c is less well identified than those at the other target values see fig 4 bochum bl1 and bl1m κ and ϕ the 95 simulation bands for the estimated fitting statistics comfortably bracket the observed for all target covariate values despite their narrow range except for 12 c in bochum see fig 5 conversely the simulation bands for the validation statistics do not always bracket the observed at the target covariate values fig 6 there is very good agreement between the estimated and observed validation summary statistics at bochum with the proportion of wet periods only falling outside the simulation bands once at target value 4 c the blrp regression model parameters perform very well in reproducing the fitting and validation statistics the response of the estimated summary statistics under the regressed parameter estimators is much smoother than with the fitted parameters which is expected however the response clearly follows the apparent underlying dependence in statistics with temperature indicated by the observations this is a very pleasing result as the regression is performed on the parameter estimators themselves and not the observations the simulation bands appear to be slightly wider than those for the fitted model parameters which is also expected because of the additional regression error and they comfortably bracket all observed fitting statistics and most observed validation statistics the only exceptions here occur in the estimation of validation statistics at atherstone where the observed lag 1 auto covariance falls outside the simulation bands at 9 10 and 14 c and the proportion of wet periods fall outside the simulation bands between 4 and 7 c these results support the hypothesis that mechanistic rainfall model parameters may be approximated by regression without significant detriment to rainfall simulation a notable observation about the rainfall statistics shown in figs 5 and 6 is their interesting temperature dependence the coefficient of variation and lag 1 autocorrelation show approximate linear relations with increasing temperatures falling and rising respectively while all other statistics show non linear rising relations which for the mean variance and lag 1 auto covariance appear nearly flat for colder temperatures especially at bochum where the temperatures drop to 1 c because these dependencies are characteristic of rainfall over a low censor they demonstrate that intense rainfall is strongly influenced by the near surface air temperature this is important for capturing rainfall extremes arising from different rainfall events and potentially offers an improvement over fitting to calendar month where both stratiform and convective events may be lumped together in any of the summer months it is therefore particularly pleasing that the nature and variation of these dependencies is well captured by both the rainfall model and temperature dependent regression model parameters for the purpose of estimating rainfall extremes with censoring these statistics demonstrate that the models are well parameterised and that the process of rainfall generation on which the estimation of extremes depends is robust the plots in both figures also show that important central moments of the rainfall time series are strongly correlated with temperature supporting the selection of near surface air temperature as an appropriate conditioning covariate 6 extreme rainfall estimation figs 7 and 8 show the estimation of 5 min rainfall extremes at bochum and atherstone respectively for the complete ensemble of 29 cmip5 climate model conditioned blrp rainfall models estimation is shown for all three blrp models and three climate scenarios historical hist medium carbon forcing rcp45 and severe carbon forcing rcp85 the plots also include un censored estimation for the present climate to demonstrate the benefit of censoring for extreme rainfall estimation tables s 1 and s 2 in the supplementary material provide the 10 and 100 year return period weighted extreme rainfall estimates in mm for bochum and atherstone respectively corresponding to the plots given in figs 7 and 8 as well as the width of the 95 simulation bands sbs at those return periods 2 2 the tabulated results are estimated from the nearest observed simulated values with actual return periods of 9 4 and 93 1 years at bochum and 10 3 and 84 1 years at atherstone the difference in these return periods has arisen from the length of realisations but is considered to represent a sufficiently accurate estimate of the 10 and 100 year rainfall amounts at these gauge locations the first and most important observation from the plots in figs 7 and 8 is that the estimation of rainfall extremes with temperature dependent censored rainfall modelling for the historical period is very good consistent with the estimation of short duration extremes by mechanistic stochastic rainfall models fitting to the whole un censored rainfall hyetograph results in their under estimation grey lines and simulation bands the improvement in extreme rainfall estimation with censoring is very similar to that in cross et al 2018 with the bl1m model once again performing slightly better than the other two and comparable width in simulation bands for all three models the similarity in these results is particularly notable given that estimation in this study differs from that in cross et al 2018 in four key ways 1 the change in conditioning variable 2 the change in fitting window 3 the use of blrp parameter estimators from the regression model given in eqs 5 and 4 the use of weighted quantile estimation across multiple climate model conditioned blrp realisations the estimation of historical extremes at both sites is very good with all observations at bochum falling within the weighted 95 simulation bands and only one observation the smallest falling outside the simulation bands for atherstone at both gauges and for all three blrp model variants the wq50 provides very good estimation of the observed annual maxima up to approximately the 10 year return period rainfall event for rarer events the bl0 and bl1 models slightly underestimate the observed extremes although the bl1m model performs better at atherstone the wq50 for the bl1m model estimates marginally higher extremes between the 10 and 100 year return periods than the bl0 and bl1 models however at bochum the bl1m model wq50 accurately estimates the highest observed extreme in comparison with the extreme rainfall estimation for the same sites and historical period in cross et al 2018 the mean estimation is very similar although there is an increase in uncertainty represented by the 95 simulation bands this increasing uncertainty is expected because the modelling framework in this study introduces climate and regression model uncertainty into estimation the estimation of future rainfall extremes under both climate warming scenarios rcp 4 5 and 8 5 shows a notable increase in rainfall amounts cyan bold line for wq50 and blue dashed lines for weighted 95 simulation bands together with the increase in wq50 extreme rainfall estimation for the two future scenarios there is a corresponding increase in the width of the simulation bands reflecting the increased error in estimation the change in simulation band width is greater at bochum than atherstone given that the error in the fitted blrp model parameter estimators is averaged across all new observations in the regression model it is expected that the larger increase in simulation band widths at bochum under the two climate change scenarios results from the regression uncertainty this hypothesis is explored further in the discussion in section 7 the greater expansion in simulation bands at bochum than atherstone may not be entirely unexpected because of the larger observed extremes in the historical period and concomitant larger estimation in simulation bands the growth in extremes at bochum appears to be larger than that at atherstone and hence the growth in simulation bands may also be expected to be faster than that at atherstone however it can be seen from the extreme value plots for rcp 8 5 fig 7 that the extreme value realisations which fall outside the upper wq97 5 simulation band for bochum predominantly green lines are more spread out than those for atherstone this potentially indicates that one or several of the cmip5 conditioned blrp models from the spectrum of greens in the legend performs much worse in estimating the ncep q50 at bochum than at atherstone this is reflected in the position of the wq97 5 upper simulation band giving confidence that the weighting applied is beneficial in adjusting the position of the 95 simulation bands in relation to all realizations however despite the use of weighted quantiles the generation of so many highly divergent extreme value realizations will impact on the location of the wq50 and wq97 5 given that the divergent extremes here are arising under rcp 8 5 it is necessary to investigate the range of extrapolation required for these models this is discussed further in section 7 7 discussion the results in section 6 show that the downscaling methodology set out in section 3 is successful at estimating rainfall extremes in the present climate and has the flexibility to enable estimation in changed climates which are broadly in line with anticipated increases in local extremes of between 5 10 per c see precipitation extremes in tfe 9 cliamte extremes in p 112 stocker et al 2013 central to the downscaling approach is the regression model which enables anticipated changes in rainfall extremes to be estimated with simulation model validation has confirmed that the regression model parameters are effective at estimating the observed rainfall statistics demonstrating that the models are well parameterised and that the rainfall generation process is robust this gives credibility to the choice of regression model 7 1 limits on extrapolation in a warming climate short duration extremes across europe may become more prevalent with many more months within the year experiencing convective storm characteristic of summer months this possible shift in rainfall pattern would be captured by the regression model on blrp parameter estimators within the range of target covariate values thereby realising the increasing frequency of extremes while more frequent summer storm events may increase the chance of higher intensity extremes any notable increase in intensity is likely to arise from the extrapolation of blrp parameter estimators for the hottest months in the year therefore an important consideration in the estimation of rainfall extremes in any hypothesised warmer climate with the downscaling approach presented is the range of extrapolation table 3 shows the maximum extrapolation in mean monthly near surface air temperature as a percentage of the target covariate range used in fitting for each cmip5 conditioned blrp model under rcp 4 5 there are a handful of models for which no extrapolation is required numbers 18 20 26 and 27 at bochum and numbers 1 and 2 at atherstone for all other models some extrapolation is required for between 1 and 4 months of the year under rcp 8 5 some extrapolation is required for all cmip5 conditioned blrp models for at least 2 months of the year and in some cases as many as 5 at bochum and 4 at atherstone the cmip5 model requiring the maximum extrapolation ipsl cm5a lr falls within the green spectrum of models in fig 7 and may be one of the models giving rise to the large number of very high extreme value realisations which fall outside the wq97 5 upper simulation band in fig 7 as highlighted in section 6 such divergent extremes can arise from poorly identified blrp model parameters upon examination of the parameter estimates for ipsl cm5a lr under rcps 4 5 and 8 5 we find that the regression uncertainty grows very quickly for bochum indicating that the estimated parameters for high extrapolations are less well identified fig 9 shows the regressions on the ipsl cm5a lr conditioned blrp estimators together with the sampling ranges for mvn distributions of blrp parameter estimators under rcps 4 5 blue hatched and 8 5 orange filled the maximum extrapolation for sampling as a percentage of the target covariate range presented in table 3 is shown in fig 9 in the panels for the blrp storm arrival rate parameter λ a1 f1 k1 p1 u1 z1 the same percentages apply to all other plots but are not repeated we can see from fig 9 that the regression uncertainty for two model parameters across each of the three blrp model variants grow exponentially within the range of extrapolation under rcp 8 5 for bochum β and γ for bl0 panels c1 and d1 and κ and ϕ for bl1 panels n1 and o1 and bl1m panels x1 and y1 in each case we can see that the sampling range at the maximum extrapolation under rcp 8 5 is much wider than the same for rcp 4 5 with these wider confidence intervals random sampling from the mvn distribution of blrp parameter estimators will result in many parameter sets which give rise to divergent extremes this uncertainty is caused by the fit of the regression model to the fitted blrp parameter estimators despite the greater extrapolation required for rcp 8 5 at atherstone 64 3 the extreme value estimation in fig 8 shows far fewer divergent extremes larger than the wq97 5 upper simulation band for atherstone and it is not clear from the colour scale for the cmip5 models whether those very high realisations are attributed to ipsl cm5a lr however we can see in fig 10 that the regression uncertainty for atherstone does not grow in the same way as it does for bochum with higher mean monthly temperatures the example of the growth in regression uncertainties for the ipsl cm5a lr conditioned blrp models at bochum and atherstone highlights an important limitation and potential source of criticism in the downscaling approach presented that is one of using a regression for extrapolation beyond the range of the data used to fit it in the context of estimating rainfall extremes in a climate warmer than today s using rainfall models it is inevitable that the rainfall model parameters will require updating the principal method for doing this is the delta change or factor of change approach which despite its own limitations see discussion in section 2 avoids the issue of extrapolation the primary concern with extrapolating regression models is the lack of knowledge about the shape the regression takes beyond the range of the data in the model applied here we have used simple linear regression on the natural logarithm of the fitted blrp parameter estimators to capture their underlying relation with mean monthly near surface air temperature while such a regression fails to match exactly the heterogeneity of individual estimators at different temperatures see figs 3 and 4 the high skill in the regression models to estimate rainfall extremes fig 7 and 8 and central moments of the rainfall time series figs 5 and 6 in the present climate give confidence in the choice of the regression model on the assumption that the linear relations between model parameters and temperature are valid for increasing unobserved temperatures and given that extrapolation is only required for between approximately 2 3 months of the year up to the most severe climate change projection we feel that the model proposed is appropriate however because of the potential for rapid growth in the regression uncertainty demonstrated in the example of ipsl cm5a lr it would be prudent to exercise caution in the use of downscaling based on regression a simple and pragmatic approach is to set an upper limit on extrapolation for the most severe climate projection as a means of identifying a subset of climate models for simulation in setting such a limit a balance is sought between avoiding the rapid growth in regression uncertainties for some cmip5 conditioned blrp models and retaining valuable information regarding the nature of climate change projected by the climate models if we leave out too many models from the ensemble which project higher changes in temperature than the others we may underestimate the change in rainfall extremes guided by the mean extrapolation across the complete ensemble of 29 cmip5 models 23 9 for rcp 4 5 and 27 for rcp 8 5 see table 3 we propose a limit of 30 at this limit the ensemble member sizes reduce to 21 for bochum and 18 for atherstone upon removing these models the number of extreme value realisations used for estimation reduces from 1160 to 840 at bochum and 720 at atherstone these are still a considerable number of realisations at each location therefore no further realisations are simulated fig 10 shows the updated extreme rainfall estimation for the new ensembles at bochum and table s 3 in the supplementary material the updated 10 and 100 year estimated rainfall amounts corresponding to the plots in fig 10 the new selections are listed in table s 4 in the supplementary material with new maximum extrapolations highlighted in red and the mean extrapolation and modal number of months for which extrapolation is required updated comparing the plots in fig 10 with those in fig 7 the change in bochum extremes for rcp 4 5 is negligible following the removal of 8 ensemble members although there is a noticeable change in the estimation of extremes for rcp 8 5 the extreme value realisations which exceed the wq97 5 are more tightly concentrated around the upper band this has resulted in a noticeable narrowing of the simulation bands for rcp 8 5 with the greatest change occurring in the location of the upper band the reductions in wq50 are marginal but the reductions in simulation band widths are substantial and represent greater confidence in the estimation of extremes reflecting the greater confidence in the estimation of blrp parameter estimators from the regression models 7 2 shift in simulation bands fixing a limit on extrapolation has improved our confidence in the estimation of rainfall extremes at bochum under rcp 8 5 although the total range of estimation has shifted marginally towards that of the present climate while we are unable to validate estimates of future rainfall it is useful to understand if those estimates are substantially different from those of the present climate fig 11 compares the range of extreme rainfall estimation at each location and climate forcing scenario with the estimation for the present climate from a visual inspection of the plots in fig 11 there is substantial overlap in estimation between sites and carbon forcing scenarios this is particularly true at atherstone under rcp 4 5 in which a very high proportion of the estimated future rainfall extremes are explained by the present day estimation within the individual plots in fig 11 are sub plots giving the overlap of the simulation bands as a percentage for return period estimation between 2 and 100 years the red dashed line gives the percentage of the historical estimation which overlaps the future this may be interpreted as the percentage of the uncertainty in the current climate estimates which is still valid in the future the two primary observations from fig 11 are that while the overlap in estimation increases for rarer events it reduces for the more severe climate forcing scenario rcp 8 5 these observations are intuitive and confirmed by the percentage values given in the sub plots with such substantial overlap at atherstone under rcp 4 5 it may be argued that rainfall extremes are not expected to change under this climate forcing scenario up to the year 2100 however at bochum between approximately 35 55 of future rainfall extremes are not estimated by the present climate therefore we argue that extremes in this location are anticipated to change under rcp 4 5 between now and 2100 with between approximately 30 65 of future extremes at atherstone and 40 75 at bochum under rcp 8 5 not estimated by the current climate we again argue that extremes are expected to increase in both locations under this climate change scenario 8 conclusions the downscaling methodology presented in this study offers a new approach for estimating rainfall extremes in a changing climate it extends the censored modelling approach for short duration rainfall extremes presented in cross et al 2018 into a downscaling framework by changing the conditioning variable from calendar month which is discrete to near surface air temperature which is continuous while the change in conditioning variable is not new following the development of a local generalised method of moments fitting procedure by kaczmarska et al 2015 the modelling framework presented here is to the best of our knowledge the first to use multivariate linear regression to simultaneously estimate the full set of blrp model parameter estimators in this research it is hypothesised that mechanistic rainfall model parameters may be approximated by regression without significant detriment to rainfall simulation and the estimation of fine scale extremes this hypothesis is born out of the recognition that parameter estimators are dependent on the arbitrary selection of training data by calendar month or season these selections are subjective and result in similar but varying parameterisations the same effect is replicated in fig 3 with the scatter plots for estimated parameters across all climate conditioned rainfall models because of the differences in simulated temperatures for the historical period from each climate model the sampling of months in the rainfall record differs slightly giving rise to marginal variations in the estimated parameters despite this we find that the resultant cloud of estimators for several model parameters follows an obvious dependence over narrow regions the objective of the regression model is to identify plausible regions in the parameter space within which well identified parameters can be sampled to ensure the reliability of observations used to fit the regression model a target covariate range for the conditioning covariate is identified based on a fixed k number of months the good performance of the ncep regression model in reproducing the fitting and validation statistics and the skill with which extremes are estimated in the current climate using regressions fitted to all cmip5 models support the research hypothesis these results also give confidence in the choice of regression model used a notable success of the downscaling methodology presented is the treatment of uncertainty the simulation and estimation of extremes incorporates rainfall model uncertainty the regression uncertainty the sampling uncertainty of the stochastic process and climate model uncertainty the choice of prediction interval for the regression model uncertainty ensures that both the error in the position of the regression line and the variance of the observations around this are included climate model uncertainty is accounted for with the ensemble of cmip5 climate model outputs and subsequent use of weighted quantiles to estimate simulation bands a limitation of the regression framework for downscaling is the potential for regression uncertainties to grow rapidly within the range of extrapolation required for more severe carbon forcing scenarios such as rcp 8 5 in this study we find that the different regression fits for the two gauges investigated resulted in the generation of poorly identified parameter estimators under rcp 8 5 at bochum but well identified estimators at atherstone this raises the important issue of using regression models for extrapolation a pragmatic solution is to place a limit on extrapolation and refine the cmip5 climate model ensemble to include only those which fall below this limit however specifying a limit on extrapolation is difficult because the level of extrapolation required is a function of the target covariate range which itself is a function of the chosen k number of months and the chosen increment between target values any imposed limit is also subjective a limit on extrapolation of 30 was used at both gauge locations to mitigate the impact of parameter uncertainty on estimation and to ensure that some climate conditioned rainfall models requiring above average extrapolation were retained despite this extrapolation is only needed for between approximately 2 and 3 months in the year therefore while the framework presented inevitably demands some extrapolation we feel that the combination of inherent and imposed limitations mean that the required extrapolation is acceptable in the context of simulating rainfall in a changing climate the choice of temperature as the conditioning variable in this research was strongly influenced by the clausius clapeyron relationship which explains the increase in atmospheric water vapour in a warming climate further research is required to investigate the relationship between blrp parameter estimators and other covariates to provide extra validation of the approach potentially with other model variants extension to multiple covariates would also enable the investigation of teleconnections such as the north atlantic oscillation which are known to influence european rainfall declaration of competing interest the authors declare they have no conflicts of interest acknowledgements david cross is grateful for the award of an industrial case studentship from the engineering and physical sciences research council epsrc in the uk and edf energy david would like to acknowledge pietro bernardara who initiated his phd with christian onof but who has since taken up a new role at edf in france we are grateful to clare goodess of the university of east anglia edward pope of the uk meteorological office and nadarajah ramesh of the university of greenwich for their valuable insights in steering this research we also thank the two anonymous referees and the editor for their constructive reviews and comments which have greatly improved this paper the environment agency of england is gratefully acknowledged for providing the uk rainfall data and deutsche montan technologie and emschergenossenschaft lippeverband in germany are gratefully acknowledged for providing the bochum data we acknowledge the world climate research programme s working group on coupled modelling which is responsible for cmip and we thank the climate modelling groups listed in table a 2 in appendix of this paper for producing and making available their model outputs for cmip the u s department of energy s program for climate model diagnosis and intercomparison provided coordinating support and led development of software infrastructure in partnership with the global organization for earth system science portals the ncep reanalysis data were provided by the noaa oar esrl psd boulder colorado usa from their website at https www esrl noaa gov psd appendix supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 103479 appendix d supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 supplementary data s2 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s2 
533,we present a new approach for estimating the frequency of sub hourly rainfall extremes in a warming climate with simulation by conditioning bartlett lewis rectangular pulse blrp rainfall model parameters on the mean monthly near surface air temperature we use a censored modelling approach with multivariate regression to capture the sensitivity of the full set of blrp parameter estimators to temperature enabling the parameter estimators to be updated the downscaling framework incorporates uncertainty in climate model projections for moderate and severe carbon forcing scenarios by using an ensemble of climate model outputs linear regression on the logarithm of blrp parameter estimators offers a robust model for parameter estimation with uncertainty the approach is tested with 5 min rainfall data from bochum in germany and atherstone in the united kingdom we find that the approach is highly effective at estimating rainfall extremes in the present climate and the estimation of future rainfall extremes appears highly plausible keywords mechanistic stochastic extremes rainfall climate change impacts k nearest neighbour multivariate regression 1 introduction drainage system design requires the robust estimation of rainfall extremes at fine spatial and temporal scales for decades this has been achieved with historical gauge records however the growing consensus that anthropogenic climate change is accelerating challenges the premise that climate stationarity can be assumed over the lifetime of civil infrastructure this not only impacts new build projects but has direct consequences for existing environments increasingly utility providers are required to assess the resilience of their assets to natural hazards while businesses and individuals alike seek greater insurance against perils not previously perceived our principal tools for projecting future climate feedbacks are global circulation gcm and earth system esm models however the coarse resolution of these models makes their outputs unsuitable for climate impact assessment at the urban scale burlando and rosso 1991 sunyer et al 2012 downscaling techniques are therefore required to quantify the effect of global scale warming on catchment processes numerous methods exist including resampling thorndahl et al 2017 scaling lenderink and attema 2015 method of fragments srikanthan and mcmahon 2001 disaggregation hay et al 1992 gyasi agyei 2011 adjusting procedures koutsoyiannis and onof 2001 yusop et al 2014 abdellatif et al 2013 debele et al 2007 and multifractal cascade models schmitt 2014 verrier et al 2011 however a popular approach is to use stochastic weather generators to synthesise long sequences of environmental variables at the catchment scale with perturbed parameters to reflect future changes at the climate scale these can be used to estimate changes to important properties such as variance and extremes or as inputs into hydrological models to investigate catchment response significant progress has been made in this respect since the early 1990s and is described in numerous reviews and comparison articles fowler et al 2007 sunyer and madsen 2009 maraun et al 2010 arnbjerg nielsen 2011 sunyer et al 2012 willems et al 2012 ailliot et al 2014 westra et al 2014 sunyer et al 2015 the purpose of this research is to investigate if temperature projections from an ensemble of cmip5 global climate models can be used to downscale 5 min rainfall extremes using stochastic simulation while there are many other environmental indicators of rainfall temperature is chosen based on its known high correlation with rainfall and the relatively high skill of climate models in estimating temperature compared with other variables furthermore several studies have investigated the relative importance of different environmental variables as indicators of rainfall with many concluding that temperature is a key indicator the bartlett lewis rectangular pulse blrp model is chosen to enhance the physical basis of extreme rainfall estimation in a changing climate mechanistic stochastic rainfall models such as the blrp model emulate the theory of rain cell clustering in storms consequently extremes are constructed from the superposition of rain cells making this class of model particularly appealing for extreme rainfall estimation taking the censored rainfall modelling approach introduced in cross et al 2018 in which blrp rainfall models are used to simulate the intense rainfall profile over a low censor we further develop the fitting methodology to allow for climate non stationarity motivated by the simple linear regression on summary statistics of wasko and sharma 2017 and the change of conditioning variable in kaczmarska et al 2015 the censored modelling approach is extended into a downscaling framework following kaczmarska et al 2015 we change the conditioning variable from calendar month to mean monthly near surface air temperature but use k nearest neighbour sampling to identify the training data for model parameter estimation to enable simulation under hypothesised warming scenarios we investigate the sensitivity of censored blrp model parameters to temperature using multivariate linear regression which preserves the known dependence between parameters the relation between rainfall model parameters and temperature is established with historical observations using an ensemble of mean monthly temperature projections from the cmip5 inter comparison experiment we estimate the change in rainfall extremes for the far future time window 2070 2100 under moderate and severe representative concentration pathways rcp4 5 and 8 5 respectively because the perturbation of model parameters is underpinned by a physical atmospheric process this downscaling approach is preferable to other established methods in particular change factors a review of the principal developments in rainfall downscaling is presented in section 2 in section 3 we set out the downscaling methodology for extreme rainfall simulation with censoring including blrp model fitting and parameter regression rainfall and climate model data are introduced in section 4 followed by the calibration and validation of the models in section 5 extreme rainfall estimation for present and future climates is presented in section 6 discussion and conclusions are given in sections 7 and 8 2 developments in rainfall downscaling 2 1 established methods early attempts at downscaling climate model outputs for hydrological analysis focussed on weather typing bárdossy and plate 1992 goodess and palutikof 1998 trigo and dacamara 2000 fowler et al 2000 bárdossy et al 2002 qian et al 2002 fowler et al 2005 in which stochastic weather generators are conditioned on homogeneous mesoscale circulation patterns this approach is attractive because circulation patterns describe the prevailing weather conditions on a given day and provide discrete integrated series of otherwise continuous non linear processes however because weather types are derived from pressure fields alone they are unlikely to fully capture the influence of thermodynamic effects on rainfall extremes maraun et al 2010 which are known to have a significant impact this approach has lost favour in the recent decade with greater emphasis being placed on factor of change methods factor of change methods also referred to as delta change or perturbation methods fowler et al 2007 apply simple scaling to historical summary statistics according to the change in control and future gcm outputs change factors cf are used to perturb weather generator parameters so that environmental variables may be simulated at the local scale but which are representative of changed future climates while they can be applied to any type of weather generator cfs have been used extensively with those based on the neyman scott rectangular pulse nsrp rainfall model following the development of several weather and rainfall simulation software tools including earwig kilsby et al 2007 awe gen fatichi et al 2011 and rainsim burton et al 2008 the environment agency rainfall and weather impacts generator earwig is a single site daily weather generator developed for the uk climate impacts programme ukcip02 by kilsby et al 2007 this downscaling tool comprises a two part stochastic weather generator which first simulates daily rainfall using the nsrp model and then daily means of temperature temperature range vapour pressure wind speed and sunshine duration using regressions on the rainfall observations cfs are calculated for the daily mean variance and skewness of rainfall and to a logit transformation of the proportion of dry days using ukcip02 future scenarios hulme et al 2002 the uk climate projections were updated in 2009 ukcp09 murphy et al 2009 following publication of the ipcc intergovernmental panel on climate change fourth assessment report ar4 correspondingly the earwig methodology was updated to provide climate change simulations with probabilistic projections from an ensemble of regional climate model rcm runs subsequently referred to as the ukcp09 weather generator ukcp09 wg this approach has formed the basis of significant research into catchment scale climate impacts including studies by manning et al 2009 burton et al 2010 and honti et al 2014 the advanced weather generator awe gen fatichi et al 2011 is a single site hourly weather generator and downscaling tool it is similar to the ukcp09 wg in structure although change factors are assigned probability distributions for a range of climate statistics and applied directly to the rainfall statistics used to fit the embedded nsrp model numerous climate impact studies have been undertaken using awe gen francipane et al 2015 liuzzo et al 2015 pumo et al 2017 to generate hourly rainfall time series for future climates a two dimensional simulator awe gen 2d was later developed by peleg et al 2017 in which the point nsrp model was replaced with the space time realizations of areal precipitation streap model paschalis et al 2013 for spatiotemporal rainfall simulation despite the significant structural and numerical developments in this model change factors are still used to estimate the future change in weather variables rainsim is an hourly spatiotemporal nsrp rainfall field generator developed by burton et al 2008 and is closely related to earwig and the ukcp09 wg it has been used in several climate impact studies using novel downscaling methods goderniaux et al 2011 scale nsrp change factors in rainsim proportionally to global temperature changes to investigate transient climate change impacts on groundwater resources in a two part publication bordoy and burlando 2014a b apply scaling laws to future daily summary statistics perturbed using change factors calculated at the daily scale the rescaled future summary statistics are used to reparameterise the space time rainsim nsrp model to simulate future sub daily rainfall in the swiss alps sørup et al 2016 use the space time rainsim model to investigate the climate change signal for extreme precipitation over a densely urbanized region of north eastern denmark for use in urban drainage design further applications of the change factor approach to downscaling can be found in the literature using similar stochastic rainfall models khazaei et al 2012 use the original nsrp model as the basis of a new weather generator for the estimation of minimum and maximum temperatures onof and arnbjerg nielsen 2009 use the random parameter bartlett lewis rectangular pulse blrp model with truncated gamma distribution to downscale rcm aerial rainfall to sub hourly point estimates using a combination of change factors and disaggregation using a semi markov chain model of rainfall events sørup et al 2017 apply change factors for system states which categorise events in the historical record change factors have been used extensively in the recent decades for climate downscaling with rainfall simulators because the method is conceptually tractable and computationally simple however it relies on a certain assumptions which are the matter of debate primarily factor of change methods assume that the scaling relationships of rainfall statistics or model parameters are stationary between present and future climates bordoy and burlando 2014a sunyer and madsen 2009 this may be appropriate in locations where there is no reason to believe that the mechanisms governing the local climate will alter significantly although there is no way to test this assumption for hypothesised future climates furthermore the relationship between mesoscale circulation and point precipitation is assumed to be valid forsythe et al 2014 on the basis that the phenomenology of rainfall generation in storms is scale invariant in each case there is no physical underpinning for these assumptions because the scaling relationships are not explained by physical processes or parameters 2 2 linking climate to rainfall trenberth et al 2003 set out the link between global warming and the intensification of rainfall extremes broadly concluding that as the climate warms atmospheric water vapour content will increase leading to more intense downpours explained by the clausius clapeyron c c relationship in which water saturation vapour pressure increases with temperature at a rate of 7 k 1 several studies have identified similar scaling relationships between high rainfall quantiles and air temperature hardwick jones et al 2010 shaw et al 2011 utsumi et al 2011 fujibe 2013 molnar et al 2015 supporting the hypothesis that rainfall extremes will intensify with increasing global temperatures further evidence is provided by westra et al 2013 who identify increasing dependence between maximum annual daily precipitation and globally averaged near surface temperature of between 5 9 and 7 7 k 1 for two thirds of over 8000 stations globally despite this local geography and climate feedbacks can dominate the precipitation temperature scaling ali and mishra 2017 identify negative scaling between surface air temperature and extreme rainfall in india because of the cooling effect of monsoon events hardwick jones et al 2010 report negative scaling for australia above 26 c while shaw et al 2011 fujibe 2013 and molnar et al 2015 find for the usa japan and switzerland respectively that scaling rates vary significantly depending on the region season temporal scale and rainfall type despite the mixed signals for the c c relationship it is physically appealing to identify dependence in precipitation at the local scale with exogenous variables at the mesoscale which capture thermodynamic effects such an approach has formed the basis of several studies in the recent decade using a modified markov model with kernel density estimator for rainfall occurrence and amounts mehrotra and sharma 2010 present a multisite stochastic downscaling framework in which both occurrence and amount models are conditioned on atmospheric variables the conditioning variables are chosen to capture atmospheric circulation moisture and short term rainfall persistence from rain gauge and ncep reanalysis data using climate projections of the exogenous conditioning variables from cisro mk3 climate model the model is used to evaluate changes in rainfall at the daily seasonal and annual scales between the current and future climates centred on 2070 the modelling framework presented by mehrotra and sharma 2010 offers a tool for climate impact assessment on water resources but urban scale impacts require rainfall simulation at shorter temporal scales mechanistic stochastic rainfall models offer this capability although as already highlighted downscaling with these models has largely focussed on change factor approaches recognising the demand for non stationary rainfall simulation in a changing climate and the limited development of transient model frameworks evin and favre 2013 reformulate the original nsrp model to allow the storm arrival process λ to vary with time with one additional model parameter representing the evolution in storm arrivals ϵ the new transient nsrp model has a total of 6 parameters a potential criticism of this approach is that the non stationarity captured in the temporal variation in λ is only representative of the historical period with global warming the rate of change may be different and could vary between the different carbon forcing scenarios set out in the ipcc s fifth assessment reports ar5 the reader is referred to the synthesis report by pachauri et al 2014 which combines the findings of the three contributing ipcc working groups wasko and sharma 2017 use linear regression to establish relationships between monthly rainfall summary statistics and mean monthly air temperature to obtain perturbed fitting statistics for the original nsrp model representative of warmer climates the simplicity of this approach makes it more pragmatic and potentially more informative than that of evin and favre 2013 because historical temperature changes implicit in evin and favre 2013 will not necessarily be representative of the future however a possible criticism is that by applying linear regression separately to each summary statistic the dependence structure between statistics may be lost taking a different approach to both evin and favre 2013 and wasko and sharma 2017 kaczmarska et al 2015 develop a new method to calibrate the original blrp model using continuous environmental covariates instead of calendar month using a nonparametric form of local linear regression blrp model parameters are estimated using monthly rainfall summary statistics coincident with monthly mean environmental variables the neighbourhood of summary statistics is weighted by a normalised gaussian distribution over all data points with a user defined bandwidth specifying the standard deviation of the distribution with this approach parameter estimation beyond the range of the conditioning covariate is highly uncertain because all conditioning months fall within the tails of the normal distribution which are down weighted this limits the usefulness of local regression for climate impact assessment for the most severe climate forcing scenarios and far future estimation 3 downscaling methodology the downscaling methodology set out here extends the censored rainfall modelling approach for the estimation of fine scale extremes introduced in cross et al 2018 to assess climate change impacts on intense rainfall in this paper bartlett lewis rectangular pulse models are used to mechanistically simulate short duration intense rainfall from which extremes may be estimated the method was motivated by the desire to improve the physical realism of extreme rainfall estimation for intense cloudburst type events to supplement frequency techniques and increase the amount of data used for estimation mechanistic stochastic rainfall models which include both the bartlett lewis and neyman scott varieties simulate rainfall by emulating the rainfall generating mechanism with poisson cluster processes in this mechanism rainfall observed on the ground arises from the clustering of rain cells in storms storm arrivals are simulated with a primary poisson process and rain cell arrivals with a secondary poisson process rain cells are typically assumed to be rectangular in shape describing their intensity and duration and storms are terminated according to an exponential distribution the summation of cells forms the unobserved continuous time rainfall which is aggregated to form discrete rainfall time series at any temporal scale because of the tendency for mechanistic stochastic rainfall models to under estimate rainfall extremes at fine temporal scales a censored approach was developed to simulate the intense rainfall profile in this approach a low censor is applied to the whole rainfall time series observations below the censor are replaced with zero rainfall and observations above the censor are reduced by the censor amount using three variants of the bartlett lewis rectangular pulse model the approach was applied to two rainfall records in the uk and germany for 5 and 15 min accumulations and shown to significantly improve the estimation of extremes sections 3 1 3 4 describe the downscaling methodology presented in this paper to aid understanding of the procedure a workflow of the steps involved is presented in fig 1 3 1 rainfall model calibration the first step in the downscaling methodology is to estimate model parameters that are optimal given the monthly near surface air temperature to this end we have to select periods within the historical rainfall record which are representative of a given temperature the standard method for fitting mechanistic rainfall models essentially uses seasonality as the conditioning covariate represented with non overlapping calendar months this is analogous to applying uniform weight to all target months in the historical record where the target month is the calendar month for which the model is being fitted the result of this is to obtain seasonally varying parameter estimators for each month of the year for the downscaling of rainfall extremes in this study we propose to change the conditioning covariate for censored rainfall models to mean monthly near surface air temperature for incremental changes in temperature we select the calibration data by applying uniform weights to months with mean monthly temperatures within a defined bandwidth in selecting the bandwidth there is a trade off between making it large enough to ensure optimised parameters are well identified while keeping it small enough to generate parameter sets which capture the variability of the rainfall generating process there are two obvious approaches for specifying the bandwidth for temperature this can be any fixed width in degrees celsius or any fixed number of months as per the standard fitting procedure because of the potential subjectivity of fixing a user specified bandwidth the latter is preferred a fixed number of months will result in different bandwidths measured in degrees celsius because of the variability in mean monthly temperature throughout the year selecting a fixed number of months is analogous to using the k nearest neighbour knn algorithm to classify the data in only one dimension with k equal to the selected number of months observations y are selected according to their euclidean distance d from a chosen value x using eq 1 1 d x y x 1 y 1 2 x 1 y 1 where x 1 and y 1 are the target and observed values for the single predictor the training data for model fitting at target value x is therefore based on the k nearest observations or the top k months taken from the rank of all calculated distances parameter estimation is performed using a generalised method of moments gmm with a weighted least squares objective function s θ t i 1 k ω i t i τ i θ 2 where θ θ 1 θ p t is the vector of p model parameters t t 1 t k t is the vector of k observed summary statistics τ θ τ 1 θ τ k θ t is the vector of k expected summary statistics and ωi is a weighting applied to the i th summary statistic the gmm minimisation routine employed in this research is performed using an updated version of the momfit software developed by chandler et al 2010 this software uses asymptotic theory to provide parameter estimators which are multivariate normally distributed mvn this captures the known dependence between blrp model parameters and enables parameter uncertainty to be propagated forward into the estimation of rainfall summary statistics and extremes the mean of the mvn distribution is the optimal parameter set and the variance covariance matrix is estimated from the least squares objective function s θ t chandler et al 2010 for consistency with the analysis and results presented in cross et al 2018 the downscaling approach is tested using the same three variants of the blrp rainfall models the original model bl0 by rodriguez iturbe et al 1987 the modified model bl1 by rodriguez iturbe et al 1988 in which the rain cell duration parameter η is randomized and the modified model with rain cell intensity μx also randomised bl1m by kaczmarska et al 2014 table a 1 in appendix compares the parameters of the three blrp model variants and provides the units for each parameter following the standard fitting procedure for seasonally varying parameters rainfall summary statistics are calculated for each month of the historical record however to fit the blrp models using the gmm it is necessary to select rainfall summary statistics coincident with selected covariate target values this is described in the following section 3 2 covariate target range knn sampling can be applied to any covariate target value within or outside the observed covariate range however for target values close to the extremes of the observed covariate range samples of historical months become increasingly unrepresentative of the target value because of the increasing scarcity of unique historical records because the downscaling methodology involves regression on the parameter estimators it is necessary to ensure that the selection of fitting months is representative of the covariate target values this gives rise to a covariate target range which is inevitably narrower than the observed range as k increases the covariate target range used for model fitting reduces this is shown graphically in fig 2 for bochum using the historical temperature projections from the ipsl cm5a mr climate model see section 4 2 for a description of the climate model data with k fixed at 52 104 and 156 months see section 4 1 for a description of the rain gauge data at bochum the mean monthly near surface air temperature ranges from 6 8 to 22 5 c for covariate target values incremented at 1 c an initial target range would span 6 to 22 c as shown by the series of box and whisker plots in fig 2 it can be seen in fig 2 that most selections closely follow the 1 1 line with both mean and median temperatures within the selection very close to the target value however this is not true at the extremes of the covariate range with several target values comprising identical selections with mean temperatures which deviate significantly from the target value grey box and whisker plots identical selections would result in identical blrp parameter estimators the covariate target range is therefore refined to remove these identical selections and those where the mean of the selection magenta line deviates by more than 1 c from the target value for k 52 these include 6 5 4 3 2 1 21 and 22 c see fig 2 therefore for k 52 months the covariate target range is 0 to 20 c shown by the series of blue box and whisker plots in fig 2 assuming that the selection bandwidth is defined by the total extent of the whiskers the bandwidths for k 52 range between approximately 1 and 5 c 3 3 model parameter regression we hypothesize that mechanistic stochastic rainfall model parameter estimators may be approximated by regression without significant detriment to rainfall simulation and the estimation of extremes therefore upon the calibration of model parameters for the full range of target values section 3 2 the relationship between parameter set estimators and temperature is approximated using multivariate linear regression in this framework we have multiple dependent response variables the set of blrp parameter estimators and one independent predictor variable our chosen covariate the mean monthly near surface air temperature the multivariate regression framework enables us to preserve the dependence between model parameters and is performed on the natural logarithm of the parameter estimators for consistency with blrp fitting methodology using the regression model we can estimate parameters within and beyond the range of present day monthly mean temperatures enabling estimation of rainfall extremes under hypothesised warmer climates for n observations m response variables and p predictor variables the multivariate regression model takes the form given in eq 2 2 y n m x n p 1 β p 1 m ϵ n m where y is the n m matrix of response variables x is the n p 1 design matrix for predictor variables β is the p 1 m matrix of regression coefficients and ϵ is the n m error matrix for our regression model n is the number of covariate target values m is the number of blrp model parameters and p 1 for the single predictor covariate the mean monthly near surface air temperature the matrix expansion of eq 2 is given in eq 3 3 y 1 1 y 1 m y n 1 y n m 1 x 1 p 1 1 x n p 1 b 0 1 b 0 m b p 1 1 b p 1 m e 1 1 e 1 m e n 1 e n m where the subscript p 1 refers to the single predictor variable here the mean monthly near surface air temperature upon fitting the regression model we obtain the matrix of regression coefficients and their error matrix which describes the regression uncertainty with these we can approximate the set of blrp parameter estimators for new observations within and beyond the covariate target range used to fit the regression for the simulation of rainfall extremes using the blrp models it is desirable to specify the multivariate normal distribution of parameter estimators so that realistic simulation bands may be produced incorporating parameter uncertainty 1 1 the blrp parameter estimators are asymptotically normally distributed this requires the mean of the distribution which is the optimal parameter set and its corresponding variance covariance matrix which describes the rainfall model uncertainty in the downscaling approach the mean of the distribution is replaced by the regression estimates at the target values however the rainfall model uncertainty needs to be updated to incorporate the regression uncertainty to obtain the combined temperature conditioned model uncertainty the regression uncertainty comprises two sources of error which arise from predicting the response variables for new observations from the regression model the first describes the positions of the regression lines at the prediction value and the second the variance around this assuming yi is the predicted response vector for the ith observation xi the variance of the expectation of response variables e yi which describes the position of the regression lines is var β t x i β t x i where β x i t x i 1 x i t y i is the least squares estimate of β the variance of the response variables around the regression lines is var y i β t x i setting σ var ϵ the prediction variance is σ 1 h where h x x t x 1 x t predicted response variables are assumed to be multivariate normally distributed hence we have the distribution of response variables given in eq 4 4 y β t x mvn x β σ 1 h the regression error variance σ is unknown therefore the unbiased estimate of the error variance for covariate xi is σ i sscp ϵ i n p 1 where sscp ϵ i ϵ i t ϵ i is the sum of squares and cross products of the residuals therefore the estimated prediction variance of response variables yi the regression uncertainty is var y i β t x i σ i 1 h this is added to the variance of the blrp parameter estimators the rainfall model uncertainty to obtain the updated variance for the blrp parameters the temperature conditioned model uncertainty for predicted blrp parameter estimators beyond the range of observations either the variance for the nearest blrp parameter estimator can be used or the mean of the blrp parameter variances therefore the multivariate normal distribution of predicted blrp parameter estimators from the regression model y blrp is given in eq 5 where σ bl is the mean rainfall model uncertainty 5 y blrp mvn x β σ 1 h σ bl 3 4 simulation and weighting of ensemble extremes when the conditioning covariate is changed to mean monthly air temperature the number of parameter sets used to capture the annual variation in rainfall depends on the increments over which the blrp model is fitted and the covariate target range the estimated blrp parameters then become the data used to fit the regression model which has only 10 regression parameters the intercept b 0 and slope bp of the separate blrp parameter regressions example regression parameters for the ncep conditioned blrp models are provided in table a 3 in appendix despite this rainfall simulation is performed in the standard way with 12 parameter sets or blrp models one for each calendar month of the year in the downscaling framework the monthly blrp parameter sets are estimated from the regression model rainfall simulation follows the same procedure as that set out in cross et al 2018 by randomly sampling n times from the distribution of blrp parameter estimators to generate n realisations of simulated rainfall above the predefined censor by sampling from the updated multivariate normal distribution of blrp parameter estimators given in eq 5 the spread of realisations generates simulations bands which capture the different sources of error in the model these include the rainfall model uncertainty the regression uncertainty and the sampling uncertainty which arises in simulation for the estimation of rainfall extremes mean monthly near surface air temperatures are selected from climate model outputs using an ensemble of cmip5 models the rainfall models are initially calibrated using the historical climate model run before applying the regression model to blrp parameter estimators then present and future climate extremes are estimated using mean monthly air temperature sampled from the historical and future climate model simulations respectively for the estimation of future extremes temperature projections are obtained corresponding to a chosen representative concentration pathway rcp see moss et al 2010 and vuuren et al 2011 and time slice the latter may represent any future period typically 30 years in duration up to 2100 the temporal extent of most climate model runs hence estimation for a high concentration pathway in the far future would use the climate model projection for rcp8 5 spanning 2070 2100 from this 30 year time slice the mean monthly air temperature for each calendar month is calculated and used as a new observation in the regression model to generate the distribution of blrp parameter estimators for simulation the benefit of using an ensemble of climate model runs in the estimation of rainfall extremes is to reduce the bias of individual climate models however it is recognised that some climate conditioned rainfall models will perform better at estimating rainfall extremes than others therefore a simple weighting is applied to each model according to its ability to reproduce 5 min rainfall extremes estimated from reanalysis conditioned rainfall models for the present climate extremes are estimated for a range of return periods using simulated annual maxima a weighted root mean square error rmse score is calculated for each climate conditioned model which applies greater weight to the ability of the models to reproduce higher return period extremes this approach was chosen to ensure that all extremes were used in the weighting while placing greater importance on the larger extremes while it is recognised that larger extremes have larger variance these extremes are more likely to be representative of convective summer storms than the more frequent ones the weighted rmse gives relative scores for each climate conditioned model enabling them to be ranked according to their extreme value performance this ranking is used to weight the quantiles of the extreme value estimates across all realisations thereby generating weighted median and 95 simulation bands 4 data 4 1 rain gauge data the downscaling of fine scale rainfall extremes with censoring for hypothetical warming climates using the methodology set out in section 3 is performed for the two rainfall datasets used to present the censored approach for the estimation of fine scale extremes in cross et al 2018 bochum in germany and atherstone in the uk both datasets were chosen for their quality and length 69 and 48 years because the methods of recording is different hellmann floating pen and tipping bucket gauges respectively and because of their different locations the bochum data are aggregated to the 5 min temporal resolution while tip time data can be aggregated to any scale guided by the results presented in cross et al 2018 the downscaling approach is performed for 5 min accumulations using a constant censor of 0 5 mm at both sites for bochum downscaling is based on a subset of the rainfall record from 1948 to 1999 to coincide with the data range of the ncep reanalysis data section 4 3 which is used for climate model weighting and blrp model validation section 5 the atherstone dataset commences in 1967 after the start of the ncep reanalysis data on january 1st 1948 and extends to 2015 4 2 climate model data in this study both present and future climate extremes are estimated using an ensemble of long term historical and future rcp climate simulations from phase 5 of the climate model intercomparison project cmip5 taylor et al 2012 the cmip5 experiments provide climate projections for a range of common scenarios in a multi model context they are differentiated by their starting point in the pre industrial control period realisation their initialisation method and physics physical constraints or parameterisation and are identified by the rip nomenclature r ealisation i nitialisation p hysics to test the downscaling methodology only long term simulations relating to ensemble number r1i1p1 were used with two climate forcing scenarios rcp4 5 and 8 5 chosen to be indicative of moderate to severe climate change a total of 29 models within the suite of cmip5 climate models were identified with the historical and two future scenarios available these are listed in table a 2 in appendix the output variable from these climate models used for analysis is the mean monthly near surface air temperature with the common abbreviation tas historical and future time series were sampled for the closest climate model grid squares to the gauge latitude and longitude locations 51 49 7 22 and 52 58 358 47 for bochum and atherstone respectively 4 3 climate reanalysis data the reanalysis data used for climate model weighting and blrp model validation is the ncep ncar national centers for environmental prediction reanalysis 1 model output from the national oceanic and atmospheric administration noaa earth systems research laboratory kalnay et al 1996 outputs for the mean monthly near surface air temperature are used for grid squares closest to the gauge latitude and longitude locations 51 49 7 22 and 52 58 358 47 for bochum and atherstone respectively which are available from january 1st 1948 5 calibration and validation 5 1 model fitting and simulation the estimation of fine scale rainfall extremes in a changing climate with censoring is first performed for the present climate both censored and uncensored extremes extremes obtained with and without censoring applied are estimated to confirm that similar improvement in estimation to that observed in cross et al 2018 is achieved when the conditioning variable is changed to mean monthly near surface air temperature annual maxima are sampled directly from simulations and the estimation of extremes is based on the weighted 50th quantile wq50 with 95 simulation bands given by weighted 2 5th and 97 5th quantiles wq2 5 and wq97 5 respectively the calibration methodology and conditioning information is set out in the remainder of this section section 5 2 presents the results of the model calibration and regression and section 5 3 the results of the validation the choice of fitting statistics at each gauge location is summarised in table 1 with timescales given in brackets because the gamma shape parameter α in the two randomised models bl1 and bl1m is insensitive and can take a wide range of values without significant impact on the estimation of extremes cross et al 2018 it is held fixed at 100 and 5 respectively the ratio of the standard deviation to mean of the cell depth r σ x μ x is fixed at 1 for all three models thereby specifying the exponential distribution for the mean cell intensity by holding these parameters fixed optimisation is only required for 5 parameters in each model bl0 λ β μx η γ bl1 λ κ μx ν ϕ and bl1m λ κ ι ν ϕ for each of the climate conditioned blrp rainfall models 40 time series realizations are simulated with duration equivalent to the duration of the observed record 47 years at atherstone and 52 years at bochum this number of realisations is large enough to give a robust estimation of the median extreme rainfall required for individual model weighting but because the total ensemble of climate conditioned models is large with 29 members the resulting number of realisations for estimation of the wq50 is very large 1160 realisations each realization is generated using a different parameter set sampled from the multivariate normal distribution of model parameter estimators given in eq 5 hereafter referred to as mvn realizations the fitting window is based on k equal to the duration of the rainfall records in years 47 and 52 months for atherstone and bochum respectively by analogy with the standard fitting methodology this is equivalent to using one calendar month albeit months may be selected from anywhere in the rainfall record according to their historical mean monthly air temperature initial model fitting using this approach resulted in some parameter sets being poorly identified in these cases k is increased by including additional months from the selections for adjacent target values this procedure is repeated until the upper limit on parameter uncertainty for all model parameters is reduced to below an arbitrary threshold of 250 units vary for each paramter see table a 1 in appendix or for a maximum of 5 iterations by including additional months from adjacent selections in this way the fitting window is effectively widened locally with k for subsequent target value parameterisations returning to the original selections 5 2 bartlett lewis parameter estimators fig 3 shows the full range of censored blrp model parameter estimators conditioned on near surface air temperature using outputs for the historical periods for each of the 29 cmip5 climate models and the ncep reanalysis data the plots reveal a clear dependence between temperature and several of the blrp model parameters at both locations including increasing relations in the storm arrival rate λ mean cell depth μx or ratio of mean cell depth to cell duration ι and decreasing relations in the cell duration parameter η or the ratio of the gamma shape and scale parameters α ν for the randomised models the remaining parameters cell arrival rate β storm duration γ and the ratios of cell arrival rate and storm duration to cell duration κ and ϕ respectively show less obvious relations broadly the relations appear to follow the same pattern at both sites and the spread of parameter estimators across all 29 cmip5 climate models and the ncep reanalysis is small indicating high consistency in the fitting it is also noted that the ncep conditioned parameter estimators sit inside the range of the 29 cmip5 climate model conditioned estimators as would be expected for reanalysis data by means of an example the fitted regression relationships with associated uncertainties for the ncep conditioned rainfall models are shown in fig 4 the plots show the rainfall model uncertainty magenta polygons ncep cond 95 bl estimator cis regression uncertainty blue polygons blrp 95 regression pis and the combined climate conditioned blrp model uncertainty blue dashed lines blrp 95 regression uncertainty as noted in section 3 3 the regression uncertainty comprises two additive error sources which when combined comprise the prediction intervals pis for the regression model multivariate analysis of variance manova is used to simultaneously test the response of the blrp parameter estimators to the ncep mean monthly near surface air temperatures the primary test statistics pillai s trace the approximate f statistic which determines the significance level of the regression and the p value associated with the f statistic are provided in table 2 for all three models at both locations the p value is very small indicating that the f statistic is highly significant and consequently the null hypothesis that temperature has no effect on the mean blpr parameter estimators is rejected the high pillai trace values demonstrate that temperature is highly informative for each of the fitted regressions while the p values in table 2 indicate that we can reject the null hypothesis of no relationship for the multivariate regressions testing each variable individually with a univariate analysis of variance anova appendix table a 3 shows that for some blrp parameter estimators such as the cell arrival rate β and storm duration γ in the bl0 model there are little or no differences in their means at the 5 significance level p values 0 05 correspondingly we see that the coefficient of determination r 2 for those parameters is low indicating that only a small percentage of the variance in those parameters is explained by their individual regressions however their inclusion in the multivariate regression is important for the estimation of the variance covariance of model parameters the regression parameter estimators b 0 and bp in eq 2 are also shown in appendix table a 3 because the regressions are performed on the natural logarithm of the rainfall model parameters the units of b 0 are the natural logarithm of the rainfall model parameter units and the units of bp are same but per kelvin the plotted regressions are shown extending beyond the range of observations used for fitting to indicate the nature of the regressions when extrapolating where the parameter response is broadly flat cell arrival rate β storm duration γ and the ratios of cell arrival rate and storm duration to cell duration κ and ϕ respectively the regression uncertainties grow much more quickly this is likely to result from one or several blrp parameter estimators lying further away from the best fit regression line thereby increasing the error variance of the regression model greater error variance is likely to arise in parameters which are relatively to the other model parameters less sensitive to the training data this may also be indicative of multiple fitting regions in the blrp parameter space resulting in larger deviations in the fitted parameters between some covariate target values because all fitted blrp parameter estimators are well identified they are assumed to be robust estimators for inclusion in the regression model however it is recognised that the rapid growth in regression uncertainty may be an issue when sampling from the mvn distribution of parameter estimators in simulation if the estimation of extremes is sensitive to large deviations in these parameters from their mean the regression uncertainty blue polygons in fig 4 show the 95 pis for the multivariate regression a visual inspection of the regression plots shows that in most instances the pis bracket most of the fitted blrp parameter estimators exceptions include bochum bl0 β bl1 λ κ and bl1m λ ι κ where only a few parameters fall just outside the pis none of the estimators for atherstone appear to fall outside the 95 pis this level of accuracy can reasonably be expected and indicates that the regression fitting is successful the updated climate conditioned blrp uncertainty blue dashed lines blrp 95 regression uncertainty in fig 4 are consistently wider than the regression uncertainty and do bracket all blrp parameter estimators and much more of the rainfall model uncertainty it is important to capture as much of the rainfall model uncertainty as possible to ensure that range over which blrp parameter estimators are randomly sampled in simulation is characteristic of the original rainfall model fitting 5 3 replication of summary statistics it is important to verify that the sampling of regressed parameters from the multivariate normal distribution given in eq 5 provides blrp parameter sets which satisfactorily reproduce the statistical profile of the data to which they are fitted given that it is neither possible to validate larger extremes in the current climate nor extremes in any hypothesised changed climate validation of the process upon which these extremes are generated is essential for the extreme value estimates to be credible validation is performed in the standard way for mechanistic stochastic rainfall models first we verify that the parameter estimators can reproduce the summary statistics used to derive them while it seems intuitive that they would this is not always true if the parameters are poorly identified or the objective function value is significantly different from zero next we check how well the blrp parameter estimators reproduce other summary statistics not used in the fitting because of the two stage fitting process used in this study validation statistics are presented for the blrp parameter estimators used to fit the regression model and blrp parameter estimators derived from the regression model at the target covariate values while there are many models in the cmip5 ensemble validation statistics are only calculated for the ncep conditioned blrp model because the bias of reanalysis outputs is marginal and the ncep conditioned extreme value estimates are used to weight the performance of the 29 cmip5 conditioned blrp models fig 5 shows the variation in censored 5 min rainfall summary statistics used to fit the blrp models mean coefficient of variation and lag 1 autocorrelation see table 1 with mean monthly near surface air temperature for validation of the fitting statistics the mean rainfall is re scaled from 1 h to 5 min the top 6 panels show the variation in summary statistics for fitted model parameters at both locations and the bottom 6 panels the equivalent statistics for estimated parameters using the regression model in eq 5 fig 6 shows the ability of the models to reproduce a selection of validation statistics not used in the fitting the variance lag 1 autocovariance and proportion of wet periods skewness has been left out of this selection because as highlighted in cross et al 2018 the blrp models are not expected to reproduce the skewness well for censored data at bochum the target covariate values for the ncep reanalysis data range from 1 to 18 c and at atherstone from 4 to 17 c in each case at 1 c increments summary statistics are estimated under each blrp model variant using 100 parameter sets randomly sampled from the mvn distribution of parameter estimators and used to generate simulation bands from the 2 5th and 97 5th quantiles the observed summary statistics are calculated from the selection of rainfall months corresponding to the separate target covariate values summary statistics relating to the mean of the distribution are also calculated using the optimal parameter sets all three models perform equally well in replicating both the fitting and validation statistics with little variation in the statistics estimated by each of the models for all target covariate values the only exception to this is the performance of the bl1 and bl1m models at bochum for the target value of 12 c where the width of the simulation bands on the estimated summary statistics diverges this divergence occurs because the parameter set for 12 c is less well identified than those at the other target values see fig 4 bochum bl1 and bl1m κ and ϕ the 95 simulation bands for the estimated fitting statistics comfortably bracket the observed for all target covariate values despite their narrow range except for 12 c in bochum see fig 5 conversely the simulation bands for the validation statistics do not always bracket the observed at the target covariate values fig 6 there is very good agreement between the estimated and observed validation summary statistics at bochum with the proportion of wet periods only falling outside the simulation bands once at target value 4 c the blrp regression model parameters perform very well in reproducing the fitting and validation statistics the response of the estimated summary statistics under the regressed parameter estimators is much smoother than with the fitted parameters which is expected however the response clearly follows the apparent underlying dependence in statistics with temperature indicated by the observations this is a very pleasing result as the regression is performed on the parameter estimators themselves and not the observations the simulation bands appear to be slightly wider than those for the fitted model parameters which is also expected because of the additional regression error and they comfortably bracket all observed fitting statistics and most observed validation statistics the only exceptions here occur in the estimation of validation statistics at atherstone where the observed lag 1 auto covariance falls outside the simulation bands at 9 10 and 14 c and the proportion of wet periods fall outside the simulation bands between 4 and 7 c these results support the hypothesis that mechanistic rainfall model parameters may be approximated by regression without significant detriment to rainfall simulation a notable observation about the rainfall statistics shown in figs 5 and 6 is their interesting temperature dependence the coefficient of variation and lag 1 autocorrelation show approximate linear relations with increasing temperatures falling and rising respectively while all other statistics show non linear rising relations which for the mean variance and lag 1 auto covariance appear nearly flat for colder temperatures especially at bochum where the temperatures drop to 1 c because these dependencies are characteristic of rainfall over a low censor they demonstrate that intense rainfall is strongly influenced by the near surface air temperature this is important for capturing rainfall extremes arising from different rainfall events and potentially offers an improvement over fitting to calendar month where both stratiform and convective events may be lumped together in any of the summer months it is therefore particularly pleasing that the nature and variation of these dependencies is well captured by both the rainfall model and temperature dependent regression model parameters for the purpose of estimating rainfall extremes with censoring these statistics demonstrate that the models are well parameterised and that the process of rainfall generation on which the estimation of extremes depends is robust the plots in both figures also show that important central moments of the rainfall time series are strongly correlated with temperature supporting the selection of near surface air temperature as an appropriate conditioning covariate 6 extreme rainfall estimation figs 7 and 8 show the estimation of 5 min rainfall extremes at bochum and atherstone respectively for the complete ensemble of 29 cmip5 climate model conditioned blrp rainfall models estimation is shown for all three blrp models and three climate scenarios historical hist medium carbon forcing rcp45 and severe carbon forcing rcp85 the plots also include un censored estimation for the present climate to demonstrate the benefit of censoring for extreme rainfall estimation tables s 1 and s 2 in the supplementary material provide the 10 and 100 year return period weighted extreme rainfall estimates in mm for bochum and atherstone respectively corresponding to the plots given in figs 7 and 8 as well as the width of the 95 simulation bands sbs at those return periods 2 2 the tabulated results are estimated from the nearest observed simulated values with actual return periods of 9 4 and 93 1 years at bochum and 10 3 and 84 1 years at atherstone the difference in these return periods has arisen from the length of realisations but is considered to represent a sufficiently accurate estimate of the 10 and 100 year rainfall amounts at these gauge locations the first and most important observation from the plots in figs 7 and 8 is that the estimation of rainfall extremes with temperature dependent censored rainfall modelling for the historical period is very good consistent with the estimation of short duration extremes by mechanistic stochastic rainfall models fitting to the whole un censored rainfall hyetograph results in their under estimation grey lines and simulation bands the improvement in extreme rainfall estimation with censoring is very similar to that in cross et al 2018 with the bl1m model once again performing slightly better than the other two and comparable width in simulation bands for all three models the similarity in these results is particularly notable given that estimation in this study differs from that in cross et al 2018 in four key ways 1 the change in conditioning variable 2 the change in fitting window 3 the use of blrp parameter estimators from the regression model given in eqs 5 and 4 the use of weighted quantile estimation across multiple climate model conditioned blrp realisations the estimation of historical extremes at both sites is very good with all observations at bochum falling within the weighted 95 simulation bands and only one observation the smallest falling outside the simulation bands for atherstone at both gauges and for all three blrp model variants the wq50 provides very good estimation of the observed annual maxima up to approximately the 10 year return period rainfall event for rarer events the bl0 and bl1 models slightly underestimate the observed extremes although the bl1m model performs better at atherstone the wq50 for the bl1m model estimates marginally higher extremes between the 10 and 100 year return periods than the bl0 and bl1 models however at bochum the bl1m model wq50 accurately estimates the highest observed extreme in comparison with the extreme rainfall estimation for the same sites and historical period in cross et al 2018 the mean estimation is very similar although there is an increase in uncertainty represented by the 95 simulation bands this increasing uncertainty is expected because the modelling framework in this study introduces climate and regression model uncertainty into estimation the estimation of future rainfall extremes under both climate warming scenarios rcp 4 5 and 8 5 shows a notable increase in rainfall amounts cyan bold line for wq50 and blue dashed lines for weighted 95 simulation bands together with the increase in wq50 extreme rainfall estimation for the two future scenarios there is a corresponding increase in the width of the simulation bands reflecting the increased error in estimation the change in simulation band width is greater at bochum than atherstone given that the error in the fitted blrp model parameter estimators is averaged across all new observations in the regression model it is expected that the larger increase in simulation band widths at bochum under the two climate change scenarios results from the regression uncertainty this hypothesis is explored further in the discussion in section 7 the greater expansion in simulation bands at bochum than atherstone may not be entirely unexpected because of the larger observed extremes in the historical period and concomitant larger estimation in simulation bands the growth in extremes at bochum appears to be larger than that at atherstone and hence the growth in simulation bands may also be expected to be faster than that at atherstone however it can be seen from the extreme value plots for rcp 8 5 fig 7 that the extreme value realisations which fall outside the upper wq97 5 simulation band for bochum predominantly green lines are more spread out than those for atherstone this potentially indicates that one or several of the cmip5 conditioned blrp models from the spectrum of greens in the legend performs much worse in estimating the ncep q50 at bochum than at atherstone this is reflected in the position of the wq97 5 upper simulation band giving confidence that the weighting applied is beneficial in adjusting the position of the 95 simulation bands in relation to all realizations however despite the use of weighted quantiles the generation of so many highly divergent extreme value realizations will impact on the location of the wq50 and wq97 5 given that the divergent extremes here are arising under rcp 8 5 it is necessary to investigate the range of extrapolation required for these models this is discussed further in section 7 7 discussion the results in section 6 show that the downscaling methodology set out in section 3 is successful at estimating rainfall extremes in the present climate and has the flexibility to enable estimation in changed climates which are broadly in line with anticipated increases in local extremes of between 5 10 per c see precipitation extremes in tfe 9 cliamte extremes in p 112 stocker et al 2013 central to the downscaling approach is the regression model which enables anticipated changes in rainfall extremes to be estimated with simulation model validation has confirmed that the regression model parameters are effective at estimating the observed rainfall statistics demonstrating that the models are well parameterised and that the rainfall generation process is robust this gives credibility to the choice of regression model 7 1 limits on extrapolation in a warming climate short duration extremes across europe may become more prevalent with many more months within the year experiencing convective storm characteristic of summer months this possible shift in rainfall pattern would be captured by the regression model on blrp parameter estimators within the range of target covariate values thereby realising the increasing frequency of extremes while more frequent summer storm events may increase the chance of higher intensity extremes any notable increase in intensity is likely to arise from the extrapolation of blrp parameter estimators for the hottest months in the year therefore an important consideration in the estimation of rainfall extremes in any hypothesised warmer climate with the downscaling approach presented is the range of extrapolation table 3 shows the maximum extrapolation in mean monthly near surface air temperature as a percentage of the target covariate range used in fitting for each cmip5 conditioned blrp model under rcp 4 5 there are a handful of models for which no extrapolation is required numbers 18 20 26 and 27 at bochum and numbers 1 and 2 at atherstone for all other models some extrapolation is required for between 1 and 4 months of the year under rcp 8 5 some extrapolation is required for all cmip5 conditioned blrp models for at least 2 months of the year and in some cases as many as 5 at bochum and 4 at atherstone the cmip5 model requiring the maximum extrapolation ipsl cm5a lr falls within the green spectrum of models in fig 7 and may be one of the models giving rise to the large number of very high extreme value realisations which fall outside the wq97 5 upper simulation band in fig 7 as highlighted in section 6 such divergent extremes can arise from poorly identified blrp model parameters upon examination of the parameter estimates for ipsl cm5a lr under rcps 4 5 and 8 5 we find that the regression uncertainty grows very quickly for bochum indicating that the estimated parameters for high extrapolations are less well identified fig 9 shows the regressions on the ipsl cm5a lr conditioned blrp estimators together with the sampling ranges for mvn distributions of blrp parameter estimators under rcps 4 5 blue hatched and 8 5 orange filled the maximum extrapolation for sampling as a percentage of the target covariate range presented in table 3 is shown in fig 9 in the panels for the blrp storm arrival rate parameter λ a1 f1 k1 p1 u1 z1 the same percentages apply to all other plots but are not repeated we can see from fig 9 that the regression uncertainty for two model parameters across each of the three blrp model variants grow exponentially within the range of extrapolation under rcp 8 5 for bochum β and γ for bl0 panels c1 and d1 and κ and ϕ for bl1 panels n1 and o1 and bl1m panels x1 and y1 in each case we can see that the sampling range at the maximum extrapolation under rcp 8 5 is much wider than the same for rcp 4 5 with these wider confidence intervals random sampling from the mvn distribution of blrp parameter estimators will result in many parameter sets which give rise to divergent extremes this uncertainty is caused by the fit of the regression model to the fitted blrp parameter estimators despite the greater extrapolation required for rcp 8 5 at atherstone 64 3 the extreme value estimation in fig 8 shows far fewer divergent extremes larger than the wq97 5 upper simulation band for atherstone and it is not clear from the colour scale for the cmip5 models whether those very high realisations are attributed to ipsl cm5a lr however we can see in fig 10 that the regression uncertainty for atherstone does not grow in the same way as it does for bochum with higher mean monthly temperatures the example of the growth in regression uncertainties for the ipsl cm5a lr conditioned blrp models at bochum and atherstone highlights an important limitation and potential source of criticism in the downscaling approach presented that is one of using a regression for extrapolation beyond the range of the data used to fit it in the context of estimating rainfall extremes in a climate warmer than today s using rainfall models it is inevitable that the rainfall model parameters will require updating the principal method for doing this is the delta change or factor of change approach which despite its own limitations see discussion in section 2 avoids the issue of extrapolation the primary concern with extrapolating regression models is the lack of knowledge about the shape the regression takes beyond the range of the data in the model applied here we have used simple linear regression on the natural logarithm of the fitted blrp parameter estimators to capture their underlying relation with mean monthly near surface air temperature while such a regression fails to match exactly the heterogeneity of individual estimators at different temperatures see figs 3 and 4 the high skill in the regression models to estimate rainfall extremes fig 7 and 8 and central moments of the rainfall time series figs 5 and 6 in the present climate give confidence in the choice of the regression model on the assumption that the linear relations between model parameters and temperature are valid for increasing unobserved temperatures and given that extrapolation is only required for between approximately 2 3 months of the year up to the most severe climate change projection we feel that the model proposed is appropriate however because of the potential for rapid growth in the regression uncertainty demonstrated in the example of ipsl cm5a lr it would be prudent to exercise caution in the use of downscaling based on regression a simple and pragmatic approach is to set an upper limit on extrapolation for the most severe climate projection as a means of identifying a subset of climate models for simulation in setting such a limit a balance is sought between avoiding the rapid growth in regression uncertainties for some cmip5 conditioned blrp models and retaining valuable information regarding the nature of climate change projected by the climate models if we leave out too many models from the ensemble which project higher changes in temperature than the others we may underestimate the change in rainfall extremes guided by the mean extrapolation across the complete ensemble of 29 cmip5 models 23 9 for rcp 4 5 and 27 for rcp 8 5 see table 3 we propose a limit of 30 at this limit the ensemble member sizes reduce to 21 for bochum and 18 for atherstone upon removing these models the number of extreme value realisations used for estimation reduces from 1160 to 840 at bochum and 720 at atherstone these are still a considerable number of realisations at each location therefore no further realisations are simulated fig 10 shows the updated extreme rainfall estimation for the new ensembles at bochum and table s 3 in the supplementary material the updated 10 and 100 year estimated rainfall amounts corresponding to the plots in fig 10 the new selections are listed in table s 4 in the supplementary material with new maximum extrapolations highlighted in red and the mean extrapolation and modal number of months for which extrapolation is required updated comparing the plots in fig 10 with those in fig 7 the change in bochum extremes for rcp 4 5 is negligible following the removal of 8 ensemble members although there is a noticeable change in the estimation of extremes for rcp 8 5 the extreme value realisations which exceed the wq97 5 are more tightly concentrated around the upper band this has resulted in a noticeable narrowing of the simulation bands for rcp 8 5 with the greatest change occurring in the location of the upper band the reductions in wq50 are marginal but the reductions in simulation band widths are substantial and represent greater confidence in the estimation of extremes reflecting the greater confidence in the estimation of blrp parameter estimators from the regression models 7 2 shift in simulation bands fixing a limit on extrapolation has improved our confidence in the estimation of rainfall extremes at bochum under rcp 8 5 although the total range of estimation has shifted marginally towards that of the present climate while we are unable to validate estimates of future rainfall it is useful to understand if those estimates are substantially different from those of the present climate fig 11 compares the range of extreme rainfall estimation at each location and climate forcing scenario with the estimation for the present climate from a visual inspection of the plots in fig 11 there is substantial overlap in estimation between sites and carbon forcing scenarios this is particularly true at atherstone under rcp 4 5 in which a very high proportion of the estimated future rainfall extremes are explained by the present day estimation within the individual plots in fig 11 are sub plots giving the overlap of the simulation bands as a percentage for return period estimation between 2 and 100 years the red dashed line gives the percentage of the historical estimation which overlaps the future this may be interpreted as the percentage of the uncertainty in the current climate estimates which is still valid in the future the two primary observations from fig 11 are that while the overlap in estimation increases for rarer events it reduces for the more severe climate forcing scenario rcp 8 5 these observations are intuitive and confirmed by the percentage values given in the sub plots with such substantial overlap at atherstone under rcp 4 5 it may be argued that rainfall extremes are not expected to change under this climate forcing scenario up to the year 2100 however at bochum between approximately 35 55 of future rainfall extremes are not estimated by the present climate therefore we argue that extremes in this location are anticipated to change under rcp 4 5 between now and 2100 with between approximately 30 65 of future extremes at atherstone and 40 75 at bochum under rcp 8 5 not estimated by the current climate we again argue that extremes are expected to increase in both locations under this climate change scenario 8 conclusions the downscaling methodology presented in this study offers a new approach for estimating rainfall extremes in a changing climate it extends the censored modelling approach for short duration rainfall extremes presented in cross et al 2018 into a downscaling framework by changing the conditioning variable from calendar month which is discrete to near surface air temperature which is continuous while the change in conditioning variable is not new following the development of a local generalised method of moments fitting procedure by kaczmarska et al 2015 the modelling framework presented here is to the best of our knowledge the first to use multivariate linear regression to simultaneously estimate the full set of blrp model parameter estimators in this research it is hypothesised that mechanistic rainfall model parameters may be approximated by regression without significant detriment to rainfall simulation and the estimation of fine scale extremes this hypothesis is born out of the recognition that parameter estimators are dependent on the arbitrary selection of training data by calendar month or season these selections are subjective and result in similar but varying parameterisations the same effect is replicated in fig 3 with the scatter plots for estimated parameters across all climate conditioned rainfall models because of the differences in simulated temperatures for the historical period from each climate model the sampling of months in the rainfall record differs slightly giving rise to marginal variations in the estimated parameters despite this we find that the resultant cloud of estimators for several model parameters follows an obvious dependence over narrow regions the objective of the regression model is to identify plausible regions in the parameter space within which well identified parameters can be sampled to ensure the reliability of observations used to fit the regression model a target covariate range for the conditioning covariate is identified based on a fixed k number of months the good performance of the ncep regression model in reproducing the fitting and validation statistics and the skill with which extremes are estimated in the current climate using regressions fitted to all cmip5 models support the research hypothesis these results also give confidence in the choice of regression model used a notable success of the downscaling methodology presented is the treatment of uncertainty the simulation and estimation of extremes incorporates rainfall model uncertainty the regression uncertainty the sampling uncertainty of the stochastic process and climate model uncertainty the choice of prediction interval for the regression model uncertainty ensures that both the error in the position of the regression line and the variance of the observations around this are included climate model uncertainty is accounted for with the ensemble of cmip5 climate model outputs and subsequent use of weighted quantiles to estimate simulation bands a limitation of the regression framework for downscaling is the potential for regression uncertainties to grow rapidly within the range of extrapolation required for more severe carbon forcing scenarios such as rcp 8 5 in this study we find that the different regression fits for the two gauges investigated resulted in the generation of poorly identified parameter estimators under rcp 8 5 at bochum but well identified estimators at atherstone this raises the important issue of using regression models for extrapolation a pragmatic solution is to place a limit on extrapolation and refine the cmip5 climate model ensemble to include only those which fall below this limit however specifying a limit on extrapolation is difficult because the level of extrapolation required is a function of the target covariate range which itself is a function of the chosen k number of months and the chosen increment between target values any imposed limit is also subjective a limit on extrapolation of 30 was used at both gauge locations to mitigate the impact of parameter uncertainty on estimation and to ensure that some climate conditioned rainfall models requiring above average extrapolation were retained despite this extrapolation is only needed for between approximately 2 and 3 months in the year therefore while the framework presented inevitably demands some extrapolation we feel that the combination of inherent and imposed limitations mean that the required extrapolation is acceptable in the context of simulating rainfall in a changing climate the choice of temperature as the conditioning variable in this research was strongly influenced by the clausius clapeyron relationship which explains the increase in atmospheric water vapour in a warming climate further research is required to investigate the relationship between blrp parameter estimators and other covariates to provide extra validation of the approach potentially with other model variants extension to multiple covariates would also enable the investigation of teleconnections such as the north atlantic oscillation which are known to influence european rainfall declaration of competing interest the authors declare they have no conflicts of interest acknowledgements david cross is grateful for the award of an industrial case studentship from the engineering and physical sciences research council epsrc in the uk and edf energy david would like to acknowledge pietro bernardara who initiated his phd with christian onof but who has since taken up a new role at edf in france we are grateful to clare goodess of the university of east anglia edward pope of the uk meteorological office and nadarajah ramesh of the university of greenwich for their valuable insights in steering this research we also thank the two anonymous referees and the editor for their constructive reviews and comments which have greatly improved this paper the environment agency of england is gratefully acknowledged for providing the uk rainfall data and deutsche montan technologie and emschergenossenschaft lippeverband in germany are gratefully acknowledged for providing the bochum data we acknowledge the world climate research programme s working group on coupled modelling which is responsible for cmip and we thank the climate modelling groups listed in table a 2 in appendix of this paper for producing and making available their model outputs for cmip the u s department of energy s program for climate model diagnosis and intercomparison provided coordinating support and led development of software infrastructure in partnership with the global organization for earth system science portals the ncep reanalysis data were provided by the noaa oar esrl psd boulder colorado usa from their website at https www esrl noaa gov psd appendix supplementary material supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 103479 appendix d supplementary materials supplementary data s1 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s1 supplementary data s2 supplementary raw research data this is open data under the cc by license http creativecommons org licenses by 4 0 supplementary data s2 
534,ordinary least squares ols regression offers a decision oriented approach for modeling trends in annual peak flows we introduce a two stage ols approach for nonstationary flood frequency analysis that i models changes in their central tendency median in response to environmental perturbations with one regression and then ii examines changes in the coefficient of variation cv by running a second regression on anscombe transformed residuals from the first regression monte carlo simulations show that this approach yields 100 year flood estimates with mean squared errors comparable to estimates made with an advanced generalized linear model based method also this second stage regression often produces approximately normal residuals which permits statistical inferences on cv trends case studies illustrate the dramatic impact that decreasing and increasing cv trends can have on 100 year floods findings motivate the incorporation of trends in variability in infrastructure design along with further research examining asymmetric changes in urban flood variability graphical abstract image graphical abstract keywords flood frequency analysis heteroscedasticity nonstationarity regression urbanization variability 1 introduction worldwide floods cause an estimated 24 billion damage and the loss of thousands of lives annually kundzewicz et al 2014 observed and anticipated increases in flooding have spawned many new statistical methods for modeling changes in flood probability distributions over time e g strupczewski et al 2001 khaliq et al 2006 vogel et al 2011 salas et al 2018 and in response to specific environmental perturbations such as climate change e g jain and lall 2000 kwon et al 2008 condon et al 2015 urbanization e g beighley and moglen 2003 villarini et al 2009b gilroy and mccuen 2012 prosdocimi et al 2015 over et al 2016 oudin et al 2018 and reservoir storage e g lópez and francés 2013 over et al 2016 this includes numerous approaches that address the following question what is the magnitude of a flood expected once every 100 years or another recurrence interval of interest on average given current conditions in a basin while planners increasingly seek tools to characterize these changes many national level agencies are still working to establish design guidelines to reflect them for instance the united states bulletin 17c notes that practices for adjusting design floods for changes in basin conditions require more research before specific recommendations can be made england et al 2018 p 23 national institutions have recommended numerous safety factors statistical methods and mechanistic models for adjusting design flood estimates for ongoing anthropogenic perturbations kjeldsen et al 2008 madsen et al 2013 prosdocimi et al 2014 asfpm 2016 recently rescinded see fema and dhs 2018 ball et al 2016 but few best practices have been established moreover some fundamental questions regarding changes in flood regimes such as changes in urban flood variability require more research before standard methods are instituted many flood frequency analyses ffa that account for ongoing changes in basin conditions have been criticized for improperly distinguishing between deterministic and stochastic components of change e g montanari and koutsoyiannis 2014 serinaldi et al 2018 true changes in probability distributions of stochastic phenomena such as precipitation are more difficult to diagnose in small samples typical of hydroclimatic records since they may be conflated with long term persistence and other forms of natural variability e g cohn and lins 2005 luke et al 2016 however unequivocal deterministic changes in phenomena that are not subject to sampling uncertainty stemming from interannual fluctuations can also affect flood probability distributions in fact prior reviews have found that most annual maximum series ams of instantaneous peak flows which exhibit strong evidence of nonstationarity are associated with major deterministic basin changes such as urbanization e g villarini et al 2009a villarini and smith 2010 this makes methods for adjusting ffa in basins that have undergone such deterministic changes especially compelling e g serinaldi and kilsby 2015 changes in variability are a fundamental aspect of nonstationary ffa nsffa that is increasingly receiving attention e g strupczewski et al 2001 cunderlik and burn 2003 villarini et al 2009a delgado et al 2010 delgado et al 2014 o brien and burn 2014 condon et al 2015 ahn and palmer 2015 spence and brown 2016 yu and stedinger 2018 this is especially critical given the greater sensitivity of extreme design floods to trends in variability than to commensurate trends in central tendency katz and brown 1992 yet methods for adjusting probability distributions for deterministic changes in variability stemming from urbanization and other deterministic phenomena are still not as developed as those for trends in central tendency urbanization can decrease annual flood variability as increases in impervious cover have been associated with greater increases in smaller floods than larger ones e g hollis 1975 usace 1993 mccuen 2003 kjeldsen 2010 braud et al 2013 over et al 2016 in contrast cross sectional studies associating greater drainage densities with greater annual flood variability pallard et al 2009 suggest that urbanization induced increases in drainage density may also amplify flood variability though this has not been assessed in urban areas with confined subsurface drainage networks ogden et al 2011 changes in precipitation have also failed to explain other observed increases in urban flood variability villarini et al 2009 trudeau and richardson 2016 practical approaches for characterizing changes in both the central tendency and variability of floods must address decision makers and stakeholder concerns hecht 2017 and serago and vogel 2018 outline many benefits of regression for nsffa which taken together offer numerous features well suited for decision oriented analyses first its ease of use effective graphical communication and parsimonious estimation of conditional moments makes it an attractive alternative to many advanced methods see serago and vogel 2018 for more on the value of parsimonious models in ffa second regression offers decision relevant information including expressions of uncertainty confidence and prediction intervals and enables hypothesis tests regarding the influence of covariates on changing floods e g kwon et al 2008 prosdocimi et al 2014 such tests are valid when model residuals are serially uncorrelated normally distributed and homoscedastic constant variance or when standard error distortions from heteroscedastic non constant variance residuals can be sufficiently ameliorated using heteroscedasticity consistent standard errors e g long and ervin 2000 or through weighted or generalized least squares regression stedinger and tasker 1985 kjeldsen and jones 2009 regression also accommodates many smooth nonlinear functions through ladders of powers transformations mosteller and tukey 1977 as well as abrupt changes bates et al 2012 missing data slater and villarini 2016 and analytical corrections to the variance of regression coefficients inflated by short and long term persistence matalas and sankarasubramanian 2003 prior applications also demonstrate its suitability for nsffa while curtailing heteroscedasticity to minimize standard error distortions is critical for many statistical applications the non constant variance of residuals also provides valuable information about a phenomenon s variability especially when associated with physical covariates one can use regression to model trends in variability by first modeling changes in central tendency using one regression and then fitting a second stage regression on the transformed squared residuals from the first stage regression carroll and ruppert 1988 as we show later transforming squared first stage residuals can produce second stage residuals well approximated by a normal distribution which enables statistical inferences on variability trends this two stage regression approach known variously as heteroscedastic regression e g smyth et al 2001 zheng et al 2013 variance function regression e g davidian and carroll 1987 or parametric dual modeling robinson and birch 2000 has been used for over half a century park 1966 harvey 1976 as western and bloome 2009 note it has been employed in diverse disciplines including for pharmacological dose response curves e g davidian and haaland 1990 fish survival rates minto et al 2008 housing prices zhang et al 2015 climate change impacts to crop yields kelbore 2012 ex prisoner income western and bloome 2009 as well as many industrial applications smyth et al 2001 while other studies have modeled heteroscedasticity in continuous hydrologic time series e g wang 2005 sun et al 2017 fathian et al 2019 few have used it to assess flood regimes latraverse et al 2002 used a nonparametric form of local polynomial regression robust to heteroscedasticity for regional quantile estimates and lim 2016 examined it when characterizing the variable source areas of urban flooding yet the gap in practical methods for adjusting design events for changes in variability that mccuen 2003 identified has lingered despite the benefits of ols regression few studies have examined its ability to model changes in the coefficient of variation cv and incorporate them into design flood estimates recently yu and stedinger 2018 applied a two stage regression approach to estimate trends in ams variability but their work differs from ours in three respects first when estimating changes in variability they used nonlinear least absolute value regression instead of ols regression the latter with which stakeholders and decision makers are likely to be more familiar second we also compare our ols based method to a more advanced approach combining iteratively weighted least squares and generalized linear models glms aitkin 1987 third we evaluate the feasibility of this model for urbanizing basins and parameterize our monte carlo experiment based on observed changes in annual peak flows in urbanizing basins to demonstrate our approach we consider 100 year flood estimates made using the ams of instantaneous peak flows in urbanizing basins in the united states see stedinger et al 1993 section 18 6 1 and stedinger 2016 section 76 2 3 and associated references for further guidance on when an ams analysis is preferred over a pot one we use total impervious area tia as an example of a physical covariate which exhibits an unequivocal deterministic change over time however our approach can explore relationships between any covariate e g climate indices and annual floods in addition it can be extended to accommodate multiple covariates including different ones in the first and second stage models this paper is organized as follows section 2 introduces regression based methods for adjusting design flood estimates for trends in both the mean and variance of the natural logarithms of ams which are monotonically related to real space trends in the median and cv respectively section 3 presents a monte carlo simulation experiment that compares the performance of four design flood adjustment methods section 4 provides case study illustrations and examines single site trends in the median and cv of ams in urbanizing basins in the united states section 5 discusses limitations and possible extensions pertaining to both model development and planning applications while section 6 concludes 2 methods 2 1 model overview in ffa one typically uses maximum likelihood l moments or the method of moments to estimate the parameters of a probability distribution here we use the method of moments where the unconditional moments of the natural logarithms of the ams are replaced by their values conditioned upon physical covariates i e explanatory variables to reflect deterministic changes to a basin s flood response we introduce our approach using bivariate regression equations with the log transformed ams as the response variable and total impervious area tia as the lone covariate see section 3 however it can be extended to multivariate models while heteroscedastic regression can accommodate ams approximating many distribution types see serago and vogel 2018 we introduce our two stage regression approach for ams that arise from a two parameter lognormal ln2 distribution this distribution often approximates ams well under both stationary beard et al 1974 stedinger 1980 vogel and wilson 1996 and nonstationary conditions e g strupczewski et al 2001 vogel et al 2011 delgado et al 2014 prosdocimi et al 2014 under stationary conditions this parsimonious distribution can produce lower mean squared errors mse associated with quantile estimates than numerous three parameter distributions often used in ffa kuczera 1982 this distribution is also especially attractive for nsffa because it requires just four estimated parameters when the mean and variance of log transformed values change linearly with one covariate aissaoui fqayeh et al 2009 vogel et al 2011 also demonstrate the promise of ols regression for modeling time conditional ln2 distributions as this model yields residuals which are well approximated by a normal distribution for over 75 of 19 430 ams at unregulated and regulated gauging records of at least ten years in the united states however we emphasize that this approach could be applied to other two and three parameter probability distribution functions pdfs often used in ffa including the generalized extreme value gev and log pearson type iii lp3 pdfs provided the resulting regressions exhibit normally distributed and uncorrelated residuals see serago and vogel 2018 consider a logarithmic transformation of the ams discharges q 1 y ln q if q follows a ln2 pdf then y is normally distributed here we present a two stage regression modeling procedure which yields estimates of the mean and variance of y conditioned on the value of a physical covariate exhibiting a deterministic change we define the nonstationary flood quantile function of discharge q with covariate conditional moments as follows 2 q p ω exp μ y ω z p σ y ω where q p ω is the annual flood with non exceedance probability p for a covariate value ω μ y ω and σ y ω are the conditional mean and standard deviation respectively of the logs of the ams and zp is a standard normal variate for an ln2 distribution a trend in the log space mean corresponds to a trend in the real space median because the mean of logs is the 50th percentile median of the ln2 distribution a trend in the log space mean μ y ω also implies a commensurate trend in the real space variance which in turn enables the real space cv to remain unchanged also when there is no trend in the log space variance there is no trend in the real space cv either since c v q exp σ y 2 1 thus we henceforth refer to trends in the mean and variance of log transformed ams as trends in the median and cv respectively next we derive conditional moments of y corresponding to ams arising from an ln2 distribution as an example first we derive a nonstationary ln2 quantile function using a homoscedastic regression model with trends in the mean of the logs but no error variance trend i e the real space cv is constant then we turn to a heteroscedastic model that considers temporal changes in the log space error variance real space cv as well as the mean of the logs real space median finally we compare quantile estimates from these derived models to ones from an iteratively weighted least square generalized linear model aitkin 1987 table 1 summarizes these four models 2 2 homoscedastic nonstationary regression model hom ns ln2 numerous nsffa studies including vogel et al 2011 prosdocimi et al 2014 read and vogel 2015 over et al 2016 yu and stedinger 2018 and serago and vogel 2018 have employed a linear regression of the natural logarithms of the ams y ln q on an explanatory variable ω 3 y β 0 β 1 ω ε where β0 and β1 are regression coefficients and ε is a normally distributed zero mean error term with a constant variance over et al 2016 show that models with this functional form nicely characterize the response of ams to linear increases in tia in the chicago metropolitan area when the residuals ε in 3 are homoscedastic and normally distributed further statistical inferences can be made on β0 and β1 along with the overall model since the expected value of the residuals is zero the conditional expectation of y is 4 e y ω μ y ω 0 β 0 β 1 ω 0 where μ y ω 0 denotes that the mean of y is conditioned upon a covariate of a given value ω0 in a regression model trends in the conditional mean directly impact the error variance estimated with residuals and consequently they affect the conditional variance one can partition the variance of a dependent variable y into two components one explained by a covariate trend and another one comprised of the unexplained variance 5 σ y 2 β 2 σ ω 2 σ ε 2 this equation is best understood by considering two extreme cases i when a trend is perfectly explained by the regression i e σ y 2 β 2 σ ω 2 all data fall exactly on the trend line and the residual variance disappears entirely and ii when there is no trend the variance in the dependent variable arises strictly from the error variance alone i e σ y 2 σ ε 2 meanwhile the variance of y conditional upon a given covariate ω0 is equal to the model error variance σ ε 2 since β0 β1 and ω0 are all constant 6 v a r y ω 0 σ y ω 0 2 σ ε 2 next when errors are homoscedastic constant σ ε 2 is 7 σ ε 2 σ y 2 β 1 2 σ ω 2 importantly when estimating σ ε 2 from residuals of a bivariate regression a degrees of freedom correction factor n 2 produces an unbiased estimate of the error variance 8 σ ε 2 1 n 2 i 1 n y i y i 2 where n is the ams length while yi and y i are the observed and modeled annual peak flows respectively thus hom ns ln2 quantiles conditioned on a single covariate ω0 are estimated as follows 9 q p ω 0 exp μ y ω 0 z p σ y ω 0 exp β 0 β 1 ω 0 z p σ ε note that 7 can also be expressed in terms of the pearson correlation coefficient ρω y 10 σ y ω 2 σ ε 2 1 ρ ω y 2 σ y 2 where ρ ω y β 1 σ ω σ y many prominent ffa studies e g stedinger and griffis 2011 vogel et al 2011 prosdocimi et al 2014 luke et al 2016 have not considered reductions in the conditional variance of y proportional to ρ ω y 2 given in 10 read and vogel 2015 also show that reductions in σ y ω 2 also lower the conditional real space cv cv q ω as follows 11 c v q ω c v q 1 1 ρ ω y 2 1 2 3 heteroscedastic nonstationary regression model het ns ln2 next heteroscedastic regression model residuals imply that the cv of an ams changes with physical covariate values we derive a coupled two stage regression model for modeling heteroscedastic residuals het ns ln2 that ensures zero mean residuals preserves the sign of the residuals and allows the cv to vary with a covariate see appendix a the parameters of this variance model can be estimated with a second ols regression which in turn yields residuals likely to follow an approximately normal distribution we replace the covariate independent error terms from 3 with covariate dependent ones so that 12 y ω β 0 β 1 ω ε ω β 0 β 1 ω sign ε ω γ 0 γ 1 ω φ ω 3 2 where γ0 and γ1 are regression coefficients and φω is a normally distributed zero mean error term with a constant variance this expression keeps the expected value of estimated residuals ε ω at zero and preserves their sign as explained below 12 also enables us to estimate the covariate dependent error variance with a second stage regression model that relates a covariate to the first stage model errors in 3 raised to the two thirds power i e ε ω 2 3 while we illustrate this second stage regression using the covariate ω from the first stage model different covariates may be used for each stage e g an indicator of urbanization for changes in the median and a climate index such as the north atlantic oscillation for changes in the cv since 12 preserves the zero mean property of εω the conditional mean e y ω0 is identical to the conditional mean of hom ns ln2 i e e y ω 0 μ y ω 0 β 0 β 1 ω 0 thus var β0 β1ω0 0 v a r y ω 0 σ y ω 0 2 σ ε ω 0 2 next since e ε ω 2 σ ε ω 2 fitting an ols regression with εω 2 as the response variable and ω as the explanatory variable may appear useful for estimating σ y ω 0 2 and making inferences regarding the associated trend however if residuals from the first stage regression are normally distributed its squared residuals will follow a highly skewed χ2 distribution if these squared residuals are then used as the response variable in a second stage regression the second stage residuals are unlikely to be normally distributed in contrast if we raise the first stage residuals in 12 to the two thirds power we obtain a response variable that follows an approximately normal distribution except at its lower tail see appendix b 13 ε ω 2 3 γ 0 γ 1 ω φ ω if this second stage regression also yields normally distributed residuals we can make statistical inferences on the second stage model along with its coefficients γ0 and γ1 the transformation to the two thirds power represents a simplified version of the anscombe 1953 transformation which converts variables arising from gamma distributions of which the χ2 distribution is a special case to approximately normal ones carroll and ruppert 1988 while this approach has been applied to transform gamma distributed wet day precipitation data e g chandler and wheater 2002 kigobe et al 2011 we could not find any flood applications ultimately we want to use γ0 and γ1 to estimate σ y ω 0 2 since σ y ω 0 2 σ ε ω 0 2 e ε ω 2 it follows that 14 σ y ω 0 2 e ε ω 2 3 3 e γ 0 γ 1 ω φ ω 3 importantly one must include the error term ϕω in 14 or else the estimate of σ y ω 0 2 may be severely downward biased this leads to the following expression for the conditional variance 15 σ y ω 0 2 γ 0 γ 1 ω 0 3 3 σ φ 2 γ 0 γ 1 ω 0 the second term on the right hand side of 15 is proportional to the error variance of the second stage model σ φ 2 which tends to comprise a large portion of the total variance of ε2 3 in fact r2 values of the conditional cv model range from just 0 06 to 0 24 at sites with cv trends significant at the 95 level see section 3 in other words σ φ 2 explains 76 to 94 of the variation in ε2 3 yet even though cv trends explain a small fraction of the overall interannual variability neglecting this second term can cause estimates of extreme floods at sites with increasing median and cv trends to be lower than estimates only considering median trends when estimating σ φ 2 the degrees of freedom correction factor in 8 should also be used next by substituting 15 into 9 we obtain the het ns ln2 quantile function for adjusting design floods with a given annual non exceedance probability p conditioned on a given covariate value ω0 see appendix a for a full derivation 16 q p ω exp μ y ω 0 z p σ y ω 0 exp β 0 β 1 ω 0 z p γ 0 γ 1 ω 0 3 3 σ φ 2 γ 0 γ 1 ω 0 2 4 an advanced two stage approach with generalized linear models iwls glm we compared the het ns ln2 model to a more sophisticated two stage modeling approach using generalized linear models glms glms accommodate dependent variables with non normal distributions belonging to the exponential family of distributions aitkin 1987 showed that using iteratively re weighted least squares iwls estimation with variance estimates from gamma glms with log link functions yields asymptotic maximum likelihood estimates this prevents heteroscedasticity induced errors in residual estimates from propagating to the cv trend model although we assume a different variance change trajectory than aitkin 1987 these benefits of iwls glm also make it an attractive procedure for our approach despite these benefits nsffa studies have not used iwls glm as other prior studies using glms have either assessed changes in location parameters clarke 2001 clarke 2002 aissaoui fqayeh et al 2006 najibi and devineni 2018 or variability in the number of coastal nuisance floods vandenberg rodes et al 2016 the iterative iwls glm approach is implemented as follows 1 fit the hom ns ln2 model assuming equal weights for just the first iteration 2 square residuals of the hom ns ln2 model 3 fit a second stage gamma glm with a log link function using the glm2 package in r statistical software r core team 2019 4 use reciprocals of fitted values of 3 as weights when fitting hom ns ln2 in the next iteration repeat until convergence we did this ten times to ensure convergence to four decimal places this method produces estimates of the conditional mean and variance that can subsequently be inserted into the conditional quantile function in 2 to compute the 100 year flood 3 monte carlo simulation experiment we conducted a monte carlo experiment to assess the accuracy of our design flood estimates for the last year of simulated 50 year records when using the four estimation methods listed in table 1 we generated random samples of length 50 from a uniform distribution ranging from zero to one then we treated these values as non exceedance probabilities and computed annual flood values using covariate conditional ln2 distributions with known initial parameters and change trajectories we applied a three factor experimental design in which we simulated 50 year ams with different initial real space cv values and different correlations driving their median and cv trends ρω y ρ ω ε 2 3 based on our 202 station sample of urbanizing basins see section 4 3 we focused on cases with increases in the median and either increases or decreases in the cv given planner interests in minimizing bias uncertainty and worst case outcomes we computed the percent error distributions percent bias pbias fractional root mean squared errors frmse and maximum possible over and under design errors of conditional quantile estimates for the last year of record from a set of 10 000 simulations this experiment does not address general sampling issues that can cause downward biased estimates of the true 100 year flood value of a population stedinger 1983 see appendix d for more details on the experimental design and results the simulation experiments demonstrate that het ns ln2 underpredicts the 100 year flood when there is an increasing cv trend and overpredicts it when there is a decreasing one fig 1 this transformation over predicts lower tail values and slightly underpredicts upper tail ones see appendix b as expected design flood estimates considering modeling cv trends when present are more accurate than ones that only consider trends in the median the simulations of decreasing cv trends also suggest that the s ln2 quantile estimate may be better than hom ns ln2 for modeling changes in extreme floods in urbanizing basins one pervasive question in nonstationary hydrology is whether the reduction in bias from adding model parameters is worthwhile given the increase in uncertainty that it can induce for instance yu and stedinger 2018 found that modeling changes in the cv log space variance in their paper only improves the rmse under extreme changes yet in this study het ns ln2 consistently registers a lower frmse than hom ns ln2 which suggests that the bias reduction benefits of modeling cv trends outweigh the increase in variance stemming from the addition of another parameter fig 2 see figures for other cv values in appendix d we also compared het ns ln2 and iwls glm as expected iwls glm estimates were considerably less biased however het ns ln2 registers a lower frmse than iwls glm especially when the cv is increasing also maximum overdesign errors are larger under iwls glm than het ns ln2 when the cv is increasing but slightly smaller when it is decreasing in contrast maximum under design errors are slightly greater under iwls glm regardless of the cv trend direction thus choices between these two methods depend on a decision maker s preference for unbiasedness versus minimizing uncertainty and worst case outcomes in addition one could bias correct het ns ln2 results as its bias is markedly correlated with the cv see appendix d however we believe that other procedures for modeling cv changes should be evaluated before attempting bias correction see section 5 finally one must remember that this experiment only evaluates het ns ln2 when simulated ams originate from an ln2 distribution and the changes in moments follow the same trajectories that models assume when the true cv trend model followed the exponential change trajectory that the gamma glm model assumes het ns ln2 still yielded lower frmse values than iwls glm did for many trend combinations see appendix d in practice distribution types and change trajectories are unknown and prone to misspecification which makes a more detailed robustness analyses examining the performance of our models when these assumptions are incorrect a natural sequitur to this study 4 applications 4 1 urbanization effects on floods many aspects of urbanization alter flood regimes leading to a wide range of responses e g smith et al 2013 salvadore et al 2015 zhou et al 2017 diem et al 2018 kokkonen et al 2018 these urban drivers include increased impervious cover soil compaction stormwater routing and detention e g ogden et al 2011 miller et al 2014 increased channel width and reduced overbank storage hirsch 1977 return flows allaire et al 2015 oudin et al 2018 groundwater pumping hopkins et al 2015 induced recharge locatelli et al 2017 and changes in precipitation due to urban heat islands yang et al 2014 underlying drivers of change such as population growth that lead to these hydrological impacts have also been identified usace 1993 villarini et al 2009 given these myriad flood altering mechanisms one major challenge with characterizing changes in urban flooding is the selection of indicators to represent urbanization standard approaches based on the percentage of total impervious area tia have been criticized especially in cross sectional studies because the relationship between tia and flooding varies due to underlying soil types gregory et al 2006 stormwater and drainage infrastructure e g leopold et al 1968 ogden et al 2011 miller et al 2014 zhou et al 2017 the connectivity and spatial location of impervious cover e g mejia and moglen 2010 ferreira et al 2015 debbage and sheppard 2018 as well as its diverse and time variable hydrologic properties redfern et al 2016 and mapped spatial resolution lee and heaney 2003 weng 2012 however we argue that tia is a suitable metric given our goal of demonstrating a method for adjusting design floods at individual sites where deterministic changes in the central tendency and variability of flooding are evident 4 2 illustrative case studies we illustrate our nsffa method with two case studies in small urbanizing basins where prior studies have also documented urbanization induced increases in flooding 4 2 1 increase in median increase in cv the aberjona basin in the boston metropolitan area offers a natural laboratory for examining urbanization induced changes in flooding allaire et al 2015 serago and vogel 2018 villarini et al 2018 the aberjona river at winchester station gage id 01102500 drainage area 61 9 km2 has recorded instantaneous peak flows since 1940 here tia increases from 24 7 in 1940 to 37 7 in 2010 fig 3 demonstrates that trends in the median ρω y 0 46 p 0 01 and cv ρ ω ε 2 3 0 29 p 0 01 exhibit highly significant positive correlations with tia it also shows that the stationary 100 year flood 44 9 m3 s roughly equals the flood of record 45 0 m3 s under hom ns ln2 the current 100 year flood rises to 53 0 m3 s an increase of 18 incorporating the cv trend raises the 100 year flood to 64 9 m3 s which is 45 greater than the s ln2 value a probability plot correlation coefficient ppcc normality test fails to reject the null hypothesis that the residuals of the second stage regression are normally distributed p 0 39 a modified breusch pagan test breusch and pagan 1979 in which we regressed ε2 3 on ω also confirmed their homoscedasticity p 0 86 together these tests validate statistical inferences from the second stage model recent increases in total and extreme precipitation ahn and palmer 2015 huang et al 2017 have not translated to increases in the cv of annual floods in many less urbanized basins in eastern new england moreover hodgkins et al 2017 find a lack of a contemporaneous increase in floods with recurrence intervals of 25 100 years in the northeastern united states while further research is needed to untangle the effects of recent climatic fluctuations including inter decadal oscillations see armstrong et al 2014 berton et al 2017 and an abrupt increase in extreme precipitation huang et al 2017 from urban induced changes in the aberjona basin the evidence presented strongly suggests that urbanization contributes to the increasing cv here 4 2 2 increase in median decrease in cv fig 4 shows that the ams at the river rouge at birmingham michigan station usgs id 04166000 drainage area 86 2 km2 which has been operating since 1951 exhibits an increasing median and a decreasing cv the tia of this basin located in the western suburbs of the detroit metropolitan area steadily increased from 8 2 in 1950 to 24 7 in 2010 fig 4 exhibits a clear increase in the lower bound of the ams while the upper bound has remained relatively constant the lower bound continues to increase after 1970 a year when annual precipitation increased abruptly in the eastern u s e g mccabe and wolock 2002 this provides another line of evidence that smaller annual floods have increased more than large ones aichele 2005 also detected an increase in maximum daily flows with a 1 exceedance probability between 1970 and 2003 at a 90 significance level and associated it with a 20 increase in residential area between 1980 and 2000 while beam and braunscheidel 1998 note that combined sewer overflows also contribute to peak runoff there are no large flood control reservoirs in the basin a major drought from 1960 1967 paulson et al 1991 may also enhance the median flood trend overall the increase in tia exhibits a highly significant positive correlation ρω y 0 47 p 0 01 with the log transformed annual floods while tia has a highly significant negative correlation ρ ω ε 2 3 0 37 p 0 01 with the anscombe transformed residuals we could not reject the null hypotheses of normality and homoscedasticity of the second stage residuals using a ppcc normality test p 0 39 and our modified breusch pagan test for residual heteroscedasticity p 0 21 respectively importantly accounting for the decreasing cv trend in addition to the increasing median trend lowers the 100 year flood estimate by 28 8 from 50 1 m3 s with hom ns ln2 to 35 7 m3 s under het ns ln2 which is even 13 lower than the s ln2 estimate of 41 1 m3 s however fig 4 does not display strong visual evidence of a decrease in low frequency events a major reason for which the het ns ln2 100 year flood is lower than its s ln2 counterpart is that het ns ln2 assumes a symmetric decrease in the variance of log transformed annual peak flows whereas numerous prior studies demonstrate a greater tendency for smaller floods to increase than for larger floods to decrease a second reason is that the s ln2 estimate may be upward biased because when assuming stationarity the site has a negatively skewed distribution 0 59 while a nonstationary distribution fits quite well p 0 59 we must reject the stationary ln2 distribution of the ams at this site using a ppcc normality test p 0 01 see appendix c for a more detailed explanation this discrepancy highlights problems with direct comparisons of stationary and non stationary distributions a challenge that motivated serago and vogel 2018 to create nonstationary probability plots to evaluate their goodness of fit 4 3 flood trends in urbanizing basins of the united states a preliminary analysis 4 3 1 identifying urbanizing basins with changing ams numerous recent studies have analyzed floods in urbanizing basins throughout the united states e g salavati et al 2016 chao lim 2016 luke et al 2016 oudin et al 2018 we examined trends in the log space mean real space median and log space variance real space cv in 202 urbanizing basins with at least 30 years of instantaneous peak flow observations through the 2016 water year 1 oct 30 sep and at least 10 impervious cover during one year in their station record please note that some stations with trends may have substantially different 100 year estimates using data beyond 2016 most notably stations in houston due to major tropical cyclones in 2017 see zhang et al 2018 our study does not aim to identify an exhaustive subset of conterminous us basins with urbanization altered floods or choose the best indicator of urbanization for attributing changes in design floods e g jato espino et al 2018 debbage and shephard 2018 rather we strive to examine the general prevalence of single site cv trends in urbanizing basins also we are not aiming to draw any statistical conclusions about the prevalence of regional trends which would require a spatial correlation analysis douglas et al 2000 recent studies have used i basin attributes from the gages ii database falcone 2011 including impervious cover thresholds e g oudin et al 2018 and ii usgs peak flow qualification codes http pubs usgs gov wdr 2005 wdr il 05 misc peakcods htm luke et al 2017 to identify urbanizing basins we first removed individual annual instantaneous peak flow observations using qualification codes before removing entire station records based on their record lengths and other criteria we eliminated peak flows with uncertain dates maximum daily flows dam failures and estimated historical floods while historical floods during pre instrumentation periods often provide valuable information such records are more likely to exist for larger events than smaller ones this could cause downward biases in estimates of trends in central tendency using ols regression future research should evaluate the feasibility of integrating this method into ffa methods that accommodate historical records and other censored data such as the expected moments algorithm cohn et al 1997 next we used numerous criteria to select stations suitable for analysis we removed all stations with ams shorter than 30 years as cunderlik and burn 2003 recommended that ams for testing trends in variability span at least 30 50 years to reduce the risk of confounding trends with inter decadal variability note that regression based methods offer easily computable standard errors that allow for estimates of uncertainty that consider record lengths to avoid misleading trend diagnoses when abnormally small or large annual floods lie at the beginning or end of an ams see slater and villarini 2016 we also removed stations that had at least one 30 year period with fewer than ten annual peak flows stations with peak flows below 1 m3 s were also omitted due to the challenges of gauging such low discharges combined with the pronounced effect that these low estimates could have on conditional median regression models fit in log space changepoint tests in the mean and variance of the log transformed ams using the at most one change amoc method in the changepoints package for r statistical software r core team 2019 did not reveal any ams with changepoint confidence levels exceeding 95 when using minimum segment lengths of 10 years after performing this initial screening we identified a subset of urbanizing basins following oudin et al 2018 we first applied the impervious cover fraction obtained from the widely used nlcd dataset to identify basins whose tia exceeded 10 in 2006 while flood magnification has been detected in basins with less than 5 impervious cover yang et al 2010 and depends upon underlying soils e g gregory 2006 this standard threshold of 10 schueler 2009 enabled us to identify a set of urbanizing basins suitable for demonstrating our modeling approach we also only evaluated stations whose reservoir storage capacity was less than 10 of their mean annual flow according to gages ii we then performed a more detailed analysis of changes in tia from 1940 2016 using decadal housing density data 1940 2010 with a 1 km spatial resolution from theobald 2005 we eliminated stations with drainage areas smaller than 5 km2 due to the tia data s coarse resolution each basin s tia was computed using the reclassify and raster clipping functions in the spatial analyst extension of arcgis 10 4 the same procedure can be implemented using freely available qgis and grass gis software we then applied previously validated relationships from oudin et al 2018 to estimate the tia in each 1 km2 cell based on housing density data and tia on land used for commerce industry and transportation decadal values were linearly interpolated to create annual time series for 202 urbanizing basins tia from 2010 2016 was assumed to increase at the same rate as during 2000 2010 annual flood peaks prior to the beginning of tia time series in 1940 were omitted from the analysis we then identified stations in this subset of 202 urbanizing basins that had ams with first stage conditional median residuals for which null hypotheses of normality and no lag one correlation could not be rejected at a 95 significance level p 0 05 using version 2 5 of the sandwich package in r we computed all trend p values using robust heteroscedasticity consistent standard errors with the hc3 estimator which adjusts squared residual values to correct for the effects of overly influential observations long and ervin 2000 we deemed all trends with p 0 05 as approximately statistically significant at the 135 stations whose residuals were normally distributed and serially independent 67 of the 202 urban stations we tested for trends in the anscombe transformed residuals using our modified breusch pagan test 4 3 2 results table 2 shows the number of urbanizing basins from the 135 station sample with each of the nine possible combinations of trends in the conditional median negative insignificant positive and cv negative insignificant positive using het ns ln2 fourteen of the 67 21 urbanizing basins with significant increases in their median annual flood also demonstrated significant concurrent changes in their cv with six sites exhibiting increasing cv trends and eight sites including one pair of nested sites in houston texas having decreasing trends this suggests that hom ns ln2 is valuable for modeling trends in many urbanizing basins with changing flood hazards but it may neglect important changes to the relative variability of ams in others differences between het ns ln2 and iwls glm estimates reflected ones obtained in the monte carlo experiments as het ns ln2 exhibited a greater upward downward bias than iwls glm for decreasing increasing cv trends appendix f fig 5 maps stations with these different combinations of trends observed cv trends at stations with increasing median annual floods motivated us to apply het ns ln2 in basins featuring both trends while correlations between tia and the anscombe transformed residuals of models with significant cv trends are relatively weak with ranges of 0 26 0 49 for increasing trends and 0 26 0 48 for decreasing trends their pronounced effects on design floods make testing for cv trends imperative to further compare the models in table 1 we calculated percent changes in nonstationary flood quantiles attributable to median and cv trends first we computed the percent increase in the 100 year flood attributable only to increased medians q 1ooyr μ y ω 17 q 1 o o y r μ y ω q 1 o o y r q 1 o o y r where q 1ooyr is the stationary s ln2 100 year flood estimate next we computed the difference between the estimate considering both median and cv trends q 1ooyr μ y ω σ y w and the one only considering increases in the median q 1ooyr μ y ω 18 q 1 o o y r μ y ω σ y w q 1 o o y r μ y ω q 1 o o y r we normalized the differences in q 1ooyr μ y ω σ y w and q 1ooyr μ y ω by q 1ooyr to compare changes in 100 year flood estimates due to trends in the median and cv respectively for example an increasing median may lead to a 100 year flood estimate 50 greater than its stationary counterpart however the 100 year flood may not change at all if a decreasing cv trend fully offsets the effects of an increasing median trend on the 100 year flood in this case 18 yields a value of 50 fig 6 plots values of 17 and 18 for each of the 14 stations exhibiting significant increasing or decreasing cv trends see table 2 using estimates for the most current year on record fig 6 illustrates the importance of accounting for both trends in the median and cv when adjusting 100 year floods to reflect current basin conditions the contours indicate the overall percent change in the 100 year flood relative to a s ln2 stationary 100 year flood estimate stations with significant decreasing cv trends register much lower 100 year flood estimates whereas stations with significant increasing cv trends register much higher estimates overall 100 year flood estimates considering both increasing trends in the median and cv can exceed estimates assuming stationarity by more than 80 such changes are substantially larger than ones resulting from changes in the median alone decreasing cv trends can dramatically reduce design flood estimates adjusted for changes in central tendency even though our monte carlo simulations show that het ns ln2 overestimates the 100 year flood many stations in fig 6 lie below the 0 contour indicating no overall change in the 100 year flood when het ns ln2 is used in lieu of s ln2 in other words at these stations decreasing cv trends lower 100 year flood estimates more than increasing median trends raise them again there are two major statistical reasons for this tendency first the ln2 distribution is unable to consider asymmetrical changes to the variance of log transformed flows second when the conditional ln2 distribution is suitable for sites with increasing trends in the median and decreasing trends in the cv the stationary ln2 distribution is negatively skewed see appendix c this negative skewness causes the 100 year flood to be overestimated in fact all eight stations with significant increasing median and decreasing cv trends had negatively skewed samples 0 32 to 0 94 when stationarity was assumed while the null hypothesis of a stationary ln2 distribution could only be rejected at less than 13 of all 135 stations with well behaved residuals it was rejected at four out of eight stations with significant increasing median and decreasing cv trends finally while we did not test the regional significance of trends we examined spatial patterns qualitatively see fig 5 some previously reported regional change patterns are evident but overall they are not overwhelmingly strong reflecting the relatively fragmented nature of flood nonstationarity in the conterminous us archfield et al 2016 in some cases strong urbanization impacts counter observed regional trends for instance at glen cove creek at glen cove new york gage id 01302500 the cv decreases despite the extreme precipitation increase in the northeastern us huang et al 2017 yet a few known regional scale climate driven trends do appear in fig 1 three decreasing cv trends at north central stations align with observed increases in the frequency but not the magnitude of floods there hirsch and archfield 2015 mallakpour and villarini 2015 although this decrease is not observed at any of the nearby chicago metropolitan area sites four of the six stations with increasing trends in the cv are scattered throughout the northeastern united states however while this region underwent an increase in the 99th percentile daily precipitation in 1996 huang et al 2017 the main increase in annual floods stems from an north atlantic oscillation induced step change in 1970 armstrong et al 2014 for instance the boston metropolitan area only has one station with an increasing cv trend the geographic dispersion of the four sites with increasing cv trends in the eastern united states suggests that local hydrologic responses to urban development may contribute to these trends especially since some are near stations with insignificant cv decreases on the other hand seattle has several urbanizing sites with near significant cv increases near one site with a significant cv increase 5 discussion this discussion begins by critically evaluating the models examined in this study before contemplating the incorporation of our method in flood management decision making 5 1 model performance and recommendations our modeling framework enables evaluations of assumed trajectories of gradual change in flooding in response to increases in tia here our first stage regression model assumes an exponential relationship between tia and the ams which is mathematically equivalent to a linear relation between tia and the logarithms of ams this functional form has nicely modeled changes in the median for ams in numerous urbanizing environments vogel et al 2011 over et al 2016 and suggests that the magnification of flooding accelerates as basins become more urbanized however changes in the logs of ams may not be linearly related to changes in different indicators of urbanization such as impervious cover moreover recent stormwater regulations may temper urbanization induced increases ntelekos 2010 zhou et al 2017 this suggests that a logarithmic change trajectory may be more appropriate for basins with linear increases in impervious cover given that ladders of powers transformations enable many smooth monotonic trajectories of change to be modeled with regression hypothesized functional forms reflecting varying trajectories of tia and other indicators of hydrologic impacts of urbanization can be evaluated with our modeling framework since improperly specifying the functional form of the first stage model can bias residual estimates and in turn affect second stage models showing variability trends robinson and birch 2000 we examined alternative functional forms however we obtained a similar set of basins with significant cv trends with these alternative models for basins whose medians increased significantly see appendix f other methods for modeling variability change trajectories also merit future attention notably yu and stedinger 2018 examine the following model that assumes the log space error variance changes exponentially in response to a linear change in a covariate ω 19 σ ε ω 2 σ ε 0 2 exp β 2 ω after log transforming this model they use nonlinear least absolute value regression to estimate β2 while not an ols procedure this relatively parsimonious method has numerous advantages including that it prevents the variance from being negative though ours did not yield negative values moreover aitkin 1987 demonstrated that iwls glm can yield maximum likelihood estimates of this model s standard errors yet while attractive for estimating change trajectories log transforming the squared residuals from the first stage model is less likely to produce normally distributed residuals than het ns ln2 see appendix b this requires more complicated procedures for making inferences about cv trends further it remains unknown whether this change trajectory models observed changes in the cv with more fidelity than het ns ln2 it would be preferable to compare these two methods in a more expansive robustness study that also examines implications of mis specifying variance change trajectories in addition regression models with binary indicator variables would enable the incorporation of abrupt changes in urban basins such as flood control reservoir construction over et al 2016 or build out maximum permitted growth dates numerous other aspects of residual modeling also merit further investigation first comparing estimates from het ns ln2 to ones that model trends in the first two conditional moments separately would illuminate the value of coupled models of conditional moments additional comparisons with other residual transformations e g efron 1982 are also warranted last we should examine the benefits of using studentized residuals to alleviate the effects of leverage from extreme observations near the ends of ams on residuals robinson and birch 2000 manning and muhally 2001 5 2 toward improved urban flood management we introduce a nsffa approach for estimating design floods that reflect current conditions in urbanizing basins including changes in both the central tendency and the relative variability of ams we have only reported trends significant using a 5 significance level this standard approach may neglect less pronounced trends that could modify the 100 year flood estimates substantially especially when those trends are in the cv more research is needed to develop risk based planning tools that consider both over and under design probabilities associated with type i and ii errors from both median and cv trends see rosner et al 2014 and prosdocimi et al 2014 for some initial efforts with trends in central tendency in practice applications of our two stage approach should examine the sensitivity of trend assessments to the start and end dates of ams burn and whitfield 2018 and large historical floods outside of gauging periods e g cohn et al 1997 finally if future design flood quantiles of interest are sought we strongly recommend reporting prediction intervals for any extrapolated estimates which tend to widen quickly with time and document our in ability to predict the future the extent to which cv trends can modify 100 year flood estimates motivates future studies that attribute such trends to physical phenomena associated with urbanization and disentangle them from concurrent climatic fluctuations oudin et al 2018 employed a model residual approach that assessed urbanization impacts to high flows q95 by calibrating a rainfall runoff model during a pre urbanization period and then running it during a post urbanization period differences between observed and simulated flows during the post urbanization period were attributed to urbanization although the possible systematic underestimation of high flows from their continuous simulation model see farmer and vogel 2016 may confound their conclusions yet if continuous simulation models can be improved or bias corrected statistical analyses of the results of factorial design experiments using mechanistic models e g jato espino et al 2017 could also elucidate specific drivers of urban flood variability such as effective impervious area ebrahimian et al 2016 stormwater detention and road construction 6 conclusions when responding to increasing trends in ams decision makers need nonstationary flood frequency analysis nsffa methods that are easy to implement and address stakeholder needs previous studies e g vogel et al 2011 prosdocimi et al 2014 have demonstrated the numerous benefits of ordinary least squares ols regression for adjusting design floods by solely accounting for changes in the mean of ams natural logarithms to extend these efforts we introduced a two stage ols regression model that accounts for changes in the cv of ams we applied this approach to 135 stations in urbanizing basins with i drainage areas of at least 5 km2 ii tia exceeding 10 at one point in their record iii record lengths of 30 years or longer without any extensive intermittency in gauging and iv conditional median models with normally distributed and temporally uncorrelated residuals of the 67 stations exhibiting significant 95 level increases in their medians 21 14 135 had concurrent cv trends see table 2 this suggests that hom ns ln2 which assesses trends strictly in medians often ends up being a viable first order approach for characterizing observed changes in design flood quantiles in urbanizing basins whose ams exhibit nonstationary behavior and normal residuals vogel et al 2011 however we document the importance of incorporating cv trends into 100 year flood estimates which have societal ramifications ranging from bridge design to property insurance finally challenges modeling the asymmetrical changes in variability in urbanizing basins and comparing stationary and nonstationary versions of theoretical distributions motivate the development of other models for basins with decreasing cv trends this motivates the use of three parameter probability distributions e g lp3 where changes in skewness can partially compensate for changes in variability indeed this work combined with serago and vogel 2018 provides an ols regression based conditional moments framework for determining the conditional skewness of ams in response to changes in the mean and variance of log transformed flows to reflect the tendency for urbanization to magnify larger annual floods much less than smaller ones statistical approaches that enable a fixed upper bound such as the gev type iii distribution and quantile regression should also be investigated future work should also evaluate our two stage approach for partial duration series peaks over threshold which has recently demonstrated a stronger association with urbanization than ams prosdocimi et al 2015 or even complete daily flow time series serinaldi et al 2018 finally we encourage applications of this modeling framework that assess other hydroclimatic extremes declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements and data the authors are indebted to the u s army corps of engineers usace institute for water resources iwr for their encouragement and support of this research the second author was supported by an appointment to the u s army corps of engineers usace research participation program administered by the oak ridge institute for science and education orise through an interagency agreement between the u s department of energy doe and the u s army corps of engineers usace orise funding was managed by orau de ac05 06or23100 all opinions expressed in this paper are the authors and do not necessarily reflect the policies and views of usace iwr doe or orau orise two national science foundation grants enabled tufts university water diplomacy nsf oia 0966093 and the university of vermont bree nsf oia 1556770 to provide in kind support the authors are also grateful for earlier reviews and discussions with annalise blum jake serago nancy barth and karen ryberg all data used are publicly available the observed annual maximum series instantaneous peak flow data are available from the national water information system at http waterdata usgs gov nwis total impervious area tia data were derived from historical housing density data used in the us environmental protection agency s integrated climate and land use scenarios iclus project https www epa gov iclus other watershed characteristics were obtained from the gages ii dataset falcone 2011 models used in this study are available from the authors upon request supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 103484 appendix supplementary materials image application 1 image application 2 
534,ordinary least squares ols regression offers a decision oriented approach for modeling trends in annual peak flows we introduce a two stage ols approach for nonstationary flood frequency analysis that i models changes in their central tendency median in response to environmental perturbations with one regression and then ii examines changes in the coefficient of variation cv by running a second regression on anscombe transformed residuals from the first regression monte carlo simulations show that this approach yields 100 year flood estimates with mean squared errors comparable to estimates made with an advanced generalized linear model based method also this second stage regression often produces approximately normal residuals which permits statistical inferences on cv trends case studies illustrate the dramatic impact that decreasing and increasing cv trends can have on 100 year floods findings motivate the incorporation of trends in variability in infrastructure design along with further research examining asymmetric changes in urban flood variability graphical abstract image graphical abstract keywords flood frequency analysis heteroscedasticity nonstationarity regression urbanization variability 1 introduction worldwide floods cause an estimated 24 billion damage and the loss of thousands of lives annually kundzewicz et al 2014 observed and anticipated increases in flooding have spawned many new statistical methods for modeling changes in flood probability distributions over time e g strupczewski et al 2001 khaliq et al 2006 vogel et al 2011 salas et al 2018 and in response to specific environmental perturbations such as climate change e g jain and lall 2000 kwon et al 2008 condon et al 2015 urbanization e g beighley and moglen 2003 villarini et al 2009b gilroy and mccuen 2012 prosdocimi et al 2015 over et al 2016 oudin et al 2018 and reservoir storage e g lópez and francés 2013 over et al 2016 this includes numerous approaches that address the following question what is the magnitude of a flood expected once every 100 years or another recurrence interval of interest on average given current conditions in a basin while planners increasingly seek tools to characterize these changes many national level agencies are still working to establish design guidelines to reflect them for instance the united states bulletin 17c notes that practices for adjusting design floods for changes in basin conditions require more research before specific recommendations can be made england et al 2018 p 23 national institutions have recommended numerous safety factors statistical methods and mechanistic models for adjusting design flood estimates for ongoing anthropogenic perturbations kjeldsen et al 2008 madsen et al 2013 prosdocimi et al 2014 asfpm 2016 recently rescinded see fema and dhs 2018 ball et al 2016 but few best practices have been established moreover some fundamental questions regarding changes in flood regimes such as changes in urban flood variability require more research before standard methods are instituted many flood frequency analyses ffa that account for ongoing changes in basin conditions have been criticized for improperly distinguishing between deterministic and stochastic components of change e g montanari and koutsoyiannis 2014 serinaldi et al 2018 true changes in probability distributions of stochastic phenomena such as precipitation are more difficult to diagnose in small samples typical of hydroclimatic records since they may be conflated with long term persistence and other forms of natural variability e g cohn and lins 2005 luke et al 2016 however unequivocal deterministic changes in phenomena that are not subject to sampling uncertainty stemming from interannual fluctuations can also affect flood probability distributions in fact prior reviews have found that most annual maximum series ams of instantaneous peak flows which exhibit strong evidence of nonstationarity are associated with major deterministic basin changes such as urbanization e g villarini et al 2009a villarini and smith 2010 this makes methods for adjusting ffa in basins that have undergone such deterministic changes especially compelling e g serinaldi and kilsby 2015 changes in variability are a fundamental aspect of nonstationary ffa nsffa that is increasingly receiving attention e g strupczewski et al 2001 cunderlik and burn 2003 villarini et al 2009a delgado et al 2010 delgado et al 2014 o brien and burn 2014 condon et al 2015 ahn and palmer 2015 spence and brown 2016 yu and stedinger 2018 this is especially critical given the greater sensitivity of extreme design floods to trends in variability than to commensurate trends in central tendency katz and brown 1992 yet methods for adjusting probability distributions for deterministic changes in variability stemming from urbanization and other deterministic phenomena are still not as developed as those for trends in central tendency urbanization can decrease annual flood variability as increases in impervious cover have been associated with greater increases in smaller floods than larger ones e g hollis 1975 usace 1993 mccuen 2003 kjeldsen 2010 braud et al 2013 over et al 2016 in contrast cross sectional studies associating greater drainage densities with greater annual flood variability pallard et al 2009 suggest that urbanization induced increases in drainage density may also amplify flood variability though this has not been assessed in urban areas with confined subsurface drainage networks ogden et al 2011 changes in precipitation have also failed to explain other observed increases in urban flood variability villarini et al 2009 trudeau and richardson 2016 practical approaches for characterizing changes in both the central tendency and variability of floods must address decision makers and stakeholder concerns hecht 2017 and serago and vogel 2018 outline many benefits of regression for nsffa which taken together offer numerous features well suited for decision oriented analyses first its ease of use effective graphical communication and parsimonious estimation of conditional moments makes it an attractive alternative to many advanced methods see serago and vogel 2018 for more on the value of parsimonious models in ffa second regression offers decision relevant information including expressions of uncertainty confidence and prediction intervals and enables hypothesis tests regarding the influence of covariates on changing floods e g kwon et al 2008 prosdocimi et al 2014 such tests are valid when model residuals are serially uncorrelated normally distributed and homoscedastic constant variance or when standard error distortions from heteroscedastic non constant variance residuals can be sufficiently ameliorated using heteroscedasticity consistent standard errors e g long and ervin 2000 or through weighted or generalized least squares regression stedinger and tasker 1985 kjeldsen and jones 2009 regression also accommodates many smooth nonlinear functions through ladders of powers transformations mosteller and tukey 1977 as well as abrupt changes bates et al 2012 missing data slater and villarini 2016 and analytical corrections to the variance of regression coefficients inflated by short and long term persistence matalas and sankarasubramanian 2003 prior applications also demonstrate its suitability for nsffa while curtailing heteroscedasticity to minimize standard error distortions is critical for many statistical applications the non constant variance of residuals also provides valuable information about a phenomenon s variability especially when associated with physical covariates one can use regression to model trends in variability by first modeling changes in central tendency using one regression and then fitting a second stage regression on the transformed squared residuals from the first stage regression carroll and ruppert 1988 as we show later transforming squared first stage residuals can produce second stage residuals well approximated by a normal distribution which enables statistical inferences on variability trends this two stage regression approach known variously as heteroscedastic regression e g smyth et al 2001 zheng et al 2013 variance function regression e g davidian and carroll 1987 or parametric dual modeling robinson and birch 2000 has been used for over half a century park 1966 harvey 1976 as western and bloome 2009 note it has been employed in diverse disciplines including for pharmacological dose response curves e g davidian and haaland 1990 fish survival rates minto et al 2008 housing prices zhang et al 2015 climate change impacts to crop yields kelbore 2012 ex prisoner income western and bloome 2009 as well as many industrial applications smyth et al 2001 while other studies have modeled heteroscedasticity in continuous hydrologic time series e g wang 2005 sun et al 2017 fathian et al 2019 few have used it to assess flood regimes latraverse et al 2002 used a nonparametric form of local polynomial regression robust to heteroscedasticity for regional quantile estimates and lim 2016 examined it when characterizing the variable source areas of urban flooding yet the gap in practical methods for adjusting design events for changes in variability that mccuen 2003 identified has lingered despite the benefits of ols regression few studies have examined its ability to model changes in the coefficient of variation cv and incorporate them into design flood estimates recently yu and stedinger 2018 applied a two stage regression approach to estimate trends in ams variability but their work differs from ours in three respects first when estimating changes in variability they used nonlinear least absolute value regression instead of ols regression the latter with which stakeholders and decision makers are likely to be more familiar second we also compare our ols based method to a more advanced approach combining iteratively weighted least squares and generalized linear models glms aitkin 1987 third we evaluate the feasibility of this model for urbanizing basins and parameterize our monte carlo experiment based on observed changes in annual peak flows in urbanizing basins to demonstrate our approach we consider 100 year flood estimates made using the ams of instantaneous peak flows in urbanizing basins in the united states see stedinger et al 1993 section 18 6 1 and stedinger 2016 section 76 2 3 and associated references for further guidance on when an ams analysis is preferred over a pot one we use total impervious area tia as an example of a physical covariate which exhibits an unequivocal deterministic change over time however our approach can explore relationships between any covariate e g climate indices and annual floods in addition it can be extended to accommodate multiple covariates including different ones in the first and second stage models this paper is organized as follows section 2 introduces regression based methods for adjusting design flood estimates for trends in both the mean and variance of the natural logarithms of ams which are monotonically related to real space trends in the median and cv respectively section 3 presents a monte carlo simulation experiment that compares the performance of four design flood adjustment methods section 4 provides case study illustrations and examines single site trends in the median and cv of ams in urbanizing basins in the united states section 5 discusses limitations and possible extensions pertaining to both model development and planning applications while section 6 concludes 2 methods 2 1 model overview in ffa one typically uses maximum likelihood l moments or the method of moments to estimate the parameters of a probability distribution here we use the method of moments where the unconditional moments of the natural logarithms of the ams are replaced by their values conditioned upon physical covariates i e explanatory variables to reflect deterministic changes to a basin s flood response we introduce our approach using bivariate regression equations with the log transformed ams as the response variable and total impervious area tia as the lone covariate see section 3 however it can be extended to multivariate models while heteroscedastic regression can accommodate ams approximating many distribution types see serago and vogel 2018 we introduce our two stage regression approach for ams that arise from a two parameter lognormal ln2 distribution this distribution often approximates ams well under both stationary beard et al 1974 stedinger 1980 vogel and wilson 1996 and nonstationary conditions e g strupczewski et al 2001 vogel et al 2011 delgado et al 2014 prosdocimi et al 2014 under stationary conditions this parsimonious distribution can produce lower mean squared errors mse associated with quantile estimates than numerous three parameter distributions often used in ffa kuczera 1982 this distribution is also especially attractive for nsffa because it requires just four estimated parameters when the mean and variance of log transformed values change linearly with one covariate aissaoui fqayeh et al 2009 vogel et al 2011 also demonstrate the promise of ols regression for modeling time conditional ln2 distributions as this model yields residuals which are well approximated by a normal distribution for over 75 of 19 430 ams at unregulated and regulated gauging records of at least ten years in the united states however we emphasize that this approach could be applied to other two and three parameter probability distribution functions pdfs often used in ffa including the generalized extreme value gev and log pearson type iii lp3 pdfs provided the resulting regressions exhibit normally distributed and uncorrelated residuals see serago and vogel 2018 consider a logarithmic transformation of the ams discharges q 1 y ln q if q follows a ln2 pdf then y is normally distributed here we present a two stage regression modeling procedure which yields estimates of the mean and variance of y conditioned on the value of a physical covariate exhibiting a deterministic change we define the nonstationary flood quantile function of discharge q with covariate conditional moments as follows 2 q p ω exp μ y ω z p σ y ω where q p ω is the annual flood with non exceedance probability p for a covariate value ω μ y ω and σ y ω are the conditional mean and standard deviation respectively of the logs of the ams and zp is a standard normal variate for an ln2 distribution a trend in the log space mean corresponds to a trend in the real space median because the mean of logs is the 50th percentile median of the ln2 distribution a trend in the log space mean μ y ω also implies a commensurate trend in the real space variance which in turn enables the real space cv to remain unchanged also when there is no trend in the log space variance there is no trend in the real space cv either since c v q exp σ y 2 1 thus we henceforth refer to trends in the mean and variance of log transformed ams as trends in the median and cv respectively next we derive conditional moments of y corresponding to ams arising from an ln2 distribution as an example first we derive a nonstationary ln2 quantile function using a homoscedastic regression model with trends in the mean of the logs but no error variance trend i e the real space cv is constant then we turn to a heteroscedastic model that considers temporal changes in the log space error variance real space cv as well as the mean of the logs real space median finally we compare quantile estimates from these derived models to ones from an iteratively weighted least square generalized linear model aitkin 1987 table 1 summarizes these four models 2 2 homoscedastic nonstationary regression model hom ns ln2 numerous nsffa studies including vogel et al 2011 prosdocimi et al 2014 read and vogel 2015 over et al 2016 yu and stedinger 2018 and serago and vogel 2018 have employed a linear regression of the natural logarithms of the ams y ln q on an explanatory variable ω 3 y β 0 β 1 ω ε where β0 and β1 are regression coefficients and ε is a normally distributed zero mean error term with a constant variance over et al 2016 show that models with this functional form nicely characterize the response of ams to linear increases in tia in the chicago metropolitan area when the residuals ε in 3 are homoscedastic and normally distributed further statistical inferences can be made on β0 and β1 along with the overall model since the expected value of the residuals is zero the conditional expectation of y is 4 e y ω μ y ω 0 β 0 β 1 ω 0 where μ y ω 0 denotes that the mean of y is conditioned upon a covariate of a given value ω0 in a regression model trends in the conditional mean directly impact the error variance estimated with residuals and consequently they affect the conditional variance one can partition the variance of a dependent variable y into two components one explained by a covariate trend and another one comprised of the unexplained variance 5 σ y 2 β 2 σ ω 2 σ ε 2 this equation is best understood by considering two extreme cases i when a trend is perfectly explained by the regression i e σ y 2 β 2 σ ω 2 all data fall exactly on the trend line and the residual variance disappears entirely and ii when there is no trend the variance in the dependent variable arises strictly from the error variance alone i e σ y 2 σ ε 2 meanwhile the variance of y conditional upon a given covariate ω0 is equal to the model error variance σ ε 2 since β0 β1 and ω0 are all constant 6 v a r y ω 0 σ y ω 0 2 σ ε 2 next when errors are homoscedastic constant σ ε 2 is 7 σ ε 2 σ y 2 β 1 2 σ ω 2 importantly when estimating σ ε 2 from residuals of a bivariate regression a degrees of freedom correction factor n 2 produces an unbiased estimate of the error variance 8 σ ε 2 1 n 2 i 1 n y i y i 2 where n is the ams length while yi and y i are the observed and modeled annual peak flows respectively thus hom ns ln2 quantiles conditioned on a single covariate ω0 are estimated as follows 9 q p ω 0 exp μ y ω 0 z p σ y ω 0 exp β 0 β 1 ω 0 z p σ ε note that 7 can also be expressed in terms of the pearson correlation coefficient ρω y 10 σ y ω 2 σ ε 2 1 ρ ω y 2 σ y 2 where ρ ω y β 1 σ ω σ y many prominent ffa studies e g stedinger and griffis 2011 vogel et al 2011 prosdocimi et al 2014 luke et al 2016 have not considered reductions in the conditional variance of y proportional to ρ ω y 2 given in 10 read and vogel 2015 also show that reductions in σ y ω 2 also lower the conditional real space cv cv q ω as follows 11 c v q ω c v q 1 1 ρ ω y 2 1 2 3 heteroscedastic nonstationary regression model het ns ln2 next heteroscedastic regression model residuals imply that the cv of an ams changes with physical covariate values we derive a coupled two stage regression model for modeling heteroscedastic residuals het ns ln2 that ensures zero mean residuals preserves the sign of the residuals and allows the cv to vary with a covariate see appendix a the parameters of this variance model can be estimated with a second ols regression which in turn yields residuals likely to follow an approximately normal distribution we replace the covariate independent error terms from 3 with covariate dependent ones so that 12 y ω β 0 β 1 ω ε ω β 0 β 1 ω sign ε ω γ 0 γ 1 ω φ ω 3 2 where γ0 and γ1 are regression coefficients and φω is a normally distributed zero mean error term with a constant variance this expression keeps the expected value of estimated residuals ε ω at zero and preserves their sign as explained below 12 also enables us to estimate the covariate dependent error variance with a second stage regression model that relates a covariate to the first stage model errors in 3 raised to the two thirds power i e ε ω 2 3 while we illustrate this second stage regression using the covariate ω from the first stage model different covariates may be used for each stage e g an indicator of urbanization for changes in the median and a climate index such as the north atlantic oscillation for changes in the cv since 12 preserves the zero mean property of εω the conditional mean e y ω0 is identical to the conditional mean of hom ns ln2 i e e y ω 0 μ y ω 0 β 0 β 1 ω 0 thus var β0 β1ω0 0 v a r y ω 0 σ y ω 0 2 σ ε ω 0 2 next since e ε ω 2 σ ε ω 2 fitting an ols regression with εω 2 as the response variable and ω as the explanatory variable may appear useful for estimating σ y ω 0 2 and making inferences regarding the associated trend however if residuals from the first stage regression are normally distributed its squared residuals will follow a highly skewed χ2 distribution if these squared residuals are then used as the response variable in a second stage regression the second stage residuals are unlikely to be normally distributed in contrast if we raise the first stage residuals in 12 to the two thirds power we obtain a response variable that follows an approximately normal distribution except at its lower tail see appendix b 13 ε ω 2 3 γ 0 γ 1 ω φ ω if this second stage regression also yields normally distributed residuals we can make statistical inferences on the second stage model along with its coefficients γ0 and γ1 the transformation to the two thirds power represents a simplified version of the anscombe 1953 transformation which converts variables arising from gamma distributions of which the χ2 distribution is a special case to approximately normal ones carroll and ruppert 1988 while this approach has been applied to transform gamma distributed wet day precipitation data e g chandler and wheater 2002 kigobe et al 2011 we could not find any flood applications ultimately we want to use γ0 and γ1 to estimate σ y ω 0 2 since σ y ω 0 2 σ ε ω 0 2 e ε ω 2 it follows that 14 σ y ω 0 2 e ε ω 2 3 3 e γ 0 γ 1 ω φ ω 3 importantly one must include the error term ϕω in 14 or else the estimate of σ y ω 0 2 may be severely downward biased this leads to the following expression for the conditional variance 15 σ y ω 0 2 γ 0 γ 1 ω 0 3 3 σ φ 2 γ 0 γ 1 ω 0 the second term on the right hand side of 15 is proportional to the error variance of the second stage model σ φ 2 which tends to comprise a large portion of the total variance of ε2 3 in fact r2 values of the conditional cv model range from just 0 06 to 0 24 at sites with cv trends significant at the 95 level see section 3 in other words σ φ 2 explains 76 to 94 of the variation in ε2 3 yet even though cv trends explain a small fraction of the overall interannual variability neglecting this second term can cause estimates of extreme floods at sites with increasing median and cv trends to be lower than estimates only considering median trends when estimating σ φ 2 the degrees of freedom correction factor in 8 should also be used next by substituting 15 into 9 we obtain the het ns ln2 quantile function for adjusting design floods with a given annual non exceedance probability p conditioned on a given covariate value ω0 see appendix a for a full derivation 16 q p ω exp μ y ω 0 z p σ y ω 0 exp β 0 β 1 ω 0 z p γ 0 γ 1 ω 0 3 3 σ φ 2 γ 0 γ 1 ω 0 2 4 an advanced two stage approach with generalized linear models iwls glm we compared the het ns ln2 model to a more sophisticated two stage modeling approach using generalized linear models glms glms accommodate dependent variables with non normal distributions belonging to the exponential family of distributions aitkin 1987 showed that using iteratively re weighted least squares iwls estimation with variance estimates from gamma glms with log link functions yields asymptotic maximum likelihood estimates this prevents heteroscedasticity induced errors in residual estimates from propagating to the cv trend model although we assume a different variance change trajectory than aitkin 1987 these benefits of iwls glm also make it an attractive procedure for our approach despite these benefits nsffa studies have not used iwls glm as other prior studies using glms have either assessed changes in location parameters clarke 2001 clarke 2002 aissaoui fqayeh et al 2006 najibi and devineni 2018 or variability in the number of coastal nuisance floods vandenberg rodes et al 2016 the iterative iwls glm approach is implemented as follows 1 fit the hom ns ln2 model assuming equal weights for just the first iteration 2 square residuals of the hom ns ln2 model 3 fit a second stage gamma glm with a log link function using the glm2 package in r statistical software r core team 2019 4 use reciprocals of fitted values of 3 as weights when fitting hom ns ln2 in the next iteration repeat until convergence we did this ten times to ensure convergence to four decimal places this method produces estimates of the conditional mean and variance that can subsequently be inserted into the conditional quantile function in 2 to compute the 100 year flood 3 monte carlo simulation experiment we conducted a monte carlo experiment to assess the accuracy of our design flood estimates for the last year of simulated 50 year records when using the four estimation methods listed in table 1 we generated random samples of length 50 from a uniform distribution ranging from zero to one then we treated these values as non exceedance probabilities and computed annual flood values using covariate conditional ln2 distributions with known initial parameters and change trajectories we applied a three factor experimental design in which we simulated 50 year ams with different initial real space cv values and different correlations driving their median and cv trends ρω y ρ ω ε 2 3 based on our 202 station sample of urbanizing basins see section 4 3 we focused on cases with increases in the median and either increases or decreases in the cv given planner interests in minimizing bias uncertainty and worst case outcomes we computed the percent error distributions percent bias pbias fractional root mean squared errors frmse and maximum possible over and under design errors of conditional quantile estimates for the last year of record from a set of 10 000 simulations this experiment does not address general sampling issues that can cause downward biased estimates of the true 100 year flood value of a population stedinger 1983 see appendix d for more details on the experimental design and results the simulation experiments demonstrate that het ns ln2 underpredicts the 100 year flood when there is an increasing cv trend and overpredicts it when there is a decreasing one fig 1 this transformation over predicts lower tail values and slightly underpredicts upper tail ones see appendix b as expected design flood estimates considering modeling cv trends when present are more accurate than ones that only consider trends in the median the simulations of decreasing cv trends also suggest that the s ln2 quantile estimate may be better than hom ns ln2 for modeling changes in extreme floods in urbanizing basins one pervasive question in nonstationary hydrology is whether the reduction in bias from adding model parameters is worthwhile given the increase in uncertainty that it can induce for instance yu and stedinger 2018 found that modeling changes in the cv log space variance in their paper only improves the rmse under extreme changes yet in this study het ns ln2 consistently registers a lower frmse than hom ns ln2 which suggests that the bias reduction benefits of modeling cv trends outweigh the increase in variance stemming from the addition of another parameter fig 2 see figures for other cv values in appendix d we also compared het ns ln2 and iwls glm as expected iwls glm estimates were considerably less biased however het ns ln2 registers a lower frmse than iwls glm especially when the cv is increasing also maximum overdesign errors are larger under iwls glm than het ns ln2 when the cv is increasing but slightly smaller when it is decreasing in contrast maximum under design errors are slightly greater under iwls glm regardless of the cv trend direction thus choices between these two methods depend on a decision maker s preference for unbiasedness versus minimizing uncertainty and worst case outcomes in addition one could bias correct het ns ln2 results as its bias is markedly correlated with the cv see appendix d however we believe that other procedures for modeling cv changes should be evaluated before attempting bias correction see section 5 finally one must remember that this experiment only evaluates het ns ln2 when simulated ams originate from an ln2 distribution and the changes in moments follow the same trajectories that models assume when the true cv trend model followed the exponential change trajectory that the gamma glm model assumes het ns ln2 still yielded lower frmse values than iwls glm did for many trend combinations see appendix d in practice distribution types and change trajectories are unknown and prone to misspecification which makes a more detailed robustness analyses examining the performance of our models when these assumptions are incorrect a natural sequitur to this study 4 applications 4 1 urbanization effects on floods many aspects of urbanization alter flood regimes leading to a wide range of responses e g smith et al 2013 salvadore et al 2015 zhou et al 2017 diem et al 2018 kokkonen et al 2018 these urban drivers include increased impervious cover soil compaction stormwater routing and detention e g ogden et al 2011 miller et al 2014 increased channel width and reduced overbank storage hirsch 1977 return flows allaire et al 2015 oudin et al 2018 groundwater pumping hopkins et al 2015 induced recharge locatelli et al 2017 and changes in precipitation due to urban heat islands yang et al 2014 underlying drivers of change such as population growth that lead to these hydrological impacts have also been identified usace 1993 villarini et al 2009 given these myriad flood altering mechanisms one major challenge with characterizing changes in urban flooding is the selection of indicators to represent urbanization standard approaches based on the percentage of total impervious area tia have been criticized especially in cross sectional studies because the relationship between tia and flooding varies due to underlying soil types gregory et al 2006 stormwater and drainage infrastructure e g leopold et al 1968 ogden et al 2011 miller et al 2014 zhou et al 2017 the connectivity and spatial location of impervious cover e g mejia and moglen 2010 ferreira et al 2015 debbage and sheppard 2018 as well as its diverse and time variable hydrologic properties redfern et al 2016 and mapped spatial resolution lee and heaney 2003 weng 2012 however we argue that tia is a suitable metric given our goal of demonstrating a method for adjusting design floods at individual sites where deterministic changes in the central tendency and variability of flooding are evident 4 2 illustrative case studies we illustrate our nsffa method with two case studies in small urbanizing basins where prior studies have also documented urbanization induced increases in flooding 4 2 1 increase in median increase in cv the aberjona basin in the boston metropolitan area offers a natural laboratory for examining urbanization induced changes in flooding allaire et al 2015 serago and vogel 2018 villarini et al 2018 the aberjona river at winchester station gage id 01102500 drainage area 61 9 km2 has recorded instantaneous peak flows since 1940 here tia increases from 24 7 in 1940 to 37 7 in 2010 fig 3 demonstrates that trends in the median ρω y 0 46 p 0 01 and cv ρ ω ε 2 3 0 29 p 0 01 exhibit highly significant positive correlations with tia it also shows that the stationary 100 year flood 44 9 m3 s roughly equals the flood of record 45 0 m3 s under hom ns ln2 the current 100 year flood rises to 53 0 m3 s an increase of 18 incorporating the cv trend raises the 100 year flood to 64 9 m3 s which is 45 greater than the s ln2 value a probability plot correlation coefficient ppcc normality test fails to reject the null hypothesis that the residuals of the second stage regression are normally distributed p 0 39 a modified breusch pagan test breusch and pagan 1979 in which we regressed ε2 3 on ω also confirmed their homoscedasticity p 0 86 together these tests validate statistical inferences from the second stage model recent increases in total and extreme precipitation ahn and palmer 2015 huang et al 2017 have not translated to increases in the cv of annual floods in many less urbanized basins in eastern new england moreover hodgkins et al 2017 find a lack of a contemporaneous increase in floods with recurrence intervals of 25 100 years in the northeastern united states while further research is needed to untangle the effects of recent climatic fluctuations including inter decadal oscillations see armstrong et al 2014 berton et al 2017 and an abrupt increase in extreme precipitation huang et al 2017 from urban induced changes in the aberjona basin the evidence presented strongly suggests that urbanization contributes to the increasing cv here 4 2 2 increase in median decrease in cv fig 4 shows that the ams at the river rouge at birmingham michigan station usgs id 04166000 drainage area 86 2 km2 which has been operating since 1951 exhibits an increasing median and a decreasing cv the tia of this basin located in the western suburbs of the detroit metropolitan area steadily increased from 8 2 in 1950 to 24 7 in 2010 fig 4 exhibits a clear increase in the lower bound of the ams while the upper bound has remained relatively constant the lower bound continues to increase after 1970 a year when annual precipitation increased abruptly in the eastern u s e g mccabe and wolock 2002 this provides another line of evidence that smaller annual floods have increased more than large ones aichele 2005 also detected an increase in maximum daily flows with a 1 exceedance probability between 1970 and 2003 at a 90 significance level and associated it with a 20 increase in residential area between 1980 and 2000 while beam and braunscheidel 1998 note that combined sewer overflows also contribute to peak runoff there are no large flood control reservoirs in the basin a major drought from 1960 1967 paulson et al 1991 may also enhance the median flood trend overall the increase in tia exhibits a highly significant positive correlation ρω y 0 47 p 0 01 with the log transformed annual floods while tia has a highly significant negative correlation ρ ω ε 2 3 0 37 p 0 01 with the anscombe transformed residuals we could not reject the null hypotheses of normality and homoscedasticity of the second stage residuals using a ppcc normality test p 0 39 and our modified breusch pagan test for residual heteroscedasticity p 0 21 respectively importantly accounting for the decreasing cv trend in addition to the increasing median trend lowers the 100 year flood estimate by 28 8 from 50 1 m3 s with hom ns ln2 to 35 7 m3 s under het ns ln2 which is even 13 lower than the s ln2 estimate of 41 1 m3 s however fig 4 does not display strong visual evidence of a decrease in low frequency events a major reason for which the het ns ln2 100 year flood is lower than its s ln2 counterpart is that het ns ln2 assumes a symmetric decrease in the variance of log transformed annual peak flows whereas numerous prior studies demonstrate a greater tendency for smaller floods to increase than for larger floods to decrease a second reason is that the s ln2 estimate may be upward biased because when assuming stationarity the site has a negatively skewed distribution 0 59 while a nonstationary distribution fits quite well p 0 59 we must reject the stationary ln2 distribution of the ams at this site using a ppcc normality test p 0 01 see appendix c for a more detailed explanation this discrepancy highlights problems with direct comparisons of stationary and non stationary distributions a challenge that motivated serago and vogel 2018 to create nonstationary probability plots to evaluate their goodness of fit 4 3 flood trends in urbanizing basins of the united states a preliminary analysis 4 3 1 identifying urbanizing basins with changing ams numerous recent studies have analyzed floods in urbanizing basins throughout the united states e g salavati et al 2016 chao lim 2016 luke et al 2016 oudin et al 2018 we examined trends in the log space mean real space median and log space variance real space cv in 202 urbanizing basins with at least 30 years of instantaneous peak flow observations through the 2016 water year 1 oct 30 sep and at least 10 impervious cover during one year in their station record please note that some stations with trends may have substantially different 100 year estimates using data beyond 2016 most notably stations in houston due to major tropical cyclones in 2017 see zhang et al 2018 our study does not aim to identify an exhaustive subset of conterminous us basins with urbanization altered floods or choose the best indicator of urbanization for attributing changes in design floods e g jato espino et al 2018 debbage and shephard 2018 rather we strive to examine the general prevalence of single site cv trends in urbanizing basins also we are not aiming to draw any statistical conclusions about the prevalence of regional trends which would require a spatial correlation analysis douglas et al 2000 recent studies have used i basin attributes from the gages ii database falcone 2011 including impervious cover thresholds e g oudin et al 2018 and ii usgs peak flow qualification codes http pubs usgs gov wdr 2005 wdr il 05 misc peakcods htm luke et al 2017 to identify urbanizing basins we first removed individual annual instantaneous peak flow observations using qualification codes before removing entire station records based on their record lengths and other criteria we eliminated peak flows with uncertain dates maximum daily flows dam failures and estimated historical floods while historical floods during pre instrumentation periods often provide valuable information such records are more likely to exist for larger events than smaller ones this could cause downward biases in estimates of trends in central tendency using ols regression future research should evaluate the feasibility of integrating this method into ffa methods that accommodate historical records and other censored data such as the expected moments algorithm cohn et al 1997 next we used numerous criteria to select stations suitable for analysis we removed all stations with ams shorter than 30 years as cunderlik and burn 2003 recommended that ams for testing trends in variability span at least 30 50 years to reduce the risk of confounding trends with inter decadal variability note that regression based methods offer easily computable standard errors that allow for estimates of uncertainty that consider record lengths to avoid misleading trend diagnoses when abnormally small or large annual floods lie at the beginning or end of an ams see slater and villarini 2016 we also removed stations that had at least one 30 year period with fewer than ten annual peak flows stations with peak flows below 1 m3 s were also omitted due to the challenges of gauging such low discharges combined with the pronounced effect that these low estimates could have on conditional median regression models fit in log space changepoint tests in the mean and variance of the log transformed ams using the at most one change amoc method in the changepoints package for r statistical software r core team 2019 did not reveal any ams with changepoint confidence levels exceeding 95 when using minimum segment lengths of 10 years after performing this initial screening we identified a subset of urbanizing basins following oudin et al 2018 we first applied the impervious cover fraction obtained from the widely used nlcd dataset to identify basins whose tia exceeded 10 in 2006 while flood magnification has been detected in basins with less than 5 impervious cover yang et al 2010 and depends upon underlying soils e g gregory 2006 this standard threshold of 10 schueler 2009 enabled us to identify a set of urbanizing basins suitable for demonstrating our modeling approach we also only evaluated stations whose reservoir storage capacity was less than 10 of their mean annual flow according to gages ii we then performed a more detailed analysis of changes in tia from 1940 2016 using decadal housing density data 1940 2010 with a 1 km spatial resolution from theobald 2005 we eliminated stations with drainage areas smaller than 5 km2 due to the tia data s coarse resolution each basin s tia was computed using the reclassify and raster clipping functions in the spatial analyst extension of arcgis 10 4 the same procedure can be implemented using freely available qgis and grass gis software we then applied previously validated relationships from oudin et al 2018 to estimate the tia in each 1 km2 cell based on housing density data and tia on land used for commerce industry and transportation decadal values were linearly interpolated to create annual time series for 202 urbanizing basins tia from 2010 2016 was assumed to increase at the same rate as during 2000 2010 annual flood peaks prior to the beginning of tia time series in 1940 were omitted from the analysis we then identified stations in this subset of 202 urbanizing basins that had ams with first stage conditional median residuals for which null hypotheses of normality and no lag one correlation could not be rejected at a 95 significance level p 0 05 using version 2 5 of the sandwich package in r we computed all trend p values using robust heteroscedasticity consistent standard errors with the hc3 estimator which adjusts squared residual values to correct for the effects of overly influential observations long and ervin 2000 we deemed all trends with p 0 05 as approximately statistically significant at the 135 stations whose residuals were normally distributed and serially independent 67 of the 202 urban stations we tested for trends in the anscombe transformed residuals using our modified breusch pagan test 4 3 2 results table 2 shows the number of urbanizing basins from the 135 station sample with each of the nine possible combinations of trends in the conditional median negative insignificant positive and cv negative insignificant positive using het ns ln2 fourteen of the 67 21 urbanizing basins with significant increases in their median annual flood also demonstrated significant concurrent changes in their cv with six sites exhibiting increasing cv trends and eight sites including one pair of nested sites in houston texas having decreasing trends this suggests that hom ns ln2 is valuable for modeling trends in many urbanizing basins with changing flood hazards but it may neglect important changes to the relative variability of ams in others differences between het ns ln2 and iwls glm estimates reflected ones obtained in the monte carlo experiments as het ns ln2 exhibited a greater upward downward bias than iwls glm for decreasing increasing cv trends appendix f fig 5 maps stations with these different combinations of trends observed cv trends at stations with increasing median annual floods motivated us to apply het ns ln2 in basins featuring both trends while correlations between tia and the anscombe transformed residuals of models with significant cv trends are relatively weak with ranges of 0 26 0 49 for increasing trends and 0 26 0 48 for decreasing trends their pronounced effects on design floods make testing for cv trends imperative to further compare the models in table 1 we calculated percent changes in nonstationary flood quantiles attributable to median and cv trends first we computed the percent increase in the 100 year flood attributable only to increased medians q 1ooyr μ y ω 17 q 1 o o y r μ y ω q 1 o o y r q 1 o o y r where q 1ooyr is the stationary s ln2 100 year flood estimate next we computed the difference between the estimate considering both median and cv trends q 1ooyr μ y ω σ y w and the one only considering increases in the median q 1ooyr μ y ω 18 q 1 o o y r μ y ω σ y w q 1 o o y r μ y ω q 1 o o y r we normalized the differences in q 1ooyr μ y ω σ y w and q 1ooyr μ y ω by q 1ooyr to compare changes in 100 year flood estimates due to trends in the median and cv respectively for example an increasing median may lead to a 100 year flood estimate 50 greater than its stationary counterpart however the 100 year flood may not change at all if a decreasing cv trend fully offsets the effects of an increasing median trend on the 100 year flood in this case 18 yields a value of 50 fig 6 plots values of 17 and 18 for each of the 14 stations exhibiting significant increasing or decreasing cv trends see table 2 using estimates for the most current year on record fig 6 illustrates the importance of accounting for both trends in the median and cv when adjusting 100 year floods to reflect current basin conditions the contours indicate the overall percent change in the 100 year flood relative to a s ln2 stationary 100 year flood estimate stations with significant decreasing cv trends register much lower 100 year flood estimates whereas stations with significant increasing cv trends register much higher estimates overall 100 year flood estimates considering both increasing trends in the median and cv can exceed estimates assuming stationarity by more than 80 such changes are substantially larger than ones resulting from changes in the median alone decreasing cv trends can dramatically reduce design flood estimates adjusted for changes in central tendency even though our monte carlo simulations show that het ns ln2 overestimates the 100 year flood many stations in fig 6 lie below the 0 contour indicating no overall change in the 100 year flood when het ns ln2 is used in lieu of s ln2 in other words at these stations decreasing cv trends lower 100 year flood estimates more than increasing median trends raise them again there are two major statistical reasons for this tendency first the ln2 distribution is unable to consider asymmetrical changes to the variance of log transformed flows second when the conditional ln2 distribution is suitable for sites with increasing trends in the median and decreasing trends in the cv the stationary ln2 distribution is negatively skewed see appendix c this negative skewness causes the 100 year flood to be overestimated in fact all eight stations with significant increasing median and decreasing cv trends had negatively skewed samples 0 32 to 0 94 when stationarity was assumed while the null hypothesis of a stationary ln2 distribution could only be rejected at less than 13 of all 135 stations with well behaved residuals it was rejected at four out of eight stations with significant increasing median and decreasing cv trends finally while we did not test the regional significance of trends we examined spatial patterns qualitatively see fig 5 some previously reported regional change patterns are evident but overall they are not overwhelmingly strong reflecting the relatively fragmented nature of flood nonstationarity in the conterminous us archfield et al 2016 in some cases strong urbanization impacts counter observed regional trends for instance at glen cove creek at glen cove new york gage id 01302500 the cv decreases despite the extreme precipitation increase in the northeastern us huang et al 2017 yet a few known regional scale climate driven trends do appear in fig 1 three decreasing cv trends at north central stations align with observed increases in the frequency but not the magnitude of floods there hirsch and archfield 2015 mallakpour and villarini 2015 although this decrease is not observed at any of the nearby chicago metropolitan area sites four of the six stations with increasing trends in the cv are scattered throughout the northeastern united states however while this region underwent an increase in the 99th percentile daily precipitation in 1996 huang et al 2017 the main increase in annual floods stems from an north atlantic oscillation induced step change in 1970 armstrong et al 2014 for instance the boston metropolitan area only has one station with an increasing cv trend the geographic dispersion of the four sites with increasing cv trends in the eastern united states suggests that local hydrologic responses to urban development may contribute to these trends especially since some are near stations with insignificant cv decreases on the other hand seattle has several urbanizing sites with near significant cv increases near one site with a significant cv increase 5 discussion this discussion begins by critically evaluating the models examined in this study before contemplating the incorporation of our method in flood management decision making 5 1 model performance and recommendations our modeling framework enables evaluations of assumed trajectories of gradual change in flooding in response to increases in tia here our first stage regression model assumes an exponential relationship between tia and the ams which is mathematically equivalent to a linear relation between tia and the logarithms of ams this functional form has nicely modeled changes in the median for ams in numerous urbanizing environments vogel et al 2011 over et al 2016 and suggests that the magnification of flooding accelerates as basins become more urbanized however changes in the logs of ams may not be linearly related to changes in different indicators of urbanization such as impervious cover moreover recent stormwater regulations may temper urbanization induced increases ntelekos 2010 zhou et al 2017 this suggests that a logarithmic change trajectory may be more appropriate for basins with linear increases in impervious cover given that ladders of powers transformations enable many smooth monotonic trajectories of change to be modeled with regression hypothesized functional forms reflecting varying trajectories of tia and other indicators of hydrologic impacts of urbanization can be evaluated with our modeling framework since improperly specifying the functional form of the first stage model can bias residual estimates and in turn affect second stage models showing variability trends robinson and birch 2000 we examined alternative functional forms however we obtained a similar set of basins with significant cv trends with these alternative models for basins whose medians increased significantly see appendix f other methods for modeling variability change trajectories also merit future attention notably yu and stedinger 2018 examine the following model that assumes the log space error variance changes exponentially in response to a linear change in a covariate ω 19 σ ε ω 2 σ ε 0 2 exp β 2 ω after log transforming this model they use nonlinear least absolute value regression to estimate β2 while not an ols procedure this relatively parsimonious method has numerous advantages including that it prevents the variance from being negative though ours did not yield negative values moreover aitkin 1987 demonstrated that iwls glm can yield maximum likelihood estimates of this model s standard errors yet while attractive for estimating change trajectories log transforming the squared residuals from the first stage model is less likely to produce normally distributed residuals than het ns ln2 see appendix b this requires more complicated procedures for making inferences about cv trends further it remains unknown whether this change trajectory models observed changes in the cv with more fidelity than het ns ln2 it would be preferable to compare these two methods in a more expansive robustness study that also examines implications of mis specifying variance change trajectories in addition regression models with binary indicator variables would enable the incorporation of abrupt changes in urban basins such as flood control reservoir construction over et al 2016 or build out maximum permitted growth dates numerous other aspects of residual modeling also merit further investigation first comparing estimates from het ns ln2 to ones that model trends in the first two conditional moments separately would illuminate the value of coupled models of conditional moments additional comparisons with other residual transformations e g efron 1982 are also warranted last we should examine the benefits of using studentized residuals to alleviate the effects of leverage from extreme observations near the ends of ams on residuals robinson and birch 2000 manning and muhally 2001 5 2 toward improved urban flood management we introduce a nsffa approach for estimating design floods that reflect current conditions in urbanizing basins including changes in both the central tendency and the relative variability of ams we have only reported trends significant using a 5 significance level this standard approach may neglect less pronounced trends that could modify the 100 year flood estimates substantially especially when those trends are in the cv more research is needed to develop risk based planning tools that consider both over and under design probabilities associated with type i and ii errors from both median and cv trends see rosner et al 2014 and prosdocimi et al 2014 for some initial efforts with trends in central tendency in practice applications of our two stage approach should examine the sensitivity of trend assessments to the start and end dates of ams burn and whitfield 2018 and large historical floods outside of gauging periods e g cohn et al 1997 finally if future design flood quantiles of interest are sought we strongly recommend reporting prediction intervals for any extrapolated estimates which tend to widen quickly with time and document our in ability to predict the future the extent to which cv trends can modify 100 year flood estimates motivates future studies that attribute such trends to physical phenomena associated with urbanization and disentangle them from concurrent climatic fluctuations oudin et al 2018 employed a model residual approach that assessed urbanization impacts to high flows q95 by calibrating a rainfall runoff model during a pre urbanization period and then running it during a post urbanization period differences between observed and simulated flows during the post urbanization period were attributed to urbanization although the possible systematic underestimation of high flows from their continuous simulation model see farmer and vogel 2016 may confound their conclusions yet if continuous simulation models can be improved or bias corrected statistical analyses of the results of factorial design experiments using mechanistic models e g jato espino et al 2017 could also elucidate specific drivers of urban flood variability such as effective impervious area ebrahimian et al 2016 stormwater detention and road construction 6 conclusions when responding to increasing trends in ams decision makers need nonstationary flood frequency analysis nsffa methods that are easy to implement and address stakeholder needs previous studies e g vogel et al 2011 prosdocimi et al 2014 have demonstrated the numerous benefits of ordinary least squares ols regression for adjusting design floods by solely accounting for changes in the mean of ams natural logarithms to extend these efforts we introduced a two stage ols regression model that accounts for changes in the cv of ams we applied this approach to 135 stations in urbanizing basins with i drainage areas of at least 5 km2 ii tia exceeding 10 at one point in their record iii record lengths of 30 years or longer without any extensive intermittency in gauging and iv conditional median models with normally distributed and temporally uncorrelated residuals of the 67 stations exhibiting significant 95 level increases in their medians 21 14 135 had concurrent cv trends see table 2 this suggests that hom ns ln2 which assesses trends strictly in medians often ends up being a viable first order approach for characterizing observed changes in design flood quantiles in urbanizing basins whose ams exhibit nonstationary behavior and normal residuals vogel et al 2011 however we document the importance of incorporating cv trends into 100 year flood estimates which have societal ramifications ranging from bridge design to property insurance finally challenges modeling the asymmetrical changes in variability in urbanizing basins and comparing stationary and nonstationary versions of theoretical distributions motivate the development of other models for basins with decreasing cv trends this motivates the use of three parameter probability distributions e g lp3 where changes in skewness can partially compensate for changes in variability indeed this work combined with serago and vogel 2018 provides an ols regression based conditional moments framework for determining the conditional skewness of ams in response to changes in the mean and variance of log transformed flows to reflect the tendency for urbanization to magnify larger annual floods much less than smaller ones statistical approaches that enable a fixed upper bound such as the gev type iii distribution and quantile regression should also be investigated future work should also evaluate our two stage approach for partial duration series peaks over threshold which has recently demonstrated a stronger association with urbanization than ams prosdocimi et al 2015 or even complete daily flow time series serinaldi et al 2018 finally we encourage applications of this modeling framework that assess other hydroclimatic extremes declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements and data the authors are indebted to the u s army corps of engineers usace institute for water resources iwr for their encouragement and support of this research the second author was supported by an appointment to the u s army corps of engineers usace research participation program administered by the oak ridge institute for science and education orise through an interagency agreement between the u s department of energy doe and the u s army corps of engineers usace orise funding was managed by orau de ac05 06or23100 all opinions expressed in this paper are the authors and do not necessarily reflect the policies and views of usace iwr doe or orau orise two national science foundation grants enabled tufts university water diplomacy nsf oia 0966093 and the university of vermont bree nsf oia 1556770 to provide in kind support the authors are also grateful for earlier reviews and discussions with annalise blum jake serago nancy barth and karen ryberg all data used are publicly available the observed annual maximum series instantaneous peak flow data are available from the national water information system at http waterdata usgs gov nwis total impervious area tia data were derived from historical housing density data used in the us environmental protection agency s integrated climate and land use scenarios iclus project https www epa gov iclus other watershed characteristics were obtained from the gages ii dataset falcone 2011 models used in this study are available from the authors upon request supplementary materials supplementary material associated with this article can be found in the online version at doi 10 1016 j advwatres 2019 103484 appendix supplementary materials image application 1 image application 2 
