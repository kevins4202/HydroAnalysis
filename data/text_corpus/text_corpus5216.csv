index,text
26080,coastal forecast systems are used for many purposes including harbor management search and rescue operations and response to extreme events however the generation and operation of these systems is time consuming requires expertise in both information technologies and modeling of coastal processes and needs dedicated computational power the new service opencoasts overcomes these difficulties by generating on demand coastal circulation forecast systems through a web platform with minimal user intervention using a web platform the user is guided through seven simple steps to generate an operational forecast system for any coastal region the only requirements are an unstructured grid of the study area and information on river flow if applicable the platform provides ocean and atmospheric forcings and data for model validation and includes interfaces for results visualization and forecasts management forecasts are generated with the community model schism and computing resources are provided through the european open science cloud graphical abstract image 1 keywords schism forecast systems unstructured grids web platform eosc software availability program title opencoasts developers joão rogeiro joana teixeira contact address jrogeiro lnec pt software access https opencoasts ncg ingrid pt year first available 2018 software required browser firefox google chrome program language python html css javascript availability and cost open access upon registration email required 1 introduction in the late 20th and early 21st centuries the oceanographic community started developing forecast systems to provide short term predictions of ocean and coastal hydrodynamics e g clancy and sadler 1992 brassington et al 2007 baptista et al 2008 mehra and rivin 2010 these forecast systems use hydrodynamic models forced by atmospheric model results to provide among other variables sea surface elevations currents and wave spectra a few days in advance more recently forecast systems were extended to oil spills e g sotillo et al 2008 oliveira et al 2014 abascal et al 2017 and biochemical models e g triantafyllou et al 2007 marta almeida et al 2012 initially fuelled by military needs these forecast systems now have a variety of uses such as search and rescue operations breivik and allen 2008 warning of extreme events fortunato et al 2017a ferrarin et al 2019 and bathing water quality viegas et al 2009 bedri et al 2014 oliveira et al 2015 as the quality of the predictions increases due to more accurate atmospheric forecasts data assimilation improved models and higher computational power ocean forecast systems are increasingly being adopted by a variety of end users such as coastal managers harbor authorities civil protection agencies and the surfing and sailing communities presently the development and operation of ocean forecast systems require significant resources large teams with expertise in both numerical modeling of ocean processes and information technologies and powerful computational resources such as computer clusters also the daily maintenance of these systems can be very time consuming since they can crash for a variety of reasons numerical instabilities of the model network communication problems power shortages hardware failures lack of forcing conditions due to crashes in forcing forecast systems etc as a result the development and operation of forecast systems have mostly been limited to large research groups from universities and research centers in some countries operational organizations such as noaa in the usa provide predictions freely for their national coasts however most countries lack the availability of coastal predictions for coastal management tasks coastal centered economic activities or even recreation opportunities for the population in particular at the required spatial resolution clearly many organizations could benefit from these forecast systems but do not have the resources required to develop and maintain them there is therefore a clear need to develop a service that can empower potential users to develop and maintain their own forecast systems with little effort and cost by significantly reducing the cost and time required to develop coastal forecast systems as similar software as a service technological platforms swain et al 2016 golding et al 2019 have achieved an on demand service available through a web platform is expected to promote a drastic increase in the number of existing forecast systems and their uptake by various end users and the society in general web portals have been recognized as an excellent solution for sharing and managing spatial data jiang et al 2019 saah et al 2019 the opencoasts service aims at filling this need supported by european open science cloud eosc resources this service assembles on demand circulation forecast systems for user defined coastal areas and keeps them running operationally it generates daily forecasts of water levels and depth averaged velocities over the region of interest for 48 h based on numerical simulations of all relevant physical processes forcing conditions at the boundaries and over the domain are defined by the user from global and regional forecast databases automatic comparison with real time in situ sensor data can be provided for a number of user specified locations taking advantage of the european marine observation and data network emodnet http www emodnet eu what emodnet the present paper describes opencoasts and illustrates its application at estuarine and shelf scales this service is in operation in the portuguese and spanish infrastructures for distributing computing in the framework of the european open science cloud eosc initiative cloud resources are increasingly popular in environmental modeling due to their flexibility to provide resources on request and to the simplicity of their use chandrasekar et al 2012 gentzsch 2014 gomes et al 2018 issues related to model performance remain a concern for environmental applications given the need for infrastructure abstraction and multi tenancy occupation o donncha et al 2016 in spite of these challenges cloud resources are the backbone of several complex modeling and data based applications providing computational power to many application fields e g glenis et al 2013 qiao et al 2019 phuong et al 2019 in the context of environmental forecasting additional concerns for the use of cloud computing include communication between nodes for parallel computations o donncha et al 2016 security of data and quality of service in particular to guarantee the delivery time of the forecasts rogeiro et al 2018 in the field of coastal hydrodynamic and water quality modelling a comparison of model performance indicators for an operational forecast simulation executed in local workstations a hpc cluster and a pilot cloud revealed the good performance of cloud computing resources rogeiro et al 2018 because the size of the computational jobs is unknown a priori opencoasts poses new challenges to cloud computing resource allocation thus calling for simple and robust solutions to deliver predictions efficiently and on time this paper is organized as follows section 2 provides an overview of opencoasts this service is then described in detail in sections 3 frontend and 4 backend section 5 presents the implementation of opencoasts in the european open science cloud eosc the application of the service and its platform is then illustrated in section 6 finally section 7 provides an overview of the experience gained in the development of opencoasts and its worldwide application and discusses its evolution in the near future 2 concept and general description 2 1 opencoasts requirements and main properties a forecast system is typically based on a local implementation of the model of choice duly validated and calibrated for local conditions forcings for this forecast are based on available providers of local boundary conditions and the user interface is built to address the targeted end users requirements this approach provides tailored results but lacks the generic methodology and high flexibility behind a service initial deployments of the water information forecast framework wiff are examples of this legacy approach e g oliveira et al 2015 fortunato et al 2017a more sophisticated approaches provide higher flexibility to the users werner et al 2013 by making several models available and allowing for some customization according to user s needs however all these approaches are based on a local implementation concept and do not automatically provide the capacity to build forecasts on demand over a web interface nor the integrated connection to the necessary computational resources as a result building and maintaining these systems in operation require considerable human resources and a continuous demand for supporting computational resources the coastal forecast service paradigm proposed herein through the opencoasts service provides fully flexible deployment customization from the user to the backend perspective forecast services are generic platforms that can be applied by users of any discipline to produce customized applications to specific sites they are often used in the atmospheric sciences domain taking advantage of the availability of global forecast system results kim et al 2018 golding et al 2019 unlike forecast systems coastal forecast services are site independent applicable to any site through customization of its platform hide the complexity of computational resources allocation from users and optimize resource usage depending on the characteristics of the application site opencoasts goes one step further from existing forecast systems and provides a forecast as a service solution similarly to software as a service saas where users are empowered to build their own forecast systems on demand this platform is therefore a forecast building service supported by a site independent model and computational infrastructure that can be used to generate site specific forecast systems on demand by the users based on the outcomes of similar services for other domains swain et al 2016 golding et al 2019 coastal forecast services should thus allow for smooth implementations to distinct domains choices of parameters and forcings and on the fly output production and visualization saah et al 2019 from a user perspective for forecast administration roles services provide simple adaptation to different models and model versions and expedite interoperability among distinct computational resources providers moreover all parts in the forecast engine are modular and can easily be used to address repetitive tasks in an efficient and quality controlled fashion building a framework to provide a forecast service of broad application in the coastal community requires the identification of the necessary requirements from both user and administration perspectives the requirements analysis for the development of the opencoasts service identified several properties that need to be addressed the list is provided below along with a short explanation on how these were addressed at the implementation stage 1 broadly available the service should be available through a broad access platform without requiring additional configuration or software installation the opencoasts service is provided through a web application usable on multiple devices 2 simplicity and usability the service should be accessible to all coastal community members and provide a straightforward experience to them opencoasts allows both inexperienced and experienced users to apply state of the art models to develop forecasts systems for their domain of interest while benefiting in a transparent way from the computational infrastructure provided through the h2020 eosc hub project a single item is required to use the service a computational grid of the domain of interest 3 comprehensive the service should provide access to all forecast related tasks through a single platform minimizing dependencies on external software for full uptake of the service opencoasts was built as a one stop shop tool for users providing forecast configuration management and output visualization facilities in a single platform from an administration perspective integrating the django web framework within the platform allows taking advantage of its automatic administration interface that provides simple and straightforward management of users all tasks on deployments and all their properties as well as accounting and quality control tasks 4 accurate and reliable forecast quality and persistence depend directly on the accuracy and robustness of the model being used for the predictions and on the quality of the deployment input conditions opencoasts is based on schism zhang et al 2016 an open source unstructured grid high performance modeling suite supported by a strong international community 5 flexible a service should provide some degree of freedom for users to select the conditions that are deemed more adequate for their system of interest opencoasts allows for the selection of the domain of simulation at the required spatial discretization of the forcings amongst broad use options and of some model configurations through this flexible approach it is also adapted to users knowledge by providing pre defined selections for less experienced users 6 modular forecast services need to be comprehensive in terms of processes and spatial coverage for full support to all entities in the water cycle the modularization of the development parts constitutes a core property to support growth and adaptation to the best available components opencoasts was built in a modular way to provide support to grow from the current two dimensional 2dh barotropic physics to all processes in the water cycle including the urban dimension simultaneously it was prepared to accommodate new models forcing options and the ability to quickly replicate an existing deployment to allow for small changes in model parameters the development of an unopinionated framework defining simple entities with clear ways of interaction and a well planned data model are the ingredients to achieve this property 7 verifiable forecast systems are based on modeling tools and their applications their validity should therefore be automatically checked against field data preferably from multiple sources to prevent misleading conclusions based on data errors in opencoasts the nearest data stations available at emodnet are automatically proposed to the user providing a systematic procedure to evaluate prediction quality against one of the major data providers an operational forecast service with worldwide coverage such as opencoasts requires the availability of considerable reliable and replicated computational resources to guarantee the delivery of the forecasts outputs to the users in due time the simulations in the opencoasts service are performed with the model schism a model based on mpi that can take advantage of several connected nodes indeed a comparison of model performance indicators for an operational forecast simulation with schism executed in local workstations a hpc cluster and a pilot cloud revealed the good performance of schism in cloud computing environments rogeiro et al 2018 for resource size allocation opencoasts defines several resource allocation classes each with different amounts of computational resources the classes are defined based on the available computational resources each of the operating deployment daily simulations is associated to one of the predefined resources size classes depending on the horizontal grid size this approach makes for an efficient and fair of use of resources while keeping the distribution of resources among the simulations fairly simple and robust in the scope of the portugal s national infrastructure for distributing computing incd infrastructure roadmap project in portugal and the h2020 eosc hub project in europe framed by the european open science cloud initiative implementation opencoasts simulation resources are provided at the incd and ifca instituto de física de cantabria spain facilities in a transparent way to the user generic access to other computing centers is being planned through the integration of the core eosc hub services for authentication accounting computation and data preservation 2 2 the overall architecture and components the opencoasts service architecture fig 1 includes 1 the apps tier a frontend that comprises the user interaction components for forecast systems configuration management and visualization via a web application 2 the services tier a backend where models and mapping services run and 3 the storage tier for preservation the services and storage tiers are briefly presented below 2 2 1 web platform this component provides access to the service through web pages hosting the configuration wizard the forecast manager and the viewer applications apps each of these apps allows users to interact with the different facilities of the service the django web framework is used as the development framework of this component which follows django s design philosophy of having a project composed of applications each with a set of concerns and functionalities 2 2 2 forecasting while the web application allows the users to interact with the opencoasts service and manage their operational real time forecast systems the forecasting component is responsible for producing the forecast results for each deployment since it is a central part of opencoasts it interacts with all other components directly or indirectly to be able to gather all the necessary information to run each deployment simulation and make the forecast results available through the web the opencoasts service is based on wiff water information forecast framework fortunato et al 2017a a framework that includes the building blocks required to assemble custom forecast systems easily these blocks cover different aspects of a forecast system the top block forecast includes a simulation block which in turn is comprised by steps detailed descriptions of each entity can be found in section 4 2 a step encapsulates self contained tasks allowing them to be re used in different simulations the definition of a simulation includes the list of steps to be executed following a specific order the modularity requirement is fulfilled by having the possibility of reusing any of the steps within different simulations for instance changing from schism to a different circulation model in opencoasts would only require replacing the step wrapping the model execution by a new one for the new model assuming both models are compatible e g use the same inputs and configurations 2 2 3 mapping the mapping services complement the web application ones by providing web map services wms which are then used by the viewer component of the web application the forecasting models output files are netcdf files composed of unstructured mesh data and considering most commonly used web map servers such as geoserver and mapserver are limited to serving vector and structured grid data ncwms2 software blower et al 2013 is used to serve unstructured grids on the web 2 2 4 storage the storage component keeps the state of all services and is a requirement for all of them the storage technologies range from typical relational databases servers to lower level shared file systems the relational database software used is the postgresql plus the postgis extension storing most structured information about the service the storage service currently provided by nextcloud makes input and output files available to the other opencoasts components shared file systems in this case nfs are used to share folders and files among the computation and mapping host resources the following sections present in detail the two main components of the opencoasts platform 3 frontend the opencoasts web application https opencoasts ncg ingrid pt has three main components the configuration assistant guides users through several steps to assemble circulation forecast systems for coastal areas of their choice the forecast systems manager provides information about the user s systems status and tools to edit them and the outputs viewer provides access to the model s output files and tools to view the results in a web map and to compare them with pre selected observation stations data in the form of charts along with these main components the web application includes a user profile page and a rating feature that enables users to provide feedback on their experience and therefore contribute to improving the service the opencoasts web application life cycle includes the development of new functionalities in the near future this section describes the main components listed above in the currently available version 3 1 the configuration assistant this component guides the user over the seven steps described below through the creation of a new forecast system at the user s region of interest the only mandatory requirement from the user is a 2d triangular computational grid which includes the bathymetry as an input file other inputs such as a spatially variable friction coefficient can also be provided by the user but are not mandatory in step 1 model the user selects the model version to use in the forecasts and the forecast period the current version of the opencoasts service offers a single model and period the schism v5 4 0 model with a 48 h forecast period in step 2 domain the user uploads a triangular 2d unstructured grid indicating its coordinate reference system from a provided list or by entering an epsg code and the vertical datum if the uploaded file is successfully validated the user can preview the grid s boundaries and bathymetry on a map for visual inspection fig 2 the third step boundaries allows users to specify the type of each open boundary ocean or river and the forcing to be used at each of them for ocean boundaries the current version of the service includes the following forcings prism2017 portuguese regional tide surge model forecasts 2017 fortunato et al 2016 2017a developed to forecast water levels along the portuguese coast prism2017 simulates tides and storm surges in ne atlantic using the schism model in 2dh barotropic mode the model is forced by the global tidal model fes2012 tidal potential atmospheric pressure and winds from the atmospheric model gfs the resolution of the grid along the portuguese coast is about 250 m fes2014 finite element solution 2014 version https www aviso altimetry fr en data products auxiliary products global tide fes description fes2014 html carrere et al 2016 the latest version of the fes tide model developed in 2014 2016 relative to its predecessor fes2012 this version takes advantage of longer altimeter time series and better altimeter standards improved modeling and data assimilation techniques a more accurate ocean bathymetry and a refined mesh in most shallow water regions for river boundaries the current version of the service requires the user to provide one average annual flow value or 12 monthly average flow values in addition to the open boundary forcings the user can also specify an atmospheric forcing in this step fig 3 two choices are available gfs global forecast system noaa ncep https www ncdc noaa gov data access is a weather forecast model produced by the national centers for environmental prediction ncep that covers the entire globe at a base horizontal resolution of 0 25 and a time step of 1 h arpege europe and atlantic météo france https donneespubliques meteofrance fr fond produit id produit 130 id rubrique 51 courtier et al 1991 is a regional atmospheric forecast model provided by météo france that covers the european continent and seas longitudes 32 to 32 latitudes 32 to 60 at a base horizontal resolution of 0 1 and a time step of 1 h step 4 stations allows users to define locations within the simulation grid where to extract model time series with the output model resolution virtual stations it also allows to compare them with observation time series provided by the emodnet web platform comparison stations the observation stations located inside the simulation grid are automatically suggested to the user as comparison stations step 5 parameters generates the file containing the physical and numerical parameters fig 4 the user can choose between a preconfigured parameter file proposed by the application or can manually customize some of the parameters in this version only a few parameters can be edited such as the time step or the initial ramp up period along with the simulation grid and the parameter file schism requires additional data that must be defined in step 6 additional data in the current version of opencoasts only the manning coefficient can be specified by the user this coefficient is used by schism in 2dh mode to determine bottom friction based on local characteristics the user can either specify a constant value or upload a file with the spatial variation of this parameter the seventh and final step submission allows the user to submit the request at this point the configuration assistant presents a summary of all the previous steps allowing the user to review previous choices the user can then submit the configuration or save it for future submission submitting the configuration will create and activate a new forecast system and launch a new deployment in the backend component of the opencoasts platform presently simulations are running on the incd and expire after a month of operation thus promoting the rational use of incd resources integration with ifca resources is underway the first model results are normally available within 24 h after submitting a new deployment the application redirects the user to the forecast systems page described below 3 2 the forecast systems manager the forecast systems manager fig 5 enables users to monitor and manage their forecast systems this component of the web application allows users to consult basic information from the systems such as the model and version used the deployment name and description the reference dates creation start end and last run dates and the system s state which can be one of the following at step n configured active expired or disabled when a system s configuration is submitted the service changes its status to active indicating that the deployment is set up to run at the backend if errors occur during a deployment s simulation run an error label is added to the system s state in such cases the user can access the deployment log files to analyze and deduce probable causes of error also when an active system approaches its expiration date an expiring label is added to the system s state and depending on the number of days left users can request an extension of the deployment after a system reaches its expiration date the service updates its state to expired and the deployment at the backend will stop running the model and producing results at any time users can pause an active system changing the state to disabled and temporarily stopping the deployment this feature allows users to manage which systems are up and running a useful feature considering that the opencoasts service restricts each user to a limited number of active systems users can re activate them at any time the possibility of launching several simultaneous deployments for the same coastal system can be used for model calibration and sensitivity analyses in general the forecast systems manager also includes tools to open a system in the configuration assistant if not submitted yet or still under configuration delete systems preview systems configuration summary or print them as a report and clone systems this last operation creates a new system based on another allowing users to change its configuration and re submit it under new conditions 3 3 the outputs viewer the outputs viewer component fig 6 provides an interface to visualize and access the forecast results and log files this interface allows the user to extract and download time series at user defined locations using standard formats download specified model outputs in standard formats visualize model outputs of water levels and velocity and access automatic data model comparisons at selected data stations the main goal designing this interface was to allow flexibility in the presentation of results users can preview a system s model outputs on a map and on charts stations time series and combine them with the outputs of other systems the interface is divided into three main panels the systems map and charts panels the systems panel lists all of the user s active systems as sub panels that contain the tools to preview and access their outputs in more detail the system panel contains three tabs the maps tab lists map outputs that can be added to the map as layers the stations tab shows the stations defined during the system s configuration stage that can be added to the charts and the files tab lists the daily model output files available for download the map panel renders model outputs as layers on the map from wms services created by the mapping component of the opencoasts platform the map displays a toolbar that contains a set of common tools e g zoom in zoom out toggle basemaps and customized tools e g add location bookmarks tool and a probing station tool along with the ability to view the layer color scheme toggle the layer opacity and drag and drop layers to change their order on the map from a legend panel the model output wms are served by ncwms software that supports time dimension therefore the map panel contains an additional toolbar that allows users to slide the map layer steps in time and a play pause button to view them in an animated fashion the charts panel presents the model time series at stations added from the system s stations tab or added on the fly using the probing tool from the map panel for comparison stations defined at the configuration stage the interface also tries to load observation time series from the emodnet platform if data are available for the same time period they are added to the charts for model validation finally the map and charts panels are synchronized so as users change the time reference on one panel the other one is automatically updated in the future developments the opencoasts service will allow users to share their systems with other users giving them permission to consult them 4 backend the opencoasts backend i e the forecasting engine constitutes the core operational component of this service it is built in modules to promote interoperability and reuse for other applications in particular to support forecasting of different water dynamics or the use of alternative simulation models the next sections present the forecasting engine its supporting framework wiff and its application within the service opencoasts 4 1 forecasting engine this component generates the forecast results by handling all aspects of the simulation chain established for each forecast fig 7 this chain is updated daily by producing predictions from new model simulations covering future time intervals starting from the conditions of the previous day hot start from an information perspective each simulation run has two types of requirements the permanent ones are supplied by the user during the setup and remain unchanged throughout the lifetime of the forecast and the transient requirements which correspond to boundary forcings are updated daily for each simulation of a particular system forecast a model simulation begins by gathering the information supplied by the user and using it as the foundation for the remaining procedure then the forcings are retrieved covering the whole daily prediction interval for all boundaries afterward parameter files are updated for the daily conditions after completing all the previous steps everything is in place to start the simulation execution which may potentially be a very time consuming operation this execution is performed by taking advantage of the parallel computing option in the model schism since the previous step produces output results at each subdomain resulting from the parallel model execution the output files for each variable must be combined into a single file before making these results available in the map server all the steps described previously are encapsulated within the wiff developed specifically to make the creation of forecast deployments as systematic and generic as possible wiff was developed to address the limitations of custom made similar purposed forecast systems taking advantage of the experience gathered over the past decade of creating and operating forecast systems at lnec for estuary to ocean domains fortunato et al 2017a wiff is described below 4 2 the water information forecast framework wiff wiff is a python based forecasting framework whose main purpose is to facilitate the creation of forecast systems encouraging reusability and pluggability to accomplish that simple core entities were defined with clearly defined relationships and interactions emphasizing the separation of tasks and scope for each of them this approach facilitates the development of complex systems by separating each component to deal with its specific requirements individually and simultaneously obtaining modular and portable components it takes advantage of the object oriented paradigm support provided by python which allows maximizing reusability and guarantees the persistence of a functionality already in operation this strategy simplifies the addition of new functionalities typically by extending through subclasses core entities already defined the wiff framework has several components or packages in python currently the existing components are simulating forcings processing and a special one called core fig 8 core contains common aspects shared by other components simulating addresses the specific details of running simulations forcings implements mechanisms to retrieve boundary conditions processing offers post processing data routines each component has its own sub components sub packages and modules in python with different structures and philosophies as well as distinct relations with other components the core component defines three common features or core entities present in almost all other entities of the framework identification configuration and logging these entities provide the most fundamental and common functionalities required by most other entities besides the core entities there is also a utilities helper providing common standalone functions such as text and date manipulation the forcings component gathers the results from global or regional numerical model services made available through the internet these global regional outputs are used as inputs at our model boundaries for the daily prediction period there are three types of entities in this component sources targets and providers each of them with a different set of tasks and capabilities by combining one of each type of these entities a complete solution for boundary conditions can be assembled source entities handle the data format of forcing services forcing data must be presented as a numpy array a common feature in python scientific data libraries like the netcdf case provider entities must be able to abstract the differences between the services supplying forcings and to offer a generic and consistent interface for fetching them currently cmems motu http marine copernicus eu faq what are the motu and python requirements from copernicus nwp dcpc http dcpc nwp meteo fr openwis user portal srv en main home from météofrance and ncep nomads https nomads ncep noaa gov from noaa are available in opencoasts lastly target entities deal with the specific details of the output format and produce valid input files by complying with the constraints and requirements imposed by the chosen numerical model two targets are implemented openboundary and sflux for model schism s ocean and atmospheric forcings respectively the simulating component implements all functionalities related to the execution of models including all sub components needed to create a forecast system being the main component of wiff it is split into sub components namely core sim mixins and schism the core sim sub component comprises four generic entities named series simulation step and context these entities are agnostic and do not perform any operation concerning a specific simulation model series builds a chain of simulations with a defined interval or a set of scenarios with different conditions simulation is responsible for the execution of its corresponding steps step defines an interface to implement concrete procedures being specified later for a specific model and context monitors and stores the state of execution being updated through the steps of a simulation although series simulation and context entities may be used directly step is just a skeleton thus its implementation must be extended to serve a particular model the mixins sub component is also a special kind of entity intended to be coupled with other entities to perform common tasks like core s base entities a mixin entity provides behavior to be shared among other entities and cannot be executed individually an example of the functionalities provided by this entity is copying files from one place to another the schism sub component encompasses everything specific about the schism numerical model from steps dealing with the simulation procedure to parameterization helpers defining systematic ways to produce the required input files all schism s steps extend the step entity from simulating s core sim sub component fig 9 illustrates the application of wiff to support schism predictions 4 3 opencoasts as a wiff application wiff is at the heart of the opencoasts service since it handles all aspects of preparing and running forecast simulations following wiff s philosophy of extending core behavior opencoasts defines new simulating steps to perform the functionality not provided already by wiff the additional steps implement specific procedures devoted only to opencoasts therefore they are not included in wiff to maintain its generic applicability the procedures range from reporting the state of each simulation throughout its execution to publishing the results and information produced during the model runs opencoasts and wiff are connected at the database level through the opencoasts steps which retrieve and store the required data during the simulation execution these steps use the django object relational mapping layer orm to generalize the interaction with a specific database solution it can be used to interact with application data from various relational databases such as postgresql or mysql this approach makes the solution more flexible for different database choices and easier to develop since database systems have native network support splitting components of opencoasts workflow across different machines in a network becomes effortless 5 implementation in eosc the opencoasts platform has been deployed in the incd cloud infrastructure one of the infrastructure provider within eosc https www eosc portal eu to provide the opencoasts service incd is based on the openstack cloud management framework and is integrated into the egi federated cloud the eosc cloud computing service eosc provides several types of services including computing from egi cloud and grid storage from eudat authentication and authorization infrastructure aai from egi the opencoasts platform is being integrated with some of these services in the framework of eosc hub thematic services presently it takes advantage of both aai and computing services in eosc opencoasts s architecture implementation in eosc is shown in fig 10 its sub services such as the web frontend and backend database ncwms and data management services are deployed in cloud resources computing resources for the opencoasts deployments and corresponding simulations can be provided by farm clusters at incd the egi grid computing infrastructure can also be used through the dirac4egi workload management service the egi checkin service provides federated user authentication for opencoasts users the authorization policies are implemented in the opencoasts backend service in the future permanent storage will be provided by the b2safe eudat service the opencoasts platform sub services are also under deployment in the csic ifca cloud infrastructure also integrated within the egi federated cloud and part of eosc resources this implementation is of major importance to provide scalability of the service as the number of users deployments grows and to provide redundancy to guarantee service operation guarantee of service is fundamental for quality assurance of operational forecast systems 6 examples of application 6 1 tides and storm surges in the tagus estuary the present implementation of opencoasts offers the choice between two alternative atmospheric forcings sources and two alternative sources for ocean boundary conditions winds and atmospheric pressure are provided by the gfs ncep or arpege météo france water elevations at the ocean boundary can be prescribed from either a global tidal model fes2014 carrere et al 2016 or a regional tide surge model prism2017 fortunato et al 2016 2017a at the river boundaries the yearly mean river flows were imposed as they do not affect the water levels inside the estuary vargas et al 2008 when tidal elevations are imposed from fes2014 and atmospheric forcings are prescribed the inverse barometer effect is also included at the boundary to estimate the incoming surge assuming that the surge in the deep ocean is dominated by the atmospheric pressure each model offers different grid resolutions gfs 0 25 arpege 0 1 and extent and represents different processes however it is unclear which options lead to the best results and comparisons are required to provide guidance to the users in order to assess the relative merits of these different forcings to simulate tides and surges a previously calibrated and validated application of schism to the tagus estuary fortunato et al 2017b fig 11 was implemented in opencoasts with different forcings the accuracy of the different implementations was compared in the simulation of the period between december 15 2018 and february 15 2019 three error measures were used in the comparison the bias the unbiased root mean square error urmse and the unbiased root mean square error at high tide uhtrmse unbiased errors were determined by removing the means from both the measurements and the model results before computing the errors unbiased errors are independent from the bias which can be due to both model errors and data errors e g inadequate vertical referencing of the tide gauge errors were computed at two tide gauges cascais and marina fig 11 data at cascais were measured with an acoustic tide gauge until the end of january and with radar in february hourly values were obtained using a butterworth filter to eliminate high frequency oscillations due to seiches the water surface at marina is measured with an optical sensor attached to a structure above the water outliers were eliminated from the time series and then the butterworth filter was again used to determine hourly data also because at low spring tide the spot under the sensor dries out elevations below a given threshold were clipped model results were adjusted to the cascais hydrographic zero by assuming the mean sea level to be 2 29 m above this datum overall water level errors table 1 compare favorably with previous hindcast applications for this estuary e g fortunato et al 2017b rodrigues and fortunato 2017 results show that simulations forced by fes2014 are more biased than those forced by prism2017 this behavior suggests that computing the surge at the model boundary using the inverse barometer effect alone underestimates its value this problem could probably be overcome by increasing the extent of the ocean part of the domain results also show that at cascais fes2014 leads to smaller urmse and uhtrmse than prism2017 these results are probably due to the larger number of tidal constituents in fes2014 since prism2017 is forced by its predecessor fes2012 at the marina station the unbiased errors are smaller for the simulations forced by prism2017 than for those forced by fes2014 possibly due to error compensation finally although arpege has a higher spatial resolution than gfs the accuracy of the simulations forced by arpege is only marginally superior 6 2 impact of domain size on surge simulation the central part of the bay of biscay each forecast relies on an unstructured grid provided by the user however it is not straightforward to define the grid geographic extension that will accurately reproduce the physical processes that are represented in the forecast system this application aims at studying the impact of domain size on total water levels and storm surges we took advantage of the recent storm gabriel which made landfall in the central part of the bay of biscay on january 29 2019 and induced a storm surge with a return period of the order of 5 years at la rochelle according to the estimations of storm surge return periods of hamdi et al 2015 water levels variations during storm gabriel were simulated with two computational grids of different geographic extents the larger grid covers the whole bay of biscay and the smaller one extends from the french coast to depths of about 130 m in the central part of the bay of biscay fig 12 both simulations were forced by 10 m winds and sea level atmospheric pressure originating from the arpege météo france along the open boundary both models were forced with the amplitude and phase of the 34 main tidal constituents originating from fes 2014 and an inverse barometer condition was also prescribed freshwater discharges were not included because previous studies showed that their impact on water levels is negligible at the considered tide gauges bertin et al 2014 as both computational domains have an identical spatial resolution 100 1200 m over regions in common and were run with the same forcings and model parameters one can expect that the differences in model response are mostly due to the different domain size the model results were compared against observations recorded 24 h before and after the storm at three tide gauges la rochelle les sables d olonne and port bloc fig 13 total water levels are well reproduced by both models with a urmse ranging from 0 10 m at les sables d olonne and 0 13 0 14 m at la pallice and port bloc respectively storm surges were computed from observations using a tidal prediction performed with u tide codiga 2011 over one year of water level observations preceding the studied period modeled storm surges were computed as the difference between the baseline run and a run forced with tides only the storm surges from the large grid are reproduced with a urmse ranging from 0 06 to 0 085 m at the three tide gauges the surge peak is quite well captured although with a negative bias ranging from 0 05 m at port bloc to 0 15 m at la pallice the comparison between the results of the two grid configurations reveals that the large grid yields total water levels and storm surges less biased than those obtained with the smaller grid at the three stations storm surges simulated with the large grid have a urmse improved by 20 30 compared to the small grid and the storm peak is better reproduced after the surge peak residual water levels display oscillations with periods of 6h00 and amplitudes ranging from 0 10 m at port bloc and les sables d olonne and 0 20 m at la pallice such oscillations were already described during the storm xynthia february 2010 and explained by bertin et al 2012 through a resonant process of the storm surge over the shelf the comparison between observed and modeled storm surge fig 13 shows that this phenomenon is much better reproduced by the model that uses the large grid compared to the small grid fig 13 this suggests that the whole shelf should be included in the computational domain to capture this resonant process the model results analysis demonstrates good predictive skills of the forecast system however the surge peak remains underestimated by up to 0 15 m previous studies on storm surges in the area reported that the wave setup driven by short wave breaking can propagate outside surf zones and contribute to storm surges by 0 10 0 20 m in the sheltered harbors where the tide gauges are located breilh et al 2014 bertin et al 2015a however these studies employed a spatial resolution of the order of 25 m along the coast which is one order of magnitude finer than the grids used in this study in the near future opencoasts will be updated to a fully coupled wave current system using the spectral wind wave model wwm roland et al 2012 the differences obtained between the two domain sizes show that the effect of the inverse barometer is improved by extending the ocean part of the grid which corroborates the results analysis of the previous application section 6 1 representing the incoming surge along the open boundary through an inverse barometer alone is less accurate when the modeled domain is restricted to the inner shelf blain et al 1994 suggested that a domain small in size compared to the size of the storm and located on the continental shelf can substantially underestimate the storm surge indeed forcing the domain with adequate boundary conditions is difficult because the corresponding ocean boundaries are situated in an area of storm surges generation on the contrary a domain sufficiently extended allows the realistic and accurate reproduction of the resonant modes of the basin and the development and propagation of the storm surge 6 3 discussion the setup of the forecasts and the comparison of the model results with tide gauge data require the knowledge of the vertical datum and its relation to the mean sea level usually tide gauge data and local bathymetries are referred to a local hydrographic zero as is the case for most elevation time series extracted from emodnet in many cases establishing the distance between the hydrographic zero and the mean sea level considered at the boundaries may be difficult this difficulty can be overcome in the model validations by focusing on unbiased errors however for the forecast to provide water elevations relative to a known datum like in the case of the simulation of the storm gabriel the user must provide the correct vertical displacement between the grid datum and the mean sea level this information is not always readily available first sea level rise and local subsidence must be taken into account to determine the present mean sea level e g wöppelmann and marcos 2016 for instance the tagus estuary forecasts presented above used the most recent data from the cascais tide gauge to estimate the present mean sea level in the modeled domain secondly sea level also varies on seasonal scales mostly due to thermo steric effects e g bertin et al 2015b these effects are associated with changes in the water temperature which lead to an expansion or a contraction of the water column at regional scales mean sea level can also vary at seasonal and inter annual timescales due to long term changes in wind forcing which can only be partly captured using short term runs with a 2dh barotropic model indeed calafat et al 2012 showed that the stratification of the ocean upper layers can impact the ekman transport and drive subtle baroclinic effects resulting for instance in coastally trapped waves present forecasts produced by opencoasts neglect thermo steric and baroclinic effects which they cannot reproduce this simplification introduces a seasonal error in the predicted elevations which probably explains part of the negative biases obtained in the tagus and bay of biscay forecasts future implementations of opencoasts will offer ocean boundary forcings that include estimates of thermo steric effects 7 conclusions and perspectives the opencoasts service presented herein builds on demand circulation forecast systems for user selected coastal regions at a global level and maintains them running operationally for the time frame defined by the user taking advantage of a transparent linkage to eosc hub resources and core services this innovative concept and tool can now be explored to address the needs of coastal managers public institutions and private companies with responsibilities in emergency and monitoring purposes across europe and worldwide as opencoasts can support planning activities from daily tasks to strategic interventions being able to reproduce the operational behavior of coastal engineering interventions even before they are implemented the opencoasts service is also a valuable tool for consultancy companies working in the field of coastal engineering to support engineering projects and their conceptualization and implementation this freely available service also facilitates the access to circulation forecasts to research groups with little experience in numerical modeling of oceanic and coastal zones physics which have strong needs in understanding the impact of water dynamics in water quality ecology and sediments dynamics by making the service available for deployment in any world coastal region opencoasts leverages the conditions for any entity to develop their research responsibilities in a faster efficient and high accuracy way users feedback on the present service collected from the training events and the service rating at the platform are very positive therefore the proof of concept attained by this 2dh circulation service is now being extended to more complex coastal physical processes such as 3d baroclinic circulation the interaction between currents and waves in 2dh and 3d and morphodynamics in particular further developments are needed to consider the wave induced contribution to storm surge level at the coast therefore requiring the forecasting of wave dynamics also forcing opencoasts with watershed forecasts e g campuzano et al 2016 can provide accurate estimations of freshwater flows in natural rivers i e without dam control these evolutions will improve the accuracy of the forecasts by including neglected physical processes such as wave induced setup and forcings such as thermo steric effects more importantly it will fully address the most relevant concerns on coastal regions such as coastal erosion inlet stability and safety of navigation and shelter in ports our experience indicates that for many users providing an unstructured grid is the major difficulty in the adoption of the opencoasts service although several grid generators are available e g geuzaine and remacle 2009 conroy et al 2012 roberts et al 2018 they can be difficult to use and the criteria for grid generation are model dependent in order to help users a podcast on grid generation is available through the opencoasts web pages and a public repository of grids for coastal systems was created https github com lnec gti opencoasts grids oliveira et al 2019 the demonstration of the capacity of a forecast service to address the coastal community needs provided herein also opens the door to its extension to other aquatic domains integrating the urban drainage component as well as the freshwater water bodies dynamics sets the stage for a global forecast service linking the water compartments that each user requires over the basin to ocean dimensions is one of the avenues to pursue providing full flexibility and coverage of water physical processes once this global provider for physical processes is available targeted water quality goals can also be analyzed and implemented bearing in mind that a strong articulation with field data through in situ and remote data assimilation and multiple water quality models becomes essential for reliable water predictions computational resources in particular for complex process interactions across multiple domains also becomes a concern and strategies for high efficiency are required linking cpu and gpu paradigms as well as distributed and hpc computing acknowledgements this work was partially funded by the european commission through the h2020 project eosc hub grant agreement no 777536 by the lisboa2020 operational program through the incd project lisboa 01 0145 feder 022153 and by the fundação portuguesa para a ciência e a tecnologia through project ubest ptdc aag maa 6899 2014 this work made use of results produced with the support of the portuguese national grid initiative more information in https wiki ncg ingrid pt data from the marina station were obtained with a senz2 sensor in the scope of the agreement between this company and lnec the authors thank sonia castanedo and fernando méndez from the university of cantabria for a thorough review of the manuscript as well as the editor and two anonymous reviewers for constructive suggestions 
26080,coastal forecast systems are used for many purposes including harbor management search and rescue operations and response to extreme events however the generation and operation of these systems is time consuming requires expertise in both information technologies and modeling of coastal processes and needs dedicated computational power the new service opencoasts overcomes these difficulties by generating on demand coastal circulation forecast systems through a web platform with minimal user intervention using a web platform the user is guided through seven simple steps to generate an operational forecast system for any coastal region the only requirements are an unstructured grid of the study area and information on river flow if applicable the platform provides ocean and atmospheric forcings and data for model validation and includes interfaces for results visualization and forecasts management forecasts are generated with the community model schism and computing resources are provided through the european open science cloud graphical abstract image 1 keywords schism forecast systems unstructured grids web platform eosc software availability program title opencoasts developers joão rogeiro joana teixeira contact address jrogeiro lnec pt software access https opencoasts ncg ingrid pt year first available 2018 software required browser firefox google chrome program language python html css javascript availability and cost open access upon registration email required 1 introduction in the late 20th and early 21st centuries the oceanographic community started developing forecast systems to provide short term predictions of ocean and coastal hydrodynamics e g clancy and sadler 1992 brassington et al 2007 baptista et al 2008 mehra and rivin 2010 these forecast systems use hydrodynamic models forced by atmospheric model results to provide among other variables sea surface elevations currents and wave spectra a few days in advance more recently forecast systems were extended to oil spills e g sotillo et al 2008 oliveira et al 2014 abascal et al 2017 and biochemical models e g triantafyllou et al 2007 marta almeida et al 2012 initially fuelled by military needs these forecast systems now have a variety of uses such as search and rescue operations breivik and allen 2008 warning of extreme events fortunato et al 2017a ferrarin et al 2019 and bathing water quality viegas et al 2009 bedri et al 2014 oliveira et al 2015 as the quality of the predictions increases due to more accurate atmospheric forecasts data assimilation improved models and higher computational power ocean forecast systems are increasingly being adopted by a variety of end users such as coastal managers harbor authorities civil protection agencies and the surfing and sailing communities presently the development and operation of ocean forecast systems require significant resources large teams with expertise in both numerical modeling of ocean processes and information technologies and powerful computational resources such as computer clusters also the daily maintenance of these systems can be very time consuming since they can crash for a variety of reasons numerical instabilities of the model network communication problems power shortages hardware failures lack of forcing conditions due to crashes in forcing forecast systems etc as a result the development and operation of forecast systems have mostly been limited to large research groups from universities and research centers in some countries operational organizations such as noaa in the usa provide predictions freely for their national coasts however most countries lack the availability of coastal predictions for coastal management tasks coastal centered economic activities or even recreation opportunities for the population in particular at the required spatial resolution clearly many organizations could benefit from these forecast systems but do not have the resources required to develop and maintain them there is therefore a clear need to develop a service that can empower potential users to develop and maintain their own forecast systems with little effort and cost by significantly reducing the cost and time required to develop coastal forecast systems as similar software as a service technological platforms swain et al 2016 golding et al 2019 have achieved an on demand service available through a web platform is expected to promote a drastic increase in the number of existing forecast systems and their uptake by various end users and the society in general web portals have been recognized as an excellent solution for sharing and managing spatial data jiang et al 2019 saah et al 2019 the opencoasts service aims at filling this need supported by european open science cloud eosc resources this service assembles on demand circulation forecast systems for user defined coastal areas and keeps them running operationally it generates daily forecasts of water levels and depth averaged velocities over the region of interest for 48 h based on numerical simulations of all relevant physical processes forcing conditions at the boundaries and over the domain are defined by the user from global and regional forecast databases automatic comparison with real time in situ sensor data can be provided for a number of user specified locations taking advantage of the european marine observation and data network emodnet http www emodnet eu what emodnet the present paper describes opencoasts and illustrates its application at estuarine and shelf scales this service is in operation in the portuguese and spanish infrastructures for distributing computing in the framework of the european open science cloud eosc initiative cloud resources are increasingly popular in environmental modeling due to their flexibility to provide resources on request and to the simplicity of their use chandrasekar et al 2012 gentzsch 2014 gomes et al 2018 issues related to model performance remain a concern for environmental applications given the need for infrastructure abstraction and multi tenancy occupation o donncha et al 2016 in spite of these challenges cloud resources are the backbone of several complex modeling and data based applications providing computational power to many application fields e g glenis et al 2013 qiao et al 2019 phuong et al 2019 in the context of environmental forecasting additional concerns for the use of cloud computing include communication between nodes for parallel computations o donncha et al 2016 security of data and quality of service in particular to guarantee the delivery time of the forecasts rogeiro et al 2018 in the field of coastal hydrodynamic and water quality modelling a comparison of model performance indicators for an operational forecast simulation executed in local workstations a hpc cluster and a pilot cloud revealed the good performance of cloud computing resources rogeiro et al 2018 because the size of the computational jobs is unknown a priori opencoasts poses new challenges to cloud computing resource allocation thus calling for simple and robust solutions to deliver predictions efficiently and on time this paper is organized as follows section 2 provides an overview of opencoasts this service is then described in detail in sections 3 frontend and 4 backend section 5 presents the implementation of opencoasts in the european open science cloud eosc the application of the service and its platform is then illustrated in section 6 finally section 7 provides an overview of the experience gained in the development of opencoasts and its worldwide application and discusses its evolution in the near future 2 concept and general description 2 1 opencoasts requirements and main properties a forecast system is typically based on a local implementation of the model of choice duly validated and calibrated for local conditions forcings for this forecast are based on available providers of local boundary conditions and the user interface is built to address the targeted end users requirements this approach provides tailored results but lacks the generic methodology and high flexibility behind a service initial deployments of the water information forecast framework wiff are examples of this legacy approach e g oliveira et al 2015 fortunato et al 2017a more sophisticated approaches provide higher flexibility to the users werner et al 2013 by making several models available and allowing for some customization according to user s needs however all these approaches are based on a local implementation concept and do not automatically provide the capacity to build forecasts on demand over a web interface nor the integrated connection to the necessary computational resources as a result building and maintaining these systems in operation require considerable human resources and a continuous demand for supporting computational resources the coastal forecast service paradigm proposed herein through the opencoasts service provides fully flexible deployment customization from the user to the backend perspective forecast services are generic platforms that can be applied by users of any discipline to produce customized applications to specific sites they are often used in the atmospheric sciences domain taking advantage of the availability of global forecast system results kim et al 2018 golding et al 2019 unlike forecast systems coastal forecast services are site independent applicable to any site through customization of its platform hide the complexity of computational resources allocation from users and optimize resource usage depending on the characteristics of the application site opencoasts goes one step further from existing forecast systems and provides a forecast as a service solution similarly to software as a service saas where users are empowered to build their own forecast systems on demand this platform is therefore a forecast building service supported by a site independent model and computational infrastructure that can be used to generate site specific forecast systems on demand by the users based on the outcomes of similar services for other domains swain et al 2016 golding et al 2019 coastal forecast services should thus allow for smooth implementations to distinct domains choices of parameters and forcings and on the fly output production and visualization saah et al 2019 from a user perspective for forecast administration roles services provide simple adaptation to different models and model versions and expedite interoperability among distinct computational resources providers moreover all parts in the forecast engine are modular and can easily be used to address repetitive tasks in an efficient and quality controlled fashion building a framework to provide a forecast service of broad application in the coastal community requires the identification of the necessary requirements from both user and administration perspectives the requirements analysis for the development of the opencoasts service identified several properties that need to be addressed the list is provided below along with a short explanation on how these were addressed at the implementation stage 1 broadly available the service should be available through a broad access platform without requiring additional configuration or software installation the opencoasts service is provided through a web application usable on multiple devices 2 simplicity and usability the service should be accessible to all coastal community members and provide a straightforward experience to them opencoasts allows both inexperienced and experienced users to apply state of the art models to develop forecasts systems for their domain of interest while benefiting in a transparent way from the computational infrastructure provided through the h2020 eosc hub project a single item is required to use the service a computational grid of the domain of interest 3 comprehensive the service should provide access to all forecast related tasks through a single platform minimizing dependencies on external software for full uptake of the service opencoasts was built as a one stop shop tool for users providing forecast configuration management and output visualization facilities in a single platform from an administration perspective integrating the django web framework within the platform allows taking advantage of its automatic administration interface that provides simple and straightforward management of users all tasks on deployments and all their properties as well as accounting and quality control tasks 4 accurate and reliable forecast quality and persistence depend directly on the accuracy and robustness of the model being used for the predictions and on the quality of the deployment input conditions opencoasts is based on schism zhang et al 2016 an open source unstructured grid high performance modeling suite supported by a strong international community 5 flexible a service should provide some degree of freedom for users to select the conditions that are deemed more adequate for their system of interest opencoasts allows for the selection of the domain of simulation at the required spatial discretization of the forcings amongst broad use options and of some model configurations through this flexible approach it is also adapted to users knowledge by providing pre defined selections for less experienced users 6 modular forecast services need to be comprehensive in terms of processes and spatial coverage for full support to all entities in the water cycle the modularization of the development parts constitutes a core property to support growth and adaptation to the best available components opencoasts was built in a modular way to provide support to grow from the current two dimensional 2dh barotropic physics to all processes in the water cycle including the urban dimension simultaneously it was prepared to accommodate new models forcing options and the ability to quickly replicate an existing deployment to allow for small changes in model parameters the development of an unopinionated framework defining simple entities with clear ways of interaction and a well planned data model are the ingredients to achieve this property 7 verifiable forecast systems are based on modeling tools and their applications their validity should therefore be automatically checked against field data preferably from multiple sources to prevent misleading conclusions based on data errors in opencoasts the nearest data stations available at emodnet are automatically proposed to the user providing a systematic procedure to evaluate prediction quality against one of the major data providers an operational forecast service with worldwide coverage such as opencoasts requires the availability of considerable reliable and replicated computational resources to guarantee the delivery of the forecasts outputs to the users in due time the simulations in the opencoasts service are performed with the model schism a model based on mpi that can take advantage of several connected nodes indeed a comparison of model performance indicators for an operational forecast simulation with schism executed in local workstations a hpc cluster and a pilot cloud revealed the good performance of schism in cloud computing environments rogeiro et al 2018 for resource size allocation opencoasts defines several resource allocation classes each with different amounts of computational resources the classes are defined based on the available computational resources each of the operating deployment daily simulations is associated to one of the predefined resources size classes depending on the horizontal grid size this approach makes for an efficient and fair of use of resources while keeping the distribution of resources among the simulations fairly simple and robust in the scope of the portugal s national infrastructure for distributing computing incd infrastructure roadmap project in portugal and the h2020 eosc hub project in europe framed by the european open science cloud initiative implementation opencoasts simulation resources are provided at the incd and ifca instituto de física de cantabria spain facilities in a transparent way to the user generic access to other computing centers is being planned through the integration of the core eosc hub services for authentication accounting computation and data preservation 2 2 the overall architecture and components the opencoasts service architecture fig 1 includes 1 the apps tier a frontend that comprises the user interaction components for forecast systems configuration management and visualization via a web application 2 the services tier a backend where models and mapping services run and 3 the storage tier for preservation the services and storage tiers are briefly presented below 2 2 1 web platform this component provides access to the service through web pages hosting the configuration wizard the forecast manager and the viewer applications apps each of these apps allows users to interact with the different facilities of the service the django web framework is used as the development framework of this component which follows django s design philosophy of having a project composed of applications each with a set of concerns and functionalities 2 2 2 forecasting while the web application allows the users to interact with the opencoasts service and manage their operational real time forecast systems the forecasting component is responsible for producing the forecast results for each deployment since it is a central part of opencoasts it interacts with all other components directly or indirectly to be able to gather all the necessary information to run each deployment simulation and make the forecast results available through the web the opencoasts service is based on wiff water information forecast framework fortunato et al 2017a a framework that includes the building blocks required to assemble custom forecast systems easily these blocks cover different aspects of a forecast system the top block forecast includes a simulation block which in turn is comprised by steps detailed descriptions of each entity can be found in section 4 2 a step encapsulates self contained tasks allowing them to be re used in different simulations the definition of a simulation includes the list of steps to be executed following a specific order the modularity requirement is fulfilled by having the possibility of reusing any of the steps within different simulations for instance changing from schism to a different circulation model in opencoasts would only require replacing the step wrapping the model execution by a new one for the new model assuming both models are compatible e g use the same inputs and configurations 2 2 3 mapping the mapping services complement the web application ones by providing web map services wms which are then used by the viewer component of the web application the forecasting models output files are netcdf files composed of unstructured mesh data and considering most commonly used web map servers such as geoserver and mapserver are limited to serving vector and structured grid data ncwms2 software blower et al 2013 is used to serve unstructured grids on the web 2 2 4 storage the storage component keeps the state of all services and is a requirement for all of them the storage technologies range from typical relational databases servers to lower level shared file systems the relational database software used is the postgresql plus the postgis extension storing most structured information about the service the storage service currently provided by nextcloud makes input and output files available to the other opencoasts components shared file systems in this case nfs are used to share folders and files among the computation and mapping host resources the following sections present in detail the two main components of the opencoasts platform 3 frontend the opencoasts web application https opencoasts ncg ingrid pt has three main components the configuration assistant guides users through several steps to assemble circulation forecast systems for coastal areas of their choice the forecast systems manager provides information about the user s systems status and tools to edit them and the outputs viewer provides access to the model s output files and tools to view the results in a web map and to compare them with pre selected observation stations data in the form of charts along with these main components the web application includes a user profile page and a rating feature that enables users to provide feedback on their experience and therefore contribute to improving the service the opencoasts web application life cycle includes the development of new functionalities in the near future this section describes the main components listed above in the currently available version 3 1 the configuration assistant this component guides the user over the seven steps described below through the creation of a new forecast system at the user s region of interest the only mandatory requirement from the user is a 2d triangular computational grid which includes the bathymetry as an input file other inputs such as a spatially variable friction coefficient can also be provided by the user but are not mandatory in step 1 model the user selects the model version to use in the forecasts and the forecast period the current version of the opencoasts service offers a single model and period the schism v5 4 0 model with a 48 h forecast period in step 2 domain the user uploads a triangular 2d unstructured grid indicating its coordinate reference system from a provided list or by entering an epsg code and the vertical datum if the uploaded file is successfully validated the user can preview the grid s boundaries and bathymetry on a map for visual inspection fig 2 the third step boundaries allows users to specify the type of each open boundary ocean or river and the forcing to be used at each of them for ocean boundaries the current version of the service includes the following forcings prism2017 portuguese regional tide surge model forecasts 2017 fortunato et al 2016 2017a developed to forecast water levels along the portuguese coast prism2017 simulates tides and storm surges in ne atlantic using the schism model in 2dh barotropic mode the model is forced by the global tidal model fes2012 tidal potential atmospheric pressure and winds from the atmospheric model gfs the resolution of the grid along the portuguese coast is about 250 m fes2014 finite element solution 2014 version https www aviso altimetry fr en data products auxiliary products global tide fes description fes2014 html carrere et al 2016 the latest version of the fes tide model developed in 2014 2016 relative to its predecessor fes2012 this version takes advantage of longer altimeter time series and better altimeter standards improved modeling and data assimilation techniques a more accurate ocean bathymetry and a refined mesh in most shallow water regions for river boundaries the current version of the service requires the user to provide one average annual flow value or 12 monthly average flow values in addition to the open boundary forcings the user can also specify an atmospheric forcing in this step fig 3 two choices are available gfs global forecast system noaa ncep https www ncdc noaa gov data access is a weather forecast model produced by the national centers for environmental prediction ncep that covers the entire globe at a base horizontal resolution of 0 25 and a time step of 1 h arpege europe and atlantic météo france https donneespubliques meteofrance fr fond produit id produit 130 id rubrique 51 courtier et al 1991 is a regional atmospheric forecast model provided by météo france that covers the european continent and seas longitudes 32 to 32 latitudes 32 to 60 at a base horizontal resolution of 0 1 and a time step of 1 h step 4 stations allows users to define locations within the simulation grid where to extract model time series with the output model resolution virtual stations it also allows to compare them with observation time series provided by the emodnet web platform comparison stations the observation stations located inside the simulation grid are automatically suggested to the user as comparison stations step 5 parameters generates the file containing the physical and numerical parameters fig 4 the user can choose between a preconfigured parameter file proposed by the application or can manually customize some of the parameters in this version only a few parameters can be edited such as the time step or the initial ramp up period along with the simulation grid and the parameter file schism requires additional data that must be defined in step 6 additional data in the current version of opencoasts only the manning coefficient can be specified by the user this coefficient is used by schism in 2dh mode to determine bottom friction based on local characteristics the user can either specify a constant value or upload a file with the spatial variation of this parameter the seventh and final step submission allows the user to submit the request at this point the configuration assistant presents a summary of all the previous steps allowing the user to review previous choices the user can then submit the configuration or save it for future submission submitting the configuration will create and activate a new forecast system and launch a new deployment in the backend component of the opencoasts platform presently simulations are running on the incd and expire after a month of operation thus promoting the rational use of incd resources integration with ifca resources is underway the first model results are normally available within 24 h after submitting a new deployment the application redirects the user to the forecast systems page described below 3 2 the forecast systems manager the forecast systems manager fig 5 enables users to monitor and manage their forecast systems this component of the web application allows users to consult basic information from the systems such as the model and version used the deployment name and description the reference dates creation start end and last run dates and the system s state which can be one of the following at step n configured active expired or disabled when a system s configuration is submitted the service changes its status to active indicating that the deployment is set up to run at the backend if errors occur during a deployment s simulation run an error label is added to the system s state in such cases the user can access the deployment log files to analyze and deduce probable causes of error also when an active system approaches its expiration date an expiring label is added to the system s state and depending on the number of days left users can request an extension of the deployment after a system reaches its expiration date the service updates its state to expired and the deployment at the backend will stop running the model and producing results at any time users can pause an active system changing the state to disabled and temporarily stopping the deployment this feature allows users to manage which systems are up and running a useful feature considering that the opencoasts service restricts each user to a limited number of active systems users can re activate them at any time the possibility of launching several simultaneous deployments for the same coastal system can be used for model calibration and sensitivity analyses in general the forecast systems manager also includes tools to open a system in the configuration assistant if not submitted yet or still under configuration delete systems preview systems configuration summary or print them as a report and clone systems this last operation creates a new system based on another allowing users to change its configuration and re submit it under new conditions 3 3 the outputs viewer the outputs viewer component fig 6 provides an interface to visualize and access the forecast results and log files this interface allows the user to extract and download time series at user defined locations using standard formats download specified model outputs in standard formats visualize model outputs of water levels and velocity and access automatic data model comparisons at selected data stations the main goal designing this interface was to allow flexibility in the presentation of results users can preview a system s model outputs on a map and on charts stations time series and combine them with the outputs of other systems the interface is divided into three main panels the systems map and charts panels the systems panel lists all of the user s active systems as sub panels that contain the tools to preview and access their outputs in more detail the system panel contains three tabs the maps tab lists map outputs that can be added to the map as layers the stations tab shows the stations defined during the system s configuration stage that can be added to the charts and the files tab lists the daily model output files available for download the map panel renders model outputs as layers on the map from wms services created by the mapping component of the opencoasts platform the map displays a toolbar that contains a set of common tools e g zoom in zoom out toggle basemaps and customized tools e g add location bookmarks tool and a probing station tool along with the ability to view the layer color scheme toggle the layer opacity and drag and drop layers to change their order on the map from a legend panel the model output wms are served by ncwms software that supports time dimension therefore the map panel contains an additional toolbar that allows users to slide the map layer steps in time and a play pause button to view them in an animated fashion the charts panel presents the model time series at stations added from the system s stations tab or added on the fly using the probing tool from the map panel for comparison stations defined at the configuration stage the interface also tries to load observation time series from the emodnet platform if data are available for the same time period they are added to the charts for model validation finally the map and charts panels are synchronized so as users change the time reference on one panel the other one is automatically updated in the future developments the opencoasts service will allow users to share their systems with other users giving them permission to consult them 4 backend the opencoasts backend i e the forecasting engine constitutes the core operational component of this service it is built in modules to promote interoperability and reuse for other applications in particular to support forecasting of different water dynamics or the use of alternative simulation models the next sections present the forecasting engine its supporting framework wiff and its application within the service opencoasts 4 1 forecasting engine this component generates the forecast results by handling all aspects of the simulation chain established for each forecast fig 7 this chain is updated daily by producing predictions from new model simulations covering future time intervals starting from the conditions of the previous day hot start from an information perspective each simulation run has two types of requirements the permanent ones are supplied by the user during the setup and remain unchanged throughout the lifetime of the forecast and the transient requirements which correspond to boundary forcings are updated daily for each simulation of a particular system forecast a model simulation begins by gathering the information supplied by the user and using it as the foundation for the remaining procedure then the forcings are retrieved covering the whole daily prediction interval for all boundaries afterward parameter files are updated for the daily conditions after completing all the previous steps everything is in place to start the simulation execution which may potentially be a very time consuming operation this execution is performed by taking advantage of the parallel computing option in the model schism since the previous step produces output results at each subdomain resulting from the parallel model execution the output files for each variable must be combined into a single file before making these results available in the map server all the steps described previously are encapsulated within the wiff developed specifically to make the creation of forecast deployments as systematic and generic as possible wiff was developed to address the limitations of custom made similar purposed forecast systems taking advantage of the experience gathered over the past decade of creating and operating forecast systems at lnec for estuary to ocean domains fortunato et al 2017a wiff is described below 4 2 the water information forecast framework wiff wiff is a python based forecasting framework whose main purpose is to facilitate the creation of forecast systems encouraging reusability and pluggability to accomplish that simple core entities were defined with clearly defined relationships and interactions emphasizing the separation of tasks and scope for each of them this approach facilitates the development of complex systems by separating each component to deal with its specific requirements individually and simultaneously obtaining modular and portable components it takes advantage of the object oriented paradigm support provided by python which allows maximizing reusability and guarantees the persistence of a functionality already in operation this strategy simplifies the addition of new functionalities typically by extending through subclasses core entities already defined the wiff framework has several components or packages in python currently the existing components are simulating forcings processing and a special one called core fig 8 core contains common aspects shared by other components simulating addresses the specific details of running simulations forcings implements mechanisms to retrieve boundary conditions processing offers post processing data routines each component has its own sub components sub packages and modules in python with different structures and philosophies as well as distinct relations with other components the core component defines three common features or core entities present in almost all other entities of the framework identification configuration and logging these entities provide the most fundamental and common functionalities required by most other entities besides the core entities there is also a utilities helper providing common standalone functions such as text and date manipulation the forcings component gathers the results from global or regional numerical model services made available through the internet these global regional outputs are used as inputs at our model boundaries for the daily prediction period there are three types of entities in this component sources targets and providers each of them with a different set of tasks and capabilities by combining one of each type of these entities a complete solution for boundary conditions can be assembled source entities handle the data format of forcing services forcing data must be presented as a numpy array a common feature in python scientific data libraries like the netcdf case provider entities must be able to abstract the differences between the services supplying forcings and to offer a generic and consistent interface for fetching them currently cmems motu http marine copernicus eu faq what are the motu and python requirements from copernicus nwp dcpc http dcpc nwp meteo fr openwis user portal srv en main home from météofrance and ncep nomads https nomads ncep noaa gov from noaa are available in opencoasts lastly target entities deal with the specific details of the output format and produce valid input files by complying with the constraints and requirements imposed by the chosen numerical model two targets are implemented openboundary and sflux for model schism s ocean and atmospheric forcings respectively the simulating component implements all functionalities related to the execution of models including all sub components needed to create a forecast system being the main component of wiff it is split into sub components namely core sim mixins and schism the core sim sub component comprises four generic entities named series simulation step and context these entities are agnostic and do not perform any operation concerning a specific simulation model series builds a chain of simulations with a defined interval or a set of scenarios with different conditions simulation is responsible for the execution of its corresponding steps step defines an interface to implement concrete procedures being specified later for a specific model and context monitors and stores the state of execution being updated through the steps of a simulation although series simulation and context entities may be used directly step is just a skeleton thus its implementation must be extended to serve a particular model the mixins sub component is also a special kind of entity intended to be coupled with other entities to perform common tasks like core s base entities a mixin entity provides behavior to be shared among other entities and cannot be executed individually an example of the functionalities provided by this entity is copying files from one place to another the schism sub component encompasses everything specific about the schism numerical model from steps dealing with the simulation procedure to parameterization helpers defining systematic ways to produce the required input files all schism s steps extend the step entity from simulating s core sim sub component fig 9 illustrates the application of wiff to support schism predictions 4 3 opencoasts as a wiff application wiff is at the heart of the opencoasts service since it handles all aspects of preparing and running forecast simulations following wiff s philosophy of extending core behavior opencoasts defines new simulating steps to perform the functionality not provided already by wiff the additional steps implement specific procedures devoted only to opencoasts therefore they are not included in wiff to maintain its generic applicability the procedures range from reporting the state of each simulation throughout its execution to publishing the results and information produced during the model runs opencoasts and wiff are connected at the database level through the opencoasts steps which retrieve and store the required data during the simulation execution these steps use the django object relational mapping layer orm to generalize the interaction with a specific database solution it can be used to interact with application data from various relational databases such as postgresql or mysql this approach makes the solution more flexible for different database choices and easier to develop since database systems have native network support splitting components of opencoasts workflow across different machines in a network becomes effortless 5 implementation in eosc the opencoasts platform has been deployed in the incd cloud infrastructure one of the infrastructure provider within eosc https www eosc portal eu to provide the opencoasts service incd is based on the openstack cloud management framework and is integrated into the egi federated cloud the eosc cloud computing service eosc provides several types of services including computing from egi cloud and grid storage from eudat authentication and authorization infrastructure aai from egi the opencoasts platform is being integrated with some of these services in the framework of eosc hub thematic services presently it takes advantage of both aai and computing services in eosc opencoasts s architecture implementation in eosc is shown in fig 10 its sub services such as the web frontend and backend database ncwms and data management services are deployed in cloud resources computing resources for the opencoasts deployments and corresponding simulations can be provided by farm clusters at incd the egi grid computing infrastructure can also be used through the dirac4egi workload management service the egi checkin service provides federated user authentication for opencoasts users the authorization policies are implemented in the opencoasts backend service in the future permanent storage will be provided by the b2safe eudat service the opencoasts platform sub services are also under deployment in the csic ifca cloud infrastructure also integrated within the egi federated cloud and part of eosc resources this implementation is of major importance to provide scalability of the service as the number of users deployments grows and to provide redundancy to guarantee service operation guarantee of service is fundamental for quality assurance of operational forecast systems 6 examples of application 6 1 tides and storm surges in the tagus estuary the present implementation of opencoasts offers the choice between two alternative atmospheric forcings sources and two alternative sources for ocean boundary conditions winds and atmospheric pressure are provided by the gfs ncep or arpege météo france water elevations at the ocean boundary can be prescribed from either a global tidal model fes2014 carrere et al 2016 or a regional tide surge model prism2017 fortunato et al 2016 2017a at the river boundaries the yearly mean river flows were imposed as they do not affect the water levels inside the estuary vargas et al 2008 when tidal elevations are imposed from fes2014 and atmospheric forcings are prescribed the inverse barometer effect is also included at the boundary to estimate the incoming surge assuming that the surge in the deep ocean is dominated by the atmospheric pressure each model offers different grid resolutions gfs 0 25 arpege 0 1 and extent and represents different processes however it is unclear which options lead to the best results and comparisons are required to provide guidance to the users in order to assess the relative merits of these different forcings to simulate tides and surges a previously calibrated and validated application of schism to the tagus estuary fortunato et al 2017b fig 11 was implemented in opencoasts with different forcings the accuracy of the different implementations was compared in the simulation of the period between december 15 2018 and february 15 2019 three error measures were used in the comparison the bias the unbiased root mean square error urmse and the unbiased root mean square error at high tide uhtrmse unbiased errors were determined by removing the means from both the measurements and the model results before computing the errors unbiased errors are independent from the bias which can be due to both model errors and data errors e g inadequate vertical referencing of the tide gauge errors were computed at two tide gauges cascais and marina fig 11 data at cascais were measured with an acoustic tide gauge until the end of january and with radar in february hourly values were obtained using a butterworth filter to eliminate high frequency oscillations due to seiches the water surface at marina is measured with an optical sensor attached to a structure above the water outliers were eliminated from the time series and then the butterworth filter was again used to determine hourly data also because at low spring tide the spot under the sensor dries out elevations below a given threshold were clipped model results were adjusted to the cascais hydrographic zero by assuming the mean sea level to be 2 29 m above this datum overall water level errors table 1 compare favorably with previous hindcast applications for this estuary e g fortunato et al 2017b rodrigues and fortunato 2017 results show that simulations forced by fes2014 are more biased than those forced by prism2017 this behavior suggests that computing the surge at the model boundary using the inverse barometer effect alone underestimates its value this problem could probably be overcome by increasing the extent of the ocean part of the domain results also show that at cascais fes2014 leads to smaller urmse and uhtrmse than prism2017 these results are probably due to the larger number of tidal constituents in fes2014 since prism2017 is forced by its predecessor fes2012 at the marina station the unbiased errors are smaller for the simulations forced by prism2017 than for those forced by fes2014 possibly due to error compensation finally although arpege has a higher spatial resolution than gfs the accuracy of the simulations forced by arpege is only marginally superior 6 2 impact of domain size on surge simulation the central part of the bay of biscay each forecast relies on an unstructured grid provided by the user however it is not straightforward to define the grid geographic extension that will accurately reproduce the physical processes that are represented in the forecast system this application aims at studying the impact of domain size on total water levels and storm surges we took advantage of the recent storm gabriel which made landfall in the central part of the bay of biscay on january 29 2019 and induced a storm surge with a return period of the order of 5 years at la rochelle according to the estimations of storm surge return periods of hamdi et al 2015 water levels variations during storm gabriel were simulated with two computational grids of different geographic extents the larger grid covers the whole bay of biscay and the smaller one extends from the french coast to depths of about 130 m in the central part of the bay of biscay fig 12 both simulations were forced by 10 m winds and sea level atmospheric pressure originating from the arpege météo france along the open boundary both models were forced with the amplitude and phase of the 34 main tidal constituents originating from fes 2014 and an inverse barometer condition was also prescribed freshwater discharges were not included because previous studies showed that their impact on water levels is negligible at the considered tide gauges bertin et al 2014 as both computational domains have an identical spatial resolution 100 1200 m over regions in common and were run with the same forcings and model parameters one can expect that the differences in model response are mostly due to the different domain size the model results were compared against observations recorded 24 h before and after the storm at three tide gauges la rochelle les sables d olonne and port bloc fig 13 total water levels are well reproduced by both models with a urmse ranging from 0 10 m at les sables d olonne and 0 13 0 14 m at la pallice and port bloc respectively storm surges were computed from observations using a tidal prediction performed with u tide codiga 2011 over one year of water level observations preceding the studied period modeled storm surges were computed as the difference between the baseline run and a run forced with tides only the storm surges from the large grid are reproduced with a urmse ranging from 0 06 to 0 085 m at the three tide gauges the surge peak is quite well captured although with a negative bias ranging from 0 05 m at port bloc to 0 15 m at la pallice the comparison between the results of the two grid configurations reveals that the large grid yields total water levels and storm surges less biased than those obtained with the smaller grid at the three stations storm surges simulated with the large grid have a urmse improved by 20 30 compared to the small grid and the storm peak is better reproduced after the surge peak residual water levels display oscillations with periods of 6h00 and amplitudes ranging from 0 10 m at port bloc and les sables d olonne and 0 20 m at la pallice such oscillations were already described during the storm xynthia february 2010 and explained by bertin et al 2012 through a resonant process of the storm surge over the shelf the comparison between observed and modeled storm surge fig 13 shows that this phenomenon is much better reproduced by the model that uses the large grid compared to the small grid fig 13 this suggests that the whole shelf should be included in the computational domain to capture this resonant process the model results analysis demonstrates good predictive skills of the forecast system however the surge peak remains underestimated by up to 0 15 m previous studies on storm surges in the area reported that the wave setup driven by short wave breaking can propagate outside surf zones and contribute to storm surges by 0 10 0 20 m in the sheltered harbors where the tide gauges are located breilh et al 2014 bertin et al 2015a however these studies employed a spatial resolution of the order of 25 m along the coast which is one order of magnitude finer than the grids used in this study in the near future opencoasts will be updated to a fully coupled wave current system using the spectral wind wave model wwm roland et al 2012 the differences obtained between the two domain sizes show that the effect of the inverse barometer is improved by extending the ocean part of the grid which corroborates the results analysis of the previous application section 6 1 representing the incoming surge along the open boundary through an inverse barometer alone is less accurate when the modeled domain is restricted to the inner shelf blain et al 1994 suggested that a domain small in size compared to the size of the storm and located on the continental shelf can substantially underestimate the storm surge indeed forcing the domain with adequate boundary conditions is difficult because the corresponding ocean boundaries are situated in an area of storm surges generation on the contrary a domain sufficiently extended allows the realistic and accurate reproduction of the resonant modes of the basin and the development and propagation of the storm surge 6 3 discussion the setup of the forecasts and the comparison of the model results with tide gauge data require the knowledge of the vertical datum and its relation to the mean sea level usually tide gauge data and local bathymetries are referred to a local hydrographic zero as is the case for most elevation time series extracted from emodnet in many cases establishing the distance between the hydrographic zero and the mean sea level considered at the boundaries may be difficult this difficulty can be overcome in the model validations by focusing on unbiased errors however for the forecast to provide water elevations relative to a known datum like in the case of the simulation of the storm gabriel the user must provide the correct vertical displacement between the grid datum and the mean sea level this information is not always readily available first sea level rise and local subsidence must be taken into account to determine the present mean sea level e g wöppelmann and marcos 2016 for instance the tagus estuary forecasts presented above used the most recent data from the cascais tide gauge to estimate the present mean sea level in the modeled domain secondly sea level also varies on seasonal scales mostly due to thermo steric effects e g bertin et al 2015b these effects are associated with changes in the water temperature which lead to an expansion or a contraction of the water column at regional scales mean sea level can also vary at seasonal and inter annual timescales due to long term changes in wind forcing which can only be partly captured using short term runs with a 2dh barotropic model indeed calafat et al 2012 showed that the stratification of the ocean upper layers can impact the ekman transport and drive subtle baroclinic effects resulting for instance in coastally trapped waves present forecasts produced by opencoasts neglect thermo steric and baroclinic effects which they cannot reproduce this simplification introduces a seasonal error in the predicted elevations which probably explains part of the negative biases obtained in the tagus and bay of biscay forecasts future implementations of opencoasts will offer ocean boundary forcings that include estimates of thermo steric effects 7 conclusions and perspectives the opencoasts service presented herein builds on demand circulation forecast systems for user selected coastal regions at a global level and maintains them running operationally for the time frame defined by the user taking advantage of a transparent linkage to eosc hub resources and core services this innovative concept and tool can now be explored to address the needs of coastal managers public institutions and private companies with responsibilities in emergency and monitoring purposes across europe and worldwide as opencoasts can support planning activities from daily tasks to strategic interventions being able to reproduce the operational behavior of coastal engineering interventions even before they are implemented the opencoasts service is also a valuable tool for consultancy companies working in the field of coastal engineering to support engineering projects and their conceptualization and implementation this freely available service also facilitates the access to circulation forecasts to research groups with little experience in numerical modeling of oceanic and coastal zones physics which have strong needs in understanding the impact of water dynamics in water quality ecology and sediments dynamics by making the service available for deployment in any world coastal region opencoasts leverages the conditions for any entity to develop their research responsibilities in a faster efficient and high accuracy way users feedback on the present service collected from the training events and the service rating at the platform are very positive therefore the proof of concept attained by this 2dh circulation service is now being extended to more complex coastal physical processes such as 3d baroclinic circulation the interaction between currents and waves in 2dh and 3d and morphodynamics in particular further developments are needed to consider the wave induced contribution to storm surge level at the coast therefore requiring the forecasting of wave dynamics also forcing opencoasts with watershed forecasts e g campuzano et al 2016 can provide accurate estimations of freshwater flows in natural rivers i e without dam control these evolutions will improve the accuracy of the forecasts by including neglected physical processes such as wave induced setup and forcings such as thermo steric effects more importantly it will fully address the most relevant concerns on coastal regions such as coastal erosion inlet stability and safety of navigation and shelter in ports our experience indicates that for many users providing an unstructured grid is the major difficulty in the adoption of the opencoasts service although several grid generators are available e g geuzaine and remacle 2009 conroy et al 2012 roberts et al 2018 they can be difficult to use and the criteria for grid generation are model dependent in order to help users a podcast on grid generation is available through the opencoasts web pages and a public repository of grids for coastal systems was created https github com lnec gti opencoasts grids oliveira et al 2019 the demonstration of the capacity of a forecast service to address the coastal community needs provided herein also opens the door to its extension to other aquatic domains integrating the urban drainage component as well as the freshwater water bodies dynamics sets the stage for a global forecast service linking the water compartments that each user requires over the basin to ocean dimensions is one of the avenues to pursue providing full flexibility and coverage of water physical processes once this global provider for physical processes is available targeted water quality goals can also be analyzed and implemented bearing in mind that a strong articulation with field data through in situ and remote data assimilation and multiple water quality models becomes essential for reliable water predictions computational resources in particular for complex process interactions across multiple domains also becomes a concern and strategies for high efficiency are required linking cpu and gpu paradigms as well as distributed and hpc computing acknowledgements this work was partially funded by the european commission through the h2020 project eosc hub grant agreement no 777536 by the lisboa2020 operational program through the incd project lisboa 01 0145 feder 022153 and by the fundação portuguesa para a ciência e a tecnologia through project ubest ptdc aag maa 6899 2014 this work made use of results produced with the support of the portuguese national grid initiative more information in https wiki ncg ingrid pt data from the marina station were obtained with a senz2 sensor in the scope of the agreement between this company and lnec the authors thank sonia castanedo and fernando méndez from the university of cantabria for a thorough review of the manuscript as well as the editor and two anonymous reviewers for constructive suggestions 
26081,earth observation eo systems play a significant role in environmental monitoring and the prediction of natural disasters these systems generate a massive amount of heterogeneous data stored in different formats the exploitation of this data is still limited while in most cases data are not linked and sources are not interoperable hence data cannot be exploited as an interoperable global knowledge graph to have more in depth analyzes of environmental phenomena ontology as a knowledge representation formalism is a promising solution for the semantic interoperability between this data in this work we present a modular ontology for environmental monitoring developed based on an original agile methodology the so called memon modular environmental monitoring ontology aims to support semantic interoperability data integration and linking of heterogeneous data collected through a variety of observation techniques and systems we also present real use case studies to show the usefulness of the proposed ontology keywords modular ontology environmental monitoring earth observation semantic interoperability data integration knowledge graph 1 introduction in recent years the earth has undergone rapid climate changes which are believed to have in increasing natural disasters such as storms floods and hurricanes these disasters have dramatically influenced not only the natural environment but also human life consequently research communities have given great importance to the development and the implementation of earth observation eo systems such as sensors and satellite platforms 1 1 https sentinels copernicus eu web sentinel missions sentinel 1 and eo programs such as the copernicus 2 2 http copernicus eu program and servir 3 3 https www servirglobal net global along with the increased number of monitoring solutions a multitude of heterogeneous environmental data is generated this data includes hundreds of millions of climate data ocean and coast data 4 4 https oceanservice noaa gov observations monitoring land data and more stored in different formats databases csv files raster images etc this volume of data keeps growing regarding semantic heterogeneity synonymy polysemy etc for example in the sentence maps of daily temperature and precipitation are produced an expert would recognize that the observation is temperature but could not determine the details related to the temperature concept atmospheric temperature sea surface temperature etc he needs to ask the data provider to get more details additionally in different disciplines the same term may correspond to various meanings in one hand for example the term environment is defined as the biological and abiotic elements surrounding an individual organism in the biological domain however it refers to all the natural components of the earth air water oils etc sauvé et al 2016 on the other hand various terms may correspond to the same meaning for instance the observatory of sahara and sahel 5 5 http www oss online org oss may use the word rainfall for the same real world feature that usually refers to precipitation in other sources the variety of terms complicates the work of emergency responders who should be familiar with the terms used in each discipline with this extensive variety of environmental data it is becoming increasingly difficult for domain experts to understand natural phenomena and reduce the adverse effects of climate changes the exploitation of eo data is still limited due to the silos between data sources systems and programs each source offers data or models to be discovered or reused both data and semantic models encode domain knowledge that resides with the experts however this knowledge is often not available and data analysts need to establish contact with original data sources and model producers to understand and use them properly for example the oss provides data about climate change such as precipitation and temperature the national oceanic and atmospheric administration noaa 6 6 https www noaa gov offers marine data and the emergency events database em dat 7 7 https www emdat be provides data about flood events unfortunately these organizations don t collaborate to link the produced data even if it is available online this data is kept as isolated silos undeniably we have not reached a level where data and models are interoperable and linked so that the experts can reuse them soundly we are still far away from the vision of common environmental information space athanasiadis 2015 our purpose is to break down with those silos to provide what we call a global data view where different eo systems and programs will have unhampered and uniform access to the available environmental data that will be linked and synthesized into a single knowledge graph this global data view allows the data sources to speak the same language and to share information so that domain experts could transform information into actionable knowledge we refer here to a knowledge graph ehrlinger and wöß 2016 which is defined as a multi relational graph composed of entities and relationships between them with this knowledge graph experts can look at all of this data and try to find meaning out of its correlations to understand natural phenomena and make the right decisions about disaster risk preventions the hurricane irma which occurred across the caribbean in 2017 serves as an example of how the lack of a global view of the environmental knowledge and the absence of linked observed data hinder the anticipation and the understanding of natural phenomena the traditional conditions of a hurricane development such as the sea level the wind speed and the atmospheric pressure were monitored as usual by the noaa national hurricane center besides the african sahara desert was observed by nasa however there was no link between these observations indeed the absence of the dry air resulting from the lack of saharan dust across the atlantic noaa 2014 acted in favor of high altitude winds when these waves of the air have enough moisture lift and instability they readily form clusters of thunderstorms and a tropical cyclone was formed as the areas of disturbed weather moving westward across the atlantic resulting in the creation of the hurricane irma if experts had the link beforehand and understood the phenomena better they might have been able to predict the power of the disaster a little bit before and alert the governments a global data view is further challenged by data integration data integration is the process of combining data retrieved from multiple and independent sources to provide an integrated and interoperable structure lenzerini 2002 the main challenges confronted by data integration are data linking and semantic interoperability to deal with these issues many studies applied the ontology based approach in the field of environmental monitoring lv and el gohary 2016 stasch et al 2014 however many problems are encountered when using these ontologies in software development for various reasons first some existing ontologies are dealing with specific aspects of the environment monitoring boughannam et al 2013 ma et al 2014 they cannot be used in other contexts so the coverage to annotate or link observed data is most cases not possible with these undoubtedly little attention was given to cover all environmental monitoring disciplines simultaneously which cannot ensure a global view of knowledge second many ontologies are built without considering their reusability zhang et al 2016 dahleh and fox 2016 finally while there have been several fundamental ideas regarding the application of ontologies to environmental monitoring e g curry et al 2013 devaraju et al 2015 there was limited take up by broader communities only a few applied attempts have been put into practice corominas et al 2018 to deal with these issues a semantic oriented platform named predicat predict natural catastrophes that aims at providing data interoperability and linking in eo and disaster prediction was proposed in masmoudi et al 2018 predicat aims to 1 ensure uniform access to heterogeneous data by providing adequate services 2 integrate eo data coming from several sources including that provided by citizens and 3 provide a decision support solution to analyze in real time all the data to effectively prevent and react against natural disasters fig 1 presents the global architecture of the predicat platform and its tiers in this paper we only address the semantic layer by proposing a modular environmental monitoring ontology memon developed based on an original agile methodology this ontology will not only provide a common vocabulary of the domain but also facilitate semantic linking of data from different sources through a knowledge graph the rest of this paper is organized as follows in section 2 we provide an overview of the current work related to ontologies that can be used to semantically model sensors observations and environmental monitoring domains in section 3 we describe the approach of memon ontology development in section 4 we present and discuss the evaluation of the approach through real case studies section 5 is devoted to discuss the proposed approach and present some concluding remarks as well as research perspectives 2 background state of the art and motivations in this section we briefly introduce the ontology classification based on the domain scope then we give an overview of sensor observation and measurement ontologies followed by related work on environmental monitoring ontologies we also underline the motivation for our proposal 2 1 ontology classification ontologies have been widely used for knowledge representation as they provide a shared vocabulary for modeling a specific domain by capturing knowledge in a structured and formal way gruber defined ontology as an explicit and formal specification of a shared conceptualization gruber 1993 besides several classifications were presented in the literature to differentiate ontology categories fig 2 illustrates the classification based on domain scope as given in falquet et al 2011 indeed foundational upper or top level ontologies are generic ontologies that can be viewed as meta ontologies describing the high level concepts or universals used to define other ontologies they guarantee interoperability between domain ontologies sharing the same upper ontology noy 2004 and facilitate the integration and knowledge reuse the most well known foundational ontologies are the suggested upper merged ontology sumo pease 2006 the descriptive ontology for linguistic and cognitive engineering dolce masolo et al 2002 and the basic formal ontology bfo arp et al 2015 core reference or mid level ontologies are often built to capture the fundamental concepts of a domain although this type of ontology is linked to a domain it integrates different viewpoints related to a specific group of users in contrast domain ontologies are only applicable to a domain with a particular point of view they describe the vocabulary related to a specific area of knowledge such as medicine environment etc specializing the terms introduced in the higher ontology finally application ontologies are specializations of domain ontologies they describe the concepts based on a particular context of the domain our work can be situated in domain ontology level 2 2 state of the art of ontologies in this section we aim at identifying ontologies that can be used to semantically model sensors section 2 2 1 and the information available from such systems as a result from the observation of the ambient environment section 2 2 2 a review of some existing environmental ontologies and their comparison according to the aspects of the knowledge acquisition method the domain the principle of reusing existing ontologies and the building purpose is presented in section 2 2 3 2 2 1 state of the art of sensor ontologies first initiatives for modeling sensor and sensor networks were driven by the open geospatial consortium ogc sensor web enablement swe botts and robin 2007 the ogc provides a set of xml schemas and open standards that enable the discovery access and processing of sensor observations the ogc sensorml is one of these schemas it describes sensors systems and processes it also provides the information needed for the discovery of sensors and the location of sensor observations sensorml was used to support the establishment of the ogc s sensor observation service sos providing access to observations from sensors and sensor systems the widely used sensor ontology is the w3c semantic sensor network ssn ontology compton et al 2012 the ssn ontology has been recently updated haller et al 2018 by including a lightweight core module called sosa sensor observation sampler and actuator this new version of ssn is a joint w3c and ogc standard specifying the semantics of sensors observations observable properties actuation and sampling it describes sensors regarding measurement processes observations and deployments however it does not include concepts about the geospatial either temporal dimension 2 2 2 state of the art of observation and measurement ontologies research on observation and measurement context was driven by different organisms the observations measurements o m standard of the ogc swe which describes a conceptual model and xml encoding for measurements and observations is an example aligned to the sensorml the o m establishes a high level framework for representing observations measurements procedures and metadata of sensor systems this standard is required by the sos for the implementation of the swe enabled architectures the semsos o m owl ontology henson et al 2009 is a semantic data model to manage sensor data based on the o m standard the significant concepts modeled in the semsos ontology are observation an act of observing a property or phenomenon to produce an estimate of the value of the property feature an abstraction of a real world phenomenon property associated with a feature that can be sensed or measured process the method system or algorithm used to generate the result such as a sensor result data an estimate of the value of some property location the location of an observation event and time the time when the phenomenon was measured in the real world the significant properties include the feature of interest observed property sampling time observation location result and procedure the extensible observation ontology oboe is a generic ontology dealing with observations data and measurement madin et al 2007 while it s a generic the concepts of this ontology should be extended to clarify the inherent meaning of scientific observations the core classes of the oboe ontology include observation measurement entity characteristic and measurement standard e g physical units and six properties labeled has context of entity has measurement has value has precision and uses standard nasa s semantic web for earth and environment terminology sweet raskin and pan 2005 is a mid level ontology for earth system science it consists of nine upper level concepts representation realm phenomena process human activities matter property state and relation that can be used as a foundation for domain specific ontologies that extend these upper level sweet components 2 2 3 state of the art of environmental monitoring ontologies several works have attempted to build domain and application ontologies for environmental monitoring in the monitor project kollartis et al 2009 kollartis defined an ontology to formalize the knowledge necessary for monitoring methods and environmental risk management monitor ontology is based on dolce upper level ontology semsorgrid4env 8 8 http linkeddata4 dia fi upm es ssg4env index php ontologies default htm is another project which uses ontology as support for sensor data integration two ontologies based on real use cases were built in this project the first one is a fire risk monitoring ontology and the second one is a coastal and estuarine flood ontology the two ontologies are mainly developed reusing the ssn ontology to represent sensor concepts and their observed information and ontologies from the sweet suite 9 9 https sweet jpl nasa gov to describe services and datasets geographic and administrative region data are covered by the ordnance survey 10 10 https www ordnancesurvey co uk ontologies law and environment ontology leo is proposed by informea 11 11 https www informea org fr to provide a semantic standard for data information and knowledge in the field of environmental law and governance it is more a taxonomy since it gives an overview of relationships between multilateral environmental agreements meas as well as concepts definitions and synonyms found in these conventions however it is not written in any ontology format it only contains information displayed as maps info graphics and text leo terms are divided into different sections such as air climate land and water sections these sections would be useful in the exploration phase of our work zhang et al proposed a meteorological disaster ontology mdo to describe the components and relationships between the different parts of the meteorological disaster system zhang et al 2016 mdo describes only meteorological disaster knowledge it does not include other disaster types like hydrological and geophysical disasters mdo is not published in a computable format but classes and relations are defined and could be useful for memon construction based on the water data transfer format wdtf schema shu et al defined a wdtf ontology using concepts and roles that describe water observation data shu et al 2016 wdtf documents were translated into ontology instances to build an application ontology that presents a semantic solution for wdtf data validation in that work shu focused only on data encoded in wdtf qui et al proposed an ontology based approach that links environmental models with disaster related data to support flood disaster management qiu et al 2017 they defined a model ontology and a data ontology by considering disaster related semantics and described the relationships of models and data with a multi level semantic mapping method oliva felipe et al proposed the waste water ontology wawo the evolution of the original wawo ontology oliva felipe et al 2017 this ontology aims to increase efficiency in data and knowledge interoperability and data integration among heterogeneous environmental data sources e g software agents in the scope of urban water resources management within a river basin however none of the ontologies mentioned above use a common upper level ontology nor reuse other domain or existing ontologies they have been developed from scratch each in its ad hoc way in such wise that they create a lack of interoperability with other ontologies in related domains in contrast the reuse of other domain ontologies was the subject of other works examples include boughannam et al 2013 who discussed the advantages of semantic technologies in implementing smarter industry solutions for monitoring analytics they proposed an ontology for the domain of managing observations and measurements based on various ontologies such as the quantity unit dimension type qudt ontology hondgson et al 2014 the ssn ontology and the geosparql ontology 12 12 http www opengeospatial org standards geosparql the adopted solution illustrates how the concepts of these ontologies are implemented using a real world use case from the environment analytics domain the environmental analytics domain ontology proposed by the authors is dedicated to monitoring the environment around their deployed platforms and assigned to a specific use case which is the oil and gas monitoring besides the ontology does not use an upper level ontology nor a modularization approach thus we could not use this ontology for our purpose however the semantic reference models could be used in our solution ma et al developed a global change information system ontology named gcis to represent the content structure of the recent national climate assessment draft report nca3 and its associated provenance information ma et al 2014 since this ontology was built to analyze use cases surrounding nca3 it is dedicated to specific contexts for building provenance aware data services however these two works do not use upper level ontology this concern was adopted by other works in the domain of environmental monitoring such as the work of buttigieg buttigieg et al 2016 which proposed the environment ontology envo aligned to bfo it delineates the environment domain as a whole and also includes other fields such as biomedicine ecology food habitats and socioeconomic development envo classes which include environmental features environmental materials environmental processes and environmental dispositions will be reused to develop the memon ontology dahleh and fox created an ontology for the representation of environmental indicators such as ozone concentration and noise pollution dahleh and fox 2016 they built three domain ontologies to represent pollution sensor and species information based on the global city indicators gci foundation ontology fox 2013 and other reused ontologies such as the ssn ontology this global ontology is specific to environmental indicators representation and it does not represent additional monitoring information such as events and environmental processes many studies have applied an ontology based approach to their emergency management applications llaves and kuhn developed an event abstraction eabs ontology to model events inferred from observations and an application ontology named flood monitoring ontology llaves and kuhn 2014 eabs ontology is kept generic enough to be reused for other applications extends the ssn ontology and allows only inferring information from observed data the ontology eabs does not perform reasoning on events since the event patterns are keeping out of the ontology instead of including them as ontology rules devaraju et al developed a sensing geographic occurrence ontology sego which represents relations between geographic events and sensor observations and an application ontology for blizzard disaster devaraju et al 2015 sego does not provide information about the environmental domain but instead it is developed as a starting point for the construction of application ontologies to infer geographic events from sensor observations sego is based on dolce foundational ontology and is centered around the sensing domain therefore devaraju et al distinguish stimulus from environmental processes to emphasize the process that actuates a sensor to produce observations however in our work we consider a stimulus as an environmental process eabs and sego used dolce as an upper level ontology in table 1 we present a comparison of the different approaches mentioned above of using ontologies to solve semantic heterogeneity in environmental monitoring domain the comparison is based on the following criteria building purposes domains methods for knowledge acquisition and links to other ontologies 2 3 synthesis and motivations despite these numerous works several limitations can be noted first many studies focused only on the knowledge expression of specific environmental observations type such as in situ or on specific requirements for a particular organization second some existing ontologies are dealing with specific aspects of the environment so the coverage to annotate or link observed data is most cases not possible with these the environment is lying on multidisciplinary fields such as meteorology hydrology geology geography and so on existing ontologies do not cover all environment fields such as the ontology mdo neither the observation conditions contexts such as the ontology envo which does not include the sensing context nor the human hazard inducing factors nor the spatiotemporal context third little attention was paid to the semantic relationships between different environmental components such as disasters observations in time and space and other correlated environmental conditions that can provide a comprehensive view to analyze environmental factors to address this issue some ontologies from specific case studies however they cannot be reused in other disciplines finally each monitoring system generally uses a specific reasoning method on specific data sources and formats to generate inferences which makes it difficult or even impossible to obtain inferences across heterogeneous data sets besides we can deduce that there are ontologies that include useful concepts for our proposed ontology such as wdtf ontology sego etc however their concepts also exist in other ontologies like the envo ontology and the ssn ontology which we prefer to use since they contain more concepts apt to be reused in our ontology our motivation is to address these listed gaps by building a modular environmental monitoring ontology memon to link and integrate observed data from various sources the ontology memon aims at 1 ensuring semantic interoperability between heterogeneous data sources 2 supporting knowledge discovery and generation 3 integrating and linking data across various disciplines and 4 providing a global data view 3 memon modular environmental monitoring ontology this section presents the methodology to build the proposed modular ontology for the environmental monitoring field building a modular ontology is not a straightforward task especially when ontologies become more significant and more complex fernandez lopez and corcho 2004 therefore a methodology that guides and manages the modular ontology development is necessary but one of the thorniest problems is how to choose the appropriate methodology taking account the complexity of the domain to be modeled and the furthest evolution and reuse to support ontologies building several methodologies such as methontology neon ontoclean have been discussed in karray et al 2012 in this work we adopt the aom agile methodology for developing ontology modules methodology gobin 2013 this choice is argued by the fact that an agile methodology will enable the incremental and iterative development of memon s modules whereas other methodologies are suited to develop ontologies in one go aom stipulates the sequence of four phases exploration planning module development and finally release phases we adjusted these phases according to our needs and contexts as shown in fig 3 the development life cycle starts with the exploration phase in which we identified the ontology requirements and objectives then we defined a set of competency questions cqs according to the studied domain and experts needs the cqs consist of a set of questions stated in natural language so that the ontology must be able to answer them correctly grüninger and fox 1994 given this set of cqs we extract stories to represent the different contexts that limit our domain the planning phase indeed stories are scenarios and contexts from which facts can be obtained which will be used to build the ontology these stories were written based on the information gathered during interview sessions with domain experts in the module development phase the cqs related to one module are used to design a semi formal module then the modules are formalized implemented and evaluated to be merged in the release phase the next sections detail these four phases according to the adapted aom methodology 3 1 exploration phase the ontology development process starts with the exploration phase whereby the domain expert is part of the development team and with the knowledge engineer both embark on the development process to have a better overview and understanding of the domain of environmental monitoring we refer to the oss our project partner which have significant expertise in environmental monitoring natural resource management and climate change during this phase the ontology engineers and the oss experts sat together and discussed the domain of interest the objectives and the requirements of the ontology the objective of the memon ontology is to provide a formal representation of the domain of environmental monitoring to support 1 semantic interoperability 2 knowledge discovery and generation 3 data integration and linking and 4 a global data view to understanding environmental phenomena better the memon is developed to encapsulate the knowledge contained in multiple environmental data sources the key requirements of the memon ontology are listed as follows the ontology captures the notions related to the domain of environmental monitoring the ontology applies the principles of modularization ontology modularization is the process of defining a module which is a subset of the global ontology doran 2009 the modularity of the ontology contributes to improve the ontology development process by reducing the complexity of designing maintaining enriching and replacing modules maintain the clarity and the coherence of the ontology by presenting ontology modules with needed knowledge promote the reuse of each module separately according to zimmermann and le duc 2008 an ontology module can be considered as a loosely coupled and self contained component of an ontology maintaining relationships with other ontology modules in the scope of this work an ontology module represents a point of view covered by an environmental domain context such as spatial context or observation context admittedly terms in an environmental domain are linked together but each term can be used in a specific context for instance the sensor class has a strong link with observation related terms but should belong to the most suitable module which is the sensor sensing module the ontology is aligned with an upper level ontology in order to ensure an interoperable reuse and integration of ontological modules elmhadhbi et al 2019 we choose bfo for basic formal ontology bfo as a starting point for our ontology building for two primary reasons firstly our domain of interest the environment field is a realistic domain thus we looked up for an upper level realist ontology that represents environmental entities as they are and not representing environmental concepts and representations existing in the domain experts minds smith and ceusters 2010 secondly the ontologies envo and cco reused in our ontology are some of those ontologies that have been developed by adopting bfo as the foundation of the ontology development these factors make its use a key enabler in promoting semantic interoperability of these ontologies and the reuse of our ontology by others the ontology reuses classes from mid level ontologies that specialize the classes in the upper level ontology and other domain ontologies 3 2 planning phase during the planning phase competency questions cqs are defined in coordination with the domain expert the oss the domain of interest is analyzed by exploring multiple data sources such as the oss copernicus emdat etc each source has its own set of relevant data in specific disciplines for example the oss generated climatological data data about temperature precipitation etc whereas the emdat database provides data about natural disasters thus to have an overview of the knowledge that can be extracted from the multiple data sources we define cqs which the ontology should be able to answer given this set of cqs stories are extracted to define the perimeter of the domain of study so we identified four main stories which contribute to environmental monitoring fig 4 1 environmental conditions story which includes geological structures rock and soil types geographical features forest land hydrological structures water oceans rivers lakes environmental processes ground shaking natural disasters and so on 2 urbanization story infrastructure 3 observing conditions story which includes sensing systems sensors satellites and so on and observation conditions properties and measurement conditions 4 and spatiotemporal conditions story time and location of environmental activity or observation from these stories we identified eight ontological modules the first story deals with environmental materials environmental processes and natural disasters regardless of the discipline hydrology geology meteorology etc so we identify them as three modules the second story results in the infrastructure module from the third story which handles the observations their measurements and systems that produce them we identify two modules the observation measurement module and sensor sensing module the final story is about the time and space conditions of events or observations knowing the importance of these two contexts we identify the temporal module that includes temporal entities and the geospatial module that covers entities about locations location country etc and environmental features such as forests since one of the memon requirements is the reuse of existing ontologies and following the style of existing geospatial ontologies the module geospatial ontology results from a combination of the first and the last stories table 2 the relationships between modules will be mapped into the ontology using axioms we can for example ensure that environmental processes are associated with a spatiotemporal context through time r and spatial r relations time r represents timing relations e g occurs on occurs before occurs after etc of different disastrous weather climate events and between them spatial r represents spatial relations e g location neighborhood etc of events occurring the disasters also can be related to the environmental processes through causal relationships in the next phase each story is transformed into cqs with the support of the domain experts for example for an environmental process story some cqs obtained are shown in table 3 3 3 module development phase the proposed ontology consists of a set of modules covering the disciplines of environmental monitoring domain memon consists of eight main modules namely observation measurement module sensor sensing module natural disaster module environmental process module environmental material module infrastructure module temporal module and geospatial module these modules are developed and released through eight iterations and increments the iterative process starts with the definition of the cqs that the chosen module should answer the cqs are discussed to define a semi formal model which is given through a concept map created using the ihmc cmaptools computer program 13 13 https cmap ihmc us fig 7 to fig 14 each semi formal model includes concepts and relationships related to these cqs by referring to existing ontologies and standards later the formal model is developed using the web ontology language owl2 14 14 http www w3 org tr owl ref finally the module is evaluated section 3 3 4 we used a hybrid approach based on a top down alignment to an open source top level ontology noy and mcguinness 2001 and a bottom up focus on classes that are grounded in environmental data sources each module was constructed by reusing the upper level ontology bfo the mid level ontologies the common core ontologies cco cubrc 2017 and lower level ontologies this latter represents specialized domains having classes that are either direct subclasses of the upper or the mid level ontologies such as the envo ontology or imported and classified according to bfo and cco such as the ssn ontology ontology reuse has several advantages first it is no longer necessary to build the ontology from scratch second it increases the quality of new ontologies because the reused components have already been tested finally ontology reuse improves the efficiency of ontology maintenance ding et al 2007 3 3 1 bfo top level ontology bfo basic formal ontology is a top level ontology which has been applied in various domains such as biomedicine and military it is also in the process of becoming an iso standard iso 21838 2 bfo provides neutral and general classes under which the domain s universals can be inserted as a top level ontology it assists in organizing data from different domain ontologies in a way that promotes the semantic interoperability degree of systems using these ontologies the two main categories of bfo classes are continuant which are entities that continue to exist over time such as objects and locations forests water etc and occurrent which are event entities processes and temporal regions such as rain earthquakes etc fig 5 and table 4 summarize the key categories encountered in bfo accordingly we choose bfo as a starting point for our ontology building for two primary reasons firstly our domain of interest the environment field is a realistic domain thus we looked up for an upper level realist ontology that represents environmental entities as they are and not representing environmental concepts and representations existing in the domain experts minds smith and ceusters 2010 secondly the ontologies envo and cco reused in our ontology are some of those ontologies that have been developed by adopting bfo as the foundation of the ontology development these factors make its use a key enabler in promoting semantic interoperability of these ontologies and the reuse of our ontology by others 3 3 2 cco mid level ontologies the common core ontologies cco form a collection of mid level ontologies in owl that extends bfo schoening et al 2015 it consists of ten interrelated modules allowing the development of interoperable ontologies the ten ontologies provide a starting set of general commonly used terms involving agents actions artifacts and measurements the ontologies in the cco include agent ontology representing agents especially persons and organizations and their roles artifact ontology ao representing deliberately created material entities along with their models specifications and functions currency unit ontology representing currencies in different countries extended relation ontology representing relations i e object properties holding between entities extended relation ontology is imported in memon since it contains several object properties such as has quality occurs at etc event ontology eo representing processes geospatial ontology geo representing sites spatial regions and other entities especially those that are located near the surface of earth as well as the relations that hold between them information entity ontology ieo representing generic types of information as well as the relationships between information and other entities quality ontology qo representing a range of attributes of entities including qualities realizable entities such as dispositions and roles and process profiles time ontology to representing temporal regions and the relations that hold between them a temporal region as defined by bfo is an occurrent entity that is part of the time as defined relative to some reference frame units of measure ontology umo representing standard units used when measuring various attributes of entities these mid level ontologies are extended for the development of the memon modules as illustrated in fig 6 the cco reuse the relations ontology ro which is a collection of owl2 relations intended to be shared among various ontologies it incorporates a set of upper level relations such as part of and has input these relations are re used in all memon modules 3 3 3 the modular environmental monitoring ontology memon in this section we present the eight modules of memon for each module we describe what classes and relations are reused what classes and relationships are added and how are they inserted under bfo and cco classes fig 7 14 illustrate partial views of the modules in each figure classes are marked with a color and a prefix the prefix marks the source ontology and the namespace used to define the class classes added in memon are marked with a namespace prefix memon classes reused from existing ontologies are marked with different colors to make the concept map easier to read for humans see colors in fig 6 and namespace prefix such as envo bfo etc for more details about modules readers may refer to the memon google site 15 15 https sites google com view predicat memon at the beginning of a module development phase a list of the cqs that the module should be able to respond is defined the cqs shown in tables 5 12 are generic and can be specialized with any given observation or event a sensor and sensing module this module describes sensor systems sensors and how they are related to observations the sensor sensing module should be able to answer the following cqs table 5 as described in section 2 2 2 the reused sensor ontology built upon the ogc standards the sos and the o m standards is the ssn ontology that describes sensors and sensor networks the sensor sensing module was developed based on this ontology besides we reused the artifact ontology ao from the cco which allows the classification of sensor systems as an artifact object and the definition of functions that can be applied to an artifact we started by importing the ssn ontology then we classify its classes under bfo and cco classes for example the class ssn system and its subclasses actuator sample and sensor are classified under ao artifact since they can be defined as an object that was designed by some agent to realize a certain function considering that the class sensor also exists in the cco as a transducer that is designed to detect events or changes in its environment and then provide a corresponding output and to avoid inconsistency of the ontology we add an equivalent relation between ssn sensor and ao sensor following the classification of the ssn ontology we define different kinds of sensors as classes in the memon such as temperature sensor sea surface topography sensor and pressure sensor we also classified them into active sensors which are radar instruments used for measuring signals transmitted by the sensor that were reflected refracted or scattered by the earth s surface or its atmosphere and passive sensors which are microwave instrument designed to receive and to measure natural emissions produced by constituents of the earth s surface and its atmosphere 16 16 https www nasa gov directorates heo scan communications outreach funfacts txt passive active html the ieo ontology of the cco define entities related to the meaning of information artifacts it classifies three kinds of information content entity descriptive entity that consists of a set of propositions that describe some entity designative entity that consists of a set of symbols that denote some entity and directive entity that consists of a set of propositions that prescribe some entity the class ssn output is classified under the class ieo descriptive information content entity and we added as subclasses satellite output and sensor output the ssn deployment is defined as the deployment of one or more systems for a particular purpose the definitions in bfo represent what is general in reality so we defined a deployment as a methodical procedure of introducing an activity process program or system to all applicable areas of an organization and we inserted it as a subclass of ieo directive information content entity a procedure is defined in the ssn ontology as a workflow protocol plan algorithm or computational method specifying how to make an observation that prescribes the inputs and outputs of a system we classify this class under the class ao artifact function specification since this latter defines each directive information content entity that prescribes some artifact function this module includes 83 classes with 1264 axioms two data properties and 38 object properties see fig 7 b observation and measurement module memon is concerned with describing how observations are related to sensors events and measurements thus the observation measurement module should be able to answer the following cqs table 6 the goal of observation might be to measure or otherwise determine the value of a property as the precedent module we import the ssn ontology which defines observation a feature of interest and properties concepts then we classify its classes under bfo and cco classes by following the philosophy of bfo observation is defined in the ssn ontology as an act of observing a property or a phenomenon to estimate or calculate a value of a property of a feature of interest we classify the sosa observation class under the class eo act of the cco sosa recognizes sosa observation about some sosa observable property of a sosa featureofinterest since the class sosa observableproperty subclass of the ssn property class is defined as an observable quality property characteristic of a feature of interest then it is classified under the bfo quality class of bfo whereas the class sosa featureofinterest is classified under ieo descriptive information content entity since it represents the thing whose property is being estimated or calculated in the course of an observation following the classification of the ssn ontology we define different kinds of observable properties as classes in the memon ontology to this end we reused classes from the quality ontology qa from the cco such as qa temperature and qa wetness classes from the envo such as envo temperature of water and envo temperature of air then we created new classes such as precipitation wind speed humidity and sea surface temperature fig 8 shows some examples of observable properties to reduce the generality of the class sosa result we added the class observation result this new class is represented as the equivalent of the class measurement information content entity of the cco defined as a descriptive information content entity that consists of a symbol that is a measure of the extent dimensions quantity or quality of an entity relative to some standard we consider that an observation result can be a nominal measurement such as a color value or an interval measurement such as temperature value the ssn ontology however does not include modeling aspects for units of measurement and domain knowledge that are related to observed properties to model measurement units we reused the unit of measure ontology umo from the cco this ontology extends from the ieo the class ieo measurement unit and provides several subtypes of the measurement unit e g umo measurement unit of speed umo measurement unit of pressure and umo measurement unit of temperature it also provides instances of these classes e g horsepower measurement unit and kilometer measurement unit fig 8 shows a partial view of the observation and measurement module it contains 102 classes with 968 axioms two data properties and 37 object properties c environmental material module the states of environmental material play an important role in environmental monitoring the relation between observations and environmental materials is evident environmental material quality such as water quality or environmental material existence such as fume are observations detected by environmental monitoring techniques and enables to monitor the environment and predict disastrous events before identifying the taxonomy we define the cqs which this module should answer table 7 we reused classes defined under envo environmental material class in envo fig 9 shows some memon classes this module contains 87 classes 802 axioms and six object properties in owl classes can be either primitive or defined primitive classes only have necessary conditions i e superclasses defined classes have necessary and sufficient conditions i e equivalent classes to enrich the environmental material module we used defined classes the definition of the liquid environmental material as being equivalent to environmental material and has quality some quality of liquid is an example of an equivalent class that satisfies necessary and sufficient conditions in other terms if liquid environmental material class is described using only necessary conditions then we can say that if an individual is a member of class liquid environmental material it must satisfy the conditions we cannot say that any random individual that satisfies these conditions must be a member of the class liquid environmental material however if the liquid environmental material class is now defined using necessary and sufficient conditions our case we can say that if an individual is a member of the class liquid environmental material then it must satisfy the conditions also we can say that if any random individual satisfies these conditions then it must be a member of the class liquid environmental material the conditions are not only necessary for membership of liquid environmental material but also sufficient to determine that something satisfying these conditions is a member of liquid environmental material consequently we can use the reasoner to compute automatically a classification hierarchy in this way to classification to ensure the refinement of the individuals classification and allow more semantic precision in the representation of knowledge in order to make the implicit knowledge more explicit equivalent classes are also defined in other memon modules d environmental process module the understanding of environmental phenomena and environmental changes needs a complete and comprehensive representation of the processes which are involved in such changes consequently a set of cqs presented in table 8 has been defined the environmental process module contains 214 classes representing environmental processes and 15 relations have been added to memon the classes were aligned with the bfo process class in bfo since they represent entities that occur and develop in time unfold in successive phases and always depend on some material entity they were based on the knowledge of experts the envo ontology and the unesco thesaurus 17 17 http vocabularies unesco org browser thesaurus and have been used to interlink observations and environmental materials for example a tree burning is related to smoke through has output relation fig 10 following the classification of the envo ontology we define different kinds of environmental processes and classify them into climatological hydrological geographical geophysical and other categories in addition to the different main categories of environmental processes we reused classes from the event ontology eo of the cco to describe changes such as the classes eo decrease of a quality and eo loss of a quality and we extended them with classes such as memon decrease of a temperature memon loss of a mass the relations between the classes of this module are primarily controlled by the relations ontology ro such as has input and occurs during in the aim of better understanding environmental phenomena their factors and especially their order of occurrence we identified the need to add object properties such as preceded by although the strong relationships between environmental processes and disasters the definition of natural disasters was set aside in a separate module this separation will promote its reuse for other applications such as an emergency response application cqs defined for the natural disaster module were summarized in the following table table 9 in envo a disaster is modeled as a subcategory of the class bfo process however to emphasize the difference between natural disasters and environmental processes we classify natural disasters as bfo disposition according to bfo a disposition is a realizable entity in virtue of which a process of a certain kind occurs in the independent continuant bearer in which the disposition inheres this process is called the realization of the disposition for example the disposition of a forest region which undergoes land degradation to desertification thus we classified the desertification disaster under the class bfo disposition and specified the class land degradation as a process we define different kinds of natural disasters in the memon ontology and classify them into climatological geophysical hydrological and meteorological categories we reused classes from the envo ontology such as envo earthquake and envo volcano and created new classes such as memon cyclone and memon hurricane in addition to classifying natural disasters into different categories we also define them regarding how they affect each other by establishing the relationships caused by and followed by and how they occurred as a result of one or many environmental processes by the relation realized in as illustrated in fig 11 fig 11 shows a partial view of natural disaster module classes the entire module contains 80 classes and 782 axioms f infrastructure module several physical objects mentioned in memon could be useful in environmental monitoring infrastructure objects which are deployed in a specific location where particular environmental processes occur could participate in environmental hazard factors for example the dam failure flood in laos 18 18 http www hydroworld com articles 2017 09 nam ao dam in laos burst last week html we defined cqs related to the infrastructure story in the following table to build this module we imported classes from the ao from the cco ao includes transportation infrastructure such as bridges and tunnel for better classification we added the class memon hydraulic infrastructure to incorporate water infrastructures such as ao dam as shown in fig 12 the infrastructure module includes 48 classes and 608 axioms and contains several object properties such as has participant table 10 g geospatial module the challenges in environmental monitoring are motivated by the necessity to take in consideration geospatial information according to beard and neville knowledge of spatial contextual differences among observations is important for interpretation and analyses beard and neville 2014 we addressed the spatial aspect after defining the cqs related to the geospatial discipline as illustrated in table 11 the identification of the cqs core elements classes and their relationships enables the creation of the module vocabulary that consists in defining environmental features locations and relationships among them by importing the geospatial ontology geo of the cco an environmental feature as defined by the geo is a site feature having a relatively stable location in some geospatial region which can be designated by location specific data it contains four categories anthropogenic features which are features relating to or resulting from the influence of human beings on nature geographic features that are natural features marine features which have characteristics of a marine entity and finally habitat which are environment features having qualities which may sustain an organism or a community of organisms referring to the w3c basic geo wgs84 lat long vocabulary 19 19 http www w3 org 2003 01 geo we represent longitude and latitude and other information about spatially located things we distinguish two primary types of properties in this module object properties that link classes to other classes and datatype properties that link classes to data values as an example of object properties we can cite located in as instances of datatype properties we can mention has latitude value and has longitude value fig 13 presents a partial view of the memon geospatial module this module includes 291 classes 2216 axioms and 21 object properties h temporal module in addition to the spatial context the temporal context of observations and events is also essential to understand and monitor environmental phenomena the cqs related to the temporal story are summarized in table 12 several ontologies have been developed to describe temporal information for example swrl temporal ontology 20 20 http swrl stanford edu ontologies built ins 3 3 temporal owl and the w3c s time ontology 21 21 http www w3 org tr owl time to model temporal information we choose the time ontology to from cco while it is based on bfo and interoperable with other modules additionally it offers a set of object properties e g interval during instant is before that can be used to reason with temporal information since the temporal information from different sources is heterogeneous we add some defined classes to represent the different representations of the temporal information for example the class memon date the class memon timestamp that is equivalent to a memon date and contains a to day a to month a to year an to hour a to minute and a to second and the class memon year month that is equivalent to a to year and a to month are added to the module fig 14 presents a partial view of the temporal module classes and properties we added and asserted some object properties as sub properties of corresponding properties in the to such as memon day which is asserted as a sub property of to interval contains memon occurs before and memon occurs after are other examples of object properties added to the ontology to describe timing relations between different environmental processes the entire module contains 60 classes 1184 axioms and 60 object properties 3 3 4 module formalize and release step the building process of the memon modules is supported by the use of protégé 22 22 https protege stanford edu a popular and open source platform that provides users with a set of tools to construct semantic models it greatly facilitates the definition of classes properties restrictions and rules of memon and supports the visualization and manipulation of the ontology memon is expressed in owl2 23 23 http www w3 org tr owl ref for the reuse of existing ontologies such as cco we used the owl import feature of owl2 indeed the import of a specific class from an ontology a to an ontology b is equivalent to the copy of all the declarations of this class including the uri the labels the definition and the relations after the formalization step the developed module was evaluated firstly the consistency of the module was checked through the reasoner pellet which is an owl2 reasoner integrated into the protégé software ontology inconsistency refers to the fact that there are logical incoherencies or modeling problems that arise if the primitives given by the ontology implementation language are not used correctly such as contradictory relations secondly the module was evaluated in comparison with specific criteria various approaches have been proposed in the literature for ontology evaluation targeting several different criteria and metrics gangemi 2005 obrst et al 2007 we used the tool for ontology modularity metrics tomm software which encompasses all the evaluation metrics presented in khan and keet 2016 these metrics are grouped into four categories of criteria the structural criteria are calculated based on the structural and hierarchical properties of the module calculating structural criteria involves evaluating the size i e the number of entities in a module m such as the number of classes object and data properties the relative size represents the size of the module compared to the global ontology o 1 r e l a t i v e s i z e m m o the atomic size is the average size of a group of interdependent axioms in a module an atom in a module is defined as a group of axioms with dependencies between each other 2 a t o m i c s i z e m a x i o m s a t o m the cohesion which refers to the extent to which entities in a module are related to each other 3 c o h e s i o n m e i m e j m s r e i e j m m 1 i f m 1 1 o t h e r w i s e where sr ei ej is the relation function the logical criteria include the correctness and the completeness metrics the correctness evaluates if every axiom that exists in the module also exists in the global ontology 4 c o r r e c t n e s s m t r u e i f a x i o m s m a x i o m s o f a l s e o t h e r w i s e the completeness evaluates if the meaning of every entity in a module is preserved as in the global ontology 5 c o m p l e t e n e s s m t r u e i f i m a x i o m s e n t i t y i m a x i o m s e n t i t y i o f a l s e o t h e r w i s e the relational criteria that deal with the relations and behavior that modules exhibit with each other such as the inter module distance imd in a set of modules which describes the number of modules that have to be considered to relate two entities 6 i m d m e i e j m i m n n m e i e j m i m n m i m n 1 m i m n 1 1 o t h e r w i s e where nm ei ej is the number of modules to consider to relate entities i and j the product of mi mn mi mn 1 represents the number of possible relations between entities in a set of modules mi mn and the richness criteria which include the attribute richness which defines the average number of attributes per class where att is the number of data properties in the module 7 a r m a t t c the inheritance richness which is defined as the number of subclasses h per class c in a module 8 i r m h c and the relationship richness that describes the diversity of relations types in the ontology p is the set of non hierarchical properties 9 i r m p h p according to the metric results applied to memon shown in table 13 we can deduce the following the atomic sizes of the modules indicate that there are on average between 3 23 and 4 79 axioms that are grouped in an atom for the eight modules most of the memon modules contain few attributes as the attribute richness is less than 1 for the majority of the modules mdisaster has the lowest rr and mtemporal has the highest one an ontology that has little value of rr may have only inheritance relationships that is the case of mdisaster since it includes only the set of disasters organized hierarchically consequently it conveys less information than mtemporal which contains a diverse set of relationships e g interval during and has ending instant ir values are comprised between 0 9 and 1 1 which represent high values indeed ontological modules with high ir are called horizontal ontologies since classes have a large number of direct subclasses this indicates that our modules represent a wide range of knowledge with a low level of detail accordingly they are more open to evolving and being specified this evolution corresponds to our objective to enrich the ontology with further information in other development iterations by comparing the cohesion values coh mo we found that the modules and mtemporal mo m menviprocess have the highest cohesion values due to the strong relatedness of different classes of each module for instance the observation and measurement module mo m deals with the classes of observed properties this module models relations between observation events measured properties and measurement units all of these classes and how they are related are the essence of the higher cohesion in the ontological module as a conclusion the result metrics values generated by the tomm software represent valuable measurements to evaluate the ontology modules the main objectives of the evaluation step via metrics are to 1 check the consistency of each module and between modules and the global ontology 2 provide an insight to the ontology users about the granularity of each module if it is not highly completed in term of classes relations they should refine it with more specific classes and relationships according to their reuse context 3 4 release phase of the global ontology at the beginning of the release phase the developed modules are merged after that the global modular ontology is enriched by inter module relations fig 15 provides an overview of the memon revealing the different modules and presenting some inter and intra module relationships each module has at least one relationship with other modules for example the disaster module is linked to the environmental process module by the relation realized in this means that a disaster can be produced when one or many environmental processes occur the environmental process module is linked with the temporal module by the relation occurs on and with the geospatial module by the relationship occurs at these connections seek to identify the time and the location of a process the global ontology includes 1246 classes 417 properties along with 10956 defining axioms logical statements interconnecting and interrelating classes readers may refer to memon google site 24 24 https sites google com view predicat memon for further details some complex relationships cannot be represented by owl2 dl for example owl2 cannot express the relations between individuals referenced by object properties to overcome the expressiveness limits of owl2 we use swrl language 25 25 http www w3 org submission swrl swrl rule is described as antecedent consequent this signifies that if all the conditions in the antecedent are held then all atoms in consequent must also be held there are also built in functions in the swrl syntax that are capable of describing the logical comparison relationship for example a built in greaterthan returns true if the first argument value is more significant than the second value see rule r1 table 14 summarizes some examples of swrl rules specified regarding the memon ontological vocabulary for instance rule r1 automatically classifies an existing rain with a precipitation rate between 16 and 50 mm hour as a very heavy rainfall indeed in our ontology there are several categories of rain process light rainfall moderate rainfall heavy rainfall and very heavy rainfall this classification is due to the precipitation detection rate other rules with the same syntax allow the classification of the other categories of rain at the beginning of a reasoning process we input different rains as instances of the water based rainfall class we still do not know the category of each rain process once the reasoning process is finished the results obtained show that each detected rain has been inserted under the corresponding category of rain hence implicit knowledge is generated from inputted data table 14 after enriching the memon ontology with the relations and rules we check if it contains any inconsistency by using the pellet reasoner finally memon is released so that it can be verified and validated against the requirements and objectives defined at the beginning of the development process 4 memon ontology evaluation in this section we present the memon evaluation step through the verification section 4 1 as well as the validation processes section 4 2 4 1 ontology verification ontology verification is the ontology evaluation which compares the ontology against the ontology specification document ontology requirements and cqs sections 3 1 and 3 2 thus ensuring that the ontology is built correctly suárez figueroa and gómez pérez 2008 in other words it allows us to answer the question are we producing the ontology right to do that we translate some cqs to sparql 26 26 https www w3 org tr rdf sparql query language to query the ontology examples of obtained results are presented in figures 16 18 cq1 what are the environmental processes that can cause a flood disaster envo 01000710 cq2 what are the environmental materials involved in an effusive volcanic eruption cq3 what is the type of sensor used for ground vibration memon 00001122 the answers to these questions verified by the domain experts show that memon has the competency in providing the right information to the questions 4 2 ontology validation use case studies ontology validation is the ontology evaluation that compares the meaning of the ontology definitions against the intended model of the world aiming to conceptualize suárez figueroa and gómez pérez 2008 in other words it allows us to answer the question are we producing the right ontology to validate memon we will focus on two case studies the exploitation and reuse of environmental knowledge the data integration 4 2 1 first use case knowledge exploitation and reuse in this section we present an example to check the possibility to explicit implicit knowledge expressed into memon we instantiate the class hurricane of the memon ontology with the instance irma that occurred in the caribbean in 2017 then we mapped this instance with characteristic instances that describe the irma hurricane such as the value 914 mb for the barometric pressure 285 km h for the wind speed and 5 4 m for the sea level as illustrated in fig 20 after that we used the reasoner pellet to infer implicit knowledge from explicit one finally we interrogate memon with the sparql query below to extract the list of the five category hurricanes memon 00001186 which exists in memon knowledge base thanks to the rule r4 table 14 that automatically classifies an instance of the hurricane class knowing that it occurred with a wind speed greater than 252 km h as a five category hurricane we get irma as a result of this query this information was not explicitly instantiated in the ontology was implicit but through the defined rule the knowledge concerning the category of the hurricane can be inferred from the knowledge base and becomes explicit fig 19 4 2 2 second use case multi source data integration in the second use case we present an example to demonstrate the ability of memon to guarantee semantic interoperability integrate and link observed data from multiple sources this use case is based on a real world example and deals with precipitation data from the oss in raster images format data about storm events from the noaa and data about flood events from the emdat in csv format one of the problems that we were faced with integrating data across sources is that data schemas metadata in the three sources are heterogeneous terms are presented differently in each source for example in emdat the term disaster is used to describe a natural disaster while the term event is used in noaa thus memon is used to resolve this problem and ensure semantic interoperability between heterogeneous data schemas to semantically integrate this multi source data we used the karma web system gupta et al 2012 a data modeling and integration framework karma provides tools to semi automatically build a semantic model of a data source this model makes it possible to rapidly map a set of sources represented in xml csv json structured text files or databases into a domain ontology once the data sources are modeled the models are then converted into a variety of formats including rdf fig 21 illustrates the whole use case process before importing data into karma we transformed the raster images provided from the oss into csv files step 1 in fig 21 to this end we have designed and implemented a semi automatic process that performs this projection using the geospatial data abstraction library gdal warmerdam 2008 then we imported data from the multi source csv files into karma where memon is also imported as the required ontology for the data mapping process step 2 fig 22 shows the snapshot of an example of the data importation in karma karma provides a graphical user interface to let users interactively model the data for each source according to the ontology modeling is the process of specifying how the different metadata items of a data source columns names in a csv file map to classes and relationships in an ontology the data mapping involves two interleaved steps the assignment of classes to data columns names and the specification of the relationships extracted from memon between the classes as illustrated in fig 23 for example we map the metadata item event id to the class envo 01000876 that refers to storm from memon and the metadata item yearmonth to the class memon 00001099 labeled yearmonth in the ontology then we link the two classes with the relation occurs on as illustrated in fig 23 step 3 the output of the mapping process is the semantic models correspondent to data sources csv files in our case once we have modeled the three data sources karma uses these models to convert the data into an rdf data model that describes the global view of data step 4 this rdf model is used to perform sparql queries across the three data sources step 5 we consider two examples of queries that need information for more than one source in doing so we used the sparql query editor integrated into the protégé software following are the two queries 1 what are the natural disasters that occurred between 2016 and 2018 2 what are the amounts of the annual precipitation of the flood events that occurred between 2016 and 2018 query 1 what are the natural disasters that occurred between 2016 and 2018 as can be seen in fig 24 two types of natural disasters floods and storms are extracted the result consists of a combination of data from both the emdat and the noaa sources to obtain information for only one type of natural disasters we change the disastertype of the sparql query to the specific class defined the disaster memon 00001019 or memon 00001020 query 2 what are the amounts of the annual precipitation of the flood events that occurred between 2016 and 2018 as has been mentioned at the beginning of the paper the purpose of the semantic data integration through memon is to provide information from multiple data sources to be able to understand the environmental phenomena the sparql query below asks for the precipitation amounts of the flood disasters occurred between 2016 and 2018 knowing that the precipitation data is imported from oss whereas the data about floods is imported from emdat fig 25 shows the result of this query analyzing the query we can note firstly that the query needs information about both average annual precipitation and flood disasters the result proofs that memon ensures a semantic data integration and linking in a way that allows more effective knowledge retrieval which are the objectives of memon secondly the result of this query illustrates how we can navigate in memon ontological model to extract the most implicit knowledge the query s result is obtained by covered relationships shown in fig 26 including realized in relation between flood and flooding classes caused by link between flooding and heavy rainfall classes and other connections such as observed on and occurs at consequently data can be viewed as a knowledge graph representation due to the relationships among the classes which allow the semantic linking of data actually with our approach we have not only data semantically integrated but we also have the correlations between those with this graph we could transform information into actionable knowledge as well as extract implicit knowledge the inferences are performed on the logical structure of the ontology and of their constituent definitions the goal of this global data view knowledge graph is to retrieve correlated information and exploit it to learn from it and prevent similar events in the future 5 discussion and conclusion in the present work we proposed memon a modular environmental monitoring ontology that will support semantic interoperability data integration and data linking to provide a global data view one of the strengths of the adopted methodology for ontology development in this work was relying on real environmental data for both collecting the vocabulary and testing the ontology we believe conforming to the vocabulary that can guarantee a global vision of data from multiple sources this global data view is ensured by the ontology based knowledge graph which specifies the semantics of the data and links the multi source data the primary benefits of a knowledge graph are the flexibility of the connections relationships in the data the experts can easily connect new data to infer new knowledge according to its expertise and traverse links to discover how parts of the environmental monitoring domain relate to each other secondly a knowledge graph supports inferences it allows implicit information to be derived from explicitly asserted data the main contributions of this work are the semantic representation of the environmental monitoring domain that allows the expressivity of the different environmental disciplines in a formal and structured way the reuse of an upper level mid level and domain ontologies to improve the interoperability of memon with existing ontologies and the modular representation of the domain by the specification of different modules that allows the independence between modules representing environmental monitoring disciplines while developing memon we consider the clarity of the ontology s structure by choosing a modular conceptualization since the beginning of memon development process to guarantee ontology evolution and maintenance the modularity of memon ensures its reuse as separate modules though memon was built to represent information about environmental monitoring the ontology itself can be employed in other domains and can be used in other contexts for example soil experts could use the environmental material module to deal with soil classification besides the modularity allows the community to expand this work memon can evolve by adding and changing its classes and properties memon reuses reference ontologies top level mid level thus other ontologies can be integrated and reused to enrich memon it can be extended by including new modules for example a service module to manage the access and the treatment of data another example is a source module that can support information about data sources source name data formats data categories besides monitoring applications may extend the presented ontology with other modules such as the actor module itself extended by the emergency responder actor module containing actors e g police and firefighters who may be concerned by environmental monitoring and the emergency response module including emergency response activities moreover memon modules can be reused to develop specific domain modules of a relatively fine granularity for example to go into further details in the environmental processes specific modules such as volcanic activities or hurricane factors can be developed this extension is particularly useful for specific environmental disaster studies besides we can detail environmental material like water by adding information about composition properties and distribution on earth the new module will need classes and relations existing in the environmental material module observation and measurement module and geospatial module the current work offers many challenges and different perspectives one of the main challenges is the maintenance of the ontology a lot of ontologies are built only a few applied attempts have been put into practice and the rest can be considered as obsolete so we need to ensure the updated and the evolvement of memon as perspectives we plan to use the proposed ontology in the near future as support for a novel multi source data integration approach an exciting follow up is to apply memon in real applications such as disaster predictions applications or climate forecast applications through memon classes that constitute the field of the environment and through inter and intra modules relations memon can be useful to generate environmental predictions and recommendations finally given the importance of the aom methodology that guided us to develop memon we recommend it for modular ontology developers 6 software availability the ontology modules described in this paper are available in a complete view at https sites google com view predicat memon the current version of memon is available in owl format to direct download readers may refer to https github com memontology memon the repository contains memon s source code and imported ontologies acknowledgments this research was financially supported by the phc utique program of the french ministry of foreign affairs managed by campus france and the tunisian ministry of higher education and scientific research managed by the cmcu project number 17g1122 code cf 37t03 nj the authors would like to thank the oss experts for their cooperation by providing support domain knowledge and environmental data 
26081,earth observation eo systems play a significant role in environmental monitoring and the prediction of natural disasters these systems generate a massive amount of heterogeneous data stored in different formats the exploitation of this data is still limited while in most cases data are not linked and sources are not interoperable hence data cannot be exploited as an interoperable global knowledge graph to have more in depth analyzes of environmental phenomena ontology as a knowledge representation formalism is a promising solution for the semantic interoperability between this data in this work we present a modular ontology for environmental monitoring developed based on an original agile methodology the so called memon modular environmental monitoring ontology aims to support semantic interoperability data integration and linking of heterogeneous data collected through a variety of observation techniques and systems we also present real use case studies to show the usefulness of the proposed ontology keywords modular ontology environmental monitoring earth observation semantic interoperability data integration knowledge graph 1 introduction in recent years the earth has undergone rapid climate changes which are believed to have in increasing natural disasters such as storms floods and hurricanes these disasters have dramatically influenced not only the natural environment but also human life consequently research communities have given great importance to the development and the implementation of earth observation eo systems such as sensors and satellite platforms 1 1 https sentinels copernicus eu web sentinel missions sentinel 1 and eo programs such as the copernicus 2 2 http copernicus eu program and servir 3 3 https www servirglobal net global along with the increased number of monitoring solutions a multitude of heterogeneous environmental data is generated this data includes hundreds of millions of climate data ocean and coast data 4 4 https oceanservice noaa gov observations monitoring land data and more stored in different formats databases csv files raster images etc this volume of data keeps growing regarding semantic heterogeneity synonymy polysemy etc for example in the sentence maps of daily temperature and precipitation are produced an expert would recognize that the observation is temperature but could not determine the details related to the temperature concept atmospheric temperature sea surface temperature etc he needs to ask the data provider to get more details additionally in different disciplines the same term may correspond to various meanings in one hand for example the term environment is defined as the biological and abiotic elements surrounding an individual organism in the biological domain however it refers to all the natural components of the earth air water oils etc sauvé et al 2016 on the other hand various terms may correspond to the same meaning for instance the observatory of sahara and sahel 5 5 http www oss online org oss may use the word rainfall for the same real world feature that usually refers to precipitation in other sources the variety of terms complicates the work of emergency responders who should be familiar with the terms used in each discipline with this extensive variety of environmental data it is becoming increasingly difficult for domain experts to understand natural phenomena and reduce the adverse effects of climate changes the exploitation of eo data is still limited due to the silos between data sources systems and programs each source offers data or models to be discovered or reused both data and semantic models encode domain knowledge that resides with the experts however this knowledge is often not available and data analysts need to establish contact with original data sources and model producers to understand and use them properly for example the oss provides data about climate change such as precipitation and temperature the national oceanic and atmospheric administration noaa 6 6 https www noaa gov offers marine data and the emergency events database em dat 7 7 https www emdat be provides data about flood events unfortunately these organizations don t collaborate to link the produced data even if it is available online this data is kept as isolated silos undeniably we have not reached a level where data and models are interoperable and linked so that the experts can reuse them soundly we are still far away from the vision of common environmental information space athanasiadis 2015 our purpose is to break down with those silos to provide what we call a global data view where different eo systems and programs will have unhampered and uniform access to the available environmental data that will be linked and synthesized into a single knowledge graph this global data view allows the data sources to speak the same language and to share information so that domain experts could transform information into actionable knowledge we refer here to a knowledge graph ehrlinger and wöß 2016 which is defined as a multi relational graph composed of entities and relationships between them with this knowledge graph experts can look at all of this data and try to find meaning out of its correlations to understand natural phenomena and make the right decisions about disaster risk preventions the hurricane irma which occurred across the caribbean in 2017 serves as an example of how the lack of a global view of the environmental knowledge and the absence of linked observed data hinder the anticipation and the understanding of natural phenomena the traditional conditions of a hurricane development such as the sea level the wind speed and the atmospheric pressure were monitored as usual by the noaa national hurricane center besides the african sahara desert was observed by nasa however there was no link between these observations indeed the absence of the dry air resulting from the lack of saharan dust across the atlantic noaa 2014 acted in favor of high altitude winds when these waves of the air have enough moisture lift and instability they readily form clusters of thunderstorms and a tropical cyclone was formed as the areas of disturbed weather moving westward across the atlantic resulting in the creation of the hurricane irma if experts had the link beforehand and understood the phenomena better they might have been able to predict the power of the disaster a little bit before and alert the governments a global data view is further challenged by data integration data integration is the process of combining data retrieved from multiple and independent sources to provide an integrated and interoperable structure lenzerini 2002 the main challenges confronted by data integration are data linking and semantic interoperability to deal with these issues many studies applied the ontology based approach in the field of environmental monitoring lv and el gohary 2016 stasch et al 2014 however many problems are encountered when using these ontologies in software development for various reasons first some existing ontologies are dealing with specific aspects of the environment monitoring boughannam et al 2013 ma et al 2014 they cannot be used in other contexts so the coverage to annotate or link observed data is most cases not possible with these undoubtedly little attention was given to cover all environmental monitoring disciplines simultaneously which cannot ensure a global view of knowledge second many ontologies are built without considering their reusability zhang et al 2016 dahleh and fox 2016 finally while there have been several fundamental ideas regarding the application of ontologies to environmental monitoring e g curry et al 2013 devaraju et al 2015 there was limited take up by broader communities only a few applied attempts have been put into practice corominas et al 2018 to deal with these issues a semantic oriented platform named predicat predict natural catastrophes that aims at providing data interoperability and linking in eo and disaster prediction was proposed in masmoudi et al 2018 predicat aims to 1 ensure uniform access to heterogeneous data by providing adequate services 2 integrate eo data coming from several sources including that provided by citizens and 3 provide a decision support solution to analyze in real time all the data to effectively prevent and react against natural disasters fig 1 presents the global architecture of the predicat platform and its tiers in this paper we only address the semantic layer by proposing a modular environmental monitoring ontology memon developed based on an original agile methodology this ontology will not only provide a common vocabulary of the domain but also facilitate semantic linking of data from different sources through a knowledge graph the rest of this paper is organized as follows in section 2 we provide an overview of the current work related to ontologies that can be used to semantically model sensors observations and environmental monitoring domains in section 3 we describe the approach of memon ontology development in section 4 we present and discuss the evaluation of the approach through real case studies section 5 is devoted to discuss the proposed approach and present some concluding remarks as well as research perspectives 2 background state of the art and motivations in this section we briefly introduce the ontology classification based on the domain scope then we give an overview of sensor observation and measurement ontologies followed by related work on environmental monitoring ontologies we also underline the motivation for our proposal 2 1 ontology classification ontologies have been widely used for knowledge representation as they provide a shared vocabulary for modeling a specific domain by capturing knowledge in a structured and formal way gruber defined ontology as an explicit and formal specification of a shared conceptualization gruber 1993 besides several classifications were presented in the literature to differentiate ontology categories fig 2 illustrates the classification based on domain scope as given in falquet et al 2011 indeed foundational upper or top level ontologies are generic ontologies that can be viewed as meta ontologies describing the high level concepts or universals used to define other ontologies they guarantee interoperability between domain ontologies sharing the same upper ontology noy 2004 and facilitate the integration and knowledge reuse the most well known foundational ontologies are the suggested upper merged ontology sumo pease 2006 the descriptive ontology for linguistic and cognitive engineering dolce masolo et al 2002 and the basic formal ontology bfo arp et al 2015 core reference or mid level ontologies are often built to capture the fundamental concepts of a domain although this type of ontology is linked to a domain it integrates different viewpoints related to a specific group of users in contrast domain ontologies are only applicable to a domain with a particular point of view they describe the vocabulary related to a specific area of knowledge such as medicine environment etc specializing the terms introduced in the higher ontology finally application ontologies are specializations of domain ontologies they describe the concepts based on a particular context of the domain our work can be situated in domain ontology level 2 2 state of the art of ontologies in this section we aim at identifying ontologies that can be used to semantically model sensors section 2 2 1 and the information available from such systems as a result from the observation of the ambient environment section 2 2 2 a review of some existing environmental ontologies and their comparison according to the aspects of the knowledge acquisition method the domain the principle of reusing existing ontologies and the building purpose is presented in section 2 2 3 2 2 1 state of the art of sensor ontologies first initiatives for modeling sensor and sensor networks were driven by the open geospatial consortium ogc sensor web enablement swe botts and robin 2007 the ogc provides a set of xml schemas and open standards that enable the discovery access and processing of sensor observations the ogc sensorml is one of these schemas it describes sensors systems and processes it also provides the information needed for the discovery of sensors and the location of sensor observations sensorml was used to support the establishment of the ogc s sensor observation service sos providing access to observations from sensors and sensor systems the widely used sensor ontology is the w3c semantic sensor network ssn ontology compton et al 2012 the ssn ontology has been recently updated haller et al 2018 by including a lightweight core module called sosa sensor observation sampler and actuator this new version of ssn is a joint w3c and ogc standard specifying the semantics of sensors observations observable properties actuation and sampling it describes sensors regarding measurement processes observations and deployments however it does not include concepts about the geospatial either temporal dimension 2 2 2 state of the art of observation and measurement ontologies research on observation and measurement context was driven by different organisms the observations measurements o m standard of the ogc swe which describes a conceptual model and xml encoding for measurements and observations is an example aligned to the sensorml the o m establishes a high level framework for representing observations measurements procedures and metadata of sensor systems this standard is required by the sos for the implementation of the swe enabled architectures the semsos o m owl ontology henson et al 2009 is a semantic data model to manage sensor data based on the o m standard the significant concepts modeled in the semsos ontology are observation an act of observing a property or phenomenon to produce an estimate of the value of the property feature an abstraction of a real world phenomenon property associated with a feature that can be sensed or measured process the method system or algorithm used to generate the result such as a sensor result data an estimate of the value of some property location the location of an observation event and time the time when the phenomenon was measured in the real world the significant properties include the feature of interest observed property sampling time observation location result and procedure the extensible observation ontology oboe is a generic ontology dealing with observations data and measurement madin et al 2007 while it s a generic the concepts of this ontology should be extended to clarify the inherent meaning of scientific observations the core classes of the oboe ontology include observation measurement entity characteristic and measurement standard e g physical units and six properties labeled has context of entity has measurement has value has precision and uses standard nasa s semantic web for earth and environment terminology sweet raskin and pan 2005 is a mid level ontology for earth system science it consists of nine upper level concepts representation realm phenomena process human activities matter property state and relation that can be used as a foundation for domain specific ontologies that extend these upper level sweet components 2 2 3 state of the art of environmental monitoring ontologies several works have attempted to build domain and application ontologies for environmental monitoring in the monitor project kollartis et al 2009 kollartis defined an ontology to formalize the knowledge necessary for monitoring methods and environmental risk management monitor ontology is based on dolce upper level ontology semsorgrid4env 8 8 http linkeddata4 dia fi upm es ssg4env index php ontologies default htm is another project which uses ontology as support for sensor data integration two ontologies based on real use cases were built in this project the first one is a fire risk monitoring ontology and the second one is a coastal and estuarine flood ontology the two ontologies are mainly developed reusing the ssn ontology to represent sensor concepts and their observed information and ontologies from the sweet suite 9 9 https sweet jpl nasa gov to describe services and datasets geographic and administrative region data are covered by the ordnance survey 10 10 https www ordnancesurvey co uk ontologies law and environment ontology leo is proposed by informea 11 11 https www informea org fr to provide a semantic standard for data information and knowledge in the field of environmental law and governance it is more a taxonomy since it gives an overview of relationships between multilateral environmental agreements meas as well as concepts definitions and synonyms found in these conventions however it is not written in any ontology format it only contains information displayed as maps info graphics and text leo terms are divided into different sections such as air climate land and water sections these sections would be useful in the exploration phase of our work zhang et al proposed a meteorological disaster ontology mdo to describe the components and relationships between the different parts of the meteorological disaster system zhang et al 2016 mdo describes only meteorological disaster knowledge it does not include other disaster types like hydrological and geophysical disasters mdo is not published in a computable format but classes and relations are defined and could be useful for memon construction based on the water data transfer format wdtf schema shu et al defined a wdtf ontology using concepts and roles that describe water observation data shu et al 2016 wdtf documents were translated into ontology instances to build an application ontology that presents a semantic solution for wdtf data validation in that work shu focused only on data encoded in wdtf qui et al proposed an ontology based approach that links environmental models with disaster related data to support flood disaster management qiu et al 2017 they defined a model ontology and a data ontology by considering disaster related semantics and described the relationships of models and data with a multi level semantic mapping method oliva felipe et al proposed the waste water ontology wawo the evolution of the original wawo ontology oliva felipe et al 2017 this ontology aims to increase efficiency in data and knowledge interoperability and data integration among heterogeneous environmental data sources e g software agents in the scope of urban water resources management within a river basin however none of the ontologies mentioned above use a common upper level ontology nor reuse other domain or existing ontologies they have been developed from scratch each in its ad hoc way in such wise that they create a lack of interoperability with other ontologies in related domains in contrast the reuse of other domain ontologies was the subject of other works examples include boughannam et al 2013 who discussed the advantages of semantic technologies in implementing smarter industry solutions for monitoring analytics they proposed an ontology for the domain of managing observations and measurements based on various ontologies such as the quantity unit dimension type qudt ontology hondgson et al 2014 the ssn ontology and the geosparql ontology 12 12 http www opengeospatial org standards geosparql the adopted solution illustrates how the concepts of these ontologies are implemented using a real world use case from the environment analytics domain the environmental analytics domain ontology proposed by the authors is dedicated to monitoring the environment around their deployed platforms and assigned to a specific use case which is the oil and gas monitoring besides the ontology does not use an upper level ontology nor a modularization approach thus we could not use this ontology for our purpose however the semantic reference models could be used in our solution ma et al developed a global change information system ontology named gcis to represent the content structure of the recent national climate assessment draft report nca3 and its associated provenance information ma et al 2014 since this ontology was built to analyze use cases surrounding nca3 it is dedicated to specific contexts for building provenance aware data services however these two works do not use upper level ontology this concern was adopted by other works in the domain of environmental monitoring such as the work of buttigieg buttigieg et al 2016 which proposed the environment ontology envo aligned to bfo it delineates the environment domain as a whole and also includes other fields such as biomedicine ecology food habitats and socioeconomic development envo classes which include environmental features environmental materials environmental processes and environmental dispositions will be reused to develop the memon ontology dahleh and fox created an ontology for the representation of environmental indicators such as ozone concentration and noise pollution dahleh and fox 2016 they built three domain ontologies to represent pollution sensor and species information based on the global city indicators gci foundation ontology fox 2013 and other reused ontologies such as the ssn ontology this global ontology is specific to environmental indicators representation and it does not represent additional monitoring information such as events and environmental processes many studies have applied an ontology based approach to their emergency management applications llaves and kuhn developed an event abstraction eabs ontology to model events inferred from observations and an application ontology named flood monitoring ontology llaves and kuhn 2014 eabs ontology is kept generic enough to be reused for other applications extends the ssn ontology and allows only inferring information from observed data the ontology eabs does not perform reasoning on events since the event patterns are keeping out of the ontology instead of including them as ontology rules devaraju et al developed a sensing geographic occurrence ontology sego which represents relations between geographic events and sensor observations and an application ontology for blizzard disaster devaraju et al 2015 sego does not provide information about the environmental domain but instead it is developed as a starting point for the construction of application ontologies to infer geographic events from sensor observations sego is based on dolce foundational ontology and is centered around the sensing domain therefore devaraju et al distinguish stimulus from environmental processes to emphasize the process that actuates a sensor to produce observations however in our work we consider a stimulus as an environmental process eabs and sego used dolce as an upper level ontology in table 1 we present a comparison of the different approaches mentioned above of using ontologies to solve semantic heterogeneity in environmental monitoring domain the comparison is based on the following criteria building purposes domains methods for knowledge acquisition and links to other ontologies 2 3 synthesis and motivations despite these numerous works several limitations can be noted first many studies focused only on the knowledge expression of specific environmental observations type such as in situ or on specific requirements for a particular organization second some existing ontologies are dealing with specific aspects of the environment so the coverage to annotate or link observed data is most cases not possible with these the environment is lying on multidisciplinary fields such as meteorology hydrology geology geography and so on existing ontologies do not cover all environment fields such as the ontology mdo neither the observation conditions contexts such as the ontology envo which does not include the sensing context nor the human hazard inducing factors nor the spatiotemporal context third little attention was paid to the semantic relationships between different environmental components such as disasters observations in time and space and other correlated environmental conditions that can provide a comprehensive view to analyze environmental factors to address this issue some ontologies from specific case studies however they cannot be reused in other disciplines finally each monitoring system generally uses a specific reasoning method on specific data sources and formats to generate inferences which makes it difficult or even impossible to obtain inferences across heterogeneous data sets besides we can deduce that there are ontologies that include useful concepts for our proposed ontology such as wdtf ontology sego etc however their concepts also exist in other ontologies like the envo ontology and the ssn ontology which we prefer to use since they contain more concepts apt to be reused in our ontology our motivation is to address these listed gaps by building a modular environmental monitoring ontology memon to link and integrate observed data from various sources the ontology memon aims at 1 ensuring semantic interoperability between heterogeneous data sources 2 supporting knowledge discovery and generation 3 integrating and linking data across various disciplines and 4 providing a global data view 3 memon modular environmental monitoring ontology this section presents the methodology to build the proposed modular ontology for the environmental monitoring field building a modular ontology is not a straightforward task especially when ontologies become more significant and more complex fernandez lopez and corcho 2004 therefore a methodology that guides and manages the modular ontology development is necessary but one of the thorniest problems is how to choose the appropriate methodology taking account the complexity of the domain to be modeled and the furthest evolution and reuse to support ontologies building several methodologies such as methontology neon ontoclean have been discussed in karray et al 2012 in this work we adopt the aom agile methodology for developing ontology modules methodology gobin 2013 this choice is argued by the fact that an agile methodology will enable the incremental and iterative development of memon s modules whereas other methodologies are suited to develop ontologies in one go aom stipulates the sequence of four phases exploration planning module development and finally release phases we adjusted these phases according to our needs and contexts as shown in fig 3 the development life cycle starts with the exploration phase in which we identified the ontology requirements and objectives then we defined a set of competency questions cqs according to the studied domain and experts needs the cqs consist of a set of questions stated in natural language so that the ontology must be able to answer them correctly grüninger and fox 1994 given this set of cqs we extract stories to represent the different contexts that limit our domain the planning phase indeed stories are scenarios and contexts from which facts can be obtained which will be used to build the ontology these stories were written based on the information gathered during interview sessions with domain experts in the module development phase the cqs related to one module are used to design a semi formal module then the modules are formalized implemented and evaluated to be merged in the release phase the next sections detail these four phases according to the adapted aom methodology 3 1 exploration phase the ontology development process starts with the exploration phase whereby the domain expert is part of the development team and with the knowledge engineer both embark on the development process to have a better overview and understanding of the domain of environmental monitoring we refer to the oss our project partner which have significant expertise in environmental monitoring natural resource management and climate change during this phase the ontology engineers and the oss experts sat together and discussed the domain of interest the objectives and the requirements of the ontology the objective of the memon ontology is to provide a formal representation of the domain of environmental monitoring to support 1 semantic interoperability 2 knowledge discovery and generation 3 data integration and linking and 4 a global data view to understanding environmental phenomena better the memon is developed to encapsulate the knowledge contained in multiple environmental data sources the key requirements of the memon ontology are listed as follows the ontology captures the notions related to the domain of environmental monitoring the ontology applies the principles of modularization ontology modularization is the process of defining a module which is a subset of the global ontology doran 2009 the modularity of the ontology contributes to improve the ontology development process by reducing the complexity of designing maintaining enriching and replacing modules maintain the clarity and the coherence of the ontology by presenting ontology modules with needed knowledge promote the reuse of each module separately according to zimmermann and le duc 2008 an ontology module can be considered as a loosely coupled and self contained component of an ontology maintaining relationships with other ontology modules in the scope of this work an ontology module represents a point of view covered by an environmental domain context such as spatial context or observation context admittedly terms in an environmental domain are linked together but each term can be used in a specific context for instance the sensor class has a strong link with observation related terms but should belong to the most suitable module which is the sensor sensing module the ontology is aligned with an upper level ontology in order to ensure an interoperable reuse and integration of ontological modules elmhadhbi et al 2019 we choose bfo for basic formal ontology bfo as a starting point for our ontology building for two primary reasons firstly our domain of interest the environment field is a realistic domain thus we looked up for an upper level realist ontology that represents environmental entities as they are and not representing environmental concepts and representations existing in the domain experts minds smith and ceusters 2010 secondly the ontologies envo and cco reused in our ontology are some of those ontologies that have been developed by adopting bfo as the foundation of the ontology development these factors make its use a key enabler in promoting semantic interoperability of these ontologies and the reuse of our ontology by others the ontology reuses classes from mid level ontologies that specialize the classes in the upper level ontology and other domain ontologies 3 2 planning phase during the planning phase competency questions cqs are defined in coordination with the domain expert the oss the domain of interest is analyzed by exploring multiple data sources such as the oss copernicus emdat etc each source has its own set of relevant data in specific disciplines for example the oss generated climatological data data about temperature precipitation etc whereas the emdat database provides data about natural disasters thus to have an overview of the knowledge that can be extracted from the multiple data sources we define cqs which the ontology should be able to answer given this set of cqs stories are extracted to define the perimeter of the domain of study so we identified four main stories which contribute to environmental monitoring fig 4 1 environmental conditions story which includes geological structures rock and soil types geographical features forest land hydrological structures water oceans rivers lakes environmental processes ground shaking natural disasters and so on 2 urbanization story infrastructure 3 observing conditions story which includes sensing systems sensors satellites and so on and observation conditions properties and measurement conditions 4 and spatiotemporal conditions story time and location of environmental activity or observation from these stories we identified eight ontological modules the first story deals with environmental materials environmental processes and natural disasters regardless of the discipline hydrology geology meteorology etc so we identify them as three modules the second story results in the infrastructure module from the third story which handles the observations their measurements and systems that produce them we identify two modules the observation measurement module and sensor sensing module the final story is about the time and space conditions of events or observations knowing the importance of these two contexts we identify the temporal module that includes temporal entities and the geospatial module that covers entities about locations location country etc and environmental features such as forests since one of the memon requirements is the reuse of existing ontologies and following the style of existing geospatial ontologies the module geospatial ontology results from a combination of the first and the last stories table 2 the relationships between modules will be mapped into the ontology using axioms we can for example ensure that environmental processes are associated with a spatiotemporal context through time r and spatial r relations time r represents timing relations e g occurs on occurs before occurs after etc of different disastrous weather climate events and between them spatial r represents spatial relations e g location neighborhood etc of events occurring the disasters also can be related to the environmental processes through causal relationships in the next phase each story is transformed into cqs with the support of the domain experts for example for an environmental process story some cqs obtained are shown in table 3 3 3 module development phase the proposed ontology consists of a set of modules covering the disciplines of environmental monitoring domain memon consists of eight main modules namely observation measurement module sensor sensing module natural disaster module environmental process module environmental material module infrastructure module temporal module and geospatial module these modules are developed and released through eight iterations and increments the iterative process starts with the definition of the cqs that the chosen module should answer the cqs are discussed to define a semi formal model which is given through a concept map created using the ihmc cmaptools computer program 13 13 https cmap ihmc us fig 7 to fig 14 each semi formal model includes concepts and relationships related to these cqs by referring to existing ontologies and standards later the formal model is developed using the web ontology language owl2 14 14 http www w3 org tr owl ref finally the module is evaluated section 3 3 4 we used a hybrid approach based on a top down alignment to an open source top level ontology noy and mcguinness 2001 and a bottom up focus on classes that are grounded in environmental data sources each module was constructed by reusing the upper level ontology bfo the mid level ontologies the common core ontologies cco cubrc 2017 and lower level ontologies this latter represents specialized domains having classes that are either direct subclasses of the upper or the mid level ontologies such as the envo ontology or imported and classified according to bfo and cco such as the ssn ontology ontology reuse has several advantages first it is no longer necessary to build the ontology from scratch second it increases the quality of new ontologies because the reused components have already been tested finally ontology reuse improves the efficiency of ontology maintenance ding et al 2007 3 3 1 bfo top level ontology bfo basic formal ontology is a top level ontology which has been applied in various domains such as biomedicine and military it is also in the process of becoming an iso standard iso 21838 2 bfo provides neutral and general classes under which the domain s universals can be inserted as a top level ontology it assists in organizing data from different domain ontologies in a way that promotes the semantic interoperability degree of systems using these ontologies the two main categories of bfo classes are continuant which are entities that continue to exist over time such as objects and locations forests water etc and occurrent which are event entities processes and temporal regions such as rain earthquakes etc fig 5 and table 4 summarize the key categories encountered in bfo accordingly we choose bfo as a starting point for our ontology building for two primary reasons firstly our domain of interest the environment field is a realistic domain thus we looked up for an upper level realist ontology that represents environmental entities as they are and not representing environmental concepts and representations existing in the domain experts minds smith and ceusters 2010 secondly the ontologies envo and cco reused in our ontology are some of those ontologies that have been developed by adopting bfo as the foundation of the ontology development these factors make its use a key enabler in promoting semantic interoperability of these ontologies and the reuse of our ontology by others 3 3 2 cco mid level ontologies the common core ontologies cco form a collection of mid level ontologies in owl that extends bfo schoening et al 2015 it consists of ten interrelated modules allowing the development of interoperable ontologies the ten ontologies provide a starting set of general commonly used terms involving agents actions artifacts and measurements the ontologies in the cco include agent ontology representing agents especially persons and organizations and their roles artifact ontology ao representing deliberately created material entities along with their models specifications and functions currency unit ontology representing currencies in different countries extended relation ontology representing relations i e object properties holding between entities extended relation ontology is imported in memon since it contains several object properties such as has quality occurs at etc event ontology eo representing processes geospatial ontology geo representing sites spatial regions and other entities especially those that are located near the surface of earth as well as the relations that hold between them information entity ontology ieo representing generic types of information as well as the relationships between information and other entities quality ontology qo representing a range of attributes of entities including qualities realizable entities such as dispositions and roles and process profiles time ontology to representing temporal regions and the relations that hold between them a temporal region as defined by bfo is an occurrent entity that is part of the time as defined relative to some reference frame units of measure ontology umo representing standard units used when measuring various attributes of entities these mid level ontologies are extended for the development of the memon modules as illustrated in fig 6 the cco reuse the relations ontology ro which is a collection of owl2 relations intended to be shared among various ontologies it incorporates a set of upper level relations such as part of and has input these relations are re used in all memon modules 3 3 3 the modular environmental monitoring ontology memon in this section we present the eight modules of memon for each module we describe what classes and relations are reused what classes and relationships are added and how are they inserted under bfo and cco classes fig 7 14 illustrate partial views of the modules in each figure classes are marked with a color and a prefix the prefix marks the source ontology and the namespace used to define the class classes added in memon are marked with a namespace prefix memon classes reused from existing ontologies are marked with different colors to make the concept map easier to read for humans see colors in fig 6 and namespace prefix such as envo bfo etc for more details about modules readers may refer to the memon google site 15 15 https sites google com view predicat memon at the beginning of a module development phase a list of the cqs that the module should be able to respond is defined the cqs shown in tables 5 12 are generic and can be specialized with any given observation or event a sensor and sensing module this module describes sensor systems sensors and how they are related to observations the sensor sensing module should be able to answer the following cqs table 5 as described in section 2 2 2 the reused sensor ontology built upon the ogc standards the sos and the o m standards is the ssn ontology that describes sensors and sensor networks the sensor sensing module was developed based on this ontology besides we reused the artifact ontology ao from the cco which allows the classification of sensor systems as an artifact object and the definition of functions that can be applied to an artifact we started by importing the ssn ontology then we classify its classes under bfo and cco classes for example the class ssn system and its subclasses actuator sample and sensor are classified under ao artifact since they can be defined as an object that was designed by some agent to realize a certain function considering that the class sensor also exists in the cco as a transducer that is designed to detect events or changes in its environment and then provide a corresponding output and to avoid inconsistency of the ontology we add an equivalent relation between ssn sensor and ao sensor following the classification of the ssn ontology we define different kinds of sensors as classes in the memon such as temperature sensor sea surface topography sensor and pressure sensor we also classified them into active sensors which are radar instruments used for measuring signals transmitted by the sensor that were reflected refracted or scattered by the earth s surface or its atmosphere and passive sensors which are microwave instrument designed to receive and to measure natural emissions produced by constituents of the earth s surface and its atmosphere 16 16 https www nasa gov directorates heo scan communications outreach funfacts txt passive active html the ieo ontology of the cco define entities related to the meaning of information artifacts it classifies three kinds of information content entity descriptive entity that consists of a set of propositions that describe some entity designative entity that consists of a set of symbols that denote some entity and directive entity that consists of a set of propositions that prescribe some entity the class ssn output is classified under the class ieo descriptive information content entity and we added as subclasses satellite output and sensor output the ssn deployment is defined as the deployment of one or more systems for a particular purpose the definitions in bfo represent what is general in reality so we defined a deployment as a methodical procedure of introducing an activity process program or system to all applicable areas of an organization and we inserted it as a subclass of ieo directive information content entity a procedure is defined in the ssn ontology as a workflow protocol plan algorithm or computational method specifying how to make an observation that prescribes the inputs and outputs of a system we classify this class under the class ao artifact function specification since this latter defines each directive information content entity that prescribes some artifact function this module includes 83 classes with 1264 axioms two data properties and 38 object properties see fig 7 b observation and measurement module memon is concerned with describing how observations are related to sensors events and measurements thus the observation measurement module should be able to answer the following cqs table 6 the goal of observation might be to measure or otherwise determine the value of a property as the precedent module we import the ssn ontology which defines observation a feature of interest and properties concepts then we classify its classes under bfo and cco classes by following the philosophy of bfo observation is defined in the ssn ontology as an act of observing a property or a phenomenon to estimate or calculate a value of a property of a feature of interest we classify the sosa observation class under the class eo act of the cco sosa recognizes sosa observation about some sosa observable property of a sosa featureofinterest since the class sosa observableproperty subclass of the ssn property class is defined as an observable quality property characteristic of a feature of interest then it is classified under the bfo quality class of bfo whereas the class sosa featureofinterest is classified under ieo descriptive information content entity since it represents the thing whose property is being estimated or calculated in the course of an observation following the classification of the ssn ontology we define different kinds of observable properties as classes in the memon ontology to this end we reused classes from the quality ontology qa from the cco such as qa temperature and qa wetness classes from the envo such as envo temperature of water and envo temperature of air then we created new classes such as precipitation wind speed humidity and sea surface temperature fig 8 shows some examples of observable properties to reduce the generality of the class sosa result we added the class observation result this new class is represented as the equivalent of the class measurement information content entity of the cco defined as a descriptive information content entity that consists of a symbol that is a measure of the extent dimensions quantity or quality of an entity relative to some standard we consider that an observation result can be a nominal measurement such as a color value or an interval measurement such as temperature value the ssn ontology however does not include modeling aspects for units of measurement and domain knowledge that are related to observed properties to model measurement units we reused the unit of measure ontology umo from the cco this ontology extends from the ieo the class ieo measurement unit and provides several subtypes of the measurement unit e g umo measurement unit of speed umo measurement unit of pressure and umo measurement unit of temperature it also provides instances of these classes e g horsepower measurement unit and kilometer measurement unit fig 8 shows a partial view of the observation and measurement module it contains 102 classes with 968 axioms two data properties and 37 object properties c environmental material module the states of environmental material play an important role in environmental monitoring the relation between observations and environmental materials is evident environmental material quality such as water quality or environmental material existence such as fume are observations detected by environmental monitoring techniques and enables to monitor the environment and predict disastrous events before identifying the taxonomy we define the cqs which this module should answer table 7 we reused classes defined under envo environmental material class in envo fig 9 shows some memon classes this module contains 87 classes 802 axioms and six object properties in owl classes can be either primitive or defined primitive classes only have necessary conditions i e superclasses defined classes have necessary and sufficient conditions i e equivalent classes to enrich the environmental material module we used defined classes the definition of the liquid environmental material as being equivalent to environmental material and has quality some quality of liquid is an example of an equivalent class that satisfies necessary and sufficient conditions in other terms if liquid environmental material class is described using only necessary conditions then we can say that if an individual is a member of class liquid environmental material it must satisfy the conditions we cannot say that any random individual that satisfies these conditions must be a member of the class liquid environmental material however if the liquid environmental material class is now defined using necessary and sufficient conditions our case we can say that if an individual is a member of the class liquid environmental material then it must satisfy the conditions also we can say that if any random individual satisfies these conditions then it must be a member of the class liquid environmental material the conditions are not only necessary for membership of liquid environmental material but also sufficient to determine that something satisfying these conditions is a member of liquid environmental material consequently we can use the reasoner to compute automatically a classification hierarchy in this way to classification to ensure the refinement of the individuals classification and allow more semantic precision in the representation of knowledge in order to make the implicit knowledge more explicit equivalent classes are also defined in other memon modules d environmental process module the understanding of environmental phenomena and environmental changes needs a complete and comprehensive representation of the processes which are involved in such changes consequently a set of cqs presented in table 8 has been defined the environmental process module contains 214 classes representing environmental processes and 15 relations have been added to memon the classes were aligned with the bfo process class in bfo since they represent entities that occur and develop in time unfold in successive phases and always depend on some material entity they were based on the knowledge of experts the envo ontology and the unesco thesaurus 17 17 http vocabularies unesco org browser thesaurus and have been used to interlink observations and environmental materials for example a tree burning is related to smoke through has output relation fig 10 following the classification of the envo ontology we define different kinds of environmental processes and classify them into climatological hydrological geographical geophysical and other categories in addition to the different main categories of environmental processes we reused classes from the event ontology eo of the cco to describe changes such as the classes eo decrease of a quality and eo loss of a quality and we extended them with classes such as memon decrease of a temperature memon loss of a mass the relations between the classes of this module are primarily controlled by the relations ontology ro such as has input and occurs during in the aim of better understanding environmental phenomena their factors and especially their order of occurrence we identified the need to add object properties such as preceded by although the strong relationships between environmental processes and disasters the definition of natural disasters was set aside in a separate module this separation will promote its reuse for other applications such as an emergency response application cqs defined for the natural disaster module were summarized in the following table table 9 in envo a disaster is modeled as a subcategory of the class bfo process however to emphasize the difference between natural disasters and environmental processes we classify natural disasters as bfo disposition according to bfo a disposition is a realizable entity in virtue of which a process of a certain kind occurs in the independent continuant bearer in which the disposition inheres this process is called the realization of the disposition for example the disposition of a forest region which undergoes land degradation to desertification thus we classified the desertification disaster under the class bfo disposition and specified the class land degradation as a process we define different kinds of natural disasters in the memon ontology and classify them into climatological geophysical hydrological and meteorological categories we reused classes from the envo ontology such as envo earthquake and envo volcano and created new classes such as memon cyclone and memon hurricane in addition to classifying natural disasters into different categories we also define them regarding how they affect each other by establishing the relationships caused by and followed by and how they occurred as a result of one or many environmental processes by the relation realized in as illustrated in fig 11 fig 11 shows a partial view of natural disaster module classes the entire module contains 80 classes and 782 axioms f infrastructure module several physical objects mentioned in memon could be useful in environmental monitoring infrastructure objects which are deployed in a specific location where particular environmental processes occur could participate in environmental hazard factors for example the dam failure flood in laos 18 18 http www hydroworld com articles 2017 09 nam ao dam in laos burst last week html we defined cqs related to the infrastructure story in the following table to build this module we imported classes from the ao from the cco ao includes transportation infrastructure such as bridges and tunnel for better classification we added the class memon hydraulic infrastructure to incorporate water infrastructures such as ao dam as shown in fig 12 the infrastructure module includes 48 classes and 608 axioms and contains several object properties such as has participant table 10 g geospatial module the challenges in environmental monitoring are motivated by the necessity to take in consideration geospatial information according to beard and neville knowledge of spatial contextual differences among observations is important for interpretation and analyses beard and neville 2014 we addressed the spatial aspect after defining the cqs related to the geospatial discipline as illustrated in table 11 the identification of the cqs core elements classes and their relationships enables the creation of the module vocabulary that consists in defining environmental features locations and relationships among them by importing the geospatial ontology geo of the cco an environmental feature as defined by the geo is a site feature having a relatively stable location in some geospatial region which can be designated by location specific data it contains four categories anthropogenic features which are features relating to or resulting from the influence of human beings on nature geographic features that are natural features marine features which have characteristics of a marine entity and finally habitat which are environment features having qualities which may sustain an organism or a community of organisms referring to the w3c basic geo wgs84 lat long vocabulary 19 19 http www w3 org 2003 01 geo we represent longitude and latitude and other information about spatially located things we distinguish two primary types of properties in this module object properties that link classes to other classes and datatype properties that link classes to data values as an example of object properties we can cite located in as instances of datatype properties we can mention has latitude value and has longitude value fig 13 presents a partial view of the memon geospatial module this module includes 291 classes 2216 axioms and 21 object properties h temporal module in addition to the spatial context the temporal context of observations and events is also essential to understand and monitor environmental phenomena the cqs related to the temporal story are summarized in table 12 several ontologies have been developed to describe temporal information for example swrl temporal ontology 20 20 http swrl stanford edu ontologies built ins 3 3 temporal owl and the w3c s time ontology 21 21 http www w3 org tr owl time to model temporal information we choose the time ontology to from cco while it is based on bfo and interoperable with other modules additionally it offers a set of object properties e g interval during instant is before that can be used to reason with temporal information since the temporal information from different sources is heterogeneous we add some defined classes to represent the different representations of the temporal information for example the class memon date the class memon timestamp that is equivalent to a memon date and contains a to day a to month a to year an to hour a to minute and a to second and the class memon year month that is equivalent to a to year and a to month are added to the module fig 14 presents a partial view of the temporal module classes and properties we added and asserted some object properties as sub properties of corresponding properties in the to such as memon day which is asserted as a sub property of to interval contains memon occurs before and memon occurs after are other examples of object properties added to the ontology to describe timing relations between different environmental processes the entire module contains 60 classes 1184 axioms and 60 object properties 3 3 4 module formalize and release step the building process of the memon modules is supported by the use of protégé 22 22 https protege stanford edu a popular and open source platform that provides users with a set of tools to construct semantic models it greatly facilitates the definition of classes properties restrictions and rules of memon and supports the visualization and manipulation of the ontology memon is expressed in owl2 23 23 http www w3 org tr owl ref for the reuse of existing ontologies such as cco we used the owl import feature of owl2 indeed the import of a specific class from an ontology a to an ontology b is equivalent to the copy of all the declarations of this class including the uri the labels the definition and the relations after the formalization step the developed module was evaluated firstly the consistency of the module was checked through the reasoner pellet which is an owl2 reasoner integrated into the protégé software ontology inconsistency refers to the fact that there are logical incoherencies or modeling problems that arise if the primitives given by the ontology implementation language are not used correctly such as contradictory relations secondly the module was evaluated in comparison with specific criteria various approaches have been proposed in the literature for ontology evaluation targeting several different criteria and metrics gangemi 2005 obrst et al 2007 we used the tool for ontology modularity metrics tomm software which encompasses all the evaluation metrics presented in khan and keet 2016 these metrics are grouped into four categories of criteria the structural criteria are calculated based on the structural and hierarchical properties of the module calculating structural criteria involves evaluating the size i e the number of entities in a module m such as the number of classes object and data properties the relative size represents the size of the module compared to the global ontology o 1 r e l a t i v e s i z e m m o the atomic size is the average size of a group of interdependent axioms in a module an atom in a module is defined as a group of axioms with dependencies between each other 2 a t o m i c s i z e m a x i o m s a t o m the cohesion which refers to the extent to which entities in a module are related to each other 3 c o h e s i o n m e i m e j m s r e i e j m m 1 i f m 1 1 o t h e r w i s e where sr ei ej is the relation function the logical criteria include the correctness and the completeness metrics the correctness evaluates if every axiom that exists in the module also exists in the global ontology 4 c o r r e c t n e s s m t r u e i f a x i o m s m a x i o m s o f a l s e o t h e r w i s e the completeness evaluates if the meaning of every entity in a module is preserved as in the global ontology 5 c o m p l e t e n e s s m t r u e i f i m a x i o m s e n t i t y i m a x i o m s e n t i t y i o f a l s e o t h e r w i s e the relational criteria that deal with the relations and behavior that modules exhibit with each other such as the inter module distance imd in a set of modules which describes the number of modules that have to be considered to relate two entities 6 i m d m e i e j m i m n n m e i e j m i m n m i m n 1 m i m n 1 1 o t h e r w i s e where nm ei ej is the number of modules to consider to relate entities i and j the product of mi mn mi mn 1 represents the number of possible relations between entities in a set of modules mi mn and the richness criteria which include the attribute richness which defines the average number of attributes per class where att is the number of data properties in the module 7 a r m a t t c the inheritance richness which is defined as the number of subclasses h per class c in a module 8 i r m h c and the relationship richness that describes the diversity of relations types in the ontology p is the set of non hierarchical properties 9 i r m p h p according to the metric results applied to memon shown in table 13 we can deduce the following the atomic sizes of the modules indicate that there are on average between 3 23 and 4 79 axioms that are grouped in an atom for the eight modules most of the memon modules contain few attributes as the attribute richness is less than 1 for the majority of the modules mdisaster has the lowest rr and mtemporal has the highest one an ontology that has little value of rr may have only inheritance relationships that is the case of mdisaster since it includes only the set of disasters organized hierarchically consequently it conveys less information than mtemporal which contains a diverse set of relationships e g interval during and has ending instant ir values are comprised between 0 9 and 1 1 which represent high values indeed ontological modules with high ir are called horizontal ontologies since classes have a large number of direct subclasses this indicates that our modules represent a wide range of knowledge with a low level of detail accordingly they are more open to evolving and being specified this evolution corresponds to our objective to enrich the ontology with further information in other development iterations by comparing the cohesion values coh mo we found that the modules and mtemporal mo m menviprocess have the highest cohesion values due to the strong relatedness of different classes of each module for instance the observation and measurement module mo m deals with the classes of observed properties this module models relations between observation events measured properties and measurement units all of these classes and how they are related are the essence of the higher cohesion in the ontological module as a conclusion the result metrics values generated by the tomm software represent valuable measurements to evaluate the ontology modules the main objectives of the evaluation step via metrics are to 1 check the consistency of each module and between modules and the global ontology 2 provide an insight to the ontology users about the granularity of each module if it is not highly completed in term of classes relations they should refine it with more specific classes and relationships according to their reuse context 3 4 release phase of the global ontology at the beginning of the release phase the developed modules are merged after that the global modular ontology is enriched by inter module relations fig 15 provides an overview of the memon revealing the different modules and presenting some inter and intra module relationships each module has at least one relationship with other modules for example the disaster module is linked to the environmental process module by the relation realized in this means that a disaster can be produced when one or many environmental processes occur the environmental process module is linked with the temporal module by the relation occurs on and with the geospatial module by the relationship occurs at these connections seek to identify the time and the location of a process the global ontology includes 1246 classes 417 properties along with 10956 defining axioms logical statements interconnecting and interrelating classes readers may refer to memon google site 24 24 https sites google com view predicat memon for further details some complex relationships cannot be represented by owl2 dl for example owl2 cannot express the relations between individuals referenced by object properties to overcome the expressiveness limits of owl2 we use swrl language 25 25 http www w3 org submission swrl swrl rule is described as antecedent consequent this signifies that if all the conditions in the antecedent are held then all atoms in consequent must also be held there are also built in functions in the swrl syntax that are capable of describing the logical comparison relationship for example a built in greaterthan returns true if the first argument value is more significant than the second value see rule r1 table 14 summarizes some examples of swrl rules specified regarding the memon ontological vocabulary for instance rule r1 automatically classifies an existing rain with a precipitation rate between 16 and 50 mm hour as a very heavy rainfall indeed in our ontology there are several categories of rain process light rainfall moderate rainfall heavy rainfall and very heavy rainfall this classification is due to the precipitation detection rate other rules with the same syntax allow the classification of the other categories of rain at the beginning of a reasoning process we input different rains as instances of the water based rainfall class we still do not know the category of each rain process once the reasoning process is finished the results obtained show that each detected rain has been inserted under the corresponding category of rain hence implicit knowledge is generated from inputted data table 14 after enriching the memon ontology with the relations and rules we check if it contains any inconsistency by using the pellet reasoner finally memon is released so that it can be verified and validated against the requirements and objectives defined at the beginning of the development process 4 memon ontology evaluation in this section we present the memon evaluation step through the verification section 4 1 as well as the validation processes section 4 2 4 1 ontology verification ontology verification is the ontology evaluation which compares the ontology against the ontology specification document ontology requirements and cqs sections 3 1 and 3 2 thus ensuring that the ontology is built correctly suárez figueroa and gómez pérez 2008 in other words it allows us to answer the question are we producing the ontology right to do that we translate some cqs to sparql 26 26 https www w3 org tr rdf sparql query language to query the ontology examples of obtained results are presented in figures 16 18 cq1 what are the environmental processes that can cause a flood disaster envo 01000710 cq2 what are the environmental materials involved in an effusive volcanic eruption cq3 what is the type of sensor used for ground vibration memon 00001122 the answers to these questions verified by the domain experts show that memon has the competency in providing the right information to the questions 4 2 ontology validation use case studies ontology validation is the ontology evaluation that compares the meaning of the ontology definitions against the intended model of the world aiming to conceptualize suárez figueroa and gómez pérez 2008 in other words it allows us to answer the question are we producing the right ontology to validate memon we will focus on two case studies the exploitation and reuse of environmental knowledge the data integration 4 2 1 first use case knowledge exploitation and reuse in this section we present an example to check the possibility to explicit implicit knowledge expressed into memon we instantiate the class hurricane of the memon ontology with the instance irma that occurred in the caribbean in 2017 then we mapped this instance with characteristic instances that describe the irma hurricane such as the value 914 mb for the barometric pressure 285 km h for the wind speed and 5 4 m for the sea level as illustrated in fig 20 after that we used the reasoner pellet to infer implicit knowledge from explicit one finally we interrogate memon with the sparql query below to extract the list of the five category hurricanes memon 00001186 which exists in memon knowledge base thanks to the rule r4 table 14 that automatically classifies an instance of the hurricane class knowing that it occurred with a wind speed greater than 252 km h as a five category hurricane we get irma as a result of this query this information was not explicitly instantiated in the ontology was implicit but through the defined rule the knowledge concerning the category of the hurricane can be inferred from the knowledge base and becomes explicit fig 19 4 2 2 second use case multi source data integration in the second use case we present an example to demonstrate the ability of memon to guarantee semantic interoperability integrate and link observed data from multiple sources this use case is based on a real world example and deals with precipitation data from the oss in raster images format data about storm events from the noaa and data about flood events from the emdat in csv format one of the problems that we were faced with integrating data across sources is that data schemas metadata in the three sources are heterogeneous terms are presented differently in each source for example in emdat the term disaster is used to describe a natural disaster while the term event is used in noaa thus memon is used to resolve this problem and ensure semantic interoperability between heterogeneous data schemas to semantically integrate this multi source data we used the karma web system gupta et al 2012 a data modeling and integration framework karma provides tools to semi automatically build a semantic model of a data source this model makes it possible to rapidly map a set of sources represented in xml csv json structured text files or databases into a domain ontology once the data sources are modeled the models are then converted into a variety of formats including rdf fig 21 illustrates the whole use case process before importing data into karma we transformed the raster images provided from the oss into csv files step 1 in fig 21 to this end we have designed and implemented a semi automatic process that performs this projection using the geospatial data abstraction library gdal warmerdam 2008 then we imported data from the multi source csv files into karma where memon is also imported as the required ontology for the data mapping process step 2 fig 22 shows the snapshot of an example of the data importation in karma karma provides a graphical user interface to let users interactively model the data for each source according to the ontology modeling is the process of specifying how the different metadata items of a data source columns names in a csv file map to classes and relationships in an ontology the data mapping involves two interleaved steps the assignment of classes to data columns names and the specification of the relationships extracted from memon between the classes as illustrated in fig 23 for example we map the metadata item event id to the class envo 01000876 that refers to storm from memon and the metadata item yearmonth to the class memon 00001099 labeled yearmonth in the ontology then we link the two classes with the relation occurs on as illustrated in fig 23 step 3 the output of the mapping process is the semantic models correspondent to data sources csv files in our case once we have modeled the three data sources karma uses these models to convert the data into an rdf data model that describes the global view of data step 4 this rdf model is used to perform sparql queries across the three data sources step 5 we consider two examples of queries that need information for more than one source in doing so we used the sparql query editor integrated into the protégé software following are the two queries 1 what are the natural disasters that occurred between 2016 and 2018 2 what are the amounts of the annual precipitation of the flood events that occurred between 2016 and 2018 query 1 what are the natural disasters that occurred between 2016 and 2018 as can be seen in fig 24 two types of natural disasters floods and storms are extracted the result consists of a combination of data from both the emdat and the noaa sources to obtain information for only one type of natural disasters we change the disastertype of the sparql query to the specific class defined the disaster memon 00001019 or memon 00001020 query 2 what are the amounts of the annual precipitation of the flood events that occurred between 2016 and 2018 as has been mentioned at the beginning of the paper the purpose of the semantic data integration through memon is to provide information from multiple data sources to be able to understand the environmental phenomena the sparql query below asks for the precipitation amounts of the flood disasters occurred between 2016 and 2018 knowing that the precipitation data is imported from oss whereas the data about floods is imported from emdat fig 25 shows the result of this query analyzing the query we can note firstly that the query needs information about both average annual precipitation and flood disasters the result proofs that memon ensures a semantic data integration and linking in a way that allows more effective knowledge retrieval which are the objectives of memon secondly the result of this query illustrates how we can navigate in memon ontological model to extract the most implicit knowledge the query s result is obtained by covered relationships shown in fig 26 including realized in relation between flood and flooding classes caused by link between flooding and heavy rainfall classes and other connections such as observed on and occurs at consequently data can be viewed as a knowledge graph representation due to the relationships among the classes which allow the semantic linking of data actually with our approach we have not only data semantically integrated but we also have the correlations between those with this graph we could transform information into actionable knowledge as well as extract implicit knowledge the inferences are performed on the logical structure of the ontology and of their constituent definitions the goal of this global data view knowledge graph is to retrieve correlated information and exploit it to learn from it and prevent similar events in the future 5 discussion and conclusion in the present work we proposed memon a modular environmental monitoring ontology that will support semantic interoperability data integration and data linking to provide a global data view one of the strengths of the adopted methodology for ontology development in this work was relying on real environmental data for both collecting the vocabulary and testing the ontology we believe conforming to the vocabulary that can guarantee a global vision of data from multiple sources this global data view is ensured by the ontology based knowledge graph which specifies the semantics of the data and links the multi source data the primary benefits of a knowledge graph are the flexibility of the connections relationships in the data the experts can easily connect new data to infer new knowledge according to its expertise and traverse links to discover how parts of the environmental monitoring domain relate to each other secondly a knowledge graph supports inferences it allows implicit information to be derived from explicitly asserted data the main contributions of this work are the semantic representation of the environmental monitoring domain that allows the expressivity of the different environmental disciplines in a formal and structured way the reuse of an upper level mid level and domain ontologies to improve the interoperability of memon with existing ontologies and the modular representation of the domain by the specification of different modules that allows the independence between modules representing environmental monitoring disciplines while developing memon we consider the clarity of the ontology s structure by choosing a modular conceptualization since the beginning of memon development process to guarantee ontology evolution and maintenance the modularity of memon ensures its reuse as separate modules though memon was built to represent information about environmental monitoring the ontology itself can be employed in other domains and can be used in other contexts for example soil experts could use the environmental material module to deal with soil classification besides the modularity allows the community to expand this work memon can evolve by adding and changing its classes and properties memon reuses reference ontologies top level mid level thus other ontologies can be integrated and reused to enrich memon it can be extended by including new modules for example a service module to manage the access and the treatment of data another example is a source module that can support information about data sources source name data formats data categories besides monitoring applications may extend the presented ontology with other modules such as the actor module itself extended by the emergency responder actor module containing actors e g police and firefighters who may be concerned by environmental monitoring and the emergency response module including emergency response activities moreover memon modules can be reused to develop specific domain modules of a relatively fine granularity for example to go into further details in the environmental processes specific modules such as volcanic activities or hurricane factors can be developed this extension is particularly useful for specific environmental disaster studies besides we can detail environmental material like water by adding information about composition properties and distribution on earth the new module will need classes and relations existing in the environmental material module observation and measurement module and geospatial module the current work offers many challenges and different perspectives one of the main challenges is the maintenance of the ontology a lot of ontologies are built only a few applied attempts have been put into practice and the rest can be considered as obsolete so we need to ensure the updated and the evolvement of memon as perspectives we plan to use the proposed ontology in the near future as support for a novel multi source data integration approach an exciting follow up is to apply memon in real applications such as disaster predictions applications or climate forecast applications through memon classes that constitute the field of the environment and through inter and intra modules relations memon can be useful to generate environmental predictions and recommendations finally given the importance of the aom methodology that guided us to develop memon we recommend it for modular ontology developers 6 software availability the ontology modules described in this paper are available in a complete view at https sites google com view predicat memon the current version of memon is available in owl format to direct download readers may refer to https github com memontology memon the repository contains memon s source code and imported ontologies acknowledgments this research was financially supported by the phc utique program of the french ministry of foreign affairs managed by campus france and the tunisian ministry of higher education and scientific research managed by the cmcu project number 17g1122 code cf 37t03 nj the authors would like to thank the oss experts for their cooperation by providing support domain knowledge and environmental data 
26082,percentile range indexed mapping and evaluation prime a new tool for long term data discovery and application shimelis b dessu a b rené m price a b john s kominoski b c stephen e davis d adam s wymore e william h mcdowell e evelyn e gaiser b c a department of earth and environment florida international university miami fl 33199 usa department of earth and environment florida international university miami fl 33199 usa department of earth and environment florida international university miami fl 33199 usa b southeast environmental research center serc florida international university miami fl 33199 usa southeast environmental research center serc florida international university miami fl 33199 usa southeast environmental research center serc florida international university miami fl 33199 usa c department of biological sciences florida international university miami fl 33199 usa department of biological sciences florida international university miami fl 33199 usa department of biological sciences florida international university miami fl 33199 usa d the everglades foundation palmetto bay fl 33157 usa the everglades foundation palmetto bay fl 33157 usa the everglades foundation palmetto bay fl 33157 usa e department of natural resources and the environment university of new hampshire durham nh 03824 usa department of natural resources and the environment university of new hampshire durham nh 03824 usa department of natural resources and the environment university of new hampshire durham nh 03824 usa corresponding author department of earth and environment florida international university 11200 sw 8th street ahc5 364 miami fl 33199 usa department of earth and environment florida international university 11200 sw 8th street ahc5 364 miami fl 33199 usa percentile range indexed mapping and evaluation prime is a new tool to visualize and quantifying spatio temporal dynamics of long term datasets prime is based on categorical partitioning of magnitude based on user defined indices assigned to ranges of percentile and mapping subsets of data at selected percentiles of long term data indices can reflect attributes such as water management decisions tolerable range of water quality to a species ecological risk response to and recovery from disturbance and values of ecosystem services prime provides visual and robust datascapes and flexibility to evaluate variability in space and time for long term environmental assessment here we demonstrate the utility of prime using 16 years of hydrologic and salinity data from 14 sites representing three unique hydrological systems in the florida coastal everglades fce the resulting prime datascapes reveal interaction between water management and sea level rise to drive salinity levels in the fce keywords percentile ecological mapping prime florida coastal everglades long term ecological research datascape software data availability name of software percentile range indexed mapping and evaluation prime developer shimelis b dessu contact tel 305 401 5898 email sbehailu gmail com availability http go fiu edu prime required hardware pc required software windows 7 or latest and text editor programming language matlab 2018b 1 introduction long term environmental observatory programs have enabled collection of a large amount of hydrological physio chemical and biological data to reveal the complex drivers of ecological change and to inform decision making national and international observatory networks are becoming more common in order to address the spatio temporal context of long term environmental change including the u s long term ecological research lter callahan 1984 hobbie et al 2003 national critical zone observatory czo the global lakes ecological observatory network gleon hanson et al 2018 national ecological observatory network neon dalton 2000 and hundreds of international lter sites kim 2006 generating massive quantities of high resolution environmental data likewise there are many government and regulatory agencies with long term water quality and hydrological monitoring programs across the globe hobbie et al 2003 read et al 2017 many of these platforms now include high frequency sensor based data streams which further magnify the volume of data available to researchers the challenge of environmental monitoring and assessment is now moving from lack or scarcity of data to data mining visualization and big data synthesis carey et al 2015 hamilton et al 2015 read et al 2017 the best use of these data requires novel synthesis methods and statistical tools to perform cross cutting spatio temporal analyses at global regional and local scales to develop and test hypotheses simple yet comprehensive analyses and visualizations are essential to communicate results and findings of long term studies among the scientific community and the public farley et al 2018 here we introduce the development and application of percentile range indexed mapping and evaluation prime a new tool designed to assess long term hydrological and ecological processes develop and test novel hypotheses and facilitate informed decision making as an end product prime creates datascapes which are multi layered semi quantitative highly visual and easy to interpret matrices to compare variability at multiple scales from days to years pre and post events and overall long term patterns the overarching objective of this paper is to present the conceptual framework algorithm and practical application of prime for long term hydrological and ecological data to meet this objective we describe the mathematical benefits and flexibility of percentile curves pcs compared to commonly used statistical approaches to establish mapping indices for the assessment of long term data we demonstrate how a complex eco hydrological system can be mapped to datascapes using prime to explore and understand processes using multiple variables while incorporating the spatiotemporal variability of critical environmental drivers to accomplish this we use 16 years of data from the florida coastal everglades fce collected as part of the fce lter program along two freshwater to marine gradients in everglades national park enp since 2000 2 comparison of commonly used statistical approaches to percentile curves environmental data are often collected and presented as time series of values observed in calendar time step consider two datasets of variables plotted as time series showing their long term relationship with each other fig 1 a such data are often used to test hypotheses using descriptive statistics fig 1b analysis of variance anova fig 1c and regression analysis fig 1d to name a few descriptive statistics summarize the central tendencies e g mean median maximum and minimum and variation e g standard deviation and variance of data integrated over time fig 1b analysis of trends in mean values however can be misleading because natural processes are seldom normally distributed more meaningful insight into long term variation and trends could be generated by quantifying the percent of time that the mean is likely to be observed the 1 factor anova provides a better insight into the variability of data and results are often presented as box plots fig 1c extreme values are often treated as outliers depending on selection and setting of underlying probability distribution used in the 1 factor anova in long term studies the outliers are likely to represent extreme events which can have long lasting effects on the dynamics of the system as such box plots are often used and offer a simple representation of the range of data whiskers along with median first and third quartiles for comparison trend analysis is yet another common method used to visualize e g scatter plots and summarize relationship among multiple data sets with a common attribute such as time fig 1d trends between a response variable are regressed with a set of drivers to determine cause and effect relationships even though linear regressions are quite common many hydrological and ecological relationships display non linearities and analyses may warrant non linear or step wise multi variate regressions trend and regression analyses results however are also prone to the large influence by extreme values and outliers the underlying bias due to extreme values is attributed to the mathematical dependence of these methods on the relative distance deviation of observations since rare and extreme events tend to initiate a lasting response reverberating through a system frequency based approaches have been used to circumvent the limitations of deviation based analyses and capture their contribution in synthesized results percentile curves pc are such a frequency based approach which utilize the linear ranking of values in ascending order to transform data into useable visualizations that are easier to differentiate and interpret fig 1e flow duration curves fdcs are a widely used version of pcs in water resources applications such as water management flood forecasting vogel and fennessey 1995 and water control structures such as reservoirs fdcs are pcs ranked in descending order and illustrate the signature characteristics of the flow regime yilmaz et al 2008 with frequency and duration of occurrence of a specific observation over a sampling list or period qian 2015 exceedance probabilities gunderson 1994 todd et al 2010 and exceedance curves dessu et al 2018 2019 are variants of pcs frequently used to assess environmental compliance e g concentration duration frequency curves u s epa 2005 in order to integrate the frequency component in percentile with magnitude and duration quantification via percentile transform the calendar time steps in to rank based on the percentage of the total number of observations since values and percentiles are monotonically increasing decreasing in pcs the curve can be segmented into mutually exclusive ranges of percentile or value pcs spread values over a percentile range of zero to hundred irrespective of the number and type of data enabling comparison of sub data sets against their long term trend or other relevant data sets these ranges can be assigned descriptive labels based on their value and percentile as extreme high low high low and normal ranges for example the values regarded as outliers observations falling beyond the whiskers in fig 1c can be grouped as extreme low or high on the percentile curve fig 1e hence the percentile curve of the long term data represents the signature characteristics of the system against which all sub datasets can be compared with to assess the long term dynamics 3 percentile range index mapping and evaluation prime 3 1 general prime framework prime utilizes percentile curves pcs to synthesize long term time series datasets by categorically comparing magnitude and frequency of sub datasets against long term observations and trends mapping and evaluation procedures are based on comparison of subsets of the time series at selected percentiles against the intervals of magnitudes from percentile ranges of the long term data prime consists of five steps 1 input and initialization where the user provides data in suitable format and sets mapping parameters fig 2 a 2 process the long term percentile curve and extract baseline values for mapping fig 2b 3 partition the long term data into groups generate their respective percentile curve and extract the value corresponding to defined mapping percentile fig 2c 4 derive status level datascapes fig 2d and 5 evaluate relationships between two corresponding maps fig 2e in this section we present the algorithm of prime fig 2 along with step by step implementation using average daily sea level data collected from january 1 2001 to december 31 2016 at the key west tide gage station holgate et al 2012 psmsl 2016 the sea level is referenced to the north american vertical datum 1988 navd88 to align with water level data discussed in section 4 3 2 input data consider a multi variable time series dataset containing time t variable site name mapping id i j and value in column format time name i j value format fig 2a mapping identifiers define rows and columns of the final datascape output for a selected variable from the dataset prime extracts the data in t name i j var format fig 2a hence the input data is grouped into i x j columns representing individual mapping unit 3 3 percentile ranges and indices prime procedures are based on comparison of percentile curves of subsets of the data defined by the combinations of mapping identifiers against the long term pc percentile curve is defined as 1 p c p x n x n n n 1 100 x n where pc is the percentile curve p xn is the percentile of the nth ranked observation xn of the variable in ascending order n is total number of observations since percentiles range from zero to a hundred the long term pc can be partitioned into mutually exclusive percentile intervals fig 2b defined by user specified cut off percentiles each of these intervals are assigned a unique index to reflect the status of observations falling within the range as 2 p r i i n d e x 1 i n d e x 2 i n d e x n p ω 1 ω 1 p ω 2 p ω n 1 where pri is the set of percentile range indices index is user defined index n is the number of intervals and ω is set of percentiles defining each interval the boundary value of the intervals is extracted from the long term pc for each cut off percentile the boundary of the corresponding long term percentile range values prv ω is extracted the sea level time series data was extracted as date site year month value and the pcs were generated for the long term data implementation of fig 2a and b five percentile intervals are defined by cut off percentiles ω 15 33 67 and 85 percentile range indices pri ll l m h and hh were established representing relative sea level status as lower low ll p 15 low l 15 p 33 median m 33 p 67 high h 67 p 85 and higher high hh p 85 eq 2 fig 2a observations percentile range values prv ω 32 5 cm 26 5 cm 17 2 cm and 9 8 cm corresponding to ω 15th 33rd 67th and 85th percentiles of the long term sea level pc were extracted fig 2b 3 4 datascapes once the long term percentile intervals and indices are established prime partitions the long term data by the mapping id i j fig 2c and generates percentile curves pc i j eq 1 let ω k be the set of k percentiles of interest for mapping and evaluation and pv i j k be the value extracted from pc i j at ω k fig 2c by comparing the long term pv and the mapping unit pv the datascape is given by 3 d s c a p e i j k i n d e x 1 i n d e x 2 i n d e x n p v i j k p r v ω 1 p r v ω 1 p v i j k p r v ω 2 p v i j k p r v ω n 1 where dscape i j k is the datascape with indices index n at row i column j of mapping percentile k prv is the long term value corresponding to the cutoff percentile ω and pv is the percentile value extracted at mapping percentiles the datascape represents the changes in the system at selected percentiles with respect of the long term trend fig 2d prime assigns a unique color for a quick visual understanding the datascape three sets of mapping categories i j were used to map sea level status fig 2c the first map used all years month mapping categories in which all values falling in the same month were combined to make the monthly pc the second map used all months year to map the relative annual status the third mapping category was month year in which pcs of individual month were generated the long term and individual monthly datascape was used to evaluate the seasonal pattern of variables across sites the annual datascape was intended to capture annual variabilities across sites associated with major events such as drought storms and hurricanes that have extended impact on the ecosystem the 33rd average median 50th and 67th percentiles i e ω 1 33 ω 2 mean x ω 3 median 50 and ω 4 67 of the pcs from sub datasets grouped by the mapping categories were extracted and compared with values of the five percentile ranges of the long term data to produce datascape fig 2d pcs for the long term daily sea level and overlays of selected sub datasets extracted by month june year 2009 and month february and october 2009 are shown in fig 3 a percentile range values prv 32 5 cm 26 5 cm 17 2 cm and 9 8 cm corresponding to pri 15th 33rd 67th and 85th percentiles of the long term sea level pc were extracted respectively as shown in fig 3a prime generates 12 pcs for the sea level sub datasets grouped in month category 16 for the year category and 192 pcs for the month year category compared to the five status ranges of the long term data the 33rd and 67th percentiles of the 2009 pc 28 8 cm 15 9 cm fall in image 1 and image 2 ranges of the long term pri respectively fig 3b similarly when the june 2001 2016 prvs at the 33rd and 67th mapping percentiles are extracted and compared with the long term prvs both fall within the range assigned image 3 status index of long term data fig 3c the monthly 33rd and 67th percentiles of sea level observations of 2009 were mapped to image 4 fig 3d in february and image 5 fig 3e in october respectively 3 5 evaluation prime datascapes are multi dimensional maps where the relative location of indices is important in determining the degree of association and correspondence hence common one dimensional rank based correlation such as spearman s rank order correlation coefficient spearman 1904 kendall s rank correlation test kendall 1955 may not fully capture the association among datascapes contingency table and chi square test are widely used methods to assess relationship between maps everitt 1992 and here we employ both contingency table and chi square to measure the degree of correspondence and association between prime datascapes chi square test whether there is a significant relationship between datascapes based on the associated p value after this significance test the degree of correlation between datascapes is measured by contingency coefficient cc as 4 c c χ 2 n χ 2 where cc is the contingency coefficient χ2 is chi square statistics n is the total number of counts and k is the number of rows or columns of a contingency table contingency coefficient values vary between 0 and 1 close to 0 shows little relationship and close to 1 shows strong relationship prime also employs a new non parametric diagonal correspondence coefficient dcc to measure the overall correspondence of datascapes as 5 d c c i j c t i j 0 5 i j 1 c t i j a l l c t i j i j c t i j i j c t f i j i j c t f i j 0 5 i j 1 c t f i j a l l c t f i j o t h e r w i s e where ct i j is the element of row i and column j of the contingency table ct f i j is the flipped contingency table determined as ct f i j ct i j j 1 dcc is the ratio of the sum of all elements along the diagonal i j plus half of the elements above and below the diagonal i 1 j divided by the total sum of the contingency table if the datascapes have an inverse relationship dcc will be negative and is calculated after flipping the contingency table dcc values vary from 1 to 1 and values close to 0 suggest little correspondence positive and negative dcc show direct and inverse correspondence respectively as per the order of indices defined by the percentile range a strong correspondence may be suggested when dcc 1 5k 3k 2 for contingency tables with four or more rows and columns k 4 the distribution and pattern of counts in the contingency table can provide further insight about relationships between datascapes not captured with the methods included in prime however the user should exercise caution in the use and interpretation of contingency table and measures of association statistical inference should be supplemented with site and disciplinary knowledge prime also includes statistical summaries of input data to supplement evaluation and interpretation of datascapes to illustrate the evaluation procedures of prime datascapes consider the 33rd and 67th percentile sea level datascapes as two separate datascapes fig 3d and e the resulting contingency table fig 3f has a total count of n 192 pair wise indices chi square 2 307 9 and p value close to 0 based on the p value the two contingency table statistically significant association between the two datascapes the cc is 0 78 eq 4 suggests a strong association the diagonal correspondence coefficient dcc 98 0 5 0 88 192 0 74 also suggests strong correspondence between the two datascapes eq 5 as expected the 67th percentile sea level datascape has equal or greater status levels compared to the 33rd percentile datascape and as such the contingency table has zero counts below the diagonal 4 application of prime mapping long term changes in water levels and salinity in the florida everglades 4 1 introduction the fce is a large subtropical wetland ecosystem in south florida usa and is the southern most portion of the remaining wetlands of the greater everglades which also include the water conservation areas wca fig 4 managed freshwater canals border the fce to the north and east while the florida keys and the gulf of mexico form the southern and southwestern boundaries respectively fig 4 florida bay is an open water area between mainland florida and the florida keys the fce receives both fresh and saline surface water and groundwater along the boundaries of enp fce is susceptible to a projected slr of 1 2 m by 2100 haigh et al 2014 as much of the landscape is less than 1 5 m above mean sea level titus and richman 2001 the combination of rising sea level with reductions in fresh water flow due to water management has increased salt water intrusion into the fce and its underlying aquifer allowing for salt tolerant communities such as mangroves to overtake formerly freshwater species dessu et al 2018 karamperidou et al 2013 krauss et al 2011 ross et al 2000 an acceleration in sea level rise is expected to increase coastal erosion and soil loss potentially replacing coastal wetlands with non vegetated open water areas todd et al 2012 trenberth et al 2014 white and kaplan 2017 wilson et al 2018 the comprehensive everglades restoration plan cerp was authorized in 2000 to restore the remaining wetland portions of the everglades to its pre drainage conditions and ensure sustainability the long term impact of cerp projects in the fce relies on maintaining suitable quantity and quality of water nutrient flux productivity and hydrologic connectivity sklar et al 2005 continual monitoring and assessment are essential to understand and evaluate restoration identify challenges and opportunities to inform management decisions and mitigate undesirable or unintended consequences here we demonstrate an application of percentile range indexed mapping and evaluation prime to facilitate data synthesis and the assessment of long term environmental restoration in the everglades we apply prime along two main freshwater flow ways of the fce the shark river slough srs and taylor slough ts and within florida bay fb following the fce lter monitoring stations to map the status and dynamics of hydrologic drivers and changes in salinity from 2001 to 2016 fig 4 the specific objectives are to 1 create datascapes describing spatial and temporal changes in water level sea level fresh to marine head difference fmhd and salinity 2 evaluate the impact of water level in freshwater marshes to counteract salinity and 3 isolate signatures of disturbances and their spatial and temporal extent output from our analysis includes long term pcs for annual seasonal and monthly datascapes of water level sea level fmhd and salinity relationships among these variables are then evaluated using contingency tables findings from this application of prime provide a holistic understanding of the hydrologic conditions of fce in a context of how freshwater flow management can be improved through cerp to reduce the negative impacts of slr 4 2 methods 4 2 1 site description freshwater inputs to the fce come from direct rainfall and surface water inflow through gated structures connecting storage in water conservation areas wca with shark river slough srs and taylor slough ts freshwater inflow from wca 3a to srs is managed by the operation of water control structures located along tamiami trail the northern boundary of enp inflow to ts is controlled by a series of detention areas and structures along the eastern boundary of enp fig 4 sites srs1 to srs3 and ts ph1 to ts ph3 are considered freshwater marsh sites dominated by sawgrass fig 4 sites srs4 to srs6 and ts ph6 ts ph7 are brackish water sites dominated by mangroves the region between srs3 and srs4 and ts ph3 and ts ph6 are ecotones transitioning between freshwater and estuarine habitats we use prime to map patterns of water level and sea level and evaluate the implications on salinity at the coastal sites of srs srs3 to srs6 and ts ts ph3 ts ph6 and ts ph7 and the florida bay ts ph9 ts ph10 to ts ph11 sites of the fce lter fig 4 4 2 2 data sixteen years of daily water level data from 2001 to 2016 for each of the fce srs and ts sites were obtained from nearby stations np201 srs1 p36 srs2 mo 215 srs3 shark river srs5 te srs4 nts1 ts ph1 tsb ts ph2 e146 ts ph3 upstream taylor river utr ts ph6 and taylor river mouth trm ts ph7 operated by the united states geological survey usgs and the everglades depth estimation network eden project usgs eden 2019 figs 4 and 5 a b daily sea level data from the key west station operated by the national oceanic and atmospheric administration noaa was used due to its comparatively longer and continuous daily sea level data available since 1913 holgate et al 2012 psmsl 2016 fig 5a and b tidal gage sea level data were converted to the 1988 north america vertical datum navd88 for comparison with water level data fmhd is determined by subtracting sea level from water levels at the most downstream freshwater site srs3 for shark river slough and ts ph3 for taylor slough respectively fig 5c and d surface water salinity data were obtained from fce lter monitoring stations in srs srs3 to srs6 gaiser and childers 2017 fig 5c ts sites ts ph3 ts ph6 and ts ph7 troxler 2017 fig 5d and florida bay ts ph9 ts ph10 and ts ph11 briceno 2017 fourqurean 2017 fig 5e surface water salinity data were three day composites of water samples collected at each of those sites 4 2 3 prime setup and output long term pcs were generated for all variables based on eq 1 fig 2a and b five percentile range indices pri were established representing the status level of the variable as lower low ll p 15 segment observations low observations l 15 p 33 observations within 17 of the median m 33 p 67 high observations h 67 p 85 and observations exceeding the 85th percentile and termed higher high hh p 85 implementation of eq 2 fig 2b these pri s were applied across all variables the pri selection criteria were 1 statistical margins based on standard deviation of approximate normal distribution at 0 5σ and 1σ 2 to provide sufficient flexibility to use the datascape for ecological assessment such as frequency of exposure to ranges of salinity 3 to capture spectrum of seasonal variability and 4 to extract mapping percentiles for data sets as small as 9 observations e g water quality sampling interval for srs4 to srs6 is a 3 day composite that translates to a maximum of 10 observations per month as all the variables used in this application are in the same daily time step and assessed with similar user defined criteria the example procedures employed to map sea level in section 3 3 are used to setup input output and evaluation using prime salinity responses of srs4 and ts ph6 to changes in freshwater level fmhd and sea level were evaluated using contingency tables based on the prime datascapes south florida experiences two seasons a summer time wet season may to october and a winter time dry season november to april as wet season is characterized by low salinity and high water level sea level and fmhd fig 5 the 33rd percentile salt scapes are compared with 67th percentile hydro scapes and vice versa 4 3 results 4 3 1 long term spatial variability the observed gap among water level pcs reflected the relative ground elevations of the individual stations along the freshwater flow direction fig 6 a in srs and ts the water level pcs for sites srs4 5 as well as ts ph3 6 7 tended to cluster together following a similar pattern as sea level long term water level pcs were above sea level except at ts ph3 water levels at srs3 and ts ph3 were below their respective mean ground level for about 10 of the time fig 6a water levels at ts ph3 were below sea level for 5 of the time sea levels exceeded mean ground levels of srs3 and ts ph3 for 15 and 75 of the time fig 6a water level trends at srs5 and ts ph7 are relatively similar but srs4 was consistently lower than ts ph6 by 5 cm compared to the sea ward hydraulic gradient of water levels in srs the order of water level pcs between ts ph3 and ts ph6 interchanged at the lower 35 percentile and higher 90 percentile ends fig 6a fmhd in srs was higher compared to ts ph3 fig 6b salinity levels were below 2 psμ 95 of the time at srs3 and below 10 psμ for about two thirds of the time at srs4 fig 6c conversely salinity at fb site ts ph10 ranged between 25 and 50 psμ the seasonal variability of salinity was higher at srs4 and ts ph6 compared to srs5 and ts ph7 their respective downstream sites compared to srs ts experiences extreme high and low salinity levels fig 6c the maximum salinity levels in srs and ts are bounded by salinity levels at srs6 and ts ph10 respectively despite their relatively close geographic proximity the gap between salinity pcs at ts ph6 and ts ph7 increased steadily up to the 65th percentile after which the gap decreased rapidly while the salinity values at both sites approached those observed at fb ts ph6 displayed the highest rate of rise in salinity after the 65th percentile from 10 psμ to 50 psμ fig 6c the long term averages and values at percentiles defining the range of indices are listed in table 1 4 3 2 cross site annual and seasonal datascapes 4 3 2 1 hydro scapes water level sea level and fmhd annual water levels and sea level showed increasing trends from 2001 to 2016 fig 7 a these temporal patterns is reflected in the hydro scapes as change in status levels from l to m and m to h relatively dry years e g 2001 2005 2007 2011 and 2015 exhibited either ll l status in freshwater level at the 33rd percentile and l status at the mean and median high h water level status was observed at the 33rd percentile in 2016 h hh water level status became dominant at most srs and ts sites at the median and 67th percentile between 2012 and 2016 fig 7a h hh water level status was most frequently observed in 2016 the fmhd status levels were l m at the mean and 33rd percentile and m h at 67th percentile for both srs and ts unusually low and high fmhd status levels were observed in srs in 2015 and 2005 respectively fig 7a the monthly hydro scape across all sites showed strong seasonality with the l ll and h hh status generally aligned with the dry and wet months respectively fig 7b water levels and sea level status were h in september and october at the 33rd percentile and h hh from august to november at 67th percentile fig 7b water levels at the 33rd percentile indicated a three month lag in ll status at the upstream freshwater sites compared to the salt water sites in both srs and ts fig 7b the fmhd for both srs and ts tended to be lowest between march and june at the 33rd percentile and between april and may at the 67th percentile fig 7b 4 3 2 2 salt scape annual salinity generally shifted from l to m to h in srs and ts at the 33rd percentile fig 7c between 2001 and 2015 the years 2008 and 2015 have either h or hh salinity status at the 67th percentile for all sites except ts ph10 fig 7c the months of march through june corresponded with h and or hh salinity conditions in both srs and ts at both percentiles mapped fig 7d higher salinity status was observed in the fb sites between the months of march and august with h status extending in to wet months at the 67th percentile fig 7d 4 3 3 temporal month year datascapes dry months from march to june were characterized by low fmhd and high salinity whereas august to november was dominantly high fmhd and low salinity both in the 33rd and 67th percentile fig 8 in general salinity levels were h hh in the dry season from march to july at srs4 and ts ph6 fig 8b and c among the sixteen years 2015 had the longest l ll fmhd status 10 months that extended into the wet season corresponding to seven months of h hh salinity status in general the fmhd from july to november decreased over the sixteen years fig 8 4 3 4 evaluation of datascapes relationships between the month year saltscapes at srs4 and ts ph6 with sea level water level and fmhd were evaluated see table 2 salinity at srs4 and ts ph6 showed strong association and inverse correspondence with water levels cc 0 5 and fmhd dcc 0 58 table 2 sea levels at key west and salinity levels at srs4 were weakly associated there was no statistically significant association between salinity at ts ph6 and sea level p value 0 05 table 2 fmhd had a stronger association with salinity at ts ph6 compared to freshwater levels at ts ph3 overall fmhd had a consistent level of association with salinity at srs4 and ts ph6 water levels at ts ph3 showed a marginally better association with sea level compared to srs3 table 2 4 4 discussion and interpretation 4 4 1 prime is a synthesis platform for long term data sets with increasing volumes of data exclusive use of descriptive statistical summaries even if they include estimates of variance over certain time intervals may mask the impact of disturbances e g hurricanes drought storm water surges and saltwater intrusion etc or other sources of abrupt shifts fce lter has been collecting long term hydrologic and ecological data along the srs ts and fb transects of the enp since 2000 figs 4 5 and table 1 results of prime extend previous fce data synthesis efforts e g childers 2006 davis et al 2018 dessu et al 2018 kelble et al 2007 and provide novel insight to the underlying hydrologic and salinity pattern at individual sites as well as potential connectivity across the three fce transects for the five variables investigated long term pcs captured signature pattern of the specific variables at each site prime datascapes visualize patterns in finer resolutions to capture processes of interest identify inherent relationships and correlations across variables or sites 4 4 2 long term pcs show signature patterns of water level and salinity across fce lter sites compared to the monthly average time series plots fig 5 hydrologic and salinity pcs provided a clearer picture of underlying patterns fig 6 at the site for instance water levels at srs3 and ts ph3 experienced similar percentiles of wetness and dryness with water levels at or above their respective ground surface elevations for at least 90 of the daily observations from 2001 to 2016 fig 6a vulnerability to sea level rise can be inferred from the long term pcs when sea level exceeded the ground surface elevation at a site which was 75 and 15 of the daily observations at ts ph3 and srs3 respectively fig 6a extended draw down in water levels either due to drought or a decrease in freshwater inputs could expose ts to increasing coastal erosion replacing coastal wetlands with non vegetated open water areas ross et al 2000 todd et al 2012 white and kaplan 2017 with the average sea level rise of 7 7 4 3 mm year from 2001 to 2016 at key west dessu et al 2018 the estuarine region of ts could be below sea level within ten to fifteen years as expected salinity increased downstream and with distance from freshwater marshes of srs and ts fig 6b interestingly the srs estuaries experience a narrow range of salinity between srs4 and srs6 over a large spatial extent compared to the wide range of salinity over a much shorter distance between ts ph6 and ts ph7 the change in the salinity pc gradient particularly at ts ph6 above a percentile of 60 may be attributed to a seasonal change in hydrologic connectivity the combined effect of a negative fmhd and higher water levels at estuarine site ts ph6 compared to the upstream site ts ph3 may facilitate salt water intrusion and increased residence time at ts ph6 resulting in not only the rapid increase in salinity but also the hypersaline conditions observed above a percentile of 95 fig 6c discharge of brackish groundwater within the region of ts ph6 and ts ph7 also account for the higher salinity observed at those sites price et al 2006 small spikes in salinity observed at srs3 are driven by the higher tidal influence in srs smith and mccormick 2001 4 4 3 fmhd can inform freshwater delivery to coastal everglades fmhd along shark river and taylor sloughs of enp measures landscape level vulnerability of downstream freshwater marshes through the oligohaline ecotone to reductions in freshwater flow caused by drought and water management and sea level rise recent work demonstrated that a combination of salt exposure and dry down makes fce systems particularly vulnerable to peat collapse dessu et al 2018 wilson et al 2018 fmhd pcs show the hydraulic gradient for freshwater marshes of srs and ts relative to sea level fig 6b the higher positive fmhd in srs may provide a higher hydraulic head to drive fresh surface and groundwater toward the coastal estuaries since fmhd is below zero in ts 15 of the time all ll levels of ts fmhd datascapes represent sea level higher than freshwater levels at ts ph3 ts estuaries are at exposure to salt water intrusion at least 33 of the time in april and 50 of the time in may fig 7b prime datascapes also highlighted temporal and spatial hot spots for alternative mitigation and management options the 2 3 months lag between the lowest freshwater levels relative to sea level corresponded with the lowest fmhd between april and june fig 7b this difference in status level is reflected in the fmhd scapes as l ll status the dry season freshwater marsh water levels in srs and ts lag by one status level from sea level in april and may resulting in extreme low fmhd and high salinity levels redistributing freshwater deliveries from august and september to april and may might increase the fmhd and reduce the extreme high salinities in those later dry season months compared to srs freshwater deliveries to ts may need to consider the prevailing salinity conditions and the change in salinity over a short distance to provide enough residence time to freshwater deliveries water levels in the freshwater marsh sites of fce depend on the amount of freshwater delivery dessu et al 2018 karamperidou et al 2013 hence the relationship between freshwater levels and salinity can indicate the effects of water management on salinity levels monthly salt scapes at srs4 displayed a better correspondence with the water levels at srs3 over the sixteen year period table 2 however it is important to factor in the effect of sea level rise which will continue to reduce the effect of freshwater levels on salinity the level of correspondence between freshwater marsh water levels srs3 and ts ph3 and sea level dcc 0 65 table 2 indicates a possible mismatch in timing of water level and sea level patterns comparison of the long term monthly and annual hydro scapes fig 6a and c with salt scapes fig 6b and d showed that fmhd is a consistent predictor of salinity status in srs and ts compared to either freshwater levels or sea level 4 4 4 salinity in florida bay is influenced by freshwater levels in srs and ts prime datascapes also suggest a potential correlation between salinity levels in fb with fmhd of srs and ts even though the effect of freshwater deliveries in ts and salinity levels in fb has been widely documented nuttle et al 2000 swart and price 2002 the link of water levels in srs and salinity levels in fb is not yet clearly established marshall et al 2011 the annual and seasonal salt scapes of fb indicated a correspondence of high salinity levels at 67th percentile with low fmhd of either srs or ts fig 7a and c the monthly salt scapes of fb also showed a similar correspondence but lagged by one or two months fig 7b and d hence the high salinity levels in fb may be reduced by maintaining suitable fmhd to drive fresh surface and groundwater flow from ts and srs to fb 4 4 5 prime datascapes facilitate detection and tracing of disturbance extreme disturbances and their impact can be detected as a single or successive hot spot or nonconformity in prime datascapes droughts and hurricanes tropical storms are major disturbances on the hydrology and water quality of the everglades davis et al 2018 the impact of hurricanes and tropical storms in 2003 2005 and 2008 is observed in the prime results with higher water levels in those annual hydro scapes and high levels of fmhd at srs4 fig 8a freshwater delivery to the srs and ts also depends on the anticipation or occurrence of drought and hurricanes dessu et al 2018 light and dineen 1994 the anticipation of either a strong or weak hurricane complicates water management and freshwater delivery to the everglades hurricanes or storm events that occur during a period of anticipated drought or a passing hurricane changing course after water levels in reservoirs are brought to low levels may disturb the normal function of the everglades the excess freshwater from hurricanes helps to reduce salinity if stored and released in the following dry season based on the monthly salinity status pattern low and high salinity levels are expected from august to november and march to may respectively high h hh salinity levels from august to february may suggest disturbance due to either lack of freshwater delivery or extended drought conditions e g 2005 2007 2008 2011 and 2015 dessu et al 2018 the dry condition in 2007 induced low water levels and fmhd that extended into 2008 until tropical storm fay august 2008 resulting in a six month streak of high salinity status at srs4 and ts ph6 between the months of january and september 2008 fig 8 a period of m status months in the wet season at the 33rd percentile 2007 and 2015 suggests prolonged exposure of the system to above normally expected l ll salinity levels prolonged salinity may induce a period of stress on the ecosystem that would have had relief from the seasonal flushing similarly low salinity levels ll l m in the dry season e g 2007 and 2016 may indicate high freshwater delivery due to excess freshwater stored in the upstream reservoirs from a hurricane or storm in the previous wet season or expectation of a coming wetter year prime datascapes provided a visual representation to detect and quantify disturbances across sites fig 7 the change in hydro scapes from 2011 to 2012 stands out across srs and ts as water levels and sea level each increased by at least one status level at the 33rd and 67th percentiles fig 7a with a corresponding decrease in salinity level by at least a level or two fig 7c the salinity levels of 2015 were unprecedented with high salinity levels observed in srs and ts particularly at the 33rd percentile fig 7c whereas 2016 was characterized by the highest water levels and sea level and low salinity levels fig 7d 4 4 6 prime can be used to evaluate potential benefits from everglades restoration scenarios the overall increasing trend of water levels in the freshwater marsh sites suggests progress towards restoring the pre development flow conditions mcvoy et al 2011 sklar et al 2005 as well as preserving the natural landscape of the everglades nungesser et al 2015 from a management and restoration perspective understanding how increased freshwater flow through the everglades can offset the adverse effects of sea level rise maintaining suitable salinity conditions throughout the ecotone and providing for the freshwater needs of downstream estuaries is critical for protecting and sustaining these resources for the future while increasing freshwater levels can help to increase fmhd and combat sea water intrusion the estuarine wetlands vulnerability to rising water levels needs to be factored in water delivery decision process prime results have demonstrated the potential of using the fmhd variable to capture the benefits of increased freshwater delivery relative to sea level rise along srs ts and fb multiple cerp projects are being implemented towards restoration of the everglades and improving freshwater delivery prime datascapes from modeled restoration scenarios can help to compare benefits across plans allowing for an understanding of increments of freshwater flow needed to offset recent rates of sea level rise hence the potential benefits of cerp projects can be evaluated by mapping the fmhd datascapes from model simulation of cerp scenarios and comparing with the baseline historical relationship of fmhd and salinity datascapes 5 conclusions the use of time series analyses has become part of the statistical tool kit of many earth and environmental scientists this is the direct result of the large volumes of data now available through hydrological and environmental observatory platforms including the lter network critical zone observatories czos and the national ecological observatory network neon the need for effective statistically robust and easily interpretable time series analyses is now required to compare patterns within and across these rich data sets and environmental gradients as a direct result of this need throughout the earth and environmental science community we developed prime as a tool based on the statistical advantages and flexibility of percentiles prime results can directly assist environmental assessment and monitoring by comparing both magnitude and recurrence prime produces visual datascapes summarizing a series of comprehensive analyses datascapes also provide a visual output that can be used to communicate results and findings to stakeholders and which demonstrate the importance of maintaining long term observatories and data repositories we demonstrate how a complex hydrological system such as the fce can be mapped using prime to explore and understand processes across variables time and space we advocate the use of prime as a new statistical tool to support cross cutting spatio temporal analyses at global regional and local scales and build comparative relationships of long term ecological and hydrological studies acknowledgment this material is based upon work supported by the national science foundation nsf through the florida coastal everglades long term ecological research program under grant no deb 9910514 2000 2006 and grant no dbi 0620409 2007 2012 and grant no deb 1237517 dec 2012 2018 support was also provided by the nsf under grant deb 1545288 through the long term ecological research network communications office lnco and the national center for ecological analysis and synthesis nceas additional support for this work was also provided by the everglades foundation the southeast environmental research center and the department of earth and environment of the school of the environment arts and society of the college of arts sciences and education of florida international university this is contribution number 932 from the southeast environmental research center in the institute of water and environment at florida international university appendix a supplementary data the following are the supplementary data to this article prime supplementary files and interface multimedia component 1 water level sea level and fmhd data multimedia component 3 salinity data multimedia component 3 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104580 
26082,percentile range indexed mapping and evaluation prime a new tool for long term data discovery and application shimelis b dessu a b rené m price a b john s kominoski b c stephen e davis d adam s wymore e william h mcdowell e evelyn e gaiser b c a department of earth and environment florida international university miami fl 33199 usa department of earth and environment florida international university miami fl 33199 usa department of earth and environment florida international university miami fl 33199 usa b southeast environmental research center serc florida international university miami fl 33199 usa southeast environmental research center serc florida international university miami fl 33199 usa southeast environmental research center serc florida international university miami fl 33199 usa c department of biological sciences florida international university miami fl 33199 usa department of biological sciences florida international university miami fl 33199 usa department of biological sciences florida international university miami fl 33199 usa d the everglades foundation palmetto bay fl 33157 usa the everglades foundation palmetto bay fl 33157 usa the everglades foundation palmetto bay fl 33157 usa e department of natural resources and the environment university of new hampshire durham nh 03824 usa department of natural resources and the environment university of new hampshire durham nh 03824 usa department of natural resources and the environment university of new hampshire durham nh 03824 usa corresponding author department of earth and environment florida international university 11200 sw 8th street ahc5 364 miami fl 33199 usa department of earth and environment florida international university 11200 sw 8th street ahc5 364 miami fl 33199 usa percentile range indexed mapping and evaluation prime is a new tool to visualize and quantifying spatio temporal dynamics of long term datasets prime is based on categorical partitioning of magnitude based on user defined indices assigned to ranges of percentile and mapping subsets of data at selected percentiles of long term data indices can reflect attributes such as water management decisions tolerable range of water quality to a species ecological risk response to and recovery from disturbance and values of ecosystem services prime provides visual and robust datascapes and flexibility to evaluate variability in space and time for long term environmental assessment here we demonstrate the utility of prime using 16 years of hydrologic and salinity data from 14 sites representing three unique hydrological systems in the florida coastal everglades fce the resulting prime datascapes reveal interaction between water management and sea level rise to drive salinity levels in the fce keywords percentile ecological mapping prime florida coastal everglades long term ecological research datascape software data availability name of software percentile range indexed mapping and evaluation prime developer shimelis b dessu contact tel 305 401 5898 email sbehailu gmail com availability http go fiu edu prime required hardware pc required software windows 7 or latest and text editor programming language matlab 2018b 1 introduction long term environmental observatory programs have enabled collection of a large amount of hydrological physio chemical and biological data to reveal the complex drivers of ecological change and to inform decision making national and international observatory networks are becoming more common in order to address the spatio temporal context of long term environmental change including the u s long term ecological research lter callahan 1984 hobbie et al 2003 national critical zone observatory czo the global lakes ecological observatory network gleon hanson et al 2018 national ecological observatory network neon dalton 2000 and hundreds of international lter sites kim 2006 generating massive quantities of high resolution environmental data likewise there are many government and regulatory agencies with long term water quality and hydrological monitoring programs across the globe hobbie et al 2003 read et al 2017 many of these platforms now include high frequency sensor based data streams which further magnify the volume of data available to researchers the challenge of environmental monitoring and assessment is now moving from lack or scarcity of data to data mining visualization and big data synthesis carey et al 2015 hamilton et al 2015 read et al 2017 the best use of these data requires novel synthesis methods and statistical tools to perform cross cutting spatio temporal analyses at global regional and local scales to develop and test hypotheses simple yet comprehensive analyses and visualizations are essential to communicate results and findings of long term studies among the scientific community and the public farley et al 2018 here we introduce the development and application of percentile range indexed mapping and evaluation prime a new tool designed to assess long term hydrological and ecological processes develop and test novel hypotheses and facilitate informed decision making as an end product prime creates datascapes which are multi layered semi quantitative highly visual and easy to interpret matrices to compare variability at multiple scales from days to years pre and post events and overall long term patterns the overarching objective of this paper is to present the conceptual framework algorithm and practical application of prime for long term hydrological and ecological data to meet this objective we describe the mathematical benefits and flexibility of percentile curves pcs compared to commonly used statistical approaches to establish mapping indices for the assessment of long term data we demonstrate how a complex eco hydrological system can be mapped to datascapes using prime to explore and understand processes using multiple variables while incorporating the spatiotemporal variability of critical environmental drivers to accomplish this we use 16 years of data from the florida coastal everglades fce collected as part of the fce lter program along two freshwater to marine gradients in everglades national park enp since 2000 2 comparison of commonly used statistical approaches to percentile curves environmental data are often collected and presented as time series of values observed in calendar time step consider two datasets of variables plotted as time series showing their long term relationship with each other fig 1 a such data are often used to test hypotheses using descriptive statistics fig 1b analysis of variance anova fig 1c and regression analysis fig 1d to name a few descriptive statistics summarize the central tendencies e g mean median maximum and minimum and variation e g standard deviation and variance of data integrated over time fig 1b analysis of trends in mean values however can be misleading because natural processes are seldom normally distributed more meaningful insight into long term variation and trends could be generated by quantifying the percent of time that the mean is likely to be observed the 1 factor anova provides a better insight into the variability of data and results are often presented as box plots fig 1c extreme values are often treated as outliers depending on selection and setting of underlying probability distribution used in the 1 factor anova in long term studies the outliers are likely to represent extreme events which can have long lasting effects on the dynamics of the system as such box plots are often used and offer a simple representation of the range of data whiskers along with median first and third quartiles for comparison trend analysis is yet another common method used to visualize e g scatter plots and summarize relationship among multiple data sets with a common attribute such as time fig 1d trends between a response variable are regressed with a set of drivers to determine cause and effect relationships even though linear regressions are quite common many hydrological and ecological relationships display non linearities and analyses may warrant non linear or step wise multi variate regressions trend and regression analyses results however are also prone to the large influence by extreme values and outliers the underlying bias due to extreme values is attributed to the mathematical dependence of these methods on the relative distance deviation of observations since rare and extreme events tend to initiate a lasting response reverberating through a system frequency based approaches have been used to circumvent the limitations of deviation based analyses and capture their contribution in synthesized results percentile curves pc are such a frequency based approach which utilize the linear ranking of values in ascending order to transform data into useable visualizations that are easier to differentiate and interpret fig 1e flow duration curves fdcs are a widely used version of pcs in water resources applications such as water management flood forecasting vogel and fennessey 1995 and water control structures such as reservoirs fdcs are pcs ranked in descending order and illustrate the signature characteristics of the flow regime yilmaz et al 2008 with frequency and duration of occurrence of a specific observation over a sampling list or period qian 2015 exceedance probabilities gunderson 1994 todd et al 2010 and exceedance curves dessu et al 2018 2019 are variants of pcs frequently used to assess environmental compliance e g concentration duration frequency curves u s epa 2005 in order to integrate the frequency component in percentile with magnitude and duration quantification via percentile transform the calendar time steps in to rank based on the percentage of the total number of observations since values and percentiles are monotonically increasing decreasing in pcs the curve can be segmented into mutually exclusive ranges of percentile or value pcs spread values over a percentile range of zero to hundred irrespective of the number and type of data enabling comparison of sub data sets against their long term trend or other relevant data sets these ranges can be assigned descriptive labels based on their value and percentile as extreme high low high low and normal ranges for example the values regarded as outliers observations falling beyond the whiskers in fig 1c can be grouped as extreme low or high on the percentile curve fig 1e hence the percentile curve of the long term data represents the signature characteristics of the system against which all sub datasets can be compared with to assess the long term dynamics 3 percentile range index mapping and evaluation prime 3 1 general prime framework prime utilizes percentile curves pcs to synthesize long term time series datasets by categorically comparing magnitude and frequency of sub datasets against long term observations and trends mapping and evaluation procedures are based on comparison of subsets of the time series at selected percentiles against the intervals of magnitudes from percentile ranges of the long term data prime consists of five steps 1 input and initialization where the user provides data in suitable format and sets mapping parameters fig 2 a 2 process the long term percentile curve and extract baseline values for mapping fig 2b 3 partition the long term data into groups generate their respective percentile curve and extract the value corresponding to defined mapping percentile fig 2c 4 derive status level datascapes fig 2d and 5 evaluate relationships between two corresponding maps fig 2e in this section we present the algorithm of prime fig 2 along with step by step implementation using average daily sea level data collected from january 1 2001 to december 31 2016 at the key west tide gage station holgate et al 2012 psmsl 2016 the sea level is referenced to the north american vertical datum 1988 navd88 to align with water level data discussed in section 4 3 2 input data consider a multi variable time series dataset containing time t variable site name mapping id i j and value in column format time name i j value format fig 2a mapping identifiers define rows and columns of the final datascape output for a selected variable from the dataset prime extracts the data in t name i j var format fig 2a hence the input data is grouped into i x j columns representing individual mapping unit 3 3 percentile ranges and indices prime procedures are based on comparison of percentile curves of subsets of the data defined by the combinations of mapping identifiers against the long term pc percentile curve is defined as 1 p c p x n x n n n 1 100 x n where pc is the percentile curve p xn is the percentile of the nth ranked observation xn of the variable in ascending order n is total number of observations since percentiles range from zero to a hundred the long term pc can be partitioned into mutually exclusive percentile intervals fig 2b defined by user specified cut off percentiles each of these intervals are assigned a unique index to reflect the status of observations falling within the range as 2 p r i i n d e x 1 i n d e x 2 i n d e x n p ω 1 ω 1 p ω 2 p ω n 1 where pri is the set of percentile range indices index is user defined index n is the number of intervals and ω is set of percentiles defining each interval the boundary value of the intervals is extracted from the long term pc for each cut off percentile the boundary of the corresponding long term percentile range values prv ω is extracted the sea level time series data was extracted as date site year month value and the pcs were generated for the long term data implementation of fig 2a and b five percentile intervals are defined by cut off percentiles ω 15 33 67 and 85 percentile range indices pri ll l m h and hh were established representing relative sea level status as lower low ll p 15 low l 15 p 33 median m 33 p 67 high h 67 p 85 and higher high hh p 85 eq 2 fig 2a observations percentile range values prv ω 32 5 cm 26 5 cm 17 2 cm and 9 8 cm corresponding to ω 15th 33rd 67th and 85th percentiles of the long term sea level pc were extracted fig 2b 3 4 datascapes once the long term percentile intervals and indices are established prime partitions the long term data by the mapping id i j fig 2c and generates percentile curves pc i j eq 1 let ω k be the set of k percentiles of interest for mapping and evaluation and pv i j k be the value extracted from pc i j at ω k fig 2c by comparing the long term pv and the mapping unit pv the datascape is given by 3 d s c a p e i j k i n d e x 1 i n d e x 2 i n d e x n p v i j k p r v ω 1 p r v ω 1 p v i j k p r v ω 2 p v i j k p r v ω n 1 where dscape i j k is the datascape with indices index n at row i column j of mapping percentile k prv is the long term value corresponding to the cutoff percentile ω and pv is the percentile value extracted at mapping percentiles the datascape represents the changes in the system at selected percentiles with respect of the long term trend fig 2d prime assigns a unique color for a quick visual understanding the datascape three sets of mapping categories i j were used to map sea level status fig 2c the first map used all years month mapping categories in which all values falling in the same month were combined to make the monthly pc the second map used all months year to map the relative annual status the third mapping category was month year in which pcs of individual month were generated the long term and individual monthly datascape was used to evaluate the seasonal pattern of variables across sites the annual datascape was intended to capture annual variabilities across sites associated with major events such as drought storms and hurricanes that have extended impact on the ecosystem the 33rd average median 50th and 67th percentiles i e ω 1 33 ω 2 mean x ω 3 median 50 and ω 4 67 of the pcs from sub datasets grouped by the mapping categories were extracted and compared with values of the five percentile ranges of the long term data to produce datascape fig 2d pcs for the long term daily sea level and overlays of selected sub datasets extracted by month june year 2009 and month february and october 2009 are shown in fig 3 a percentile range values prv 32 5 cm 26 5 cm 17 2 cm and 9 8 cm corresponding to pri 15th 33rd 67th and 85th percentiles of the long term sea level pc were extracted respectively as shown in fig 3a prime generates 12 pcs for the sea level sub datasets grouped in month category 16 for the year category and 192 pcs for the month year category compared to the five status ranges of the long term data the 33rd and 67th percentiles of the 2009 pc 28 8 cm 15 9 cm fall in image 1 and image 2 ranges of the long term pri respectively fig 3b similarly when the june 2001 2016 prvs at the 33rd and 67th mapping percentiles are extracted and compared with the long term prvs both fall within the range assigned image 3 status index of long term data fig 3c the monthly 33rd and 67th percentiles of sea level observations of 2009 were mapped to image 4 fig 3d in february and image 5 fig 3e in october respectively 3 5 evaluation prime datascapes are multi dimensional maps where the relative location of indices is important in determining the degree of association and correspondence hence common one dimensional rank based correlation such as spearman s rank order correlation coefficient spearman 1904 kendall s rank correlation test kendall 1955 may not fully capture the association among datascapes contingency table and chi square test are widely used methods to assess relationship between maps everitt 1992 and here we employ both contingency table and chi square to measure the degree of correspondence and association between prime datascapes chi square test whether there is a significant relationship between datascapes based on the associated p value after this significance test the degree of correlation between datascapes is measured by contingency coefficient cc as 4 c c χ 2 n χ 2 where cc is the contingency coefficient χ2 is chi square statistics n is the total number of counts and k is the number of rows or columns of a contingency table contingency coefficient values vary between 0 and 1 close to 0 shows little relationship and close to 1 shows strong relationship prime also employs a new non parametric diagonal correspondence coefficient dcc to measure the overall correspondence of datascapes as 5 d c c i j c t i j 0 5 i j 1 c t i j a l l c t i j i j c t i j i j c t f i j i j c t f i j 0 5 i j 1 c t f i j a l l c t f i j o t h e r w i s e where ct i j is the element of row i and column j of the contingency table ct f i j is the flipped contingency table determined as ct f i j ct i j j 1 dcc is the ratio of the sum of all elements along the diagonal i j plus half of the elements above and below the diagonal i 1 j divided by the total sum of the contingency table if the datascapes have an inverse relationship dcc will be negative and is calculated after flipping the contingency table dcc values vary from 1 to 1 and values close to 0 suggest little correspondence positive and negative dcc show direct and inverse correspondence respectively as per the order of indices defined by the percentile range a strong correspondence may be suggested when dcc 1 5k 3k 2 for contingency tables with four or more rows and columns k 4 the distribution and pattern of counts in the contingency table can provide further insight about relationships between datascapes not captured with the methods included in prime however the user should exercise caution in the use and interpretation of contingency table and measures of association statistical inference should be supplemented with site and disciplinary knowledge prime also includes statistical summaries of input data to supplement evaluation and interpretation of datascapes to illustrate the evaluation procedures of prime datascapes consider the 33rd and 67th percentile sea level datascapes as two separate datascapes fig 3d and e the resulting contingency table fig 3f has a total count of n 192 pair wise indices chi square 2 307 9 and p value close to 0 based on the p value the two contingency table statistically significant association between the two datascapes the cc is 0 78 eq 4 suggests a strong association the diagonal correspondence coefficient dcc 98 0 5 0 88 192 0 74 also suggests strong correspondence between the two datascapes eq 5 as expected the 67th percentile sea level datascape has equal or greater status levels compared to the 33rd percentile datascape and as such the contingency table has zero counts below the diagonal 4 application of prime mapping long term changes in water levels and salinity in the florida everglades 4 1 introduction the fce is a large subtropical wetland ecosystem in south florida usa and is the southern most portion of the remaining wetlands of the greater everglades which also include the water conservation areas wca fig 4 managed freshwater canals border the fce to the north and east while the florida keys and the gulf of mexico form the southern and southwestern boundaries respectively fig 4 florida bay is an open water area between mainland florida and the florida keys the fce receives both fresh and saline surface water and groundwater along the boundaries of enp fce is susceptible to a projected slr of 1 2 m by 2100 haigh et al 2014 as much of the landscape is less than 1 5 m above mean sea level titus and richman 2001 the combination of rising sea level with reductions in fresh water flow due to water management has increased salt water intrusion into the fce and its underlying aquifer allowing for salt tolerant communities such as mangroves to overtake formerly freshwater species dessu et al 2018 karamperidou et al 2013 krauss et al 2011 ross et al 2000 an acceleration in sea level rise is expected to increase coastal erosion and soil loss potentially replacing coastal wetlands with non vegetated open water areas todd et al 2012 trenberth et al 2014 white and kaplan 2017 wilson et al 2018 the comprehensive everglades restoration plan cerp was authorized in 2000 to restore the remaining wetland portions of the everglades to its pre drainage conditions and ensure sustainability the long term impact of cerp projects in the fce relies on maintaining suitable quantity and quality of water nutrient flux productivity and hydrologic connectivity sklar et al 2005 continual monitoring and assessment are essential to understand and evaluate restoration identify challenges and opportunities to inform management decisions and mitigate undesirable or unintended consequences here we demonstrate an application of percentile range indexed mapping and evaluation prime to facilitate data synthesis and the assessment of long term environmental restoration in the everglades we apply prime along two main freshwater flow ways of the fce the shark river slough srs and taylor slough ts and within florida bay fb following the fce lter monitoring stations to map the status and dynamics of hydrologic drivers and changes in salinity from 2001 to 2016 fig 4 the specific objectives are to 1 create datascapes describing spatial and temporal changes in water level sea level fresh to marine head difference fmhd and salinity 2 evaluate the impact of water level in freshwater marshes to counteract salinity and 3 isolate signatures of disturbances and their spatial and temporal extent output from our analysis includes long term pcs for annual seasonal and monthly datascapes of water level sea level fmhd and salinity relationships among these variables are then evaluated using contingency tables findings from this application of prime provide a holistic understanding of the hydrologic conditions of fce in a context of how freshwater flow management can be improved through cerp to reduce the negative impacts of slr 4 2 methods 4 2 1 site description freshwater inputs to the fce come from direct rainfall and surface water inflow through gated structures connecting storage in water conservation areas wca with shark river slough srs and taylor slough ts freshwater inflow from wca 3a to srs is managed by the operation of water control structures located along tamiami trail the northern boundary of enp inflow to ts is controlled by a series of detention areas and structures along the eastern boundary of enp fig 4 sites srs1 to srs3 and ts ph1 to ts ph3 are considered freshwater marsh sites dominated by sawgrass fig 4 sites srs4 to srs6 and ts ph6 ts ph7 are brackish water sites dominated by mangroves the region between srs3 and srs4 and ts ph3 and ts ph6 are ecotones transitioning between freshwater and estuarine habitats we use prime to map patterns of water level and sea level and evaluate the implications on salinity at the coastal sites of srs srs3 to srs6 and ts ts ph3 ts ph6 and ts ph7 and the florida bay ts ph9 ts ph10 to ts ph11 sites of the fce lter fig 4 4 2 2 data sixteen years of daily water level data from 2001 to 2016 for each of the fce srs and ts sites were obtained from nearby stations np201 srs1 p36 srs2 mo 215 srs3 shark river srs5 te srs4 nts1 ts ph1 tsb ts ph2 e146 ts ph3 upstream taylor river utr ts ph6 and taylor river mouth trm ts ph7 operated by the united states geological survey usgs and the everglades depth estimation network eden project usgs eden 2019 figs 4 and 5 a b daily sea level data from the key west station operated by the national oceanic and atmospheric administration noaa was used due to its comparatively longer and continuous daily sea level data available since 1913 holgate et al 2012 psmsl 2016 fig 5a and b tidal gage sea level data were converted to the 1988 north america vertical datum navd88 for comparison with water level data fmhd is determined by subtracting sea level from water levels at the most downstream freshwater site srs3 for shark river slough and ts ph3 for taylor slough respectively fig 5c and d surface water salinity data were obtained from fce lter monitoring stations in srs srs3 to srs6 gaiser and childers 2017 fig 5c ts sites ts ph3 ts ph6 and ts ph7 troxler 2017 fig 5d and florida bay ts ph9 ts ph10 and ts ph11 briceno 2017 fourqurean 2017 fig 5e surface water salinity data were three day composites of water samples collected at each of those sites 4 2 3 prime setup and output long term pcs were generated for all variables based on eq 1 fig 2a and b five percentile range indices pri were established representing the status level of the variable as lower low ll p 15 segment observations low observations l 15 p 33 observations within 17 of the median m 33 p 67 high observations h 67 p 85 and observations exceeding the 85th percentile and termed higher high hh p 85 implementation of eq 2 fig 2b these pri s were applied across all variables the pri selection criteria were 1 statistical margins based on standard deviation of approximate normal distribution at 0 5σ and 1σ 2 to provide sufficient flexibility to use the datascape for ecological assessment such as frequency of exposure to ranges of salinity 3 to capture spectrum of seasonal variability and 4 to extract mapping percentiles for data sets as small as 9 observations e g water quality sampling interval for srs4 to srs6 is a 3 day composite that translates to a maximum of 10 observations per month as all the variables used in this application are in the same daily time step and assessed with similar user defined criteria the example procedures employed to map sea level in section 3 3 are used to setup input output and evaluation using prime salinity responses of srs4 and ts ph6 to changes in freshwater level fmhd and sea level were evaluated using contingency tables based on the prime datascapes south florida experiences two seasons a summer time wet season may to october and a winter time dry season november to april as wet season is characterized by low salinity and high water level sea level and fmhd fig 5 the 33rd percentile salt scapes are compared with 67th percentile hydro scapes and vice versa 4 3 results 4 3 1 long term spatial variability the observed gap among water level pcs reflected the relative ground elevations of the individual stations along the freshwater flow direction fig 6 a in srs and ts the water level pcs for sites srs4 5 as well as ts ph3 6 7 tended to cluster together following a similar pattern as sea level long term water level pcs were above sea level except at ts ph3 water levels at srs3 and ts ph3 were below their respective mean ground level for about 10 of the time fig 6a water levels at ts ph3 were below sea level for 5 of the time sea levels exceeded mean ground levels of srs3 and ts ph3 for 15 and 75 of the time fig 6a water level trends at srs5 and ts ph7 are relatively similar but srs4 was consistently lower than ts ph6 by 5 cm compared to the sea ward hydraulic gradient of water levels in srs the order of water level pcs between ts ph3 and ts ph6 interchanged at the lower 35 percentile and higher 90 percentile ends fig 6a fmhd in srs was higher compared to ts ph3 fig 6b salinity levels were below 2 psμ 95 of the time at srs3 and below 10 psμ for about two thirds of the time at srs4 fig 6c conversely salinity at fb site ts ph10 ranged between 25 and 50 psμ the seasonal variability of salinity was higher at srs4 and ts ph6 compared to srs5 and ts ph7 their respective downstream sites compared to srs ts experiences extreme high and low salinity levels fig 6c the maximum salinity levels in srs and ts are bounded by salinity levels at srs6 and ts ph10 respectively despite their relatively close geographic proximity the gap between salinity pcs at ts ph6 and ts ph7 increased steadily up to the 65th percentile after which the gap decreased rapidly while the salinity values at both sites approached those observed at fb ts ph6 displayed the highest rate of rise in salinity after the 65th percentile from 10 psμ to 50 psμ fig 6c the long term averages and values at percentiles defining the range of indices are listed in table 1 4 3 2 cross site annual and seasonal datascapes 4 3 2 1 hydro scapes water level sea level and fmhd annual water levels and sea level showed increasing trends from 2001 to 2016 fig 7 a these temporal patterns is reflected in the hydro scapes as change in status levels from l to m and m to h relatively dry years e g 2001 2005 2007 2011 and 2015 exhibited either ll l status in freshwater level at the 33rd percentile and l status at the mean and median high h water level status was observed at the 33rd percentile in 2016 h hh water level status became dominant at most srs and ts sites at the median and 67th percentile between 2012 and 2016 fig 7a h hh water level status was most frequently observed in 2016 the fmhd status levels were l m at the mean and 33rd percentile and m h at 67th percentile for both srs and ts unusually low and high fmhd status levels were observed in srs in 2015 and 2005 respectively fig 7a the monthly hydro scape across all sites showed strong seasonality with the l ll and h hh status generally aligned with the dry and wet months respectively fig 7b water levels and sea level status were h in september and october at the 33rd percentile and h hh from august to november at 67th percentile fig 7b water levels at the 33rd percentile indicated a three month lag in ll status at the upstream freshwater sites compared to the salt water sites in both srs and ts fig 7b the fmhd for both srs and ts tended to be lowest between march and june at the 33rd percentile and between april and may at the 67th percentile fig 7b 4 3 2 2 salt scape annual salinity generally shifted from l to m to h in srs and ts at the 33rd percentile fig 7c between 2001 and 2015 the years 2008 and 2015 have either h or hh salinity status at the 67th percentile for all sites except ts ph10 fig 7c the months of march through june corresponded with h and or hh salinity conditions in both srs and ts at both percentiles mapped fig 7d higher salinity status was observed in the fb sites between the months of march and august with h status extending in to wet months at the 67th percentile fig 7d 4 3 3 temporal month year datascapes dry months from march to june were characterized by low fmhd and high salinity whereas august to november was dominantly high fmhd and low salinity both in the 33rd and 67th percentile fig 8 in general salinity levels were h hh in the dry season from march to july at srs4 and ts ph6 fig 8b and c among the sixteen years 2015 had the longest l ll fmhd status 10 months that extended into the wet season corresponding to seven months of h hh salinity status in general the fmhd from july to november decreased over the sixteen years fig 8 4 3 4 evaluation of datascapes relationships between the month year saltscapes at srs4 and ts ph6 with sea level water level and fmhd were evaluated see table 2 salinity at srs4 and ts ph6 showed strong association and inverse correspondence with water levels cc 0 5 and fmhd dcc 0 58 table 2 sea levels at key west and salinity levels at srs4 were weakly associated there was no statistically significant association between salinity at ts ph6 and sea level p value 0 05 table 2 fmhd had a stronger association with salinity at ts ph6 compared to freshwater levels at ts ph3 overall fmhd had a consistent level of association with salinity at srs4 and ts ph6 water levels at ts ph3 showed a marginally better association with sea level compared to srs3 table 2 4 4 discussion and interpretation 4 4 1 prime is a synthesis platform for long term data sets with increasing volumes of data exclusive use of descriptive statistical summaries even if they include estimates of variance over certain time intervals may mask the impact of disturbances e g hurricanes drought storm water surges and saltwater intrusion etc or other sources of abrupt shifts fce lter has been collecting long term hydrologic and ecological data along the srs ts and fb transects of the enp since 2000 figs 4 5 and table 1 results of prime extend previous fce data synthesis efforts e g childers 2006 davis et al 2018 dessu et al 2018 kelble et al 2007 and provide novel insight to the underlying hydrologic and salinity pattern at individual sites as well as potential connectivity across the three fce transects for the five variables investigated long term pcs captured signature pattern of the specific variables at each site prime datascapes visualize patterns in finer resolutions to capture processes of interest identify inherent relationships and correlations across variables or sites 4 4 2 long term pcs show signature patterns of water level and salinity across fce lter sites compared to the monthly average time series plots fig 5 hydrologic and salinity pcs provided a clearer picture of underlying patterns fig 6 at the site for instance water levels at srs3 and ts ph3 experienced similar percentiles of wetness and dryness with water levels at or above their respective ground surface elevations for at least 90 of the daily observations from 2001 to 2016 fig 6a vulnerability to sea level rise can be inferred from the long term pcs when sea level exceeded the ground surface elevation at a site which was 75 and 15 of the daily observations at ts ph3 and srs3 respectively fig 6a extended draw down in water levels either due to drought or a decrease in freshwater inputs could expose ts to increasing coastal erosion replacing coastal wetlands with non vegetated open water areas ross et al 2000 todd et al 2012 white and kaplan 2017 with the average sea level rise of 7 7 4 3 mm year from 2001 to 2016 at key west dessu et al 2018 the estuarine region of ts could be below sea level within ten to fifteen years as expected salinity increased downstream and with distance from freshwater marshes of srs and ts fig 6b interestingly the srs estuaries experience a narrow range of salinity between srs4 and srs6 over a large spatial extent compared to the wide range of salinity over a much shorter distance between ts ph6 and ts ph7 the change in the salinity pc gradient particularly at ts ph6 above a percentile of 60 may be attributed to a seasonal change in hydrologic connectivity the combined effect of a negative fmhd and higher water levels at estuarine site ts ph6 compared to the upstream site ts ph3 may facilitate salt water intrusion and increased residence time at ts ph6 resulting in not only the rapid increase in salinity but also the hypersaline conditions observed above a percentile of 95 fig 6c discharge of brackish groundwater within the region of ts ph6 and ts ph7 also account for the higher salinity observed at those sites price et al 2006 small spikes in salinity observed at srs3 are driven by the higher tidal influence in srs smith and mccormick 2001 4 4 3 fmhd can inform freshwater delivery to coastal everglades fmhd along shark river and taylor sloughs of enp measures landscape level vulnerability of downstream freshwater marshes through the oligohaline ecotone to reductions in freshwater flow caused by drought and water management and sea level rise recent work demonstrated that a combination of salt exposure and dry down makes fce systems particularly vulnerable to peat collapse dessu et al 2018 wilson et al 2018 fmhd pcs show the hydraulic gradient for freshwater marshes of srs and ts relative to sea level fig 6b the higher positive fmhd in srs may provide a higher hydraulic head to drive fresh surface and groundwater toward the coastal estuaries since fmhd is below zero in ts 15 of the time all ll levels of ts fmhd datascapes represent sea level higher than freshwater levels at ts ph3 ts estuaries are at exposure to salt water intrusion at least 33 of the time in april and 50 of the time in may fig 7b prime datascapes also highlighted temporal and spatial hot spots for alternative mitigation and management options the 2 3 months lag between the lowest freshwater levels relative to sea level corresponded with the lowest fmhd between april and june fig 7b this difference in status level is reflected in the fmhd scapes as l ll status the dry season freshwater marsh water levels in srs and ts lag by one status level from sea level in april and may resulting in extreme low fmhd and high salinity levels redistributing freshwater deliveries from august and september to april and may might increase the fmhd and reduce the extreme high salinities in those later dry season months compared to srs freshwater deliveries to ts may need to consider the prevailing salinity conditions and the change in salinity over a short distance to provide enough residence time to freshwater deliveries water levels in the freshwater marsh sites of fce depend on the amount of freshwater delivery dessu et al 2018 karamperidou et al 2013 hence the relationship between freshwater levels and salinity can indicate the effects of water management on salinity levels monthly salt scapes at srs4 displayed a better correspondence with the water levels at srs3 over the sixteen year period table 2 however it is important to factor in the effect of sea level rise which will continue to reduce the effect of freshwater levels on salinity the level of correspondence between freshwater marsh water levels srs3 and ts ph3 and sea level dcc 0 65 table 2 indicates a possible mismatch in timing of water level and sea level patterns comparison of the long term monthly and annual hydro scapes fig 6a and c with salt scapes fig 6b and d showed that fmhd is a consistent predictor of salinity status in srs and ts compared to either freshwater levels or sea level 4 4 4 salinity in florida bay is influenced by freshwater levels in srs and ts prime datascapes also suggest a potential correlation between salinity levels in fb with fmhd of srs and ts even though the effect of freshwater deliveries in ts and salinity levels in fb has been widely documented nuttle et al 2000 swart and price 2002 the link of water levels in srs and salinity levels in fb is not yet clearly established marshall et al 2011 the annual and seasonal salt scapes of fb indicated a correspondence of high salinity levels at 67th percentile with low fmhd of either srs or ts fig 7a and c the monthly salt scapes of fb also showed a similar correspondence but lagged by one or two months fig 7b and d hence the high salinity levels in fb may be reduced by maintaining suitable fmhd to drive fresh surface and groundwater flow from ts and srs to fb 4 4 5 prime datascapes facilitate detection and tracing of disturbance extreme disturbances and their impact can be detected as a single or successive hot spot or nonconformity in prime datascapes droughts and hurricanes tropical storms are major disturbances on the hydrology and water quality of the everglades davis et al 2018 the impact of hurricanes and tropical storms in 2003 2005 and 2008 is observed in the prime results with higher water levels in those annual hydro scapes and high levels of fmhd at srs4 fig 8a freshwater delivery to the srs and ts also depends on the anticipation or occurrence of drought and hurricanes dessu et al 2018 light and dineen 1994 the anticipation of either a strong or weak hurricane complicates water management and freshwater delivery to the everglades hurricanes or storm events that occur during a period of anticipated drought or a passing hurricane changing course after water levels in reservoirs are brought to low levels may disturb the normal function of the everglades the excess freshwater from hurricanes helps to reduce salinity if stored and released in the following dry season based on the monthly salinity status pattern low and high salinity levels are expected from august to november and march to may respectively high h hh salinity levels from august to february may suggest disturbance due to either lack of freshwater delivery or extended drought conditions e g 2005 2007 2008 2011 and 2015 dessu et al 2018 the dry condition in 2007 induced low water levels and fmhd that extended into 2008 until tropical storm fay august 2008 resulting in a six month streak of high salinity status at srs4 and ts ph6 between the months of january and september 2008 fig 8 a period of m status months in the wet season at the 33rd percentile 2007 and 2015 suggests prolonged exposure of the system to above normally expected l ll salinity levels prolonged salinity may induce a period of stress on the ecosystem that would have had relief from the seasonal flushing similarly low salinity levels ll l m in the dry season e g 2007 and 2016 may indicate high freshwater delivery due to excess freshwater stored in the upstream reservoirs from a hurricane or storm in the previous wet season or expectation of a coming wetter year prime datascapes provided a visual representation to detect and quantify disturbances across sites fig 7 the change in hydro scapes from 2011 to 2012 stands out across srs and ts as water levels and sea level each increased by at least one status level at the 33rd and 67th percentiles fig 7a with a corresponding decrease in salinity level by at least a level or two fig 7c the salinity levels of 2015 were unprecedented with high salinity levels observed in srs and ts particularly at the 33rd percentile fig 7c whereas 2016 was characterized by the highest water levels and sea level and low salinity levels fig 7d 4 4 6 prime can be used to evaluate potential benefits from everglades restoration scenarios the overall increasing trend of water levels in the freshwater marsh sites suggests progress towards restoring the pre development flow conditions mcvoy et al 2011 sklar et al 2005 as well as preserving the natural landscape of the everglades nungesser et al 2015 from a management and restoration perspective understanding how increased freshwater flow through the everglades can offset the adverse effects of sea level rise maintaining suitable salinity conditions throughout the ecotone and providing for the freshwater needs of downstream estuaries is critical for protecting and sustaining these resources for the future while increasing freshwater levels can help to increase fmhd and combat sea water intrusion the estuarine wetlands vulnerability to rising water levels needs to be factored in water delivery decision process prime results have demonstrated the potential of using the fmhd variable to capture the benefits of increased freshwater delivery relative to sea level rise along srs ts and fb multiple cerp projects are being implemented towards restoration of the everglades and improving freshwater delivery prime datascapes from modeled restoration scenarios can help to compare benefits across plans allowing for an understanding of increments of freshwater flow needed to offset recent rates of sea level rise hence the potential benefits of cerp projects can be evaluated by mapping the fmhd datascapes from model simulation of cerp scenarios and comparing with the baseline historical relationship of fmhd and salinity datascapes 5 conclusions the use of time series analyses has become part of the statistical tool kit of many earth and environmental scientists this is the direct result of the large volumes of data now available through hydrological and environmental observatory platforms including the lter network critical zone observatories czos and the national ecological observatory network neon the need for effective statistically robust and easily interpretable time series analyses is now required to compare patterns within and across these rich data sets and environmental gradients as a direct result of this need throughout the earth and environmental science community we developed prime as a tool based on the statistical advantages and flexibility of percentiles prime results can directly assist environmental assessment and monitoring by comparing both magnitude and recurrence prime produces visual datascapes summarizing a series of comprehensive analyses datascapes also provide a visual output that can be used to communicate results and findings to stakeholders and which demonstrate the importance of maintaining long term observatories and data repositories we demonstrate how a complex hydrological system such as the fce can be mapped using prime to explore and understand processes across variables time and space we advocate the use of prime as a new statistical tool to support cross cutting spatio temporal analyses at global regional and local scales and build comparative relationships of long term ecological and hydrological studies acknowledgment this material is based upon work supported by the national science foundation nsf through the florida coastal everglades long term ecological research program under grant no deb 9910514 2000 2006 and grant no dbi 0620409 2007 2012 and grant no deb 1237517 dec 2012 2018 support was also provided by the nsf under grant deb 1545288 through the long term ecological research network communications office lnco and the national center for ecological analysis and synthesis nceas additional support for this work was also provided by the everglades foundation the southeast environmental research center and the department of earth and environment of the school of the environment arts and society of the college of arts sciences and education of florida international university this is contribution number 932 from the southeast environmental research center in the institute of water and environment at florida international university appendix a supplementary data the following are the supplementary data to this article prime supplementary files and interface multimedia component 1 water level sea level and fmhd data multimedia component 3 salinity data multimedia component 3 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104580 
26083,commercial scale carbon capture and storage ccs technology will involve deploying infrastructure on a massive and costly scale this effort will require careful and comprehensive planning to ensure that capture locations storage sites and the dedicated co2 distribution pipelines are selected in a robust and cost effective manner introduced in 2009 simccs is an optimization model for integrated system design that enables researchers stakeholders and policy makers to design ccs infrastructure networks simccs 2 0 is a complete ground up redesign that is now a portable software package useable and shareable by the ccs research industrial policy and public communities simccs 2 0 integrates multiple new capabilities including a refined optimization model novel candidate network generation techniques and optional integration with high performance computing platforms accessing user provided co2 source sink and transportation data simccs 2 0 creates candidate transportation routes and formalizes an optimization problem that determines the most cost effective ccs system design this optimization problem is then solved either through a high performance computing interface or through third party software on a local desktop computing platform finally simccs 2 0 employs an open access geographic information system framework to enable analysis and visualization capabilities simccs 2 0 is written in java and is publicly available via github to encourage collaboration modification and community development 1 introduction carbon dioxide capture and storage ccs or ccus with utilization is a key technology for reducing co2 emissions to the atmosphere stauffer et al 2011 this technology involves capturing co2 at large stationary sources such as power plants and biorefineries transporting the co2 in dedicated co2 pipelines and injecting and storing it in subsurface geologic reservoirs such as deep saline formations and mature oil fields to have a significant impact on mitigating climate change commercial scale ccs infrastructure will have to be deployed on a massive scale in the united states alone this could require managing a greater volume of co2 than domestic oil production middleton et al 2012b including capturing co2 from hundreds of sources constructing tens or hundreds of thousands of kilometers of pipelines and injecting and storing co2 into many widespread geologic formations deploying infrastructure on this scale requires careful and comprehensive modeling and as a result multiple ccs planning tools have been developed over the past two decades simccs middleton and bielicki 2009 is an economic engineering software tool for making integrated ccs infrastructure decisions simccs has been used by government academia industry and other researches to understand how ccs infrastructure could and should be developed the integrated decision support capability of simccs is illustrated in fig 1 although it is already arguably the leading ccs infrastructure software the original software faced several issues for instance the source code was outdated and included limited documentation thus poorly suited for collaboration and further development here we introduce completely new open source software simccs 2 0 written in java and containing a new mathematical model updated network generation algorithms seamless integration with optimization software by using the standardized mps file format and comprehensive documentation simccs 2 0 is freely available in the public domain and available to all users globally yaw et al 2018 we also provide a fully functional case study of the southeastern united states to demonstrate the software capabilities and to guide users in developing their own datasets and optimization problems of interest finally simccs 2 0 includes the ability to solve infrastructure design problems with open access high performance computing hpc platforms wang et al 2018 enabling larger and more complex problems to be solved simccs 2 0 will serve as a platform for future collaborative development efforts including custom optimization models large ensemble runs to address uncertainty and other geospatial network design applications 2 background since its introduction in 2009 middleton and bielicki 2009 simccs has become the most widely used and documented ccs planning tool with application to a variety of case studies and research questions other tools like infraccs morbee et al 2011 getco gale et al 2001 and markal nl uu van den broek et al 2011 have some of the capabilities of simccs but lack the fully integrated turnkey solution that simccs offers for example simccs includes multi variate optimization economically flexible modeling uncertainty analysis and pipeline geolocation in the simccs framework there are three separate model formulations to address the underlying drivers for managing co2 1 simccs cap middleton and bielicki 2009 users define a system wide co2 capture target and the simccs optimization engine minimizes infrastructure costs to reach that target inversely users can specify an available budget in us dollars euros or any other currency and the engine maximizes the amount of co2 that can be captured and stored simccs cap is intended for stakeholders and policy makers to understand how ccs infrastructure could be deployed within a cap and trade environment 2 simccs price kuby et al 2011a users set a co2 emissions price or tax and the simccs optimization engine projects the amount of system wide co2 that should either be emitted or captured and transported for geologic storage based on a least cost solution the engine deploys ccs infrastructure only when the costs are lower than the co2 price i e the cost to emit co2 consequently a rising co2 price will tend to stimulate more extensive ccs networks simccs price is intended for researchers stakeholders and policy makers to understand the impact of setting a range of co2 emissions prices 3 simccs time middleton et al 2012d users are able to set a dynamic co2 target or emissions price and simccs optimizes investment decisions across multiple time periods as an example the simccs optimization engine is able to over build infrastructure in early time periods in anticipation of greater co2 volumes in the future simccs time can be executed in both the simccs cap and simccs price modes simccs 2 0 is currently equipped with the simccs cap formulation but is actively being developed to include all three versions of the model between 2009 and 2013 simccs relied on users to specify a set of discrete pipeline diameters that the optimization engine could utilize the default provided diameters were based on the most commonly used pipeline diameters in the united states and parameterized costs co2 flows for the mit co2 pipeline transport and cost model mit 2006 the realistic network optimization particularly the ability to aggregate increasing volumes of co2 into trunk lines is a key capability that sets simccs apart from competing models however an increasing number of discrete pipeline diameters is a substantial challenge to the optimization engine in response simccs uses a pipeline approximation approach that vastly reduces solution times with minimal quantified impact to solution accuracy middleton 2013 particularly when considering uncertainty in the costs engineering and policy decisions that currently exist for the nascent ccs enterprise simccs has been applied in multiple regions and countries and for multiple co2 source and reservoir types although many of the applications are us focused simccs has been applied to problems in alberta canada middleton and brandt 2013 all of france bielicki et al 2014 and the ordos basin in china stauffer et al 2014 simccs applications to date have considered a range of co2 sources including coal fired and natural gas power plants middleton et al 2012a 2012c ethanol plants middleton et al 2012d oil sands middleton and brandt 2013 and shale oil extraction and processing keating et al 2011 ethylene manufacture middleton 2013 middleton et al 2015b ammonia and natural gas processing middleton et al 2014 oil refining middleton et al 2011 and iron and steel manufacturing stauffer et al 2014 although most simccs cases studies considered co2 storage in deep saline formations co2 utilization options have also been implemented including co2 enhanced oil recovery co2 eor reservoirs ellett et al 2017 middleton 2013 middleton et al 2011 hydrocarbon depleted fractured shales bielicki et al 2018 enhanced coalbed methane ellett et al 2017 middleton et al 2015a and acid gas reservoirs middleton and brandt 2013 the unique capability of simccs to explicitly handle stacked reservoir systems with co located opportunities for co2 storage and utilization makes it a powerful new tool for facilitating ccs business planning ellett et al 2017 simccs has also helped inspire a range of related ccs infrastructure optimization frameworks particularly in europe and asia han and lee 2011 klokk et al 2010 morbee et al 2012 prasodjo and pratson 2011 tan et al 2013 van den broek et al 2010 in addition to ccs simccs has also been used as a direct inspiration for other energy optimization modeling including hydrogen production and transport johnson and ogden 2012 geothermal energy extraction langenfeld and bielicki 2016 and optimization of wind farm generation and electricity transmission phillips and middleton 2012 2 1 novel approaches simccs evolved as an outgrowth of operations research and geographical information science gis techniques being merged with subject matter experts in ccs eccles and middleton 2017 however simccs did introduce several novel advancements that have yet to be matched a key advance was the ability to deploy a realistic pipeline network that both represented reality and could take advantage of massive economies of scale of trunk pipelines for example a 42 inch trunk co2 pipeline costs more than 30 times less than a 4 inch pipeline on a per unit distance basis kuby et al 2011b models that cannot capture this cost saving almost certainly cannot build trunk pipelines and without realistic pipelines it is exceptionally likely that suboptimal or even infeasible sets of co2 sources and sinks will be recommended for deployment in a given scenario a second key advance was the ability to construct a candidate network of routes where pipelines could be constructed middleton et al 2012c yaw et al 2019 this is critical since pre defined co2 pipeline routes do not exist in most regions and yet pipeline routing is an essential input for any ccs infrastructure model that attempts to design large scale ccs networks a third key advance is the ability to handle uncertainty without reducing the fidelity of pipeline networks simccs has demonstrated this by focusing on the impact of geologic uncertainty where the performance of a geologic reservoir injectivity and storage capacity is not fully known until the infrastructure has been built and injection and storage has commenced analysis has revealed that when considering geologic uncertainty each ccs network design sources pipelines and storage reservoirs and total infrastructure cost is different each time the model is executed middleton et al 2012b furthermore mitigating deficiencies in infrastructure to ensure all captured co2 can be managed i e new pipelines wells and storage reservoirs can increase transport and storage costs by as much as 50 middleton and yaw 2018 a final novel contribution of simccs has been the ability to develop and integrate realistic costs throughout the ccs supply chain including detailed reservoir modeling keating et al 2011 middleton et al 2012a dynamic co2 capture costs middleton and eccles 2013 and important but often overlooked issues such as treating and disposing of extracted brine sullivan et al 2013 extracting brine is a critical way to increase co2 storage potential reduce risks such as induced seismicity and caprock failure and potentially produce a stream of valuable minerals and treatable water for consumptive use harp et al 2017 depending on the brine chemistry 2 2 barriers despite achieving status as the leading ccs infrastructure optimization model simccs has multiple drawbacks that limit collaboration and provide a poor platform for moving to next generation problems e g problems of increased complexity or vast geographical domains the primary barrier is that simccs was designed as research code only the release never had any accompanying documentation and the code was developed quickly using legacy gis based visual basic vb code from prior work middleton 2006 consequently simccs works only on the windows desktop operating system and because vb was classified as legacy by microsoft in 2008 installing simccs has become increasingly challenging this has limited collaboration and has stifled development of new simccs applications such as evaluating national scale ccs implementation and solving increasingly complex model formulations while endogenously accounting for uncertainty that is given a parameter distribution at each source and sink simccs 2 0 would be able to identify an infrastructure solution that is robust to a wide range of potential realizations middleton and yaw 2018 2 3 opportunities simccs 2 0 presents multiple research policy and industrial opportunities the new open source platform will allow straightforward collaboration including allowing users to develop customized versions of the code such as developing their own candidate network algorithms or inserting valid inequalities to the original simccs formulation lobo 2017 simccs 2 0 runs on any java enable machine and requires no dependencies beyond what is packaged with the code users can employ their own optimization solver e g ibm s cplex on their local machine to solve simccs mixed integer linear programming mip problems alternately simccs 2 0 allows users to solve the mips with hpc resources within the extreme science and engineering discovery environment xsede network via indiana university s science gateway platform wang et al 2018 simccs gateway users have the option to send problems to the cplex solver for non profit and research purposes or to use open source solvers for other purposes this new hpc capability further enables simccs 2 0 to extend from solving regional scale to national scale problems such as for the continental united states or china this unique capability enables simccs 2 0 to help scientists stakeholders and policy makers better understand the deployment of ccs infrastructure on a massive scale this also avoids the perennial issue of boundary conditions where co2 near the edge of a regional domain might be expected to leave the domain rather than join an integrated network within the domain in addition development of simccs 2 0 within the science gateway framework will improve the collaborative efforts of the whole community by providing an integrated platform for all ccs related data sets studies and results 3 model formulation the simccs mip formulation has been updated since the original release middleton and bielicki 2009 including major modifications to account for co2 prices dynamic targets and the linearized pipeline approach in addition incremental nomenclature and formulation changes have appeared to accompany the simccs 2 0 software release we provide a clean formulation of the linearized pipeline mip simccs 2 0 includes both this linearized mip formulation as well as the original discrete pipeline formulation and can be easily modified by the user for other custom formulations pipelines in an mip formulation can intuitively be represented as a number of discrete pipeline sizes however this representation requires a large number of variables in the formulation and can quickly lead to intractable formulations instead pipelines in an mip formulation can be represented as a smaller set of linearized functions of pipeline capacity versus cost i e trends this allows for simpler formulations compared to the discrete formulation while still ensuring that the cost model is realistic middleton 2013 the mip based on linearized pipelines can be formulated using the following input parameters and model decision variables input parameters f i s r c fixed cost for opening source i m f j r e s fixed cost of opening reservoir j m f j w e l l fixed cost of opening each well at reservoir j m v i s r c variable cost for capturing co2 from source i tco2 v j w e l l variable cost of injecting co2 in each well at reservoir j tco2 s set of sources r set of reservoirs i set of all graph nodes k set of candidate pipeline arcs c set of pipeline capacity trends q i s r c co2 production rate at source i mtco2 yr q j w e l l capacity of each well at reservoir j mtco2 q j r e s capacity of reservoir j mtco2 q k c m a x maximum capacity of pipeline k with trend c mtco2 yr q k c m i n minimum capacity of pipeline k with trend c mtco2 yr α k c cost to transport one tonne of co2 on pipeline k with trend c β k c cost to build pipeline k with trend c model decision variables s i 0 1 indicates if source at node i is opened r j 0 1 indicates if reservoir at node j is opened y k c 0 1 indicates if pipeline k with trend c is built a i r amount of co2 captured at source i tco2 yr b j r amount of co2 injected into reservoir j tco2 yr w j n number of wells opened at reservoir j p k c r amount of flow in pipeline k with trend c tco2 yr c o 2 c a p r target co2 capture amount for project tco2 yr the mip is driven by the objective function min i s f i s r c s i v i s r c a i c a p t u r e cos t k k c c α k c p k c p i p e l i n e u s e c o s t k k c c β k c y k c p i p e l i n e b u i l d c o s t j r f j r e s r j f j w e l l w j v j w e l l b j s t o r a g e c o s t subject to the following constraints a q k c m i n y k c p k c q k c m a x y k c k k c c b c c y k c 1 k k c k k s r c k n c c p k c k k d s t k n c c p k c a n i f n s b n i f n r 0 o t h e r w i s e n i d a i q i s r c s i i s e b j q j w e l l w j j r f b j q j r e s r j j r g i s a i c o 2 c a p where constraint a ensures that pipeline is built before transporting co2 and that its capacity is appropriate for the amount of flow b allows at most one linear piece i e trend pipeline between any two nodes c enforces conservation of flow at each node d ensures a source must be opened to capture co2 and that capture is capped by its maximum production e ensures a well must be constructed to inject co2 and caps injection by its maximum injectivity f caps storage for each reservoir by its maximum capacity and g ensures the total capture amount meets the target simccs 2 0 allows users to model co2 storage economics and engineering to a much more detailed level than any other ccs infrastructure model for example f j r e s does not depend on which wells are opened it gives the user the chance to separate opening costs that are independent of the number of wells opened from the costs that are incurred on a per well basis the value of b j is influenced by the number of wells the model selects constraint e but b j is the total injected amount for the entire reservoir not on a per well basis the model assumes that for a given reservoir each well is identical same costs same capacities if this were not the case and some region of a reservoir were logistically challenging to put wells into higher operating costs or had much lower injectivity smaller well capacity then each region could be treated as a separate reservoir since all wells are assumed to be identical for a given reservoir the amount of co2 to inject per well is defined by b j w j 3 1 simccs 2 0 platform and development simccs 2 0 is a java application that consists of a graphical user interface gui that is used to drive data management pipeline network generation and optimization model formulation solution and visualization fig 2 3 2 data management simccs 2 0 determines infrastructure designs based on the tradeoff of costs associated with capturing transporting and storing co2 in the region of interest these costs and other parameters are supplied to simccs 2 0 in the form of source location parameters storage location parameters and weighted cost surface input files source data includes parameters for each source location including an id name latitude longitude location fixed opening cost variable operating cost per unit capture cost and a maximum co2 production rate storage data includes parameters for each storage location including a label latitude longitude location fixed opening cost for the entire location variable operating cost for the entire location fixed opening and variable operating costs for each well injection cost and a maximum capacity for each well and for the entire location weighted cost surface data are used to determine the cost of building pipeline networks generating the weighted cost surface involves laying a grid over the modeled domain and determining the cost to traverse from one cell to a neighboring cell this is a function of underlying topography slope and aspect land ownership 10 default classes land use 16 default types crossings rail river and roads existing pipeline right of ways rows and population density hoover et al 2019 separate weighted cost surfaces are used for construction costs and for row costs since these costs are independent for example a steep slope will affect construction but not row costs pipelines are typically buried in trenches while construction costs are essentially identical for cropland and grassland whereas land purchases for these rows are not once simccs 2 0 is directed to a specific dataset the user can visualize the geographic area with source and storage location overlaid on a graphical representation of the weighted cost surface fig 3 1 3 3 pipeline network generation the pipeline network is one of the three key cost drivers for ccs infrastructure design the other two being capture and storage costs unlike capture and storage facilities whose locations are fixed a set of candidate pipeline routes needs to be determined before an optimal ccs design can be constructed simccs 2 0 considers the weighted cost surface as a graph with nodes in the centers of cells and edges connecting each center to the neighbor cells centers costs to traverse cells are accounted for in edge weights direct moves to a diagonal cell are multiplied by a factor of 2 to account for the increased distance of the move compared with a cardinal move this edge weighted graph structure is then used for the rest of the pipeline route generation process an all pairs graph is constructed by finding the shortest cheapest paths between all source storage locations by using shortest path algorithms with cell to cell paths representing the routing of that pipeline a subgraph of the all pairs graph is selected as a candidate pipeline network which in turn corresponds to the set of possible pipeline routes available in the mip there are various methods of selecting the candidate network with simccs 2 0 supporting delaunay triangulation high quality solution and quick and steiner tree preserving subgraphs provable performance guarantees middleton et al 2012c fig 3 2 shows a candidate pipeline network generated by simccs 2 0 the candidate network or routes is not provably optimal but it is demonstrably close to optimal and in practice is typically optimal the candidate network only omits potential arcs that add only marginal benefit in terms of lowest cost routes but prohibitively expand the model size candidate network generation is a very active research area the algorithm utilized in simccs 2 0 is detailed in previous work middleton et al 2012c and is the subject of ongoing research the under development new algorithm approximately preserves the cost of steiner trees from the cost surface graph since steiner trees are the least cost trees that connect a subset of nodes every connected component of a solution will consist of a steiner tree due to this the quality of the candidate network is bounded as a function of optimal 3 4 optimization model formulation and solution once source and storage locations are parameterized and a candidate pipeline network has been identified the user is able to start formulating infrastructure design optimization problems these formulations take the form of mips as detailed in the preceding section simccs 2 0 is packaged with the capability to build both the discrete pipeline and linearized model versions and it is simple for the user to build their own custom model formulations the user formulates the model by determining the project s capital recovery rate project length and target co2 capture amount simccs 2 0 then builds the mip formulation and stores it in a mathematical programming system mps file which is a standardized file format for mip problems fig 3 3 shows the controls simccs 2 0 presents to the user for formulating the optimization model the model is solved by using optimization software to solve the mps file since the model is stored in an mps file many commercial and open source software packages are available to accomplish this task simccs 2 0 default setup is to use ibm s cplex to solve the mps file in the form of an sol solution file modifying simccs 2 0 to work with different optimization software is a straightforward process since simccs 2 0 already stores the model formulation as an mps file the user must simply replace the cplex solver call with one to the desired optimization software in this case the method for reading in solutions should also be modified to reflect the output format of the chosen optimization software since simccs 2 0 is open source users are able to directly modify the source code or contact the developers to suggest feature additions as mentioned previously simccs 2 0 also includes the capability to seamlessly pass mps files to hpc resources through the simccs gateway platform via a web interface https simccs org this capability requires the user to obtain an account that will enable them to transmit the mps file to hpc resources where it is then solved and presents the user with the ability to retrieve the sol file 3 5 optimization model solution display displaying infrastructure design solutions in simccs 2 0 is a straightforward process simccs 2 0 uses the mps and sol files from a given run to determine which source and storage locations were opened how much co2 was captured and stored and where various sized pipelines were purchased this information is then visualized in the gui total costs and costs broken down by capture transport and storage are also displayed for comparison purposes gis shapefiles that represent all of this information are also generated including source locations storage locations pipeline routes and co2 flows these shapefiles can be used in any gis visualization application fig 3 4 shows a solution displayed by simccs 2 0 along with the associated costs of the solution 3 6 model sensitivity simccs 2 0 can visualize solutions that were not output by the optimization software to do this the user would to construct their own solutions or modify existing ones which is easily done once simccs 2 0 visualizes it the user can see the cost of the solution and output it to shapefiles for analysis in other programs to compare two different solutions both optimal or not the user would create two different scenarios and in the software to switch between visualizations of both scenarios users can also compare and contrast the outputted solutions as shapefiles or text files for further analysis simccs 2 0 also innately allows for extensive contingency planning by manipulating input parameters e g making capture costs of 0 will ensure that simccs 2 0 opens a source changing the co2 capture target is done directly in simccs 2 0 as part of the model formulation step other contingency simulations can be performed by manipulating the input data or changing the open source code directly 4 infrastructure design dataset the simccs 2 0 code repository includes data from a case study that models impacts to ccs infrastructure deployment at various scales from pore space considerations all the way up to regional aspects middleton et al 2012a the data included with simccs 2 0 is the regional scale component of the study and consists of capture transportation and storage parameters in the southeastern united states from 20 coal fired power plants and seven potential storage locations the region of interest covers an area of roughly 1000 km by 625 km in southern louisiana mississippi alabama georgia and northern florida detailed justification of the various cost capacity injectivity and emission parameters can be found in the original case study publication this dataset in conjunction with the user manual provides a meaningful example on which new users can familize themselves with the simccs 2 0 software a useful cost surface for a significant region of the united states and a guidline of the required data format and organization fig 4 illustrates simccs 2 0 s solutions for the southeastern united states dataset and a scenario consisting of a 30 year project capturing 110 mtco2 yr in this figure red dots correspond to opened sources with the fraction of available co2 captured represented by the dark red component of the pie chart blue dots correspond to the opened reservoirs with the fraction of available storage capacity used represented by the dark blue component of the pie chart and green links correspond to selected pipeline routes 4 1 availability simccs 2 0 is freely available through a public github repository yaw et al 2018 this repository includes source code for users that wish to directly modify simccs 2 0 as well as a packaged executable version jar file for users that do not wish to compile code on their own the data from the sample dataset described above is included to provide a fully functional example and representative file formats a complete user manual is included in the repository that details the functionality of simccs 2 0 explains the software architecture presents various examples and describes the required data formats this github repository will be continually updated as simccs 2 0 is improved and new capabilities are developed 5 conclusion simccs 2 0 is a software tool with the potential to revolutionize ccs integrated infrastructure design by providing a versatile open source platform for collaborative study researchers are able to modify the software to test novel optimization models and network generation algorithms and also to develop custom hpc resource integration simccs 2 0 is intended to serve as a platform for future research efforts via shared resources throughout the ccs stakeholder community current efforts include expanded integration with hpc resources across the xsede network development of a new optimization model that natively accounts for source and storage uncertainty and development of an entirely new optimization framework that is not dependent on solving computationally expensive mips improvements of simccs 2 0 are also being undertaken that will automate the generation of the weighted cost surface as well as provide the capability to determine source and storage parameters from within the software this effort will reduce the dependency on strictly formatted source storage and weighted cost surface data and will enable users to design ccs infrastructure deployments without having to generate this data by themselves software data availability the source code and an executable java archive jar file of simccs 2 0 is available to the general public as an open source github repository yaw et al 2018 https github com simccs simccs the github repository also includes a detailed user manual including contact information for software developers and a sample dataset from a ccs study in the southeastern united states middleton et al 2012a this is provided for users to acquaint themselves with simccs 2 0 s capability as well as to demonstrate the required data formats acknowledgments this research was funded by the us china advanced coal technology consortium under management of west virginia university the project simccs development and applications funded through the u s department of energy s doe fossil energy program under award no fe 1017 18 fy18 and by the u s department of energy s doe national energy technology laboratory netl through the integrated mid continent stacked carbon storage hub carbonsafe project under award no de fe0029164 the early co2 storage complex in kemper country carbonsafe project under award no de fe0029465 and the rocky mountain carbonsafe project under award no de fe0029280 appendix a supplementary data the following is the supplementary data to this article supplementary data supplementary data appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104560 
26083,commercial scale carbon capture and storage ccs technology will involve deploying infrastructure on a massive and costly scale this effort will require careful and comprehensive planning to ensure that capture locations storage sites and the dedicated co2 distribution pipelines are selected in a robust and cost effective manner introduced in 2009 simccs is an optimization model for integrated system design that enables researchers stakeholders and policy makers to design ccs infrastructure networks simccs 2 0 is a complete ground up redesign that is now a portable software package useable and shareable by the ccs research industrial policy and public communities simccs 2 0 integrates multiple new capabilities including a refined optimization model novel candidate network generation techniques and optional integration with high performance computing platforms accessing user provided co2 source sink and transportation data simccs 2 0 creates candidate transportation routes and formalizes an optimization problem that determines the most cost effective ccs system design this optimization problem is then solved either through a high performance computing interface or through third party software on a local desktop computing platform finally simccs 2 0 employs an open access geographic information system framework to enable analysis and visualization capabilities simccs 2 0 is written in java and is publicly available via github to encourage collaboration modification and community development 1 introduction carbon dioxide capture and storage ccs or ccus with utilization is a key technology for reducing co2 emissions to the atmosphere stauffer et al 2011 this technology involves capturing co2 at large stationary sources such as power plants and biorefineries transporting the co2 in dedicated co2 pipelines and injecting and storing it in subsurface geologic reservoirs such as deep saline formations and mature oil fields to have a significant impact on mitigating climate change commercial scale ccs infrastructure will have to be deployed on a massive scale in the united states alone this could require managing a greater volume of co2 than domestic oil production middleton et al 2012b including capturing co2 from hundreds of sources constructing tens or hundreds of thousands of kilometers of pipelines and injecting and storing co2 into many widespread geologic formations deploying infrastructure on this scale requires careful and comprehensive modeling and as a result multiple ccs planning tools have been developed over the past two decades simccs middleton and bielicki 2009 is an economic engineering software tool for making integrated ccs infrastructure decisions simccs has been used by government academia industry and other researches to understand how ccs infrastructure could and should be developed the integrated decision support capability of simccs is illustrated in fig 1 although it is already arguably the leading ccs infrastructure software the original software faced several issues for instance the source code was outdated and included limited documentation thus poorly suited for collaboration and further development here we introduce completely new open source software simccs 2 0 written in java and containing a new mathematical model updated network generation algorithms seamless integration with optimization software by using the standardized mps file format and comprehensive documentation simccs 2 0 is freely available in the public domain and available to all users globally yaw et al 2018 we also provide a fully functional case study of the southeastern united states to demonstrate the software capabilities and to guide users in developing their own datasets and optimization problems of interest finally simccs 2 0 includes the ability to solve infrastructure design problems with open access high performance computing hpc platforms wang et al 2018 enabling larger and more complex problems to be solved simccs 2 0 will serve as a platform for future collaborative development efforts including custom optimization models large ensemble runs to address uncertainty and other geospatial network design applications 2 background since its introduction in 2009 middleton and bielicki 2009 simccs has become the most widely used and documented ccs planning tool with application to a variety of case studies and research questions other tools like infraccs morbee et al 2011 getco gale et al 2001 and markal nl uu van den broek et al 2011 have some of the capabilities of simccs but lack the fully integrated turnkey solution that simccs offers for example simccs includes multi variate optimization economically flexible modeling uncertainty analysis and pipeline geolocation in the simccs framework there are three separate model formulations to address the underlying drivers for managing co2 1 simccs cap middleton and bielicki 2009 users define a system wide co2 capture target and the simccs optimization engine minimizes infrastructure costs to reach that target inversely users can specify an available budget in us dollars euros or any other currency and the engine maximizes the amount of co2 that can be captured and stored simccs cap is intended for stakeholders and policy makers to understand how ccs infrastructure could be deployed within a cap and trade environment 2 simccs price kuby et al 2011a users set a co2 emissions price or tax and the simccs optimization engine projects the amount of system wide co2 that should either be emitted or captured and transported for geologic storage based on a least cost solution the engine deploys ccs infrastructure only when the costs are lower than the co2 price i e the cost to emit co2 consequently a rising co2 price will tend to stimulate more extensive ccs networks simccs price is intended for researchers stakeholders and policy makers to understand the impact of setting a range of co2 emissions prices 3 simccs time middleton et al 2012d users are able to set a dynamic co2 target or emissions price and simccs optimizes investment decisions across multiple time periods as an example the simccs optimization engine is able to over build infrastructure in early time periods in anticipation of greater co2 volumes in the future simccs time can be executed in both the simccs cap and simccs price modes simccs 2 0 is currently equipped with the simccs cap formulation but is actively being developed to include all three versions of the model between 2009 and 2013 simccs relied on users to specify a set of discrete pipeline diameters that the optimization engine could utilize the default provided diameters were based on the most commonly used pipeline diameters in the united states and parameterized costs co2 flows for the mit co2 pipeline transport and cost model mit 2006 the realistic network optimization particularly the ability to aggregate increasing volumes of co2 into trunk lines is a key capability that sets simccs apart from competing models however an increasing number of discrete pipeline diameters is a substantial challenge to the optimization engine in response simccs uses a pipeline approximation approach that vastly reduces solution times with minimal quantified impact to solution accuracy middleton 2013 particularly when considering uncertainty in the costs engineering and policy decisions that currently exist for the nascent ccs enterprise simccs has been applied in multiple regions and countries and for multiple co2 source and reservoir types although many of the applications are us focused simccs has been applied to problems in alberta canada middleton and brandt 2013 all of france bielicki et al 2014 and the ordos basin in china stauffer et al 2014 simccs applications to date have considered a range of co2 sources including coal fired and natural gas power plants middleton et al 2012a 2012c ethanol plants middleton et al 2012d oil sands middleton and brandt 2013 and shale oil extraction and processing keating et al 2011 ethylene manufacture middleton 2013 middleton et al 2015b ammonia and natural gas processing middleton et al 2014 oil refining middleton et al 2011 and iron and steel manufacturing stauffer et al 2014 although most simccs cases studies considered co2 storage in deep saline formations co2 utilization options have also been implemented including co2 enhanced oil recovery co2 eor reservoirs ellett et al 2017 middleton 2013 middleton et al 2011 hydrocarbon depleted fractured shales bielicki et al 2018 enhanced coalbed methane ellett et al 2017 middleton et al 2015a and acid gas reservoirs middleton and brandt 2013 the unique capability of simccs to explicitly handle stacked reservoir systems with co located opportunities for co2 storage and utilization makes it a powerful new tool for facilitating ccs business planning ellett et al 2017 simccs has also helped inspire a range of related ccs infrastructure optimization frameworks particularly in europe and asia han and lee 2011 klokk et al 2010 morbee et al 2012 prasodjo and pratson 2011 tan et al 2013 van den broek et al 2010 in addition to ccs simccs has also been used as a direct inspiration for other energy optimization modeling including hydrogen production and transport johnson and ogden 2012 geothermal energy extraction langenfeld and bielicki 2016 and optimization of wind farm generation and electricity transmission phillips and middleton 2012 2 1 novel approaches simccs evolved as an outgrowth of operations research and geographical information science gis techniques being merged with subject matter experts in ccs eccles and middleton 2017 however simccs did introduce several novel advancements that have yet to be matched a key advance was the ability to deploy a realistic pipeline network that both represented reality and could take advantage of massive economies of scale of trunk pipelines for example a 42 inch trunk co2 pipeline costs more than 30 times less than a 4 inch pipeline on a per unit distance basis kuby et al 2011b models that cannot capture this cost saving almost certainly cannot build trunk pipelines and without realistic pipelines it is exceptionally likely that suboptimal or even infeasible sets of co2 sources and sinks will be recommended for deployment in a given scenario a second key advance was the ability to construct a candidate network of routes where pipelines could be constructed middleton et al 2012c yaw et al 2019 this is critical since pre defined co2 pipeline routes do not exist in most regions and yet pipeline routing is an essential input for any ccs infrastructure model that attempts to design large scale ccs networks a third key advance is the ability to handle uncertainty without reducing the fidelity of pipeline networks simccs has demonstrated this by focusing on the impact of geologic uncertainty where the performance of a geologic reservoir injectivity and storage capacity is not fully known until the infrastructure has been built and injection and storage has commenced analysis has revealed that when considering geologic uncertainty each ccs network design sources pipelines and storage reservoirs and total infrastructure cost is different each time the model is executed middleton et al 2012b furthermore mitigating deficiencies in infrastructure to ensure all captured co2 can be managed i e new pipelines wells and storage reservoirs can increase transport and storage costs by as much as 50 middleton and yaw 2018 a final novel contribution of simccs has been the ability to develop and integrate realistic costs throughout the ccs supply chain including detailed reservoir modeling keating et al 2011 middleton et al 2012a dynamic co2 capture costs middleton and eccles 2013 and important but often overlooked issues such as treating and disposing of extracted brine sullivan et al 2013 extracting brine is a critical way to increase co2 storage potential reduce risks such as induced seismicity and caprock failure and potentially produce a stream of valuable minerals and treatable water for consumptive use harp et al 2017 depending on the brine chemistry 2 2 barriers despite achieving status as the leading ccs infrastructure optimization model simccs has multiple drawbacks that limit collaboration and provide a poor platform for moving to next generation problems e g problems of increased complexity or vast geographical domains the primary barrier is that simccs was designed as research code only the release never had any accompanying documentation and the code was developed quickly using legacy gis based visual basic vb code from prior work middleton 2006 consequently simccs works only on the windows desktop operating system and because vb was classified as legacy by microsoft in 2008 installing simccs has become increasingly challenging this has limited collaboration and has stifled development of new simccs applications such as evaluating national scale ccs implementation and solving increasingly complex model formulations while endogenously accounting for uncertainty that is given a parameter distribution at each source and sink simccs 2 0 would be able to identify an infrastructure solution that is robust to a wide range of potential realizations middleton and yaw 2018 2 3 opportunities simccs 2 0 presents multiple research policy and industrial opportunities the new open source platform will allow straightforward collaboration including allowing users to develop customized versions of the code such as developing their own candidate network algorithms or inserting valid inequalities to the original simccs formulation lobo 2017 simccs 2 0 runs on any java enable machine and requires no dependencies beyond what is packaged with the code users can employ their own optimization solver e g ibm s cplex on their local machine to solve simccs mixed integer linear programming mip problems alternately simccs 2 0 allows users to solve the mips with hpc resources within the extreme science and engineering discovery environment xsede network via indiana university s science gateway platform wang et al 2018 simccs gateway users have the option to send problems to the cplex solver for non profit and research purposes or to use open source solvers for other purposes this new hpc capability further enables simccs 2 0 to extend from solving regional scale to national scale problems such as for the continental united states or china this unique capability enables simccs 2 0 to help scientists stakeholders and policy makers better understand the deployment of ccs infrastructure on a massive scale this also avoids the perennial issue of boundary conditions where co2 near the edge of a regional domain might be expected to leave the domain rather than join an integrated network within the domain in addition development of simccs 2 0 within the science gateway framework will improve the collaborative efforts of the whole community by providing an integrated platform for all ccs related data sets studies and results 3 model formulation the simccs mip formulation has been updated since the original release middleton and bielicki 2009 including major modifications to account for co2 prices dynamic targets and the linearized pipeline approach in addition incremental nomenclature and formulation changes have appeared to accompany the simccs 2 0 software release we provide a clean formulation of the linearized pipeline mip simccs 2 0 includes both this linearized mip formulation as well as the original discrete pipeline formulation and can be easily modified by the user for other custom formulations pipelines in an mip formulation can intuitively be represented as a number of discrete pipeline sizes however this representation requires a large number of variables in the formulation and can quickly lead to intractable formulations instead pipelines in an mip formulation can be represented as a smaller set of linearized functions of pipeline capacity versus cost i e trends this allows for simpler formulations compared to the discrete formulation while still ensuring that the cost model is realistic middleton 2013 the mip based on linearized pipelines can be formulated using the following input parameters and model decision variables input parameters f i s r c fixed cost for opening source i m f j r e s fixed cost of opening reservoir j m f j w e l l fixed cost of opening each well at reservoir j m v i s r c variable cost for capturing co2 from source i tco2 v j w e l l variable cost of injecting co2 in each well at reservoir j tco2 s set of sources r set of reservoirs i set of all graph nodes k set of candidate pipeline arcs c set of pipeline capacity trends q i s r c co2 production rate at source i mtco2 yr q j w e l l capacity of each well at reservoir j mtco2 q j r e s capacity of reservoir j mtco2 q k c m a x maximum capacity of pipeline k with trend c mtco2 yr q k c m i n minimum capacity of pipeline k with trend c mtco2 yr α k c cost to transport one tonne of co2 on pipeline k with trend c β k c cost to build pipeline k with trend c model decision variables s i 0 1 indicates if source at node i is opened r j 0 1 indicates if reservoir at node j is opened y k c 0 1 indicates if pipeline k with trend c is built a i r amount of co2 captured at source i tco2 yr b j r amount of co2 injected into reservoir j tco2 yr w j n number of wells opened at reservoir j p k c r amount of flow in pipeline k with trend c tco2 yr c o 2 c a p r target co2 capture amount for project tco2 yr the mip is driven by the objective function min i s f i s r c s i v i s r c a i c a p t u r e cos t k k c c α k c p k c p i p e l i n e u s e c o s t k k c c β k c y k c p i p e l i n e b u i l d c o s t j r f j r e s r j f j w e l l w j v j w e l l b j s t o r a g e c o s t subject to the following constraints a q k c m i n y k c p k c q k c m a x y k c k k c c b c c y k c 1 k k c k k s r c k n c c p k c k k d s t k n c c p k c a n i f n s b n i f n r 0 o t h e r w i s e n i d a i q i s r c s i i s e b j q j w e l l w j j r f b j q j r e s r j j r g i s a i c o 2 c a p where constraint a ensures that pipeline is built before transporting co2 and that its capacity is appropriate for the amount of flow b allows at most one linear piece i e trend pipeline between any two nodes c enforces conservation of flow at each node d ensures a source must be opened to capture co2 and that capture is capped by its maximum production e ensures a well must be constructed to inject co2 and caps injection by its maximum injectivity f caps storage for each reservoir by its maximum capacity and g ensures the total capture amount meets the target simccs 2 0 allows users to model co2 storage economics and engineering to a much more detailed level than any other ccs infrastructure model for example f j r e s does not depend on which wells are opened it gives the user the chance to separate opening costs that are independent of the number of wells opened from the costs that are incurred on a per well basis the value of b j is influenced by the number of wells the model selects constraint e but b j is the total injected amount for the entire reservoir not on a per well basis the model assumes that for a given reservoir each well is identical same costs same capacities if this were not the case and some region of a reservoir were logistically challenging to put wells into higher operating costs or had much lower injectivity smaller well capacity then each region could be treated as a separate reservoir since all wells are assumed to be identical for a given reservoir the amount of co2 to inject per well is defined by b j w j 3 1 simccs 2 0 platform and development simccs 2 0 is a java application that consists of a graphical user interface gui that is used to drive data management pipeline network generation and optimization model formulation solution and visualization fig 2 3 2 data management simccs 2 0 determines infrastructure designs based on the tradeoff of costs associated with capturing transporting and storing co2 in the region of interest these costs and other parameters are supplied to simccs 2 0 in the form of source location parameters storage location parameters and weighted cost surface input files source data includes parameters for each source location including an id name latitude longitude location fixed opening cost variable operating cost per unit capture cost and a maximum co2 production rate storage data includes parameters for each storage location including a label latitude longitude location fixed opening cost for the entire location variable operating cost for the entire location fixed opening and variable operating costs for each well injection cost and a maximum capacity for each well and for the entire location weighted cost surface data are used to determine the cost of building pipeline networks generating the weighted cost surface involves laying a grid over the modeled domain and determining the cost to traverse from one cell to a neighboring cell this is a function of underlying topography slope and aspect land ownership 10 default classes land use 16 default types crossings rail river and roads existing pipeline right of ways rows and population density hoover et al 2019 separate weighted cost surfaces are used for construction costs and for row costs since these costs are independent for example a steep slope will affect construction but not row costs pipelines are typically buried in trenches while construction costs are essentially identical for cropland and grassland whereas land purchases for these rows are not once simccs 2 0 is directed to a specific dataset the user can visualize the geographic area with source and storage location overlaid on a graphical representation of the weighted cost surface fig 3 1 3 3 pipeline network generation the pipeline network is one of the three key cost drivers for ccs infrastructure design the other two being capture and storage costs unlike capture and storage facilities whose locations are fixed a set of candidate pipeline routes needs to be determined before an optimal ccs design can be constructed simccs 2 0 considers the weighted cost surface as a graph with nodes in the centers of cells and edges connecting each center to the neighbor cells centers costs to traverse cells are accounted for in edge weights direct moves to a diagonal cell are multiplied by a factor of 2 to account for the increased distance of the move compared with a cardinal move this edge weighted graph structure is then used for the rest of the pipeline route generation process an all pairs graph is constructed by finding the shortest cheapest paths between all source storage locations by using shortest path algorithms with cell to cell paths representing the routing of that pipeline a subgraph of the all pairs graph is selected as a candidate pipeline network which in turn corresponds to the set of possible pipeline routes available in the mip there are various methods of selecting the candidate network with simccs 2 0 supporting delaunay triangulation high quality solution and quick and steiner tree preserving subgraphs provable performance guarantees middleton et al 2012c fig 3 2 shows a candidate pipeline network generated by simccs 2 0 the candidate network or routes is not provably optimal but it is demonstrably close to optimal and in practice is typically optimal the candidate network only omits potential arcs that add only marginal benefit in terms of lowest cost routes but prohibitively expand the model size candidate network generation is a very active research area the algorithm utilized in simccs 2 0 is detailed in previous work middleton et al 2012c and is the subject of ongoing research the under development new algorithm approximately preserves the cost of steiner trees from the cost surface graph since steiner trees are the least cost trees that connect a subset of nodes every connected component of a solution will consist of a steiner tree due to this the quality of the candidate network is bounded as a function of optimal 3 4 optimization model formulation and solution once source and storage locations are parameterized and a candidate pipeline network has been identified the user is able to start formulating infrastructure design optimization problems these formulations take the form of mips as detailed in the preceding section simccs 2 0 is packaged with the capability to build both the discrete pipeline and linearized model versions and it is simple for the user to build their own custom model formulations the user formulates the model by determining the project s capital recovery rate project length and target co2 capture amount simccs 2 0 then builds the mip formulation and stores it in a mathematical programming system mps file which is a standardized file format for mip problems fig 3 3 shows the controls simccs 2 0 presents to the user for formulating the optimization model the model is solved by using optimization software to solve the mps file since the model is stored in an mps file many commercial and open source software packages are available to accomplish this task simccs 2 0 default setup is to use ibm s cplex to solve the mps file in the form of an sol solution file modifying simccs 2 0 to work with different optimization software is a straightforward process since simccs 2 0 already stores the model formulation as an mps file the user must simply replace the cplex solver call with one to the desired optimization software in this case the method for reading in solutions should also be modified to reflect the output format of the chosen optimization software since simccs 2 0 is open source users are able to directly modify the source code or contact the developers to suggest feature additions as mentioned previously simccs 2 0 also includes the capability to seamlessly pass mps files to hpc resources through the simccs gateway platform via a web interface https simccs org this capability requires the user to obtain an account that will enable them to transmit the mps file to hpc resources where it is then solved and presents the user with the ability to retrieve the sol file 3 5 optimization model solution display displaying infrastructure design solutions in simccs 2 0 is a straightforward process simccs 2 0 uses the mps and sol files from a given run to determine which source and storage locations were opened how much co2 was captured and stored and where various sized pipelines were purchased this information is then visualized in the gui total costs and costs broken down by capture transport and storage are also displayed for comparison purposes gis shapefiles that represent all of this information are also generated including source locations storage locations pipeline routes and co2 flows these shapefiles can be used in any gis visualization application fig 3 4 shows a solution displayed by simccs 2 0 along with the associated costs of the solution 3 6 model sensitivity simccs 2 0 can visualize solutions that were not output by the optimization software to do this the user would to construct their own solutions or modify existing ones which is easily done once simccs 2 0 visualizes it the user can see the cost of the solution and output it to shapefiles for analysis in other programs to compare two different solutions both optimal or not the user would create two different scenarios and in the software to switch between visualizations of both scenarios users can also compare and contrast the outputted solutions as shapefiles or text files for further analysis simccs 2 0 also innately allows for extensive contingency planning by manipulating input parameters e g making capture costs of 0 will ensure that simccs 2 0 opens a source changing the co2 capture target is done directly in simccs 2 0 as part of the model formulation step other contingency simulations can be performed by manipulating the input data or changing the open source code directly 4 infrastructure design dataset the simccs 2 0 code repository includes data from a case study that models impacts to ccs infrastructure deployment at various scales from pore space considerations all the way up to regional aspects middleton et al 2012a the data included with simccs 2 0 is the regional scale component of the study and consists of capture transportation and storage parameters in the southeastern united states from 20 coal fired power plants and seven potential storage locations the region of interest covers an area of roughly 1000 km by 625 km in southern louisiana mississippi alabama georgia and northern florida detailed justification of the various cost capacity injectivity and emission parameters can be found in the original case study publication this dataset in conjunction with the user manual provides a meaningful example on which new users can familize themselves with the simccs 2 0 software a useful cost surface for a significant region of the united states and a guidline of the required data format and organization fig 4 illustrates simccs 2 0 s solutions for the southeastern united states dataset and a scenario consisting of a 30 year project capturing 110 mtco2 yr in this figure red dots correspond to opened sources with the fraction of available co2 captured represented by the dark red component of the pie chart blue dots correspond to the opened reservoirs with the fraction of available storage capacity used represented by the dark blue component of the pie chart and green links correspond to selected pipeline routes 4 1 availability simccs 2 0 is freely available through a public github repository yaw et al 2018 this repository includes source code for users that wish to directly modify simccs 2 0 as well as a packaged executable version jar file for users that do not wish to compile code on their own the data from the sample dataset described above is included to provide a fully functional example and representative file formats a complete user manual is included in the repository that details the functionality of simccs 2 0 explains the software architecture presents various examples and describes the required data formats this github repository will be continually updated as simccs 2 0 is improved and new capabilities are developed 5 conclusion simccs 2 0 is a software tool with the potential to revolutionize ccs integrated infrastructure design by providing a versatile open source platform for collaborative study researchers are able to modify the software to test novel optimization models and network generation algorithms and also to develop custom hpc resource integration simccs 2 0 is intended to serve as a platform for future research efforts via shared resources throughout the ccs stakeholder community current efforts include expanded integration with hpc resources across the xsede network development of a new optimization model that natively accounts for source and storage uncertainty and development of an entirely new optimization framework that is not dependent on solving computationally expensive mips improvements of simccs 2 0 are also being undertaken that will automate the generation of the weighted cost surface as well as provide the capability to determine source and storage parameters from within the software this effort will reduce the dependency on strictly formatted source storage and weighted cost surface data and will enable users to design ccs infrastructure deployments without having to generate this data by themselves software data availability the source code and an executable java archive jar file of simccs 2 0 is available to the general public as an open source github repository yaw et al 2018 https github com simccs simccs the github repository also includes a detailed user manual including contact information for software developers and a sample dataset from a ccs study in the southeastern united states middleton et al 2012a this is provided for users to acquaint themselves with simccs 2 0 s capability as well as to demonstrate the required data formats acknowledgments this research was funded by the us china advanced coal technology consortium under management of west virginia university the project simccs development and applications funded through the u s department of energy s doe fossil energy program under award no fe 1017 18 fy18 and by the u s department of energy s doe national energy technology laboratory netl through the integrated mid continent stacked carbon storage hub carbonsafe project under award no de fe0029164 the early co2 storage complex in kemper country carbonsafe project under award no de fe0029465 and the rocky mountain carbonsafe project under award no de fe0029280 appendix a supplementary data the following is the supplementary data to this article supplementary data supplementary data appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104560 
26084,eliciting stakeholders mental models is an important participatory modeling pm tool for building systems knowledge a frequent challenge in natural resource management therefore mental models constitute a valuable source of information making it imperative to document them in detail while preserving the integrity of stakeholders beliefs we propose a methodology the rich elicitation approach rea which combines direct and indirect elicitation techniques to meet these goals we describe the approach in the context of the effects of climate change on baltic salmon the rea produced holistic depictions of mental models with more variables and causal relationships per diagram than direct elicitation alone thus providing a solid knowledge base on which to begin pm studies the rea was well received by stakeholders and fulfilled the substantive normative instrumental and educational functions of pm however motivating stakeholders to confirm the accuracy of their models during the verification stage of the rea was challenging keywords cognitive maps participatory modeling stakeholder engagement natural resource management environmental modeling transdisciplinary research 1 introduction mental model or cognitive map elicitation is an essential tool within the increasingly popular field of participatory modeling pm voinov et al 2016 voinov and bousquet 2010 owing to its utility in formalizing knowledge and facilitating problem solving özesmi and özesmi 2004 this technique extracts a person s internal representation of an external reality jones et al 2011 or put another way their conceptualization of a system s causal dynamics moray 1998 built on personal experience knowledge and values johnson laird 1983 jones et al 2011 to cognitively aid in reasoning johnson laird 2010 nersessian 2002 here we develop a methodology the rich elicitation approach rea to improve the elicitation and documentation of stakeholders mental models advancing these processes is crucial to ensure they adequately contribute to the functions of pm fiorino 1990 jones et al 2009 which are 1 the normative function which suggests incorporating stakeholder knowledge into a model increases its legitimacy in the decision making context 2 the substantive function which is the capacity to synthesize available knowledge from a variety of sources to enhance problem solving 3 the instrumental function which describes the process of relationship building between stakeholder groups 1 1 in this study we specifically consider the role of the instrumental function in relationship building between pm practitioners and other stakeholder groups may reduce conflict and ease the implementation of decisions made using model outputs pm also serves a fourth function 4 the educational function which describes the act of engaging with the pm process as an educational experience for stakeholders voinov et al 2018 voinov and bousquet 2010 the rea primarily develops the substantive function of mental model elicitation and documentation while preserving the educational experience elicitation provides voinov et al 2018 voinov and bousquet 2010 however we briefly address our participants perceptions of the process s normative and instrumental value as well substantive gain is an important output of many pm techniques including mental model elicitation which collect inventory and build knowledge relevant to a given issue particularly in complex and data poor contexts özesmi and özesmi 2004 under such circumstances stakeholder knowledge is often the best available source of information krueger et al 2012 kuhnert et al 2010 sutherland 2006 hence its prior application to a variety of natural resource issues including fisheries stock assessment chrysafi et al 2017 haapasaari et al 2013 2012 mäntyniemi et al 2013 and arctic oil spills nevalainen et al 2018 mental model elicitation specifically provides substantive gain because individual stakeholders mental models offer alternative hypotheses about a system s causal dynamics krueger et al 2012 and when aggregated allow for the co production of systems knowledge olazabal et al 2018 stakeholder input is particularly relevant for solving high stakes natural resource related problems when time is limited and action cannot be delayed while more formal scientific knowledge is generated kangas and leskinen 2005 knol et al 2010 moreover mental model elicitation serves an educational function allowing stakeholders to engage with their cognitive structures exposing personal knowledge gaps kaplan and kaplan 1982 zellner 2008 and aiding the development of an integrated understanding of complex socio ecological systems fortuin et al 2011 novak and cañas 2008 smajgl and ward 2013 hence the use of mental models is commonplace in a variety of pm methods including the development of bayesian belief networks haapasaari et al 2012 meynecke et al 2017 smith et al 2018 fuzzy cognitive maps olazabal et al 2018 özesmi and özesmi 2004 solana gutiérrez et al 2017 conceptual content cognitive maps kearney and kaplan 1997 and actors resources dynamics and interactions models etienne et al 2011 mathevet et al 2011 guidelines for discerning whether mental modeling cognitive mapping is an appropriate methodological choice for a given study are given by voinov et al 2018 presently stakeholders mental models are obtained via either direct or indirect elicitation jones et al 2011 during direct elicitation participants individually or in groups actively create and define the structure of their models themselves typically assisted by a facilitator and visualization tools depicting system variables and the connections between them dray et al 2006 haapasaari et al 2012 özesmi and özesmi 2004 this deliberate articulation and visualization of knowledge constitute a learning experience for participants marcot et al 2001 uusitalo 2007 the hallmark of direct elicitation however is an immediate means of verifying participants representations of their mental models jones et al 2011 reducing reliance on the skill and interpretation of an analyst abel et al 1998 this immediate feedback is likely why direct elicitation is more common than indirect elicitation in natural resource related pm alternatively during indirect elicitation an analyst determines the structure of the participant s mental model based on textual information like interview transcripts or written questionnaire responses carley and palmquist 1992 masinde et al 2018 verkerk et al 2017 however an important and largely unexplored question is do these techniques accurately transfer knowledge from brain to paper this question became important when we noticed that the mental models we documented with stakeholders during direct elicitation were simplified in comparison with their verbal descriptions equating to a loss of potentially critical information we suspect this was the unintentional result of time constraints and stakeholder fatigue burgman 2005 coupled with the difficulty in articulating and visualizing variables and complex model structures both on the part of the stakeholder and the pm facilitator hence direct elicitation did not represent stakeholders mental models as accurately as we would have hoped we believed taking an indirect elicitation approach instead may have reduced simplification and subsequent information loss as this technique allows an analyst time to carefully consider and define model variables and structure however this technique is also susceptible to inaccuracies since an indirectly elicited representation of a stakeholder s mental model reflects only the analyst s interpretation of it which may be influenced by their own biases beliefs and values besides we were not keen to give up the educational benefits direct elicitation provides therefore we propose combining direct and indirect elicitation methods to retain the strengths of each while compensating for their shortcomings radonic 2018 adopted a related approach in his cultural anthropological study using audio recordings of stakeholder interviews to ensure they had succeeded in drawing the concepts vocalized during direct elicitation radonic 2018 we take radonic s approach one step further employing indirect elicitation as a verification method for directly elicited models and as a measure to ensure they are represented in the same level of detail as described verbally while maintaining the stakeholder s control over their accuracy we demonstrate that the rea reduces information loss compared to direct elicitation alone resulting in rich models which are holistic depictions of stakeholder knowledge created per their understanding of the world around them in addition to improving the process of mental model elicitation we also believe it is imperative to improve the documentation of pm methodologies which is often poor or bypassed altogether in the literature voinov et al 2016 some resources provide general guidelines relevant to the pm process e g durham et al 2014 but peer reviewed articles describing the lessons learned from completed pm studies are few gray et al 2018 nevalainen et al 2018 and the methodological mistakes others could learn from are rarely published krueger et al 2012 since pm methods are included in virtually all environmental modeling efforts today gray et al 2018 voinov and bousquet 2010 it is important to describe the techniques used to produce and analyze the results of these processes to these ends gray et al 2018 assert that despite increasing interest pm has not been able to establish itself as a cohesive field of study owing to poor reporting and a lack of reproducibility as a solution they suggest the 4p framework for reporting pm studies which encourages documentation of 1 the purpose for choosing the pm approach why 2 the process used to involve participants how 3 the partnerships formed as a result of the pm process who and 4 the products resulting from the pm study what we are dedicated to the improvement of pm methodologies reproducibility and wish to promote the effective and transparent use of stakeholder knowledge in natural resource management and as such report our study using the 4p framework therefore the purpose of this article is to present the methodology the rea we used to enhance mental model richness without sacrificing the integrity of stakeholders ideas or their learning experience through the combination of direct and indirect elicitation techniques further we hope this article will contribute to the development of guidelines for best practices in pm thus deepening the rigor and expanding the utility of these techniques in the scientific process 2 the why the effects of climate change on the salmon system we tested the rea in the context of a problem framing study a process for exploring the dimensions of the problem from multiple perspectives and re defining it to allow for the development of clearer more mutually beneficial and creative solutions bardwell 1991 the goal of our problem framing study was to determine the effects climate change may have on atlantic salmon salmo salar l in the baltic sea hereafter referred to as baltic salmon and to begin to adapt salmon management accordingly we explored this problem with expert stakeholders individually asking them targeted questions and documenting the mental models underlying their answers this text specifically addresses the elicitation of these expert stakeholders mental models not their analysis or aggregation these steps were conducted later to produce a synthesis of these individual results which is intended to be the first step in a multi step modeling process to incorporate climate change effects into the pre existing baltic salmon stock assessment model michielsens et al 2008 the results of this model form the basis of the international council for the exploration of the seas ices fishery management advice to the european union kuikka et al 2014 here we provide the context which prompted us to begin our study baltic salmon management has been a priority since the formation of the internationally adopted salmon action plan in 1997 implemented in response to severe population declines attributed largely to decades of overfishing romakkaniemi et al 2003 and reduced access to spawning rivers romakkaniemi et al 2003 in recent years however salmon populations have rebounded and fish have been observed returning in increasing numbers to many of their traditional spawning grounds helcom 2011 ices 2018 luke 2016 nevertheless baltic salmon are still considered vulnerable helcom 2013 therefore continued management efforts to support the long lasting sustainability and recovery of salmon stocks in the baltic sea are imperative including prompt action to address existent and emerging threats while many factors like overfishing romakkaniemi et al 2003 reduced access to spawning rivers romakkaniemi et al 2003 changing food web dynamics and nutrient deficiencies ejsmond et al 2019 ices 2018 are already understood to affect salmon climate change presents a new challenge for these fish since this phenomenon is expected to bring substantial change to the baltic sea environment graham 2004 helcom 2013 reusch et al 2018 though articles discussing the effects of these changes on baltic salmon are limited a tentative link between these fish and climate change has been established huusko and hyvarinen 2012 jokikokko et al 2016 jutila et al 2005 kallio nyberg et al 2004 russell et al 2012 nevertheless the causal mechanisms by which climate change is likely to impact baltic salmon are under researched at present therefore we believe it is critical to develop a better understanding of the ways in which climate change may affect or may already be affecting baltic salmon and their fishery to better direct further research explore management goals and strategies and to begin the process of incorporating these effects into ices existing stock assessment model as expediently as possible although climate change is by no means the only factor influencing baltic salmon populations incorporating its effects into ices model is a step towards ensuring stock estimates are realistic and that they thereby assist in producing reasonable management recommendations the results of this problem framing can also help kick start a conversation about the impacts of climate change on salmon how management goals and strategies may need to be reassessed considering these changes and the importance of including this issue in a new long term management plan for baltic salmon stocks additionally since the research linking climate change and baltic salmon is limited it is worthwhile to develop a holistic understanding of the intertwined social and ecological systems linking climate change salmon and their fishery as changes in salmon populations are likely to cascade impacting other species the environment and human society this task is time sensitive and research about the topic is lacking therefore we identified pm specifically mental modeling as an appropriate strategy to develop a knowledge base about the effects of climate change on the salmon system 3 who how eliciting documenting mental models via the rich elicitation approach to build our knowledge base about the salmon climate change problem we elicited stakeholders mental models and documented them as influence diagrams using the rea approach illustrated in fig 1 here we describe our experience conducting this process although we expect that the rea will be adapted to suit the specific contexts where it may be applied in the future voinov et al 2016 those interested in implementing the rea should consult both this section of the text and the discussion section for guidance 3 1 preparation 3 1 1 stakeholder selection we included only expert stakeholders hereafter referred to as stakeholders in our study since domain specific knowledge enhances mental model richness nersessian 2002 the distinction between experts stakeholders and expert stakeholders is presently fuzzy within the pm literature krueger et al 2012 however we consider expert stakeholders to be individuals encompassing both the concept of expert defined by extent and depth of their experience fazey et al 2006 and stakeholder broadly considered to be those who influence or are influenced by the research in question durham et al 2014 we considered stakeholders working with salmon and salmon issues either professionally or as part of a registered leisure organization like angling clubs to be sufficiently experienced to be considered experts although we determined expert stakeholders to be the correct group to engage for this pm process we recognize that salmon are a common pool resource managed for the benefit of society and the environment as such engaging non expert stakeholders should also be considered at appropriate points in the salmon management process we identified stakeholders to participate in our study via snowball sampling browne 2005 first we reached out to our contacts whom we considered to be baltic salmon experts based on their contributory and interactional expertise mcbride and burgman 2012 then we asked them to pass on our request for participation to others who might be interested and fit our criteria the 11 stakeholders who chose to participate described their salmon expertise in a variety of contexts and came from a diversity of organizations including a transnational management agency a government ministry a university three county management agencies and five non government organizations see table a1 only stakeholders working in finland and sweden were asked to take part as the majority of baltic salmon production occurs in these two countries ices 2018 to protect our participants privacy they were randomly assigned a letter pseudonym from a k e g stakeholder k 3 1 2 facilitator learning before the direct mental model elicitation sessions hereafter referred to as elicitation sessions mock sessions were conducted to help the facilitator develop the skills needed to elicit and document mental models afterward we revised the elicitation session protocol and questionnaire per the mock participants comments and critiques 3 2 direct mental model elicitation sessions administering the questionnaire elicitation sessions were semi structured and one on one between a facilitator and stakeholder lasting approximately 2 h each we chose to conduct elicitation sessions individually because we were interested in aggregating the thoughts of individuals rather than producing a single collective response the rationale behind this decision was to avoid the influence of perceived normative pressures heeren alexander et al 2016 on the stakeholders depictions of their mental models or over representing the views of the most influential stakeholders burgman 2005 martin et al 2012 stakeholders were asked to describe their own beliefs rather than attempting to represent those of their organizations and audio was recorded throughout the duration of each session 3 2 1 stakeholder learning at the beginning of their elicitation session the stakeholder was asked to describe their career background and interests related to salmon next the facilitator prepared the stakeholder for the task of representing their mental model as an influence diagram haapasaari et al 2012 by explaining how to interpret and visualize them in this format using simple examples fig 2 influence diagrams include three variable types uncertain variables actions and personal valuation of the outcomes goals haapasaari et al 2012 these variables are then connected with arrows representing the causal relationships between them and the direction of the effect haapasaari et al 2012 influence diagrams include personal uncertainty about these relationships expressed as degrees of belief which are elicited either qualitatively haapasaari et al 2012 varis and fraboulet jussila 2002 varis and lahtela 2002 or quantitatively as joint probability distributions e g mäntyniemi et al 2013 by eliciting quantitative degrees of belief influence diagrams are easily transformed into risk assessment models haapasaari et al 2012 however for problem framing we chose a qualitative approach representing uncertainty by the thickness of the arrows drawn between variables with thicker arrows representing more certain relationships whether quantitative or qualitative model building as influence diagrams or otherwise is a useful tool to encourage participants to think deeply and clearly articulate their thoughts lynam et al 2007 marcot et al 2001 uusitalo 2007 3 2 2 direct elicitation during the elicitation session the facilitator asked the stakeholder to consider three primary problem framing questions adapted from those used by haapasaari et al 2012 1 what variables and causal relationships do you think should be considered when determining the impacts of climate change on baltic salmon and their associated fishery 2 what goals do you have for salmon and their fishery in the future considering climate change 3 what management strategies or actions can be undertaken to achieve those goals the purpose of the first question was to elicit the stakeholders mental models of both the direct and indirect cause and effect relationships between salmon the environment society and climate change the second explored these mental models further eliciting the future outcomes the stakeholders hoped to see for salmon highlighting the perceived value of this species it s utility and the importance of addressing the issue of climate change we asked the final question to identify human actions that stakeholders believed affect or could affect the achievement of these goals these questions were reiterated as needed over the course of the elicitation session other questions asked by the facilitator were meant to provide clarification keep the session on task or prompt discussion while influencing the stakeholder as minimally as possible as a starting point for their influence diagrams we gave the stakeholders one variable climate change we also provided them with a core biological model fig a1 originally developed to depict the annual stock dynamics of central baltic herring by haapasaari et al 2012 its inclusion in their diagrams was optional the rationale behind providing this model was to give the stakeholders some sense of the types of connections that could be drawn between different aspects of the salmon life history and the fishery system and to provide a starting point for idea generation at the time we believed the herring model was general enough to be repurposed for this use and vague enough to avoid the anchoring effect oppenheimer et al 2008 tversky and kahneman 1974 the influence diagrams were hand drawn on large sheets of paper and stakeholders chose to either draw the diagrams themselves or allow the facilitator to draw when the facilitator was responsible for drawing she encouraged the stakeholders to explicitly describe the relationships between variables frequently reminded them to interject if the influence diagram was not drawn according to their beliefs and regularly confirmed the model s accuracy after defining the system variables and the structure of their influence diagrams stakeholders were asked to express the certainty or strength of the relationships between variables as either 1 weak 2 medium or 3 strong recording these effect strengths was the last step in the elicitation process to avoid disrupting the stakeholders flow of thoughts as they constructed their models 3 2 3 administering the questionnaire following each elicitation session we sent the stakeholder a link to an online questionnaire created on the surveyplanet https surveyplanet com website the questionnaire included questions intended to 1 contextualize thestakeholders perceptions of the salmon climate change problem 2 determine the utility of problem framing and the elicitation process and 3 assist in improving future elicitations the questions were asked in multiple choice scoring and short answer formats most multiple choice and scoring questions were followed by space for the stakeholders to elaborate if desired unexpectedly surveyplanet recorded multiple values when a stakeholder chose to change their answer in these cases it was impossible to distinguish their final choice and therefore we report the average of the two answers stakeholders could skip questions they did not wish to answer all responses were anonymous except for one stakeholder who completed the questionnaire in paper form and therefore that stakeholder s responses were not anonymous to us 3 2 4 digitizing influence diagrams once the elicitation sessions were completed the facilitator digitized each hand drawn influence diagram using genie 2 0 academic software bayesfusion llc 3 3 indirect elicitation 3 3 1 transcription note taking coding to begin the indirect elicitation phase of our study the audio recording from each elicitation session was transcribed then the analyst read and coded each transcription categorizing passages as variables within the salmon system causal linkages between them effect strengths or remarks about the elicitation session itself additionally the analyst recorded two sets of notes on each transcription hereafter referred to as elicitation notes a and b a documents the stakeholders comments during the elicitation session about the activity itself major themes described during the session and novel ideas about either the elicitation process or the effects of climate change on salmon b documents the stakeholders predictions about how climate change will affect baltic salmon and their associated fishery which are not necessarily reflected by their influence diagrams these notes were used for two purposes 1 to analyze and improve the elicitation process and 2 to supplement problem framing analysis later 3 3 2 enhancing influence diagrams following coding each directly elicited influence diagram was enhanced by restoring all the variables and causal relationships that had been described verbally by the stakeholders during the elicitation session but which were later identified by the analyst as missing or over simplified upon inspection of the coded transcripts and elicitation notes a 3 4 verification standardization 3 4 1 sending enhanced diagrams for stakeholder approval first revision once the enhanced versions of the influence diagrams were completed they were sent back to the stakeholders via email for their approval and to ensure the analyst had represented their beliefs accurately the stakeholders were given a deadline to contact the analyst to express their approval or disapproval of the diagrams and describe any changes they wished to make they were explicitly instructed that no response would be interpreted as their approval additionally stakeholders were asked to provide effect strengths for any new causal linkages included in their influence diagram or original linkages that had not been assigned a strength during the elicitation session after the allotted time had passed any corrections the stakeholders indicated were addressed by the analyst and the influence diagrams were revised 3 4 2 variable harmonization final revision next variables included in the influence diagrams were harmonized at the analyst s discretion as was done in martinez et al 2018 and olazabal et al 2018 as stakeholders often articulated the same concepts in slightly different terms for example how warm the river is and the temperature of the river were both changed to temperature river after harmonization the influence diagrams were revised a final time to reflect these changes 3 5 methodological evaluation an evaluation of our methodology s performance was conducted in two parts first we analyzed the rea s ability to produce richer depictions of stakeholders mental models we compared the number of variables and causal relationships included in the influence diagrams produced via direct elicitation alone with their final versions completed after indirect elicitation verification and standardization in addition to interest in producing richer more accurate mental models we also believe that by taking note of stakeholders experiences and suggestions we can improve the process of mental model elicitation leading to better results in the future therefore for the second stage of our methodological evaluation we consulted select questionnaire questions and elicitation notes a to assess how well the stakeholders felt the approach addressed the four functions of pm additionally we used the stakeholders comments and recommendations extracted from the coded transcriptions and the facilitator s observations to identify areas for methodological improvement and suggest solutions the results of the methodological evaluation of the rea will be the focus of the remainder of this article 4 the what study results 4 1 comparison of influence diagrams post direct elicitation versus post rea the rea produced richer representations of stakeholders mental models than direct elicitation alone following the rea the stakeholders diagrams contained more variables and causal relationships see table 1 for a numerical comparison of the influence diagrams post direct elicitation versus post rea and fig 3 for a visual comparison 4 2 evaluation of the elicitation process table 2 summarizes the stakeholders responses to the questionnaire questions concerning their perceptions of the mental model elicitation process and its ability to fulfill the four functions of pm although their responses varied they generally found the process useful for fulfilling these functions they were most convinced that mental model elicitation offers substantive and educational gains with the clear majority recording the highest or second highest score in favor of these capabilities the stakeholders were less certain however about mental model elicitation s normative and instrumental value their opinions spanned from negative to positive for most questions in these categories see table 2 however the stakeholders would in general feel more satisfied with management decisions if their influence diagrams were considered during the decision making process normative exemplified by majority reporting scores of 4 when asked whether mental model elicitation could be useful in reaching consensus about how to manage the fishery instrumental the majority of stakeholders reported scores of 4 the second instrumental function question received the highest number of negative responses when asked if they would feel more invested in the scientific process by attending problem framing events with researchers instrumental three stakeholders responded with a score of 2 two with non committal 3s and six with scores of 4 the general positive attitude toward mental model elicitation was reflected in the elicitation notes a as well with several stakeholders remarking that they found the process interesting two stakeholders asked to take pictures of their influence diagrams to show colleagues and one later enquired about facilitating a group mental model elicitation for problem framing about another topic one stakeholder mentioned that the elicitation process helped him think deeply about his ideas specifically in response to the facilitator s question about why biodiversity is important intended to encourage him to clarify his thoughts for the influence diagram during the elicitation sessions the majority of stakeholders quickly understood the process of creating influence diagrams only three expressed some uncertainty or difficulty beginning the task this challenge was largely overcome however since the majority of stakeholders preferred to allow the facilitator to draw three suggested both they and the facilitator should contribute while the facilitator was drawing the diagram some stakeholders were highly engaged in ensuring their thoughts were represented accurately providing detailed instructions about how the diagram should be drawn others were more interested in verbalizing their thoughts than documenting them on paper the core biological model see appendix seemed to be of little interest to the stakeholders three stakeholders mentioned they felt it might be useful and wished to keep it on hand as reference material however only one stakeholder incorporated a portion of the model into his influence diagram understanding how to document the strengths of the effects between variables proved to be a more conceptually challenging task than understanding how to draw influence diagrams two stakeholders wondered whether the effect strengths should be considered in isolation concentrating on only the strength of the effect between one variable and another or whether an entire causal pathway with several connected variables should be given the same effect strength in such cases the stakeholders were advised to concentrate on individual linkages between two variables there was also some concern over the relative proportions of the three types of effect strengths one stakeholder wondered whether there should be 33 of each strength category included in the diagram some also questioned whether it was appropriate to include a greater proportion of one strength than the others based on the stakeholders comments it is also apparent that the effect strengths represented their level of certainty about the relationship between two variables as intended but also perhaps their perceived importance of the relationship when questioned about his reasoning behind his assignment of effect strengths one stakeholder stated the weak effect strengths represented relationships he was uncertain of another mentioned however that a strong effect represented established knowledge and also signified that the relationship was important two stakeholders chose not to include effect strengths in their diagrams at all while one did not give a reason for this the other was too fatigued after creating the influence diagram to continue with the effect strengths as well many stakeholders felt uncomfortable with the uncertainty inherent in the complex interactions between climate change salmon and society some also found the scope of the problem difficult to grapple with these issues were reflected in one stakeholder s comment that there are factors related to the effects of climate change on the salmon system that we cannot even begin to speculate about and another stated that there are so many dependencies and feedbacks that i find it difficult to write down phrases like i guess and i don t know were very common throughout the elicitation sessions this uncertainty promptedthe facilitator reassure the stakeholders and remind them that they were not expected to produce a correct diagram but rather one reflecting their own best judgments several stakeholders acknowledged the complexity of the influence diagrams as the elicitation sessions progressed most commonly by indicating that they felt the diagrams were messy two stated their unease with the complexity of the maps more explicitly one jokingly commented there might be too much information in the diagram and another indicated that portions of his diagram had become confusing due to the number of causal linkages the facilitator also remarked that the stakeholders may have altered their description of their mental models in response to her background credentials for example some stakeholders meticulously described the salmon lifecycle when they assumed the facilitator to be a non expert but became less explicit about perceived areas of common knowledge after asking about her educational background and work experience in fisheries science after the enhanced versions influence diagrams were sent back to the stakeholders fig 1 only two responded one stakeholder remarked that the new diagram looked complicated and he trusted our judgment to represent his thoughts accurately a second completed the task in its entirety returned the diagram with most of the effect strengths filled in and confirmed his approval of the enhanced diagram the remaining nine stakeholders chose not to respond to our inquiry thereby tacitly giving their approval one stakeholder contacted us about completing the task after the allotted time but never submitted any requested changes 5 discussion 5 1 rich representations of stakeholders mental models demonstrated by the clear increase in the number of variables and causal linkages included in the stakeholders influence diagrams after implementing the rea coupled with our process to ensure all their articulated thoughts were documented clearly leads us to conclude that the rea was successful in producing richer models we believe this increase in detail equates to more accurate depictions of stakeholders thoughts allowing us to take fuller advantage of the valuable knowledge they possess in short the rea improves the transfer accuracy of mental models from brain to paper making a more complete depiction of stakeholders knowledge accessible as such we suggest rich models are useful in the development of a holistic knowledge base from which to begin comprehensive pm studies on complex knowledge limited socio ecological systems although the rea provides methodological advancements intended to reduce unintentional model simplification we argue a paradigm shift is also necessary to reduce premature intentional simplification typically studies using either direct or indirect elicitation methods contain relatively few variables and causal relationships indicating that needless simplification and subsequent information loss may be widespread in mental model elicitation of both types purposeful simplification may be driven by adherence to a simpler is better mindset analytical and technological constraints or a lack of resources needed to conduct in depth and time consuming studies kuparinen et al 2012 the merit of the simpler is better mindset is often attributed to the principle known as occam s razor however the principle states entities should not be multiplied without necessity schaffer 2015 as such the definition of necessity dictates when and where occam s razor should be applied here the objective is to gather a holistic picture of potentially important social and environmental factors according to a set of stakeholders the diversity and depth of thought they provide useful for informing the models and the problem framing results we use to better understand and develop solutions to complex and poorly understood socio environmental issues in this sense the acquisition of holistic mental models is necessary to develop the broad and comprehensive knowledgebase this task requires from this perspective everything a stakeholder says is necessary and should not be discarded on grounds of artificial preference for simpler explanation or practicality therefore we urge pm practitioners to alter their perception that simpler is always better and to do their best to meet the practical demands of studying complex systems after all to keep every cog and wheel is the first precaution of intelligent tinkering aldo leopold 1972 5 1 1 rich mental models their applications in environmental modelling problem framing environmental modeling processes begin with perceptual models which are systematic and qualitative representations of reality analogous to the mental models discussed here beven 2009 these perceptual models are later simplified and abstracted to accommodate the terms of mathematics and coding beven 2009 since necessary simplification is built into the modeling process which should be deliberate and iterative by default jakeman et al 2006 a rich perceptual model should not hinder later stages of the model development process additionally it is more practical to reduce a rich model than to return to stakeholders later to develop a wider knowledge base if new ideas hypotheses and potential solutions are required we posit that if care is taken to ensure mental models are elicited from truly expert stakeholders dedicated to the success of the pm project richer models may provide a more realistic understanding of complex natural resource problems improving realism is frequently considered a goal of environmental modeling aben et al 2016 kuparinen et al 2012 since improved realism equates to a more accurate representation of the world and therefore improved model performance kuparinen et al 2012 illustrate the necessity of model realism in the context of fisheries stock assessment models notorious for their lack of biological realism and tendency to neglect relevant information as a result these models have frequently failed to provide adequate population dynamics information contributing to stock collapse problem framing which may be used to develop perceptual models at the outset of environmental modeling processes haapasaari et al 2013 2012 is intended to develop a holistic understanding of a problem to best direct problem solving bardwell 1991 in the research policy management and risk assessment arenas as özesmi and özesmi 2004 suggest the more comprehensive the understanding of a problem is the more interventions that can be identified and explored since mental model elicitation is often included in problem framing it is imperative to elicit rich mental models to ensure a more detailed understanding of the issue at hand reducing the chance that important aspects are overlooked in addition to the sources of simplification described simple mental models may also indicate poorly informed stakeholders since learning domain specific information is linked with increased model richness and reasoning capabilities kinchin et al 2000 nersessian 2002 however this is a question of stakeholder selection and beyond the scope of this article 5 2 fulfilling the functions of pm based on the feedback the stakeholders provided they were interested in the elicitations sessions and building their influence diagrams found the topic relevant and felt that the incorporation of their influence diagrams i e knowledge into the fishery management process could substantively aid in problem solving and decision making although their perceptions of elicitation and more generally problem framings normative and instrumental value were more mixed their generally positive responses do suggest these methods hold value in these domains the stakeholders overall positive response to the questionnaire s learning related question and their comments about how building influence diagrams made them think deeply about their beliefs suggest direct elicitation was a valuable learning experience this finding is consistent with existing literature documenting the value of mental models in scientific and environmental education fortuin et al 2011 kinchin et al 2000 we believe this provides a clear rationale for the direct elicitation phase of the rea further excluding the option of performing indirect elicitation alone 5 3 methodological considerations 5 3 1 improving stakeholder response rate to enhanced influence diagrams despite the rea s successes post hoc analysis of its implementation indicated potential areas for improvement one of the most concerning issues was the low response rate we received when we requested that stakeholders review revise approve and include missing effect strengths in the enhanced versions of their influence diagrams verification stage in fig 1 by providing stakeholders with this opportunity for revision we intended to reduce misinterpretations of their thoughts made by the analyst following indirect elicitation abel et al 1998 we suspect the low response rate resulted in part from our choice to communicate remotely via email which often leads to poorer response rates than face to face communication kuhnert et al 2010 nevalainen et al 2018 the complexity of the enhanced influence diagrams may have also made the task of assessing them and providing missing information seem daunting and time consuming a general lack of commitment to the project may have also been at play if the stakeholders felt burdened by the collaboration process or uncertain about its real world value and impact bracken et al 2015 the long gap in time between the elicitations sessions and the delivery of the enhanced models to the stakeholders may have also been to blame for their disengagement in the final stages of the rea to rectify this issue we suggest discussing expectations for the elicitation processes thoroughly with prospective participants before beginning the study we also recommend clearly explaining the importance of the stakeholders role in the study its implications for the natural resource they and pm practitioners are mutually invested in and adhering to a pre determined schedule for interactions with the stakeholders fig 1 this way they will clearly understand what the study will require of them before committing and be fully aware of the real world impacts of their involvement additionally we propose a second face to face meeting between the stakeholder and facilitator after indirect elicitation to assist with the process of correcting and completing the enhanced diagrams to reduce confusion and frustration with the task fig 1 5 3 2 coping with complexity uncertainty messiness another important consideration for future use of the rea is coping with the messiness and uncertainty inherent in describing the cause and effect structure of complex socio ecological systems the influence diagrams messiness which troubled some stakeholders could be reduced by creating them digitally during direct elicitation however this tactic may reduce the ease and accessibility that drawing by hand provides additionally it is logical that the complexity of a well informed expert stakeholder s mental model would reflect the complexity of the socio ecological system in question as such when the goal of elicitation is to preserve the details of stakeholders complex mental models as it is in the rea some degree of messiness is unavoidable the discomfort with messiness however may have been less to do with the actual organization of the figures and more to do with complexity itself the complexity of the salmon system adn the effects climate change has on it seemed to be the source of the uncertainty which caused nearly all the stakeholders to express concern about the validity of their ideas when stakeholders question their ability to provide accurate information they should be encouraged to speak freely and contribute their ideas to the best of their ability despite their uncertainty since expert knowledge is likely the best source of information when access to more traditional forms of data is unavailable their opinions are valuable kuhnert et al 2010 sutherland 2006 during his elicitation session one stakeholder suggested that asking participants only about issues directly related to their fields of expertise may go a step beyond encouragement to help them feel more competent and confident in expressing their ideas firstly he suggested that since climate change is highly complex and predicting its effects on the abiotic environment is generally not directly within the knowledge domain of salmon experts it may help to provide a specific climate change scenario and subsequent abiotic effects at the outset of the elicitation process this would allow stakeholders to focus on what they know well how the abiotic environment affects fish the biological community salmon are embedded within and the downstream impacts on the socio economic system taking this idea a step further he suggested the creation of an aggregated model of the salmon climate change system developed by combining the knowledge of a variety of experts each specializing in one portion of the system although this may be the best possible way to produce the most thorough depiction of the system as burgman 2005 writes modeling is a balancing act between keeping experts within their domain of knowledge and putting aside sufficient time for the elicitation process in short such an effort may require more time and effort than is feasible for practical reasons like budget restrictions or the timeline of the political decision making process 5 3 3 representing mental models as influence diagrams in addition to the issues mentioned above several small improvements should be made to the rea s direct elicitation phase fig 1 based on our experience firstly the mild confusion expressed by some stakeholders about how to draw influence diagrams could be corrected by spending more time teaching them to express their thoughts in this format perhaps the facilitator could coach stakeholders to develop this skill through practice with a simpler and more straightforward subject facilitators should consider this tactic when stakeholders directly express confusion or ask the facilitator to draw their map without explaining why they prefer this arrangement the high percentage of stakeholders who preferred to allow the facilitator to draw could be linked with discomfort in visualizing mental models as influence diagrams stakeholders must develop enough competence with this process to interject if they feel their views have been misrepresented since there was some confusion surrounding the concept of effects strengths and how to assign them we suggest describing this more thoroughly during the stakeholder learning portion of the elicitation sessions fig 1 additionally many stakeholders did not assign effect strengths to their models or did not complete the task presumably since the demanding modeling process exhausted them burgman 2005 this issue could be corrected by incorporating more breaks into the elicitations sessions breaking it into multiple sessions or recording effects strengths throughout the diagramming process rather than leaving the task until the end assigning effects strengths at the end of the elicitation sessions may be preferable however because stakeholders may fatigue before completing their model s structure which is likely more detrimental than missing effect strengths 5 3 4 coping with biases beliefs heuristics values other considerations for future users of the rea are the biases beliefs heuristics and values bbhvs of both participants and modelers which are inherent in every pm process and shape their outcomes glynn et al 2017 hämäläinen 2015 left unchecked bbhvs can diminish the rigor and credibility of the scientific process glynn et al 2017 therefore pm practitioners must recognize the influence bbhvs exert on their studies and their origins which are not only the study s participants but also themselves glynn et al 2017 hämäläinen 2015 to these ends pm practitioners should introspectively evaluate how their own preferences values and motivations may affect a project from its inception to the publication of its results in addition to how the participants do further a pm practitioner s bbhvs may affect the participants behavior slotte and hämäläinen 2015 therefore all conscience pm practitioners must acknowledge the problems bbhvs pose and strive to reduce their influence where appropriate hämäläinen 2015 to aid in this pursuit all pm practitioners should familiarize themselves with the bbhv literature when planning a pm study seminal works regarding bbhvs include kahneman 2011 kahneman and tversky 2012 1979 tversky and kahneman 1981 1974 glynn 2014 2017 glynn et al 2017 hämäläinen 2015 voinov et al 2016 describe bbhvs in the context of natural resource related pm while a robust understanding of this field will assist pm practitioners in anticipating and therefore mitigating the effects of bbhvs their consideration should not end with the preparation phase of the rea but should continue throughout the project fig 1 while conducting elicitations using the rea approach we addressed the potential effects of bbhvs during all its steps from preparation to verification standardization fig 1 during preparation we carefully considered the wording of the three problem framing questions see section 3 2 and the questionnaire and later discussed and revised them per the suggestions made by our test participant we did this to reduce framing biases tversky and kahneman 1981 which may have caused us to unintentionally lead stakeholders to respond in a manner supporting our own beliefs about the effects of climate change on salmon we were also careful not to include leading information in our communications with the stakeholders before the study discussing only the necessary logistics information our justification for contacting them and minimally stating the topic we planned to discuss nevertheless our study s focus on the effects of climate change may have itself introduced a framing bias implying that climate change does indeed impact these fish to control for this potential bias however we included questions 6 11 in the questionnaire see appendix which were intended to gauge whether the stakeholder believed climate change will affect salmon by how much and during what time frame in general the stakeholders tended to believe that it would affect these fish although some indicated that climate change was not the most pressing threat facing baltic salmon as described in section 3 2 we conducted direct elicitations sessions one on one with each stakeholder to avoid the influences of social norms heeren alexander et al 2016 and group dynamics glynn et al 2017 these issues may have otherwise altered the stakeholders depictions of their mental models via for example groupthink janis 1982 mccauley 1989 or lead to issues like the overrepresentation of the most influential stakeholder s ideas burgman 2005 martin et al 2012 although conducting elicitations individually may limit problem solving potential or diminish the benefits a group setting offers like strengthening relationships between stakeholders we believe our strategy was justifiable given our aims our problem framing study and hence the rea were intended to build as holistic a view of a complex system as possible as such we were uncomfortable with the limits group interactions may have imposed nevertheless the rea as with all pm methodologies should be adaptable depending on the circumstances and aim of individual pm processes voinov et al 2016 therefore we acknowledge that the rea could be conducted in a group setting if desired following the same procedure we have described here during direct elicitation the facilitator s bbhvs may also affect stakeholders influence diagrams when drawing a facilitator may depict a skewed interpretation of the stakeholder s statements which instead reflect their own bbhvs to reduce this effect in our study the facilitator ensured the stakeholder could see what she was drawing consistently asked for confirmation of her interpretation s accuracy and encouraged interjection at the first sign of departure from the stakeholder s ideas see section 3 2 2 however if a stakeholder is uncertain or uncomfortable with the process of drawing causal diagrams their ability and confidence to interject may be jeopardized for this reason the facilitator was as thorough as possible when explaining causal diagrams how to read them and how to produce them during the stakeholder learning portion of direct elicitation for future implementations of the rea we would add that if a stakeholder seems unsure about how to proceed or asks the facilitator to draw the facilitator should ask the stakeholder to briefly draw a sample causal diagram and explain its contents before moving on additionally when confirming the accuracy of the diagram with a stakeholder the facilitator should verbally describe what each portion of the diagram depicts indicating her position within the diagram as she describes it the facilitator s behavior or speech may also subtly influence the visualization of a participant s mental model to reduce this influence in our study the facilitator made an effort to remain encouraging of the participant s progress drawing or describing their mental model while remaining neutral about its contents instead she made comments intended to request clarification further explanation confirm the model s accuracy before conducting our elicitations we were keenly aware of the problem inherent in indirect elicitation without the stakeholder s immediate guidance a pm practitioner may inject their own bbhvs as they attempt to reproduce the stakeholder s mental model from textual information see introduction for this reason we conducted direct elicitation first it was imperative to ensure in real time that each stakeholder s influence diagram was drawn according to their internal mental model during indirect elicitation careful coding and note taking before enhancing the diagrams served to limit the analyst s interpretation and provide a trail of justification for each change made lastly we incorporated the verification stage during which that stakeholder must approve of the changes made by the pm practitioner during this phase the stakeholder also has the chance to make changes to the model or revoke permission for its use providing one further check on the pm practitioner s interpretation however we discussed the difficulties we encountered during this stage in section 5 3 1 despite these safeguards we suspect some of our actions during direct elicitation may have unintentionally biased the stakeholders influence diagrams although we initially thought the core biological model for herring see fig a1 was general enough not to trigger anchoring bias oppenheimer et al 2008 tversky and kahneman 1974 when used as an example during the stakeholder learning fig 1 we cannot rule out the possibility that it did additionally based on the stakeholders disinterest in it we believe it was unnecessary as a tool for idea generation explaining the core biological model also took valuable time which could have been better spent practicing the process of building influence diagrams or on the elicitation process itself due to its lack of utility and its potential to induce anchoring bias we recommend only exposing participants to example diagrams entirely unrelated to the subject at hand during the stakeholder learning we are also concerned that the stakeholders knowledge about the facilitator s professional background may have influenced the study stakeholders often asked about the facilitator s career background and educational history while building rapport at the beginning of their elicitations sessions later some seemed to tailor their descriptions of their mental models accordingly after learning of the facilitator s experience with fisheries science stakeholders tended to leave out the causal relationships within their mental model they assumed were common knowledge between them for example the stages of the salmon life cycle to adapt to this situation in real time during our study the facilitator frequently requested that the stakeholder be explicit about the causal relationships within their mental model explain as if to a non expert and clarify portions of their model where they appeared to assume something about the facilitator s prior knowledge we recommend that other pm facilitators follow this practice as well further the facilitator should not discuss their background with stakeholders although any interested stakeholder could undoubtedly find this information online before meeting the facilitator despite the potential for a pm practitioner s bbhvs to affect the visualization of stakeholders mental models the rea coupled with our advice for its implementation and perhaps facilitation training voinov et al 2016 can address many of these issues further we reiterate that by coupling direct and indirect elicitation the rea reduces the potential introduction of an analyst s bbhvs while maintaining the thoroughness indirect elicitation provides additionally questionnaires implemented after both direct and indirect elicitation fig 1 should include questions to gauge the participants understanding of and satisfaction with these processes this information should be used to improve future facilitation of the rea and to develop the methodology over time as such the rea provides a cohesive methodology which acknowledges and reduces the effects of bbhvs we believe this is a step forward for mental model elicitation in pm nevertheless pm practitioners cannot control for their influence completely and therefore in addition to learning from past facilitations and endeavoring to reduce their influence in each subsequent facilitation pm practitioners should thoughtfully and honestly report how bbhvs may have influenced their study 5 3 5 formalizing core principles process in addition to the methodological recommendations we have provided here we also recommend that all pm processes including the rea be governed by a set of thoughtfully developed core principles agreed upon by all the actors involved voinov et al 2016 although by necessity these principles will differ between pm processes owing to the vastly different circumstances under which they are conducted their aim should be to establish norms or rules of engagement for each pm process and to ensure it is conducted in an effective manner and that results contribute meaningfully to knowledge co production and decision making voinov et al 2016 these principles should be based on ensuring transparency accountability and follow through between pm practitioners participants and the end users of the pm process s products additionally they should act as a code of conduct between these groups ensuring fairness civility and fostering trust between them while reducing the influence of bbhvs glynn et al 2017 voinov et al 2016 an example of such core principles is available in voinov et al 2016 to ensure the pm process adheres to these principles a process for appropriately documenting and reporting the pm process is essential although no standard process has yet been identified gray et al 2018 s 4p framework the records of engagement suggested by glynn et al 2018 or both could help fulfill this need although we discussed the general aims of our study with the stakeholders how their contributions would be used and made a commitment to inform them about how the study results were ultimately employed we believe a more formalized process for discussing and agreeing on the study process and its core principles would have been warranted fig 1 principles process discussion a more thorough discussion about what to expect from the pm process about the aims of the study and more contact with the stakeholders throughout may have improved their commitment to it potentially encouraging more engagement during the model verification stage of the rea in the future discussing the rea process itself with stakeholders may prepare them better for elicitation and emphasize the importance of their continued involvement in the study after direct elicitation additionally agreement on the core principles of the project may have given the stakeholders a stronger voice in shaping the study and its outcomes into more relevant and mutually beneficial real world advice for salmon management in the face of climate change a greater degree of connection transparency and shared control over the project may improve the fulfillment of the instrumental and normative pm functions as well it is also worth noting that pm is typically a sub process within a larger scientific and decision making efforts to govern socio ecological systems while we have discussed the importance of defining core principles and processes in the context of pm specifically these ideas also apply to the overarching processes they are embedded within see glynn et al 2018 and glynn et al 2017 for discussion of these ideas in the broader scientific and decision making context 5 4 analyzing rich mental models problem framing although the purpose of this article is to describe the rea not discuss the results of our problem framing study we find it pertinent to exemplify how one can use rich depictions of mental models as stated previously the purpose of collecting rich influence diagrams was to frame the problem of the impacts of climate change on baltic salmon this study s results are intended to guide the incorporation of climate change effects into ices existing baltic salmon stock assessment model by indicating areas of special concern that the model should take into account and prepare for future management challenges we suggest the number of times the stakeholders included a particular variable category of variables life stage or habitat in their causal diagrams can serve as a proxy for their importance within the salmon climate change system therefore more frequently identified variables should be prioritized for inclusion in the model we recognize there are disadvantages to prioritization based on frequency alone as this technique is not well suited for capturing important causal pathways or themes for example we may be able to detect that river temperature age at smoltification and the number of salmon occur frequently in stakeholders influence diagrams however by considering their frequency alone we may miss the concept they represent together a stakeholder may have used these variables to indicate that changes in river temperature alter the time it takes for young salmon to leave the river altering the age at maturity resulting in changes in population size stakeholders may describe the same process in vastly different terms and different levels of specificity further complicating the task of identifying themes therefore we suggest it is imperative to couple this analysis strategy with analysis of elicitation notes a and b fig 1 which describe the stakeholder s thoughts predictions and common themes alternatively visualization techniques like fuzzy cognitive mapping fcm olazabal et al 2018 özesmi and özesmi 2004 solana gutiérrez et al 2017 may help to more smoothly aggregate stakeholders depictions of their mental models making identification of frequently described causal pathways themes easier fcms can also be deconstructed to explore causal relationships pertaining to particular variables of interest olazabal et al 2018 creating fmcs also requires mental model elicitation and therefore the rea approach could be applied to this methodology as well additionally we recognize the number of times a variable is mentioned may also be indicative of the limits of stakeholders knowledge with well understood variables and causal pathways occurring more frequently than potentially poorly understood but highly instrumental ones nevertheless the inclusion of frequently cited variables in ices stock assessment model should reflect the best available knowledge providing a starting point for further work and promoting the model s legitimacy in the eyes of the stakeholders and hopefully in their peers if desired after the decisions about which aspects of the salmon climate change system to include in the stock assessment model have been made a second pm process similar to task 1 described in haapasaari et al 2013 can begin as in their study we propose that stakeholders build directed acyclic graphs dag explaining how the variables identified during the elicitation sessions connect with the current stock assessment model and with climate change unlike the qualitative influence diagrams we produced for problem framing the strengths of the effects between variables in dags should be quantified expressed as joint probability distributions jensen and nielsen 2007 whether a causal link is positive or negative should also be noted in their study mäntyniemi et al 2013 2 2 further analysis of this study is available in haapasaari et al 2013 instructed stakeholders to choose variables to include in their dags from a previously collected list provided by herring experts in our case these variables would come from the influence diagrams developed during the elicitation sessions described here once completed the dags can be pooled via bayesian model averaging mäntyniemi et al 2013 and subsequently incorporated into the stock assessment model fully quantified influence diagrams also enable value of information voi analysis mäntyniemi et al 2009 which measures the maximum amount a decision maker should be willing to pay to obtain more precise information before making a decision as such voi can help determine what data to collect to assist decision making in addition to the utility they provide in incorporating new information into environmental models rich depictions of mental models provide context and justification for doing so their development also serves as a brainstorming tool identifying the goals stakeholders have for a particular natural resource strategies to reach those goals areas where knowledge is lacking and providing new testable hypotheses about how a system works beyond the results of the study themselves the process of conducting mental model elicitation promotes stakeholder learning and causal thinking fortuin et al 2011 kinchin et al 2000 marcot et al 2001 uusitalo 2007 and begins to build stakeholder support for management recommendations fiorino 1990 jones et al 2009 5 5 future directions we suggest future users of the rea implement the suggestions for methodological improvement discussed in section 5 3 and that targeted questionnaires administered following direct elicitation and the completion of the full rea process could develop the methodology further fig 1 final questionnaire additionally we encourage further advancements to ensure mental model elicitation fulfills all functions of pm effectively beyond the practical advice and suggestions we provide here we also believe pm practitioners should strive to reduce the loss of valuable stakeholder knowledge even further the most pressing barrier preventing the acquisition of holistic stakeholder knowledge in pm is limited participation since typical studies include 30 or fewer participants voinov and bousquet 2010 eliciting the beliefs of such a small segment of the stakeholder population constitutes its own form of knowledge simplification and potentially reduces pm s substantive normative instrumental and educational value these low numbers may be the result of limited time financial and analytical resources or arise from the difficulty in securing stakeholder participation nevalainen et al 2018 these obstacles may be circumvented by making the pm process easier more enjoyable or by connecting it more clearly with real world impact improving communication styles and channels between stakeholders and researchers may also help traditionally stakeholder participation has been increased by conducting group pm activities like group problem framings but eliciting stakeholder knowledge in groups presents drawbacks as well see sections 3 2 and 5 3 4 still group participation may only marginally increase the number of stakeholders involved in pm processes less traditionally text analysis techniques like topic modelling blei 2012 from the computational social sciences literature could provide analytical tools for extracting important insights from text interview or elicitation session transcripts questionnaires tweets blogs etc provided by a number of stakeholders orders of magnitude greater than more traditional means allow however employing these techniques could mean less face to face time between researchers and stakeholders limited verifiability of their ideas and less opportunity for learning additionally the time consuming steps like coding and note taking involved in traditional pm studies force the analyst to engage deeply with the data improving their depth of knowledge and ability to draw conclusions from the data as such alternative means for incorporating more stakeholders into the pm process should be developed 6 conclusions in conclusion the rich elicitation approach rea presented here is a strategy for eliciting rich mental models from expert stakeholders the approach s novelty comes from the deliberate combination of direct and indirect elicitation strategies to ensure stakeholders mental models are represented as holistically as possible while preserving the integrity of their knowledge and the learning process inherent in direct elicitation we believe this approach can and should be adapted and applied to any pm process involving the elicitation of mental models though the rich mental models the rea produces are time consuming to create and complex making their analysis more challenging we believe they are necessary for forming a strong knowledge base on which to begin any pm project in the face of data limitation and incomplete scientific knowledge greater detail allows researchers to take advantage of the full depth of stakeholder knowledge reducing the propensity to overlook potentially instrumental causal pathways promoting the production of new testable hypotheses improving realism and generating more thorough solutions and management strategies developing a comprehensive knowledgebase may also improve the resilience of pm projects by providing a reservoir of new ideas when researchers find themselves going back to the drawing board despite the challenges the best available expert knowledge should be used to its fullest potential to solve natural resource related challenges we also encourage researchers and pm practitioners to transparently and thoughtfully share their experiences to allow for the development of best practices and to bring pm into the light as a legitimate and valuable field of its own acknowledgments we would like to warmly thank the 11 stakeholders who participated in this study and tapani pakarinen who provided helpful feedback in preparation for the elicitations sessions thank you also to atso romakkaniemi and the ices assessment working group on baltic salmon and trout wgbast for their support author contributions k l s m and p h designed the research k l performed the research k l analyzed the data s m j v and p h supervised the research and analysis and k l s m j v and p h wrote the article appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104589 funding this study received funding from the university of helsinki s doctoral program in interdisciplinary environmental sciences denvi and the finnish national agency for education 
26084,eliciting stakeholders mental models is an important participatory modeling pm tool for building systems knowledge a frequent challenge in natural resource management therefore mental models constitute a valuable source of information making it imperative to document them in detail while preserving the integrity of stakeholders beliefs we propose a methodology the rich elicitation approach rea which combines direct and indirect elicitation techniques to meet these goals we describe the approach in the context of the effects of climate change on baltic salmon the rea produced holistic depictions of mental models with more variables and causal relationships per diagram than direct elicitation alone thus providing a solid knowledge base on which to begin pm studies the rea was well received by stakeholders and fulfilled the substantive normative instrumental and educational functions of pm however motivating stakeholders to confirm the accuracy of their models during the verification stage of the rea was challenging keywords cognitive maps participatory modeling stakeholder engagement natural resource management environmental modeling transdisciplinary research 1 introduction mental model or cognitive map elicitation is an essential tool within the increasingly popular field of participatory modeling pm voinov et al 2016 voinov and bousquet 2010 owing to its utility in formalizing knowledge and facilitating problem solving özesmi and özesmi 2004 this technique extracts a person s internal representation of an external reality jones et al 2011 or put another way their conceptualization of a system s causal dynamics moray 1998 built on personal experience knowledge and values johnson laird 1983 jones et al 2011 to cognitively aid in reasoning johnson laird 2010 nersessian 2002 here we develop a methodology the rich elicitation approach rea to improve the elicitation and documentation of stakeholders mental models advancing these processes is crucial to ensure they adequately contribute to the functions of pm fiorino 1990 jones et al 2009 which are 1 the normative function which suggests incorporating stakeholder knowledge into a model increases its legitimacy in the decision making context 2 the substantive function which is the capacity to synthesize available knowledge from a variety of sources to enhance problem solving 3 the instrumental function which describes the process of relationship building between stakeholder groups 1 1 in this study we specifically consider the role of the instrumental function in relationship building between pm practitioners and other stakeholder groups may reduce conflict and ease the implementation of decisions made using model outputs pm also serves a fourth function 4 the educational function which describes the act of engaging with the pm process as an educational experience for stakeholders voinov et al 2018 voinov and bousquet 2010 the rea primarily develops the substantive function of mental model elicitation and documentation while preserving the educational experience elicitation provides voinov et al 2018 voinov and bousquet 2010 however we briefly address our participants perceptions of the process s normative and instrumental value as well substantive gain is an important output of many pm techniques including mental model elicitation which collect inventory and build knowledge relevant to a given issue particularly in complex and data poor contexts özesmi and özesmi 2004 under such circumstances stakeholder knowledge is often the best available source of information krueger et al 2012 kuhnert et al 2010 sutherland 2006 hence its prior application to a variety of natural resource issues including fisheries stock assessment chrysafi et al 2017 haapasaari et al 2013 2012 mäntyniemi et al 2013 and arctic oil spills nevalainen et al 2018 mental model elicitation specifically provides substantive gain because individual stakeholders mental models offer alternative hypotheses about a system s causal dynamics krueger et al 2012 and when aggregated allow for the co production of systems knowledge olazabal et al 2018 stakeholder input is particularly relevant for solving high stakes natural resource related problems when time is limited and action cannot be delayed while more formal scientific knowledge is generated kangas and leskinen 2005 knol et al 2010 moreover mental model elicitation serves an educational function allowing stakeholders to engage with their cognitive structures exposing personal knowledge gaps kaplan and kaplan 1982 zellner 2008 and aiding the development of an integrated understanding of complex socio ecological systems fortuin et al 2011 novak and cañas 2008 smajgl and ward 2013 hence the use of mental models is commonplace in a variety of pm methods including the development of bayesian belief networks haapasaari et al 2012 meynecke et al 2017 smith et al 2018 fuzzy cognitive maps olazabal et al 2018 özesmi and özesmi 2004 solana gutiérrez et al 2017 conceptual content cognitive maps kearney and kaplan 1997 and actors resources dynamics and interactions models etienne et al 2011 mathevet et al 2011 guidelines for discerning whether mental modeling cognitive mapping is an appropriate methodological choice for a given study are given by voinov et al 2018 presently stakeholders mental models are obtained via either direct or indirect elicitation jones et al 2011 during direct elicitation participants individually or in groups actively create and define the structure of their models themselves typically assisted by a facilitator and visualization tools depicting system variables and the connections between them dray et al 2006 haapasaari et al 2012 özesmi and özesmi 2004 this deliberate articulation and visualization of knowledge constitute a learning experience for participants marcot et al 2001 uusitalo 2007 the hallmark of direct elicitation however is an immediate means of verifying participants representations of their mental models jones et al 2011 reducing reliance on the skill and interpretation of an analyst abel et al 1998 this immediate feedback is likely why direct elicitation is more common than indirect elicitation in natural resource related pm alternatively during indirect elicitation an analyst determines the structure of the participant s mental model based on textual information like interview transcripts or written questionnaire responses carley and palmquist 1992 masinde et al 2018 verkerk et al 2017 however an important and largely unexplored question is do these techniques accurately transfer knowledge from brain to paper this question became important when we noticed that the mental models we documented with stakeholders during direct elicitation were simplified in comparison with their verbal descriptions equating to a loss of potentially critical information we suspect this was the unintentional result of time constraints and stakeholder fatigue burgman 2005 coupled with the difficulty in articulating and visualizing variables and complex model structures both on the part of the stakeholder and the pm facilitator hence direct elicitation did not represent stakeholders mental models as accurately as we would have hoped we believed taking an indirect elicitation approach instead may have reduced simplification and subsequent information loss as this technique allows an analyst time to carefully consider and define model variables and structure however this technique is also susceptible to inaccuracies since an indirectly elicited representation of a stakeholder s mental model reflects only the analyst s interpretation of it which may be influenced by their own biases beliefs and values besides we were not keen to give up the educational benefits direct elicitation provides therefore we propose combining direct and indirect elicitation methods to retain the strengths of each while compensating for their shortcomings radonic 2018 adopted a related approach in his cultural anthropological study using audio recordings of stakeholder interviews to ensure they had succeeded in drawing the concepts vocalized during direct elicitation radonic 2018 we take radonic s approach one step further employing indirect elicitation as a verification method for directly elicited models and as a measure to ensure they are represented in the same level of detail as described verbally while maintaining the stakeholder s control over their accuracy we demonstrate that the rea reduces information loss compared to direct elicitation alone resulting in rich models which are holistic depictions of stakeholder knowledge created per their understanding of the world around them in addition to improving the process of mental model elicitation we also believe it is imperative to improve the documentation of pm methodologies which is often poor or bypassed altogether in the literature voinov et al 2016 some resources provide general guidelines relevant to the pm process e g durham et al 2014 but peer reviewed articles describing the lessons learned from completed pm studies are few gray et al 2018 nevalainen et al 2018 and the methodological mistakes others could learn from are rarely published krueger et al 2012 since pm methods are included in virtually all environmental modeling efforts today gray et al 2018 voinov and bousquet 2010 it is important to describe the techniques used to produce and analyze the results of these processes to these ends gray et al 2018 assert that despite increasing interest pm has not been able to establish itself as a cohesive field of study owing to poor reporting and a lack of reproducibility as a solution they suggest the 4p framework for reporting pm studies which encourages documentation of 1 the purpose for choosing the pm approach why 2 the process used to involve participants how 3 the partnerships formed as a result of the pm process who and 4 the products resulting from the pm study what we are dedicated to the improvement of pm methodologies reproducibility and wish to promote the effective and transparent use of stakeholder knowledge in natural resource management and as such report our study using the 4p framework therefore the purpose of this article is to present the methodology the rea we used to enhance mental model richness without sacrificing the integrity of stakeholders ideas or their learning experience through the combination of direct and indirect elicitation techniques further we hope this article will contribute to the development of guidelines for best practices in pm thus deepening the rigor and expanding the utility of these techniques in the scientific process 2 the why the effects of climate change on the salmon system we tested the rea in the context of a problem framing study a process for exploring the dimensions of the problem from multiple perspectives and re defining it to allow for the development of clearer more mutually beneficial and creative solutions bardwell 1991 the goal of our problem framing study was to determine the effects climate change may have on atlantic salmon salmo salar l in the baltic sea hereafter referred to as baltic salmon and to begin to adapt salmon management accordingly we explored this problem with expert stakeholders individually asking them targeted questions and documenting the mental models underlying their answers this text specifically addresses the elicitation of these expert stakeholders mental models not their analysis or aggregation these steps were conducted later to produce a synthesis of these individual results which is intended to be the first step in a multi step modeling process to incorporate climate change effects into the pre existing baltic salmon stock assessment model michielsens et al 2008 the results of this model form the basis of the international council for the exploration of the seas ices fishery management advice to the european union kuikka et al 2014 here we provide the context which prompted us to begin our study baltic salmon management has been a priority since the formation of the internationally adopted salmon action plan in 1997 implemented in response to severe population declines attributed largely to decades of overfishing romakkaniemi et al 2003 and reduced access to spawning rivers romakkaniemi et al 2003 in recent years however salmon populations have rebounded and fish have been observed returning in increasing numbers to many of their traditional spawning grounds helcom 2011 ices 2018 luke 2016 nevertheless baltic salmon are still considered vulnerable helcom 2013 therefore continued management efforts to support the long lasting sustainability and recovery of salmon stocks in the baltic sea are imperative including prompt action to address existent and emerging threats while many factors like overfishing romakkaniemi et al 2003 reduced access to spawning rivers romakkaniemi et al 2003 changing food web dynamics and nutrient deficiencies ejsmond et al 2019 ices 2018 are already understood to affect salmon climate change presents a new challenge for these fish since this phenomenon is expected to bring substantial change to the baltic sea environment graham 2004 helcom 2013 reusch et al 2018 though articles discussing the effects of these changes on baltic salmon are limited a tentative link between these fish and climate change has been established huusko and hyvarinen 2012 jokikokko et al 2016 jutila et al 2005 kallio nyberg et al 2004 russell et al 2012 nevertheless the causal mechanisms by which climate change is likely to impact baltic salmon are under researched at present therefore we believe it is critical to develop a better understanding of the ways in which climate change may affect or may already be affecting baltic salmon and their fishery to better direct further research explore management goals and strategies and to begin the process of incorporating these effects into ices existing stock assessment model as expediently as possible although climate change is by no means the only factor influencing baltic salmon populations incorporating its effects into ices model is a step towards ensuring stock estimates are realistic and that they thereby assist in producing reasonable management recommendations the results of this problem framing can also help kick start a conversation about the impacts of climate change on salmon how management goals and strategies may need to be reassessed considering these changes and the importance of including this issue in a new long term management plan for baltic salmon stocks additionally since the research linking climate change and baltic salmon is limited it is worthwhile to develop a holistic understanding of the intertwined social and ecological systems linking climate change salmon and their fishery as changes in salmon populations are likely to cascade impacting other species the environment and human society this task is time sensitive and research about the topic is lacking therefore we identified pm specifically mental modeling as an appropriate strategy to develop a knowledge base about the effects of climate change on the salmon system 3 who how eliciting documenting mental models via the rich elicitation approach to build our knowledge base about the salmon climate change problem we elicited stakeholders mental models and documented them as influence diagrams using the rea approach illustrated in fig 1 here we describe our experience conducting this process although we expect that the rea will be adapted to suit the specific contexts where it may be applied in the future voinov et al 2016 those interested in implementing the rea should consult both this section of the text and the discussion section for guidance 3 1 preparation 3 1 1 stakeholder selection we included only expert stakeholders hereafter referred to as stakeholders in our study since domain specific knowledge enhances mental model richness nersessian 2002 the distinction between experts stakeholders and expert stakeholders is presently fuzzy within the pm literature krueger et al 2012 however we consider expert stakeholders to be individuals encompassing both the concept of expert defined by extent and depth of their experience fazey et al 2006 and stakeholder broadly considered to be those who influence or are influenced by the research in question durham et al 2014 we considered stakeholders working with salmon and salmon issues either professionally or as part of a registered leisure organization like angling clubs to be sufficiently experienced to be considered experts although we determined expert stakeholders to be the correct group to engage for this pm process we recognize that salmon are a common pool resource managed for the benefit of society and the environment as such engaging non expert stakeholders should also be considered at appropriate points in the salmon management process we identified stakeholders to participate in our study via snowball sampling browne 2005 first we reached out to our contacts whom we considered to be baltic salmon experts based on their contributory and interactional expertise mcbride and burgman 2012 then we asked them to pass on our request for participation to others who might be interested and fit our criteria the 11 stakeholders who chose to participate described their salmon expertise in a variety of contexts and came from a diversity of organizations including a transnational management agency a government ministry a university three county management agencies and five non government organizations see table a1 only stakeholders working in finland and sweden were asked to take part as the majority of baltic salmon production occurs in these two countries ices 2018 to protect our participants privacy they were randomly assigned a letter pseudonym from a k e g stakeholder k 3 1 2 facilitator learning before the direct mental model elicitation sessions hereafter referred to as elicitation sessions mock sessions were conducted to help the facilitator develop the skills needed to elicit and document mental models afterward we revised the elicitation session protocol and questionnaire per the mock participants comments and critiques 3 2 direct mental model elicitation sessions administering the questionnaire elicitation sessions were semi structured and one on one between a facilitator and stakeholder lasting approximately 2 h each we chose to conduct elicitation sessions individually because we were interested in aggregating the thoughts of individuals rather than producing a single collective response the rationale behind this decision was to avoid the influence of perceived normative pressures heeren alexander et al 2016 on the stakeholders depictions of their mental models or over representing the views of the most influential stakeholders burgman 2005 martin et al 2012 stakeholders were asked to describe their own beliefs rather than attempting to represent those of their organizations and audio was recorded throughout the duration of each session 3 2 1 stakeholder learning at the beginning of their elicitation session the stakeholder was asked to describe their career background and interests related to salmon next the facilitator prepared the stakeholder for the task of representing their mental model as an influence diagram haapasaari et al 2012 by explaining how to interpret and visualize them in this format using simple examples fig 2 influence diagrams include three variable types uncertain variables actions and personal valuation of the outcomes goals haapasaari et al 2012 these variables are then connected with arrows representing the causal relationships between them and the direction of the effect haapasaari et al 2012 influence diagrams include personal uncertainty about these relationships expressed as degrees of belief which are elicited either qualitatively haapasaari et al 2012 varis and fraboulet jussila 2002 varis and lahtela 2002 or quantitatively as joint probability distributions e g mäntyniemi et al 2013 by eliciting quantitative degrees of belief influence diagrams are easily transformed into risk assessment models haapasaari et al 2012 however for problem framing we chose a qualitative approach representing uncertainty by the thickness of the arrows drawn between variables with thicker arrows representing more certain relationships whether quantitative or qualitative model building as influence diagrams or otherwise is a useful tool to encourage participants to think deeply and clearly articulate their thoughts lynam et al 2007 marcot et al 2001 uusitalo 2007 3 2 2 direct elicitation during the elicitation session the facilitator asked the stakeholder to consider three primary problem framing questions adapted from those used by haapasaari et al 2012 1 what variables and causal relationships do you think should be considered when determining the impacts of climate change on baltic salmon and their associated fishery 2 what goals do you have for salmon and their fishery in the future considering climate change 3 what management strategies or actions can be undertaken to achieve those goals the purpose of the first question was to elicit the stakeholders mental models of both the direct and indirect cause and effect relationships between salmon the environment society and climate change the second explored these mental models further eliciting the future outcomes the stakeholders hoped to see for salmon highlighting the perceived value of this species it s utility and the importance of addressing the issue of climate change we asked the final question to identify human actions that stakeholders believed affect or could affect the achievement of these goals these questions were reiterated as needed over the course of the elicitation session other questions asked by the facilitator were meant to provide clarification keep the session on task or prompt discussion while influencing the stakeholder as minimally as possible as a starting point for their influence diagrams we gave the stakeholders one variable climate change we also provided them with a core biological model fig a1 originally developed to depict the annual stock dynamics of central baltic herring by haapasaari et al 2012 its inclusion in their diagrams was optional the rationale behind providing this model was to give the stakeholders some sense of the types of connections that could be drawn between different aspects of the salmon life history and the fishery system and to provide a starting point for idea generation at the time we believed the herring model was general enough to be repurposed for this use and vague enough to avoid the anchoring effect oppenheimer et al 2008 tversky and kahneman 1974 the influence diagrams were hand drawn on large sheets of paper and stakeholders chose to either draw the diagrams themselves or allow the facilitator to draw when the facilitator was responsible for drawing she encouraged the stakeholders to explicitly describe the relationships between variables frequently reminded them to interject if the influence diagram was not drawn according to their beliefs and regularly confirmed the model s accuracy after defining the system variables and the structure of their influence diagrams stakeholders were asked to express the certainty or strength of the relationships between variables as either 1 weak 2 medium or 3 strong recording these effect strengths was the last step in the elicitation process to avoid disrupting the stakeholders flow of thoughts as they constructed their models 3 2 3 administering the questionnaire following each elicitation session we sent the stakeholder a link to an online questionnaire created on the surveyplanet https surveyplanet com website the questionnaire included questions intended to 1 contextualize thestakeholders perceptions of the salmon climate change problem 2 determine the utility of problem framing and the elicitation process and 3 assist in improving future elicitations the questions were asked in multiple choice scoring and short answer formats most multiple choice and scoring questions were followed by space for the stakeholders to elaborate if desired unexpectedly surveyplanet recorded multiple values when a stakeholder chose to change their answer in these cases it was impossible to distinguish their final choice and therefore we report the average of the two answers stakeholders could skip questions they did not wish to answer all responses were anonymous except for one stakeholder who completed the questionnaire in paper form and therefore that stakeholder s responses were not anonymous to us 3 2 4 digitizing influence diagrams once the elicitation sessions were completed the facilitator digitized each hand drawn influence diagram using genie 2 0 academic software bayesfusion llc 3 3 indirect elicitation 3 3 1 transcription note taking coding to begin the indirect elicitation phase of our study the audio recording from each elicitation session was transcribed then the analyst read and coded each transcription categorizing passages as variables within the salmon system causal linkages between them effect strengths or remarks about the elicitation session itself additionally the analyst recorded two sets of notes on each transcription hereafter referred to as elicitation notes a and b a documents the stakeholders comments during the elicitation session about the activity itself major themes described during the session and novel ideas about either the elicitation process or the effects of climate change on salmon b documents the stakeholders predictions about how climate change will affect baltic salmon and their associated fishery which are not necessarily reflected by their influence diagrams these notes were used for two purposes 1 to analyze and improve the elicitation process and 2 to supplement problem framing analysis later 3 3 2 enhancing influence diagrams following coding each directly elicited influence diagram was enhanced by restoring all the variables and causal relationships that had been described verbally by the stakeholders during the elicitation session but which were later identified by the analyst as missing or over simplified upon inspection of the coded transcripts and elicitation notes a 3 4 verification standardization 3 4 1 sending enhanced diagrams for stakeholder approval first revision once the enhanced versions of the influence diagrams were completed they were sent back to the stakeholders via email for their approval and to ensure the analyst had represented their beliefs accurately the stakeholders were given a deadline to contact the analyst to express their approval or disapproval of the diagrams and describe any changes they wished to make they were explicitly instructed that no response would be interpreted as their approval additionally stakeholders were asked to provide effect strengths for any new causal linkages included in their influence diagram or original linkages that had not been assigned a strength during the elicitation session after the allotted time had passed any corrections the stakeholders indicated were addressed by the analyst and the influence diagrams were revised 3 4 2 variable harmonization final revision next variables included in the influence diagrams were harmonized at the analyst s discretion as was done in martinez et al 2018 and olazabal et al 2018 as stakeholders often articulated the same concepts in slightly different terms for example how warm the river is and the temperature of the river were both changed to temperature river after harmonization the influence diagrams were revised a final time to reflect these changes 3 5 methodological evaluation an evaluation of our methodology s performance was conducted in two parts first we analyzed the rea s ability to produce richer depictions of stakeholders mental models we compared the number of variables and causal relationships included in the influence diagrams produced via direct elicitation alone with their final versions completed after indirect elicitation verification and standardization in addition to interest in producing richer more accurate mental models we also believe that by taking note of stakeholders experiences and suggestions we can improve the process of mental model elicitation leading to better results in the future therefore for the second stage of our methodological evaluation we consulted select questionnaire questions and elicitation notes a to assess how well the stakeholders felt the approach addressed the four functions of pm additionally we used the stakeholders comments and recommendations extracted from the coded transcriptions and the facilitator s observations to identify areas for methodological improvement and suggest solutions the results of the methodological evaluation of the rea will be the focus of the remainder of this article 4 the what study results 4 1 comparison of influence diagrams post direct elicitation versus post rea the rea produced richer representations of stakeholders mental models than direct elicitation alone following the rea the stakeholders diagrams contained more variables and causal relationships see table 1 for a numerical comparison of the influence diagrams post direct elicitation versus post rea and fig 3 for a visual comparison 4 2 evaluation of the elicitation process table 2 summarizes the stakeholders responses to the questionnaire questions concerning their perceptions of the mental model elicitation process and its ability to fulfill the four functions of pm although their responses varied they generally found the process useful for fulfilling these functions they were most convinced that mental model elicitation offers substantive and educational gains with the clear majority recording the highest or second highest score in favor of these capabilities the stakeholders were less certain however about mental model elicitation s normative and instrumental value their opinions spanned from negative to positive for most questions in these categories see table 2 however the stakeholders would in general feel more satisfied with management decisions if their influence diagrams were considered during the decision making process normative exemplified by majority reporting scores of 4 when asked whether mental model elicitation could be useful in reaching consensus about how to manage the fishery instrumental the majority of stakeholders reported scores of 4 the second instrumental function question received the highest number of negative responses when asked if they would feel more invested in the scientific process by attending problem framing events with researchers instrumental three stakeholders responded with a score of 2 two with non committal 3s and six with scores of 4 the general positive attitude toward mental model elicitation was reflected in the elicitation notes a as well with several stakeholders remarking that they found the process interesting two stakeholders asked to take pictures of their influence diagrams to show colleagues and one later enquired about facilitating a group mental model elicitation for problem framing about another topic one stakeholder mentioned that the elicitation process helped him think deeply about his ideas specifically in response to the facilitator s question about why biodiversity is important intended to encourage him to clarify his thoughts for the influence diagram during the elicitation sessions the majority of stakeholders quickly understood the process of creating influence diagrams only three expressed some uncertainty or difficulty beginning the task this challenge was largely overcome however since the majority of stakeholders preferred to allow the facilitator to draw three suggested both they and the facilitator should contribute while the facilitator was drawing the diagram some stakeholders were highly engaged in ensuring their thoughts were represented accurately providing detailed instructions about how the diagram should be drawn others were more interested in verbalizing their thoughts than documenting them on paper the core biological model see appendix seemed to be of little interest to the stakeholders three stakeholders mentioned they felt it might be useful and wished to keep it on hand as reference material however only one stakeholder incorporated a portion of the model into his influence diagram understanding how to document the strengths of the effects between variables proved to be a more conceptually challenging task than understanding how to draw influence diagrams two stakeholders wondered whether the effect strengths should be considered in isolation concentrating on only the strength of the effect between one variable and another or whether an entire causal pathway with several connected variables should be given the same effect strength in such cases the stakeholders were advised to concentrate on individual linkages between two variables there was also some concern over the relative proportions of the three types of effect strengths one stakeholder wondered whether there should be 33 of each strength category included in the diagram some also questioned whether it was appropriate to include a greater proportion of one strength than the others based on the stakeholders comments it is also apparent that the effect strengths represented their level of certainty about the relationship between two variables as intended but also perhaps their perceived importance of the relationship when questioned about his reasoning behind his assignment of effect strengths one stakeholder stated the weak effect strengths represented relationships he was uncertain of another mentioned however that a strong effect represented established knowledge and also signified that the relationship was important two stakeholders chose not to include effect strengths in their diagrams at all while one did not give a reason for this the other was too fatigued after creating the influence diagram to continue with the effect strengths as well many stakeholders felt uncomfortable with the uncertainty inherent in the complex interactions between climate change salmon and society some also found the scope of the problem difficult to grapple with these issues were reflected in one stakeholder s comment that there are factors related to the effects of climate change on the salmon system that we cannot even begin to speculate about and another stated that there are so many dependencies and feedbacks that i find it difficult to write down phrases like i guess and i don t know were very common throughout the elicitation sessions this uncertainty promptedthe facilitator reassure the stakeholders and remind them that they were not expected to produce a correct diagram but rather one reflecting their own best judgments several stakeholders acknowledged the complexity of the influence diagrams as the elicitation sessions progressed most commonly by indicating that they felt the diagrams were messy two stated their unease with the complexity of the maps more explicitly one jokingly commented there might be too much information in the diagram and another indicated that portions of his diagram had become confusing due to the number of causal linkages the facilitator also remarked that the stakeholders may have altered their description of their mental models in response to her background credentials for example some stakeholders meticulously described the salmon lifecycle when they assumed the facilitator to be a non expert but became less explicit about perceived areas of common knowledge after asking about her educational background and work experience in fisheries science after the enhanced versions influence diagrams were sent back to the stakeholders fig 1 only two responded one stakeholder remarked that the new diagram looked complicated and he trusted our judgment to represent his thoughts accurately a second completed the task in its entirety returned the diagram with most of the effect strengths filled in and confirmed his approval of the enhanced diagram the remaining nine stakeholders chose not to respond to our inquiry thereby tacitly giving their approval one stakeholder contacted us about completing the task after the allotted time but never submitted any requested changes 5 discussion 5 1 rich representations of stakeholders mental models demonstrated by the clear increase in the number of variables and causal linkages included in the stakeholders influence diagrams after implementing the rea coupled with our process to ensure all their articulated thoughts were documented clearly leads us to conclude that the rea was successful in producing richer models we believe this increase in detail equates to more accurate depictions of stakeholders thoughts allowing us to take fuller advantage of the valuable knowledge they possess in short the rea improves the transfer accuracy of mental models from brain to paper making a more complete depiction of stakeholders knowledge accessible as such we suggest rich models are useful in the development of a holistic knowledge base from which to begin comprehensive pm studies on complex knowledge limited socio ecological systems although the rea provides methodological advancements intended to reduce unintentional model simplification we argue a paradigm shift is also necessary to reduce premature intentional simplification typically studies using either direct or indirect elicitation methods contain relatively few variables and causal relationships indicating that needless simplification and subsequent information loss may be widespread in mental model elicitation of both types purposeful simplification may be driven by adherence to a simpler is better mindset analytical and technological constraints or a lack of resources needed to conduct in depth and time consuming studies kuparinen et al 2012 the merit of the simpler is better mindset is often attributed to the principle known as occam s razor however the principle states entities should not be multiplied without necessity schaffer 2015 as such the definition of necessity dictates when and where occam s razor should be applied here the objective is to gather a holistic picture of potentially important social and environmental factors according to a set of stakeholders the diversity and depth of thought they provide useful for informing the models and the problem framing results we use to better understand and develop solutions to complex and poorly understood socio environmental issues in this sense the acquisition of holistic mental models is necessary to develop the broad and comprehensive knowledgebase this task requires from this perspective everything a stakeholder says is necessary and should not be discarded on grounds of artificial preference for simpler explanation or practicality therefore we urge pm practitioners to alter their perception that simpler is always better and to do their best to meet the practical demands of studying complex systems after all to keep every cog and wheel is the first precaution of intelligent tinkering aldo leopold 1972 5 1 1 rich mental models their applications in environmental modelling problem framing environmental modeling processes begin with perceptual models which are systematic and qualitative representations of reality analogous to the mental models discussed here beven 2009 these perceptual models are later simplified and abstracted to accommodate the terms of mathematics and coding beven 2009 since necessary simplification is built into the modeling process which should be deliberate and iterative by default jakeman et al 2006 a rich perceptual model should not hinder later stages of the model development process additionally it is more practical to reduce a rich model than to return to stakeholders later to develop a wider knowledge base if new ideas hypotheses and potential solutions are required we posit that if care is taken to ensure mental models are elicited from truly expert stakeholders dedicated to the success of the pm project richer models may provide a more realistic understanding of complex natural resource problems improving realism is frequently considered a goal of environmental modeling aben et al 2016 kuparinen et al 2012 since improved realism equates to a more accurate representation of the world and therefore improved model performance kuparinen et al 2012 illustrate the necessity of model realism in the context of fisheries stock assessment models notorious for their lack of biological realism and tendency to neglect relevant information as a result these models have frequently failed to provide adequate population dynamics information contributing to stock collapse problem framing which may be used to develop perceptual models at the outset of environmental modeling processes haapasaari et al 2013 2012 is intended to develop a holistic understanding of a problem to best direct problem solving bardwell 1991 in the research policy management and risk assessment arenas as özesmi and özesmi 2004 suggest the more comprehensive the understanding of a problem is the more interventions that can be identified and explored since mental model elicitation is often included in problem framing it is imperative to elicit rich mental models to ensure a more detailed understanding of the issue at hand reducing the chance that important aspects are overlooked in addition to the sources of simplification described simple mental models may also indicate poorly informed stakeholders since learning domain specific information is linked with increased model richness and reasoning capabilities kinchin et al 2000 nersessian 2002 however this is a question of stakeholder selection and beyond the scope of this article 5 2 fulfilling the functions of pm based on the feedback the stakeholders provided they were interested in the elicitations sessions and building their influence diagrams found the topic relevant and felt that the incorporation of their influence diagrams i e knowledge into the fishery management process could substantively aid in problem solving and decision making although their perceptions of elicitation and more generally problem framings normative and instrumental value were more mixed their generally positive responses do suggest these methods hold value in these domains the stakeholders overall positive response to the questionnaire s learning related question and their comments about how building influence diagrams made them think deeply about their beliefs suggest direct elicitation was a valuable learning experience this finding is consistent with existing literature documenting the value of mental models in scientific and environmental education fortuin et al 2011 kinchin et al 2000 we believe this provides a clear rationale for the direct elicitation phase of the rea further excluding the option of performing indirect elicitation alone 5 3 methodological considerations 5 3 1 improving stakeholder response rate to enhanced influence diagrams despite the rea s successes post hoc analysis of its implementation indicated potential areas for improvement one of the most concerning issues was the low response rate we received when we requested that stakeholders review revise approve and include missing effect strengths in the enhanced versions of their influence diagrams verification stage in fig 1 by providing stakeholders with this opportunity for revision we intended to reduce misinterpretations of their thoughts made by the analyst following indirect elicitation abel et al 1998 we suspect the low response rate resulted in part from our choice to communicate remotely via email which often leads to poorer response rates than face to face communication kuhnert et al 2010 nevalainen et al 2018 the complexity of the enhanced influence diagrams may have also made the task of assessing them and providing missing information seem daunting and time consuming a general lack of commitment to the project may have also been at play if the stakeholders felt burdened by the collaboration process or uncertain about its real world value and impact bracken et al 2015 the long gap in time between the elicitations sessions and the delivery of the enhanced models to the stakeholders may have also been to blame for their disengagement in the final stages of the rea to rectify this issue we suggest discussing expectations for the elicitation processes thoroughly with prospective participants before beginning the study we also recommend clearly explaining the importance of the stakeholders role in the study its implications for the natural resource they and pm practitioners are mutually invested in and adhering to a pre determined schedule for interactions with the stakeholders fig 1 this way they will clearly understand what the study will require of them before committing and be fully aware of the real world impacts of their involvement additionally we propose a second face to face meeting between the stakeholder and facilitator after indirect elicitation to assist with the process of correcting and completing the enhanced diagrams to reduce confusion and frustration with the task fig 1 5 3 2 coping with complexity uncertainty messiness another important consideration for future use of the rea is coping with the messiness and uncertainty inherent in describing the cause and effect structure of complex socio ecological systems the influence diagrams messiness which troubled some stakeholders could be reduced by creating them digitally during direct elicitation however this tactic may reduce the ease and accessibility that drawing by hand provides additionally it is logical that the complexity of a well informed expert stakeholder s mental model would reflect the complexity of the socio ecological system in question as such when the goal of elicitation is to preserve the details of stakeholders complex mental models as it is in the rea some degree of messiness is unavoidable the discomfort with messiness however may have been less to do with the actual organization of the figures and more to do with complexity itself the complexity of the salmon system adn the effects climate change has on it seemed to be the source of the uncertainty which caused nearly all the stakeholders to express concern about the validity of their ideas when stakeholders question their ability to provide accurate information they should be encouraged to speak freely and contribute their ideas to the best of their ability despite their uncertainty since expert knowledge is likely the best source of information when access to more traditional forms of data is unavailable their opinions are valuable kuhnert et al 2010 sutherland 2006 during his elicitation session one stakeholder suggested that asking participants only about issues directly related to their fields of expertise may go a step beyond encouragement to help them feel more competent and confident in expressing their ideas firstly he suggested that since climate change is highly complex and predicting its effects on the abiotic environment is generally not directly within the knowledge domain of salmon experts it may help to provide a specific climate change scenario and subsequent abiotic effects at the outset of the elicitation process this would allow stakeholders to focus on what they know well how the abiotic environment affects fish the biological community salmon are embedded within and the downstream impacts on the socio economic system taking this idea a step further he suggested the creation of an aggregated model of the salmon climate change system developed by combining the knowledge of a variety of experts each specializing in one portion of the system although this may be the best possible way to produce the most thorough depiction of the system as burgman 2005 writes modeling is a balancing act between keeping experts within their domain of knowledge and putting aside sufficient time for the elicitation process in short such an effort may require more time and effort than is feasible for practical reasons like budget restrictions or the timeline of the political decision making process 5 3 3 representing mental models as influence diagrams in addition to the issues mentioned above several small improvements should be made to the rea s direct elicitation phase fig 1 based on our experience firstly the mild confusion expressed by some stakeholders about how to draw influence diagrams could be corrected by spending more time teaching them to express their thoughts in this format perhaps the facilitator could coach stakeholders to develop this skill through practice with a simpler and more straightforward subject facilitators should consider this tactic when stakeholders directly express confusion or ask the facilitator to draw their map without explaining why they prefer this arrangement the high percentage of stakeholders who preferred to allow the facilitator to draw could be linked with discomfort in visualizing mental models as influence diagrams stakeholders must develop enough competence with this process to interject if they feel their views have been misrepresented since there was some confusion surrounding the concept of effects strengths and how to assign them we suggest describing this more thoroughly during the stakeholder learning portion of the elicitation sessions fig 1 additionally many stakeholders did not assign effect strengths to their models or did not complete the task presumably since the demanding modeling process exhausted them burgman 2005 this issue could be corrected by incorporating more breaks into the elicitations sessions breaking it into multiple sessions or recording effects strengths throughout the diagramming process rather than leaving the task until the end assigning effects strengths at the end of the elicitation sessions may be preferable however because stakeholders may fatigue before completing their model s structure which is likely more detrimental than missing effect strengths 5 3 4 coping with biases beliefs heuristics values other considerations for future users of the rea are the biases beliefs heuristics and values bbhvs of both participants and modelers which are inherent in every pm process and shape their outcomes glynn et al 2017 hämäläinen 2015 left unchecked bbhvs can diminish the rigor and credibility of the scientific process glynn et al 2017 therefore pm practitioners must recognize the influence bbhvs exert on their studies and their origins which are not only the study s participants but also themselves glynn et al 2017 hämäläinen 2015 to these ends pm practitioners should introspectively evaluate how their own preferences values and motivations may affect a project from its inception to the publication of its results in addition to how the participants do further a pm practitioner s bbhvs may affect the participants behavior slotte and hämäläinen 2015 therefore all conscience pm practitioners must acknowledge the problems bbhvs pose and strive to reduce their influence where appropriate hämäläinen 2015 to aid in this pursuit all pm practitioners should familiarize themselves with the bbhv literature when planning a pm study seminal works regarding bbhvs include kahneman 2011 kahneman and tversky 2012 1979 tversky and kahneman 1981 1974 glynn 2014 2017 glynn et al 2017 hämäläinen 2015 voinov et al 2016 describe bbhvs in the context of natural resource related pm while a robust understanding of this field will assist pm practitioners in anticipating and therefore mitigating the effects of bbhvs their consideration should not end with the preparation phase of the rea but should continue throughout the project fig 1 while conducting elicitations using the rea approach we addressed the potential effects of bbhvs during all its steps from preparation to verification standardization fig 1 during preparation we carefully considered the wording of the three problem framing questions see section 3 2 and the questionnaire and later discussed and revised them per the suggestions made by our test participant we did this to reduce framing biases tversky and kahneman 1981 which may have caused us to unintentionally lead stakeholders to respond in a manner supporting our own beliefs about the effects of climate change on salmon we were also careful not to include leading information in our communications with the stakeholders before the study discussing only the necessary logistics information our justification for contacting them and minimally stating the topic we planned to discuss nevertheless our study s focus on the effects of climate change may have itself introduced a framing bias implying that climate change does indeed impact these fish to control for this potential bias however we included questions 6 11 in the questionnaire see appendix which were intended to gauge whether the stakeholder believed climate change will affect salmon by how much and during what time frame in general the stakeholders tended to believe that it would affect these fish although some indicated that climate change was not the most pressing threat facing baltic salmon as described in section 3 2 we conducted direct elicitations sessions one on one with each stakeholder to avoid the influences of social norms heeren alexander et al 2016 and group dynamics glynn et al 2017 these issues may have otherwise altered the stakeholders depictions of their mental models via for example groupthink janis 1982 mccauley 1989 or lead to issues like the overrepresentation of the most influential stakeholder s ideas burgman 2005 martin et al 2012 although conducting elicitations individually may limit problem solving potential or diminish the benefits a group setting offers like strengthening relationships between stakeholders we believe our strategy was justifiable given our aims our problem framing study and hence the rea were intended to build as holistic a view of a complex system as possible as such we were uncomfortable with the limits group interactions may have imposed nevertheless the rea as with all pm methodologies should be adaptable depending on the circumstances and aim of individual pm processes voinov et al 2016 therefore we acknowledge that the rea could be conducted in a group setting if desired following the same procedure we have described here during direct elicitation the facilitator s bbhvs may also affect stakeholders influence diagrams when drawing a facilitator may depict a skewed interpretation of the stakeholder s statements which instead reflect their own bbhvs to reduce this effect in our study the facilitator ensured the stakeholder could see what she was drawing consistently asked for confirmation of her interpretation s accuracy and encouraged interjection at the first sign of departure from the stakeholder s ideas see section 3 2 2 however if a stakeholder is uncertain or uncomfortable with the process of drawing causal diagrams their ability and confidence to interject may be jeopardized for this reason the facilitator was as thorough as possible when explaining causal diagrams how to read them and how to produce them during the stakeholder learning portion of direct elicitation for future implementations of the rea we would add that if a stakeholder seems unsure about how to proceed or asks the facilitator to draw the facilitator should ask the stakeholder to briefly draw a sample causal diagram and explain its contents before moving on additionally when confirming the accuracy of the diagram with a stakeholder the facilitator should verbally describe what each portion of the diagram depicts indicating her position within the diagram as she describes it the facilitator s behavior or speech may also subtly influence the visualization of a participant s mental model to reduce this influence in our study the facilitator made an effort to remain encouraging of the participant s progress drawing or describing their mental model while remaining neutral about its contents instead she made comments intended to request clarification further explanation confirm the model s accuracy before conducting our elicitations we were keenly aware of the problem inherent in indirect elicitation without the stakeholder s immediate guidance a pm practitioner may inject their own bbhvs as they attempt to reproduce the stakeholder s mental model from textual information see introduction for this reason we conducted direct elicitation first it was imperative to ensure in real time that each stakeholder s influence diagram was drawn according to their internal mental model during indirect elicitation careful coding and note taking before enhancing the diagrams served to limit the analyst s interpretation and provide a trail of justification for each change made lastly we incorporated the verification stage during which that stakeholder must approve of the changes made by the pm practitioner during this phase the stakeholder also has the chance to make changes to the model or revoke permission for its use providing one further check on the pm practitioner s interpretation however we discussed the difficulties we encountered during this stage in section 5 3 1 despite these safeguards we suspect some of our actions during direct elicitation may have unintentionally biased the stakeholders influence diagrams although we initially thought the core biological model for herring see fig a1 was general enough not to trigger anchoring bias oppenheimer et al 2008 tversky and kahneman 1974 when used as an example during the stakeholder learning fig 1 we cannot rule out the possibility that it did additionally based on the stakeholders disinterest in it we believe it was unnecessary as a tool for idea generation explaining the core biological model also took valuable time which could have been better spent practicing the process of building influence diagrams or on the elicitation process itself due to its lack of utility and its potential to induce anchoring bias we recommend only exposing participants to example diagrams entirely unrelated to the subject at hand during the stakeholder learning we are also concerned that the stakeholders knowledge about the facilitator s professional background may have influenced the study stakeholders often asked about the facilitator s career background and educational history while building rapport at the beginning of their elicitations sessions later some seemed to tailor their descriptions of their mental models accordingly after learning of the facilitator s experience with fisheries science stakeholders tended to leave out the causal relationships within their mental model they assumed were common knowledge between them for example the stages of the salmon life cycle to adapt to this situation in real time during our study the facilitator frequently requested that the stakeholder be explicit about the causal relationships within their mental model explain as if to a non expert and clarify portions of their model where they appeared to assume something about the facilitator s prior knowledge we recommend that other pm facilitators follow this practice as well further the facilitator should not discuss their background with stakeholders although any interested stakeholder could undoubtedly find this information online before meeting the facilitator despite the potential for a pm practitioner s bbhvs to affect the visualization of stakeholders mental models the rea coupled with our advice for its implementation and perhaps facilitation training voinov et al 2016 can address many of these issues further we reiterate that by coupling direct and indirect elicitation the rea reduces the potential introduction of an analyst s bbhvs while maintaining the thoroughness indirect elicitation provides additionally questionnaires implemented after both direct and indirect elicitation fig 1 should include questions to gauge the participants understanding of and satisfaction with these processes this information should be used to improve future facilitation of the rea and to develop the methodology over time as such the rea provides a cohesive methodology which acknowledges and reduces the effects of bbhvs we believe this is a step forward for mental model elicitation in pm nevertheless pm practitioners cannot control for their influence completely and therefore in addition to learning from past facilitations and endeavoring to reduce their influence in each subsequent facilitation pm practitioners should thoughtfully and honestly report how bbhvs may have influenced their study 5 3 5 formalizing core principles process in addition to the methodological recommendations we have provided here we also recommend that all pm processes including the rea be governed by a set of thoughtfully developed core principles agreed upon by all the actors involved voinov et al 2016 although by necessity these principles will differ between pm processes owing to the vastly different circumstances under which they are conducted their aim should be to establish norms or rules of engagement for each pm process and to ensure it is conducted in an effective manner and that results contribute meaningfully to knowledge co production and decision making voinov et al 2016 these principles should be based on ensuring transparency accountability and follow through between pm practitioners participants and the end users of the pm process s products additionally they should act as a code of conduct between these groups ensuring fairness civility and fostering trust between them while reducing the influence of bbhvs glynn et al 2017 voinov et al 2016 an example of such core principles is available in voinov et al 2016 to ensure the pm process adheres to these principles a process for appropriately documenting and reporting the pm process is essential although no standard process has yet been identified gray et al 2018 s 4p framework the records of engagement suggested by glynn et al 2018 or both could help fulfill this need although we discussed the general aims of our study with the stakeholders how their contributions would be used and made a commitment to inform them about how the study results were ultimately employed we believe a more formalized process for discussing and agreeing on the study process and its core principles would have been warranted fig 1 principles process discussion a more thorough discussion about what to expect from the pm process about the aims of the study and more contact with the stakeholders throughout may have improved their commitment to it potentially encouraging more engagement during the model verification stage of the rea in the future discussing the rea process itself with stakeholders may prepare them better for elicitation and emphasize the importance of their continued involvement in the study after direct elicitation additionally agreement on the core principles of the project may have given the stakeholders a stronger voice in shaping the study and its outcomes into more relevant and mutually beneficial real world advice for salmon management in the face of climate change a greater degree of connection transparency and shared control over the project may improve the fulfillment of the instrumental and normative pm functions as well it is also worth noting that pm is typically a sub process within a larger scientific and decision making efforts to govern socio ecological systems while we have discussed the importance of defining core principles and processes in the context of pm specifically these ideas also apply to the overarching processes they are embedded within see glynn et al 2018 and glynn et al 2017 for discussion of these ideas in the broader scientific and decision making context 5 4 analyzing rich mental models problem framing although the purpose of this article is to describe the rea not discuss the results of our problem framing study we find it pertinent to exemplify how one can use rich depictions of mental models as stated previously the purpose of collecting rich influence diagrams was to frame the problem of the impacts of climate change on baltic salmon this study s results are intended to guide the incorporation of climate change effects into ices existing baltic salmon stock assessment model by indicating areas of special concern that the model should take into account and prepare for future management challenges we suggest the number of times the stakeholders included a particular variable category of variables life stage or habitat in their causal diagrams can serve as a proxy for their importance within the salmon climate change system therefore more frequently identified variables should be prioritized for inclusion in the model we recognize there are disadvantages to prioritization based on frequency alone as this technique is not well suited for capturing important causal pathways or themes for example we may be able to detect that river temperature age at smoltification and the number of salmon occur frequently in stakeholders influence diagrams however by considering their frequency alone we may miss the concept they represent together a stakeholder may have used these variables to indicate that changes in river temperature alter the time it takes for young salmon to leave the river altering the age at maturity resulting in changes in population size stakeholders may describe the same process in vastly different terms and different levels of specificity further complicating the task of identifying themes therefore we suggest it is imperative to couple this analysis strategy with analysis of elicitation notes a and b fig 1 which describe the stakeholder s thoughts predictions and common themes alternatively visualization techniques like fuzzy cognitive mapping fcm olazabal et al 2018 özesmi and özesmi 2004 solana gutiérrez et al 2017 may help to more smoothly aggregate stakeholders depictions of their mental models making identification of frequently described causal pathways themes easier fcms can also be deconstructed to explore causal relationships pertaining to particular variables of interest olazabal et al 2018 creating fmcs also requires mental model elicitation and therefore the rea approach could be applied to this methodology as well additionally we recognize the number of times a variable is mentioned may also be indicative of the limits of stakeholders knowledge with well understood variables and causal pathways occurring more frequently than potentially poorly understood but highly instrumental ones nevertheless the inclusion of frequently cited variables in ices stock assessment model should reflect the best available knowledge providing a starting point for further work and promoting the model s legitimacy in the eyes of the stakeholders and hopefully in their peers if desired after the decisions about which aspects of the salmon climate change system to include in the stock assessment model have been made a second pm process similar to task 1 described in haapasaari et al 2013 can begin as in their study we propose that stakeholders build directed acyclic graphs dag explaining how the variables identified during the elicitation sessions connect with the current stock assessment model and with climate change unlike the qualitative influence diagrams we produced for problem framing the strengths of the effects between variables in dags should be quantified expressed as joint probability distributions jensen and nielsen 2007 whether a causal link is positive or negative should also be noted in their study mäntyniemi et al 2013 2 2 further analysis of this study is available in haapasaari et al 2013 instructed stakeholders to choose variables to include in their dags from a previously collected list provided by herring experts in our case these variables would come from the influence diagrams developed during the elicitation sessions described here once completed the dags can be pooled via bayesian model averaging mäntyniemi et al 2013 and subsequently incorporated into the stock assessment model fully quantified influence diagrams also enable value of information voi analysis mäntyniemi et al 2009 which measures the maximum amount a decision maker should be willing to pay to obtain more precise information before making a decision as such voi can help determine what data to collect to assist decision making in addition to the utility they provide in incorporating new information into environmental models rich depictions of mental models provide context and justification for doing so their development also serves as a brainstorming tool identifying the goals stakeholders have for a particular natural resource strategies to reach those goals areas where knowledge is lacking and providing new testable hypotheses about how a system works beyond the results of the study themselves the process of conducting mental model elicitation promotes stakeholder learning and causal thinking fortuin et al 2011 kinchin et al 2000 marcot et al 2001 uusitalo 2007 and begins to build stakeholder support for management recommendations fiorino 1990 jones et al 2009 5 5 future directions we suggest future users of the rea implement the suggestions for methodological improvement discussed in section 5 3 and that targeted questionnaires administered following direct elicitation and the completion of the full rea process could develop the methodology further fig 1 final questionnaire additionally we encourage further advancements to ensure mental model elicitation fulfills all functions of pm effectively beyond the practical advice and suggestions we provide here we also believe pm practitioners should strive to reduce the loss of valuable stakeholder knowledge even further the most pressing barrier preventing the acquisition of holistic stakeholder knowledge in pm is limited participation since typical studies include 30 or fewer participants voinov and bousquet 2010 eliciting the beliefs of such a small segment of the stakeholder population constitutes its own form of knowledge simplification and potentially reduces pm s substantive normative instrumental and educational value these low numbers may be the result of limited time financial and analytical resources or arise from the difficulty in securing stakeholder participation nevalainen et al 2018 these obstacles may be circumvented by making the pm process easier more enjoyable or by connecting it more clearly with real world impact improving communication styles and channels between stakeholders and researchers may also help traditionally stakeholder participation has been increased by conducting group pm activities like group problem framings but eliciting stakeholder knowledge in groups presents drawbacks as well see sections 3 2 and 5 3 4 still group participation may only marginally increase the number of stakeholders involved in pm processes less traditionally text analysis techniques like topic modelling blei 2012 from the computational social sciences literature could provide analytical tools for extracting important insights from text interview or elicitation session transcripts questionnaires tweets blogs etc provided by a number of stakeholders orders of magnitude greater than more traditional means allow however employing these techniques could mean less face to face time between researchers and stakeholders limited verifiability of their ideas and less opportunity for learning additionally the time consuming steps like coding and note taking involved in traditional pm studies force the analyst to engage deeply with the data improving their depth of knowledge and ability to draw conclusions from the data as such alternative means for incorporating more stakeholders into the pm process should be developed 6 conclusions in conclusion the rich elicitation approach rea presented here is a strategy for eliciting rich mental models from expert stakeholders the approach s novelty comes from the deliberate combination of direct and indirect elicitation strategies to ensure stakeholders mental models are represented as holistically as possible while preserving the integrity of their knowledge and the learning process inherent in direct elicitation we believe this approach can and should be adapted and applied to any pm process involving the elicitation of mental models though the rich mental models the rea produces are time consuming to create and complex making their analysis more challenging we believe they are necessary for forming a strong knowledge base on which to begin any pm project in the face of data limitation and incomplete scientific knowledge greater detail allows researchers to take advantage of the full depth of stakeholder knowledge reducing the propensity to overlook potentially instrumental causal pathways promoting the production of new testable hypotheses improving realism and generating more thorough solutions and management strategies developing a comprehensive knowledgebase may also improve the resilience of pm projects by providing a reservoir of new ideas when researchers find themselves going back to the drawing board despite the challenges the best available expert knowledge should be used to its fullest potential to solve natural resource related challenges we also encourage researchers and pm practitioners to transparently and thoughtfully share their experiences to allow for the development of best practices and to bring pm into the light as a legitimate and valuable field of its own acknowledgments we would like to warmly thank the 11 stakeholders who participated in this study and tapani pakarinen who provided helpful feedback in preparation for the elicitations sessions thank you also to atso romakkaniemi and the ices assessment working group on baltic salmon and trout wgbast for their support author contributions k l s m and p h designed the research k l performed the research k l analyzed the data s m j v and p h supervised the research and analysis and k l s m j v and p h wrote the article appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 multimedia component 2 multimedia component 2 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2019 104589 funding this study received funding from the university of helsinki s doctoral program in interdisciplinary environmental sciences denvi and the finnish national agency for education 
