index,text
2020,this study proposes an approach to detect and remove systematic outliers in rainfall measurement of ground gauges in the watershed of a reservoir this method utilizes a conceptual rainfall runoff model with the downstream observed reservoir inflow to supervise the detection and cleanse of upstream rain measurements the analysis is based on a nonlinear optimization with the objective designed as only removing outliers which leads to relevant improvement in runoff simulation to satisfy required precision the minimum acceptable precision in hydrological simulation is specified based on the objective of reservoir flood control operation this method can be utilized to purify historical rain data for hydrological studies as well as to estimate the current floodwater detained in the watershed for more robust flood management in real time a realistic case study along with synthetically designed experiments are demonstrated to verify the effectiveness of the proposed methods keywords rainfall runoff outlier detection reservoir flood control operation optimization data availability data will be made available on request 1 introduction real time rainfall measurement is essential for reservoir flood control operation it drives the hydrological analysis for estimating current storage in the watershed and predicting future runoff with the forecasted rainfall in some cases rainfall data are further used to evaluate the performance of previous forecasts the biases can then be feed backed into the error correction process to generate potentially more correct forecasted rainfall hung et al 2009 xue et al 2013 jabbari and bae 2020 any type of rain measurement is associated with some degree of error for example measurement errors from tipping bucket rain gauges tbrgs are often reported due to inadequate exposure of a gauge shielded by surrounding obstructions wind deformation funnel clogging power failure telemetry interruption and the mechanical errors of losing water during tipping of the bucket under high rainfall intensity habib et al 2001 lanza and stagi 2008 yeung et al 2014 segovia cardozo et al 2021 most of the above factors lead to underestimation of rain magnitude on the other hand weather radar which estimates rainfall by raindrop size distribution is also subject to uncertainties including noise or inadequate calibration of the radar system ground clutter and occultation of the beam propagation and the variance in the reflectivity and rain rate relationship berne and krajewski 2013 dai et al 2018 it is commonly believed that ground gauges deliver more reliable magnitude at fixed locations while radar provides better information about spatial distribution hence numerous studies have tried to merge both measurements to generate more trustworthy rainfall fields habib et al 2004 yeung et al 2014 ochoa rodriguez et al 2019 shao et al 2021 benoit 2021 these studies all assumed that the gauge measurements are accurate enough to be assimilated with the radar estimation this assumption is not necessarily correct especially for gauges located in mountainous areas and affected simultaneously by all of the aforementioned factors during heavy rainfall as a representative example which motivates this study typhoon morakot in 2009 brought record breaking precipitation and caused significant underestimation of rainfall measurements for every primary reservoir in southern taiwan it led to unreasonable runoff coefficients ranging from 1 1 to 1 5 for the reservoirs this issue is frequently encountered and continuously disturbs flood management practices in situ in the decision making process of reservoir flood control operation underestimation of occurred rainfall leads to estimated current inflows and water surface levels wsls that are lower than actual values this situation is stressful for decision makers and usually leads to more conservative operation subsequently i e increasing reservoir outflow and potentially impairing the objectives of attenuating and storing floods in a modern flood warning system telemetry data are usually automatically transferred into the database detailed manual reviews or modifications are rare due to the restricted time constraint in real time decision making cross validation analysis by spatial statistics or unsupervised machine learning approaches can be applied to improve the quality of measurements zimek and filzmoser 2018 this procedure infers the rainfall value of the under evaluating gauge by information from other neighboring gauges the target gauge is regarded as an outlier if the difference between its measured and inferred rainfall exceeds a threshold such as the 43 mm hour used by yeung et al 2014 or three times of the standard deviation of estimated residuals by the ordinary kriging method used by the central weather burau of taiwan manysplendid engineering consultants 2016 nonetheless it is possible that the majority of the gauge network underestimates rainfall while the well functioned gauges are the minority during an extreme event according to our experiences in typhoon morakot as a result cross validation analysis may instead filter out data from the fewer but accurate gauges if those gauges happen to be statistical outliers or the tolerance of errors will be magnified i e only significant outliers will be detected and removed reservoir flood control operation relies on accurate rainfall to assess future inflow and releasing strategy to achieve goals of mitigating floods protecting dam safety and storing adequate water chou and wu 2015 measurement errors may hinder the attainment of operating goals thus the degeneration of the latter can be used to quantify the impact of the former the effect of outlier removal can also be interpreted as the improvement of objective achievements in addition the dam controls a mega scale of water compared to the rain gauge its inflow is derived by the continuity equation based on measured wsl and outflow of the reservoir the sensor to measure the wsl is usually installed in the chamber of intake work to minimize the impact of fluctuation by wind the sensor is set to monitor the wsl several times per minute the average of which is then calculated as the representative measurement by the minute abnormal data of wsl and inflow is filtered out by simple rules this smoothing and filtering mechanism served the operation well during past floods it is commonly believed that the reservoir inflow is much more reliable than the rainfall measurements thus this study proposes an inverse problem approach to detect outliers in rain gauge measurements it utilizes measured reservoir inflow to indirectly supervise the outlier removal of rainfall measurements throughout the watershed on the basis of an accurate rainfall runoff model this method identifies problematic rainfall records by regulating the deviation between simulated and observed reservoir inflows within acceptable precision in regards to attaining flood operation objectives this method can be utilized to purify historical rain data for hydrological studies as well as to estimate current watershed storage for more robust flood management in real time in the following sections the assumptions and procedures of the proposed method are presented first followed by a realistic case study and simulated numerical experiments which demonstrate the effectiveness of the approach 2 methodology 2 1 major assumptions and distinctions from previous studies the errors in rainfall runoff simulation are mixtures of uncertainties from model structure parameters and input data troutman 1985a 1985b kavetski et al 2006a 2006b moges et al 2021 the idea of removing potentially problematic input data in order to reduce unacceptable runoff output requires the following assumptions 1 the adopted rainfall runoff model is representative and consistent representativeness means the model produces runoff with acceptable precision without input errors when input errors are present a consistent model still provides more accurate runoff with higher quality rainfall data 2 there are adequate rain gauges in the reservoir watershed thus the errors in rain magnitude measurements have more impacts on runoff simulation than the imprecise estimation of spatial distribution of precipitation combining the first two assumptions allows this study to utilize the commonly adopted thiessen polygon method and conceptual tank model to estimate areal rainfall and simulate the hydrological process 3 the unit time period of reservoir flood control operation is one hour in our case study area the required hydrologic analysis is thus developed with the same unit time step while the rain intensity and runoff discharge within one hour are assumed to be steady on the basis of this assumption the proposed approach regards the time which potential malfunction occurs for each rain gauge as a continuous decision variable the measurement of a specific gauge after this time is discarded from the estimation of areal rainfall since multiple gauges might malfunction within the same unit period the hourly interval which contains the values of multiple decision variables are further split into corresponding sub intervals as shown in fig 1 each sub interval adopts the respective uncontaminated gauges with rainfall depths adjusted in proportion to their hourly record to estimate the areal rainfall magnitude for the sub period the hourly total rainfall is then calculated by summing the estimated values of all sub intervals the concept of the above procedure is depicted in fig 1 as well fig 2 shows the flowchart of the proposed outlier detection analysis this method is designed to cleanse systematic errors which contaminate rainfall estimation for extended time periods its major distinction from the conventional outlier detection practice is that the latter is centered on the abnormalities of problematic input data and its evaluation for each time period is independently performed zimek and filzmoser 2018 the proposed method further exploits the rainfall runoff relationship and the downstream observed outcome the potential shortcoming of the conventional approach on the current problem is that the majority of gauges may malfunction and underestimate rainfall in an extreme event thus the problematic records may form a large cluster and erroneous data is surrounded by numerous other outliers this hinders the effectiveness of the conventional approach which primarily identifies random independent and abnormal errors a potential situation where the conventional method can complement the proposed method is the clogging of telemetry signals of rain gauges this usually results in a short series of underestimated records followed by a compensated high rainfall accumulated from previous clogged signals thus as shown in fig 2 the k th nearest neighbor knn method is further integrated to pre purify the raw data the procedure of knn is listed as follows 1 the euclidean distance from an hourly measurement by a gauge to each of the other gauges is calculated as r t i j r t i r t j 2 in the formula r t i denotes the measurement of the i th gauge in the t th hour and r t i j represents the distance between the i th and j th gauges in the t th hour and i j 2 for the i th gauge the computed distances to the other gauges are ranked and the k th least distance r t i r a n k k is selected for further evaluation 3 if r t i r a n k k exceeds a predefined threshold rknn the measurement r t i is regarded as an outlier the case study in this paper includes ten rain gauges and sets k as 3 based on the rule of thumbs which commonly utilize the square of sample numbers as k the value of rknn is set as 40 mm referring to the value used by yeung et al 2014 the evaluation is independently performed for the records in each hour the identified problematic data are excluded from the estimation of areal rainfall in the following optimization analysis 2 2 rainfall runoff simulation and parameters calibration simulation of rainfall runoff process serves as a medium to conduct the inverse problem approach for the outlier removal of rainfall measurements based on this purpose the simpler yet well calibrated conceptual model is adequate further the optimization analysis of outlier removal requires iterative callings of the rainfall runoff simulation the nearly instant computational time of the conceptual model is suitable especially for the demonstration purpose which requires evaluating numerous scenarios and flood events thus an improved version from the modified tank model kadoya and tanakamaru 1995 is used the structure of the conceptual model is illustrated in fig 3 where zi and ai represent the height and discharge coefficient of the i th lateral orifice and bi is the discharge coefficient of the i th infiltration orifice the continuity equation of each tank is expressed as follows 1 s t t 1 s t t 1 1 r t a f t 1 q t 1 q t 2 2 s t t i s t t 1 i f t i 1 f t i q t i 1 i 2 3 3 s t t 4 s t t 1 4 f t 3 q t 5 where s t t i is the storage depth of the i th tank mm and i 1 4 f t i is the infiltration depth through the i th downward orifice and i 1 3 q t i is the runoff depth through the i th lateral orifice and i 1 5 the flow discharge through each orifice is determined as the following 1 the formula to calculate discharge through the top lateral orifice mimics manning s equation to simulate the characteristics of surface runoff and channel flow 4 q t 1 a 1 s t t 1 1 z 1 5 3 2 a parameter z4 representing the height of the second orifice from the top is designed to simulate the initial loss after rainfall the discharge through this orifice is determined by eq 5 5 q t 2 a 2 s t t 1 1 z 4 3 the downward flow from the top tank is proportional to the available storage space in the second tank to simulate the impact of soil moisture on infiltration 6 f t 1 b 1 z 2 z 3 s t t 1 2 4 the discharges through other orifices are determined as follows 7 f t i b i s t t 1 i i 2 3 8 q t 3 a 3 s t t 1 2 z 3 9 q t i a i s t t 1 i 1 i 4 5 according to eqs 1 to 9 the watershed runoff between the t 1 and t hours denoted as i t tk m3 s are computed by i 1 5 q t i a 3 6 where a is the watershed area km2 the model parameters include the lateral discharge coefficients a 1 a 2 a 3 a 4 and a 5 the infiltration coefficients b 1 b 2 and b 3 and the heights of lateral orifices z 1 z 2 z 3 and z 4 these parameters are calibrated by minimizing the average coefficients of efficiency ce from the simulated results of selected historical events the formula of ce is as follows 10 c e i 1 i i i tk i t i i i tk i i i i i t i i i i where cei represents the ce of simulated results of the i th event i i is a column vector the elements of which represent the observed reservoir inflow series for the i th event i tk i is the vector contains the simulated runoff series and i i is the average measured inflow of the i th event 2 3 the required precision of runoff simulation specified by reservoir flood control operation the proposed inverse problem approach cuts off potential input outliers to minimize the deviation of simulated runoff from the observed reservoir inflow nonetheless it may not only reduce input errors but also compensate for inherent model and parameter errors by unnecessarily removing normal measurements or simply termed as overfitting for example in order to match the observed peak inflow the algorithm may only preserve the gauge with the highest recorded rainfall intensity while eliminating all the others to prevent this situation the proposed method is designed to only adjust the rainfall input to regulate ce above an acceptable threshold rather than blindly elevating it the acceptable threshold is defined by the perspective of attaining goals of reservoir flood control operation for different scales of floods this section explains the logic behind this setting flood control operation analysis is extended from the authors previous research chou and wu 2015 the operation is divided into three stages each of which has specific operating objectives for flood preparation attenuation and conservation respectively the current study focuses on the second stage which starts once the inflow exceeds the non damage discharge of the downstream river channel and extends until the peak inflow occurs the stage wise objective is to attenuate the flood while keeping the wsl of the reservoir from exceeding the acceptable safety level of surcharge sl a major constraint is to avoid man made flooding damage to the downstream area which prescribes the reservoir release in this stage to abide the following rule 11 o t 1 min i t max o t δ i t max o max where i t max and δ i t max are the highest discharge and hourly increment of reservoir inflow observed before the t th hour o max is a controlled upper bound of reservoir outflow discharge to achieve maximum and feasible flood attenuation eq 11 restricts the reservoir release to be less than the inflow during the flood rising limb thus the excess inflow will be detained in the reservoir and elevate the wsl consequently an optimal strategy fully utilizes the available space below sl to detain flood inflow and maximize attenuation since sl is usually higher than or at least close to the desired end of operation target level attaining sl at the end of the second stage almost automatically ensures water conservation post peak in other words the highest wsl during the second stage denoted as hmax encapsulates the degree to which the operating objectives of a releasing strategy have succeeded a hmax higher than sl is not favored due to the concern of dam safety while the opposite means degeneration of flood mitigation and potential inadequacy of end of operation storage the omax in eq 11 is determined in an iterative manner with an initial value as the peak inflow discharge it is iteratively reduced until hmax attains sl fig 4 demonstrates this iterative concept details of operation rules for the other flood stages can be found in chou and wu 2013 2015 through flood control operation the observed reservoir inflow series i is converted into an optimal outflow series denoted as o i since h max is dependent on i and o i it is denoted as h max i o i and abbreviated as h max i in the following context similarly the operating strategy and results of the simulated inflow i tk can be expressed as o i tk and h max i tk this allows this study to quantify the impact of biases between i and i tk in terms of the deviation of h max i tk from h max i which is further denoted as d h max i tk h max i tk h max i an allowable range of d h max i tk based on field operating experiences can then be assigned to identify the required minimum acceptable ce for i tk this study sets the allowable d h max i tk as between 2 to 2 m in the case study the considerations behind this setting are explained as follows 1 an overestimation of i tk leads to overestimated o i tk which impairs the attenuation and conservation of flood and results in a h max i tk lower than h max i albeit being non optimal o i tk stills avoids man made flooding damage following eq 11 as for the goal of achieving final storage a difference of wsl as 2 m corresponds to storage volume as only 2 to 2 6 million m3 for the case study reservoir this deficiency can be easily recovered by reducing release more aggressively to enhance water storing in the flood recession limb 2 outliers among rainfall measurements usually induce underestimation of inflow and cause h max i tk to exceed h max i nonetheless the value of sl is usually set well below the designed flood level or the top of the dam with the margin around 4 to 6 m capable of accommodating d h max i tk while the margin is preserved for the uncertainty of future flood flow the forecast and optimal operation strategy is updated sequentially in a time rolling manner in real time with new measured and projected information available in each hour in this process a previous underestimation of inflow which results in a current wsl that is higher than expected will lead to a higher release in order to adjust the storage trajectory this procedure further alleviates the impact of variance from hydrologic simulation and justifies the setting of maximum acceptable d h max i tk as 2 m in order to establish the relationship between ce and d h max i tk the calibration of parameters for the rainfall runoff model is further integrated with simulation of reservoir flood control operation the coupled computation starts from a random initial set of parameters for the tank model during each iteration of parameter modification while ce is reduced progressively the simulation of reservoir flood control is also performed with the updated i tk the value of the corresponding d h max i tk can then be determined after the paired samples are generated for each historical event the minimum acceptable ce can then be identified according to the maximum allowable d h max i tk fig 5 illustrates this calculation process as shown in eq 10 ce is normalized by the variance of i as the denominator in the second element of the right hand side of the equation the numerator represents the magnitude of hydrological simulating biases and has more direct impacts on d h max i tk to facilitate this feature the historical events are classified into large medium and small categories the generated samples within each of which are scanned to establish the minimum acceptable ce for each respective flood scale the identified criterion is termed as ce thld in the following ce thld represents an acceptable standard and the benefit of further boost of ce over this threshold by adjusting the input rainfall is set to degenerate in the following optimization formulation another effort to address overfitting is that removing rainfall records is only favored when it leads to significant improvement of ce this relevant improvement termed as δ c e is defined by the reduction of absolute value of d h max i tk which is set as 0 5 m in the case study an irrelevant improvement of δ c e and d h max i tk would not be worth removing rainfall measurements for each candidate point in the samples generated as in fig 5 a subset with reduction of d h max i tk higher than 0 5 m can be identified the alternative point with least ce improvement among the subset is identified to determine the value of δ c e for the selected candidate after connecting each candidate in the samples with another relevant improved point the relationship between ce and δ c e can be established representative δ c e can then be suggested details of the results of this analysis are described in section 3 1 2 4 optimization formulation of outlier removal of rainfall measurements two optimization formulations indexed as i and ii are designed for comparison purposes each regarding the potential time of malfunction for all rain gauges as the decision variables the set of all decision variables is termed as t the i th element of which is denoted as t i t the measurements of the i th gauge after the time ti are discarded from the estimation of areal rainfall ti should be greater than zero and less than the duration of flood event δ formulation i is the base scenario and simply aims to minimize the deviation between i and i tk the objective function is 12 minimize i i tk t t i i tk t where i tk t is the column vector containing simulated runoff i tk is a function of the decision variables t which drives the estimation of areal rainfall as the input to the rainfall runoff model formulation ii is the theme of this paper and is based on the concept of only removing necessary rainfall measurements to achieve relevant improvement of precision in runoff simulation before optimization it requires the following preprocessing 1 the gauges are clustered into groups according to their locations 2 the knn distance of each gauge in each hour is calculated according to the process stated in section 2 1 it is then summed from every hour to solve for the cumulative knn distance i e the value for the i th gauge is r sum knn i t 1 n r t i r a n k k 3 the gauge with least value of r sum knn i in each group is regarded as a standard the gauges with accumulative rainfall less than 95 of the standard s in the specific group are deemed as anomalous candidates to be inspected the others are regarded as normal gauges the formulation ii is expressed as 13 m i n i m i z e coth c e i tk t c e i tk d δ c e m i a δ t i subject to 14 c e i tk t c e thld 15 c e i tk t c e i tk d 0 where a is the set of all anomalous gauges t is the set of decision variables within which ti is the element for the i th gauge δ is the duration of event and d corresponding to t is the set with all elements as δ which represents the base scenario that all gauges are functioning i tk d and i tk t are the simulated runoff with inputting areal rainfall determined by d and t respectively c e i tk d and c e i tk t are the coefficients of efficiency determined based on i with i tk d and i tk t respectively c e thld and δ c e are the minimum acceptable ce and its relevant improvement as explained in section 2 3 coth x is the hyperbolic cotangent function with form as e 2 x 1 e 2 x 1 the function converges to 1 0 and while x approaches and 0 respectively the constraints are integrated into eq 13 as penalty functions unconstrained nonlinear optimization algorithm is then employed to solve the coupled objective function as the following equation 16 m i n i m i z e coth c e i tk t c e i tk d δ c e m i a δ t i w v 1 v 2 where v 1 and v 2 are the violation amount of eqs 14 and 15 and w is the weighting factor for penalties and set as 104 in eqs 13 and 16 the input to the hyperbolic cotangent function is the ratio of c e i tk t c e i tk d to δ c e to m th power the order of the power m is set to 10 to magnify the ratio when it is larger than 1 this leads to rapid convergence of the function output to 1 and allows the optimization to manage rainfall measurements for all gauges without restraint on the other hand the power would conduct rapid convergence of the function input to 0 when the ratio is less than 1 this leads to a large output value from the coth function thus creating a barrier to prevent the optimization removing rainfall measurements of standard or normal gauges even violating eqs 14 and 15 for examples assume δ c e equals 0 02 if 30 h from i a δ t i could contribute to the c e i tk t c e i tk d as 0 016 the ratio value is 0 8 and the output from the coth function would be 9 35 this gives a value of 280 by multiplying the function output with i a δ t i based on c e i tk t c e i tk d the value of v 1 is expected to be reduced by 0 016 this leads to change of the value of the penalty function w v 1 as 160 which could not compensate for the increase of the first element in eq 16 the optimization would favor leaving rain records of gauges not in the set a unchanged even c e i tk d is not feasible for eq 14 a summary of formulation ii is listed as below 1 the default run of the algorithm initially sets t d if this already satisfies eq 14 the optimal value of objective function would be zero and no measurements should be removed 2 if the initial solution is infeasible t is changed to reduce the violation of constraints 1 the algorithm can manage the measurements of gauges in a unrestrainedly free from the impact of the barrier by the coth function 2 for gauges not in a the algorithm seeks improving feasibility of the solution with minimum hours of measurement removal as long as it leads to relevant improvement of runoff simulation precision the bobyqa algorithm by powell 2009 is utilized to solve the optimization problem it approximates the objective function by a quadratic function and updates the decision variables in a trust region manner the algorithm explicitly considers simple bounds of decision variables is free from evaluating derivatives of objective function and is efficient for large scale problems unconstrained nonlinear programming methods only converge to a local minimum and are sensitive to initial solutions the multi start approach is employed in both formulations the optimization is performed with different random initial solutions for one hundred trials the best from which in regard to objective function value is then selected as the final solution to formulation i for formulation ii the best ten solutions from the one hundred trials are identified first the solution with the least hours of rainfall measurement removal among the identified ten candidates is then selected as the final solution 3 results and discussion 3 1 case study area mudan reservoir and the background data the mudan reservoir in southern taiwan and its watershed are selected for this case study the reservoir is managed by the southern water resources office swro of the water resources agency wra in taiwan it provides public water supply to pingtung county with a daily supplied amount as 80 000 m3 the reservoir is located at the convergence of two upstream creeks which are the northwestern rujuan and the southeastern mudan creeks the watershed area of the reservoir is 69 2 km2 within which are five rain gauges managed by wra and swro these are the gauges as rujuan dam wra the 2 nd pinshi bridge shihmen and mudan wra the central weather bureau cwb of taiwan also installed another two gauges in the watershed which are the mudan cwb and chishan a project to develop a smart reservoir system initiated in 2017 chunghwa telecom co ltd 2020 established three additional gauges which are the chialotong bridge dam sr and sangyuan treatment plant fig 6 maps the locations of the ten telemetry rain gauges in relation to the watershed the program developed by chou and cheng 2000 is utilized to enumerate and generate a total number of 1 023 210 1 sets of thessian polygon weighting factors for areal rainfall estimation under different combinations of gauges the capacity of mudan reservoir under its normal full level as 142 el m is 26 51 million m3 according to the survey in 2020 the peak of the designed probable maximum flood pmf is 3 815 m3 s the designed flood level is 142 13 el m with storage volume as 26 7 million m3 and it is obtained by operating against pmf from initial level as 132 el m the upper limit of reservoir operating rule curves varies from 132 to 139 el m during the flood season from may to october historical operations kept the highest wsl of the reservoir during floods below 138 el m with volume as 21 16 million m3 which is set as the value of sl in the case study thirty five historical flood events are collected with the measurements from different rain gauges and observed reservoir inflow table 1 summarizes the data of these events the operator originally adopts three gauges which are rujuan dam wra and the 2nd pinshi bridge respectively to estimate and archive areal rainfall of the watershed this archived rainfall data is listed in the second column in table 1 and leads to runoff coefficients ranging from 0 41 to 2 21 the average runoff coefficients from the thirty five events is 0 90 the first step of the case study analysis is to include all available rain gauges during events and the runoff coefficients are updated ranging from 0 47 to 1 32 with an average value as 0 76 events with runoff coefficients less than 1 0 in table 1 are selected for calibrating the parameters of the tank model simulation results by the calibrated parameters in terms of ce are listed in the columns 11 to 14 for all events under different areal rainfall scenarios it shows that based on the original archived rainfall the ce ranges from 4 59 to 0 94 with an average value as only 0 5 of all thirty five events the range of ce can be improved to 0 37 to 0 97 with the average value as 0 81 if measurements from all rain gauges are utilized the procedure as shown in fig 5 is employed to identify the minimum required ce for the following optimization analysis the case study assumes that the reservoir is operated from the initial wsl as 132 el m with sl as 138 el m and desired final target as 136 el m simulation of reservoir flood control operation is performed for each event with all intermediate parameters during the calibration process the generated paired samples of ce and d h max i tk are depicted in fig 7 the samples are further divided into three groups representing small medium and large flood events the scales of total inflow volume are below 15 between 15 and 55 and above 55 million m3 and the minimum required ce ce thld is identified as 0 87 0 93 and 0 96 for each respective group d h max i tk is identified for each sample in the figure to induct δ c e the final adopted δ c e is dependent on the value of c e i tk d the value of δ c e is set as 0 16 0 08 0 03 and 0 02 for c e i tk d less than 0 6 between 0 6 and 0 7 between 0 7 and 0 9 and above 0 9 respectively according to fig 6 the gauges are separated into three clusters the first includes the gauges along the mudan creek which are mudan wra chishan sangyuan treatment plant and the 2nd pinshi bridge the second contains the gauges around the dam which are shihmen and mudan cwb the two gauges on the top of the dam the third includes rujuan and the chialotong bridge the purpose of this setting is explained in section 2 4 3 2 correcting areal rainfall data of historical events the outlier detection and removal analysis is applied to the historical events listed in table 1 the two formulations mentioned in section 2 4 are applied respectively the results of formulation i including the modified areal rainfall updated runoff coefficient and ce are listed in the 4 9 and 13 columns in table 1 columns 5 10 and 14 of the table show the respective results of formulation ii the average runoff coefficient from all events are reduced from 0 76 of the virgin records to 0 69 and 0 70 for formulations i and ii with the largest values reduced to 0 94 and 0 96 for the two formulations respectively the average ce are elevated from 0 81 as the average of c e i tk d to 0 91 and 0 90 for the two formulations with respective ranges of ce narrowing down to 0 64 0 99 and 0 80 0 97 typhoon morakot is used to demonstrate the difference between the two formulations fig 8 depicts the isohyet with six all available rain gauges during the typhoon fig 9 shows the simulated hydrograph under four areal rainfall scenarios which are the original archived data the estimation using all six gauges and the results from the two formulations the runoff coefficients for these scenarios are 1 35 0 99 0 91 and 0 93 with ce as 0 55 0 93 0 99 and 0 96 respectively figs 10 and 11 are the inspected potential time of malfunction of gauges and the associated isohyet by formulations i and ii for this event in formulation i only the data from mudan wra is entirely preserved many of the other measurements are eliminated with total hours of removal as 233 h on the contrary formulation ii only removes the records for 139 h and only the data of three gauges are deemed as outliers post the flood peak the spatial pattern of total rainfall from formulation ii indicates an increasing trend from the southwestern to the northeastern part of the watershed which is also the direction of increase of terrain elevation the orographic effect accompanied with the counter clock direction of typhoon circulation usually leads to higher rainfall intensity in the upstream part of the watershed the ce of simulated runoff series by formulation ii is 0 96 slightly less than the counterpart 0 99 from formulation i nonetheless the simulation of reservoir flood control operation against c e i tk t of the second formulation delivers a hmax as 138 4 el m the highest wsl is very close to the pre defined sl 138 0 el m which approves the effectiveness of the proposed formulation ii of conducting minimal number of rainfall measurement removal yet still yielding relevant improvement in runoff simulation fig 12 3 3 synthetical numerical experiments while the results from the realistic case study are promising they are not yet conclusive due to the lack of ground truth of rainfall data for verification numerical experiments are synthetically designed to address this issue the experimental settings are described below 1 the designed rainfall as ground truth typhoon morakot in 2009 aug 23 torrential rain in 2018 and typhoon megi in 2016 are selected to represent large medium and small events respectively for experiments the cleansed measurements from the previous section under formulation ii are used to establish the true rain field the rainfall at the gauge without historical records or detected as outliers is replaced by the estimation from the other normal gauges using the nearest neighbor principle the true areal rainfall is then calculated using the hybrid data from all gauges using the thiessian s approach r is used to denote the vector with elements as the series of true hourly areal rainfall the accumulative rain depth of the event is denoted as r 2 model errors the results from section 3 2 produces 2 611 residual samples as the differences between hourly observed and simulated inflow discharges under formulation ii for the thirty five historical events the residual at the t th hour is calculated by subtracting i t tk from i t and denoted as ϛ t an auto regression model for ϛ t is then developed as 17 ϛ t 1 11 ϛ t 1 0 38 ϛ t 2 0 004 ϛ t 3 0 48 i t 0 79 i t 1 0 37 i t 2 0 04 i t 3 2 37 ε where ε is a white noise with hypothesis test suggesting it obeys normal distribution with mean and standard deviation as 0 0 and 14 4 denoted as n 0 0 14 4 the average and standard deviation from the 2 611 samples of residuals is 0 and 14 1 eq 17 thus facilitates the introduction of random model errors into the runoff series of experimental flood events the generated model errors are denoted as ϛ which is a vector with dimension as the flood duration δ 3 the true runoff series the tank model is used to produce the simulated runoff series i tk with r for a specific event combing i tk with the randomly generated ϛ yields the designed true runoff 4 the intentionally corrupted rain data several runs of outlier removal analysis are performed for a case of specific realization of model errors within a run the measurement errors of rainfall are introduced by the following procedure 1 the number of gauge malfunctions is randomly determined from a discrete range of five to ten of which each value is set to be equally likely to occur 2 the time of malfunction for a specific gauge is randomly determined by the uniform distribution the lower and upper bounds on random variables are set to symmetrically center around the time of flood peak with a radius of 20 h 3 the underestimation ratio is set to be uniformly distributed between 0 0 and 0 5 the generated ratio is multiplied with the true rain data past the generated malfunctioning time 4 the generated rainfall data with errors for each gauge is then weighted to produce r a vector containing the series of contaminated areal rainfall 5 experimental cases 1 for each of the three selected events nine different sets of statistical parameters of model errors are tested the nine sets are combinations generated by assigning the normally distributed ε with mean as 0 15 and 30 and standard deviation as 14 4 25 and 30 respectively this procedure produces twenty seven experimental cases by combining flood events and statistical parameters of model errors 2 two hundred random runs are routed for each experimental case each run is performed with randomly generated i and r formulations i and ii are applied to each run each produces the estimation of corrected areal rainfall denoted as r with the same number of elements as r and r 6 the indices of performance evaluation for each case 1 the percentage of total areal rain deviation is calculated as d r r r r 100 for each run taking the average of d r from the two hundreds run yields d r 2 the ce by the designed true r and the estimated r by optimization is calculated for each run its average from two hundreds runs is denoted as c e r r 3 the hitting rate for a run denoted as hb represents the detected malfunction times of a number of 10 hb gauges fall within a window with radius as b hours symmetrically centering the true value in other words an absolute deviation of ti from the designed true value within b hours is regarded as a hit two values of b as 6 and 12 h are evaluated h 6 and h 12 can then be averaged from the hitting rate of all runs tables 2 3 and 4 show the experimental results for the three events each of which are trialed for nine sets of parameters for model errors they reveal the following characteristics 1 the case representing the actual model errors with ε n 0 0 14 4 serves the purpose of demonstrating the proposed method both formulations deliver good estimation of temporal pattern and total magnitude of areal rainfall the d r is under 5 and c e r r exceeds 0 95 for both formulations nonetheless formulation i performs poorly in regards to hitting rate since it tends to overly fit the observed inflow by arbitrarily modifying the rain data to compensate for model errors formulation ii outperforms its counterpart in hitting rates shows more resistance to overfitting only removes relevant outliers and still delivers simulated runoff with acceptable precision 2 the tables show that the proposed method performs better in large and medium events the small events have relatively large proportions of loss in rainfall and base flow in the observed reservoir inflow this may offset the effectiveness of the proposed inverse problem based approach however significant difficulties in determining reasonable rainfall are primarily encountered during large or extreme events also the flood detention capacity of the reservoir in the case study is adequate to accommodate the total inflow volume of small events even the proposed method could not remove the outliers of rainfall measurements effectively the operating consequence under the underestimated rainfall will still be to release insignificant volume of water this is indifferent to the situations with exact rainfall information thus small events are insignificant in regards to operating reservoirs to attain goals even if errors in rainfall measurement are present 3 the experiments from different cases show that under an unbiased model with means of ε as 0 the increase in variance of model errors has an insignificant impact on outlier detection analysis 4 the last columns of tables 2 and 3 suggest that formulation ii yields hitting rates above 60 for medium or large events providing that the hydrologic model can produce ce above 0 8 with true rainfall r 4 conclusions and suggestions an inverse problem approach is proposed to improve the rainfall measurements of ground gauges based on the rainfall runoff modeling and observed reservoir inflow downstream the time of potential malfunction of rain gauges are regarded as decision variables and an optimization analysis is utilized to determine the optimal combinations of measurements from different gauges to regulate the the deviation between simulated and observed reservoir inflows within acceptable precision the perspectives of achieving the goals of reservoir flood control operation are used to define the acceptable precision of hydrologic simulation the objective function of the optimization analysis is designed to conduct precise and relevant outlier removal and prevent overfitting a realistic case study along with synthetically designed experiments verify the effectiveness of the proposed methods for significant flood events the proposed method can be integrated into the calibration of parameters of an event based conceptual rainfall runoff model in a typical practice measurements of several flood events are joined as training samples to improve the representativeness of calibrated parameters the calibration process requires iterative updating of parameters according to the results from simulations of all events the proposed method can be implemented to cleanse rainfall data during the simulation of each event a nested optimization framework will be created by inserting the proposed method in the inner loop to simulate each event with parameters updated by the outer loop while being more computationally intensive this approach prevents overfitting of model parameters by improving the qualities of training samples and allowing more historical events to be included in the calibration the utilization of a conceptual rainfall runoff model and the availability of telemetry measurements of reservoir inflow and upstream rainfall allows the proposed method to be implemented in real time the real time implementation of the outlier analysis assists in filtering out the underestimated rainfall input to the hydrological model thus the floodwater detained in the watershed can be simulated more accurately additional information can supplement reservoir inflow in the analysis such as the measurements from upstream river stage stations or reservoirs a distributed hydrological model with the locations of upstream stations as the outlets of sub watersheds can be developed to include the river stages or upstream reservoir inflow in the detection of outliers of rainfall measurements the purified data of ground gauges also provide a more reliable foundation for further implementation such as assimilation with observed data from weather radar to more precisely monitor the spatial pattern of rainfall author contribution the study was initiated by fnfc who also got the funds cww and fnfc both contribute to the methodological development of simulation and optimization models cww carried out the contents of all analysis in the case study the manuscript was jointly drafted by cww and fnfc declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is originated from a project funded by the water resources agency no moeawra1090120 taiwan r o c and carried out by the chunghwa telecom co ltd in cooperation of the authors and the manysplendid engineering consultants co ltd the project review committee from the southern water resources office of water resources agency have provided valuable comments and supports especially the chief of the zengwen reservoir management center mr po chung chen 
2020,this study proposes an approach to detect and remove systematic outliers in rainfall measurement of ground gauges in the watershed of a reservoir this method utilizes a conceptual rainfall runoff model with the downstream observed reservoir inflow to supervise the detection and cleanse of upstream rain measurements the analysis is based on a nonlinear optimization with the objective designed as only removing outliers which leads to relevant improvement in runoff simulation to satisfy required precision the minimum acceptable precision in hydrological simulation is specified based on the objective of reservoir flood control operation this method can be utilized to purify historical rain data for hydrological studies as well as to estimate the current floodwater detained in the watershed for more robust flood management in real time a realistic case study along with synthetically designed experiments are demonstrated to verify the effectiveness of the proposed methods keywords rainfall runoff outlier detection reservoir flood control operation optimization data availability data will be made available on request 1 introduction real time rainfall measurement is essential for reservoir flood control operation it drives the hydrological analysis for estimating current storage in the watershed and predicting future runoff with the forecasted rainfall in some cases rainfall data are further used to evaluate the performance of previous forecasts the biases can then be feed backed into the error correction process to generate potentially more correct forecasted rainfall hung et al 2009 xue et al 2013 jabbari and bae 2020 any type of rain measurement is associated with some degree of error for example measurement errors from tipping bucket rain gauges tbrgs are often reported due to inadequate exposure of a gauge shielded by surrounding obstructions wind deformation funnel clogging power failure telemetry interruption and the mechanical errors of losing water during tipping of the bucket under high rainfall intensity habib et al 2001 lanza and stagi 2008 yeung et al 2014 segovia cardozo et al 2021 most of the above factors lead to underestimation of rain magnitude on the other hand weather radar which estimates rainfall by raindrop size distribution is also subject to uncertainties including noise or inadequate calibration of the radar system ground clutter and occultation of the beam propagation and the variance in the reflectivity and rain rate relationship berne and krajewski 2013 dai et al 2018 it is commonly believed that ground gauges deliver more reliable magnitude at fixed locations while radar provides better information about spatial distribution hence numerous studies have tried to merge both measurements to generate more trustworthy rainfall fields habib et al 2004 yeung et al 2014 ochoa rodriguez et al 2019 shao et al 2021 benoit 2021 these studies all assumed that the gauge measurements are accurate enough to be assimilated with the radar estimation this assumption is not necessarily correct especially for gauges located in mountainous areas and affected simultaneously by all of the aforementioned factors during heavy rainfall as a representative example which motivates this study typhoon morakot in 2009 brought record breaking precipitation and caused significant underestimation of rainfall measurements for every primary reservoir in southern taiwan it led to unreasonable runoff coefficients ranging from 1 1 to 1 5 for the reservoirs this issue is frequently encountered and continuously disturbs flood management practices in situ in the decision making process of reservoir flood control operation underestimation of occurred rainfall leads to estimated current inflows and water surface levels wsls that are lower than actual values this situation is stressful for decision makers and usually leads to more conservative operation subsequently i e increasing reservoir outflow and potentially impairing the objectives of attenuating and storing floods in a modern flood warning system telemetry data are usually automatically transferred into the database detailed manual reviews or modifications are rare due to the restricted time constraint in real time decision making cross validation analysis by spatial statistics or unsupervised machine learning approaches can be applied to improve the quality of measurements zimek and filzmoser 2018 this procedure infers the rainfall value of the under evaluating gauge by information from other neighboring gauges the target gauge is regarded as an outlier if the difference between its measured and inferred rainfall exceeds a threshold such as the 43 mm hour used by yeung et al 2014 or three times of the standard deviation of estimated residuals by the ordinary kriging method used by the central weather burau of taiwan manysplendid engineering consultants 2016 nonetheless it is possible that the majority of the gauge network underestimates rainfall while the well functioned gauges are the minority during an extreme event according to our experiences in typhoon morakot as a result cross validation analysis may instead filter out data from the fewer but accurate gauges if those gauges happen to be statistical outliers or the tolerance of errors will be magnified i e only significant outliers will be detected and removed reservoir flood control operation relies on accurate rainfall to assess future inflow and releasing strategy to achieve goals of mitigating floods protecting dam safety and storing adequate water chou and wu 2015 measurement errors may hinder the attainment of operating goals thus the degeneration of the latter can be used to quantify the impact of the former the effect of outlier removal can also be interpreted as the improvement of objective achievements in addition the dam controls a mega scale of water compared to the rain gauge its inflow is derived by the continuity equation based on measured wsl and outflow of the reservoir the sensor to measure the wsl is usually installed in the chamber of intake work to minimize the impact of fluctuation by wind the sensor is set to monitor the wsl several times per minute the average of which is then calculated as the representative measurement by the minute abnormal data of wsl and inflow is filtered out by simple rules this smoothing and filtering mechanism served the operation well during past floods it is commonly believed that the reservoir inflow is much more reliable than the rainfall measurements thus this study proposes an inverse problem approach to detect outliers in rain gauge measurements it utilizes measured reservoir inflow to indirectly supervise the outlier removal of rainfall measurements throughout the watershed on the basis of an accurate rainfall runoff model this method identifies problematic rainfall records by regulating the deviation between simulated and observed reservoir inflows within acceptable precision in regards to attaining flood operation objectives this method can be utilized to purify historical rain data for hydrological studies as well as to estimate current watershed storage for more robust flood management in real time in the following sections the assumptions and procedures of the proposed method are presented first followed by a realistic case study and simulated numerical experiments which demonstrate the effectiveness of the approach 2 methodology 2 1 major assumptions and distinctions from previous studies the errors in rainfall runoff simulation are mixtures of uncertainties from model structure parameters and input data troutman 1985a 1985b kavetski et al 2006a 2006b moges et al 2021 the idea of removing potentially problematic input data in order to reduce unacceptable runoff output requires the following assumptions 1 the adopted rainfall runoff model is representative and consistent representativeness means the model produces runoff with acceptable precision without input errors when input errors are present a consistent model still provides more accurate runoff with higher quality rainfall data 2 there are adequate rain gauges in the reservoir watershed thus the errors in rain magnitude measurements have more impacts on runoff simulation than the imprecise estimation of spatial distribution of precipitation combining the first two assumptions allows this study to utilize the commonly adopted thiessen polygon method and conceptual tank model to estimate areal rainfall and simulate the hydrological process 3 the unit time period of reservoir flood control operation is one hour in our case study area the required hydrologic analysis is thus developed with the same unit time step while the rain intensity and runoff discharge within one hour are assumed to be steady on the basis of this assumption the proposed approach regards the time which potential malfunction occurs for each rain gauge as a continuous decision variable the measurement of a specific gauge after this time is discarded from the estimation of areal rainfall since multiple gauges might malfunction within the same unit period the hourly interval which contains the values of multiple decision variables are further split into corresponding sub intervals as shown in fig 1 each sub interval adopts the respective uncontaminated gauges with rainfall depths adjusted in proportion to their hourly record to estimate the areal rainfall magnitude for the sub period the hourly total rainfall is then calculated by summing the estimated values of all sub intervals the concept of the above procedure is depicted in fig 1 as well fig 2 shows the flowchart of the proposed outlier detection analysis this method is designed to cleanse systematic errors which contaminate rainfall estimation for extended time periods its major distinction from the conventional outlier detection practice is that the latter is centered on the abnormalities of problematic input data and its evaluation for each time period is independently performed zimek and filzmoser 2018 the proposed method further exploits the rainfall runoff relationship and the downstream observed outcome the potential shortcoming of the conventional approach on the current problem is that the majority of gauges may malfunction and underestimate rainfall in an extreme event thus the problematic records may form a large cluster and erroneous data is surrounded by numerous other outliers this hinders the effectiveness of the conventional approach which primarily identifies random independent and abnormal errors a potential situation where the conventional method can complement the proposed method is the clogging of telemetry signals of rain gauges this usually results in a short series of underestimated records followed by a compensated high rainfall accumulated from previous clogged signals thus as shown in fig 2 the k th nearest neighbor knn method is further integrated to pre purify the raw data the procedure of knn is listed as follows 1 the euclidean distance from an hourly measurement by a gauge to each of the other gauges is calculated as r t i j r t i r t j 2 in the formula r t i denotes the measurement of the i th gauge in the t th hour and r t i j represents the distance between the i th and j th gauges in the t th hour and i j 2 for the i th gauge the computed distances to the other gauges are ranked and the k th least distance r t i r a n k k is selected for further evaluation 3 if r t i r a n k k exceeds a predefined threshold rknn the measurement r t i is regarded as an outlier the case study in this paper includes ten rain gauges and sets k as 3 based on the rule of thumbs which commonly utilize the square of sample numbers as k the value of rknn is set as 40 mm referring to the value used by yeung et al 2014 the evaluation is independently performed for the records in each hour the identified problematic data are excluded from the estimation of areal rainfall in the following optimization analysis 2 2 rainfall runoff simulation and parameters calibration simulation of rainfall runoff process serves as a medium to conduct the inverse problem approach for the outlier removal of rainfall measurements based on this purpose the simpler yet well calibrated conceptual model is adequate further the optimization analysis of outlier removal requires iterative callings of the rainfall runoff simulation the nearly instant computational time of the conceptual model is suitable especially for the demonstration purpose which requires evaluating numerous scenarios and flood events thus an improved version from the modified tank model kadoya and tanakamaru 1995 is used the structure of the conceptual model is illustrated in fig 3 where zi and ai represent the height and discharge coefficient of the i th lateral orifice and bi is the discharge coefficient of the i th infiltration orifice the continuity equation of each tank is expressed as follows 1 s t t 1 s t t 1 1 r t a f t 1 q t 1 q t 2 2 s t t i s t t 1 i f t i 1 f t i q t i 1 i 2 3 3 s t t 4 s t t 1 4 f t 3 q t 5 where s t t i is the storage depth of the i th tank mm and i 1 4 f t i is the infiltration depth through the i th downward orifice and i 1 3 q t i is the runoff depth through the i th lateral orifice and i 1 5 the flow discharge through each orifice is determined as the following 1 the formula to calculate discharge through the top lateral orifice mimics manning s equation to simulate the characteristics of surface runoff and channel flow 4 q t 1 a 1 s t t 1 1 z 1 5 3 2 a parameter z4 representing the height of the second orifice from the top is designed to simulate the initial loss after rainfall the discharge through this orifice is determined by eq 5 5 q t 2 a 2 s t t 1 1 z 4 3 the downward flow from the top tank is proportional to the available storage space in the second tank to simulate the impact of soil moisture on infiltration 6 f t 1 b 1 z 2 z 3 s t t 1 2 4 the discharges through other orifices are determined as follows 7 f t i b i s t t 1 i i 2 3 8 q t 3 a 3 s t t 1 2 z 3 9 q t i a i s t t 1 i 1 i 4 5 according to eqs 1 to 9 the watershed runoff between the t 1 and t hours denoted as i t tk m3 s are computed by i 1 5 q t i a 3 6 where a is the watershed area km2 the model parameters include the lateral discharge coefficients a 1 a 2 a 3 a 4 and a 5 the infiltration coefficients b 1 b 2 and b 3 and the heights of lateral orifices z 1 z 2 z 3 and z 4 these parameters are calibrated by minimizing the average coefficients of efficiency ce from the simulated results of selected historical events the formula of ce is as follows 10 c e i 1 i i i tk i t i i i tk i i i i i t i i i i where cei represents the ce of simulated results of the i th event i i is a column vector the elements of which represent the observed reservoir inflow series for the i th event i tk i is the vector contains the simulated runoff series and i i is the average measured inflow of the i th event 2 3 the required precision of runoff simulation specified by reservoir flood control operation the proposed inverse problem approach cuts off potential input outliers to minimize the deviation of simulated runoff from the observed reservoir inflow nonetheless it may not only reduce input errors but also compensate for inherent model and parameter errors by unnecessarily removing normal measurements or simply termed as overfitting for example in order to match the observed peak inflow the algorithm may only preserve the gauge with the highest recorded rainfall intensity while eliminating all the others to prevent this situation the proposed method is designed to only adjust the rainfall input to regulate ce above an acceptable threshold rather than blindly elevating it the acceptable threshold is defined by the perspective of attaining goals of reservoir flood control operation for different scales of floods this section explains the logic behind this setting flood control operation analysis is extended from the authors previous research chou and wu 2015 the operation is divided into three stages each of which has specific operating objectives for flood preparation attenuation and conservation respectively the current study focuses on the second stage which starts once the inflow exceeds the non damage discharge of the downstream river channel and extends until the peak inflow occurs the stage wise objective is to attenuate the flood while keeping the wsl of the reservoir from exceeding the acceptable safety level of surcharge sl a major constraint is to avoid man made flooding damage to the downstream area which prescribes the reservoir release in this stage to abide the following rule 11 o t 1 min i t max o t δ i t max o max where i t max and δ i t max are the highest discharge and hourly increment of reservoir inflow observed before the t th hour o max is a controlled upper bound of reservoir outflow discharge to achieve maximum and feasible flood attenuation eq 11 restricts the reservoir release to be less than the inflow during the flood rising limb thus the excess inflow will be detained in the reservoir and elevate the wsl consequently an optimal strategy fully utilizes the available space below sl to detain flood inflow and maximize attenuation since sl is usually higher than or at least close to the desired end of operation target level attaining sl at the end of the second stage almost automatically ensures water conservation post peak in other words the highest wsl during the second stage denoted as hmax encapsulates the degree to which the operating objectives of a releasing strategy have succeeded a hmax higher than sl is not favored due to the concern of dam safety while the opposite means degeneration of flood mitigation and potential inadequacy of end of operation storage the omax in eq 11 is determined in an iterative manner with an initial value as the peak inflow discharge it is iteratively reduced until hmax attains sl fig 4 demonstrates this iterative concept details of operation rules for the other flood stages can be found in chou and wu 2013 2015 through flood control operation the observed reservoir inflow series i is converted into an optimal outflow series denoted as o i since h max is dependent on i and o i it is denoted as h max i o i and abbreviated as h max i in the following context similarly the operating strategy and results of the simulated inflow i tk can be expressed as o i tk and h max i tk this allows this study to quantify the impact of biases between i and i tk in terms of the deviation of h max i tk from h max i which is further denoted as d h max i tk h max i tk h max i an allowable range of d h max i tk based on field operating experiences can then be assigned to identify the required minimum acceptable ce for i tk this study sets the allowable d h max i tk as between 2 to 2 m in the case study the considerations behind this setting are explained as follows 1 an overestimation of i tk leads to overestimated o i tk which impairs the attenuation and conservation of flood and results in a h max i tk lower than h max i albeit being non optimal o i tk stills avoids man made flooding damage following eq 11 as for the goal of achieving final storage a difference of wsl as 2 m corresponds to storage volume as only 2 to 2 6 million m3 for the case study reservoir this deficiency can be easily recovered by reducing release more aggressively to enhance water storing in the flood recession limb 2 outliers among rainfall measurements usually induce underestimation of inflow and cause h max i tk to exceed h max i nonetheless the value of sl is usually set well below the designed flood level or the top of the dam with the margin around 4 to 6 m capable of accommodating d h max i tk while the margin is preserved for the uncertainty of future flood flow the forecast and optimal operation strategy is updated sequentially in a time rolling manner in real time with new measured and projected information available in each hour in this process a previous underestimation of inflow which results in a current wsl that is higher than expected will lead to a higher release in order to adjust the storage trajectory this procedure further alleviates the impact of variance from hydrologic simulation and justifies the setting of maximum acceptable d h max i tk as 2 m in order to establish the relationship between ce and d h max i tk the calibration of parameters for the rainfall runoff model is further integrated with simulation of reservoir flood control operation the coupled computation starts from a random initial set of parameters for the tank model during each iteration of parameter modification while ce is reduced progressively the simulation of reservoir flood control is also performed with the updated i tk the value of the corresponding d h max i tk can then be determined after the paired samples are generated for each historical event the minimum acceptable ce can then be identified according to the maximum allowable d h max i tk fig 5 illustrates this calculation process as shown in eq 10 ce is normalized by the variance of i as the denominator in the second element of the right hand side of the equation the numerator represents the magnitude of hydrological simulating biases and has more direct impacts on d h max i tk to facilitate this feature the historical events are classified into large medium and small categories the generated samples within each of which are scanned to establish the minimum acceptable ce for each respective flood scale the identified criterion is termed as ce thld in the following ce thld represents an acceptable standard and the benefit of further boost of ce over this threshold by adjusting the input rainfall is set to degenerate in the following optimization formulation another effort to address overfitting is that removing rainfall records is only favored when it leads to significant improvement of ce this relevant improvement termed as δ c e is defined by the reduction of absolute value of d h max i tk which is set as 0 5 m in the case study an irrelevant improvement of δ c e and d h max i tk would not be worth removing rainfall measurements for each candidate point in the samples generated as in fig 5 a subset with reduction of d h max i tk higher than 0 5 m can be identified the alternative point with least ce improvement among the subset is identified to determine the value of δ c e for the selected candidate after connecting each candidate in the samples with another relevant improved point the relationship between ce and δ c e can be established representative δ c e can then be suggested details of the results of this analysis are described in section 3 1 2 4 optimization formulation of outlier removal of rainfall measurements two optimization formulations indexed as i and ii are designed for comparison purposes each regarding the potential time of malfunction for all rain gauges as the decision variables the set of all decision variables is termed as t the i th element of which is denoted as t i t the measurements of the i th gauge after the time ti are discarded from the estimation of areal rainfall ti should be greater than zero and less than the duration of flood event δ formulation i is the base scenario and simply aims to minimize the deviation between i and i tk the objective function is 12 minimize i i tk t t i i tk t where i tk t is the column vector containing simulated runoff i tk is a function of the decision variables t which drives the estimation of areal rainfall as the input to the rainfall runoff model formulation ii is the theme of this paper and is based on the concept of only removing necessary rainfall measurements to achieve relevant improvement of precision in runoff simulation before optimization it requires the following preprocessing 1 the gauges are clustered into groups according to their locations 2 the knn distance of each gauge in each hour is calculated according to the process stated in section 2 1 it is then summed from every hour to solve for the cumulative knn distance i e the value for the i th gauge is r sum knn i t 1 n r t i r a n k k 3 the gauge with least value of r sum knn i in each group is regarded as a standard the gauges with accumulative rainfall less than 95 of the standard s in the specific group are deemed as anomalous candidates to be inspected the others are regarded as normal gauges the formulation ii is expressed as 13 m i n i m i z e coth c e i tk t c e i tk d δ c e m i a δ t i subject to 14 c e i tk t c e thld 15 c e i tk t c e i tk d 0 where a is the set of all anomalous gauges t is the set of decision variables within which ti is the element for the i th gauge δ is the duration of event and d corresponding to t is the set with all elements as δ which represents the base scenario that all gauges are functioning i tk d and i tk t are the simulated runoff with inputting areal rainfall determined by d and t respectively c e i tk d and c e i tk t are the coefficients of efficiency determined based on i with i tk d and i tk t respectively c e thld and δ c e are the minimum acceptable ce and its relevant improvement as explained in section 2 3 coth x is the hyperbolic cotangent function with form as e 2 x 1 e 2 x 1 the function converges to 1 0 and while x approaches and 0 respectively the constraints are integrated into eq 13 as penalty functions unconstrained nonlinear optimization algorithm is then employed to solve the coupled objective function as the following equation 16 m i n i m i z e coth c e i tk t c e i tk d δ c e m i a δ t i w v 1 v 2 where v 1 and v 2 are the violation amount of eqs 14 and 15 and w is the weighting factor for penalties and set as 104 in eqs 13 and 16 the input to the hyperbolic cotangent function is the ratio of c e i tk t c e i tk d to δ c e to m th power the order of the power m is set to 10 to magnify the ratio when it is larger than 1 this leads to rapid convergence of the function output to 1 and allows the optimization to manage rainfall measurements for all gauges without restraint on the other hand the power would conduct rapid convergence of the function input to 0 when the ratio is less than 1 this leads to a large output value from the coth function thus creating a barrier to prevent the optimization removing rainfall measurements of standard or normal gauges even violating eqs 14 and 15 for examples assume δ c e equals 0 02 if 30 h from i a δ t i could contribute to the c e i tk t c e i tk d as 0 016 the ratio value is 0 8 and the output from the coth function would be 9 35 this gives a value of 280 by multiplying the function output with i a δ t i based on c e i tk t c e i tk d the value of v 1 is expected to be reduced by 0 016 this leads to change of the value of the penalty function w v 1 as 160 which could not compensate for the increase of the first element in eq 16 the optimization would favor leaving rain records of gauges not in the set a unchanged even c e i tk d is not feasible for eq 14 a summary of formulation ii is listed as below 1 the default run of the algorithm initially sets t d if this already satisfies eq 14 the optimal value of objective function would be zero and no measurements should be removed 2 if the initial solution is infeasible t is changed to reduce the violation of constraints 1 the algorithm can manage the measurements of gauges in a unrestrainedly free from the impact of the barrier by the coth function 2 for gauges not in a the algorithm seeks improving feasibility of the solution with minimum hours of measurement removal as long as it leads to relevant improvement of runoff simulation precision the bobyqa algorithm by powell 2009 is utilized to solve the optimization problem it approximates the objective function by a quadratic function and updates the decision variables in a trust region manner the algorithm explicitly considers simple bounds of decision variables is free from evaluating derivatives of objective function and is efficient for large scale problems unconstrained nonlinear programming methods only converge to a local minimum and are sensitive to initial solutions the multi start approach is employed in both formulations the optimization is performed with different random initial solutions for one hundred trials the best from which in regard to objective function value is then selected as the final solution to formulation i for formulation ii the best ten solutions from the one hundred trials are identified first the solution with the least hours of rainfall measurement removal among the identified ten candidates is then selected as the final solution 3 results and discussion 3 1 case study area mudan reservoir and the background data the mudan reservoir in southern taiwan and its watershed are selected for this case study the reservoir is managed by the southern water resources office swro of the water resources agency wra in taiwan it provides public water supply to pingtung county with a daily supplied amount as 80 000 m3 the reservoir is located at the convergence of two upstream creeks which are the northwestern rujuan and the southeastern mudan creeks the watershed area of the reservoir is 69 2 km2 within which are five rain gauges managed by wra and swro these are the gauges as rujuan dam wra the 2 nd pinshi bridge shihmen and mudan wra the central weather bureau cwb of taiwan also installed another two gauges in the watershed which are the mudan cwb and chishan a project to develop a smart reservoir system initiated in 2017 chunghwa telecom co ltd 2020 established three additional gauges which are the chialotong bridge dam sr and sangyuan treatment plant fig 6 maps the locations of the ten telemetry rain gauges in relation to the watershed the program developed by chou and cheng 2000 is utilized to enumerate and generate a total number of 1 023 210 1 sets of thessian polygon weighting factors for areal rainfall estimation under different combinations of gauges the capacity of mudan reservoir under its normal full level as 142 el m is 26 51 million m3 according to the survey in 2020 the peak of the designed probable maximum flood pmf is 3 815 m3 s the designed flood level is 142 13 el m with storage volume as 26 7 million m3 and it is obtained by operating against pmf from initial level as 132 el m the upper limit of reservoir operating rule curves varies from 132 to 139 el m during the flood season from may to october historical operations kept the highest wsl of the reservoir during floods below 138 el m with volume as 21 16 million m3 which is set as the value of sl in the case study thirty five historical flood events are collected with the measurements from different rain gauges and observed reservoir inflow table 1 summarizes the data of these events the operator originally adopts three gauges which are rujuan dam wra and the 2nd pinshi bridge respectively to estimate and archive areal rainfall of the watershed this archived rainfall data is listed in the second column in table 1 and leads to runoff coefficients ranging from 0 41 to 2 21 the average runoff coefficients from the thirty five events is 0 90 the first step of the case study analysis is to include all available rain gauges during events and the runoff coefficients are updated ranging from 0 47 to 1 32 with an average value as 0 76 events with runoff coefficients less than 1 0 in table 1 are selected for calibrating the parameters of the tank model simulation results by the calibrated parameters in terms of ce are listed in the columns 11 to 14 for all events under different areal rainfall scenarios it shows that based on the original archived rainfall the ce ranges from 4 59 to 0 94 with an average value as only 0 5 of all thirty five events the range of ce can be improved to 0 37 to 0 97 with the average value as 0 81 if measurements from all rain gauges are utilized the procedure as shown in fig 5 is employed to identify the minimum required ce for the following optimization analysis the case study assumes that the reservoir is operated from the initial wsl as 132 el m with sl as 138 el m and desired final target as 136 el m simulation of reservoir flood control operation is performed for each event with all intermediate parameters during the calibration process the generated paired samples of ce and d h max i tk are depicted in fig 7 the samples are further divided into three groups representing small medium and large flood events the scales of total inflow volume are below 15 between 15 and 55 and above 55 million m3 and the minimum required ce ce thld is identified as 0 87 0 93 and 0 96 for each respective group d h max i tk is identified for each sample in the figure to induct δ c e the final adopted δ c e is dependent on the value of c e i tk d the value of δ c e is set as 0 16 0 08 0 03 and 0 02 for c e i tk d less than 0 6 between 0 6 and 0 7 between 0 7 and 0 9 and above 0 9 respectively according to fig 6 the gauges are separated into three clusters the first includes the gauges along the mudan creek which are mudan wra chishan sangyuan treatment plant and the 2nd pinshi bridge the second contains the gauges around the dam which are shihmen and mudan cwb the two gauges on the top of the dam the third includes rujuan and the chialotong bridge the purpose of this setting is explained in section 2 4 3 2 correcting areal rainfall data of historical events the outlier detection and removal analysis is applied to the historical events listed in table 1 the two formulations mentioned in section 2 4 are applied respectively the results of formulation i including the modified areal rainfall updated runoff coefficient and ce are listed in the 4 9 and 13 columns in table 1 columns 5 10 and 14 of the table show the respective results of formulation ii the average runoff coefficient from all events are reduced from 0 76 of the virgin records to 0 69 and 0 70 for formulations i and ii with the largest values reduced to 0 94 and 0 96 for the two formulations respectively the average ce are elevated from 0 81 as the average of c e i tk d to 0 91 and 0 90 for the two formulations with respective ranges of ce narrowing down to 0 64 0 99 and 0 80 0 97 typhoon morakot is used to demonstrate the difference between the two formulations fig 8 depicts the isohyet with six all available rain gauges during the typhoon fig 9 shows the simulated hydrograph under four areal rainfall scenarios which are the original archived data the estimation using all six gauges and the results from the two formulations the runoff coefficients for these scenarios are 1 35 0 99 0 91 and 0 93 with ce as 0 55 0 93 0 99 and 0 96 respectively figs 10 and 11 are the inspected potential time of malfunction of gauges and the associated isohyet by formulations i and ii for this event in formulation i only the data from mudan wra is entirely preserved many of the other measurements are eliminated with total hours of removal as 233 h on the contrary formulation ii only removes the records for 139 h and only the data of three gauges are deemed as outliers post the flood peak the spatial pattern of total rainfall from formulation ii indicates an increasing trend from the southwestern to the northeastern part of the watershed which is also the direction of increase of terrain elevation the orographic effect accompanied with the counter clock direction of typhoon circulation usually leads to higher rainfall intensity in the upstream part of the watershed the ce of simulated runoff series by formulation ii is 0 96 slightly less than the counterpart 0 99 from formulation i nonetheless the simulation of reservoir flood control operation against c e i tk t of the second formulation delivers a hmax as 138 4 el m the highest wsl is very close to the pre defined sl 138 0 el m which approves the effectiveness of the proposed formulation ii of conducting minimal number of rainfall measurement removal yet still yielding relevant improvement in runoff simulation fig 12 3 3 synthetical numerical experiments while the results from the realistic case study are promising they are not yet conclusive due to the lack of ground truth of rainfall data for verification numerical experiments are synthetically designed to address this issue the experimental settings are described below 1 the designed rainfall as ground truth typhoon morakot in 2009 aug 23 torrential rain in 2018 and typhoon megi in 2016 are selected to represent large medium and small events respectively for experiments the cleansed measurements from the previous section under formulation ii are used to establish the true rain field the rainfall at the gauge without historical records or detected as outliers is replaced by the estimation from the other normal gauges using the nearest neighbor principle the true areal rainfall is then calculated using the hybrid data from all gauges using the thiessian s approach r is used to denote the vector with elements as the series of true hourly areal rainfall the accumulative rain depth of the event is denoted as r 2 model errors the results from section 3 2 produces 2 611 residual samples as the differences between hourly observed and simulated inflow discharges under formulation ii for the thirty five historical events the residual at the t th hour is calculated by subtracting i t tk from i t and denoted as ϛ t an auto regression model for ϛ t is then developed as 17 ϛ t 1 11 ϛ t 1 0 38 ϛ t 2 0 004 ϛ t 3 0 48 i t 0 79 i t 1 0 37 i t 2 0 04 i t 3 2 37 ε where ε is a white noise with hypothesis test suggesting it obeys normal distribution with mean and standard deviation as 0 0 and 14 4 denoted as n 0 0 14 4 the average and standard deviation from the 2 611 samples of residuals is 0 and 14 1 eq 17 thus facilitates the introduction of random model errors into the runoff series of experimental flood events the generated model errors are denoted as ϛ which is a vector with dimension as the flood duration δ 3 the true runoff series the tank model is used to produce the simulated runoff series i tk with r for a specific event combing i tk with the randomly generated ϛ yields the designed true runoff 4 the intentionally corrupted rain data several runs of outlier removal analysis are performed for a case of specific realization of model errors within a run the measurement errors of rainfall are introduced by the following procedure 1 the number of gauge malfunctions is randomly determined from a discrete range of five to ten of which each value is set to be equally likely to occur 2 the time of malfunction for a specific gauge is randomly determined by the uniform distribution the lower and upper bounds on random variables are set to symmetrically center around the time of flood peak with a radius of 20 h 3 the underestimation ratio is set to be uniformly distributed between 0 0 and 0 5 the generated ratio is multiplied with the true rain data past the generated malfunctioning time 4 the generated rainfall data with errors for each gauge is then weighted to produce r a vector containing the series of contaminated areal rainfall 5 experimental cases 1 for each of the three selected events nine different sets of statistical parameters of model errors are tested the nine sets are combinations generated by assigning the normally distributed ε with mean as 0 15 and 30 and standard deviation as 14 4 25 and 30 respectively this procedure produces twenty seven experimental cases by combining flood events and statistical parameters of model errors 2 two hundred random runs are routed for each experimental case each run is performed with randomly generated i and r formulations i and ii are applied to each run each produces the estimation of corrected areal rainfall denoted as r with the same number of elements as r and r 6 the indices of performance evaluation for each case 1 the percentage of total areal rain deviation is calculated as d r r r r 100 for each run taking the average of d r from the two hundreds run yields d r 2 the ce by the designed true r and the estimated r by optimization is calculated for each run its average from two hundreds runs is denoted as c e r r 3 the hitting rate for a run denoted as hb represents the detected malfunction times of a number of 10 hb gauges fall within a window with radius as b hours symmetrically centering the true value in other words an absolute deviation of ti from the designed true value within b hours is regarded as a hit two values of b as 6 and 12 h are evaluated h 6 and h 12 can then be averaged from the hitting rate of all runs tables 2 3 and 4 show the experimental results for the three events each of which are trialed for nine sets of parameters for model errors they reveal the following characteristics 1 the case representing the actual model errors with ε n 0 0 14 4 serves the purpose of demonstrating the proposed method both formulations deliver good estimation of temporal pattern and total magnitude of areal rainfall the d r is under 5 and c e r r exceeds 0 95 for both formulations nonetheless formulation i performs poorly in regards to hitting rate since it tends to overly fit the observed inflow by arbitrarily modifying the rain data to compensate for model errors formulation ii outperforms its counterpart in hitting rates shows more resistance to overfitting only removes relevant outliers and still delivers simulated runoff with acceptable precision 2 the tables show that the proposed method performs better in large and medium events the small events have relatively large proportions of loss in rainfall and base flow in the observed reservoir inflow this may offset the effectiveness of the proposed inverse problem based approach however significant difficulties in determining reasonable rainfall are primarily encountered during large or extreme events also the flood detention capacity of the reservoir in the case study is adequate to accommodate the total inflow volume of small events even the proposed method could not remove the outliers of rainfall measurements effectively the operating consequence under the underestimated rainfall will still be to release insignificant volume of water this is indifferent to the situations with exact rainfall information thus small events are insignificant in regards to operating reservoirs to attain goals even if errors in rainfall measurement are present 3 the experiments from different cases show that under an unbiased model with means of ε as 0 the increase in variance of model errors has an insignificant impact on outlier detection analysis 4 the last columns of tables 2 and 3 suggest that formulation ii yields hitting rates above 60 for medium or large events providing that the hydrologic model can produce ce above 0 8 with true rainfall r 4 conclusions and suggestions an inverse problem approach is proposed to improve the rainfall measurements of ground gauges based on the rainfall runoff modeling and observed reservoir inflow downstream the time of potential malfunction of rain gauges are regarded as decision variables and an optimization analysis is utilized to determine the optimal combinations of measurements from different gauges to regulate the the deviation between simulated and observed reservoir inflows within acceptable precision the perspectives of achieving the goals of reservoir flood control operation are used to define the acceptable precision of hydrologic simulation the objective function of the optimization analysis is designed to conduct precise and relevant outlier removal and prevent overfitting a realistic case study along with synthetically designed experiments verify the effectiveness of the proposed methods for significant flood events the proposed method can be integrated into the calibration of parameters of an event based conceptual rainfall runoff model in a typical practice measurements of several flood events are joined as training samples to improve the representativeness of calibrated parameters the calibration process requires iterative updating of parameters according to the results from simulations of all events the proposed method can be implemented to cleanse rainfall data during the simulation of each event a nested optimization framework will be created by inserting the proposed method in the inner loop to simulate each event with parameters updated by the outer loop while being more computationally intensive this approach prevents overfitting of model parameters by improving the qualities of training samples and allowing more historical events to be included in the calibration the utilization of a conceptual rainfall runoff model and the availability of telemetry measurements of reservoir inflow and upstream rainfall allows the proposed method to be implemented in real time the real time implementation of the outlier analysis assists in filtering out the underestimated rainfall input to the hydrological model thus the floodwater detained in the watershed can be simulated more accurately additional information can supplement reservoir inflow in the analysis such as the measurements from upstream river stage stations or reservoirs a distributed hydrological model with the locations of upstream stations as the outlets of sub watersheds can be developed to include the river stages or upstream reservoir inflow in the detection of outliers of rainfall measurements the purified data of ground gauges also provide a more reliable foundation for further implementation such as assimilation with observed data from weather radar to more precisely monitor the spatial pattern of rainfall author contribution the study was initiated by fnfc who also got the funds cww and fnfc both contribute to the methodological development of simulation and optimization models cww carried out the contents of all analysis in the case study the manuscript was jointly drafted by cww and fnfc declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this work is originated from a project funded by the water resources agency no moeawra1090120 taiwan r o c and carried out by the chunghwa telecom co ltd in cooperation of the authors and the manysplendid engineering consultants co ltd the project review committee from the southern water resources office of water resources agency have provided valuable comments and supports especially the chief of the zengwen reservoir management center mr po chung chen 
2021,this paper presents a robust cost effective framework for assessment of coastal fluvial flooding due to compound action of multivariate dependent drivers the methodology is an 8 step process that links statistical and hydrodynamic models to determine probabilities of multiple driver flood events and associated hazards the method involves individual and combined extreme value analysis assessment of dependencies and interactions between flood drivers multivariate joint probability determination accounting for dependencies high resolution hydrodynamic modelling of flood scenarios derived from multivariate statistical analysis and ultimately mapping of inundation using cork city on the south coast of ireland as a study case the research shows that the interactions and dependencies between tides surges and river flows affect flood severity when they occur jointly tide surge interactions have a damping effect on the total water level while dependence between the surge residual and river flow amplifies the risk of flooding the multivariate joint exceedance probability occurrence of high discharges and water levels represents a more realistic representation of the spatially variable water surface profiles then the combined univariate marginal scenarios multivariate analysis allows also considering multiple combinations of joint probability solutions along rp iso curves the results show that the quantification of compound flood impacts must be performed along the entire rp probability curve this is because the physical hydrological impacts of multiple driver same rp flood events can be very different leading to substantially different characteristics of flooding the multi scale nested flood model msn flood was used to simulate flood wave propagation over urban floodplains for an ensemble of statistically derived flood scenarios the hydrodynamic runs provide inundation maps that can be used to draw inferences about flood mechanisms and impacts keywords flood modeling compound events coastal fluvial flooding interactions dependence joint probability data availability data will be made available on request 1 introduction coastal conurbations are at risk of flooding caused by a combination of astronomical meteorological hydrological and climatic factors gallien et al 2011 in the future urban flood probability and risk will increase as a consequence of several factors including population growth in flood prone areas climate change and decaying or poorly engineered flood control infrastructure kirkpatrick and olbert 2020 gallegos et al 2009 bevacqua et al 2019 many large population centres are located along the coastline and many of these are along estuaries where freshwater flows merge with tidally driven sea water orton et al 2012 these centres lie most commonly in intertidal zones where water levels are directly affected by the upstream flow and the downstream coastal conditions naturally such coastal zones can be vulnerable to flood events from a single source or several sources acting in combination archetti et al 2011 that leads to compound flooding leonard et al 2014 moftakhari et al 2017 zscheischler et al 2018 the coastal tidal wave composed of astronomical tide mean sea level and non tidal residual constituents storm surges inter annual variability baroclinic processes may propagate up the river channel network and cause a flood far from the coast hendry et al 2019 variable coastal water levels change both the river stage and discharge and form the downstream river boundary of unsteady and non uniform flow these water levels may impede river drainage to an estuary by a backwater effect as their upstream propagation may reverse the seaward flow in a river ganguli and merz 2019 hoitink and jay 2016 on the other hand a river discharge may raise mean coastal water levels and generate friction that makes tides lose energy and shrink in amplitude piecuch et al 2018 moftakhari et al 2016 in summary river coastal interactions can contribute to subtidal friction modulate tidal amplitudes and impede flows impacting river discharge downstream sassi and hoitink 2013 the dependencies between non tidal residuals of coastal water levels and fluvial peak discharge may be significant as both events may result from a common meteorological cause severe storm periods are often associated with high winds and low pressure systems that generate storm surges while at the same time causing orographically enhanced high precipitation on coastal catchments resulting in high peak river discharges kew et al 2013 such simultaneous or successive occurrence of high coastal and river water levels may produce extreme impacts even when hazards from individual drivers in isolation would be unlikely bevacqua et al 2017 commonly coastal flood hazard assessments rely on univariate statistical modelling methods where river discharges and extreme sea levels are considered separately ganguli and merz 2019 such approaches assume stationary and unconditional distribution of flood signals and therefore may not correctly estimate the probability of a given hydrologic event salvadori and de michele 2004 moftakhari et al 2017 since coastal cities are at risk of compound flooding effects from multiple drivers univariate approaches should not be used to characterize the flood hazard moftakhari et al 2017 more recently the influence of compound events on flood hazards was studied using physically based and stochastic models van den hurk et al 2015 most efforts on compound flooding to date are based on a joint probability bivariate flood hazard assessment that accounts for compound flooding from river flow and coastal water level some of these methods account for dependencies among multiple drivers hendry et al 2019 most recently the copula based models have been used to analyse the joint frequency of compound floods bevacqua et al 2017 2020 yazdandoost et al 2020 zhong et al 2021 moradian et al 2023 these simulations however do not take account of the likelihood and intensity of fluvial floods conditional on coastal water levels or the severity of coastal water levels conditional on river discharges ganguli and merz 2019 and therefore do not explore all possible flood scenarios regardless of the compound event approach employed the aforementioned statistical methods do not provide important information on compounding effects of spatially distributed interactions between the river discharge and downstream ocean level in tidal channels and estuaries most commonly the bivariate models of coastal fluvial flooding use time series records of river discharge at the reach entry and water level measurements at the downstream end of the reach in a long complex estuary these data could be representative of two different hydrodynamic regimes moftakhari et al 2019 this issue can be addressed by linking the statistical and hydraulic models the two model approach allows one to hydrodynamically determine mechanistic routing of flood water onto urban floodplains under statistically derived exceedance probabilities ultimately this information can be used to assess flood hazards e g depth and velocity and understand the impact of such flood events gallien et al 2011 van den hurk et al 2023 gallien et al 2018 uddin et al 2022 while statistical models have been widely used their linkage to hydraulic models is not so common although simplistic bathtub models have been widely applied for estimating coastal flooding hazards they may potentially lead to underestimation of flood consequences due to their inability to vary flood stage with distance inland due to river tidal interactions lanzoni and seminara 1998 and variable non linear flood dynamics gallien et al 2014 sanders 2017 in a broader sense mapping flood hazard in complex estuaries using approaches that ignore local hydrodynamics can underestimate flood extent and depth fema 2015 while hydrodynamic modelling of rapid flood events in urban environments is a very complex and challenging task a number of successful investigations into combined coastal fluvial flooding has been demonstrated in recent years e g yang et al 2012 comer et al 2017 moftakhari et al 2017 olbert et al 2017 gallien et al 2018 griffiths et al 2019 however none of these studies links hydrodynamic and statistical models in a way that considers flood impacts for a spectrum of statistical conditions of a certain return period as such answering the following fundamental questions how severe these events can be or what combination of extreme signals can result in the most hazardous events is impossible importantly many different compound events have the same return period despite its iso curve representing different combinations of river flow tide and surge signals now these different events result in the specification of a range of flow tide and surge boundary conditions in the model having the same return period each different set of boundary conditions can give rise to spatially different flooding conditions throughout the model domain for one given return period in this complex problem model linking is possibly the only way to assesses the compound nature of coastal sea levels and river discharges and specifically to quantify severity of compound floods for a combination of extreme signals and by that to facilitate a comprehensive management of flood hazards luke et al 2018 it is quite clear that a robust and integral assessment of compound flood hazards should leverage the intrinsic characteristics of both modelling approaches into a coupled approach muñoz et al 2020 in this context the objective of this research is to develop a statistical hydrodynamic modeling toolbox and to provide a methodology for assessing the combined effects of multiple source flooding in urban areas the toolbox comprises 1 statistical models of frequency analysis extreme value of storm surges tides and river flows and ultimately the joint probability models for calculating joint exceedance return periods and 2 a hydrodynamic numerical model as the central engine where flood inundation under numerous coastal and fluvial flood scenarios derived from the statistical model are simulated such a statistical hydrodynamic modeling system allows to model coastal fluvial floods across the joint probability spectrum for a given return period event and this is a very important consideration for flood management modelling within this complex system thus allows to forensically investigate all possible impacts of a flood hazard and clearly disentangle between coastal and fluvial effects and by that provide a useful information for flood management engineers and policy makers to the authors knowledge the across jp spectrum modelling and hazard quantification have not been presented in a detail yet this new statistical hydrodynamic system is very significant for flood management as illustrated by the following 2 points i it is important to realise in a coastal flooding context that modelling the flood extent of one set of boundary conditions jointly having a particular return period say 50 years will not provide an answer to the question what are the flood extents having a return period of 50 years ii as in the system application described in section 3 below when a river boundary is located upstream of the modelled domain and the tide surge boundary is at the opposite end of the domain a joint event having a return period of say 50 years may give significantly different inundated depths and extents if we combine a relatively low river flow with a high combination of tide and surge than if we combine a relatively high river flow with a low combination of tide and surge thus to obtain the full extent and depth of inundation having a return period of 50 years a series of joint events must be simulated and the results processed to identify the particular compound event causing maximum flooding in all grid cells of the modelled domain the need and usefulness of the new toolbox has been demonstrated in this research for the case of the river lee in cork city this is located on the south coast of ireland and it regularly experiences compound coastal and fluvial flooding the impact of flooding is quantified based on a range of physical aspects such as coastal fluvial flood inundation extent and water depth the toolbox has been developed in a general purpose approach and so can easily be applied to other case studies the paper is structured as follows section 2 details the methodology where the numerical models and their setups are described along with the statistical approaches to extreme value analysis and determination of joint probabilities section 3 compares numerical model results for various flood scenarios and mechanisms it also presents extreme value analysis results which provide the boundary conditions and scenarios for the numerical models finally section 4 presents conclusions from the research and discusses the usefulness of the methodology and the implications of the results 2 methods in this section the methodology for assessment of flood probability and severity is briefly described followed by the review of statistical approaches used for extreme value and joint probability jp analyses and the descriptions of the flood modelling system used for cork city 2 1 flood assessment methodology in the absence of a systematic approach to statistical hydrodynamic modelling of floods an 8 step methodology for urban flood assessment is developed in this paper and presented on fig 1 as shown in this flowchart for compound flood events the analysis requires statistical and hydraulic modelling used jointly in a multi step process each driver must first be statistically analysed individually in order to estimate its independent frequency of occurrence following which jps can be estimated and flood events modelled a number of flood scenarios s for a range of extreme conditions must be developed from extreme value analysis to establish univariate and multivariate boundary conditions for the numerical model three scenarios are considered in this research s1 a marginal river discharge scenario of fluvial flood only t year rp discharge and non extreme coastal water level s2 a marginal coastal water level scenario of coastal flood only t year rp sea water level and non extreme river discharge s3 an and scenario of joint probability occurrence of high discharges and water levels of t year rp the hydrodynamic model is forced with a range of boundary conditions that represent a combination of flood drivers for conditional s1 s2 and joint probability s3 scenarios and various t year rps a synthesis of hydrodynamic modelling results is used to map inundation levels under various extreme conditions and to quantify physical compounding effects 2 2 statistical modelling in the first stage of statistical modelling astronomical tides surge residuals and river flows are subject to univariate frequency analysis and modelling of their extremes in the second stage a multivariate joint probability of their occurrence is estimated this analysis takes into account the potential multivariate dependencies between drivers the following sections briefly outline the methods used to calculate extreme values multi driver dependencies and joint probabilities of their co occurrence outputs from these analyses will be subsequently used to investigate flood hazards due to extreme events 2 2 1 extreme value analysis univariate frequency analysis is used to determine extreme values of individual flood drivers probabilistic models from the field of extreme value statistics are usually adequate to quantify variables in terms of their extreme values and associated return periods moftakhari et al 2017 many studies including lowe et al 2001 butler et al 2007 olbert et al 2013 and sun et al 2017 demonstrate their suitability to the analysis of tides surges and or river flows the probability that a water level z will be exceeded by any water level in any given time period is termed the probability of exceedance of the water level z p z and is mathematically expressed as 1 p z 1 f z z f x d x where f z nd f x re the cumulative distribution function cdf and probability density function pdf respectively the return period rp time t represents the average time between consecutive occurrences of water levels equal to or greater than the given level z 2 t 1 p z 1 1 f z a generalized extreme value model gev coles 2001 was used to estimate extreme values of tides surges and river flows and their rps the cdf of the gev is expressed as 3 f z exp 1 ξ z μ σ 1 ξ where μ is a location parameter σ a scale parameter and ξ a shape parameter the value ξ determines the asymptotic extreme value distribution and hence the type of distribution ξ 0 epresents a gumbel distribution ξ 0 frechet distribution and ξ 0 weibull distribution 2 2 2 multivariate dependencies in order to accurately calculate the joint probability of extreme events dependent interactions between any of the variables must be first assessed the method utilised here is the χ dependency measure based on tail dependence coles et al 1999 4 χ u 2 ln p u u v u ln p u u f o r 0 u 1 where u is the upper threshold of the uniform distribution u and v are transformed variables with uniform margins 0 1 such that 5 p u u v u number o f x y p a i r s s u c h t h a t x x a n d y y total n u m b e r o f x y and 6 p u u 1 2 number o f x x total n u m b e r o f x number o f y y total n u m b e r o f y where x y s an observational pair from the original data series while x y s the threshold level for the observed series of the same probability of exceedance p coles et al 1999 suggest interpretation of total dependence as χ 1 nd total independence as χ 0 partial dependence for example χ 0 1 means that there is a 10 risk of the two variables exceeding their threshold at the same time with the threshold for each variable corresponding to the same probability 2 2 3 joint probability jp can refer to the exceedance of river discharge and coastal water level referred to here as and scenarios or the exceedance of river discharge or coastal water level referred to here as or scenarios there is a wide range of parametric and non parametric methods developed for bivariate frequency analysis the copulas functions sklar 1959 are used to formulate joint distributions of bivariate pairs moftakhari et al 2019 bevacqua et al 2017 sadegh et al 2018 use a bayesian framework with copula functions and dependence structure in this study where a dependence between two variables exists the jp rp of an event t x y where both variables exceed their thresholds is represented as 7 t x y 1 1 1 t x t y 2 χ u 2 t x t y 1 in cases where there is no dependency between two components a method developed by pugh and vassie 1980 and revised by tawn 1992 is proposed for a given water level z if the pdf of one variable is f x η s nd the pdf of another variable is f y s the pdf of the total water level f η s a combined probability of two variables occurring simultaneously is 8 f η f x η s f y s d s the joint probability rp t x y of an event where both variables occur simultaneously can be expressed as 9 t x y 1 1 f z where f z s the joint cdf for a given extreme sea level z efined as 10 f z z f η d η in the rjpm method given in equation 8 the normalized pdfs of two variables i e tides and surges are then used to construct a joint probability matrix in this matrix each diagonal represents a probability of joint occurrence of two variables due to a certain combination of variable one and variable two the sum of each diagonal representing a certain joint value i e water level due to tide and surge gives the probability of occurrence of that joint variable the example of construction of joint probability matrix is presented in pugh and vassie 1980 ultimately probabilities are converted to rps using equation 9 as explained in tawn 1992 the outputs from the univariate and multivariate statistical analyses of river discharges and coastal water level pairs are used to construct flood hazard scenarios for hydrodynamic runs 2 3 hydrodynamic modelling in this research modelling of coastal fluvial flood inundation extent and water depth was carried out using a state of the art multi scale nested msn flood hydrodynamic flood model msn flood is a two dimensional depth averaged finite difference model that solves the depth integrated navier stokes equations and includes effects of local and advective accelerations earth rotation barotropic and free surface pressure gradients wind action bed resistance and prandtl mixing length turbulence scheme the nesting structure of the model comprises a two level cascade of dynamically linked nested grids at 6 and 2 m resolutions a 6 m parent grid model provides water level conditions from the greater cork region to a 2 m ultra high resolution nest which covers the downstream section of the river lee channel and cork city fig 2 a novelty of the model is its unique nesting scheme which utilizes a sophisticated approach to nested boundary formulations to allow the location of nested boundaries in the flooding and drying zones this means that large sections of the boundary may alternatively flood and dry extensive model validation at each of the nested levels can be found in nash and hartnett 2010 and comer et al 2017 while details of model parameterization sensitivity and comprehensive validation of the 2 m model can be found in olbert et al 2017 since the same model configuration is used in this research readers are referred to olbert et al 2017 for the model description and performance assessment 3 results in this section the outputs from statistical hydrodynamic modelling are presented statistical modelling involves the analyses of frequency extreme value of storm surges tides and river flows and jps of exceedance the jp values are used to construct the boundary forcing conditions for the hydrodynamic model which is used to simulate coastal fluvial floods for a range of extreme events and across the joint probability spectrum iso curves for each of the rp event an ultimate set of results are the maps of inundation extents and water depths for a combination of river flows and sea levels of given rp 3 1 univariate analysis of individual flood drivers in the proposed methodology the univariate statistical analysis of extremes step 3 is performed to establish extreme conditions of individual drivers for marginal scenarios s1 s2 and joint probabilities of multiple drivers for multivariate flood scenarios s3 statistical analysis of extreme values of river lee discharges and coastal water levels decomposed to tides and surge residuals in cork estuary are based on records of peak over threshold pot data the extreme value analysis of surge residuals was conducted on a dataset of surge values obtained from surge simulations of 48 storm events over the 46 year period 1959 2005 only the maximum surge value from each of the simulated events was extracted to guarantee independent events this yielded a dataset of 48 maximum values the highest of which was 0 81 m astronomical tides extracted from a 46 year time series for tivoli cork noaa 1982 located on the eastern boundary of the 6 m model domain were used in the frequency analysis the accuracy of this dataset was corroborated by comparison against a harmonic dataset constructed from existing records for tivoli tidal gauge station and the nautical almanac hewitt and lees spalding 1982 the 38 largest river discharges were identified from a 5 9 year long timeseries of river gauge records on the river lee gauge number 19012 the frequency analysis was used to estimate the probability of occurrence of any of the three flood components considered here for the exceedances to be considered extreme high threshold values were set in the pot analysis the pot levels for surge and river flow were 0 4 m and 70 m3 s respectively a gev statistical distribution was fitted to the pot data the uncertainty associated with the selection of a probability density function was significantly reduced by ignoring the lower bound values the computed rps are presented in table 1 and fig 3 although the fitted distributions provide frequencies of occurrence of up to 1000 year rp only the values of the 50 and 200 year rps were considered in the analysis davie 2008 recommends that the extrapolated rps should not exceed twice the length of the dataset while hall et al 2006 found that analysis over a timescale of 30 100 years introduces uncertainties the method may introduce a degree of model uncertainty for some variables e g sea level as a result of interferences produced by climatic signals such as climate modes or climate change 3 2 extreme water level analysis in assessing the potential flood risk of a coastal region like cork city the likelihood of occurrence of joint extremes of tide surge and river discharges is important although the method for the determination of extreme water levels due to the independent actions of tides surges and river flows gives reasonably good estimates of flood risk they are known to be inaccurate but no universally accepted approach exists for determination of the water level due to the combined effect of all three signals haigh et al 2010a the treatment of potential dependencies between variables in a multivariate problem is a main reason for the difficulty there is a good body of evidence showing that tides and surges interact e g olbert et al 2013 idier et al 2012 and river flows and surges can be dependent when driven by the same meteorological conditions orton et al 2012 the analysis of dependency between tides surges and river flows is the second step of the methodology and is used in conjunction with the univariate extreme analysis step 3 to estimate joint probabilities of occurrence of multiple flood drivers step 4 3 2 1 tide surge interactions extreme sea water levels result from a combination of astronomical tides and non tidal processes such as storm driven surges wind waves and baroclinic flows these flood drivers may interact and exhibit dependencies olbert et al 2013 found that extreme sea levels in cork harbour are the product of a moderate to high surge coinciding with high water spring tide nonetheless the likelihood of the simultaneous occurrence of these two signals requires in depth analysis fig 4 shows the temporal variation of surges and tides and their contribution to total water levels in cork city during a 2009 flood event the characteristic pattern is that the surge tends to peak during a rising tide around half way between mid flood and high water so its temporal variation although driven by meteorological conditions seems to be modulated by tides prandle and wolf 1978 and horsburgh and wilson 2007 describe this mechanism of tide surge wave modulation in detail this pattern is not an isolated incident it occurred in over 50 of the 48 surge events investigated fig 5 shows surges classified into one of eight groups each representing a particular phase of tide while there is no clear relationship between the surge magnitude and phase of tide the distribution of surge peak occurrence over a tidal cycle indicates the presence of non linear interactions haigh et al 2010b olbert et al 2013 analysed this type of interaction in irish coastal waters and found that the level of interaction varies geographically a χ2 statistical model was used to quantify the level of dependence at the 95 significance level and n 1 degrees of freedom χ 7 0 95 2 14 07 in the above research χ2 12 0 was found for cork this implies a low degree of interaction between surge and phase of tide despite the fact that surges tend to peak on rising tide more frequently than on other phases fig 5 interestingly no dependence between surge peak and tidal height was found and surge heights do not vary across tidal phases as tidal currents are stronger than surge currents in cork tides modulate the interaction in a non linear manner the quadratic bottom friction being here the principal cause of tide surge interactions in this region olbert and hartnett 2010 attenuates and smooths the amplitude of the surge as explained in dinápoli et al 2020 taking a conservative solution an assumption of tide surge independence was used in the joint probability calculations 3 2 2 surge river flow dependence recent studies clearly show that dependence may exist between river discharge and either coastal water level or storm surges and not accounting for dependencies in joint probability analysis may underestimate the compounding effect de michele et al 2005 ward et al 2018 in coastal sites the dependence between river flow and total sea water level or surge often results from a common meteorological cause kew et al 2013 so they may occur simultaneously the time lag between river flow and total water level is another aspect to consider in the dependence analysis the time lagged analysis accounts for the fact that the storm surge peak may arrive at a different time than the river flow peak despite both being generated by the same storm event fig 6 presents a scatter plot of daily mean river levels and daily maximum surge residuals the dependence measure χ equation 4 between river gauge data and storm surge residuals was calculated for pairs of daily mean river level and daily maximum surge residual the thresholds for surge residual and river flows converted to water levels are 0 488 m and 1 8 m respectively the dependence analysis was carried out for two scenarios a maximum values occurring on the same day or b time lagged scenarios the lagged analysis was to determine whether or not a significant time lag existed between the response of the two processes results from these analyses are shown in fig 7 for the same day occurrence scenario the dependence measure χ between surge residual and river flow of 0 101 is considered to be significant at the 5 significance level χ 0 05 0 06 the 5 and 95 confidence intervals of χ dependence are 0 012 and 0 207 respectively for the time lagged analysis the daily maximum surge residuals were selected corresponding to daily maximum discharge values with time lags of 3 to 3 days the χ values calculated when surge precedes river level are greater than the values calculated for the opposite scenario or the same day dependence when surge precedes river level by one day the highest dependence of 0 131 is exhibited this 1 day lag dependence between high river discharges and coastal water levels is in line with observations from the west south of the uk hendry et al 2019 where meteorological conditions are often part of the same large scale weather systems common to the uk and ireland moreover according to hendry et al 2019 this phenomenon is characteristic of relatively small catchments with a low baseflow such as river lee is 1 253 km2 40 4 m3 s since the high river discharges occur on the day of or just after peak surge compound flooding is still a concern the flood modelling results of olbert et al 2015 clearly show that the time between river and tide peaks in cork is too short for initial floodwaters to recede as the most conservative approach the 1 day dependence level was therefore used to derive joint exceedance probability of high river flows and surge residuals 3 2 3 multivariate joint probability in step 4 of the methodology the multivariate join probabilities of compound floods are estimated a number of jp methods were considered described in section 2 however the methodology adopted in this study being a combination of bivariate and trivariate analyses is best suited for multivariate problems with various levels of dependencies between variables accurate assessment requires the dependencies if they exist to be included to account for lower marginal exceedance probabilities the jp analysis is a multistep process illustrated in fig 3 the first step is the bivariate jp analysis of the extreme sea levels the extreme sea levels are calculated using the revised jp method rjpm of tawn 1992 given in equation 8 in this method assuming tide surge independence tides and surges are firstly independently modelled using the statistical gev model and their normalized pdfs are then used to construct a jp matrix the sum of each diagonal in the matrix being a probability of a water level due to a certain combination of tide and surge and representing a certain water level gives the probability of occurrence of that water level the extreme water levels due to the combined action of tide and surge calculated using the rjpm are presented in table 1 and fig 3 c for the 50 and 200 year rps the coastal water levels are 5 39mod and 5 49mod above msl respectively for a complete set of conditions contributing to flooding high river flows fig 3b and their interactions with surges are accounted for using trivariate joint probability the trivariate joint rp is calculated using equation 7 for a combination of the selected rps of water levels from bivariate joint probability of occurrence of tides and surges and rps of river flows dependence between two variables is quantified through χ equation 4 the 50 and 200 year iso curves of joint exceedance rps for a combination of river flows and sea levels calculated for the and hazard scenario are presented in fig 3 e while the rps iso curves represent the joint events of the same exceedance probability the physical hydrological impact of such events can be very different leading to substantially different characteristics of flooding flood inundation maps for these iso curves are generated in step 6 using a hydrodynamic model 3 3 flood hazard modelling the statistical hydrodynamic methodology proposed here is possibly the only way to comprehensively assesses the compound nature of coastal sea levels and river discharges the high resolution numerical model of cork city covers the downstream reach of the river lee and the adjoining floodplains of cork city centre the upstream boundary was prescribed as the river discharge while the downstream boundary was forced with a variable water elevation to simulate the tidal signal and non tidal residual the boundaries were placed far enough apart to let compounding effects develop within the domain the boundary conditions were generated from the univariate and bivariate analyses for the s1 s3 flood scenarios defined in section 2 1 the hydrodynamic runs for each scenario were used to generate maps of extreme water levels for the selected rps this is a simple and efficient assessment approach that requires only a small computational effort while delivering comprehensive quantification of flood hazards across a spectrum of conditions including compounding events each map presents the maximum flood extent and the maximum flood water depth on floodplains based on the maximum water level recorded over an unsteady simulation covering the rise and fall of a flow peak with the rise and fall of a coastal water level 3 3 1 coastal flooding the hydrodynamic simulations of coastal flood only represent the hazards associated with a single driver marginal scenario of a flood event driven by an extreme sea water level fig 8 a presents the maximum water depths simulated for the marginal scenario of 200 year rp sea levels and average river discharge where the extreme sea levels were derived from a joint probability analysis of the independent occurrence of tides and surges this scenario only results in a small number of very localised floods along the river channel that do not constitute a major flood threat to the city which indicates that the cork city flood defence systems are well able to protect against the 200 year coastal flood indeed the existing coastal defence structures along urbanized areas of cork harbour coastline had been designed to prevent spring tides from causing flooding and therefore also they may protect against the high surges that peak on low to moderate tides by comparison fig 8b presents the maximum water depths simulated for the more unlikely scenario of 50 year rp tide with 200 year rp surge peaking at high water and average river discharge this scenario not only produces widespread flooding along the riverbanks but also in the business commercial downtown city area the timing of the surge peak plays a crucial role in flooding extents as demonstrated in section 3 2 1 the acceleration of a surge wave when travelling along with the tidal wave in cork harbour prevents the surge from peaking on a high tide this interaction is attributed to a shallow water effect idier et al 2012 and significantly alleviates flooding comparing the flooded area from a simulation where the surge peaks on the flood tide fig 8c to that in fig 8b where it peaks at high water there is a 30 reduction in the inundation area 3 3 2 fluvial flooding the univariate flood hazard assessment for the marginal river flow scenario was conducted by running an ensemble of river discharge simulations conditional on the mean high water level on the eastern tidal boundary representing average spring tide conditions with no surge the fluvial flood events in cork city occur when the run off exceeds the conveyance capacity of the river lee channel and spills into the street network fig 9 shows maximum water depths due to the river flood wave propagating through the city floodplains for 50 and 200 year rp river flows once the conveyance capacity of the river channel is exceeded the flow starts to spill into floodplains at numerous locations along the riverbank for both 50 and 200 year rps flooding progresses very rapidly to reach maximum extents approximately 10 h later during this period the flood wave propagates along preferential flow paths in the main west east direction through recreational fields along the river and major streets before reaching the ponding areas of the low lying downtown city streets while the downtown zone is only marginally flooded at rp 50 fig 9a the rp 200 peak results in a significant portion of the city centre being submerged fig 9b 3 3 3 marginal scenario flooding comparing the pattern of flood water distributions for marginal coastal and fluvial scenarios some distinctive differences in flooding characteristics emerge the results show that there is a shift in the flood hazard patterns along the length of the river reach depending on the dominant mechanism controlling the flood the coastal water levels control the outlet with floods mostly affecting the downtown area while the river discharges control flood hazards further inland the fluvial floods dominate in the upstream city suburbs along the north channel corridor before spreading downtown during larger flood events 50 year rp the length of riverine control is much longer than the tidal length which spans a relatively short downstream reach of the tidally active river lee as such the pattern of flood defence systems overtopping is remarkably different for both mechanisms during coastal flooding the major inundation stems from south channel spillage while fluvial flood waters enter floodplains primarily from the north channel with limited overtopping of flood defences along the south channel in addition the extent of flooding for the 200 year rp is substantially greater for the fluvial scenario the composite maximum water depths due to marginal coastal and fluvial floods generated by superimposing marginal fluvial over marginal coastal flood water depths are shown in fig 10 these results do not account for interactions and joint occurrence of fluvial coastal drivers fig 10b presents a synthetic map of 1 in 200 year fluvial flood and a highly unlikely low probability coastal flood of rp200 surge coinciding with a rp50 tide 3 3 4 combined coastal and fluvial flooding the final step of the methodology is to quantify the flood hazard due to combined action of tides surges and river flows and the interactions between them in the tidally active reach of river lee the raising sea level pushes ocean tides upstream as the tidal signal propagates inland from the estuary the aggravated interaction between the river and coastal signals causes the backwater river profile to develop an elevated water overtops the riverbanks and spills into urban floodplains the hydrodynamic model was forced with iso probability pairs of river discharge and sea water levels fig 3e generated for 50 and 200 year rp events considering dependence between the drivers the iso probability curves were composed of the joint exceedance return values calculated using the trivariate statistical analysis of univariate river discharges with bivariate sea levels the curves present combinations of sea levels and river flows ranging from extreme sea levels and moderate flows on one end to extreme river flows and moderate sea levels at the other end this type of combined analysis allows one to draw inferences about compound flood hazards using only a limited number of simulations while the flood probabilities are equal along a joint probability curve the severity and therefore risk may be different along the curve this is because the joint probability curves are constructed from a combination of flood drivers of various probabilities i e low probability discharge and high probability sea level or high probability discharge and low probability sea level the risk and hence impact of each driver is different and varies spatially the flood severity for a flood event of a given rp depends on flood defence systems design and stability and hydraulic properties of the floodplains such as topography bed slope and roughness the case study of cork clearly illustrates that the flood risk is a function of probability and severity fig 11 shows maps of maximum inundation for a combination of flood drivers contributing to the 200 year joint exceedance event assuming dependence between drivers the total area inundated by each of these events is summarized in table 2 as can be inferred from these plots and table the most severe floods are associated with events characterized by high river discharges and therefore driven primarily by the fluvial mechanism however sea water levels due to tides and surges pose an elevated risk of flooding the fluvial flooding is initiated from the upstream area of the north channel of the river lee after overtopping the defences flood waters spill away from the river channel travelling eastward across the central part of the city and flowing downhill towards the most eastward part of the downtown area in contrast when the flood is controlled by the coastal mechanism the major inundation originates from the south channel and propagates northward and southward the overall coastal inundation is relatively small which suggests that the flood defence systems in the tidal section of the river are capable of protecting against 200 year coastal driven floods for the worst case scenario fig 11f the 200 year rp flood is severe and results in 75 9 ha of urban inundation this is due to a combination of high river flow 647 1 m3 s corresponding to a 343 year rp event and moderate sea water levels 4 98 mod representing a 2 year rp event the level of dependence between flood drivers is another aspect to be considered in the joint probability analysis as shown in fig 3e the magnitudes of flood drivers for a given joint occurrence probability vary depending on the level of dependence between the drivers the higher the dependence the larger the joint exceedance values for a given probability of occurrence and consequently the larger the flood extents fig 12 summarizes the total area of inundation due to 200 year rp joint probability coastal and fluvial flooding for the three dependence scenarios only the worst case fluvial dominated scenarios are presented as expected the most severe inundation results from a compounding event of high dependence between flood drivers such as high discharges prescribed at upstream boundary coinciding with medium to high sea water levels at downstream boundary additionally the co occurrence generates interactions between river discharges and the upstream propagating tidal wave the unsteady non uniform upstream wave impedes the river flow slows down the rate of draining to the estuary and generates a backwater profile upstream with elevated water levels in the intertidal reach the interaction also can modulate the amplitude and shape of the tidal wave itself the effect of driver interactions was investigated by comparing flood hazards due to the joint probability and scenario fig 11 and the combined marginal scenarios fig 10 the difference in the prediction of flood water depths and extents between simulations is clear in the tidal reaches of the river where water levels are subject and sensitive to both riverine discharge and sea levels in the floodplains of the intertidal zone the water depths and velocity magnitudes are generally higher for the and scenario then for the two combined marginal predictions interestingly this happens despite much lower water levels at the upstream and downstream boundaries of the and scenario i e fig 11e 568 m3 s 5 18 mod when compared to the two marginal scenarios combined fig 10a 600 m3 s 5 49 mod this is due to physical compounding effects and nonlinear interactions between the discharge and water level described by shallow water wave theory for the and scenario with no dependencies fig 12 with χ 0 0 the flood extent is very similar to that of the combined marginal scenario 600 m3 s 5 49 mod despite significantly lower boundary values of the and scenario 483 m3 s 4 98 mod which clearly implies the amplifying effect of the interactions in contrast the and scenario predicts lower flood hazard levels in the non tidal zone upstream river reach compared to the marginal profiles not shown here because the river discharges contributing to the joint probability event are lower than those contributing to the riverine flooding only 4 discussion flood risk is a function of the probability of flooding and the consequential damage integrated over all possible flood events hall et al 2006 both variables in this function are subject to large uncertainties this paper presents a robust methodology for assessment of a hazard associated with a compound flood which can be further used to draw inferences about risks associated with such complex events if there is only one driver responsible for flooding the accuracy of flood probability depends on the data availability and statistical methods used in assessment of extremes in cork when flooding is primarily driven by one driver only fluvial or coastal the univariate gev analysis of long timeseries of river flows or sea water levels provide a relatively accurate assessment of return levels for upper tail low probability events however when flooding is compound and driven by both drivers in addition to the probabilities of individual drivers the joint probability of simultaneous occurrence of the two drivers must also be considered data for cork imply that majority of flood events are compound but the contribution of each driver to the flood event may vary from one event to another complexity is further exacerbated by the presence of interactions tide and surge or dependencies between drivers river discharge and surge as such cork city is a good example to demonstrate that to fully understand the complexity of multi driver flood dynamics and the overall impact of a flood hazard the drivers need to be assessed in an integrated manner using the modelling methods the methodology proposed here is an eight step process that combines statistical and hydrodynamic modelling 4 1 data collection and analysis the first step concerns a collection of data records of flood drivers from which the multivariate dependencies step 2 and univariate extreme distributions step 3 can be generated in general the longer the record the more hydrologically meaningful the estimate of low frequency peaks to fulfil this requirement where availability of long term gauge data is problematic as in many coastal areas a partial duration series or model simulations may be used instead this hybrid approach was used for cork case study where the limited length tidal gauge records for surge residual univariate analysis were complemented by the hindcast model data obtained from hydrodynamic model runs for known past high surge events in the methodology proposed here the upper tail datasets of tides surge residuals and rivers discharges are used to inform the multivariate joint probability analysis and hydrodynamic modelling 4 2 multivariate dependence analysis in order to accurately calculate the joint probability of extreme events step 4 dependent interactions step 2 between any of the variables must be first assessed recent studies clearly show that dependence may exist between river discharge and either coastal water level or storm surges but very few of them provide return levels of compounding events moreover only a small number of studies quantify a flood risk due to compounding effects of extreme coastal fluvial water levels de michele et al 2005 found that ignoring dependencies may result in over or underestimation of flood risk in coastal sites where storm surges are small relative to astronomical tides the correlation between total water level and river discharge is not statistically significant moftakhari et al 2019 however correlation may be found between river discharge and the non tidal residual such as surge considering that high river flows and surges can be generated by the same mechanism such as a low pressure weather system the likelihood of both events occurring simultaneously can be high khanal et al 2019 hence the second step of the methodology concerns the multivariate dependence analysis and an assessment of correlation structure for a pair combination of flood drivers there is a range of linear and rank correlation coefficient measures that are used to quantify dependencies hawkes et al 2002 heffernan and tawn 2004 coles et al 1999 the rank based kendall s rank correlation and spearman s rank correlation are robust against outliers however correlation coefficient only detects the degree of association between two variables and does not capture the dependencies well in low probability extreme ranges ganguli and merz 2019 for this reason in this research the dependence is quantified through the χ tail dependence coles et al 2000 the results show that there is a statistically significant dependence between surge residual and river flows on the south of ireland where cork harbour is located the storms are generally generated by low pressure systems to the south west of ireland and strong south westerly winds with propagation patterns towards the north and northwest olbert and hartnett 2010 these storm track patterns may justify the dependence between the two drivers in cork according to hendry et al 2019 the dependence between high river discharges and high surges occurs at sites where storms that generate these events are typically similar in characteristics and track across on comparable pathways the time lag between river flow and total water level is another aspect to consider in the dependence analysis ward et al 2018 showed that the lagged occurrence of river and coastal peaks can influence overall inundation extent and thus need to be considered for hazard mapping and planning of emergency responses the time lagged analysis accounts for the fact that the storm surge peak may arrive at a different time than the river flow peak despite both being generated by the same storm event in large catchments a storm approaching the coast may generate a high storm surge before travelling inland and generating high precipitation and elevated river discharge likewise a storm may travel over land before reaching the coast so the peak in river flow may precede the surge peak hendry et al 2019 imply that surges coincide with high river discharge in catchments characterised by a lower base flow smaller catchment area and steeper bed slope while the peak river flow may occur several days after the surge in large catchments with a high base flow and mild elevation gradient in case of cork the highest dependence was found when surge precedes river level by one day this time lag dependence in river lee characterised as a relatively small catchment with a low baseflow 1 253 km2 40 4 m3 s confirms findings of hendry et al 2019 these results are in line with observations from the west south of the uk where meteorological conditions are often part of the same large scale weather systems common to the uk and ireland hendry et al 2019 the surge discharge analysis shows that dependence is not only a function of geographical position of river catchments versus coastline and storm trajectory but also depends on the response time of a river as a function of catchment characteristics such as size or elevation gradients holtan and overton 1963 ward et al 2018 show that where high river discharge and high sea levels are exceeded and these drivers are dependent the joint probability of events can be several magnitudes higher compared to the independent scenario since not accounting for surge discharge dependencies in joint probability analysis may underestimate the compounding effect the need to include them is paramount in a multivariate dependency analysis another set of variables to consider are the components of total sea water level generally in sheltered coastal locations waves do not affect water levels because of a limited direct impact of winds relative to open coastlines melet et al 2018 and storm driven residuals are often weak or significantly smaller than astronomical tides so that surge impacts are only significant when they coincide with high spring tides along the european coast extreme sea levels are typically generated by moderate surges co occurring with spring astronomical high tides haigh et al 2010b this is also the case for extreme sea levels in cork harbour which are typically the product of a moderate to high surge coinciding with high water spring tide this phenomenon is due to the interaction between storm surges and tides that prevent the surges from peaking on high water in fact the maximum surges are more likely to peak on the rising tide 3 5 h before tidal high water which can amplify the surge magnitude idier et al 2012 this holds true for many coastal sites around ireland olbert et al 2013 in cork however while surges tend to peak on rising tide more frequently than on other phases these interactions are weak and statistically not significant therefore it is reasonable to assume an independence between tides and surges and take a more conservative approach in the joint probability analysis this means that extreme sea water levels may be higher as surge residuals may peak on a high water tide 4 3 univariate extreme value analysis in step 3 of the methodology the probabilistic models from the field of extreme value statistics are used to calculate extreme values of individual flood drivers such models can be used to draw inferences about extremes from relatively extreme values alone and so do not require multi year timeseries of data like analytical methods do however they are sensitive to the choice of the distribution and the fitting procedures and so involve some uncertainty around the often subjective fitting of statistical distributions woth et al 2006 for all three drivers in cork the gev model exhibits a very good fit to peak over threshold data so the extreme values and associated rps seem to be adequately quantified 4 4 multivariate joint probability in the fourth step of the methodology the univariate extreme values step 3 for three individual flood drivers are combined with dependence results step 2 to estimate joint probabilities of occurrence of multiple flood drivers salvadori et al 2016 defines joint probability as 1 the exceedance of river discharge and coastal water level or 2 the exceedance of river discharge or coastal water level while flood events can be driven by either and or or scenarios in an estuary where an upstream discharge and downstream water level co occur the and scenario of joint exceedance probability is more appropriate particularly when dependencies exist in fact moftakhari et al 2019 found that the or scenario significantly overestimates the return levels given by univariate analysis marginal scenarios and the joint probability and scenario the or scenario with unrealistically high extreme values in areas of low probability density represents the highly unlikely highly conservative approach and therefore it is not considered in the joint probability analysis of this study the multivariate joint probability of the three signals tides surges and river discharges was firstly calculated as bivariate probability of tides and surges occurring simultaneously with no dependency to obtain total sea water levels and then probabilities of sea water levels coinciding with river discharge were considered in so called trivariate joint probability the iso curves of joint probabilities of all three signals occurring simultaneously are the final statistical outcome each curve represents a combination of drivers magnitudes that jointly generate a condition of certain probability of occurrence while the rps iso curves represent the joint events of the same exceedance probability the physical hydrological impact of such events can be very different and hence leading to substantially different flood risk maps therefore when assessing flood risk it is crucial to evaluate flood impacts across the whole spectrum of exceedance probabilities and this has been done in this study using a hydrodynamic model in the multivariate joint probability analysis the effect of inclusion dependencies was also considered rps iso curves generated with various levels of dependencies for cork show that the occurrence of dependencies modifies the joint extremes and the stronger dependencies the higher the joint exceedance values along the curve of a certain rp dependencies between flood drivers in the inter tidal river reach in cork lead to higher flood risks in this zone and this is confirmed by hydrodynamic modelling ward et al 2018 also found that where high river discharge and high sea levels are exceeded and these drivers are dependent the joint probability of events can be several magnitudes higher compared to the independent scenario accounting for joint occurrence of multiple flood drivers is important for hazard mapping designing flood protection infrastructure and planning emergency responses 4 5 selection of flood scenarios the step 5 of the methodology concerns the selection of flood scenarios for hydrodynamic runs in total four sets of runs for cork were performed two sets of runs considered univariate marginal scenarios fluvial flood or coastal flood only one run considered combined marginal coastal and fluvial runs and finally one set of runs considered and joint probability scenario in reality observations show that cork city floods are due to a combination of drivers with varying degree of contribution and sole action of one driver would be rare and linked to river flood defence management rather than meteorological conditions and storm modulated co occurrences of drivers anyway in this study all the four sets of conditions were investigated to explore all potential risks 4 6 ensemble flood model simulations step 6 concerns hydrodynamic modelling of flood scenarios there have been many studies in recent years that investigate joint occurrence of river flows vs coastal water levels and dependencies between them e g wahl et al 2017 khanal et al 2019 klerk et al 2015 bevacqua et al 2020 although these studies are useful to understand processes that drive flooding they do not answer the fundamental questions of how severe these events can be and what combination of extreme signals can result in the most hazardous events moftakhari et al 2019 go a step further and link statistical analyses with a hydrological model to answer first of these questions this research builds on moftakhari et al 2019 to answer both questions while the present study uses a similar methodology to moftakhari et al 2017 it is more heavily focused on the hydrodynamic component and provides a full quantification of impacts along the entire rp probability curve and across various flood scenarios moreover the hydrodynamic model accounts for the effect of friction inertia and topographic complexity in flooding dynamics as well as physical compounding e g backwater wave damping amplification due to riverine and or tidal forcing that change flood stage and routing these aspects have not been analysed in depth yet 4 7 assessment of inundation due to multiple flood drivers step 7 concerns an assessment of inundation due to multiple drivers comparing modelled inundation extents with those observed historically for similar flood conditions it is apparent that the joint probability and scenario represents a more realistic representation of the spatially variable water surface profile then the combined marginal scenarios or individual univariate marginal scenarios interestingly water depths and flood wave velocities are higher for the and scenario than for the combined marginal scenarios for the same rp event despite the fact that the and scenario is driven by substantially lower extreme univariate conditions that confirms the amplifying effect of the coinciding drivers in their moderate ranges another aspect of multivariate analysis is a consideration of multiple combinations of joint probability solutions the extent of a flood varies greatly for various combinations of drivers magnitudes although each combination drives a flood event of the same probability hence for cork city much more impactful are events where the contribution of a fluvial component is larger than a coastal one and this is because the city is better protected against high sea water levels rather than high river discharges the hydrodynamic analysis shows that flood impacts may vary greatly along a single probability curve so statistical modelling need to be accompanied with hydrodynamic modelling to include all possible combinations of drivers and their joint impact therefore the statistical hydrodynamic methodology proposed here is perhaps the only way to comprehensively assesses the compound nature of coastal sea levels and river discharges also it is apparent from the analysis that the multivariate approach is more accurate than the traditional univariate assessment methods moreover the proposed method requires only a limited number of hydrodynamic simulations to map flood hazards these findings have particularly important socio economic implications for cork as the intertidal reach is adjacent to floodplains characterized by high urbanization with a significant accumulated economic wealth 4 8 flood mechanism establishment step 8 concerns assessment of the flood mechanisms the flood investigation based on the proposed statistical hydrodynamic method allows one to draw inferences about flood mechanisms propagation dynamics and hazards under various flood probabilities the hydrodynamic model gives an opportunity to test multiple scenarios so a good understanding of fluvial coastal mechanisms can be gained the hindcast runs for cork s past flood events validated against observations confirm that the city is more vulnerable to fluvial driver as the fluvial floods often originate in the rural areas far upstream and propagate to the city centre through the streets sloping along the river channel also the costal flood defence systems are designed to protect against a 200rp sea level event the mechanism however is likely to shift in future climate towards coastal component and have more pronounced effect on floods as demonstrated in kirkpatrick and olbert 2020 estimating the potential risk to flooding and understanding flood controlling conditions greatly aids flood risk management so that flood prevention or alleviation schemes can be evaluated and or optimized 5 conclusions the paper presents a development of a statistical hydrodynamic modeling toolbox and proposes a methodology for assessing the combined effects of multiple source flooding in urban areas the proposed methodological framework is an 8 step process that combines multivariate statistical analysis and dependence analysis with hydrodynamic modelling the method involves individual and combined extreme value analysis assessment of dependencies and interactions between flood drivers multivariate joint probability determination accounting for dependencies and high resolution hydrodynamic modelling of flood scenarios derived from the multivariate statistical analysis the probability and severity of individual drivers and their compounding effects on flooding are investigated by analysing the pattern of flood wave propagation inundation depths and overall extent of inundation the methodology was applied to a case study site cork city ireland for which the 2d urban flood model msn flood was applied while a successful attempt to link statistical and hydrodynamic model exist moftakhari et al 2017 moftakhari et al 2019 the methodology for combining statistical hydrodynamic modelling developed in this research is heavily focused on the hydrodynamic component and proves a necessity for quantification of impacts along the entire rp probability curve and across various flood scenarios this is the single most important finding from this research other main conclusions regarding the methodology are in the absence of long term records a statistical analysis of water level extremes provided reliable long term estimates of flood driving conditions the gev model exhibited a good fit to data interactions and dependencies between tides surges and river flows affect flood severity when they occur jointly and therefore need to be included in the analysis of extreme water levels the bivariate and trivariate joint probability and scenarios provide plausible results for joint exceedance return levels of water elevations and account for compound effects the multivariate methodology approach is superior to the traditional univariate assessment methods based on synthetic water profiles derived from combined marginal scenarios multivariate analysis allows also considering multiple combinations of joint probability solutions along a rp iso curve while the rps iso curve represents the joint events of the same exceedance probability the physical hydrological impact of such events can be very different leading to substantially different characteristics of flooding for these reasons combining the statistical and hydrodynamic modelling is very important a high resolution urban flood model forced with joint probability boundary conditions can be used to draw inferences about flood mechanisms and extents in urban environments the msn flood model used here proved very robust and ran stably under quite severe inflow conditions such as those designated by 1 in 500 year river flow and 1 in 200 year sea water level flood mapping requires only a limited number of hydrodynamic simulations derived from a multivariate analysis without a need for large ensemble simulations for a combination of extreme univariate conditions as such the proposed methodology provides a cost effective practical approach for delineating compound flood hazards driven by complex multivariate mechanisms the methodology proposed in this paper provides a robust framework for mapping coastal flood hazards in tidally active river channels that takes advantage of recent advances in multivariate statistical modelling and hydrodynamic coastal flood modelling the method also facilitates better understanding of the conditions that control flooding under multivariate sources tide surge and fluvial and allows an estimation of the compounding effects of multiple flood drivers estimating the potential risk of flooding and understanding flood controlling conditions is extremely important for flood risk assessment so that the flood prevention or alleviation schemes can be designed evaluated and or optimized this is a critical task given a typical design life of coastal defence structures of 50 to 100 years the information derived from the application of this methodology can provide a significant support for long term planning investment decisions on flood defence infrastructure and quantification of probable flood damage of economic social and environmental natures declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors would like to thank opw ireland for hydrological data this research has been funded by the environmental protection agency ireland under the epa research programme 2021 2030 project code 2021 ccen ct7 
2021,this paper presents a robust cost effective framework for assessment of coastal fluvial flooding due to compound action of multivariate dependent drivers the methodology is an 8 step process that links statistical and hydrodynamic models to determine probabilities of multiple driver flood events and associated hazards the method involves individual and combined extreme value analysis assessment of dependencies and interactions between flood drivers multivariate joint probability determination accounting for dependencies high resolution hydrodynamic modelling of flood scenarios derived from multivariate statistical analysis and ultimately mapping of inundation using cork city on the south coast of ireland as a study case the research shows that the interactions and dependencies between tides surges and river flows affect flood severity when they occur jointly tide surge interactions have a damping effect on the total water level while dependence between the surge residual and river flow amplifies the risk of flooding the multivariate joint exceedance probability occurrence of high discharges and water levels represents a more realistic representation of the spatially variable water surface profiles then the combined univariate marginal scenarios multivariate analysis allows also considering multiple combinations of joint probability solutions along rp iso curves the results show that the quantification of compound flood impacts must be performed along the entire rp probability curve this is because the physical hydrological impacts of multiple driver same rp flood events can be very different leading to substantially different characteristics of flooding the multi scale nested flood model msn flood was used to simulate flood wave propagation over urban floodplains for an ensemble of statistically derived flood scenarios the hydrodynamic runs provide inundation maps that can be used to draw inferences about flood mechanisms and impacts keywords flood modeling compound events coastal fluvial flooding interactions dependence joint probability data availability data will be made available on request 1 introduction coastal conurbations are at risk of flooding caused by a combination of astronomical meteorological hydrological and climatic factors gallien et al 2011 in the future urban flood probability and risk will increase as a consequence of several factors including population growth in flood prone areas climate change and decaying or poorly engineered flood control infrastructure kirkpatrick and olbert 2020 gallegos et al 2009 bevacqua et al 2019 many large population centres are located along the coastline and many of these are along estuaries where freshwater flows merge with tidally driven sea water orton et al 2012 these centres lie most commonly in intertidal zones where water levels are directly affected by the upstream flow and the downstream coastal conditions naturally such coastal zones can be vulnerable to flood events from a single source or several sources acting in combination archetti et al 2011 that leads to compound flooding leonard et al 2014 moftakhari et al 2017 zscheischler et al 2018 the coastal tidal wave composed of astronomical tide mean sea level and non tidal residual constituents storm surges inter annual variability baroclinic processes may propagate up the river channel network and cause a flood far from the coast hendry et al 2019 variable coastal water levels change both the river stage and discharge and form the downstream river boundary of unsteady and non uniform flow these water levels may impede river drainage to an estuary by a backwater effect as their upstream propagation may reverse the seaward flow in a river ganguli and merz 2019 hoitink and jay 2016 on the other hand a river discharge may raise mean coastal water levels and generate friction that makes tides lose energy and shrink in amplitude piecuch et al 2018 moftakhari et al 2016 in summary river coastal interactions can contribute to subtidal friction modulate tidal amplitudes and impede flows impacting river discharge downstream sassi and hoitink 2013 the dependencies between non tidal residuals of coastal water levels and fluvial peak discharge may be significant as both events may result from a common meteorological cause severe storm periods are often associated with high winds and low pressure systems that generate storm surges while at the same time causing orographically enhanced high precipitation on coastal catchments resulting in high peak river discharges kew et al 2013 such simultaneous or successive occurrence of high coastal and river water levels may produce extreme impacts even when hazards from individual drivers in isolation would be unlikely bevacqua et al 2017 commonly coastal flood hazard assessments rely on univariate statistical modelling methods where river discharges and extreme sea levels are considered separately ganguli and merz 2019 such approaches assume stationary and unconditional distribution of flood signals and therefore may not correctly estimate the probability of a given hydrologic event salvadori and de michele 2004 moftakhari et al 2017 since coastal cities are at risk of compound flooding effects from multiple drivers univariate approaches should not be used to characterize the flood hazard moftakhari et al 2017 more recently the influence of compound events on flood hazards was studied using physically based and stochastic models van den hurk et al 2015 most efforts on compound flooding to date are based on a joint probability bivariate flood hazard assessment that accounts for compound flooding from river flow and coastal water level some of these methods account for dependencies among multiple drivers hendry et al 2019 most recently the copula based models have been used to analyse the joint frequency of compound floods bevacqua et al 2017 2020 yazdandoost et al 2020 zhong et al 2021 moradian et al 2023 these simulations however do not take account of the likelihood and intensity of fluvial floods conditional on coastal water levels or the severity of coastal water levels conditional on river discharges ganguli and merz 2019 and therefore do not explore all possible flood scenarios regardless of the compound event approach employed the aforementioned statistical methods do not provide important information on compounding effects of spatially distributed interactions between the river discharge and downstream ocean level in tidal channels and estuaries most commonly the bivariate models of coastal fluvial flooding use time series records of river discharge at the reach entry and water level measurements at the downstream end of the reach in a long complex estuary these data could be representative of two different hydrodynamic regimes moftakhari et al 2019 this issue can be addressed by linking the statistical and hydraulic models the two model approach allows one to hydrodynamically determine mechanistic routing of flood water onto urban floodplains under statistically derived exceedance probabilities ultimately this information can be used to assess flood hazards e g depth and velocity and understand the impact of such flood events gallien et al 2011 van den hurk et al 2023 gallien et al 2018 uddin et al 2022 while statistical models have been widely used their linkage to hydraulic models is not so common although simplistic bathtub models have been widely applied for estimating coastal flooding hazards they may potentially lead to underestimation of flood consequences due to their inability to vary flood stage with distance inland due to river tidal interactions lanzoni and seminara 1998 and variable non linear flood dynamics gallien et al 2014 sanders 2017 in a broader sense mapping flood hazard in complex estuaries using approaches that ignore local hydrodynamics can underestimate flood extent and depth fema 2015 while hydrodynamic modelling of rapid flood events in urban environments is a very complex and challenging task a number of successful investigations into combined coastal fluvial flooding has been demonstrated in recent years e g yang et al 2012 comer et al 2017 moftakhari et al 2017 olbert et al 2017 gallien et al 2018 griffiths et al 2019 however none of these studies links hydrodynamic and statistical models in a way that considers flood impacts for a spectrum of statistical conditions of a certain return period as such answering the following fundamental questions how severe these events can be or what combination of extreme signals can result in the most hazardous events is impossible importantly many different compound events have the same return period despite its iso curve representing different combinations of river flow tide and surge signals now these different events result in the specification of a range of flow tide and surge boundary conditions in the model having the same return period each different set of boundary conditions can give rise to spatially different flooding conditions throughout the model domain for one given return period in this complex problem model linking is possibly the only way to assesses the compound nature of coastal sea levels and river discharges and specifically to quantify severity of compound floods for a combination of extreme signals and by that to facilitate a comprehensive management of flood hazards luke et al 2018 it is quite clear that a robust and integral assessment of compound flood hazards should leverage the intrinsic characteristics of both modelling approaches into a coupled approach muñoz et al 2020 in this context the objective of this research is to develop a statistical hydrodynamic modeling toolbox and to provide a methodology for assessing the combined effects of multiple source flooding in urban areas the toolbox comprises 1 statistical models of frequency analysis extreme value of storm surges tides and river flows and ultimately the joint probability models for calculating joint exceedance return periods and 2 a hydrodynamic numerical model as the central engine where flood inundation under numerous coastal and fluvial flood scenarios derived from the statistical model are simulated such a statistical hydrodynamic modeling system allows to model coastal fluvial floods across the joint probability spectrum for a given return period event and this is a very important consideration for flood management modelling within this complex system thus allows to forensically investigate all possible impacts of a flood hazard and clearly disentangle between coastal and fluvial effects and by that provide a useful information for flood management engineers and policy makers to the authors knowledge the across jp spectrum modelling and hazard quantification have not been presented in a detail yet this new statistical hydrodynamic system is very significant for flood management as illustrated by the following 2 points i it is important to realise in a coastal flooding context that modelling the flood extent of one set of boundary conditions jointly having a particular return period say 50 years will not provide an answer to the question what are the flood extents having a return period of 50 years ii as in the system application described in section 3 below when a river boundary is located upstream of the modelled domain and the tide surge boundary is at the opposite end of the domain a joint event having a return period of say 50 years may give significantly different inundated depths and extents if we combine a relatively low river flow with a high combination of tide and surge than if we combine a relatively high river flow with a low combination of tide and surge thus to obtain the full extent and depth of inundation having a return period of 50 years a series of joint events must be simulated and the results processed to identify the particular compound event causing maximum flooding in all grid cells of the modelled domain the need and usefulness of the new toolbox has been demonstrated in this research for the case of the river lee in cork city this is located on the south coast of ireland and it regularly experiences compound coastal and fluvial flooding the impact of flooding is quantified based on a range of physical aspects such as coastal fluvial flood inundation extent and water depth the toolbox has been developed in a general purpose approach and so can easily be applied to other case studies the paper is structured as follows section 2 details the methodology where the numerical models and their setups are described along with the statistical approaches to extreme value analysis and determination of joint probabilities section 3 compares numerical model results for various flood scenarios and mechanisms it also presents extreme value analysis results which provide the boundary conditions and scenarios for the numerical models finally section 4 presents conclusions from the research and discusses the usefulness of the methodology and the implications of the results 2 methods in this section the methodology for assessment of flood probability and severity is briefly described followed by the review of statistical approaches used for extreme value and joint probability jp analyses and the descriptions of the flood modelling system used for cork city 2 1 flood assessment methodology in the absence of a systematic approach to statistical hydrodynamic modelling of floods an 8 step methodology for urban flood assessment is developed in this paper and presented on fig 1 as shown in this flowchart for compound flood events the analysis requires statistical and hydraulic modelling used jointly in a multi step process each driver must first be statistically analysed individually in order to estimate its independent frequency of occurrence following which jps can be estimated and flood events modelled a number of flood scenarios s for a range of extreme conditions must be developed from extreme value analysis to establish univariate and multivariate boundary conditions for the numerical model three scenarios are considered in this research s1 a marginal river discharge scenario of fluvial flood only t year rp discharge and non extreme coastal water level s2 a marginal coastal water level scenario of coastal flood only t year rp sea water level and non extreme river discharge s3 an and scenario of joint probability occurrence of high discharges and water levels of t year rp the hydrodynamic model is forced with a range of boundary conditions that represent a combination of flood drivers for conditional s1 s2 and joint probability s3 scenarios and various t year rps a synthesis of hydrodynamic modelling results is used to map inundation levels under various extreme conditions and to quantify physical compounding effects 2 2 statistical modelling in the first stage of statistical modelling astronomical tides surge residuals and river flows are subject to univariate frequency analysis and modelling of their extremes in the second stage a multivariate joint probability of their occurrence is estimated this analysis takes into account the potential multivariate dependencies between drivers the following sections briefly outline the methods used to calculate extreme values multi driver dependencies and joint probabilities of their co occurrence outputs from these analyses will be subsequently used to investigate flood hazards due to extreme events 2 2 1 extreme value analysis univariate frequency analysis is used to determine extreme values of individual flood drivers probabilistic models from the field of extreme value statistics are usually adequate to quantify variables in terms of their extreme values and associated return periods moftakhari et al 2017 many studies including lowe et al 2001 butler et al 2007 olbert et al 2013 and sun et al 2017 demonstrate their suitability to the analysis of tides surges and or river flows the probability that a water level z will be exceeded by any water level in any given time period is termed the probability of exceedance of the water level z p z and is mathematically expressed as 1 p z 1 f z z f x d x where f z nd f x re the cumulative distribution function cdf and probability density function pdf respectively the return period rp time t represents the average time between consecutive occurrences of water levels equal to or greater than the given level z 2 t 1 p z 1 1 f z a generalized extreme value model gev coles 2001 was used to estimate extreme values of tides surges and river flows and their rps the cdf of the gev is expressed as 3 f z exp 1 ξ z μ σ 1 ξ where μ is a location parameter σ a scale parameter and ξ a shape parameter the value ξ determines the asymptotic extreme value distribution and hence the type of distribution ξ 0 epresents a gumbel distribution ξ 0 frechet distribution and ξ 0 weibull distribution 2 2 2 multivariate dependencies in order to accurately calculate the joint probability of extreme events dependent interactions between any of the variables must be first assessed the method utilised here is the χ dependency measure based on tail dependence coles et al 1999 4 χ u 2 ln p u u v u ln p u u f o r 0 u 1 where u is the upper threshold of the uniform distribution u and v are transformed variables with uniform margins 0 1 such that 5 p u u v u number o f x y p a i r s s u c h t h a t x x a n d y y total n u m b e r o f x y and 6 p u u 1 2 number o f x x total n u m b e r o f x number o f y y total n u m b e r o f y where x y s an observational pair from the original data series while x y s the threshold level for the observed series of the same probability of exceedance p coles et al 1999 suggest interpretation of total dependence as χ 1 nd total independence as χ 0 partial dependence for example χ 0 1 means that there is a 10 risk of the two variables exceeding their threshold at the same time with the threshold for each variable corresponding to the same probability 2 2 3 joint probability jp can refer to the exceedance of river discharge and coastal water level referred to here as and scenarios or the exceedance of river discharge or coastal water level referred to here as or scenarios there is a wide range of parametric and non parametric methods developed for bivariate frequency analysis the copulas functions sklar 1959 are used to formulate joint distributions of bivariate pairs moftakhari et al 2019 bevacqua et al 2017 sadegh et al 2018 use a bayesian framework with copula functions and dependence structure in this study where a dependence between two variables exists the jp rp of an event t x y where both variables exceed their thresholds is represented as 7 t x y 1 1 1 t x t y 2 χ u 2 t x t y 1 in cases where there is no dependency between two components a method developed by pugh and vassie 1980 and revised by tawn 1992 is proposed for a given water level z if the pdf of one variable is f x η s nd the pdf of another variable is f y s the pdf of the total water level f η s a combined probability of two variables occurring simultaneously is 8 f η f x η s f y s d s the joint probability rp t x y of an event where both variables occur simultaneously can be expressed as 9 t x y 1 1 f z where f z s the joint cdf for a given extreme sea level z efined as 10 f z z f η d η in the rjpm method given in equation 8 the normalized pdfs of two variables i e tides and surges are then used to construct a joint probability matrix in this matrix each diagonal represents a probability of joint occurrence of two variables due to a certain combination of variable one and variable two the sum of each diagonal representing a certain joint value i e water level due to tide and surge gives the probability of occurrence of that joint variable the example of construction of joint probability matrix is presented in pugh and vassie 1980 ultimately probabilities are converted to rps using equation 9 as explained in tawn 1992 the outputs from the univariate and multivariate statistical analyses of river discharges and coastal water level pairs are used to construct flood hazard scenarios for hydrodynamic runs 2 3 hydrodynamic modelling in this research modelling of coastal fluvial flood inundation extent and water depth was carried out using a state of the art multi scale nested msn flood hydrodynamic flood model msn flood is a two dimensional depth averaged finite difference model that solves the depth integrated navier stokes equations and includes effects of local and advective accelerations earth rotation barotropic and free surface pressure gradients wind action bed resistance and prandtl mixing length turbulence scheme the nesting structure of the model comprises a two level cascade of dynamically linked nested grids at 6 and 2 m resolutions a 6 m parent grid model provides water level conditions from the greater cork region to a 2 m ultra high resolution nest which covers the downstream section of the river lee channel and cork city fig 2 a novelty of the model is its unique nesting scheme which utilizes a sophisticated approach to nested boundary formulations to allow the location of nested boundaries in the flooding and drying zones this means that large sections of the boundary may alternatively flood and dry extensive model validation at each of the nested levels can be found in nash and hartnett 2010 and comer et al 2017 while details of model parameterization sensitivity and comprehensive validation of the 2 m model can be found in olbert et al 2017 since the same model configuration is used in this research readers are referred to olbert et al 2017 for the model description and performance assessment 3 results in this section the outputs from statistical hydrodynamic modelling are presented statistical modelling involves the analyses of frequency extreme value of storm surges tides and river flows and jps of exceedance the jp values are used to construct the boundary forcing conditions for the hydrodynamic model which is used to simulate coastal fluvial floods for a range of extreme events and across the joint probability spectrum iso curves for each of the rp event an ultimate set of results are the maps of inundation extents and water depths for a combination of river flows and sea levels of given rp 3 1 univariate analysis of individual flood drivers in the proposed methodology the univariate statistical analysis of extremes step 3 is performed to establish extreme conditions of individual drivers for marginal scenarios s1 s2 and joint probabilities of multiple drivers for multivariate flood scenarios s3 statistical analysis of extreme values of river lee discharges and coastal water levels decomposed to tides and surge residuals in cork estuary are based on records of peak over threshold pot data the extreme value analysis of surge residuals was conducted on a dataset of surge values obtained from surge simulations of 48 storm events over the 46 year period 1959 2005 only the maximum surge value from each of the simulated events was extracted to guarantee independent events this yielded a dataset of 48 maximum values the highest of which was 0 81 m astronomical tides extracted from a 46 year time series for tivoli cork noaa 1982 located on the eastern boundary of the 6 m model domain were used in the frequency analysis the accuracy of this dataset was corroborated by comparison against a harmonic dataset constructed from existing records for tivoli tidal gauge station and the nautical almanac hewitt and lees spalding 1982 the 38 largest river discharges were identified from a 5 9 year long timeseries of river gauge records on the river lee gauge number 19012 the frequency analysis was used to estimate the probability of occurrence of any of the three flood components considered here for the exceedances to be considered extreme high threshold values were set in the pot analysis the pot levels for surge and river flow were 0 4 m and 70 m3 s respectively a gev statistical distribution was fitted to the pot data the uncertainty associated with the selection of a probability density function was significantly reduced by ignoring the lower bound values the computed rps are presented in table 1 and fig 3 although the fitted distributions provide frequencies of occurrence of up to 1000 year rp only the values of the 50 and 200 year rps were considered in the analysis davie 2008 recommends that the extrapolated rps should not exceed twice the length of the dataset while hall et al 2006 found that analysis over a timescale of 30 100 years introduces uncertainties the method may introduce a degree of model uncertainty for some variables e g sea level as a result of interferences produced by climatic signals such as climate modes or climate change 3 2 extreme water level analysis in assessing the potential flood risk of a coastal region like cork city the likelihood of occurrence of joint extremes of tide surge and river discharges is important although the method for the determination of extreme water levels due to the independent actions of tides surges and river flows gives reasonably good estimates of flood risk they are known to be inaccurate but no universally accepted approach exists for determination of the water level due to the combined effect of all three signals haigh et al 2010a the treatment of potential dependencies between variables in a multivariate problem is a main reason for the difficulty there is a good body of evidence showing that tides and surges interact e g olbert et al 2013 idier et al 2012 and river flows and surges can be dependent when driven by the same meteorological conditions orton et al 2012 the analysis of dependency between tides surges and river flows is the second step of the methodology and is used in conjunction with the univariate extreme analysis step 3 to estimate joint probabilities of occurrence of multiple flood drivers step 4 3 2 1 tide surge interactions extreme sea water levels result from a combination of astronomical tides and non tidal processes such as storm driven surges wind waves and baroclinic flows these flood drivers may interact and exhibit dependencies olbert et al 2013 found that extreme sea levels in cork harbour are the product of a moderate to high surge coinciding with high water spring tide nonetheless the likelihood of the simultaneous occurrence of these two signals requires in depth analysis fig 4 shows the temporal variation of surges and tides and their contribution to total water levels in cork city during a 2009 flood event the characteristic pattern is that the surge tends to peak during a rising tide around half way between mid flood and high water so its temporal variation although driven by meteorological conditions seems to be modulated by tides prandle and wolf 1978 and horsburgh and wilson 2007 describe this mechanism of tide surge wave modulation in detail this pattern is not an isolated incident it occurred in over 50 of the 48 surge events investigated fig 5 shows surges classified into one of eight groups each representing a particular phase of tide while there is no clear relationship between the surge magnitude and phase of tide the distribution of surge peak occurrence over a tidal cycle indicates the presence of non linear interactions haigh et al 2010b olbert et al 2013 analysed this type of interaction in irish coastal waters and found that the level of interaction varies geographically a χ2 statistical model was used to quantify the level of dependence at the 95 significance level and n 1 degrees of freedom χ 7 0 95 2 14 07 in the above research χ2 12 0 was found for cork this implies a low degree of interaction between surge and phase of tide despite the fact that surges tend to peak on rising tide more frequently than on other phases fig 5 interestingly no dependence between surge peak and tidal height was found and surge heights do not vary across tidal phases as tidal currents are stronger than surge currents in cork tides modulate the interaction in a non linear manner the quadratic bottom friction being here the principal cause of tide surge interactions in this region olbert and hartnett 2010 attenuates and smooths the amplitude of the surge as explained in dinápoli et al 2020 taking a conservative solution an assumption of tide surge independence was used in the joint probability calculations 3 2 2 surge river flow dependence recent studies clearly show that dependence may exist between river discharge and either coastal water level or storm surges and not accounting for dependencies in joint probability analysis may underestimate the compounding effect de michele et al 2005 ward et al 2018 in coastal sites the dependence between river flow and total sea water level or surge often results from a common meteorological cause kew et al 2013 so they may occur simultaneously the time lag between river flow and total water level is another aspect to consider in the dependence analysis the time lagged analysis accounts for the fact that the storm surge peak may arrive at a different time than the river flow peak despite both being generated by the same storm event fig 6 presents a scatter plot of daily mean river levels and daily maximum surge residuals the dependence measure χ equation 4 between river gauge data and storm surge residuals was calculated for pairs of daily mean river level and daily maximum surge residual the thresholds for surge residual and river flows converted to water levels are 0 488 m and 1 8 m respectively the dependence analysis was carried out for two scenarios a maximum values occurring on the same day or b time lagged scenarios the lagged analysis was to determine whether or not a significant time lag existed between the response of the two processes results from these analyses are shown in fig 7 for the same day occurrence scenario the dependence measure χ between surge residual and river flow of 0 101 is considered to be significant at the 5 significance level χ 0 05 0 06 the 5 and 95 confidence intervals of χ dependence are 0 012 and 0 207 respectively for the time lagged analysis the daily maximum surge residuals were selected corresponding to daily maximum discharge values with time lags of 3 to 3 days the χ values calculated when surge precedes river level are greater than the values calculated for the opposite scenario or the same day dependence when surge precedes river level by one day the highest dependence of 0 131 is exhibited this 1 day lag dependence between high river discharges and coastal water levels is in line with observations from the west south of the uk hendry et al 2019 where meteorological conditions are often part of the same large scale weather systems common to the uk and ireland moreover according to hendry et al 2019 this phenomenon is characteristic of relatively small catchments with a low baseflow such as river lee is 1 253 km2 40 4 m3 s since the high river discharges occur on the day of or just after peak surge compound flooding is still a concern the flood modelling results of olbert et al 2015 clearly show that the time between river and tide peaks in cork is too short for initial floodwaters to recede as the most conservative approach the 1 day dependence level was therefore used to derive joint exceedance probability of high river flows and surge residuals 3 2 3 multivariate joint probability in step 4 of the methodology the multivariate join probabilities of compound floods are estimated a number of jp methods were considered described in section 2 however the methodology adopted in this study being a combination of bivariate and trivariate analyses is best suited for multivariate problems with various levels of dependencies between variables accurate assessment requires the dependencies if they exist to be included to account for lower marginal exceedance probabilities the jp analysis is a multistep process illustrated in fig 3 the first step is the bivariate jp analysis of the extreme sea levels the extreme sea levels are calculated using the revised jp method rjpm of tawn 1992 given in equation 8 in this method assuming tide surge independence tides and surges are firstly independently modelled using the statistical gev model and their normalized pdfs are then used to construct a jp matrix the sum of each diagonal in the matrix being a probability of a water level due to a certain combination of tide and surge and representing a certain water level gives the probability of occurrence of that water level the extreme water levels due to the combined action of tide and surge calculated using the rjpm are presented in table 1 and fig 3 c for the 50 and 200 year rps the coastal water levels are 5 39mod and 5 49mod above msl respectively for a complete set of conditions contributing to flooding high river flows fig 3b and their interactions with surges are accounted for using trivariate joint probability the trivariate joint rp is calculated using equation 7 for a combination of the selected rps of water levels from bivariate joint probability of occurrence of tides and surges and rps of river flows dependence between two variables is quantified through χ equation 4 the 50 and 200 year iso curves of joint exceedance rps for a combination of river flows and sea levels calculated for the and hazard scenario are presented in fig 3 e while the rps iso curves represent the joint events of the same exceedance probability the physical hydrological impact of such events can be very different leading to substantially different characteristics of flooding flood inundation maps for these iso curves are generated in step 6 using a hydrodynamic model 3 3 flood hazard modelling the statistical hydrodynamic methodology proposed here is possibly the only way to comprehensively assesses the compound nature of coastal sea levels and river discharges the high resolution numerical model of cork city covers the downstream reach of the river lee and the adjoining floodplains of cork city centre the upstream boundary was prescribed as the river discharge while the downstream boundary was forced with a variable water elevation to simulate the tidal signal and non tidal residual the boundaries were placed far enough apart to let compounding effects develop within the domain the boundary conditions were generated from the univariate and bivariate analyses for the s1 s3 flood scenarios defined in section 2 1 the hydrodynamic runs for each scenario were used to generate maps of extreme water levels for the selected rps this is a simple and efficient assessment approach that requires only a small computational effort while delivering comprehensive quantification of flood hazards across a spectrum of conditions including compounding events each map presents the maximum flood extent and the maximum flood water depth on floodplains based on the maximum water level recorded over an unsteady simulation covering the rise and fall of a flow peak with the rise and fall of a coastal water level 3 3 1 coastal flooding the hydrodynamic simulations of coastal flood only represent the hazards associated with a single driver marginal scenario of a flood event driven by an extreme sea water level fig 8 a presents the maximum water depths simulated for the marginal scenario of 200 year rp sea levels and average river discharge where the extreme sea levels were derived from a joint probability analysis of the independent occurrence of tides and surges this scenario only results in a small number of very localised floods along the river channel that do not constitute a major flood threat to the city which indicates that the cork city flood defence systems are well able to protect against the 200 year coastal flood indeed the existing coastal defence structures along urbanized areas of cork harbour coastline had been designed to prevent spring tides from causing flooding and therefore also they may protect against the high surges that peak on low to moderate tides by comparison fig 8b presents the maximum water depths simulated for the more unlikely scenario of 50 year rp tide with 200 year rp surge peaking at high water and average river discharge this scenario not only produces widespread flooding along the riverbanks but also in the business commercial downtown city area the timing of the surge peak plays a crucial role in flooding extents as demonstrated in section 3 2 1 the acceleration of a surge wave when travelling along with the tidal wave in cork harbour prevents the surge from peaking on a high tide this interaction is attributed to a shallow water effect idier et al 2012 and significantly alleviates flooding comparing the flooded area from a simulation where the surge peaks on the flood tide fig 8c to that in fig 8b where it peaks at high water there is a 30 reduction in the inundation area 3 3 2 fluvial flooding the univariate flood hazard assessment for the marginal river flow scenario was conducted by running an ensemble of river discharge simulations conditional on the mean high water level on the eastern tidal boundary representing average spring tide conditions with no surge the fluvial flood events in cork city occur when the run off exceeds the conveyance capacity of the river lee channel and spills into the street network fig 9 shows maximum water depths due to the river flood wave propagating through the city floodplains for 50 and 200 year rp river flows once the conveyance capacity of the river channel is exceeded the flow starts to spill into floodplains at numerous locations along the riverbank for both 50 and 200 year rps flooding progresses very rapidly to reach maximum extents approximately 10 h later during this period the flood wave propagates along preferential flow paths in the main west east direction through recreational fields along the river and major streets before reaching the ponding areas of the low lying downtown city streets while the downtown zone is only marginally flooded at rp 50 fig 9a the rp 200 peak results in a significant portion of the city centre being submerged fig 9b 3 3 3 marginal scenario flooding comparing the pattern of flood water distributions for marginal coastal and fluvial scenarios some distinctive differences in flooding characteristics emerge the results show that there is a shift in the flood hazard patterns along the length of the river reach depending on the dominant mechanism controlling the flood the coastal water levels control the outlet with floods mostly affecting the downtown area while the river discharges control flood hazards further inland the fluvial floods dominate in the upstream city suburbs along the north channel corridor before spreading downtown during larger flood events 50 year rp the length of riverine control is much longer than the tidal length which spans a relatively short downstream reach of the tidally active river lee as such the pattern of flood defence systems overtopping is remarkably different for both mechanisms during coastal flooding the major inundation stems from south channel spillage while fluvial flood waters enter floodplains primarily from the north channel with limited overtopping of flood defences along the south channel in addition the extent of flooding for the 200 year rp is substantially greater for the fluvial scenario the composite maximum water depths due to marginal coastal and fluvial floods generated by superimposing marginal fluvial over marginal coastal flood water depths are shown in fig 10 these results do not account for interactions and joint occurrence of fluvial coastal drivers fig 10b presents a synthetic map of 1 in 200 year fluvial flood and a highly unlikely low probability coastal flood of rp200 surge coinciding with a rp50 tide 3 3 4 combined coastal and fluvial flooding the final step of the methodology is to quantify the flood hazard due to combined action of tides surges and river flows and the interactions between them in the tidally active reach of river lee the raising sea level pushes ocean tides upstream as the tidal signal propagates inland from the estuary the aggravated interaction between the river and coastal signals causes the backwater river profile to develop an elevated water overtops the riverbanks and spills into urban floodplains the hydrodynamic model was forced with iso probability pairs of river discharge and sea water levels fig 3e generated for 50 and 200 year rp events considering dependence between the drivers the iso probability curves were composed of the joint exceedance return values calculated using the trivariate statistical analysis of univariate river discharges with bivariate sea levels the curves present combinations of sea levels and river flows ranging from extreme sea levels and moderate flows on one end to extreme river flows and moderate sea levels at the other end this type of combined analysis allows one to draw inferences about compound flood hazards using only a limited number of simulations while the flood probabilities are equal along a joint probability curve the severity and therefore risk may be different along the curve this is because the joint probability curves are constructed from a combination of flood drivers of various probabilities i e low probability discharge and high probability sea level or high probability discharge and low probability sea level the risk and hence impact of each driver is different and varies spatially the flood severity for a flood event of a given rp depends on flood defence systems design and stability and hydraulic properties of the floodplains such as topography bed slope and roughness the case study of cork clearly illustrates that the flood risk is a function of probability and severity fig 11 shows maps of maximum inundation for a combination of flood drivers contributing to the 200 year joint exceedance event assuming dependence between drivers the total area inundated by each of these events is summarized in table 2 as can be inferred from these plots and table the most severe floods are associated with events characterized by high river discharges and therefore driven primarily by the fluvial mechanism however sea water levels due to tides and surges pose an elevated risk of flooding the fluvial flooding is initiated from the upstream area of the north channel of the river lee after overtopping the defences flood waters spill away from the river channel travelling eastward across the central part of the city and flowing downhill towards the most eastward part of the downtown area in contrast when the flood is controlled by the coastal mechanism the major inundation originates from the south channel and propagates northward and southward the overall coastal inundation is relatively small which suggests that the flood defence systems in the tidal section of the river are capable of protecting against 200 year coastal driven floods for the worst case scenario fig 11f the 200 year rp flood is severe and results in 75 9 ha of urban inundation this is due to a combination of high river flow 647 1 m3 s corresponding to a 343 year rp event and moderate sea water levels 4 98 mod representing a 2 year rp event the level of dependence between flood drivers is another aspect to be considered in the joint probability analysis as shown in fig 3e the magnitudes of flood drivers for a given joint occurrence probability vary depending on the level of dependence between the drivers the higher the dependence the larger the joint exceedance values for a given probability of occurrence and consequently the larger the flood extents fig 12 summarizes the total area of inundation due to 200 year rp joint probability coastal and fluvial flooding for the three dependence scenarios only the worst case fluvial dominated scenarios are presented as expected the most severe inundation results from a compounding event of high dependence between flood drivers such as high discharges prescribed at upstream boundary coinciding with medium to high sea water levels at downstream boundary additionally the co occurrence generates interactions between river discharges and the upstream propagating tidal wave the unsteady non uniform upstream wave impedes the river flow slows down the rate of draining to the estuary and generates a backwater profile upstream with elevated water levels in the intertidal reach the interaction also can modulate the amplitude and shape of the tidal wave itself the effect of driver interactions was investigated by comparing flood hazards due to the joint probability and scenario fig 11 and the combined marginal scenarios fig 10 the difference in the prediction of flood water depths and extents between simulations is clear in the tidal reaches of the river where water levels are subject and sensitive to both riverine discharge and sea levels in the floodplains of the intertidal zone the water depths and velocity magnitudes are generally higher for the and scenario then for the two combined marginal predictions interestingly this happens despite much lower water levels at the upstream and downstream boundaries of the and scenario i e fig 11e 568 m3 s 5 18 mod when compared to the two marginal scenarios combined fig 10a 600 m3 s 5 49 mod this is due to physical compounding effects and nonlinear interactions between the discharge and water level described by shallow water wave theory for the and scenario with no dependencies fig 12 with χ 0 0 the flood extent is very similar to that of the combined marginal scenario 600 m3 s 5 49 mod despite significantly lower boundary values of the and scenario 483 m3 s 4 98 mod which clearly implies the amplifying effect of the interactions in contrast the and scenario predicts lower flood hazard levels in the non tidal zone upstream river reach compared to the marginal profiles not shown here because the river discharges contributing to the joint probability event are lower than those contributing to the riverine flooding only 4 discussion flood risk is a function of the probability of flooding and the consequential damage integrated over all possible flood events hall et al 2006 both variables in this function are subject to large uncertainties this paper presents a robust methodology for assessment of a hazard associated with a compound flood which can be further used to draw inferences about risks associated with such complex events if there is only one driver responsible for flooding the accuracy of flood probability depends on the data availability and statistical methods used in assessment of extremes in cork when flooding is primarily driven by one driver only fluvial or coastal the univariate gev analysis of long timeseries of river flows or sea water levels provide a relatively accurate assessment of return levels for upper tail low probability events however when flooding is compound and driven by both drivers in addition to the probabilities of individual drivers the joint probability of simultaneous occurrence of the two drivers must also be considered data for cork imply that majority of flood events are compound but the contribution of each driver to the flood event may vary from one event to another complexity is further exacerbated by the presence of interactions tide and surge or dependencies between drivers river discharge and surge as such cork city is a good example to demonstrate that to fully understand the complexity of multi driver flood dynamics and the overall impact of a flood hazard the drivers need to be assessed in an integrated manner using the modelling methods the methodology proposed here is an eight step process that combines statistical and hydrodynamic modelling 4 1 data collection and analysis the first step concerns a collection of data records of flood drivers from which the multivariate dependencies step 2 and univariate extreme distributions step 3 can be generated in general the longer the record the more hydrologically meaningful the estimate of low frequency peaks to fulfil this requirement where availability of long term gauge data is problematic as in many coastal areas a partial duration series or model simulations may be used instead this hybrid approach was used for cork case study where the limited length tidal gauge records for surge residual univariate analysis were complemented by the hindcast model data obtained from hydrodynamic model runs for known past high surge events in the methodology proposed here the upper tail datasets of tides surge residuals and rivers discharges are used to inform the multivariate joint probability analysis and hydrodynamic modelling 4 2 multivariate dependence analysis in order to accurately calculate the joint probability of extreme events step 4 dependent interactions step 2 between any of the variables must be first assessed recent studies clearly show that dependence may exist between river discharge and either coastal water level or storm surges but very few of them provide return levels of compounding events moreover only a small number of studies quantify a flood risk due to compounding effects of extreme coastal fluvial water levels de michele et al 2005 found that ignoring dependencies may result in over or underestimation of flood risk in coastal sites where storm surges are small relative to astronomical tides the correlation between total water level and river discharge is not statistically significant moftakhari et al 2019 however correlation may be found between river discharge and the non tidal residual such as surge considering that high river flows and surges can be generated by the same mechanism such as a low pressure weather system the likelihood of both events occurring simultaneously can be high khanal et al 2019 hence the second step of the methodology concerns the multivariate dependence analysis and an assessment of correlation structure for a pair combination of flood drivers there is a range of linear and rank correlation coefficient measures that are used to quantify dependencies hawkes et al 2002 heffernan and tawn 2004 coles et al 1999 the rank based kendall s rank correlation and spearman s rank correlation are robust against outliers however correlation coefficient only detects the degree of association between two variables and does not capture the dependencies well in low probability extreme ranges ganguli and merz 2019 for this reason in this research the dependence is quantified through the χ tail dependence coles et al 2000 the results show that there is a statistically significant dependence between surge residual and river flows on the south of ireland where cork harbour is located the storms are generally generated by low pressure systems to the south west of ireland and strong south westerly winds with propagation patterns towards the north and northwest olbert and hartnett 2010 these storm track patterns may justify the dependence between the two drivers in cork according to hendry et al 2019 the dependence between high river discharges and high surges occurs at sites where storms that generate these events are typically similar in characteristics and track across on comparable pathways the time lag between river flow and total water level is another aspect to consider in the dependence analysis ward et al 2018 showed that the lagged occurrence of river and coastal peaks can influence overall inundation extent and thus need to be considered for hazard mapping and planning of emergency responses the time lagged analysis accounts for the fact that the storm surge peak may arrive at a different time than the river flow peak despite both being generated by the same storm event in large catchments a storm approaching the coast may generate a high storm surge before travelling inland and generating high precipitation and elevated river discharge likewise a storm may travel over land before reaching the coast so the peak in river flow may precede the surge peak hendry et al 2019 imply that surges coincide with high river discharge in catchments characterised by a lower base flow smaller catchment area and steeper bed slope while the peak river flow may occur several days after the surge in large catchments with a high base flow and mild elevation gradient in case of cork the highest dependence was found when surge precedes river level by one day this time lag dependence in river lee characterised as a relatively small catchment with a low baseflow 1 253 km2 40 4 m3 s confirms findings of hendry et al 2019 these results are in line with observations from the west south of the uk where meteorological conditions are often part of the same large scale weather systems common to the uk and ireland hendry et al 2019 the surge discharge analysis shows that dependence is not only a function of geographical position of river catchments versus coastline and storm trajectory but also depends on the response time of a river as a function of catchment characteristics such as size or elevation gradients holtan and overton 1963 ward et al 2018 show that where high river discharge and high sea levels are exceeded and these drivers are dependent the joint probability of events can be several magnitudes higher compared to the independent scenario since not accounting for surge discharge dependencies in joint probability analysis may underestimate the compounding effect the need to include them is paramount in a multivariate dependency analysis another set of variables to consider are the components of total sea water level generally in sheltered coastal locations waves do not affect water levels because of a limited direct impact of winds relative to open coastlines melet et al 2018 and storm driven residuals are often weak or significantly smaller than astronomical tides so that surge impacts are only significant when they coincide with high spring tides along the european coast extreme sea levels are typically generated by moderate surges co occurring with spring astronomical high tides haigh et al 2010b this is also the case for extreme sea levels in cork harbour which are typically the product of a moderate to high surge coinciding with high water spring tide this phenomenon is due to the interaction between storm surges and tides that prevent the surges from peaking on high water in fact the maximum surges are more likely to peak on the rising tide 3 5 h before tidal high water which can amplify the surge magnitude idier et al 2012 this holds true for many coastal sites around ireland olbert et al 2013 in cork however while surges tend to peak on rising tide more frequently than on other phases these interactions are weak and statistically not significant therefore it is reasonable to assume an independence between tides and surges and take a more conservative approach in the joint probability analysis this means that extreme sea water levels may be higher as surge residuals may peak on a high water tide 4 3 univariate extreme value analysis in step 3 of the methodology the probabilistic models from the field of extreme value statistics are used to calculate extreme values of individual flood drivers such models can be used to draw inferences about extremes from relatively extreme values alone and so do not require multi year timeseries of data like analytical methods do however they are sensitive to the choice of the distribution and the fitting procedures and so involve some uncertainty around the often subjective fitting of statistical distributions woth et al 2006 for all three drivers in cork the gev model exhibits a very good fit to peak over threshold data so the extreme values and associated rps seem to be adequately quantified 4 4 multivariate joint probability in the fourth step of the methodology the univariate extreme values step 3 for three individual flood drivers are combined with dependence results step 2 to estimate joint probabilities of occurrence of multiple flood drivers salvadori et al 2016 defines joint probability as 1 the exceedance of river discharge and coastal water level or 2 the exceedance of river discharge or coastal water level while flood events can be driven by either and or or scenarios in an estuary where an upstream discharge and downstream water level co occur the and scenario of joint exceedance probability is more appropriate particularly when dependencies exist in fact moftakhari et al 2019 found that the or scenario significantly overestimates the return levels given by univariate analysis marginal scenarios and the joint probability and scenario the or scenario with unrealistically high extreme values in areas of low probability density represents the highly unlikely highly conservative approach and therefore it is not considered in the joint probability analysis of this study the multivariate joint probability of the three signals tides surges and river discharges was firstly calculated as bivariate probability of tides and surges occurring simultaneously with no dependency to obtain total sea water levels and then probabilities of sea water levels coinciding with river discharge were considered in so called trivariate joint probability the iso curves of joint probabilities of all three signals occurring simultaneously are the final statistical outcome each curve represents a combination of drivers magnitudes that jointly generate a condition of certain probability of occurrence while the rps iso curves represent the joint events of the same exceedance probability the physical hydrological impact of such events can be very different and hence leading to substantially different flood risk maps therefore when assessing flood risk it is crucial to evaluate flood impacts across the whole spectrum of exceedance probabilities and this has been done in this study using a hydrodynamic model in the multivariate joint probability analysis the effect of inclusion dependencies was also considered rps iso curves generated with various levels of dependencies for cork show that the occurrence of dependencies modifies the joint extremes and the stronger dependencies the higher the joint exceedance values along the curve of a certain rp dependencies between flood drivers in the inter tidal river reach in cork lead to higher flood risks in this zone and this is confirmed by hydrodynamic modelling ward et al 2018 also found that where high river discharge and high sea levels are exceeded and these drivers are dependent the joint probability of events can be several magnitudes higher compared to the independent scenario accounting for joint occurrence of multiple flood drivers is important for hazard mapping designing flood protection infrastructure and planning emergency responses 4 5 selection of flood scenarios the step 5 of the methodology concerns the selection of flood scenarios for hydrodynamic runs in total four sets of runs for cork were performed two sets of runs considered univariate marginal scenarios fluvial flood or coastal flood only one run considered combined marginal coastal and fluvial runs and finally one set of runs considered and joint probability scenario in reality observations show that cork city floods are due to a combination of drivers with varying degree of contribution and sole action of one driver would be rare and linked to river flood defence management rather than meteorological conditions and storm modulated co occurrences of drivers anyway in this study all the four sets of conditions were investigated to explore all potential risks 4 6 ensemble flood model simulations step 6 concerns hydrodynamic modelling of flood scenarios there have been many studies in recent years that investigate joint occurrence of river flows vs coastal water levels and dependencies between them e g wahl et al 2017 khanal et al 2019 klerk et al 2015 bevacqua et al 2020 although these studies are useful to understand processes that drive flooding they do not answer the fundamental questions of how severe these events can be and what combination of extreme signals can result in the most hazardous events moftakhari et al 2019 go a step further and link statistical analyses with a hydrological model to answer first of these questions this research builds on moftakhari et al 2019 to answer both questions while the present study uses a similar methodology to moftakhari et al 2017 it is more heavily focused on the hydrodynamic component and provides a full quantification of impacts along the entire rp probability curve and across various flood scenarios moreover the hydrodynamic model accounts for the effect of friction inertia and topographic complexity in flooding dynamics as well as physical compounding e g backwater wave damping amplification due to riverine and or tidal forcing that change flood stage and routing these aspects have not been analysed in depth yet 4 7 assessment of inundation due to multiple flood drivers step 7 concerns an assessment of inundation due to multiple drivers comparing modelled inundation extents with those observed historically for similar flood conditions it is apparent that the joint probability and scenario represents a more realistic representation of the spatially variable water surface profile then the combined marginal scenarios or individual univariate marginal scenarios interestingly water depths and flood wave velocities are higher for the and scenario than for the combined marginal scenarios for the same rp event despite the fact that the and scenario is driven by substantially lower extreme univariate conditions that confirms the amplifying effect of the coinciding drivers in their moderate ranges another aspect of multivariate analysis is a consideration of multiple combinations of joint probability solutions the extent of a flood varies greatly for various combinations of drivers magnitudes although each combination drives a flood event of the same probability hence for cork city much more impactful are events where the contribution of a fluvial component is larger than a coastal one and this is because the city is better protected against high sea water levels rather than high river discharges the hydrodynamic analysis shows that flood impacts may vary greatly along a single probability curve so statistical modelling need to be accompanied with hydrodynamic modelling to include all possible combinations of drivers and their joint impact therefore the statistical hydrodynamic methodology proposed here is perhaps the only way to comprehensively assesses the compound nature of coastal sea levels and river discharges also it is apparent from the analysis that the multivariate approach is more accurate than the traditional univariate assessment methods moreover the proposed method requires only a limited number of hydrodynamic simulations to map flood hazards these findings have particularly important socio economic implications for cork as the intertidal reach is adjacent to floodplains characterized by high urbanization with a significant accumulated economic wealth 4 8 flood mechanism establishment step 8 concerns assessment of the flood mechanisms the flood investigation based on the proposed statistical hydrodynamic method allows one to draw inferences about flood mechanisms propagation dynamics and hazards under various flood probabilities the hydrodynamic model gives an opportunity to test multiple scenarios so a good understanding of fluvial coastal mechanisms can be gained the hindcast runs for cork s past flood events validated against observations confirm that the city is more vulnerable to fluvial driver as the fluvial floods often originate in the rural areas far upstream and propagate to the city centre through the streets sloping along the river channel also the costal flood defence systems are designed to protect against a 200rp sea level event the mechanism however is likely to shift in future climate towards coastal component and have more pronounced effect on floods as demonstrated in kirkpatrick and olbert 2020 estimating the potential risk to flooding and understanding flood controlling conditions greatly aids flood risk management so that flood prevention or alleviation schemes can be evaluated and or optimized 5 conclusions the paper presents a development of a statistical hydrodynamic modeling toolbox and proposes a methodology for assessing the combined effects of multiple source flooding in urban areas the proposed methodological framework is an 8 step process that combines multivariate statistical analysis and dependence analysis with hydrodynamic modelling the method involves individual and combined extreme value analysis assessment of dependencies and interactions between flood drivers multivariate joint probability determination accounting for dependencies and high resolution hydrodynamic modelling of flood scenarios derived from the multivariate statistical analysis the probability and severity of individual drivers and their compounding effects on flooding are investigated by analysing the pattern of flood wave propagation inundation depths and overall extent of inundation the methodology was applied to a case study site cork city ireland for which the 2d urban flood model msn flood was applied while a successful attempt to link statistical and hydrodynamic model exist moftakhari et al 2017 moftakhari et al 2019 the methodology for combining statistical hydrodynamic modelling developed in this research is heavily focused on the hydrodynamic component and proves a necessity for quantification of impacts along the entire rp probability curve and across various flood scenarios this is the single most important finding from this research other main conclusions regarding the methodology are in the absence of long term records a statistical analysis of water level extremes provided reliable long term estimates of flood driving conditions the gev model exhibited a good fit to data interactions and dependencies between tides surges and river flows affect flood severity when they occur jointly and therefore need to be included in the analysis of extreme water levels the bivariate and trivariate joint probability and scenarios provide plausible results for joint exceedance return levels of water elevations and account for compound effects the multivariate methodology approach is superior to the traditional univariate assessment methods based on synthetic water profiles derived from combined marginal scenarios multivariate analysis allows also considering multiple combinations of joint probability solutions along a rp iso curve while the rps iso curve represents the joint events of the same exceedance probability the physical hydrological impact of such events can be very different leading to substantially different characteristics of flooding for these reasons combining the statistical and hydrodynamic modelling is very important a high resolution urban flood model forced with joint probability boundary conditions can be used to draw inferences about flood mechanisms and extents in urban environments the msn flood model used here proved very robust and ran stably under quite severe inflow conditions such as those designated by 1 in 500 year river flow and 1 in 200 year sea water level flood mapping requires only a limited number of hydrodynamic simulations derived from a multivariate analysis without a need for large ensemble simulations for a combination of extreme univariate conditions as such the proposed methodology provides a cost effective practical approach for delineating compound flood hazards driven by complex multivariate mechanisms the methodology proposed in this paper provides a robust framework for mapping coastal flood hazards in tidally active river channels that takes advantage of recent advances in multivariate statistical modelling and hydrodynamic coastal flood modelling the method also facilitates better understanding of the conditions that control flooding under multivariate sources tide surge and fluvial and allows an estimation of the compounding effects of multiple flood drivers estimating the potential risk of flooding and understanding flood controlling conditions is extremely important for flood risk assessment so that the flood prevention or alleviation schemes can be designed evaluated and or optimized this is a critical task given a typical design life of coastal defence structures of 50 to 100 years the information derived from the application of this methodology can provide a significant support for long term planning investment decisions on flood defence infrastructure and quantification of probable flood damage of economic social and environmental natures declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgement the authors would like to thank opw ireland for hydrological data this research has been funded by the environmental protection agency ireland under the epa research programme 2021 2030 project code 2021 ccen ct7 
2022,accurate estimation of the wetting distribution pattern wdp around the emitters of a drip irrigation system in sloping lands can minimize surface runoff losses by determining the placement status of plants and emitters in this study both experimental and computational efforts were made to estimate the wdp in sloping lands with drip irrigation 486 sets of laboratory experiments were conducted and a series of soil characteristics data were collected particularly the upstream wetting radius r downstream wetting radius r and wetting depth d were measured and further used as the target variables of three modeling scenarios in the modeling effort a new hybrid framework consisting of a light gradient boosting machine lightgbm and best subset regression bsr integrated with bidirectional recurrent neural network bi rnn was developed for precise simulation of wdp the main model i e bi rnn was compared with the elman recurrent neural network ernn and bagging regression tree bgrt in the advanced multi filtering framework for all the scenarios in the first stage the lightgbm tree based feature selection filtered the significant predictors in each scenario in the second stage the three best possible input combinations using the n predictors selected in the first stage were extracted among 2n possible combinations via the bsr strategy the performances of the models were evaluated by using different statistical metrics it was demonstrated that bi rnn achieved the highest accuracy in all the hybrid models for the three scenarios followed by the ernn and bgrt models also a resampling bootstrap based uncertainty analysis proved that the developed multistage filtering strategy before the deep learning model feeding decreased the uncertainty associated with input combination effects by determining the placement status of plants and emitters the proposed framework can effectively reduce surface runoff losses keywords wetting distribution pattern sloping lands drip irrigation bi rnn bgrt data availability data will be made available on request 1 introduction according to the population census the upsurge in need for food generation is inevitable with insufficient water resources an effective drip irrigation system can assist in tackling this issue and improve water usage with minimum losses via controlling surface runoff soil evaporation and deep percolation kanda et al 2020 liu et al 2015a samadianfard et al 2014 a well established and accurate operating system regulates a uniform water distribution between the emitters and laterals which can maximize the water fertilizer efficacy and decrease water loss al ogaidi et al 2016 karimi et al 2022 shiri et al 2020 the dimensions of the wetting distribution pattern wdp in horizontal and vertical directions in a drip irrigation system are two key components to augment emitter space lateral placement and installation depth of emitters al ogaidi et al 2016 kilic 2020 samadianfard et al 2012 wdp is one of the important parameters that are effective in the construction of irrigation systems such as the distance between the laterals emitters and installation depth of emitters the design of wdp will affect the estimation of the emitter and lateral distance and the fitting depth of emitters as well as the water use efficiency of a drip irrigation system thus an accurate wdp of the emitters is critical to the suitable development of drip irrigation systems a drip irrigation system creates a partially saturated wetting pattern depending on soil properties e g saturated hydraulic conductivity bulk density and initial moisture content and the characteristics of the irrigation system e g emitter discharge total volume of applied water spacing between emitters and laterals water application mode continuous or pulse and emitter s position surface or subsurface al ogaidi et al 2016 elnesr and alazba 2017 shiri et al 2020 however land slope or soil surface topography can have a major impact on wetting front dimensions when drip irrigation systems are installed on sloped ground e g esmaili et al 2016 heidari et al 2015 norouzian and sadraddini 2017 solat et al 2021 since a significant percentage of cultivated agricultural lands worldwide is steep with a slope greater than 5 and drip irrigation is one of the most widely used systems in such lands it is logical to identify the precise dimensions of the moisture bulb through modeling substantial research has been performed so far to estimate wdp under drip irrigation leading to different modeling approaches including empirical models al ogaidi et al 2016 analytical models cook et al 2003 moncef and khemaies 2016 and numerical models elnesr and alazba 2019 šimůnek et al 2006 solat et al 2021 empirical models are derived from field observations or well controlled laboratory experiments the goal of these models is to develop empirical equations based on the influential variables e g physical properties of the soil and the characteristics of the drip irrigation system even though empirical models are simple they can be site specific and cannot be used in other places kisi et al 2021 shiri et al 2020 analytical approaches often involve solving the flow governing equations for a given set of initial and boundary conditions however the applicability of these models is limited due to numerous assumptions e g soil homogeneity and uniform initial soil moisture distribution introduced in order to derive closed form analytical solutions in contrast numerical models solve the governing equations e g richards equation for variably saturated flow using numerical methods e g finite difference and finite element methods even though numerical models can be helpful they are not always easy to use in addition they are computationally intensive and also subject to numerical dispersion and stability problems most relevant studies have focused on wetting front analysis on flat lands and only a few studies have been conducted for sloping lands esmaili et al 2016 heidari et al 2015 norouzian and sadraddini 2017 these researchers showed that gravity and suction forces are the main factors in the horizontal moisture distribution for sloping and flat lands respectively they also found that the horizontal distribution of moisture bulbs in sloping lands was higher than that in flat lands and a significant portion of the wetted dimensions of the moisture bulbs were created at the bottom of the emitters heidari et al 2015 and norouzian and sadraddini 2017 showed that the dimensions of wetted patterns in flat lands were significantly different from those in sloping lands they also found that as the land slope increased the horizontal distribution increased and the vertical distribution decreased because water had less chance to penetrate into deep soil due to the major differences in wetted patterns between the sloping and flat lands the current wdp models for flat lands are not applicable for sloping lands heidari et al 2015 and norouzian and sadraddini 2017 simulated moisture patterns for sloping lands using numerical models e g hydrus solat et al 2021 simulated the wdp by using empirical and numerical approaches for sloping lands and demonstrated that the accuracy of both approaches was acceptable it is imperative to develop and implement robust approaches to estimate the wdp around the emitters with suitable accuracy and lower computational power soft computing e g machine learning ml can be a good alternative to predict moisture front patterns under drip irrigation in recent years data driven approaches e g anfis adaptive neural fuzzy inference system gep gene expression programming svm support vector machines and rf random forest have been extensively employed in many areas of hydrology and water resource engineering such studies involve the simulation of hydraulic performance in drainage envelopes adnan et al 2021 modeling of water distribution uniformity in sprinkler irrigation maroufpoor et al 2019 simulation of nitrate distribution under drip irrigation jamei et al 2022b assessment of crop water prerequisites landeras et al 2018 and estimation of soil bulk density shiri et al 2017 some studies have been conducted to simulate the wdp dimensions e g elnesr and alazba 2019 2017 kanda et al 2020 kilic 2020 moncef and khemaies 2016 shiri et al 2020 nowadays deep learning dl techniques have been employed in different engineering disciplines due to their capacity of handling complicated non linear problems for example shen et al 2020 used a 5 layer structured deep belief network dbn to estimate air temperature in china alibabaei et al 2021 employed a long short terms memory lstm approach to estimate crop water requirement and soil moisture under irrigation scheduling jamei et al 2022d used a new hybrid dl technique to estimate the wdp dimensions and area for two layer soil profiles under pulse drip irrigation barzegar et al 2020 developed a convolutional neural network cnn lstm and a new hybrid cnn lstm model to determine water quality indices in the perspa basin odebiri et al 2022 used a remote sensing and dl technique to predict the distribution pattern of soil organic carbon under various land uses in south africa various machine learning and soft computing techniques have been used to model the wdp dimensions of moisture bulbs and good performances have been reported jamei et al 2022d karimi et al 2020 kisi et al 2021 shiri et al 2020 however few studies have been conducted to apply hybrid machine learning to solve wdp problems for sloping lands the main objectives of this research are 1 to make an experimental effort to estimate the wdp dimensions in sloping soil profiles for three scenarios including upstream wetting radius r downstream wetting radius r and wetting depth d patterns fig 1 and 2 to develop a new hybrid framework consisting of a light gradient boosting machine lightgbm and best subset regression bsr integrated with elman recurrent neural network ernn for estimating the wdp 486 sets of laboratory experiments were conducted in the college of agriculture at the university of kurdistan iran in this research the aforementioned scenarios for the first time were simulated by using a new gradient boosting multi filtering hybrid ml framework that was comprised of the lightgbm feature selection bsr wrapper input combination selector and advanced ml models including the bidirectional recurrent neural network bi rnn ernn and bagging regression tree bgrt models to accurately simulate the wdp dimensions in sloping soil profiles the best set of three input combinations for each scenario was extracted by using the multiphase data filtering technique to feed the ml models four statistical indices graphical analysis and an external validation process were carried out to validate the main model i e bi rnn in comparison with ernn and bgrt 2 material and methods 2 1 experimental setup and data the experimental research was performed in the water science and engineering division of the college of agriculture at the university of kurdistan in iran two soil boxes with lengths of 120 and 140 cm widths of 60 and 70 cm and depths of 120 cm and 120 cm were used for the experiments one side of the soil boxes was made of transparent plexiglass sheets for observing the advancement of moisture bulbs at different times during the experiments the smaller and larger boxes were used for experiments with lower and higher discharge rates respectively the air dried soils were filled in the box and compacted to achieve the target bulk density of each soil the physical characteristics of the examined soils are presented in table 1 in order to achieve uniform initial moisture distribution the soil boxes were kept in the laboratory for 24 to 48 h to avoid preferential flow along the walls the plexiglass sheets were first fixed by glue and then sprayed with sand to produce a coarse grained surface before filling the soils into the boxes karimi et al 2022 shiri et al 2020 the equipment used for the experiments included a reservoir with a volume of 200 l a pressure gauge for measuring the system pressure each experiment was conducted at the same pressure of 2 bars an assembly to adjust the inflow and send the extra water back to the reservoir a float to achieve a constant water level a filtration system to prevent emitters from clogging by suspended particles and a valve to control flow the emitters were placed close to the transparent wall and at the center of the visible area water was delivered from the reservoir to the emitters using micro polyethylene tubes main sub main and lateral tubes with diameters of 32 20 and 16 mm respectively fig 2 shows the diagrammatic depiction of the experimental apparatus this box denotes one half of the entire cylinder or one half of the wetted zone and hence the actual emitter discharge was doubled shiri et al 2020 the saturated hydraulic conductivity was computed by using the rosetta software schaap et al 2001 the experiments were conducted for three soil types with different textures light medium and heavy for three discharge rates of 2 4 and 6 l h further the experiments were conducted under four different slopes 0 10 20 and 30 and the irrigation time was 3 h at a series of selected times the shape of the wetness advance front was drawn on the transparent wall in the end the images of the wetting zone were processed the coordinates of the downstream and upstream horizontal distributions and the vertical distributions of the wetting zone were determined and the downstream and upstream wetting areas were calculated using the grapher software version 7 table 2 shows the statistical characteristics of the datasets obtained from the experiments the kurtosis parameter of all the datasets 1 503 0 002 fell in the allowable interval 2 2 2 2 geroge and mallery 2003 nie 1975 and had a platykurtic distribution with respect to kurtosis 0 besides the skewness also fell in a reasonable range 0 143 0 891 for a ml based predictive model in this study the upstream wetting radius of emitter for sloping lands r the downstream wetting radius of emitter for slopping lands r and the wetting depth d were simultaneously measured for each treatment with specified discharge slope and soil type the details of the measured parameters are described in solat et al 2021 3 theoretical overview of the computational approaches 3 1 light gradient boosting method lightgbm feature selection the feature selection strategy can enhance model performance minimize training time and prevent overfitting to obtain these benefits the lightgbm feature selection data filtering strategy was implemented as the first stage of the preprocessing microsoft introduced lightgbm in 2017 as an ml technique using a gradient boosting decision tree gbdt ke et al 2017 it potentially reduces memory usage to 1 8 of the original supports distributed processing and effectively handles batch data it has a high parallel training efficiency and speed yao et al 2022 lightgbm provides two innovative strategies to cope with input predictors with a number of features such as exclusive feature bundling and gradient based one side sampling lightgbm primarily employs the histogram algorithm and the leaf wise algorithmto improve the algorithm s computational capacity lightgbm utilizes the histogram technique to integrate mutually discordant characteristics the basic idea is to discretize the continuous feature values into m integers prior to creating an m width histogram to locate the decision tree the data are explored based on the discretized values of the histogram due to the decision tree s weak model and the histogram approach s large reduction in time complexity this fuzzy partitioning strategy typically produces better outcomes than the decision tree wang et al 2022 the leaf wise technique with depth limitation entails choosing the leaf with the greatest gain to divide and loop at each split simultaneously by restricting the depth of the tree and the number of leaves the model s complexity is minimized and overfitting is avoided the gradient boosting technique is adopted to train the lightgbm model and its hyperparameters are optimized using the grid search technique the hyperparameters of the lightgbm model optimized by the grid search approach include the number of weak regression trees the number of leaves the learning rate and the number of iterations ren et al 2022 3 2 best subset regression bsr wrapper optimizer the bsr is a model selection approach that tests all possible combinations of the predictor variables from the first preprocessing stage and then selects the best model according to statistical criteria jović et al 2015 for k independent variables the optimal subset selection can be summarized as follows 1 consider all possible models with one variable two variables up to k variables 2 select the best model of size 1 the best model of size 2 and the best model of size k 3 select the best overall model among the finalists the optimal subset selection ultimately selects one model out of 2 k alternative models the statistical criteria including mallow s coefficient c p kobayashi and sakata 1990 mean square error mse adjusted r 2 akaike s information criterion aic akaike 1974 and determination coefficient r 2 were used for the best subset selection cp and aic are respectively given by sushanth et al 2023 1 c p rss k mse m 2 k n m k 2 aic 2 p n l n 1 n i 1 n e i 2 where p denotes the number of the model s parameters k denotes the number of variables n is the number of data e i is the residual and rss k is the residual sum of squares the lowest c p mse and aic values represent the best model performance in this research the bsr is implemented for the second preprocessing stage 3 3 bidirectional recurrent neural network bi rnn to circumvent the restrictions of a standard rnn schuster and paliwal 1997 developed a bi rnn which was trained on all data from the past and future of a single time frame the goal of bi rnn is to partition a standard rnn s state neurons into two parts one for time going forward forward states and one for time going backward backward states jaihuni et al 2022 the outputs of forward states are not coupled to the inputs of backward states and vice versa fig 3 illustrates a simple bi rnn structure this structure is reduced to a normal unidirectional forward rnn without the backward states because the two state neurons do not contact the bi rnn can be trained like a regular unidirectional rnn and unfolded into a generic feedforward network 3 4 elman recurrent neural network ernn multilayer perceptron mlp neural networks are often utilized for classification modeling and prediction gao et al 2019 sabour et al 2022 cagcag yolcu et al 2021 an mlp consists of an input layer an output layer and one or more hidden layers mlp can be classified as feed forward neural networks ffnn and recurrent neural networks rnn ffnns are artificial neural networks which are simpler than rnns in that the connections between units do not create a cycle elman proposed the elman neural network enn elman 1990 enns are two layer backpropagation networks featuring a feedback link between the output and input of the hidden layer this relationship is referred to as the context nodes the hidden nodes are turned on by both the input units and the context nodes the hidden nodes then feed forward to turn on the output nodes the hidden nodes also trigger the context nodes through feedback fig 3 depicts a typical topology of ernn ernn has a local memory unit and a local feedback connection making it a type of recurrent neural network specifically it has four layers input hidden transport and output the network s transport layer may convey status information and is equipped with memory enabling enn to make more accurate predictions enn is ideally suited for tackling issues involving series and time series data the ernn s dynamics can be mathematically expressed as liu et al 2015b 3 x h k 1 s w c x c k 1 w 1 u k 4 x c k 1 w r x h 5 y k 1 w 0 x h k 1 where x c and x h denote the context nodes and hidden layers u k and y k are the input and output of the network at discrete time k w 0 w 1 w c and w r are the weight matrices for the hidden output input hidden context hidden and hidden context layers respectively and s is a hyperbolic tangent function the quick bp method is utilized for the training of ernn 3 5 bagging regression tree bgrt the ensemble regression tree algorithm which is an integrated learning algorithm provides greater self learning capability and precision than a single regression tree method peng et al 2023 one of the first ensemble learning methods is bagging i e bootstrap aggregating breiman 1996 the bagging technique helps minimize the decision tree s variance hajian et al 2022 this method takes a training dataset and makes several bootstrap samples from it then classifiers or base learners are trained on the new learning sets before their predictions are averaged for a numerical output or a plurality vote is done for a class outcome the bagging regression tree bgrt model is constructed in the new training data set using the regression tree technique and the expected output value of each regression tree is obtained the bgrt technique uses the average of all regression tree models output projected values as its final value as a result the bgrt algorithm s mathematical model can be expressed as peng et al 2023 6 h a x s i g n h i x where h a x is the result of the bgrt and h i x is the result of individual regression tree i bagging is used to decrease variation and prevent the overfitting of a learning procedure the primary hyperparameters for constructing a bgrt model including the number of trees and the number of splits for each tree are optimized when the number of regression trees is insufficient the accuracy of the bgrt model decreases too many regression trees will result in excessive computation but the performance of the bgrt model will not increase the hyperparameters are optimized by the grid search method fig 3 portrays the flowchart of the bgrt method 3 6 evaluation indicators four statistical indices including r correlation coefficient kge kling gupta efficiency gupta et al 2009 u95 uncertainty coefficient with 95 confidence level and rmse root mean square error were utilized to evaluate the performances of the models the four metrics are given by 7 r i 1 n wdp obs i wdp obs wdp pred i wdp pred i 1 n wdp obs i wdp obs 2 i 1 n wdp pred i wdp pred 2 8 rmse 1 n i 1 n wdp obs i wdp pred i 2 9 kge 1 r 1 2 α 1 2 β 1 2 10 u 95 1 96 std 2 rmse 2 wdp obs i and wdp pred i are the ith observed and predicted wetting distribution pattern wdp values respectively wdp obs and wdp pred are the averages of the observed and predicted wdp values respectively n is the number of data points and std is the standard deviation moreover α represents the comparative variation of the expected and observed values whereas β is the ratio of the predicted and observed mean values for a perfect model performance r and kge equal 1 whereas rmse and u 95 equal 0 4 interpretation of hybrid model components the proposed multi filtering based deep learning lightgbm was integrated with bsr and the three advanced ml models i e bi rnn ernn and bgrt to estimate the dimensions of moisture bulb or wdp on sloping lands under drip irrigation for three scenarios r upstream wetting radius r downstream wetting radius and d wetting depth according to the experiments nine predictors were selected to estimate the wdp they included the elapsed time time emitter outflow rate q saturated hydraulic conductivity ks initial soil moisture content θ soil surface slope s percentage of sand sand percentage of silt silt percentage of clay clay and bulk density ρb one of the simplest methods of examining the behavior of the input datasets versus the target is linear correlation based on the pearson coefficient rp which is shown statistically by a correlogram fig 4 shows the linear correlation of all the predictors attained by the experiments against three targets i e r r and d it is clear that the time with the highest rp 0 80 0 79 and 0 86 for r r and d scenarios respectively is the most influential predictor followed by q and ks however in the scenario of r except for the time the linear correlation between r and other predictors is significantly low the above analysis confirms the necessity of further investigation using a feature selection with the ability to support non linear relationships jamei et al 2022a one of the novelties of this study is the establishment of an advanced data filtering strategy to achieve the best accuracy of the ml models thus a novel tree based feature selection lightgbm is proposed to specify the most effective parameters for constructing candidate input combinations for each scenario in an effort to improve the accuracy and reduce the computational cost fig 5 clearly reveals the optimal predictors in each scenario using three sun plots with the important factors related to all the preditors listed under the plots according to the importance factor values the elapsed time emitter outflow rate q ks θ and s are selected among the nine exisitng predictors for model construction for the three scenarios the order of the filtered predictors is time θ s q ks sand ρb clay silt for scenario r time θ s q ks clay silt sand ρb for scenario r and time θ q s ks ρb silt sand clay for scenario d using the bsr method three optimal combinations c1 c2 and c3 are extracted from n selected predictors during the second preprocessing stage instead of testing 2 n possible combinations this technique allows to extract the best combinations based on mse r2 ad r2 cp and aic if r and ad r2 are high and mse cp and aic are low the input combination order is deemed superior according to table 3 the bsr method s criteria show that the best combination in scenario r has three variables c1 time q s the best combination in scenario r has five variables c3 time q ks θ s and the best combination in scenario d has four variables c2 time q ks s the final combinations for each scenario are limited to 3 5 predictors from a total of nine variables resulting in far fewer options this demonstrates the efficacy of the new data filtering framework based on the powerful non linear feature selection method however the optimum input combinations selected by the bsr approach are specified by the bold font table 3 it is required to test the potential of the three existing combinations by feeding the ml models in the next section the hyperparameter tuning process is one of the most important aspects of ai based modeling if reasonable attention is not paid to this step the nature of the study as a whole may change jamei et al 2022c random search is an efficient and popular strategy to find optimal hyperparameters it has a search space containing different predefined hyperparameter combinations this method is useful for quick and efficient finding of the optimal hyperparameter combination in a high dimensional search space in the present study random search is utilized to tune the hyperparameters and the tuned hyperparameters of all the ml models are listed in table 4 the main hyperparameters of bi rnn as the main model are the number of layers number of neurons and learning rate the learning curves loss function versus epoch values associated with the bi rnn models for the three scenarios i e r r and d considering all the input features are shown in fig 6 the loss functions in all the deep learning models are defined based on the mse metric it can be observed that with a smaller epoch 200 the bi rnn model for scenario r exhibits faster covergence than the other scenarios in the training phase to avoid overfitting the k fold cross validation is implemented using k 5 folds the k fold cross validation splits the training set into k folds each split uses k 1 folds of the training data to train a model malik et al 2022 the remaining fold is used to validate the model for each split the held out fold is scored splits scores in both training and testing stages are averaged to describe the accuracy of each stage equation 11 is used in this study to scale the inputs and targets in the scenarios linearly to a range of 0 1 jamei et al 2022a 11 x i n x i x m i n x m a x x m i n where x i denotes the input vector and x m i n and x m a x are the scaled versions of the measured data s minimum and maximum respectively the schematic road map of the modeling stages for estimating the wdp in the sloping lands is exhibited in fig 7 5 results and analysis based on the input combinations combo 1 combo 2 and combo 3 the performances of the bi rnn ernn and bgrt models for predicting wdp for scenarios r r and d were evaluated by using r rmse kge and u95 table 5 shows that for the input combination combo 3 the bi rnn model accomplished the highest degree of precision to predict wdp for scenario r r 0 9920 rmse 1 2441 kge 0 9916 and u95 3 4556 in the testing period followed by ernn r 0 9900 rmse 1 3983 kge 0 9711 and u95 3 8825 bgrt with combo 2 yielded the best result r 0 9711 rmse 2 6283 kge 0 8479 and u95 7 2965 overall bi rnn with combo 3 appeared to be the best model for scenario r which also indicated that combo 3 was better for prediction than combo 1 and combo 2 according to table 5 bi rnn for combo 3 attained better accuracy r 0 9910 rmse 0 9207 kge 0 9777 and u95 2 5418 to predict wdp for scenario r than ernn and bgrt the bgrt model with combo 2 provided a better prediction of wdp for scenario r overall bi rnn and combo 3 provided better predictions for scenario r than ernn and bgrt with combo 1 and combo 2 for scenario d in table 5 bi rnn again outperformed ernn and bgrt with combo 3 r 0 9948 rmse 0 9813 kge 0 9917 and u95 2 7223 in the prediction of wdp in the testing period the results also revealed that combo 3 was a better input combination than combo 1 and combo 2 as shown in table 5 bi rnn with combo 3 acquired better accuracy in terms of goodness of fit to predict wdps for scenarios r r and d than ernn and bgrt fig 8 displays the comparisons of the wdp prediction results from the three ml models i e bi rnn ernn and bgrt with three different input combinations i e combo 1 combo 2 and combo 3 in the form of stacked plots using r rmse kge and u95 for scenarios r r and d it can be observed that bi rnn combo 3 generated the best accuracy in terms of these statistical metrics followed by ernn combo 3 and bgrt combo 2 for the three scenarios as shown in fig 8 combo 3 was the best input combination to accomplish the highest degree of precision for bi rnn ernn and bgrt followed by combo 2 and then combo 1 in the prediction of wdp for scenarios r r and d fig 9 shows an overview of the comparison based on combo 1 combo 2 and combo 3 among the bi rnn ernn and bgrt models between the predicted estimated and observed wdp in terms of scatter plots right for r scenario the bi rnn based on combo 3 with rcombo 3 0 9920 shows better performance followed by ernn rcombo 3 0 9900 and bgrt rcombo 2 0 9711 similarly the bi rnn demonstrates the best precision with rcombo 3 0 9910 against ernn and bgrt models to predict wdp for r lastly the birnn with combo 3 again obtained better prediction in terms of wdp for the d scenario with rcombo 3 0 9948 which is slightly better than ernn and bgrt models fig 10 shows the competence and effectiveness of the bi rnn ernn and bgrt models against the observed wdps for combo 1 combo 2 and combo 3 using the boxplots and probability distribution functions in scenarios r r and d the wdps predicted by combo 3 bi rnn are more closely matching the observed values and their probability distributions for all three scenarios in contrast the boxplots for ernn and bgrt with combo 1 and 2 for scenarios r r and d are dispersed exhibiting outliers with larger probability distribution function values thus the bi rnn model with combo 3 provided more accurate predictions of wdp fig 11 shows the plots of the physical expected behavior relative errors re and residuals of the wdps simulated by bi rnn ernn and bgrt with combo 3 for the three scenarios i e r r and d the wdps acquired by the bi rnn model red color appeared to be more stable and accurate along with lower re and residuals in contrast the predictions from ernn and bgrt show slightly large fluctuations and spikes in comparison with the observed wdps as well as the maximum re and residuals in scenarios r r and d overall bi rnn achieved the highest precision based on the re and residual plots fig 12 shows the histograms and smooth kernel distribution functions of the relative frequency of absolute relative errors re of the wdps simulated by bi rnn ernn and bgrt based on combo 3 for scenarios r r and d the frequency distributions of re for the bi rnn model ranged in a smaller error bracket from 0 to 3 5 for scenario r followed by ernn 0 to 4 0 and bgrt 0 to 14 in the prediction of wdp again bi rnn appeared to be reasonably good with re varying from 0 to 30 for scenario r and from 0 to 30 in scenario d fig 12 also shows the overall better precision of the proposed bi rnn model with combo 3 for scenarios r r and d to predict wdps table 6 and fig 13 show the external validation results of bi rnn ernn and bgrt based on four statistics k kp m and n it can be observed that the bi rnn model acquired better and consistent external validation results for the prediction of wdps k 0 9998 kp 0 9970 m 0 0162 n 0 0162 for scenario r k 1 0031 kp 0 993 m 0 0182 n 0 0180 for scenario r and k 0 9944 kp 1 0031 m 0 0104 n 0 0105 for scenario d ernn is reasonably better than bgrt in the external validation for all three scenarios overall the bi rnn model surpassed both ernn and bgrt in the prediction of wdps 6 further discussion in this research a novel modeling framework was developed based on the lightgbm and bsr algorithms hybridized with the bi rnn deep learning model to precisely estimate wdps for three scenarios i e r r d the bi rnn model was compared with the ernn and bgrt models as well as two other models nlr non linear regression and hydrus solat et al 2021 their performances were evaluated by using different statistical metrics demonstrating the better accuracy of the hybrid bi rnn model fig 14 displays spider diagrams in terms of r and rmse for the bi rnn ernn bgrt nlr and hydrus models to simulate wdps for scenarios r r and d the bi rnn model exhibits a bigger spider web based on the r values than the four other models indicating the highest precision of the simulations for scenarios r r and d in addition the spider web of bi rnn based on the rmse values is smaller than those from all other models which again confirms the highest accuracy of bi rnn to estimate the wdps for scenarios r r and d to further assess the performance of the intelligent framework the uncertainty quantification was investigated based on the model structure input combinations affecting the model training and the uncertainty from the experimental datasets herein the u factor is a benchmark criterion based on the mean 95 uncertainty band extracted by using the resampling bootstrap distribution strategy rousselet 2020 the resampling bootstrap strategy replaces multiple samples from an individual random sample to compute the mean 95 confidence band the u factor can be defined as 12 u factor mean 95 confidence band std ob where st d ob is the standard deviation of the observed wdp values in each scenario the least u f a c t o r an uncertainty benchmark criterion is related to scenario d followed by scenarios r and r the uncertainty factor was computed for all the input combinations or scenarios to assess the impacts of the input combinations and model structure concerning the effect of the input combinations as shown in fig 15 combo 3 and combo 2 in all scenarios r r and d had the least uncertainty in terms of the model structure bi rnn and ernn yielded the minimum uncertainties also analyzing the uncertainty results upon dual perspectives of the input combinations and the model structure shows that the model structure had a higher impact than input combinations this finding highlights the enhancement of the predictive framework by implementing intelligent data filtering based modeling of wdp also the red dash lines in the heat maps of fig 15 indicate the optimal paths in both uncertainty assessment directions fig 16 depicts the 95 confidence band for bi rnn with combo 3 as the superior case in the testing phase scenario d achieved the least uncertainty u factor 0 148 followed by scenario r and scenario r it should be noted that the experiments of the current research showed the role of human and laboratory equipment errors in the uncertainty related to the extracted data used in the modeling however the quantitative experimental uncertainty analysis coleman and steele 2018 1995 is a specialized topic that is beyond the scope of this study and can be addressed in the future research the mechanism and the modeling structure proposed in this study have several advantages over others for example the lightgbm feature selection strategy can enhance the model performance minimize the training time and prevent overfitting this is why the lightgbm was implemented as a data filtering strategy in the first stage the lightgbm provides two innovative strategies to cope with input predictors with a number of features such as exclusive feature bundling and gradient based one side sampling the best subset regression bsr wrapper optimizer is a model selection approach to test all possible combinations of the input predictor variables in the second stage and then select the best input combination based on the statistical criteria the bsr selects three best possible input combinations using the n selected input predictors in the first stage finally the bi rnn processes the inputs in both forward and reverse time orders which allows bi rnn to examine future context as well bi rnn helps in analyzing the future events by not limiting the model s learning to the past and present bi rnn tries to learn high level features from the data in an incremental manner which eliminates the need of domain expertise and hard core feature extraction moreover the results demonstrate that the proposed model structure yields the minimum uncertainties the proposed modelling strategy has been developed in a way that it requires a smaller number of input predictors which is particularly significant and plays a vital role in the prediction of wdp for scenarios r r and d in this study for example the lightgbm feature selection model ranked and selected the best and optimal input features only instead of using all features this strategy enhanced the model prediction accuracy and computational efficiency and avoided overfitting additionally the bsr model is working to select the best and optimal set combination of the input features which further improves the model s precision and reduces the computational time the accuracy of the proposed model is significantly higher than that of all other models used in this study as shown in table 5 fig 8 and fig 9 the proposed modeling strategy to estimate wdps demonstrated that the data driven model developed in this study can be applied to solve other problems in hydrology water resource and other relevant areas to assist government agencies for better decision making the proposed strategy was efficient and effective to develop a deep learning model i e hybrid bi rnn for wdp estimation to further improve the relevant studies following aspects can be considered in the future efforts satellite data can be a feasible alternative to the ground based data to improve the performance of the models due to the restriction of deep learning models that depend on the black box nature it is hard and challenging to understand the relationship between the inputs and target outputs during the learning stage thus the integration of deep learning and numerical weather prediction models can be a good choice to coin a new research area moreover the hybrid bi rnn and the counterpart models in this study can also be enriched by integrating with bayesian model averaging sloughter et al 2010 and ensemble approaches tiwari and chatterjee 2011 to cope with the model uncertainties 7 conclusion and remarks in this research the dimensions of moisture bulb or wetting distribution patterns wdp were estimated and assessed experimentally and computationally for the first time in sloping lands for this purpose 486 sets of experiments were conducted and the data collected included the elapsed time emitter outflow rate saturated hydraulic conductivity initial soil moisture content soil surface slope percentages of sand silt and clay as well as bulk density particularly upstream wetting radius r downstream wetting radius r and wetting depth d were measured the modeling effort focused on developing a gradient boosting multi level data filtering technique that consisted of the lightgbm and bsr wrapper combination optimizer hybridized with the bi rnn deep learning model for precise simulation of wdps besides the main model i e bi rnn was compared with ernn and bgrt in the hybrid framework for different candidate input combinations namely combo 1 combo 2 and combo 3 with the aim of designing a novel robust modeling system to predict wdps scenarios r r and d the performance assessment using a set of statistical metrics and diagnostic plots indicated that bi rnn was the most suitable and accurate model to achieve the optimum precision for combo 1 combo 2 and combo 3 the bi rnn model with combo 3 was superior in the prediction of wdp r 0 9920 rmse 1 2441 kge 0 9916 and u95 3 4556 for scenario r r 0 9910 rmse 0 9207 kge 0 9777 and u95 2 5418 for scenario r and r 0 9948 rmse 0 9813 kge 0 9917 and u95 2 7223 for scenario d besides the uncertainty analyses for the three scenarios in two directions i e input combinations and model structure revealed that the data filtering based pre processing lightgbm feature selection coupled with bsr effectively reduced the uncertainty and sensitivity associated with the effect of input combinations the new methods developed in this study can be further extended and applied to solve the problems in other areas such as environmental science hydrology agriculture and renewable energy credit authorship contribution statement mehdi jamei conceptualization formal analysis visualization software project administration bakhtiar karimi conceptualization visualization investigation mumtaz ali validation methodology fariba alinazari methodology masoud karbasi investigation validation eisa maroufpoor investigation xuefeng chu supervision writing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
2022,accurate estimation of the wetting distribution pattern wdp around the emitters of a drip irrigation system in sloping lands can minimize surface runoff losses by determining the placement status of plants and emitters in this study both experimental and computational efforts were made to estimate the wdp in sloping lands with drip irrigation 486 sets of laboratory experiments were conducted and a series of soil characteristics data were collected particularly the upstream wetting radius r downstream wetting radius r and wetting depth d were measured and further used as the target variables of three modeling scenarios in the modeling effort a new hybrid framework consisting of a light gradient boosting machine lightgbm and best subset regression bsr integrated with bidirectional recurrent neural network bi rnn was developed for precise simulation of wdp the main model i e bi rnn was compared with the elman recurrent neural network ernn and bagging regression tree bgrt in the advanced multi filtering framework for all the scenarios in the first stage the lightgbm tree based feature selection filtered the significant predictors in each scenario in the second stage the three best possible input combinations using the n predictors selected in the first stage were extracted among 2n possible combinations via the bsr strategy the performances of the models were evaluated by using different statistical metrics it was demonstrated that bi rnn achieved the highest accuracy in all the hybrid models for the three scenarios followed by the ernn and bgrt models also a resampling bootstrap based uncertainty analysis proved that the developed multistage filtering strategy before the deep learning model feeding decreased the uncertainty associated with input combination effects by determining the placement status of plants and emitters the proposed framework can effectively reduce surface runoff losses keywords wetting distribution pattern sloping lands drip irrigation bi rnn bgrt data availability data will be made available on request 1 introduction according to the population census the upsurge in need for food generation is inevitable with insufficient water resources an effective drip irrigation system can assist in tackling this issue and improve water usage with minimum losses via controlling surface runoff soil evaporation and deep percolation kanda et al 2020 liu et al 2015a samadianfard et al 2014 a well established and accurate operating system regulates a uniform water distribution between the emitters and laterals which can maximize the water fertilizer efficacy and decrease water loss al ogaidi et al 2016 karimi et al 2022 shiri et al 2020 the dimensions of the wetting distribution pattern wdp in horizontal and vertical directions in a drip irrigation system are two key components to augment emitter space lateral placement and installation depth of emitters al ogaidi et al 2016 kilic 2020 samadianfard et al 2012 wdp is one of the important parameters that are effective in the construction of irrigation systems such as the distance between the laterals emitters and installation depth of emitters the design of wdp will affect the estimation of the emitter and lateral distance and the fitting depth of emitters as well as the water use efficiency of a drip irrigation system thus an accurate wdp of the emitters is critical to the suitable development of drip irrigation systems a drip irrigation system creates a partially saturated wetting pattern depending on soil properties e g saturated hydraulic conductivity bulk density and initial moisture content and the characteristics of the irrigation system e g emitter discharge total volume of applied water spacing between emitters and laterals water application mode continuous or pulse and emitter s position surface or subsurface al ogaidi et al 2016 elnesr and alazba 2017 shiri et al 2020 however land slope or soil surface topography can have a major impact on wetting front dimensions when drip irrigation systems are installed on sloped ground e g esmaili et al 2016 heidari et al 2015 norouzian and sadraddini 2017 solat et al 2021 since a significant percentage of cultivated agricultural lands worldwide is steep with a slope greater than 5 and drip irrigation is one of the most widely used systems in such lands it is logical to identify the precise dimensions of the moisture bulb through modeling substantial research has been performed so far to estimate wdp under drip irrigation leading to different modeling approaches including empirical models al ogaidi et al 2016 analytical models cook et al 2003 moncef and khemaies 2016 and numerical models elnesr and alazba 2019 šimůnek et al 2006 solat et al 2021 empirical models are derived from field observations or well controlled laboratory experiments the goal of these models is to develop empirical equations based on the influential variables e g physical properties of the soil and the characteristics of the drip irrigation system even though empirical models are simple they can be site specific and cannot be used in other places kisi et al 2021 shiri et al 2020 analytical approaches often involve solving the flow governing equations for a given set of initial and boundary conditions however the applicability of these models is limited due to numerous assumptions e g soil homogeneity and uniform initial soil moisture distribution introduced in order to derive closed form analytical solutions in contrast numerical models solve the governing equations e g richards equation for variably saturated flow using numerical methods e g finite difference and finite element methods even though numerical models can be helpful they are not always easy to use in addition they are computationally intensive and also subject to numerical dispersion and stability problems most relevant studies have focused on wetting front analysis on flat lands and only a few studies have been conducted for sloping lands esmaili et al 2016 heidari et al 2015 norouzian and sadraddini 2017 these researchers showed that gravity and suction forces are the main factors in the horizontal moisture distribution for sloping and flat lands respectively they also found that the horizontal distribution of moisture bulbs in sloping lands was higher than that in flat lands and a significant portion of the wetted dimensions of the moisture bulbs were created at the bottom of the emitters heidari et al 2015 and norouzian and sadraddini 2017 showed that the dimensions of wetted patterns in flat lands were significantly different from those in sloping lands they also found that as the land slope increased the horizontal distribution increased and the vertical distribution decreased because water had less chance to penetrate into deep soil due to the major differences in wetted patterns between the sloping and flat lands the current wdp models for flat lands are not applicable for sloping lands heidari et al 2015 and norouzian and sadraddini 2017 simulated moisture patterns for sloping lands using numerical models e g hydrus solat et al 2021 simulated the wdp by using empirical and numerical approaches for sloping lands and demonstrated that the accuracy of both approaches was acceptable it is imperative to develop and implement robust approaches to estimate the wdp around the emitters with suitable accuracy and lower computational power soft computing e g machine learning ml can be a good alternative to predict moisture front patterns under drip irrigation in recent years data driven approaches e g anfis adaptive neural fuzzy inference system gep gene expression programming svm support vector machines and rf random forest have been extensively employed in many areas of hydrology and water resource engineering such studies involve the simulation of hydraulic performance in drainage envelopes adnan et al 2021 modeling of water distribution uniformity in sprinkler irrigation maroufpoor et al 2019 simulation of nitrate distribution under drip irrigation jamei et al 2022b assessment of crop water prerequisites landeras et al 2018 and estimation of soil bulk density shiri et al 2017 some studies have been conducted to simulate the wdp dimensions e g elnesr and alazba 2019 2017 kanda et al 2020 kilic 2020 moncef and khemaies 2016 shiri et al 2020 nowadays deep learning dl techniques have been employed in different engineering disciplines due to their capacity of handling complicated non linear problems for example shen et al 2020 used a 5 layer structured deep belief network dbn to estimate air temperature in china alibabaei et al 2021 employed a long short terms memory lstm approach to estimate crop water requirement and soil moisture under irrigation scheduling jamei et al 2022d used a new hybrid dl technique to estimate the wdp dimensions and area for two layer soil profiles under pulse drip irrigation barzegar et al 2020 developed a convolutional neural network cnn lstm and a new hybrid cnn lstm model to determine water quality indices in the perspa basin odebiri et al 2022 used a remote sensing and dl technique to predict the distribution pattern of soil organic carbon under various land uses in south africa various machine learning and soft computing techniques have been used to model the wdp dimensions of moisture bulbs and good performances have been reported jamei et al 2022d karimi et al 2020 kisi et al 2021 shiri et al 2020 however few studies have been conducted to apply hybrid machine learning to solve wdp problems for sloping lands the main objectives of this research are 1 to make an experimental effort to estimate the wdp dimensions in sloping soil profiles for three scenarios including upstream wetting radius r downstream wetting radius r and wetting depth d patterns fig 1 and 2 to develop a new hybrid framework consisting of a light gradient boosting machine lightgbm and best subset regression bsr integrated with elman recurrent neural network ernn for estimating the wdp 486 sets of laboratory experiments were conducted in the college of agriculture at the university of kurdistan iran in this research the aforementioned scenarios for the first time were simulated by using a new gradient boosting multi filtering hybrid ml framework that was comprised of the lightgbm feature selection bsr wrapper input combination selector and advanced ml models including the bidirectional recurrent neural network bi rnn ernn and bagging regression tree bgrt models to accurately simulate the wdp dimensions in sloping soil profiles the best set of three input combinations for each scenario was extracted by using the multiphase data filtering technique to feed the ml models four statistical indices graphical analysis and an external validation process were carried out to validate the main model i e bi rnn in comparison with ernn and bgrt 2 material and methods 2 1 experimental setup and data the experimental research was performed in the water science and engineering division of the college of agriculture at the university of kurdistan in iran two soil boxes with lengths of 120 and 140 cm widths of 60 and 70 cm and depths of 120 cm and 120 cm were used for the experiments one side of the soil boxes was made of transparent plexiglass sheets for observing the advancement of moisture bulbs at different times during the experiments the smaller and larger boxes were used for experiments with lower and higher discharge rates respectively the air dried soils were filled in the box and compacted to achieve the target bulk density of each soil the physical characteristics of the examined soils are presented in table 1 in order to achieve uniform initial moisture distribution the soil boxes were kept in the laboratory for 24 to 48 h to avoid preferential flow along the walls the plexiglass sheets were first fixed by glue and then sprayed with sand to produce a coarse grained surface before filling the soils into the boxes karimi et al 2022 shiri et al 2020 the equipment used for the experiments included a reservoir with a volume of 200 l a pressure gauge for measuring the system pressure each experiment was conducted at the same pressure of 2 bars an assembly to adjust the inflow and send the extra water back to the reservoir a float to achieve a constant water level a filtration system to prevent emitters from clogging by suspended particles and a valve to control flow the emitters were placed close to the transparent wall and at the center of the visible area water was delivered from the reservoir to the emitters using micro polyethylene tubes main sub main and lateral tubes with diameters of 32 20 and 16 mm respectively fig 2 shows the diagrammatic depiction of the experimental apparatus this box denotes one half of the entire cylinder or one half of the wetted zone and hence the actual emitter discharge was doubled shiri et al 2020 the saturated hydraulic conductivity was computed by using the rosetta software schaap et al 2001 the experiments were conducted for three soil types with different textures light medium and heavy for three discharge rates of 2 4 and 6 l h further the experiments were conducted under four different slopes 0 10 20 and 30 and the irrigation time was 3 h at a series of selected times the shape of the wetness advance front was drawn on the transparent wall in the end the images of the wetting zone were processed the coordinates of the downstream and upstream horizontal distributions and the vertical distributions of the wetting zone were determined and the downstream and upstream wetting areas were calculated using the grapher software version 7 table 2 shows the statistical characteristics of the datasets obtained from the experiments the kurtosis parameter of all the datasets 1 503 0 002 fell in the allowable interval 2 2 2 2 geroge and mallery 2003 nie 1975 and had a platykurtic distribution with respect to kurtosis 0 besides the skewness also fell in a reasonable range 0 143 0 891 for a ml based predictive model in this study the upstream wetting radius of emitter for sloping lands r the downstream wetting radius of emitter for slopping lands r and the wetting depth d were simultaneously measured for each treatment with specified discharge slope and soil type the details of the measured parameters are described in solat et al 2021 3 theoretical overview of the computational approaches 3 1 light gradient boosting method lightgbm feature selection the feature selection strategy can enhance model performance minimize training time and prevent overfitting to obtain these benefits the lightgbm feature selection data filtering strategy was implemented as the first stage of the preprocessing microsoft introduced lightgbm in 2017 as an ml technique using a gradient boosting decision tree gbdt ke et al 2017 it potentially reduces memory usage to 1 8 of the original supports distributed processing and effectively handles batch data it has a high parallel training efficiency and speed yao et al 2022 lightgbm provides two innovative strategies to cope with input predictors with a number of features such as exclusive feature bundling and gradient based one side sampling lightgbm primarily employs the histogram algorithm and the leaf wise algorithmto improve the algorithm s computational capacity lightgbm utilizes the histogram technique to integrate mutually discordant characteristics the basic idea is to discretize the continuous feature values into m integers prior to creating an m width histogram to locate the decision tree the data are explored based on the discretized values of the histogram due to the decision tree s weak model and the histogram approach s large reduction in time complexity this fuzzy partitioning strategy typically produces better outcomes than the decision tree wang et al 2022 the leaf wise technique with depth limitation entails choosing the leaf with the greatest gain to divide and loop at each split simultaneously by restricting the depth of the tree and the number of leaves the model s complexity is minimized and overfitting is avoided the gradient boosting technique is adopted to train the lightgbm model and its hyperparameters are optimized using the grid search technique the hyperparameters of the lightgbm model optimized by the grid search approach include the number of weak regression trees the number of leaves the learning rate and the number of iterations ren et al 2022 3 2 best subset regression bsr wrapper optimizer the bsr is a model selection approach that tests all possible combinations of the predictor variables from the first preprocessing stage and then selects the best model according to statistical criteria jović et al 2015 for k independent variables the optimal subset selection can be summarized as follows 1 consider all possible models with one variable two variables up to k variables 2 select the best model of size 1 the best model of size 2 and the best model of size k 3 select the best overall model among the finalists the optimal subset selection ultimately selects one model out of 2 k alternative models the statistical criteria including mallow s coefficient c p kobayashi and sakata 1990 mean square error mse adjusted r 2 akaike s information criterion aic akaike 1974 and determination coefficient r 2 were used for the best subset selection cp and aic are respectively given by sushanth et al 2023 1 c p rss k mse m 2 k n m k 2 aic 2 p n l n 1 n i 1 n e i 2 where p denotes the number of the model s parameters k denotes the number of variables n is the number of data e i is the residual and rss k is the residual sum of squares the lowest c p mse and aic values represent the best model performance in this research the bsr is implemented for the second preprocessing stage 3 3 bidirectional recurrent neural network bi rnn to circumvent the restrictions of a standard rnn schuster and paliwal 1997 developed a bi rnn which was trained on all data from the past and future of a single time frame the goal of bi rnn is to partition a standard rnn s state neurons into two parts one for time going forward forward states and one for time going backward backward states jaihuni et al 2022 the outputs of forward states are not coupled to the inputs of backward states and vice versa fig 3 illustrates a simple bi rnn structure this structure is reduced to a normal unidirectional forward rnn without the backward states because the two state neurons do not contact the bi rnn can be trained like a regular unidirectional rnn and unfolded into a generic feedforward network 3 4 elman recurrent neural network ernn multilayer perceptron mlp neural networks are often utilized for classification modeling and prediction gao et al 2019 sabour et al 2022 cagcag yolcu et al 2021 an mlp consists of an input layer an output layer and one or more hidden layers mlp can be classified as feed forward neural networks ffnn and recurrent neural networks rnn ffnns are artificial neural networks which are simpler than rnns in that the connections between units do not create a cycle elman proposed the elman neural network enn elman 1990 enns are two layer backpropagation networks featuring a feedback link between the output and input of the hidden layer this relationship is referred to as the context nodes the hidden nodes are turned on by both the input units and the context nodes the hidden nodes then feed forward to turn on the output nodes the hidden nodes also trigger the context nodes through feedback fig 3 depicts a typical topology of ernn ernn has a local memory unit and a local feedback connection making it a type of recurrent neural network specifically it has four layers input hidden transport and output the network s transport layer may convey status information and is equipped with memory enabling enn to make more accurate predictions enn is ideally suited for tackling issues involving series and time series data the ernn s dynamics can be mathematically expressed as liu et al 2015b 3 x h k 1 s w c x c k 1 w 1 u k 4 x c k 1 w r x h 5 y k 1 w 0 x h k 1 where x c and x h denote the context nodes and hidden layers u k and y k are the input and output of the network at discrete time k w 0 w 1 w c and w r are the weight matrices for the hidden output input hidden context hidden and hidden context layers respectively and s is a hyperbolic tangent function the quick bp method is utilized for the training of ernn 3 5 bagging regression tree bgrt the ensemble regression tree algorithm which is an integrated learning algorithm provides greater self learning capability and precision than a single regression tree method peng et al 2023 one of the first ensemble learning methods is bagging i e bootstrap aggregating breiman 1996 the bagging technique helps minimize the decision tree s variance hajian et al 2022 this method takes a training dataset and makes several bootstrap samples from it then classifiers or base learners are trained on the new learning sets before their predictions are averaged for a numerical output or a plurality vote is done for a class outcome the bagging regression tree bgrt model is constructed in the new training data set using the regression tree technique and the expected output value of each regression tree is obtained the bgrt technique uses the average of all regression tree models output projected values as its final value as a result the bgrt algorithm s mathematical model can be expressed as peng et al 2023 6 h a x s i g n h i x where h a x is the result of the bgrt and h i x is the result of individual regression tree i bagging is used to decrease variation and prevent the overfitting of a learning procedure the primary hyperparameters for constructing a bgrt model including the number of trees and the number of splits for each tree are optimized when the number of regression trees is insufficient the accuracy of the bgrt model decreases too many regression trees will result in excessive computation but the performance of the bgrt model will not increase the hyperparameters are optimized by the grid search method fig 3 portrays the flowchart of the bgrt method 3 6 evaluation indicators four statistical indices including r correlation coefficient kge kling gupta efficiency gupta et al 2009 u95 uncertainty coefficient with 95 confidence level and rmse root mean square error were utilized to evaluate the performances of the models the four metrics are given by 7 r i 1 n wdp obs i wdp obs wdp pred i wdp pred i 1 n wdp obs i wdp obs 2 i 1 n wdp pred i wdp pred 2 8 rmse 1 n i 1 n wdp obs i wdp pred i 2 9 kge 1 r 1 2 α 1 2 β 1 2 10 u 95 1 96 std 2 rmse 2 wdp obs i and wdp pred i are the ith observed and predicted wetting distribution pattern wdp values respectively wdp obs and wdp pred are the averages of the observed and predicted wdp values respectively n is the number of data points and std is the standard deviation moreover α represents the comparative variation of the expected and observed values whereas β is the ratio of the predicted and observed mean values for a perfect model performance r and kge equal 1 whereas rmse and u 95 equal 0 4 interpretation of hybrid model components the proposed multi filtering based deep learning lightgbm was integrated with bsr and the three advanced ml models i e bi rnn ernn and bgrt to estimate the dimensions of moisture bulb or wdp on sloping lands under drip irrigation for three scenarios r upstream wetting radius r downstream wetting radius and d wetting depth according to the experiments nine predictors were selected to estimate the wdp they included the elapsed time time emitter outflow rate q saturated hydraulic conductivity ks initial soil moisture content θ soil surface slope s percentage of sand sand percentage of silt silt percentage of clay clay and bulk density ρb one of the simplest methods of examining the behavior of the input datasets versus the target is linear correlation based on the pearson coefficient rp which is shown statistically by a correlogram fig 4 shows the linear correlation of all the predictors attained by the experiments against three targets i e r r and d it is clear that the time with the highest rp 0 80 0 79 and 0 86 for r r and d scenarios respectively is the most influential predictor followed by q and ks however in the scenario of r except for the time the linear correlation between r and other predictors is significantly low the above analysis confirms the necessity of further investigation using a feature selection with the ability to support non linear relationships jamei et al 2022a one of the novelties of this study is the establishment of an advanced data filtering strategy to achieve the best accuracy of the ml models thus a novel tree based feature selection lightgbm is proposed to specify the most effective parameters for constructing candidate input combinations for each scenario in an effort to improve the accuracy and reduce the computational cost fig 5 clearly reveals the optimal predictors in each scenario using three sun plots with the important factors related to all the preditors listed under the plots according to the importance factor values the elapsed time emitter outflow rate q ks θ and s are selected among the nine exisitng predictors for model construction for the three scenarios the order of the filtered predictors is time θ s q ks sand ρb clay silt for scenario r time θ s q ks clay silt sand ρb for scenario r and time θ q s ks ρb silt sand clay for scenario d using the bsr method three optimal combinations c1 c2 and c3 are extracted from n selected predictors during the second preprocessing stage instead of testing 2 n possible combinations this technique allows to extract the best combinations based on mse r2 ad r2 cp and aic if r and ad r2 are high and mse cp and aic are low the input combination order is deemed superior according to table 3 the bsr method s criteria show that the best combination in scenario r has three variables c1 time q s the best combination in scenario r has five variables c3 time q ks θ s and the best combination in scenario d has four variables c2 time q ks s the final combinations for each scenario are limited to 3 5 predictors from a total of nine variables resulting in far fewer options this demonstrates the efficacy of the new data filtering framework based on the powerful non linear feature selection method however the optimum input combinations selected by the bsr approach are specified by the bold font table 3 it is required to test the potential of the three existing combinations by feeding the ml models in the next section the hyperparameter tuning process is one of the most important aspects of ai based modeling if reasonable attention is not paid to this step the nature of the study as a whole may change jamei et al 2022c random search is an efficient and popular strategy to find optimal hyperparameters it has a search space containing different predefined hyperparameter combinations this method is useful for quick and efficient finding of the optimal hyperparameter combination in a high dimensional search space in the present study random search is utilized to tune the hyperparameters and the tuned hyperparameters of all the ml models are listed in table 4 the main hyperparameters of bi rnn as the main model are the number of layers number of neurons and learning rate the learning curves loss function versus epoch values associated with the bi rnn models for the three scenarios i e r r and d considering all the input features are shown in fig 6 the loss functions in all the deep learning models are defined based on the mse metric it can be observed that with a smaller epoch 200 the bi rnn model for scenario r exhibits faster covergence than the other scenarios in the training phase to avoid overfitting the k fold cross validation is implemented using k 5 folds the k fold cross validation splits the training set into k folds each split uses k 1 folds of the training data to train a model malik et al 2022 the remaining fold is used to validate the model for each split the held out fold is scored splits scores in both training and testing stages are averaged to describe the accuracy of each stage equation 11 is used in this study to scale the inputs and targets in the scenarios linearly to a range of 0 1 jamei et al 2022a 11 x i n x i x m i n x m a x x m i n where x i denotes the input vector and x m i n and x m a x are the scaled versions of the measured data s minimum and maximum respectively the schematic road map of the modeling stages for estimating the wdp in the sloping lands is exhibited in fig 7 5 results and analysis based on the input combinations combo 1 combo 2 and combo 3 the performances of the bi rnn ernn and bgrt models for predicting wdp for scenarios r r and d were evaluated by using r rmse kge and u95 table 5 shows that for the input combination combo 3 the bi rnn model accomplished the highest degree of precision to predict wdp for scenario r r 0 9920 rmse 1 2441 kge 0 9916 and u95 3 4556 in the testing period followed by ernn r 0 9900 rmse 1 3983 kge 0 9711 and u95 3 8825 bgrt with combo 2 yielded the best result r 0 9711 rmse 2 6283 kge 0 8479 and u95 7 2965 overall bi rnn with combo 3 appeared to be the best model for scenario r which also indicated that combo 3 was better for prediction than combo 1 and combo 2 according to table 5 bi rnn for combo 3 attained better accuracy r 0 9910 rmse 0 9207 kge 0 9777 and u95 2 5418 to predict wdp for scenario r than ernn and bgrt the bgrt model with combo 2 provided a better prediction of wdp for scenario r overall bi rnn and combo 3 provided better predictions for scenario r than ernn and bgrt with combo 1 and combo 2 for scenario d in table 5 bi rnn again outperformed ernn and bgrt with combo 3 r 0 9948 rmse 0 9813 kge 0 9917 and u95 2 7223 in the prediction of wdp in the testing period the results also revealed that combo 3 was a better input combination than combo 1 and combo 2 as shown in table 5 bi rnn with combo 3 acquired better accuracy in terms of goodness of fit to predict wdps for scenarios r r and d than ernn and bgrt fig 8 displays the comparisons of the wdp prediction results from the three ml models i e bi rnn ernn and bgrt with three different input combinations i e combo 1 combo 2 and combo 3 in the form of stacked plots using r rmse kge and u95 for scenarios r r and d it can be observed that bi rnn combo 3 generated the best accuracy in terms of these statistical metrics followed by ernn combo 3 and bgrt combo 2 for the three scenarios as shown in fig 8 combo 3 was the best input combination to accomplish the highest degree of precision for bi rnn ernn and bgrt followed by combo 2 and then combo 1 in the prediction of wdp for scenarios r r and d fig 9 shows an overview of the comparison based on combo 1 combo 2 and combo 3 among the bi rnn ernn and bgrt models between the predicted estimated and observed wdp in terms of scatter plots right for r scenario the bi rnn based on combo 3 with rcombo 3 0 9920 shows better performance followed by ernn rcombo 3 0 9900 and bgrt rcombo 2 0 9711 similarly the bi rnn demonstrates the best precision with rcombo 3 0 9910 against ernn and bgrt models to predict wdp for r lastly the birnn with combo 3 again obtained better prediction in terms of wdp for the d scenario with rcombo 3 0 9948 which is slightly better than ernn and bgrt models fig 10 shows the competence and effectiveness of the bi rnn ernn and bgrt models against the observed wdps for combo 1 combo 2 and combo 3 using the boxplots and probability distribution functions in scenarios r r and d the wdps predicted by combo 3 bi rnn are more closely matching the observed values and their probability distributions for all three scenarios in contrast the boxplots for ernn and bgrt with combo 1 and 2 for scenarios r r and d are dispersed exhibiting outliers with larger probability distribution function values thus the bi rnn model with combo 3 provided more accurate predictions of wdp fig 11 shows the plots of the physical expected behavior relative errors re and residuals of the wdps simulated by bi rnn ernn and bgrt with combo 3 for the three scenarios i e r r and d the wdps acquired by the bi rnn model red color appeared to be more stable and accurate along with lower re and residuals in contrast the predictions from ernn and bgrt show slightly large fluctuations and spikes in comparison with the observed wdps as well as the maximum re and residuals in scenarios r r and d overall bi rnn achieved the highest precision based on the re and residual plots fig 12 shows the histograms and smooth kernel distribution functions of the relative frequency of absolute relative errors re of the wdps simulated by bi rnn ernn and bgrt based on combo 3 for scenarios r r and d the frequency distributions of re for the bi rnn model ranged in a smaller error bracket from 0 to 3 5 for scenario r followed by ernn 0 to 4 0 and bgrt 0 to 14 in the prediction of wdp again bi rnn appeared to be reasonably good with re varying from 0 to 30 for scenario r and from 0 to 30 in scenario d fig 12 also shows the overall better precision of the proposed bi rnn model with combo 3 for scenarios r r and d to predict wdps table 6 and fig 13 show the external validation results of bi rnn ernn and bgrt based on four statistics k kp m and n it can be observed that the bi rnn model acquired better and consistent external validation results for the prediction of wdps k 0 9998 kp 0 9970 m 0 0162 n 0 0162 for scenario r k 1 0031 kp 0 993 m 0 0182 n 0 0180 for scenario r and k 0 9944 kp 1 0031 m 0 0104 n 0 0105 for scenario d ernn is reasonably better than bgrt in the external validation for all three scenarios overall the bi rnn model surpassed both ernn and bgrt in the prediction of wdps 6 further discussion in this research a novel modeling framework was developed based on the lightgbm and bsr algorithms hybridized with the bi rnn deep learning model to precisely estimate wdps for three scenarios i e r r d the bi rnn model was compared with the ernn and bgrt models as well as two other models nlr non linear regression and hydrus solat et al 2021 their performances were evaluated by using different statistical metrics demonstrating the better accuracy of the hybrid bi rnn model fig 14 displays spider diagrams in terms of r and rmse for the bi rnn ernn bgrt nlr and hydrus models to simulate wdps for scenarios r r and d the bi rnn model exhibits a bigger spider web based on the r values than the four other models indicating the highest precision of the simulations for scenarios r r and d in addition the spider web of bi rnn based on the rmse values is smaller than those from all other models which again confirms the highest accuracy of bi rnn to estimate the wdps for scenarios r r and d to further assess the performance of the intelligent framework the uncertainty quantification was investigated based on the model structure input combinations affecting the model training and the uncertainty from the experimental datasets herein the u factor is a benchmark criterion based on the mean 95 uncertainty band extracted by using the resampling bootstrap distribution strategy rousselet 2020 the resampling bootstrap strategy replaces multiple samples from an individual random sample to compute the mean 95 confidence band the u factor can be defined as 12 u factor mean 95 confidence band std ob where st d ob is the standard deviation of the observed wdp values in each scenario the least u f a c t o r an uncertainty benchmark criterion is related to scenario d followed by scenarios r and r the uncertainty factor was computed for all the input combinations or scenarios to assess the impacts of the input combinations and model structure concerning the effect of the input combinations as shown in fig 15 combo 3 and combo 2 in all scenarios r r and d had the least uncertainty in terms of the model structure bi rnn and ernn yielded the minimum uncertainties also analyzing the uncertainty results upon dual perspectives of the input combinations and the model structure shows that the model structure had a higher impact than input combinations this finding highlights the enhancement of the predictive framework by implementing intelligent data filtering based modeling of wdp also the red dash lines in the heat maps of fig 15 indicate the optimal paths in both uncertainty assessment directions fig 16 depicts the 95 confidence band for bi rnn with combo 3 as the superior case in the testing phase scenario d achieved the least uncertainty u factor 0 148 followed by scenario r and scenario r it should be noted that the experiments of the current research showed the role of human and laboratory equipment errors in the uncertainty related to the extracted data used in the modeling however the quantitative experimental uncertainty analysis coleman and steele 2018 1995 is a specialized topic that is beyond the scope of this study and can be addressed in the future research the mechanism and the modeling structure proposed in this study have several advantages over others for example the lightgbm feature selection strategy can enhance the model performance minimize the training time and prevent overfitting this is why the lightgbm was implemented as a data filtering strategy in the first stage the lightgbm provides two innovative strategies to cope with input predictors with a number of features such as exclusive feature bundling and gradient based one side sampling the best subset regression bsr wrapper optimizer is a model selection approach to test all possible combinations of the input predictor variables in the second stage and then select the best input combination based on the statistical criteria the bsr selects three best possible input combinations using the n selected input predictors in the first stage finally the bi rnn processes the inputs in both forward and reverse time orders which allows bi rnn to examine future context as well bi rnn helps in analyzing the future events by not limiting the model s learning to the past and present bi rnn tries to learn high level features from the data in an incremental manner which eliminates the need of domain expertise and hard core feature extraction moreover the results demonstrate that the proposed model structure yields the minimum uncertainties the proposed modelling strategy has been developed in a way that it requires a smaller number of input predictors which is particularly significant and plays a vital role in the prediction of wdp for scenarios r r and d in this study for example the lightgbm feature selection model ranked and selected the best and optimal input features only instead of using all features this strategy enhanced the model prediction accuracy and computational efficiency and avoided overfitting additionally the bsr model is working to select the best and optimal set combination of the input features which further improves the model s precision and reduces the computational time the accuracy of the proposed model is significantly higher than that of all other models used in this study as shown in table 5 fig 8 and fig 9 the proposed modeling strategy to estimate wdps demonstrated that the data driven model developed in this study can be applied to solve other problems in hydrology water resource and other relevant areas to assist government agencies for better decision making the proposed strategy was efficient and effective to develop a deep learning model i e hybrid bi rnn for wdp estimation to further improve the relevant studies following aspects can be considered in the future efforts satellite data can be a feasible alternative to the ground based data to improve the performance of the models due to the restriction of deep learning models that depend on the black box nature it is hard and challenging to understand the relationship between the inputs and target outputs during the learning stage thus the integration of deep learning and numerical weather prediction models can be a good choice to coin a new research area moreover the hybrid bi rnn and the counterpart models in this study can also be enriched by integrating with bayesian model averaging sloughter et al 2010 and ensemble approaches tiwari and chatterjee 2011 to cope with the model uncertainties 7 conclusion and remarks in this research the dimensions of moisture bulb or wetting distribution patterns wdp were estimated and assessed experimentally and computationally for the first time in sloping lands for this purpose 486 sets of experiments were conducted and the data collected included the elapsed time emitter outflow rate saturated hydraulic conductivity initial soil moisture content soil surface slope percentages of sand silt and clay as well as bulk density particularly upstream wetting radius r downstream wetting radius r and wetting depth d were measured the modeling effort focused on developing a gradient boosting multi level data filtering technique that consisted of the lightgbm and bsr wrapper combination optimizer hybridized with the bi rnn deep learning model for precise simulation of wdps besides the main model i e bi rnn was compared with ernn and bgrt in the hybrid framework for different candidate input combinations namely combo 1 combo 2 and combo 3 with the aim of designing a novel robust modeling system to predict wdps scenarios r r and d the performance assessment using a set of statistical metrics and diagnostic plots indicated that bi rnn was the most suitable and accurate model to achieve the optimum precision for combo 1 combo 2 and combo 3 the bi rnn model with combo 3 was superior in the prediction of wdp r 0 9920 rmse 1 2441 kge 0 9916 and u95 3 4556 for scenario r r 0 9910 rmse 0 9207 kge 0 9777 and u95 2 5418 for scenario r and r 0 9948 rmse 0 9813 kge 0 9917 and u95 2 7223 for scenario d besides the uncertainty analyses for the three scenarios in two directions i e input combinations and model structure revealed that the data filtering based pre processing lightgbm feature selection coupled with bsr effectively reduced the uncertainty and sensitivity associated with the effect of input combinations the new methods developed in this study can be further extended and applied to solve the problems in other areas such as environmental science hydrology agriculture and renewable energy credit authorship contribution statement mehdi jamei conceptualization formal analysis visualization software project administration bakhtiar karimi conceptualization visualization investigation mumtaz ali validation methodology fariba alinazari methodology masoud karbasi investigation validation eisa maroufpoor investigation xuefeng chu supervision writing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper 
2023,since 1999 large scale ecological restoration activities have been launched across china s loess plateau known as the grain for green program which has substantially changed the landscape as well as the physicochemical properties of soil including saturated hydraulic conductivity ks an essential parameter in hydrological cycle this could also contribute to the reduction of water and sediment in the yellow river in this century however there is little study about the changes in root zone upper one meter ks profiles at large scale quantitative description of the revegetation impacts on ks is needed to assess the impacts of revegetation on local water cycle in this research 2665 soil samples at six depths within root zone 0 10 10 20 20 30 30 50 50 70 70 100 cm from four land use types forestland shrubland grassland and sparse grassland at 25 different sites across the central loess plateau were collected and analyzed our results suggest that revegetation has significantly improved ks values with the largest increment in the near surface layers enlarging the vertical heterogeneity of ks the parameters of vertical profiles of ks were correlated to vegetation index ndvi especially for ks and its decay rate at surface and the correlations under well vegetated land use types collapsed on the same line the change of ks due to revegetation was also correlated to the change of ndvi this contrasting ks amplified by revegetation could lead to a shift of runoff generation mechanism in the loess plateau our results could provide support for estimation of local water usage and availability changes caused by revegetation shedding light on the vegetation restoration implementation in the loess plateau keywords saturated hydraulic conductivity revegetation land use land cover change normalized difference vegetation index ndvi runoff generation mechanism data availability data will be made available on request 1 introduction in 2021 the united nations launched the decade on ecosystem restoration 2021 2030 a globally collaborative project to prevent halt and reverse the degraded ecosystems united nations 2021 van hoek dijke et al 2022 given the considerable potential of carbon sequestration in forest ecosystems revegetation has become one of the most primary and effective nature based solutions for carbon neutrality around the world as well griscom et al 2017 bastin et al 2019 efforts have been made to restore fragile ecosystem worldwide such as the grain for green program gfgp across china s loess plateau feng et al 2016 the riparian forest restoration project in brazil s atlantic rainforest wuethrich 2007 and the great green wall of africa surrounding the sahara desert cernansky 2018 to alleviate soil erosion control desertification and improve the agricultural productivity in the loess plateau one of the most intensively eroded areas in the world china implemented the largest current afforestation activities the grain for green program in 1999 feng et al 2016 fu et al 2017 this program has effectively changed the regional land use patterns and almost doubled the vegetation coverage from 31 6 in 1999 to 59 6 in 2013 chen et al 2015 water and sediment yield have also reduced significantly since this program chen et al 2015 bryan et al 2018 taking the tongguan hydrological station as example the annual average runoff decreased from about 26 0 billion m3 in 1986 1999 to about 22 65 billion m3 in 2000 2017 and the annual average sediment load declined from about 773 million tons in 1986 1999 to approximately 242 million tons in 2000 2017 chen et al 2021 analysis suggested that from 2000 to 2010 26 of the water yield reduction and 21 of the sediment load reduction in the yellow river could be attributed to large scale vegetation restoration wang et al 2016b revegetation influences local water cycle through its effects on evapotranspiration infiltration runoff generation and etc which could further impact soil erosion as well as sediment and solutes transport huang and shao 2019 studies have shown that revegetation could substantially change the physicochemical properties of soil profiles gould et al 2016 coban et al 2022 plant roots and fungal mycelia can help form aggregates and stabilize soil structure through abundant branches harris 2009 biotic processes such as animal activities growth and decomposition of large roots and microbial decomposition could increase the content of soil organic matter and change the porosity bulk density structure and texture of the soil these could alter the soil hydraulic characteristics hardie et al 2014 coban et al 2022 zhu et al 2022 effectively change infiltration process and regulate the replenishment of soil water storage resulting in the change of runoff generation patterns eventually wilcox and huang 2010 ghimire et al 2013 ran et al 2019 one of the most influential soil characteristics in hydrological processes is saturated hydraulic conductivity ks thompson et al 2010 assouline 2013 mirus and loague 2013 studies have been conducted to investigate its change with revegetation such as vegetation cover land use type restoration age etc previous measurements found that surface ks 0 10 cm usually increased after revegetation bonell et al 2010 perkins et al 2012 jarvis et al 2013 zhang et al 2013 lozano baez et al 2019 lopes et al 2020 zhang et al 2021b bai et al 2022 while the change of ks below a certain depth e g 50 cm may be limited zimmermann et al 2006 hassler et al 2011 peng et al 2012 yuksek 2012 wang et al 2013 gu et al 2019 bai et al 2022 zhu et al 2022 however this declining trend in vertical profile was not necessarily observed in places with ongoing human activities i e overgrazing collecting vegetation litter where ks could increase with depth zhang et al 2013 or firstly increase and then decrease ghimire et al 2013 in the early stage of revegetation the surface ks would increase every year this increment tended to become gentle afterwards gradually reaching a steady state zhao et al 2013 ren et al 2016 wu et al 2016 in contrast to the apparent increase in surface ks 0 10 cm changes in ks of the upper soil depths were limited in the process of vegetation restoration li and shao 2006 gu et al 2019 despite of the change in ks it would still be difficult to recover from degraded land by ecosystem restoration in short term bonell et al 2010 chandrasoma et al 2016 yang et al 2017 lozano baez et al 2019 decades or even a century may be required to reach the original natural state li and shao 2006 hassler et al 2011 peng et al 2012 field experiments and soil sampling analyses suggest that the recovery of ks is correlated with land use types or vegetation types of restoration some studies found that forestland and shrubland could more preferably elevate ks than grassland gonzalez sosa et al 2010 yao et al 2013 for grassland the ones with tap root systems had larger ks than those with fibrous root systems zhu et al 2020 study on a forest successional series from abandoned land to climax evergreen broad leaved forest in eastern china showed that ks increased from abandoned land to shrubland and eventually reached maximum under mature evergreen broad leaved forest during the recovery peng et al 2012 soil samples from a gradient of dry tropical woody vegetation in nicaragua revealed that leaf area index lai had greater correlation to surface ks than soil texture livestock impact and soil porosity niemeyer et al 2014 these changes could be attributed to the more abundant aboveground biomass litter amount and root systems of different vegetation types leading to the improvement of soil hydraulic properties fischer et al 2015 zhu et al 2020 in contrast there are also researches showing that the change of near surface ks was not necessarily correlated with land use type for example the increase of ks caused by grassland restoration could be more significant than by shrubland or forestland yang et al 2017 hao et al 2020 zhang et al 2020 experiments from nine study plots suggested that top layers ks 0 20 cm of shrubland was larger than ks of grassland and forestland and the ks of grassland was also higher than that of forestland with relatively low coverage chen et al 2018 soil samples across the loess plateau presented close correlation between surface ks and land use as well as vegetation coverage while the subsurface ks soil depth at 20 25 cm was not closely related to vegetation coverage wang et al 2013 a study of two neighboring basins indicated that increase of vegetation did not necessarily improve surface soil near saturated hydraulic conductivity in the early stage of revegetation zhao et al 2014 some studies have also tried to use distribution models to fit the vertical profiles of ks to characterize the vertical heterogeneity such as the logarithmic model zhu et al 2022 exponent distribution yao et al 2013 wang et al 2018 and polynomial fitting yao et al 2013 tian et al 2017 for example zhu et al 2022 described the decreasing trend of ks with depth using one logarithmic model fitted for different vegetation types for studies with many samples of ks profiles different fitting models could be applied in a single study yao et al 2013 tian et al 2017 causing difficulties in the comparisons among ks profiles most of the studies focused only on surface ks i e 0 10 cm in a small catchment i e 20 km2 ziegler and giambelluca 1998 hu et al 2009 gonzalez sosa et al 2010 perkins et al 2012 jarvis et al 2013 ren et al 2016 lozano baez et al 2019 hao et al 2020 analyses of the change in the vertical profile of ks were mostly within the top 40 cm which was usually simply divided into surface layer and subsurface layer li and shao 2006 zimmermann et al 2006 hassler et al 2011 yuksek 2012 yang et al 2017 gu et al 2019 zhang et al 2020 thus the characteristics of the vertical profiles of root zone the top 1 m soil layer babaeian et al 2019 were less described or quantified only a few works studied the change of ks in relatively deep soil layers i e 50 cm peng et al 2012 ghimire et al 2013 yao et al 2013 chandrasoma et al 2016 zhu et al 2022 most of which were conducted in catchments smaller than 10 km2 while studies with large scale soil sampling were mainly concentrated on the ks of upper soil layers wang et al 2013 zhang et al 2017 zhang et al 2021b bai et al 2022 although there were many researches about the impacts of land use type on ks few of them quantified the correlation between revegetation and its impacts on ks profiles at large scale in this work we collected soil samples from four different land used types forestland shrubland grassland and sparse grassland across the regions that used to have the most severe soil erosion issues contributing the largest amount of sediment load to the yellow river and have been through substantial vegetation restoration since 1999 the soil profiles were extended to one meter to take into account of the change of ks in the root zone the goals of this study were to 1 investigate the vertical profiles of ks under different land use types across the study region 2 estimate the change of ks profiles since the implementation of the grain for green program in 1999 and 3 explore the correlation between the characteristics of ecosystem restoration i e land use types vegetation index and restoration age etc and the ks profiles 2 material and methods 2 1 study area the loess plateau 33 23 n 41 17 n 100 24 e 114 37 e covering an area of approximately 640 000 km2 has the thickest loess deposit in the world fu et al 2017 classified as arid and semi arid zone for most areas the loess plateau is featured by temperate continental monsoon climate with multi year average precipitation fluctuating around 400 mm and mean annual potential evaporation over 1000 mm fu et al 2017 the precipitation decreases from southeast to northwest across the loess plateau and concentrates between june and september xiao 2014 zhang et al 2016 the soil texture varies with environmental conditions from northwest to southeast in the order of sandy loam light loam medium loam and heavy loam with sand content decreasing from 50 to 5 approximatively chen et al 2008 fu et al 2017 farmland grassland and forestland are three dominant land use types in the loess plateau in the last three decades li et al 2016 with the implementation of gfgp land use land cover patterns across the loess plateau have changed drastically forestland and grassland coverage have increased by 4 9 and 6 6 respectively while farmland decreased by 10 8 between 2000 and 2008 lü et al 2012 2 2 soil sampling soil samples were collected across the mid yellow river basin fig 1 a in the center of the loess plateau covering three typical loess landforms loess yuan loess liang loess mao of the loess plateau fu et al 2017 this region has once suffered severe ecological degradation and is the main contributor of coarse sediment in the yellow river zhang et al 2016 fu et al 2017 since the implementation of gfgp the dominant land use type has shifted from farmland to grassland at most sampling sites and the vegetation density has improved substantially the normalized difference vegetation index ndvi has increased by at least 30 at most sampling sites fig 2 25 sampling sites were selected from the main tributaries in the mid yellow river basin fig 1a which used to contribute most of the coarse sediment in the yellow river and have been through significant vegetation restoration fig 2 the number of sampling sites was based on the drainage area of these tributaries to ensure a certain degree of distribution as well as to cover a range of climate and vegetation variation at each sampling site samples were collected from four land use types which were usually around 100 m apart fig 1b sparse grassland sgd grassland gd shrubland sd and forestland fd sparse grassland was cropland abandoned in recent years with limited vegetation coverage i e at least 40 of it was bare soil both human intervention and vegetation impacts were limited at sparse grassland thus sparse grassland was used as reference rather than cropland to minimize anthropogenic impacts and to represent the soil condition at the early stage of revegetation the representative vegetation species for the other three well vegetated land use types were robinia pseudoacacia l and prunus armeniaca l for forestland hippophae rhamnoides linn and caragana korshinkii kom for shrubland agropyron cristatum l gaertn and artemisia sacrorum ledeb for grassland for each land use we collected five duplicated soil profiles at spots that were about 50 m apart to reduce the uncertainties in sampling six soil samples were collected for each soil profile at 0 10 cm 10 20 cm 20 30 cm 30 50 cm 50 70 cm and 70 100 cm depths field sampling was carried out from may to september between 2018 and 2021 the undisturbed soil samples were collected by steel cylinder with a volume of approximately 98 cm3 height 5 cm diameter 5 cm the geographic location topography land use type plant species and restoration age were photographed and recorded in total 2665 undisturbed soil samples were collected for 463 soil profiles 97 of sgd 125 of gd 118 of sd 123 of fd at 25 sampling sites 2 3 laboratory measurement the constant head method based on darcy s law was used to measure ks klute and dirksen 1986 ilek and kucza 2014 tian et al 2017 the saturated hydraulic conductivity k s t mm h with experimental water temperature t was calculated using eq 1 and then converted to ks mm h at the temperature of 10 according to eq 2 1 k s t 600 q l aht 2 ks k s t 1 359 1 0 0337 t 0 00022 t 2 where q cm3 is the volume of water permeating in time interval t min l is the height of the soil sample in the steel cylinder 5 cm a is the cross sectional area of the soil sample 19 63 cm2 and h cm is the difference of hydraulic potential between the top and bottom of soil sample 2 4 data collection the digital elevation model dem with 90 m resolution of the loess plateau and the map of yellow river were obtained from loess plateau sub center national earth system science data center https loess geodata cn sand content distribution with 1 km resolution of the loess plateau was provided by data center for resources and environmental sciences chinese academy of sciences resdc https www resdc cn landsat normalized difference vegetation index ndvi of 1998 and 2019 with 30 m spatial resolution and 16 day temporal resolution were downloaded from the united states geological survey earth resources observation and science center https www usgs gov landsat missions landsat normalized difference vegetation index masek et al 2006 vermote et al 2016 landsat ndvi of 1998 and 2019 were chosen as 1998 was the year before the implementation of gfgp while 2019 was during our sampling period the difference of ndvi between 2019 and 1998 was considered as the change of vegetation density caused by the ecosystem restoration the maximum value composites method was used to obtain the representative annual ndvi values for each pixel indicating the highest vegetation cover and productivity ye et al 2019 chen et al 2020 this method can reduce the influences on data quality caused by band missing and cloud cover 2 5 data process and statistical analysis grubbs test was applied to identify outliers in the sampled ks grubbs 1969 tian et al 2017 erkuş and purutçuoğlu 2021 46 soil samples were identified as outliers and removed leaving 2619 soil samples for further analysis accounting for 98 27 of the total effective samples the duplicated soil profiles taken at same land use were then averaged to get the representative soil profile for each land use type at 25 sampling sites that is 96 representative soil profiles with mean ks values at 0 10 cm 10 20 cm 20 30 cm 30 50 cm 50 70 cm and 70 100 cm depths kolmogorov smirnov test was used to assess normality lognormality lilliefors 1967 zhang et al 2020 the results indicated that ks followed lognormal distribution in general so ks values were transformed to log10 ks when conducting the following statistical analysis wang et al 2013 one way anova test was then performed to the data sets assuming normal distributions and homoscedasticity for comparison across different groups acutis et al 2012 since the soil depth and land use type were considered as the main influential factors here the samples were grouped by these two factors for anova test the differences among all the groups of land use types soil depths were tested in anova test when the global null hypothesis of one way anova had been rejected at the significance level 0 05 as a follow up to anova fisher s least significant difference test lsd test was applied to further examine the difference between the means of any two groups shaffer 1995 cabral 2008 tian et al 2017 2 6 logistic function the logistic function was used to fit the vertical profiles of ks from our soil samples this model had been demonstrated for its reliable performance in fitting nonlinear relationship mohseni et al 1998 archontoulis and miguez 2015 its curve exhibits a monotonic trend with maximum slope at the inflection point which is fixed at ksat axis in this study fig s1 the logistic function was defined below 3 k s z c a c 1 e x p b z where ks z is the estimated saturated hydraulic conductivity at depth z mm h z is the soil depth cm here we used the mean depth of each layer i e 5 cm for 0 10 cm depth coefficient a is the hypothetically estimated maximum value of ks and b is the shape parameter of this logistic distribution model which portrays the decline trend of ks coefficient c was added to the original logistic function to make sure ks was larger than zero representing the minimum value of the estimated ks mohseni et al 1998 it could be considered as the ks of deep soil depth 100 cm and would be referred as kb in this study given the physical meanings of coefficients a b and c they were all limited above zero as we can see from the schematic diagram of logistic function fig s1 according to eq 3 we could derive two more parameters 4 k 0 1 2 a c 5 b 0 tan θ 1 4 b a c where k0 mm h is when z 0 that is the estimated surface ks at depth 0 cm b0 mm h cm is the steepest slope of this curve which could be considered as the decay rate of ks at surface nonlinear least square method was used for regression which was evaluated by the nash sutcliffe efficiency nse nash and sutcliffe 1970 mohseni et al 1998 moriasi et al 2007 schaefli and gupta 2007 at each sampling site we collected soil profiles from four different land use types sparse grassland grassland shrubland and forestland because the four land use types were of similar topography and geographically close to each other around 100 m apart the soil conditions before revegetation were assumed the same for the four land use types and the sparse grassland was considered as control group approximating the soil condition at the early stage of revegetation the difference between the other three well vegetated land use types and sparse grassland were assumed to be the impacts of revegetation the difference of ndvi between 1998 and 2019 δndvi was calculated to represent the level of revegetation at each sampling spot assuming the soil conditions before revegetation were the same for the four land use types close to each other the major differences among different sampling spots were from the land use land cover change the ratio of k0 and b0 between the other three well vegetated land use types and sparse grassland at the same site k0 b0 were calculated as the change in ks profiles due to revegetation 6 k 0 k 0 i k 0 sgd 7 b 0 b 0 i b 0 sgd where i indicates other three well vegetated land use types grassland shrubland and forestland and sgd refers to sparse grassland 3 results 3 1 the mean ks profiles and its spatial distribution fig 3 presents the vertical profiles of ks averaged for each land use type as we can see all of the four mean ks profiles show a decreasing trend with depth ks of the upper layers 0 30 cm was much greater than that of deep layers 30 100 cm which may be ascribed to the frequent biotic activities in the shallow soil the decrease rate was large within the upper 50 cm as we expected the mean ks profile of forestland fd was the largest while that of sparse grassland sgd was the smallest this difference was more significant in the upper 50 cm soil layers the mean ks values of shrubland sd were similar with those of grassland gd in the upper 50 cm but were larger in the soil layers between 50 and 100 cm in general the vertical heterogeneity of ks was gradually enhanced between layers from sparse grassland to forestland within each land use type the variation of ks among different soil profiles was also larger in the upper 30 cm soil than in the lower 70 cm especially for forestland shrubland and grassland the variability reduced quickly in depth below 30 cm particularly the top surface layer 0 10 cm showed the most significant variance of ks indicating considerable spatial heterogeneity across the sampling sites the map of surface ks 0 10 cm displayed in fig 4 provides a more direct picture of the spatial heterogeneity of ks as we can see sampling sites in the north and south of our study region had relatively larger surface ks than the sites in the middle of our study region this pattern existed for all the four land use types and was more apparent for the three well vegetated land use fig 4b d this may be attributed to the vegetation impacts and soil texture differences the sand content was much higher at the northern sampling sites fig s2 since soil with higher sand content tend to have larger ks garcía gutiérrez et al 2018 surface ks of the northern sites was larger than those at the middle region when ndvi was similar although the sand content in the southern region was relatively low the vegetation coverage ndvi was much higher fig 2b leading to the larger surface ks similar with the mean values shown in fig 3 the surface ks values in almost all sampling sites of the three well vegetated land use types were much larger than those from sparse grassland where most of the ks values were 70 mm h with low spatial heterogeneity the surface ks values of forestland which mostly ranged from 50 to 300 mm h were significantly higher than those of grassland and shrubland which were mainly between 30 and 200 mm h and 40 250 mm h respectively in the southern part of the study region where the ndvi values were also high fig 2b the surface ks of grassland shrubland and forestland could exceed 200 mm h the spatial heterogeneity was also larger for forestland ks values as seen in fig 3 3 2 differences of ks among depths and land use types fig 5 compares ks values among different soil depths for the four land use types based on fisher s lsd test at significance level of 0 05 as we can see the ks values of the upper two layers 0 20 cm were significantly larger than those of the three lower layers 30 100 cm for all four land use types despite of the drop from the first layer 0 10 cm to the second layer 10 20 cm there was no statistically significant difference between these two layers except in the shrubland although the mean ks of the first layer 0 10 cm was almost twice of that of second layer 10 20 cm in forestland however there was no statistically significant difference between these two layers probably due to the larger variance among sampling sites the decline between second layer 10 20 cm and third layer 20 30 cm was gradual showing no significant difference between the two layers yet the difference between first layer and third layer was statistically significant below 30 cm depth decrease in ks was limited with no significant difference among the lower three layers 30 50 cm 50 70 cm 70 100 cm fig 6 compares ks among four land use types at each soil depth there was statistically significant difference of ks among the four land use types in the upper 70 cm while no significant difference was observed below 70 cm p 0 05 specifically ks values of the three well vegetated land use types were significantly larger than those of the sparse grassland in the upper 30 cm for the ks values of soil between 30 cm and 70 cm there was no significant difference between grassland and sparse grassland yet the ks values of shrubland and forestland were still substantially larger than those of sparse grassland note that these comparisons were based on the anova test and fisher s lsd test among different land use types and soil depths the results could be improved with a two factor anova where the site would be a random factor 3 3 regional patterns of logistic parameters 3 3 1 performance of logistic distribution fitting fig 7 presents the histogram of the nses of 96 averaged ks profiles fitted by logistic distribution model the model showed good performance with about two thirds of soil profiles having nse 0 8 the model performed better in most soil profiles with monotonically declining trend yet failed in soil profiles if the maximum ks appeared in the middle layer due to the coupling effect of various factors such as gravel rotten roots animal burrows and so on here we selected soil profiles with nse 0 5 for further analysis moriasi et al 2007 ritter and muñoz carpena 2013 geng et al 2021 zhu et al 2022 that is 80 logistic distribution of ks profiles specifically sparse grassland accounted for 17 profiles grassland 22 profiles shrubland 21 profiles and forestland 20 profiles fig s3 among these 80 cases 75 profiles presented decreasing trend with depth 5 cases showed increasing ks with depth both of them revealed the soil profiles in current vegetation restoration along with anthropogenic disturbance and would be discussed in following discussion 3 3 2 characteristic parameters of the logistic distribution model fig 8 illustrates the values of four characteristic parameters derived from the logistic distribution model the estimated surface ks at depth 0 cm k0 the estimated ks of deep soil kb the decay rate of ks at depth 0 cm b0 and the overall trend of ks b statistical test suggested that there was no significant difference among the four land use types for parameter b and parameter kb p 0 05 that is the change in ks at deep soil layers i e 100 cm and the overall trend of ks profiles were limited under different land use types fig 8b d this is consistent with the findings in fig 6 there was no significant difference of ks among the four land use types when soil depth was 70 cm on the other hand k0 and b0 which represented the saturated hydraulic conductivity and its decay rate at soil surface 0 cm exhibited significant difference between sparse grassland and the three well vegetated land use types p 0 05 fig 8a c this is also consistent with the findings in fig 6 the difference among the control group sgd and the three well vegetated land use types were more significant in the upper layers 0 30 cm hence further analysis of ks change under different land use types will focus on these two parameters k0 and b0 3 4 land use and ks the restoration age obtained at sampling spots were usually about the whole sampling site making it difficult to narrow down to each soil profile fig s4 our records suggested that there was relatively good positive correlation between ndvi and restoration age fig s5 since the spatial resolution of ndvi 30 m is higher than the distance between sampling spots 50 m ndvi was used as surrogate for the vegetation difference among sampling spots fig 9 shows the relationship between the four characteristic parameters of the fitted ks profiles and ndvi in 2019 as we can see all three parameters k0 b0 and b had positive correlation with ndvi p 0 05 except kb particularly k0 and b0 were significantly correlated with ndvi p 0 001 that is both the value of ks and its decay rate at soil surface increased with vegetation density fig 9 also suggests that the soil profiles from the four land use types all collapsed on the same regression line following similar trend between ndvi and the parameters with most points of sparse grassland distributed below the regression curve fig 9a c the correlation between these four characteristic parameters k0 b0 kb b and restoration age showed similar trend as with ndvi but more scattered partly due to the coarse resolution of restoration age fig s6e h similar trend was also observed in the comparison between ndvi and the measured ks at 0 10 cm fig s7 3 5 ks response to land use change the difference of ndvi between 1998 and 2019 δndvi was calculated to represent the level of revegetation considering the sparse grassland as control group sgd to gd sgd to sd and sgd to fd were regarded as the change caused by three different land use change patterns fig 10 presents the scatterplots between the change of ks and its decay rate at soil surface k0 and b0 and ndvi in 2019 as well as δndvi as shown in fig 10 most of k0 were larger than one that is ks increased after revegetation despite of restoration age or vegetation type k0 was positively correlated with ndvi in 2019 p 0 01 and three different vegetation types all collapsed on the the same regression line fig 10a yet the correlation between k0 and δndvi was relatively weak p 0 1 fig 10b that is the increase in ks was more correlated with the current vegetation level than with the change of vegetation coverage on the other hand b0 had a positive correlation with both ndvi and the change of ndvi δndvi and had a more statistically significant correlation with the latter one fig 10c d that is the change of the decay rate at soil surface b0 increased with current vegetation level as well as the revegetation level the more intensive the revegetation was the larger the difference between ks of soil surface and upper soil in other words the larger the vertical heterogeneity was in the ks profile 4 discussion 4 1 revegetation impacts on the vertical profiles of ks and runoff generation since the samples from four land use types at each location were close with each other it can be assumed that the original soil conditions e g soil texture saturated hydraulic conductivity etc were similar before revegetation using the sparse grassland as control group representing the soil condition at the early stage of revegetation the difference between the other three land use types and the sparse grassland could be considered as revegetation impacts on soil property i e ks our measurements suggested that all of the three vegetation types could effectively increase ks in the top one meter root zone figs 3 5 the closer to the soil surface the larger the increase in ks specifically ks values of grassland could be substantially elevated in the top 30 cm at significance level of 0 05 when compared with the control group sparse grassland the depth of elevated ks values would extend to the top 70 cm in shrubland and forestland fig 6 the increases in ks were similar in the top 30 cm soil in grassland and shrubland yet shrubs were able to further elevate ks in deep soil 30 100 cm figs 3 and 6 the larger increment in ks values near surface may be explained by the abundant roots and insects in the top soil resulting in relatively large porosity in the near surface and enhancing soil conductivity gould et al 2016 coban et al 2022 the difference in the increment of ks values in deep soil may be attributed to vegetation species distribution of root density vertical distribution of soil texture soil chemistry properties illuviation biotic activities and so on schenk and jackson 2002 gonzalez sosa et al 2010 ghimire et al 2013 wang et al 2013 with the increase in ks in the top soil layers the vertical heterogeneity of ks was amplified from the original soil condition due to revegetation the decline in vertical profiles of ks in sparse grassland was gradual there was only significant difference between 0 and 20 cm and 30 100 cm fig 5 when the land use type changed into grassland ks values of the top soil were substantially elevated fig 4b further deepening the divergence between 0 and 20 cm and 30 100 cm fig 5 besides ks of the middle layer 20 30 cm also increased and became significantly larger than the bottom layers 50 100 cm when the land use type changed into shrubland and forestland not only the top 30 cm but the top 70 cm soil had become more conductive in infiltration comparing with the original soil condition fig 6 this may be attributed to the difference of biomass and root distribution of vegetation types gould et al 2016 wang et al 2016a with the increase of depth the weakened impacts of roots and insect burrows along with the compaction of overlying soil layers made ks of deeper soil tend to be constant gradually due to the limited rainfall and the thickness of loess in our study region it was difficult for soil to reach saturation horton runoff used to be the dominant runoff generation mechanism in our study region kang et al 2001 liu et al 2012 runoff was generated under heavy rainfall with short duration while most proportion of the soil profile in the catchment remained unsaturated bonell et al 2010 ghimire et al 2013 qiao et al 2017 however the large scale revegetation may change the runoff generation pattern in our study region liu et al 2012 ran et al 2019 with the increase in vegetation coverage more rainfall would be intercepted by canopy attenuating the direct splash of raindrops on surface soil and reducing soil crusting which would hinder infiltration vaezi et al 2017 ran et al 2018 secondly the significantly improved ks of the near surface layer would facilitate rainfall infiltration once entering the soil the large difference between ks values of upper and lower soil layers would generate a relatively impermeable layer and lead to water accumulation in the upper soil ghimire et al 2013 tian et al 2017 as a result perched water table would be developed in the upper soil leading to dunne overland flow as the upper soil reached saturation that is the runoff generation mechanism would shift from infiltration excess to saturation excess the saturation excess runoff generated by contrasting ks values has been identified in previous studies zimmer and mcglynn 2017 peterson et al 2019 zhang et al 2021a our preliminary analysis in an experimental catchment in the loess plateau has also suggested this switch of runoff generation mechanism hong 2020 more analyses of large scale hydrological observations are needed for further validation besides increased vegetation coverage could enhance transpiration which may lead to reduction in soil water storage lv et al 2019 in short the increase in ks and the divergence in the vertical profile of ks due to revegetation would inevitably affect the local water cycle which would further affect soil erosion and sediment transport ran et al 2019 resulting in the change of water sediment relationship across the loess plateau and yellow river basin wang et al 2017 4 2 the influential factor of revegetation impacts on ks our measurements suggested that revegetation could substantially enhance hydraulic conductivity yet the elevation in ks did not necessarily increase from grassland to shrubland and to forestland fig 8 instead of land use types vegetation index ndvi was a more direct indicator of revegetation impacts on the characteristic parameters of ks profile i e k0 b0 b the three characteristic parameters all positively correlated with ndvi the correlations of different land use types all collapsed into one line figs 9 and 10 that is though forests could elevate ks more than shrubs and grasses in general fig 6 ks from young forests with less vegetation density was smaller than ks from well developed shrubland and grassland this may help explain the findings in the literature indicating that increment of ks in grassland could be larger than that in shrubland or forestland yang et al 2017 hao et al 2020 zhang et al 2020 and that ks of grassland could be larger than that of forestland for the higher vegetation coverage of grassland chen et al 2018 thus the vegetation coverage density which could be represented by ndvi could be a better indicator of revegetation impacts on ks than the vegetation types there have been studies indicated that though the grain for green program in the loess plateau have substantially increased the regional vegetation coverage it has also exacerbated the local water shortage feng et al 2016 han et al 2020 as forests and shrubs transpired more water than the original sparse grassland and grassland it increased water demand and exacerbated water shortage in dry region like the loess plateau ren et al 2018 ma et al 2020 therefore it is necessary to assess the water capacity of the loess plateau to balance the social and environmental water demand liang et al 2019 our results indicated that based on the vegetation index ndvi that was regularly measured we could estimate the revegetation impacts on soil i e ks that was difficult to acquire at large scale which could then be used for hydrological simulations of catchment water usage this could also be conducted backwards given the water availability for vegetation we could back calculate the ndvi that optimizes the water resources allocation li 2021 the optimal ndvi could then be used as guide for land use type and or vegetation type selection as well as the scale and density of revegetation the progress of revegetation could also be adjusted based on the temporal development of ndvi 5 conclusions in this study we collected 2665 soil samples across the region contributing most of the coarse sediment in the yellow river the vertical profiles 0 100 cm of ks from four typical land use types sparse grassland grassland shrubland and forestland were derived across the study region considering the sparse grassland as reference of original soil condition at the initial stage of revegetation the impacts of revegetation on ks profiles were investigated our results suggest that the vertical heterogeneity of ks profile was limited in the original soil sparse grassland while revegetation activities have substantially elevated the whole profile of ks especially for the upper 30 cm in general ks values of grassland were significantly larger than sparse grassland in the upper 30 cm while this increase in ks could further extend to the upper 70 cm in shrubland and forestland our analysis also shows that the characteristic parameters of ks profile were positively correlated with ndvi under well vegetated land use types particularly the surface ks k0 and its decay rate b0 besides the change of these two parameters were also correlated with the change of ndvi regardless of their land use types these changes in ks amplified the vertical heterogeneity of ks profiles and could generate transient perched water table in the upper soil due to the drastic drop in ks leading to saturation in the upper soil as a result the runoff generation mechanism may switch from horton overland flow to dunne overland flow altering the water cycle in the loess plateau although our preliminary analysis in an experimental catchment has suggested this switch more evidences are needed from soil moisture and streamflow measurements to validate this hypothesis our results also show that the ks values which were difficult to measure and with great heterogeneity had statistically significant correlation with ndvi which has been regularly measured for a long time that is we could derive ks profiles based on the regularly measured ndvi to simulate the change of hydrological processes caused by revegetation this could provide scientific estimation for the potential water usage of designed vegetation restoration plan as well as the capacity of revegetation given local water availability shedding light on the vegetation restoration implementation in the loess plateau credit authorship contribution statement hailong pan conceptualization methodology formal analysis investigation visualization writing original draft qihua ran conceptualization supervision funding acquisition writing review editing yanyan hong methodology investigation jin wang investigation xiuxiu chen investigation sheng ye conceptualization supervision methodology formal analysis writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the work was funded by the national natural science foundation of china grant numbers 51979243 52009120 and national key research and development program of china grant number 2019yfc1510701 01 we are grateful to partners from xi an university of technology for their support and assistance in field sampling appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2023 129337 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2023,since 1999 large scale ecological restoration activities have been launched across china s loess plateau known as the grain for green program which has substantially changed the landscape as well as the physicochemical properties of soil including saturated hydraulic conductivity ks an essential parameter in hydrological cycle this could also contribute to the reduction of water and sediment in the yellow river in this century however there is little study about the changes in root zone upper one meter ks profiles at large scale quantitative description of the revegetation impacts on ks is needed to assess the impacts of revegetation on local water cycle in this research 2665 soil samples at six depths within root zone 0 10 10 20 20 30 30 50 50 70 70 100 cm from four land use types forestland shrubland grassland and sparse grassland at 25 different sites across the central loess plateau were collected and analyzed our results suggest that revegetation has significantly improved ks values with the largest increment in the near surface layers enlarging the vertical heterogeneity of ks the parameters of vertical profiles of ks were correlated to vegetation index ndvi especially for ks and its decay rate at surface and the correlations under well vegetated land use types collapsed on the same line the change of ks due to revegetation was also correlated to the change of ndvi this contrasting ks amplified by revegetation could lead to a shift of runoff generation mechanism in the loess plateau our results could provide support for estimation of local water usage and availability changes caused by revegetation shedding light on the vegetation restoration implementation in the loess plateau keywords saturated hydraulic conductivity revegetation land use land cover change normalized difference vegetation index ndvi runoff generation mechanism data availability data will be made available on request 1 introduction in 2021 the united nations launched the decade on ecosystem restoration 2021 2030 a globally collaborative project to prevent halt and reverse the degraded ecosystems united nations 2021 van hoek dijke et al 2022 given the considerable potential of carbon sequestration in forest ecosystems revegetation has become one of the most primary and effective nature based solutions for carbon neutrality around the world as well griscom et al 2017 bastin et al 2019 efforts have been made to restore fragile ecosystem worldwide such as the grain for green program gfgp across china s loess plateau feng et al 2016 the riparian forest restoration project in brazil s atlantic rainforest wuethrich 2007 and the great green wall of africa surrounding the sahara desert cernansky 2018 to alleviate soil erosion control desertification and improve the agricultural productivity in the loess plateau one of the most intensively eroded areas in the world china implemented the largest current afforestation activities the grain for green program in 1999 feng et al 2016 fu et al 2017 this program has effectively changed the regional land use patterns and almost doubled the vegetation coverage from 31 6 in 1999 to 59 6 in 2013 chen et al 2015 water and sediment yield have also reduced significantly since this program chen et al 2015 bryan et al 2018 taking the tongguan hydrological station as example the annual average runoff decreased from about 26 0 billion m3 in 1986 1999 to about 22 65 billion m3 in 2000 2017 and the annual average sediment load declined from about 773 million tons in 1986 1999 to approximately 242 million tons in 2000 2017 chen et al 2021 analysis suggested that from 2000 to 2010 26 of the water yield reduction and 21 of the sediment load reduction in the yellow river could be attributed to large scale vegetation restoration wang et al 2016b revegetation influences local water cycle through its effects on evapotranspiration infiltration runoff generation and etc which could further impact soil erosion as well as sediment and solutes transport huang and shao 2019 studies have shown that revegetation could substantially change the physicochemical properties of soil profiles gould et al 2016 coban et al 2022 plant roots and fungal mycelia can help form aggregates and stabilize soil structure through abundant branches harris 2009 biotic processes such as animal activities growth and decomposition of large roots and microbial decomposition could increase the content of soil organic matter and change the porosity bulk density structure and texture of the soil these could alter the soil hydraulic characteristics hardie et al 2014 coban et al 2022 zhu et al 2022 effectively change infiltration process and regulate the replenishment of soil water storage resulting in the change of runoff generation patterns eventually wilcox and huang 2010 ghimire et al 2013 ran et al 2019 one of the most influential soil characteristics in hydrological processes is saturated hydraulic conductivity ks thompson et al 2010 assouline 2013 mirus and loague 2013 studies have been conducted to investigate its change with revegetation such as vegetation cover land use type restoration age etc previous measurements found that surface ks 0 10 cm usually increased after revegetation bonell et al 2010 perkins et al 2012 jarvis et al 2013 zhang et al 2013 lozano baez et al 2019 lopes et al 2020 zhang et al 2021b bai et al 2022 while the change of ks below a certain depth e g 50 cm may be limited zimmermann et al 2006 hassler et al 2011 peng et al 2012 yuksek 2012 wang et al 2013 gu et al 2019 bai et al 2022 zhu et al 2022 however this declining trend in vertical profile was not necessarily observed in places with ongoing human activities i e overgrazing collecting vegetation litter where ks could increase with depth zhang et al 2013 or firstly increase and then decrease ghimire et al 2013 in the early stage of revegetation the surface ks would increase every year this increment tended to become gentle afterwards gradually reaching a steady state zhao et al 2013 ren et al 2016 wu et al 2016 in contrast to the apparent increase in surface ks 0 10 cm changes in ks of the upper soil depths were limited in the process of vegetation restoration li and shao 2006 gu et al 2019 despite of the change in ks it would still be difficult to recover from degraded land by ecosystem restoration in short term bonell et al 2010 chandrasoma et al 2016 yang et al 2017 lozano baez et al 2019 decades or even a century may be required to reach the original natural state li and shao 2006 hassler et al 2011 peng et al 2012 field experiments and soil sampling analyses suggest that the recovery of ks is correlated with land use types or vegetation types of restoration some studies found that forestland and shrubland could more preferably elevate ks than grassland gonzalez sosa et al 2010 yao et al 2013 for grassland the ones with tap root systems had larger ks than those with fibrous root systems zhu et al 2020 study on a forest successional series from abandoned land to climax evergreen broad leaved forest in eastern china showed that ks increased from abandoned land to shrubland and eventually reached maximum under mature evergreen broad leaved forest during the recovery peng et al 2012 soil samples from a gradient of dry tropical woody vegetation in nicaragua revealed that leaf area index lai had greater correlation to surface ks than soil texture livestock impact and soil porosity niemeyer et al 2014 these changes could be attributed to the more abundant aboveground biomass litter amount and root systems of different vegetation types leading to the improvement of soil hydraulic properties fischer et al 2015 zhu et al 2020 in contrast there are also researches showing that the change of near surface ks was not necessarily correlated with land use type for example the increase of ks caused by grassland restoration could be more significant than by shrubland or forestland yang et al 2017 hao et al 2020 zhang et al 2020 experiments from nine study plots suggested that top layers ks 0 20 cm of shrubland was larger than ks of grassland and forestland and the ks of grassland was also higher than that of forestland with relatively low coverage chen et al 2018 soil samples across the loess plateau presented close correlation between surface ks and land use as well as vegetation coverage while the subsurface ks soil depth at 20 25 cm was not closely related to vegetation coverage wang et al 2013 a study of two neighboring basins indicated that increase of vegetation did not necessarily improve surface soil near saturated hydraulic conductivity in the early stage of revegetation zhao et al 2014 some studies have also tried to use distribution models to fit the vertical profiles of ks to characterize the vertical heterogeneity such as the logarithmic model zhu et al 2022 exponent distribution yao et al 2013 wang et al 2018 and polynomial fitting yao et al 2013 tian et al 2017 for example zhu et al 2022 described the decreasing trend of ks with depth using one logarithmic model fitted for different vegetation types for studies with many samples of ks profiles different fitting models could be applied in a single study yao et al 2013 tian et al 2017 causing difficulties in the comparisons among ks profiles most of the studies focused only on surface ks i e 0 10 cm in a small catchment i e 20 km2 ziegler and giambelluca 1998 hu et al 2009 gonzalez sosa et al 2010 perkins et al 2012 jarvis et al 2013 ren et al 2016 lozano baez et al 2019 hao et al 2020 analyses of the change in the vertical profile of ks were mostly within the top 40 cm which was usually simply divided into surface layer and subsurface layer li and shao 2006 zimmermann et al 2006 hassler et al 2011 yuksek 2012 yang et al 2017 gu et al 2019 zhang et al 2020 thus the characteristics of the vertical profiles of root zone the top 1 m soil layer babaeian et al 2019 were less described or quantified only a few works studied the change of ks in relatively deep soil layers i e 50 cm peng et al 2012 ghimire et al 2013 yao et al 2013 chandrasoma et al 2016 zhu et al 2022 most of which were conducted in catchments smaller than 10 km2 while studies with large scale soil sampling were mainly concentrated on the ks of upper soil layers wang et al 2013 zhang et al 2017 zhang et al 2021b bai et al 2022 although there were many researches about the impacts of land use type on ks few of them quantified the correlation between revegetation and its impacts on ks profiles at large scale in this work we collected soil samples from four different land used types forestland shrubland grassland and sparse grassland across the regions that used to have the most severe soil erosion issues contributing the largest amount of sediment load to the yellow river and have been through substantial vegetation restoration since 1999 the soil profiles were extended to one meter to take into account of the change of ks in the root zone the goals of this study were to 1 investigate the vertical profiles of ks under different land use types across the study region 2 estimate the change of ks profiles since the implementation of the grain for green program in 1999 and 3 explore the correlation between the characteristics of ecosystem restoration i e land use types vegetation index and restoration age etc and the ks profiles 2 material and methods 2 1 study area the loess plateau 33 23 n 41 17 n 100 24 e 114 37 e covering an area of approximately 640 000 km2 has the thickest loess deposit in the world fu et al 2017 classified as arid and semi arid zone for most areas the loess plateau is featured by temperate continental monsoon climate with multi year average precipitation fluctuating around 400 mm and mean annual potential evaporation over 1000 mm fu et al 2017 the precipitation decreases from southeast to northwest across the loess plateau and concentrates between june and september xiao 2014 zhang et al 2016 the soil texture varies with environmental conditions from northwest to southeast in the order of sandy loam light loam medium loam and heavy loam with sand content decreasing from 50 to 5 approximatively chen et al 2008 fu et al 2017 farmland grassland and forestland are three dominant land use types in the loess plateau in the last three decades li et al 2016 with the implementation of gfgp land use land cover patterns across the loess plateau have changed drastically forestland and grassland coverage have increased by 4 9 and 6 6 respectively while farmland decreased by 10 8 between 2000 and 2008 lü et al 2012 2 2 soil sampling soil samples were collected across the mid yellow river basin fig 1 a in the center of the loess plateau covering three typical loess landforms loess yuan loess liang loess mao of the loess plateau fu et al 2017 this region has once suffered severe ecological degradation and is the main contributor of coarse sediment in the yellow river zhang et al 2016 fu et al 2017 since the implementation of gfgp the dominant land use type has shifted from farmland to grassland at most sampling sites and the vegetation density has improved substantially the normalized difference vegetation index ndvi has increased by at least 30 at most sampling sites fig 2 25 sampling sites were selected from the main tributaries in the mid yellow river basin fig 1a which used to contribute most of the coarse sediment in the yellow river and have been through significant vegetation restoration fig 2 the number of sampling sites was based on the drainage area of these tributaries to ensure a certain degree of distribution as well as to cover a range of climate and vegetation variation at each sampling site samples were collected from four land use types which were usually around 100 m apart fig 1b sparse grassland sgd grassland gd shrubland sd and forestland fd sparse grassland was cropland abandoned in recent years with limited vegetation coverage i e at least 40 of it was bare soil both human intervention and vegetation impacts were limited at sparse grassland thus sparse grassland was used as reference rather than cropland to minimize anthropogenic impacts and to represent the soil condition at the early stage of revegetation the representative vegetation species for the other three well vegetated land use types were robinia pseudoacacia l and prunus armeniaca l for forestland hippophae rhamnoides linn and caragana korshinkii kom for shrubland agropyron cristatum l gaertn and artemisia sacrorum ledeb for grassland for each land use we collected five duplicated soil profiles at spots that were about 50 m apart to reduce the uncertainties in sampling six soil samples were collected for each soil profile at 0 10 cm 10 20 cm 20 30 cm 30 50 cm 50 70 cm and 70 100 cm depths field sampling was carried out from may to september between 2018 and 2021 the undisturbed soil samples were collected by steel cylinder with a volume of approximately 98 cm3 height 5 cm diameter 5 cm the geographic location topography land use type plant species and restoration age were photographed and recorded in total 2665 undisturbed soil samples were collected for 463 soil profiles 97 of sgd 125 of gd 118 of sd 123 of fd at 25 sampling sites 2 3 laboratory measurement the constant head method based on darcy s law was used to measure ks klute and dirksen 1986 ilek and kucza 2014 tian et al 2017 the saturated hydraulic conductivity k s t mm h with experimental water temperature t was calculated using eq 1 and then converted to ks mm h at the temperature of 10 according to eq 2 1 k s t 600 q l aht 2 ks k s t 1 359 1 0 0337 t 0 00022 t 2 where q cm3 is the volume of water permeating in time interval t min l is the height of the soil sample in the steel cylinder 5 cm a is the cross sectional area of the soil sample 19 63 cm2 and h cm is the difference of hydraulic potential between the top and bottom of soil sample 2 4 data collection the digital elevation model dem with 90 m resolution of the loess plateau and the map of yellow river were obtained from loess plateau sub center national earth system science data center https loess geodata cn sand content distribution with 1 km resolution of the loess plateau was provided by data center for resources and environmental sciences chinese academy of sciences resdc https www resdc cn landsat normalized difference vegetation index ndvi of 1998 and 2019 with 30 m spatial resolution and 16 day temporal resolution were downloaded from the united states geological survey earth resources observation and science center https www usgs gov landsat missions landsat normalized difference vegetation index masek et al 2006 vermote et al 2016 landsat ndvi of 1998 and 2019 were chosen as 1998 was the year before the implementation of gfgp while 2019 was during our sampling period the difference of ndvi between 2019 and 1998 was considered as the change of vegetation density caused by the ecosystem restoration the maximum value composites method was used to obtain the representative annual ndvi values for each pixel indicating the highest vegetation cover and productivity ye et al 2019 chen et al 2020 this method can reduce the influences on data quality caused by band missing and cloud cover 2 5 data process and statistical analysis grubbs test was applied to identify outliers in the sampled ks grubbs 1969 tian et al 2017 erkuş and purutçuoğlu 2021 46 soil samples were identified as outliers and removed leaving 2619 soil samples for further analysis accounting for 98 27 of the total effective samples the duplicated soil profiles taken at same land use were then averaged to get the representative soil profile for each land use type at 25 sampling sites that is 96 representative soil profiles with mean ks values at 0 10 cm 10 20 cm 20 30 cm 30 50 cm 50 70 cm and 70 100 cm depths kolmogorov smirnov test was used to assess normality lognormality lilliefors 1967 zhang et al 2020 the results indicated that ks followed lognormal distribution in general so ks values were transformed to log10 ks when conducting the following statistical analysis wang et al 2013 one way anova test was then performed to the data sets assuming normal distributions and homoscedasticity for comparison across different groups acutis et al 2012 since the soil depth and land use type were considered as the main influential factors here the samples were grouped by these two factors for anova test the differences among all the groups of land use types soil depths were tested in anova test when the global null hypothesis of one way anova had been rejected at the significance level 0 05 as a follow up to anova fisher s least significant difference test lsd test was applied to further examine the difference between the means of any two groups shaffer 1995 cabral 2008 tian et al 2017 2 6 logistic function the logistic function was used to fit the vertical profiles of ks from our soil samples this model had been demonstrated for its reliable performance in fitting nonlinear relationship mohseni et al 1998 archontoulis and miguez 2015 its curve exhibits a monotonic trend with maximum slope at the inflection point which is fixed at ksat axis in this study fig s1 the logistic function was defined below 3 k s z c a c 1 e x p b z where ks z is the estimated saturated hydraulic conductivity at depth z mm h z is the soil depth cm here we used the mean depth of each layer i e 5 cm for 0 10 cm depth coefficient a is the hypothetically estimated maximum value of ks and b is the shape parameter of this logistic distribution model which portrays the decline trend of ks coefficient c was added to the original logistic function to make sure ks was larger than zero representing the minimum value of the estimated ks mohseni et al 1998 it could be considered as the ks of deep soil depth 100 cm and would be referred as kb in this study given the physical meanings of coefficients a b and c they were all limited above zero as we can see from the schematic diagram of logistic function fig s1 according to eq 3 we could derive two more parameters 4 k 0 1 2 a c 5 b 0 tan θ 1 4 b a c where k0 mm h is when z 0 that is the estimated surface ks at depth 0 cm b0 mm h cm is the steepest slope of this curve which could be considered as the decay rate of ks at surface nonlinear least square method was used for regression which was evaluated by the nash sutcliffe efficiency nse nash and sutcliffe 1970 mohseni et al 1998 moriasi et al 2007 schaefli and gupta 2007 at each sampling site we collected soil profiles from four different land use types sparse grassland grassland shrubland and forestland because the four land use types were of similar topography and geographically close to each other around 100 m apart the soil conditions before revegetation were assumed the same for the four land use types and the sparse grassland was considered as control group approximating the soil condition at the early stage of revegetation the difference between the other three well vegetated land use types and sparse grassland were assumed to be the impacts of revegetation the difference of ndvi between 1998 and 2019 δndvi was calculated to represent the level of revegetation at each sampling spot assuming the soil conditions before revegetation were the same for the four land use types close to each other the major differences among different sampling spots were from the land use land cover change the ratio of k0 and b0 between the other three well vegetated land use types and sparse grassland at the same site k0 b0 were calculated as the change in ks profiles due to revegetation 6 k 0 k 0 i k 0 sgd 7 b 0 b 0 i b 0 sgd where i indicates other three well vegetated land use types grassland shrubland and forestland and sgd refers to sparse grassland 3 results 3 1 the mean ks profiles and its spatial distribution fig 3 presents the vertical profiles of ks averaged for each land use type as we can see all of the four mean ks profiles show a decreasing trend with depth ks of the upper layers 0 30 cm was much greater than that of deep layers 30 100 cm which may be ascribed to the frequent biotic activities in the shallow soil the decrease rate was large within the upper 50 cm as we expected the mean ks profile of forestland fd was the largest while that of sparse grassland sgd was the smallest this difference was more significant in the upper 50 cm soil layers the mean ks values of shrubland sd were similar with those of grassland gd in the upper 50 cm but were larger in the soil layers between 50 and 100 cm in general the vertical heterogeneity of ks was gradually enhanced between layers from sparse grassland to forestland within each land use type the variation of ks among different soil profiles was also larger in the upper 30 cm soil than in the lower 70 cm especially for forestland shrubland and grassland the variability reduced quickly in depth below 30 cm particularly the top surface layer 0 10 cm showed the most significant variance of ks indicating considerable spatial heterogeneity across the sampling sites the map of surface ks 0 10 cm displayed in fig 4 provides a more direct picture of the spatial heterogeneity of ks as we can see sampling sites in the north and south of our study region had relatively larger surface ks than the sites in the middle of our study region this pattern existed for all the four land use types and was more apparent for the three well vegetated land use fig 4b d this may be attributed to the vegetation impacts and soil texture differences the sand content was much higher at the northern sampling sites fig s2 since soil with higher sand content tend to have larger ks garcía gutiérrez et al 2018 surface ks of the northern sites was larger than those at the middle region when ndvi was similar although the sand content in the southern region was relatively low the vegetation coverage ndvi was much higher fig 2b leading to the larger surface ks similar with the mean values shown in fig 3 the surface ks values in almost all sampling sites of the three well vegetated land use types were much larger than those from sparse grassland where most of the ks values were 70 mm h with low spatial heterogeneity the surface ks values of forestland which mostly ranged from 50 to 300 mm h were significantly higher than those of grassland and shrubland which were mainly between 30 and 200 mm h and 40 250 mm h respectively in the southern part of the study region where the ndvi values were also high fig 2b the surface ks of grassland shrubland and forestland could exceed 200 mm h the spatial heterogeneity was also larger for forestland ks values as seen in fig 3 3 2 differences of ks among depths and land use types fig 5 compares ks values among different soil depths for the four land use types based on fisher s lsd test at significance level of 0 05 as we can see the ks values of the upper two layers 0 20 cm were significantly larger than those of the three lower layers 30 100 cm for all four land use types despite of the drop from the first layer 0 10 cm to the second layer 10 20 cm there was no statistically significant difference between these two layers except in the shrubland although the mean ks of the first layer 0 10 cm was almost twice of that of second layer 10 20 cm in forestland however there was no statistically significant difference between these two layers probably due to the larger variance among sampling sites the decline between second layer 10 20 cm and third layer 20 30 cm was gradual showing no significant difference between the two layers yet the difference between first layer and third layer was statistically significant below 30 cm depth decrease in ks was limited with no significant difference among the lower three layers 30 50 cm 50 70 cm 70 100 cm fig 6 compares ks among four land use types at each soil depth there was statistically significant difference of ks among the four land use types in the upper 70 cm while no significant difference was observed below 70 cm p 0 05 specifically ks values of the three well vegetated land use types were significantly larger than those of the sparse grassland in the upper 30 cm for the ks values of soil between 30 cm and 70 cm there was no significant difference between grassland and sparse grassland yet the ks values of shrubland and forestland were still substantially larger than those of sparse grassland note that these comparisons were based on the anova test and fisher s lsd test among different land use types and soil depths the results could be improved with a two factor anova where the site would be a random factor 3 3 regional patterns of logistic parameters 3 3 1 performance of logistic distribution fitting fig 7 presents the histogram of the nses of 96 averaged ks profiles fitted by logistic distribution model the model showed good performance with about two thirds of soil profiles having nse 0 8 the model performed better in most soil profiles with monotonically declining trend yet failed in soil profiles if the maximum ks appeared in the middle layer due to the coupling effect of various factors such as gravel rotten roots animal burrows and so on here we selected soil profiles with nse 0 5 for further analysis moriasi et al 2007 ritter and muñoz carpena 2013 geng et al 2021 zhu et al 2022 that is 80 logistic distribution of ks profiles specifically sparse grassland accounted for 17 profiles grassland 22 profiles shrubland 21 profiles and forestland 20 profiles fig s3 among these 80 cases 75 profiles presented decreasing trend with depth 5 cases showed increasing ks with depth both of them revealed the soil profiles in current vegetation restoration along with anthropogenic disturbance and would be discussed in following discussion 3 3 2 characteristic parameters of the logistic distribution model fig 8 illustrates the values of four characteristic parameters derived from the logistic distribution model the estimated surface ks at depth 0 cm k0 the estimated ks of deep soil kb the decay rate of ks at depth 0 cm b0 and the overall trend of ks b statistical test suggested that there was no significant difference among the four land use types for parameter b and parameter kb p 0 05 that is the change in ks at deep soil layers i e 100 cm and the overall trend of ks profiles were limited under different land use types fig 8b d this is consistent with the findings in fig 6 there was no significant difference of ks among the four land use types when soil depth was 70 cm on the other hand k0 and b0 which represented the saturated hydraulic conductivity and its decay rate at soil surface 0 cm exhibited significant difference between sparse grassland and the three well vegetated land use types p 0 05 fig 8a c this is also consistent with the findings in fig 6 the difference among the control group sgd and the three well vegetated land use types were more significant in the upper layers 0 30 cm hence further analysis of ks change under different land use types will focus on these two parameters k0 and b0 3 4 land use and ks the restoration age obtained at sampling spots were usually about the whole sampling site making it difficult to narrow down to each soil profile fig s4 our records suggested that there was relatively good positive correlation between ndvi and restoration age fig s5 since the spatial resolution of ndvi 30 m is higher than the distance between sampling spots 50 m ndvi was used as surrogate for the vegetation difference among sampling spots fig 9 shows the relationship between the four characteristic parameters of the fitted ks profiles and ndvi in 2019 as we can see all three parameters k0 b0 and b had positive correlation with ndvi p 0 05 except kb particularly k0 and b0 were significantly correlated with ndvi p 0 001 that is both the value of ks and its decay rate at soil surface increased with vegetation density fig 9 also suggests that the soil profiles from the four land use types all collapsed on the same regression line following similar trend between ndvi and the parameters with most points of sparse grassland distributed below the regression curve fig 9a c the correlation between these four characteristic parameters k0 b0 kb b and restoration age showed similar trend as with ndvi but more scattered partly due to the coarse resolution of restoration age fig s6e h similar trend was also observed in the comparison between ndvi and the measured ks at 0 10 cm fig s7 3 5 ks response to land use change the difference of ndvi between 1998 and 2019 δndvi was calculated to represent the level of revegetation considering the sparse grassland as control group sgd to gd sgd to sd and sgd to fd were regarded as the change caused by three different land use change patterns fig 10 presents the scatterplots between the change of ks and its decay rate at soil surface k0 and b0 and ndvi in 2019 as well as δndvi as shown in fig 10 most of k0 were larger than one that is ks increased after revegetation despite of restoration age or vegetation type k0 was positively correlated with ndvi in 2019 p 0 01 and three different vegetation types all collapsed on the the same regression line fig 10a yet the correlation between k0 and δndvi was relatively weak p 0 1 fig 10b that is the increase in ks was more correlated with the current vegetation level than with the change of vegetation coverage on the other hand b0 had a positive correlation with both ndvi and the change of ndvi δndvi and had a more statistically significant correlation with the latter one fig 10c d that is the change of the decay rate at soil surface b0 increased with current vegetation level as well as the revegetation level the more intensive the revegetation was the larger the difference between ks of soil surface and upper soil in other words the larger the vertical heterogeneity was in the ks profile 4 discussion 4 1 revegetation impacts on the vertical profiles of ks and runoff generation since the samples from four land use types at each location were close with each other it can be assumed that the original soil conditions e g soil texture saturated hydraulic conductivity etc were similar before revegetation using the sparse grassland as control group representing the soil condition at the early stage of revegetation the difference between the other three land use types and the sparse grassland could be considered as revegetation impacts on soil property i e ks our measurements suggested that all of the three vegetation types could effectively increase ks in the top one meter root zone figs 3 5 the closer to the soil surface the larger the increase in ks specifically ks values of grassland could be substantially elevated in the top 30 cm at significance level of 0 05 when compared with the control group sparse grassland the depth of elevated ks values would extend to the top 70 cm in shrubland and forestland fig 6 the increases in ks were similar in the top 30 cm soil in grassland and shrubland yet shrubs were able to further elevate ks in deep soil 30 100 cm figs 3 and 6 the larger increment in ks values near surface may be explained by the abundant roots and insects in the top soil resulting in relatively large porosity in the near surface and enhancing soil conductivity gould et al 2016 coban et al 2022 the difference in the increment of ks values in deep soil may be attributed to vegetation species distribution of root density vertical distribution of soil texture soil chemistry properties illuviation biotic activities and so on schenk and jackson 2002 gonzalez sosa et al 2010 ghimire et al 2013 wang et al 2013 with the increase in ks in the top soil layers the vertical heterogeneity of ks was amplified from the original soil condition due to revegetation the decline in vertical profiles of ks in sparse grassland was gradual there was only significant difference between 0 and 20 cm and 30 100 cm fig 5 when the land use type changed into grassland ks values of the top soil were substantially elevated fig 4b further deepening the divergence between 0 and 20 cm and 30 100 cm fig 5 besides ks of the middle layer 20 30 cm also increased and became significantly larger than the bottom layers 50 100 cm when the land use type changed into shrubland and forestland not only the top 30 cm but the top 70 cm soil had become more conductive in infiltration comparing with the original soil condition fig 6 this may be attributed to the difference of biomass and root distribution of vegetation types gould et al 2016 wang et al 2016a with the increase of depth the weakened impacts of roots and insect burrows along with the compaction of overlying soil layers made ks of deeper soil tend to be constant gradually due to the limited rainfall and the thickness of loess in our study region it was difficult for soil to reach saturation horton runoff used to be the dominant runoff generation mechanism in our study region kang et al 2001 liu et al 2012 runoff was generated under heavy rainfall with short duration while most proportion of the soil profile in the catchment remained unsaturated bonell et al 2010 ghimire et al 2013 qiao et al 2017 however the large scale revegetation may change the runoff generation pattern in our study region liu et al 2012 ran et al 2019 with the increase in vegetation coverage more rainfall would be intercepted by canopy attenuating the direct splash of raindrops on surface soil and reducing soil crusting which would hinder infiltration vaezi et al 2017 ran et al 2018 secondly the significantly improved ks of the near surface layer would facilitate rainfall infiltration once entering the soil the large difference between ks values of upper and lower soil layers would generate a relatively impermeable layer and lead to water accumulation in the upper soil ghimire et al 2013 tian et al 2017 as a result perched water table would be developed in the upper soil leading to dunne overland flow as the upper soil reached saturation that is the runoff generation mechanism would shift from infiltration excess to saturation excess the saturation excess runoff generated by contrasting ks values has been identified in previous studies zimmer and mcglynn 2017 peterson et al 2019 zhang et al 2021a our preliminary analysis in an experimental catchment in the loess plateau has also suggested this switch of runoff generation mechanism hong 2020 more analyses of large scale hydrological observations are needed for further validation besides increased vegetation coverage could enhance transpiration which may lead to reduction in soil water storage lv et al 2019 in short the increase in ks and the divergence in the vertical profile of ks due to revegetation would inevitably affect the local water cycle which would further affect soil erosion and sediment transport ran et al 2019 resulting in the change of water sediment relationship across the loess plateau and yellow river basin wang et al 2017 4 2 the influential factor of revegetation impacts on ks our measurements suggested that revegetation could substantially enhance hydraulic conductivity yet the elevation in ks did not necessarily increase from grassland to shrubland and to forestland fig 8 instead of land use types vegetation index ndvi was a more direct indicator of revegetation impacts on the characteristic parameters of ks profile i e k0 b0 b the three characteristic parameters all positively correlated with ndvi the correlations of different land use types all collapsed into one line figs 9 and 10 that is though forests could elevate ks more than shrubs and grasses in general fig 6 ks from young forests with less vegetation density was smaller than ks from well developed shrubland and grassland this may help explain the findings in the literature indicating that increment of ks in grassland could be larger than that in shrubland or forestland yang et al 2017 hao et al 2020 zhang et al 2020 and that ks of grassland could be larger than that of forestland for the higher vegetation coverage of grassland chen et al 2018 thus the vegetation coverage density which could be represented by ndvi could be a better indicator of revegetation impacts on ks than the vegetation types there have been studies indicated that though the grain for green program in the loess plateau have substantially increased the regional vegetation coverage it has also exacerbated the local water shortage feng et al 2016 han et al 2020 as forests and shrubs transpired more water than the original sparse grassland and grassland it increased water demand and exacerbated water shortage in dry region like the loess plateau ren et al 2018 ma et al 2020 therefore it is necessary to assess the water capacity of the loess plateau to balance the social and environmental water demand liang et al 2019 our results indicated that based on the vegetation index ndvi that was regularly measured we could estimate the revegetation impacts on soil i e ks that was difficult to acquire at large scale which could then be used for hydrological simulations of catchment water usage this could also be conducted backwards given the water availability for vegetation we could back calculate the ndvi that optimizes the water resources allocation li 2021 the optimal ndvi could then be used as guide for land use type and or vegetation type selection as well as the scale and density of revegetation the progress of revegetation could also be adjusted based on the temporal development of ndvi 5 conclusions in this study we collected 2665 soil samples across the region contributing most of the coarse sediment in the yellow river the vertical profiles 0 100 cm of ks from four typical land use types sparse grassland grassland shrubland and forestland were derived across the study region considering the sparse grassland as reference of original soil condition at the initial stage of revegetation the impacts of revegetation on ks profiles were investigated our results suggest that the vertical heterogeneity of ks profile was limited in the original soil sparse grassland while revegetation activities have substantially elevated the whole profile of ks especially for the upper 30 cm in general ks values of grassland were significantly larger than sparse grassland in the upper 30 cm while this increase in ks could further extend to the upper 70 cm in shrubland and forestland our analysis also shows that the characteristic parameters of ks profile were positively correlated with ndvi under well vegetated land use types particularly the surface ks k0 and its decay rate b0 besides the change of these two parameters were also correlated with the change of ndvi regardless of their land use types these changes in ks amplified the vertical heterogeneity of ks profiles and could generate transient perched water table in the upper soil due to the drastic drop in ks leading to saturation in the upper soil as a result the runoff generation mechanism may switch from horton overland flow to dunne overland flow altering the water cycle in the loess plateau although our preliminary analysis in an experimental catchment has suggested this switch more evidences are needed from soil moisture and streamflow measurements to validate this hypothesis our results also show that the ks values which were difficult to measure and with great heterogeneity had statistically significant correlation with ndvi which has been regularly measured for a long time that is we could derive ks profiles based on the regularly measured ndvi to simulate the change of hydrological processes caused by revegetation this could provide scientific estimation for the potential water usage of designed vegetation restoration plan as well as the capacity of revegetation given local water availability shedding light on the vegetation restoration implementation in the loess plateau credit authorship contribution statement hailong pan conceptualization methodology formal analysis investigation visualization writing original draft qihua ran conceptualization supervision funding acquisition writing review editing yanyan hong methodology investigation jin wang investigation xiuxiu chen investigation sheng ye conceptualization supervision methodology formal analysis writing review editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the work was funded by the national natural science foundation of china grant numbers 51979243 52009120 and national key research and development program of china grant number 2019yfc1510701 01 we are grateful to partners from xi an university of technology for their support and assistance in field sampling appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2023 129337 appendix a supplementary data the following are the supplementary data to this article supplementary data 1 
2024,the classical cubic law deviates from the estimation of the flow rate because of the surface roughness distributed along the fracture surface which can be decomposed as the primary and the secondary roughness a piecewise equivalent model is developed to characterise the relationship between primary and secondary roughness and flow rate inside the fracture setting the fractal dimension 2 0 as the piecewise point the rough walled fracture influenced by the primary and secondary roughness is quantitatively evaluated based on the wavelet transform and the power spectrum method a classification criterion is then proposed to accurately capture the secondary roughness by investigating the non linear flow behavior along rough walled fractures using the lattice boltzmann method the results found that secondary roughness greatly enhances the flow non linearity when the fracture fractal dimension exceeds 2 0 while secondary roughness has little effect on non linear flow behavior with the fracture fractal dimension less than 2 0 the developed piecewise equivalent model shows that the non linear flow pattern is affected by both fracture primary and secondary roughness the flow variation along fractures with identical fracture apertures and primary roughness is greatly related to the influence of the secondary roughness the increase of secondary roughness greatly decreases the hydraulic aperture and the permeability this work provides an improved equivalent model which can be potentially used in field practices to precisely predict the non linear flow property by emphasising the great impacts of the secondary roughness keywords non darcy flow secondary roughness equivalent cubic law lattice boltzmann method data availability the data that has been used is confidential 1 introduction fractures in natural rocks play a significant part in the fluid transportation behaviors because of their relatively high permeability compared to the rock matrix berkowitz 2002 zimmerman and bodvarsson 1996 studying fluid flow behavior in fractures helps to deepen the understanding of groundwater resource extraction folch et al 2011 ma et al 2020 subsurface contaminant control tsang et al 2005 qian et al 2011 and underground petroleum and mining engineering zimmerman and yeo 2000 chen et al 2018 because geological formations are anisotropy and heterogeneous at different scales on the subsurface the investigation of the non linear flow remains a challenge in complex geometric fractures wang et al 2016 dou et al 2019 the fluid in a fracture features non darcy flow behavior caused by the complex fracture geometric properties neuman 2005 zimmerman et al 1992 brush and thomson 2003b numerious efforts have been conducted on the applicability of the cubic law considering various geometry parameters including aperture roughness and contact area qian et al 2011 ge 1997 kishida et al 2009 konzuk and dippenaar emphasised that cubic law failed to describe flow patterns because of discontinuous fracture surfaces and extreme roughness konzuk and kueper 2004 dippenaar and van rooy 2016 hajjar numerically simulated the sinusoidal walls with different amplitudes and wavelengths it was indicated that the high phase difference between the top and bottom surfaces caused the fluid flow behavior to deviate from the cubic law hajjar et al 2018 oron found that low contact areas also affected the accuracy of the cubic law oron and berkowitz 1998 accurate surface morphology and fracture geometry description as well as assessment of their influence on fluid flow enable more exact prediction of fracture permeability to characterise the fracture geometry different parameters are proposed for the quantitative analysis of complex fractures zhang and chai 2020 yin et al 2017 wu et al 2020 as shown in table 1 the fluid flow in rough walled fracture should satisfy mass and momentum conservation which is governed by the continuity equations and navier stokes equations zimmerman and bodvarsson 1996 1 u 0 2 u t u u f 1 ρ p μ ρ 2 u where u t f ρ μ and p is the flow field velocity flow time body force fluid density fluid viscosity and the pressure gradient when the flow is constant density steady state flow the navier stokes equations can be simplified as follow 3 ρ u u μ 2 u p where ρ u u and μ 2 u represent the inertia term and viscous term furthermore assuming that the fracture flow follows the smooth parallel plate model the viscous term can be ignored and the navier stokes equation can be simplified to the cubic law which can measure the hydraulic characteristics in single fracture from the steady pressure drop and flow rate zimmerman and yeo 2000 brush and thomson 2003b 4 q w e h 3 12 μ p where q w and e h is the volumetric flow rate the width of the fracture and the hydraulic aperture however the real fractures often formed of roughness tortuosity and varying aperture which leads to the deviation of the calculation of flow rate by cubic law therefore scholars modified the cubic law by using parameters related to fracture geometry to improve the accuracy renshaw 1995 nazridoust et al 2006 wang et al 2015 a large number of numerical tools are developed to depict the flow behavior through individual fractures with the above parameters table 2 shows the numerical models describing non linear flow with different methods these models can be summarised as two types the first type considers the correction factors based on the modified cubic law and the other type accounts for the spatial variability by solving the reynolds equation based on the local cubic law louis 1972 wang et al 2018 rong et al 2020 gao et al 2023 the former features high computational efficiency but the estimation of permeability deviates by a margin from the actual permeability the latter has high calculation accuracy but the computational efficiency is low and there is a lack of a unified standard for the definition the local size and a quantitative segmentation method along the entire fracture he et al 2021 furthermore numerical simulation is the most commonly used method of fractured modeling mohammadnejad and khoei 2013 antonietti et al 2016 hajibeygi et al 2011 the multi scale discrete fracture model was established by combining the finite element method and multi scale basis function efendiev et al 2015 a thermal flow model was developed to study the material transport in the fracture based on the finite volume method xiong et al 2022 an embedded discrete model was made based on the finite difference method which greatly improved the calculation efficiency of complex fracture flow yan et al 2016 in brief the establishment of a numerical model along 3d rough walled fractures is urgently required to capture the non linear flow mechanism influenced by the complex fracture geometry qian et al 2011 neuman 2005 zimmerman et al 1992 due to the surface roughness the mass transport in the fracture is an extremely complex process especially when non linear flow occurs and inertial effect greatly impacts the flow behavior tzelepis et al 2015 radilla et al 2013 the surface roughness is divided into large scale waviness and small scale unevenness in international society of rock mechanics barton 1978 jing et al 1992 put forward the concepts of primary roughness and secondary roughness the former is defined as the fluctuation of the main and large scale wavy surface and the latter is defined as the unevenness superposition of small scale wavy surfaces that are randomly distributed on the primary wavy surface zou et al 2015 proposed the definition of the secondary roughness which greatly impacted the dynamic evolution of eddy flow wang et al 2016 found that primary roughness determined the property of the global flow field distribution while secondary roughness controlled the local fluid non linear characteristics numerious results have indicated that secondary roughness was the main factor affecting flow non linearity in most natural fractures dou et al 2019 rong et al 2020 dou et al 2018 zhang et al 2021 however the quantification of the secondary roughness is poorly understood also it remains a challenge to develop an equivalent model in complex geometry fracture considering the effect of secondary roughness to accurately assess fracture flow rates and permeability to simulate the fluid flow in the fracture with complex boundary the lattice boltzmann method lbm as a mesoscopic method is used to describe fluid behavior that links molecular dynamics and macroscopic fluid mechanics briggs et al 2014 krüger et al 2017 compared with other numerical methods lbm has the characteristics of discreteness and local parallelism which greatly improves its efficiency in dealing with complex boundary problems geller et al 2006 based on the above the fracture influenced by the secondary roughness is quantitatively evaluated based on the power spectrum and the wavelet transform a classification criterion is then proposed to accurately capture the secondary roughness by investigating the non linear flow behavior along rough walled fractures using lbm a piecewise equivalent model is further established to analyse the relationship between primary and secondary roughness and flow rate along a rough walled fracture 2 numerical methodology 2 1 creation of a rough walled fracture a synthetic fracture is generated using the synfrac software which is based on the self affine fractal theory briggs et al 2017 rong et al 2020a two functions are used to describe fracture roughness brown 1995 the first function is the probability density function p z for height 5 p z 1 σ 2 π e z z 2 2 σ 2 where z z and σ represents height mean height and standard deviation of the aperture height respectively the power spectrum is another function that employs the gaussian distribution to get the fracture height using fourier decomposition approach as follows 6 g k c f k ϵ where g k k and c f is the power spectral density wavenumber and proportional factor respectively ϵ is the exponent index related to the fractal dimension and is calculated based on the slope of the power spectrum function in the double log curve five parameters are used to characterise isotropic 3d fractures in the brown model fractal dimension d resolution n standard deviation σ anisotropy factor a n and mismatch length m l brown 1995 d denotes the surface roughness as calculated by the exponent index of power spectral density function 7 d 7 ϵ 2 2 2 wavelet transform method for analysing fracture roughness as a mathematical tool for multi scale analysis wavelet transform has been widely used in signal processing image decomposition seismic exploration and other engineering and physics fields seo et al 2015 given the similarity of geometric characteristics between fracture roughness and the signal waveform the wavelet transform technique is an effective method for the accurate analysis of fracture roughness wang et al 2016 zou et al 2015 a 3d rough walled fracture is defined by z x y x and y in z x y represent the coordinates along the length and width directions of the fracture model respectively wavelet transform is carried out on the fracture in two directions and the 2d wavelet transform is derived as zou et al 2015 wang and lu 2010 8 w a b x b y z x y ψ a b x b y x y d x d y 9 ψ a b x b y x y 1 a ψ x b x a y b y a where ψ a b x b y is the 2d wavelet mother function a is a scaling transformation parameter and b x b y are the displacement transformation parameters along the length and width directions respectively a and b control the expansion and displacement of the wavelet mother function respectively the original height profile at each level needs to be reconstructed by wavelet decomposition with inverse integration as follows 10 z x y 1 c ψ 0 a 3 w a b x b y ψ a b x b y x y d b x d b y d a and 11 c ψ 4 π 2 ψ ˆ ω 2 ω 2 d ω where ω is the 2d vector ψ ˆ is the fourier transform of ψ c ψ is defined as the admissibility condition after the wavelet transform procedure the elevation data of fracture is used as the signal source which is filtered through the wavelet mother function the 2d discrete wavelet technology combined with mallat s pyramidal algorithm is used to achieve a multi level decomposition procedure for rough walled fracture surfaces wang et al 2016 mallat 1989 the discrete wavelet decomposition can discretise rough walled fractures into different scales conversely the wavelet reconstruction is the inverse process of the 2d discrete wavelet decomposition which can reconstruct rough walled fractures at different scales the whole operation process is shown in fig 1 where a j is the approximation coefficient representing large scale low frequency wavelets in rough walled surfaces d j h d j v and d j d are the horizontal vertical and diagonal detail coefficients representing the small scale wavelets in the horizontal vertical and diagonal directions of the fracture surface respectively according to the above definitions a j and d j d serve as the primary fracture surface and the secondary fracture surface the daubechies wavelet db8 can be used as the wavelet mother function zou et al 2015 the original fracture surfaces a 0 with different fractal dimensions is decomposed and reconstructed into 8 sub surfaces as shown in fig 2 which are noted by a 1 a 8 primary fracture surface and d 1 d d 8 d secondary fracture surface where the variance of the height of a j and d j d are defined as the primary roughness σ p and the high order wavelet σ h of each level primary fracture surface respectively 2 3 simulation of the non linear flow field the lbm has been commonly utilized in the study of fracture non linear flow given the simplicity of boundary conditions and strong intrinsic parallelism rong et al 2020a ma et al 2021 the d3q19 multiple relaxation time mrt model with high computational stability is used to investigate the 3d flow behaviors d humières 2002 19 velocity distribution functions f i i 0 18 are distributed at the coordinate of each lattice node x i i 0 18 which is shown in fig 3a the evolution equation for the mrt lbm on a 3d lattice x with discrete time δ t and discrete velocity vector c i is 12 f i x c i δ t t δ t f i x t m 1 s ˆ m m e q where m is a 19 19 matrix which linearly transforms the distribution functions f i to the velocity moments m m m f i and s ˆ and m e q represent the relaxation matrix and the equilibrium of the moments detailed calculation rules and the value of parameters can be referred to yu et al 2006 and d humières 2002 several important parameters need to be set in the lbm simulation the lattice viscosity ν can be obtained as 13 ν τ 0 5 c s 2 δ t where τ is relaxation coefficient τ 0 8 and c s is lattice sound speed c s 2 1 3 the macroscopic moments including density velocity and pressure are calculated as follows 14 ρ i 0 18 f i u 1 ρ i 0 18 f i c i p ρ c s 2 moreover the classical no slip condition is adopted on the boundary lattice with bounce back scheme which works on the principle that fluid particles hitting a rigid wall during propagation are reflected back to the originally position ma et al 2022 2 4 validation of lbm as the classical analytical solution of 3d pipe flow hagen poiseuille flow is the most commonly used method to verify the accuracy of the numerical model wang et al 2016 gutfraind and hansen 1995 ma et al 2022 which represents the relationship between flow rate and pressure gradient 15 u x p x 1 μ d 2 16 r 2 4 where p x 5 1 0 3 pa mm d 10 mm and r is the pressure gradient the diameter of the cylindrical tube and the radial distance from the centerline of the tube fig 4a shows the comparison results of analytical solution and numerical solution in the middle of tube the numerical results obtained using the mrt d3q19 model illustrate good agreement with the theoretical solution by comparing the velocity curves when the resolution n is larger than 100 which means the results are reliable when the mesh accuracy is less than 0 1 mm fig 4b shows the contours of 3d hagen poiseuille flow and the 2d slices in the middle of tube at the normal and tangential direction the results show that the velocity distribution by numerical results follows the poiseuille flow in the whole 3d flow field and the 2d local flow field the results of the lbm calculation are therefore numerically accurate and reliable 3 model setup the fracture model is 5 11 mm in length and width the mechanical aperture and standard deviation of the fractures are set as e m 0 1 0 3 mm and σ 0 2 mm respectively the absolute roughness is defined as σ a σ e m 0 67 2 00 the seven levels of roughness are quantified by the fractal dimension with d 1 2 2 4 rong et al 2020 which are shown in fig 3b this paper only studies isotropic fractures with parallel upper and lower surfaces with m l 0 a n 1 the open source program palabos is used to capture the evolution process of fluid along the original fracture surface and each primary fracture surface five pressure gradients ranging from 1 000 to 5000 pa m are set at both ends of the fracture the density and viscosity of water are taken as ρ 0 998 1 0 3 kg m 3 and μ 1 0 3 pa m respectively the reynold number is defined as r e ρ q μ w to evaluate the flow state and the weak inertia region of the flow field is discussed with r e 6 5 in this paper wang et al 2015 zimmerman et al 2004 the rough walled fracture model is discretised into 512 512 175 lattice nodes in the directions of length width and height respectively thus the unit length of one lattice is 10 μ m three models are analysed with different resolutions to ensure the mesh independency of the results the top bottom left and right surfaces of the fracture adopt the no slip boundary condition pressure boundaries are applied at the inlet and outlet fig 3c the initialization of the flow field is set for initial velocity u i 0 initial pressure p i 0 and time step δ t i 10 5 s the flow behavior is considered as the steady state when the velocity difference of the adjacent time steps satisfied u i 1 u i 1 1 0 5 m s wang et al 2016 furthermore unit conversion is also an important step in lattice simulation krüger et al 2017 the lattice unit and the physical unit generally have the following mapping relationship 16 l m l p h l l b ρ m ρ p h ρ l b ν m ν p h ν l b where the subscripts ph and lb indicate the physical and lbm systems respectively the kinematic viscosity ν p h and density are adopted to represent the time and mass mapping in this study the length mapping l m is directly related to resolution in the simulation for example 512 lattice nodes represent the length of 5 11 mm with δ l p h 5 11 mm 511 0 01 mm the unity length is taken as the default value with δ l l b 1 and l m δ l p h δ l l b 0 01 mm the initial density is set as 1 and the mass mapping m m is ρ p h ρ l b l m 3 in the simulation the lattice viscosity can be obtained from eq 13 based on the time mapping t m ν l b ν p h l m 2 the mapping relationship of other variables can be found in ma et al 2022 4 results and analysis 4 1 quantitative evaluation of secondary roughness the classification criterion proposed by zou et al 2015 to determine the secondary roughness includes 1 the similarity of primary and original fracture surfaces 2 secondary roughness profile treated as a white noise random process obeying a gaussian distribution i e fig 5 3 secondary roughness is small enough compared to the primary roughness the value of the secondary roughness σ s can be calculated as the sum of the first j order of the height variance of the high order wavelet surface σ s σ h j wang et al 2016 as shown in fig 6a the red dotted line represents the criterion to determine the classification of secondary roughness according to the criterion in zou et al 2015 the secondary roughness is quantified for the fracture with different fractal dimensions fig 5 shows a frequency histogram for the height of secondary roughness and shapiro wilk s test p 0 05 shapiro and wilk 1965 which demonstrates that the secondary roughness profile satisfies the gaussian distribution condition the result indicates that the level of the classification to determine the secondary roughness decreases as the fractal dimension grows when the fractal dimension reaches 2 4 the primary surface a 2 is the classification of secondary roughness and when the fractal dimension reduces to 1 2 the primary surface a 5 is the classification of secondary roughness fig 6b shows the correlation between the secondary roughness and the fracture with different fractal dimensions the numbers in the red boxes in fig 6b represent the sum of the order of secondary roughness classification the secondary roughness increases with the increase of the fractal dimension the classification of secondary roughness is at the fifth order surface d 5 d when the fractal dimension is less than 1 6 however the secondary roughness has a sudden drop when the fractal dimension is 1 8 this is caused by the uncertainty of the classification of secondary roughness and it is a non quantitative concept as the secondary roughness approaches zero compared to the primary roughness if the red dotted line moves down the secondary roughness becomes the sum of the variance of d 1 d d 5 d rather than d 1 d d 4 d when the fractal dimension is 1 8 fig 6a in fact the sum of first three to five order of the variance of high order wavelet surface satisfy the criterion of secondary roughness for most fracture surfaces therefore an improved quantification criterion of secondary roughness need to be further studied the secondary roughness has an important effect on the non linear flow inside the primary fracture the classification criterion can be determined by the non linear flow properties however the fractal dimension of the primary fracture dominates the flow pattern therefore it is necessary to study the fractal dimension of the primary fracture influenced by the high order wavelet 4 2 quantitative analysis of primary fracture surface this section investigates the effect of secondary roughness on fracture geometry using power spectrum method power spectral density psd as an effective spectral analysis tool has been widely utilized in the analysis of different fracture geometric profiles chae et al 2004 pickering and aydin 2016 power and tullis 1991 the advantage of psd is that its statistics are not affected by the specific geometry size and resolution accuracy develi and babadagli 1998 the fractal dimension is obtained from the slope of the double log curve of spectral density function as shown in eqs 6 and 7 brown 1995 however the major disadvantage of this method is that it is difficult to get the proper slope of double log curve and large amount of high frequency noise of the fracture data klinkenberg 1994 furthermore fractal dimensions have large errors when the distribution of fracture height does not conform to the gaussian distribution therefore an improved method based on power spectral density is proposed to quantitatively evaluate the roughness of a rough walled fracture after wavelet transform to eliminate errors caused by the non normality of fracture distribution the rough wall fracture surfaces with different fractal dimensions are generated using synfrac as prefabricated template surface profiles the discrete fourier transform is used to calculate the psd of a template surface profile the third order polynomial function has been fitted to the psd plots ünlüsoy and süzen 2020 17 p s d p o l y f a 1 a 2 k a 3 k 2 a 4 k 3 the double log curve of the correlation of template profiles psd and spatial frequency using third order polynomial fits are shown in fig 7 the fractal dimension can be calculated from the area difference by comparing the psd of the sample and template the interpolation algorithm is given by comparing the smallest area difference of psd between the sample and three templates the fractal dimension is calculated by the ratio of area differences as follows 18 d s d i δ a i d i d i 1 δ a i δ a i 1 δ a i 1 δ a i 1 d i δ a i d i d i 1 δ a i δ a i 1 δ a i 1 δ a i 1 where d s represents the fractal dimension of the sample profile and δ a i represents the smallest area difference between the psd of the sample and template the specific calculation process is 1 prepare template data of the coordinates of fracture height from synfrac and remove the mean and linear trend of the height data of the fractures 2 window the result with a 10 cosine tapered window in the form 19 w n w 0 5 0 5 cos 2 π n w 5 n 0 n w n 10 1 n 10 n w 9 n 10 0 5 0 5 cos 2 π n n w 5 n n w 9 n 10 where w n w is the window coefficient at the n w position of the sample profile 3 apply twice the fast fourier transform to get the psd of each template fit the data of each template using a third order polynomial function in double log space i e fig 7 4 select the sample profile with the same resolution accuracy as the template psd of the sample surface profile can be obtained and fitted by a third order polynomial function 5 calculate area interpolation between the psd of the template fracture and sample fracture choose the smallest area difference as the standard template for calculating the fractal dimension of the sample profile by its upper and lower templates using eq 18 the method is simple and solves the error caused by high order noise it can also be applied to rough walled fractures without conforming to the gaussian distribution fig 8 shows the fractal dimension of a 2 using the psd area interpolation the green curve represents the psd of the a 2 surface black shading represents the area interpolation of the power spectrum curve the result indicates that a 2 is the closest to the template with a fractal dimension of 2 3 fig 8a by comparing samples with the power spectrum templates of d 2 4 and d 2 2 fig 8b and c the fractal dimension of a 2 is obtained by using eq 18 and found to be 2 27 fractal dimension of the primary fracture affected by the first fifth order wavelet is also analysed as shown in table 3 the results show that the influence of the first order wavelet on the fractal dimension is almost negligible however the second order wavelet greatly reduces the fractal dimension of the decomposed fracture the fractal dimension is less than 2 0 when the surface is decomposed to fourth order as the fracture surface is decomposed to the fifth order the fractal dimension of the fracture decreases suddenly there is almost no correlation between the fifth order primary fracture and the original fracture 4 3 non linear analysis of flow field along a rough walled fracture in this section the fluid flow simulations are carried out for the surfaces of a 0 a 8 to analyse the flow non linearity influenced by the secondary roughness with lbm the non darcy flow characteristics are quantitatively assessed using the forchheimer s law zimmerman et al 2004 yin et al 2019 guo et al 2020a 20 p a f q b f q 2 where a f 12 μ w e h 3 and b f β ρ w 2 e h 2 and β is the forchheimer coefficient based on eq 20 the non linear effect factor reflecting the non linear strength of the flow field is defined as follow chen et al 2015 yin et al 2018 21 α b f q 2 a f q b f q 2 fig 9 shows the best fitted curve of the correlation between flow rate and pressure using forchheimer equation the forchheimer coefficient is listed in the rough walled fracture with different fractal dimensions by wavelet transform as shown in table 4 the results of b f indicate that the non linearity of flow field gradually decreases with increasing wavelet classification the sum of first fifth order wavelet can be the secondary roughness of the fracture with fractal dimensions of 1 2 and 1 6 using zou et al s criterion when the fractal dimension is 1 2 and 1 6 shown in fig 9a and b the difference of the flow rate is not large at a 0 a 5 which indicates that secondary roughness has little effect on the flow rate along fractures with a small fractal dimension however the flow rate variation influenced by first fifth order wavelet is much larger than the cases with small fractal dimension when the fractal dimension is 2 0 and 2 4 shown in fig 9c and d the secondary roughness causes obvious flow non linearity phenomenon in a fracture with a high fractal dimension furthermore as shown in fig 9a d when the surface is decomposed to a 6 a 8 the variation of the flow rate does not change much at each fractal dimension the main factors affecting the flow field are the aperture the primary roughness and the secondary roughness of the fracture when the fracture surface aperture is consistent and the high order wavelet is filtered out the influence of secondary roughness is almost ignored due to the relatively smooth surface the only variable of a 6 a 8 fracture surface is the primary roughness which represents the undulation of fracture surface the undulation of fractures with different fractal dimensions is similar at the sixth to eighth order therefore the flow pattern in a 6 a 8 is mainly dominated by the fracture undulant properties rather than the fracture secondary roughness furthermore the significant difference of flow rate is mainly caused by the first fourth order wavelet for the fracture with different fractal dimension such as the blue black green and pink curves shown in fig 9a d therefore the flow behavior is mainly affected by the wavelet from first order to the fourth order to further analyse the influence of wavelets at all levels on flow non linearity the non linear coefficients are compared in original fracture surface and eighth order primary surface fig 10 when the fractal dimension is 1 2 and 1 6 the non linear coefficient increases only with the pressure gradient and is not affected by the wavelet decomposition of the sub surface when the fractal dimension is 2 0 the non linear differences of the sub surfaces gradually appear there is a large non linear difference of the sub surfaces at all levels when the fractal dimension is 2 4 thus when the fractal dimension is less than 2 0 the secondary roughness has little effect on the flow non linearity the effect of secondary roughness on non linear flow behavior must be considered when the fractal dimension is higher than 2 0 therefore taking d 2 0 as the classification boundary the fourth order fracture surface is selected as the critical fracture surface refer to the case with d 2 4 in table 3 fig 10 also presents the mean non linear coefficient error α e between the original fracture surface a 0 and the fourth order primary fracture surface a 4 with different pressure gradients which is defined as 22 α e 1 n i 0 n α o α p α o where α o and α p represent the non linear coefficient of original fracture surface and fourth order primary fracture surface respectively the mean non linear coefficient error increases with the increase of the fractal dimension when the fractal dimension is 2 0 and 2 4 the mean non linear coefficient error reaches 2 15 and 23 0 the results indicate that secondary roughness has great influence on flow non linearity when the fractal dimension of original fracture is greater than 2 0 furthermore fig 11a shows an exponential relationship between primary roughness and flow rate the primary roughness barely changes while the flow rate changes significantly when the fractal dimension is higher than 2 0 which indicates that the flow behavior is mainly affected by the secondary roughness when the fractal dimension is less than 2 0 the primary roughness changes greatly which indicates that the fracture surface after the wavelet transform has lost the main property of the original fracture surface fig 11b shows the correlation between the high order wavelet and the difference of flow rate of two adjacent primary surfaces with wavelet transform when the fractal dimension is higher than 2 0 the flow difference has a linear relationship with first fourth order wavelet however the linear correlation is not valid when the fractal dimension is less than 2 0 therefore the first fourth order wavelet results in the linear trend of the flow rate difference according to the above analysis the classification criterion of secondary roughness is redefined as follow the classification of fracture surface is the fifth order primary surface according to the criterion in zou et al 2015 when the fractal dimension of the original fracture is less than 2 0 the classification of fracture surface is the first primary surface with fractal dimension less than 2 0 when the fractal dimension of the original fracture is larger than 2 0 the improved criterion is determined by the effect of high order roughness on the fracture fractal dimension and non linear flow behavior although the fifth order surface is the classification criterion surface of secondary roughness of fractures with fractal dimensions of 1 2 and 1 6 the flow rate difference between the original fracture surface and the fifth order fracture surface is less than 1 the flow rate changes by about 2 7 compared with the case considering the secondary roughness when the fractal dimension is 2 0 therefore the effect of secondary roughness can be ignored for the fractures with fractal dimension less than 2 0 furthermore for fractures with fractal dimension larger than 2 1 the four order surface can be selected as the classification criterion surface for fractures with fractal dimension from 2 0 to 2 1 it is recommended to take the third order surface as the classification criterion surface the suggested classification for the secondary roughness can be referred to table 3 4 4 piecewise equivalent cubic law model piecewise equivalent model is established to describe the relationship between flow behavior and primary and secondary roughness in this section fig 12 illustrates the local velocity contour of rough walled fracture after wavelet transform the rough walled surface gradually becomes smooth with the increase of surface classification when the fractal dimension is higher than 2 0 fig 12a and 12b numerious burrs are distributed along the fracture surface the uneven distribution of the velocity field is affected by the burrs and undulation of the fracture when the fractal dimension is less than 2 0 the fracture surface is relatively smooth which indicates that the flow field is just subject to the undulation along rough walled fracture when the fracture surface is decomposed to the sixth order fig 12d the flow field can be approximately regarded as a darcy flow and the velocity field is parabolically distributed along the aperture direction therefore the surface roughness characteristic should be divided into two parts undulation and burrs which are determined by the primary and the secondary roughness respectively to quantitatively describe the flow behavior an equivalent model learned from the form summarised by previous scholars wang et al 2015 2018 ju et al 2019 is expressed as 23 q w e m 3 12 μ δ p l 1 1 σ a x y the standard deviation of the fracture height can be regarded as the superposition of the high order wavelet of each level surface and the primary roughness of the last level surface 24 σ σ h a 1 σ h a 2 σ h a 3 σ h a j σ p a j when the fractal dimension is higher than 2 0 the surface a j is selected as the surface whose fractal dimension is just less than 2 0 based on the definition of secondary roughness eq 24 can be written as 25 σ σ s σ p a j d 2 0 σ σ p a j d 2 0 when the fractal dimension is less than 2 0 the effect of secondary roughness is ignored therefore a piecewise equivalent model is established to quantify the correlation between flow rate and secondary roughness 26 q w e m 3 12 μ δ p l 1 1 σ p e m x y 1 f σ s d 2 0 w e m 3 12 μ δ p l 1 1 σ p e m x y d 2 0 the above formula shows that the reduction of flow rate is affected by two aspects primary roughness and secondary roughness which are characterised by undulation and burrs furthermore the hydraulic aperture e h permeability k can be derived as 27 e h e m 1 σ p e m x y 3 f σ s 1 3 d 2 0 e m 1 σ p e m x y 3 d 2 0 28 k e m 2 12 1 σ p e m x 2 y 3 f σ s 2 3 d 2 0 e m 2 12 1 σ p e m x 2 y 3 d 2 0 the effect of secondary roughness is assumed as f σ s to determine the model coefficients and the form of f σ s the classical form proposed by renshaw is adopted by the numerical fitting method in this paper where x 2 from a commonly used value of the previous model renshaw 1995 y is defined as the piecewise coefficient which is calculated from the flow rate in the fracture with a fractal dimension of 2 0 fig 13a shows the change of piecewise coefficient with absolute roughness the piecewise coefficient shows the exponential decrease relationship with the increase of absolute roughness the relationship between f σ s and σ s can be established when the piecewise coefficient is determined the primary and secondary roughness is non dimensionalized by introducing the mechanical aperture and the characteristic length of secondary roughness l σ s 1 0 5 m by comparing the numerical results of different combinations of σ s and f σ s exponential relationship f σ s l σ s λ e η σ s l σ s is found to be a reasonable function where f σ s l σ s is defined as the secondary roughness intensity factor and η and λ are the fitted coefficient fig 14a shows the best fitted curve of f σ s l σ s and dimensionless secondary roughness using numerical results when the absolute roughness σ a 1 0 where f σ s l σ s is obtained by calculation of the numerical results by eq 26 the exponential function can describe the relationship between flow rate and secondary roughness the flow behavior along rough walled fractures is only discussed in the weak inertia region with r e 6 5 in this study wang et al 2015 zimmerman et al 2004 therefore the pressure difference has little effect on the fitted coefficient the fitted parameters are further analyzed with different absolute roughness fig 13b and c shows the change of fitted coefficient with absolute roughness when σ a is less than 1 18 λ is stable at 0 64 however when σ a is large λ becomes unstable η increases first and then reduces with the increase of absolute roughness both λ and η determine the strength of secondary roughness the reference values of parameters are shown in table 5 for the application of the equivalent model furthermore the validity of the equivalent model is further demonstrated by comparing with the model proposed by patir and cheng 1978 the patir model is an empirical formula obtained by theoretical derivation and numerical verification which has been widely validated zimmerman and bodvarsson 1996 zhang et al 2016 zhang and chai 2020 gao et al 2023 29 q w e m 3 12 μ δ p l 1 0 9 e 0 56 e m σ p fig 14b shows the flow rate versus pressure gradient using cubic law patir model renshaw model and the developed equivalent model for fractures with identical aperture and primary roughness the result of equivalent model with d 2 0 is used to compare with the patir model which ignores the influence of secondary roughness the error between the equivalent model and patir model is less than 1 5 the accuracy of the equivalent model is greatly improved compared with cubic law and renshaw model the results show that the mechanical aperture and primary roughness are not the only two factors to affect the flow rate the decrease of flow rate influenced by the secondary roughness is greater than that only considering primary roughness when the fractal dimension is 2 4 in this case the flow rate is mainly dominated by the effect of secondary roughness even if the primary roughness and the fracture aperture remain constant table 6 compares the hydraulic aperture and permeability with different models renshaw model and louis model consider only the primary roughness of the fracture the permeability is similar to that of the equivalent model with d 2 2 in fact the two models estimate only two cases of constant secondary roughness the permeability is also calculated using the rasouli model with the fractal dimension of 2 4 the transformation relationship between jrc and fractal dimension adopts the method proposed by li and huang 2015 and jrc is 13 1 in this study rasouli and hosseinian 2011 the results shows that rasouli model ignores the influence of the secondary roughness which leads to overestimate the hydraulic aperture and permeability of the fracture compared with the equivalent model in summary when the secondary roughness effect is obvious none of the above three models can effectively estimate the permeability characteristics of the fractures therefore the increase of secondary roughness greatly reduces the hydraulic aperture and permeability which indicates that the burrs on the fracture surface enhance flow non linearity resulting in the decrease of the flow rate the flow rate loss caused by secondary roughness and primary roughness is further analyzed with different fractal dimensions and mechanical apertures based on the equivalent model as shown in fig 14c and d δ q 1 δ q 2 and δ q 3 represent the flow rate loss by primary roughness secondary roughness and total flow rate loss respectively 30 δ q 1 w e m 3 12 μ δ p l 1 1 1 σ p e m x y δ q 2 w e m 3 12 μ δ p l 1 1 σ p e m x y 1 1 f σ s δ q 3 w e m 3 12 μ δ p l 1 1 1 σ p e m x y 1 f σ s when the fracture aperture remains constant δ q 1 is constant and δ q 2 increases with the increase of fractal dimension δ q 1 is larger than δ q 2 when the fractal dimension is up to 2 45 the secondary roughness dominants the change trend of the total flow rate when the fractal dimension remains constant both δ q 1 and δ q 2 increases with the increase of mechanical aperture however the growth rate of δ q 2 is less than that of δ q 1 with the increase of fracture aperture which indicates that the effect of secondary roughness is weakened compared with the primary roughness when the mechanical aperture increases in summary the equivalent model considers the influence of the primary and secondary roughness on the flow along the fracture with complex geometry compared with the cubic law which can be used in the fracture with high roughness and undulation the equivalent model reveals non linear flow mechanism influenced by the secondary roughness and describes the non linear flow behavior simply and accurately by incorporating the effect of fracture secondary roughness 5 conclusions and prospects in this study a piecewise equivalent model is developed to characterise the relationship between primary and secondary roughness and flow rate inside the fracture setting the fractal dimension 2 0 as the piecewise point the rough walled fracture influenced by the primary and secondary roughness is quantitatively evaluated based on the wavelet transform and the power spectrum method a classification criterion is then proposed to accurately capture the secondary roughness by investigating the non linear flow behavior along rough walled fractures using the lbm the main conclusions are summarised as follows the sum of first three to five order of the high order wavelet satisfies the criterion of the secondary roughness for most fracture surfaces current classification criterion of secondary roughness cannot achieve unique quantification of all original fracture surfaces with different fractal dimensions the improved classification criterion of secondary roughness is redefined as follow the classification criterion surface is the fifth order primary surface when the original fracture fractal dimension is less than 2 0 while the classification criterion surface is the first primary surface with fractal dimension less than 2 0 when the original fracture fractal dimension is larger than 2 0 the modified power spectrum method can effectively estimate the fractal dimension of the fracture without following the normal distribution the influence of high order wavelet on the fractal dimension of rough walled fracture surfaces is analysed the effect of the first order wavelet on the fractal dimension is almost negligible the wavelet from the second order to the fourth order greatly decreases the fracture roughness the fractal dimension of surface is less than 2 0 when the fracture surface profile is decomposed to the fourth order there is almost no correlation between the fifth order primary fracture and the original fracture higher fractal dimensions enhance the fracture secondary roughness which greatly increases the non linear behavior of the flow field however the effect of the secondary roughness on flow non linearity is negligible when the fractal dimension is less than 2 0 the decrease of flow rate is mainly caused by the primary roughness and secondary roughness and the secondary roughness significantly increases the non linearity of the flow field when the fractal dimension is larger than 2 0 a quantitative piecewise model considering the effect of secondary roughness is established to predict the equivalent flow behavior in a rough walled fracture it is indicated that the non linear flow pattern is influenced by both undulation and burrs in fractures with higher fractal dimension d 2 0 the variation of flow rates is greatly influenced by burrs and thus dominated by the secondary roughness even if fracture apertures and primary roughness are identical the increase of secondary roughness greatly reduces the hydraulic aperture and the permeability and the effect of secondary roughness is weakened compared to the primary roughness when the mechanical aperture of the fracture increases nevertheless when the absolute roughness is large the fitted coefficient of the equivalent model changes irregularly the influence of fitted coefficient on the secondary roughness intensity factor needs further study at large absolute roughness credit authorship contribution statement chunlei ma conceptualization methodology software data curation writing original draft visualization investigation yun chen supervision methodology validation writing review editing funding acquisition xiaoliang tong supervision validation reviewing funding acquisition guowei ma conceptualization methodology supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by the national natural science foundation of china grant nos 52061160367 u1965204 52109121 science and technology program of hebei grant nos e2022202041 e2021202073 e2020050015 the support from the project of key technologies of seepage control system for large scale hydraulic projects was also gratefully appreciated 
2024,the classical cubic law deviates from the estimation of the flow rate because of the surface roughness distributed along the fracture surface which can be decomposed as the primary and the secondary roughness a piecewise equivalent model is developed to characterise the relationship between primary and secondary roughness and flow rate inside the fracture setting the fractal dimension 2 0 as the piecewise point the rough walled fracture influenced by the primary and secondary roughness is quantitatively evaluated based on the wavelet transform and the power spectrum method a classification criterion is then proposed to accurately capture the secondary roughness by investigating the non linear flow behavior along rough walled fractures using the lattice boltzmann method the results found that secondary roughness greatly enhances the flow non linearity when the fracture fractal dimension exceeds 2 0 while secondary roughness has little effect on non linear flow behavior with the fracture fractal dimension less than 2 0 the developed piecewise equivalent model shows that the non linear flow pattern is affected by both fracture primary and secondary roughness the flow variation along fractures with identical fracture apertures and primary roughness is greatly related to the influence of the secondary roughness the increase of secondary roughness greatly decreases the hydraulic aperture and the permeability this work provides an improved equivalent model which can be potentially used in field practices to precisely predict the non linear flow property by emphasising the great impacts of the secondary roughness keywords non darcy flow secondary roughness equivalent cubic law lattice boltzmann method data availability the data that has been used is confidential 1 introduction fractures in natural rocks play a significant part in the fluid transportation behaviors because of their relatively high permeability compared to the rock matrix berkowitz 2002 zimmerman and bodvarsson 1996 studying fluid flow behavior in fractures helps to deepen the understanding of groundwater resource extraction folch et al 2011 ma et al 2020 subsurface contaminant control tsang et al 2005 qian et al 2011 and underground petroleum and mining engineering zimmerman and yeo 2000 chen et al 2018 because geological formations are anisotropy and heterogeneous at different scales on the subsurface the investigation of the non linear flow remains a challenge in complex geometric fractures wang et al 2016 dou et al 2019 the fluid in a fracture features non darcy flow behavior caused by the complex fracture geometric properties neuman 2005 zimmerman et al 1992 brush and thomson 2003b numerious efforts have been conducted on the applicability of the cubic law considering various geometry parameters including aperture roughness and contact area qian et al 2011 ge 1997 kishida et al 2009 konzuk and dippenaar emphasised that cubic law failed to describe flow patterns because of discontinuous fracture surfaces and extreme roughness konzuk and kueper 2004 dippenaar and van rooy 2016 hajjar numerically simulated the sinusoidal walls with different amplitudes and wavelengths it was indicated that the high phase difference between the top and bottom surfaces caused the fluid flow behavior to deviate from the cubic law hajjar et al 2018 oron found that low contact areas also affected the accuracy of the cubic law oron and berkowitz 1998 accurate surface morphology and fracture geometry description as well as assessment of their influence on fluid flow enable more exact prediction of fracture permeability to characterise the fracture geometry different parameters are proposed for the quantitative analysis of complex fractures zhang and chai 2020 yin et al 2017 wu et al 2020 as shown in table 1 the fluid flow in rough walled fracture should satisfy mass and momentum conservation which is governed by the continuity equations and navier stokes equations zimmerman and bodvarsson 1996 1 u 0 2 u t u u f 1 ρ p μ ρ 2 u where u t f ρ μ and p is the flow field velocity flow time body force fluid density fluid viscosity and the pressure gradient when the flow is constant density steady state flow the navier stokes equations can be simplified as follow 3 ρ u u μ 2 u p where ρ u u and μ 2 u represent the inertia term and viscous term furthermore assuming that the fracture flow follows the smooth parallel plate model the viscous term can be ignored and the navier stokes equation can be simplified to the cubic law which can measure the hydraulic characteristics in single fracture from the steady pressure drop and flow rate zimmerman and yeo 2000 brush and thomson 2003b 4 q w e h 3 12 μ p where q w and e h is the volumetric flow rate the width of the fracture and the hydraulic aperture however the real fractures often formed of roughness tortuosity and varying aperture which leads to the deviation of the calculation of flow rate by cubic law therefore scholars modified the cubic law by using parameters related to fracture geometry to improve the accuracy renshaw 1995 nazridoust et al 2006 wang et al 2015 a large number of numerical tools are developed to depict the flow behavior through individual fractures with the above parameters table 2 shows the numerical models describing non linear flow with different methods these models can be summarised as two types the first type considers the correction factors based on the modified cubic law and the other type accounts for the spatial variability by solving the reynolds equation based on the local cubic law louis 1972 wang et al 2018 rong et al 2020 gao et al 2023 the former features high computational efficiency but the estimation of permeability deviates by a margin from the actual permeability the latter has high calculation accuracy but the computational efficiency is low and there is a lack of a unified standard for the definition the local size and a quantitative segmentation method along the entire fracture he et al 2021 furthermore numerical simulation is the most commonly used method of fractured modeling mohammadnejad and khoei 2013 antonietti et al 2016 hajibeygi et al 2011 the multi scale discrete fracture model was established by combining the finite element method and multi scale basis function efendiev et al 2015 a thermal flow model was developed to study the material transport in the fracture based on the finite volume method xiong et al 2022 an embedded discrete model was made based on the finite difference method which greatly improved the calculation efficiency of complex fracture flow yan et al 2016 in brief the establishment of a numerical model along 3d rough walled fractures is urgently required to capture the non linear flow mechanism influenced by the complex fracture geometry qian et al 2011 neuman 2005 zimmerman et al 1992 due to the surface roughness the mass transport in the fracture is an extremely complex process especially when non linear flow occurs and inertial effect greatly impacts the flow behavior tzelepis et al 2015 radilla et al 2013 the surface roughness is divided into large scale waviness and small scale unevenness in international society of rock mechanics barton 1978 jing et al 1992 put forward the concepts of primary roughness and secondary roughness the former is defined as the fluctuation of the main and large scale wavy surface and the latter is defined as the unevenness superposition of small scale wavy surfaces that are randomly distributed on the primary wavy surface zou et al 2015 proposed the definition of the secondary roughness which greatly impacted the dynamic evolution of eddy flow wang et al 2016 found that primary roughness determined the property of the global flow field distribution while secondary roughness controlled the local fluid non linear characteristics numerious results have indicated that secondary roughness was the main factor affecting flow non linearity in most natural fractures dou et al 2019 rong et al 2020 dou et al 2018 zhang et al 2021 however the quantification of the secondary roughness is poorly understood also it remains a challenge to develop an equivalent model in complex geometry fracture considering the effect of secondary roughness to accurately assess fracture flow rates and permeability to simulate the fluid flow in the fracture with complex boundary the lattice boltzmann method lbm as a mesoscopic method is used to describe fluid behavior that links molecular dynamics and macroscopic fluid mechanics briggs et al 2014 krüger et al 2017 compared with other numerical methods lbm has the characteristics of discreteness and local parallelism which greatly improves its efficiency in dealing with complex boundary problems geller et al 2006 based on the above the fracture influenced by the secondary roughness is quantitatively evaluated based on the power spectrum and the wavelet transform a classification criterion is then proposed to accurately capture the secondary roughness by investigating the non linear flow behavior along rough walled fractures using lbm a piecewise equivalent model is further established to analyse the relationship between primary and secondary roughness and flow rate along a rough walled fracture 2 numerical methodology 2 1 creation of a rough walled fracture a synthetic fracture is generated using the synfrac software which is based on the self affine fractal theory briggs et al 2017 rong et al 2020a two functions are used to describe fracture roughness brown 1995 the first function is the probability density function p z for height 5 p z 1 σ 2 π e z z 2 2 σ 2 where z z and σ represents height mean height and standard deviation of the aperture height respectively the power spectrum is another function that employs the gaussian distribution to get the fracture height using fourier decomposition approach as follows 6 g k c f k ϵ where g k k and c f is the power spectral density wavenumber and proportional factor respectively ϵ is the exponent index related to the fractal dimension and is calculated based on the slope of the power spectrum function in the double log curve five parameters are used to characterise isotropic 3d fractures in the brown model fractal dimension d resolution n standard deviation σ anisotropy factor a n and mismatch length m l brown 1995 d denotes the surface roughness as calculated by the exponent index of power spectral density function 7 d 7 ϵ 2 2 2 wavelet transform method for analysing fracture roughness as a mathematical tool for multi scale analysis wavelet transform has been widely used in signal processing image decomposition seismic exploration and other engineering and physics fields seo et al 2015 given the similarity of geometric characteristics between fracture roughness and the signal waveform the wavelet transform technique is an effective method for the accurate analysis of fracture roughness wang et al 2016 zou et al 2015 a 3d rough walled fracture is defined by z x y x and y in z x y represent the coordinates along the length and width directions of the fracture model respectively wavelet transform is carried out on the fracture in two directions and the 2d wavelet transform is derived as zou et al 2015 wang and lu 2010 8 w a b x b y z x y ψ a b x b y x y d x d y 9 ψ a b x b y x y 1 a ψ x b x a y b y a where ψ a b x b y is the 2d wavelet mother function a is a scaling transformation parameter and b x b y are the displacement transformation parameters along the length and width directions respectively a and b control the expansion and displacement of the wavelet mother function respectively the original height profile at each level needs to be reconstructed by wavelet decomposition with inverse integration as follows 10 z x y 1 c ψ 0 a 3 w a b x b y ψ a b x b y x y d b x d b y d a and 11 c ψ 4 π 2 ψ ˆ ω 2 ω 2 d ω where ω is the 2d vector ψ ˆ is the fourier transform of ψ c ψ is defined as the admissibility condition after the wavelet transform procedure the elevation data of fracture is used as the signal source which is filtered through the wavelet mother function the 2d discrete wavelet technology combined with mallat s pyramidal algorithm is used to achieve a multi level decomposition procedure for rough walled fracture surfaces wang et al 2016 mallat 1989 the discrete wavelet decomposition can discretise rough walled fractures into different scales conversely the wavelet reconstruction is the inverse process of the 2d discrete wavelet decomposition which can reconstruct rough walled fractures at different scales the whole operation process is shown in fig 1 where a j is the approximation coefficient representing large scale low frequency wavelets in rough walled surfaces d j h d j v and d j d are the horizontal vertical and diagonal detail coefficients representing the small scale wavelets in the horizontal vertical and diagonal directions of the fracture surface respectively according to the above definitions a j and d j d serve as the primary fracture surface and the secondary fracture surface the daubechies wavelet db8 can be used as the wavelet mother function zou et al 2015 the original fracture surfaces a 0 with different fractal dimensions is decomposed and reconstructed into 8 sub surfaces as shown in fig 2 which are noted by a 1 a 8 primary fracture surface and d 1 d d 8 d secondary fracture surface where the variance of the height of a j and d j d are defined as the primary roughness σ p and the high order wavelet σ h of each level primary fracture surface respectively 2 3 simulation of the non linear flow field the lbm has been commonly utilized in the study of fracture non linear flow given the simplicity of boundary conditions and strong intrinsic parallelism rong et al 2020a ma et al 2021 the d3q19 multiple relaxation time mrt model with high computational stability is used to investigate the 3d flow behaviors d humières 2002 19 velocity distribution functions f i i 0 18 are distributed at the coordinate of each lattice node x i i 0 18 which is shown in fig 3a the evolution equation for the mrt lbm on a 3d lattice x with discrete time δ t and discrete velocity vector c i is 12 f i x c i δ t t δ t f i x t m 1 s ˆ m m e q where m is a 19 19 matrix which linearly transforms the distribution functions f i to the velocity moments m m m f i and s ˆ and m e q represent the relaxation matrix and the equilibrium of the moments detailed calculation rules and the value of parameters can be referred to yu et al 2006 and d humières 2002 several important parameters need to be set in the lbm simulation the lattice viscosity ν can be obtained as 13 ν τ 0 5 c s 2 δ t where τ is relaxation coefficient τ 0 8 and c s is lattice sound speed c s 2 1 3 the macroscopic moments including density velocity and pressure are calculated as follows 14 ρ i 0 18 f i u 1 ρ i 0 18 f i c i p ρ c s 2 moreover the classical no slip condition is adopted on the boundary lattice with bounce back scheme which works on the principle that fluid particles hitting a rigid wall during propagation are reflected back to the originally position ma et al 2022 2 4 validation of lbm as the classical analytical solution of 3d pipe flow hagen poiseuille flow is the most commonly used method to verify the accuracy of the numerical model wang et al 2016 gutfraind and hansen 1995 ma et al 2022 which represents the relationship between flow rate and pressure gradient 15 u x p x 1 μ d 2 16 r 2 4 where p x 5 1 0 3 pa mm d 10 mm and r is the pressure gradient the diameter of the cylindrical tube and the radial distance from the centerline of the tube fig 4a shows the comparison results of analytical solution and numerical solution in the middle of tube the numerical results obtained using the mrt d3q19 model illustrate good agreement with the theoretical solution by comparing the velocity curves when the resolution n is larger than 100 which means the results are reliable when the mesh accuracy is less than 0 1 mm fig 4b shows the contours of 3d hagen poiseuille flow and the 2d slices in the middle of tube at the normal and tangential direction the results show that the velocity distribution by numerical results follows the poiseuille flow in the whole 3d flow field and the 2d local flow field the results of the lbm calculation are therefore numerically accurate and reliable 3 model setup the fracture model is 5 11 mm in length and width the mechanical aperture and standard deviation of the fractures are set as e m 0 1 0 3 mm and σ 0 2 mm respectively the absolute roughness is defined as σ a σ e m 0 67 2 00 the seven levels of roughness are quantified by the fractal dimension with d 1 2 2 4 rong et al 2020 which are shown in fig 3b this paper only studies isotropic fractures with parallel upper and lower surfaces with m l 0 a n 1 the open source program palabos is used to capture the evolution process of fluid along the original fracture surface and each primary fracture surface five pressure gradients ranging from 1 000 to 5000 pa m are set at both ends of the fracture the density and viscosity of water are taken as ρ 0 998 1 0 3 kg m 3 and μ 1 0 3 pa m respectively the reynold number is defined as r e ρ q μ w to evaluate the flow state and the weak inertia region of the flow field is discussed with r e 6 5 in this paper wang et al 2015 zimmerman et al 2004 the rough walled fracture model is discretised into 512 512 175 lattice nodes in the directions of length width and height respectively thus the unit length of one lattice is 10 μ m three models are analysed with different resolutions to ensure the mesh independency of the results the top bottom left and right surfaces of the fracture adopt the no slip boundary condition pressure boundaries are applied at the inlet and outlet fig 3c the initialization of the flow field is set for initial velocity u i 0 initial pressure p i 0 and time step δ t i 10 5 s the flow behavior is considered as the steady state when the velocity difference of the adjacent time steps satisfied u i 1 u i 1 1 0 5 m s wang et al 2016 furthermore unit conversion is also an important step in lattice simulation krüger et al 2017 the lattice unit and the physical unit generally have the following mapping relationship 16 l m l p h l l b ρ m ρ p h ρ l b ν m ν p h ν l b where the subscripts ph and lb indicate the physical and lbm systems respectively the kinematic viscosity ν p h and density are adopted to represent the time and mass mapping in this study the length mapping l m is directly related to resolution in the simulation for example 512 lattice nodes represent the length of 5 11 mm with δ l p h 5 11 mm 511 0 01 mm the unity length is taken as the default value with δ l l b 1 and l m δ l p h δ l l b 0 01 mm the initial density is set as 1 and the mass mapping m m is ρ p h ρ l b l m 3 in the simulation the lattice viscosity can be obtained from eq 13 based on the time mapping t m ν l b ν p h l m 2 the mapping relationship of other variables can be found in ma et al 2022 4 results and analysis 4 1 quantitative evaluation of secondary roughness the classification criterion proposed by zou et al 2015 to determine the secondary roughness includes 1 the similarity of primary and original fracture surfaces 2 secondary roughness profile treated as a white noise random process obeying a gaussian distribution i e fig 5 3 secondary roughness is small enough compared to the primary roughness the value of the secondary roughness σ s can be calculated as the sum of the first j order of the height variance of the high order wavelet surface σ s σ h j wang et al 2016 as shown in fig 6a the red dotted line represents the criterion to determine the classification of secondary roughness according to the criterion in zou et al 2015 the secondary roughness is quantified for the fracture with different fractal dimensions fig 5 shows a frequency histogram for the height of secondary roughness and shapiro wilk s test p 0 05 shapiro and wilk 1965 which demonstrates that the secondary roughness profile satisfies the gaussian distribution condition the result indicates that the level of the classification to determine the secondary roughness decreases as the fractal dimension grows when the fractal dimension reaches 2 4 the primary surface a 2 is the classification of secondary roughness and when the fractal dimension reduces to 1 2 the primary surface a 5 is the classification of secondary roughness fig 6b shows the correlation between the secondary roughness and the fracture with different fractal dimensions the numbers in the red boxes in fig 6b represent the sum of the order of secondary roughness classification the secondary roughness increases with the increase of the fractal dimension the classification of secondary roughness is at the fifth order surface d 5 d when the fractal dimension is less than 1 6 however the secondary roughness has a sudden drop when the fractal dimension is 1 8 this is caused by the uncertainty of the classification of secondary roughness and it is a non quantitative concept as the secondary roughness approaches zero compared to the primary roughness if the red dotted line moves down the secondary roughness becomes the sum of the variance of d 1 d d 5 d rather than d 1 d d 4 d when the fractal dimension is 1 8 fig 6a in fact the sum of first three to five order of the variance of high order wavelet surface satisfy the criterion of secondary roughness for most fracture surfaces therefore an improved quantification criterion of secondary roughness need to be further studied the secondary roughness has an important effect on the non linear flow inside the primary fracture the classification criterion can be determined by the non linear flow properties however the fractal dimension of the primary fracture dominates the flow pattern therefore it is necessary to study the fractal dimension of the primary fracture influenced by the high order wavelet 4 2 quantitative analysis of primary fracture surface this section investigates the effect of secondary roughness on fracture geometry using power spectrum method power spectral density psd as an effective spectral analysis tool has been widely utilized in the analysis of different fracture geometric profiles chae et al 2004 pickering and aydin 2016 power and tullis 1991 the advantage of psd is that its statistics are not affected by the specific geometry size and resolution accuracy develi and babadagli 1998 the fractal dimension is obtained from the slope of the double log curve of spectral density function as shown in eqs 6 and 7 brown 1995 however the major disadvantage of this method is that it is difficult to get the proper slope of double log curve and large amount of high frequency noise of the fracture data klinkenberg 1994 furthermore fractal dimensions have large errors when the distribution of fracture height does not conform to the gaussian distribution therefore an improved method based on power spectral density is proposed to quantitatively evaluate the roughness of a rough walled fracture after wavelet transform to eliminate errors caused by the non normality of fracture distribution the rough wall fracture surfaces with different fractal dimensions are generated using synfrac as prefabricated template surface profiles the discrete fourier transform is used to calculate the psd of a template surface profile the third order polynomial function has been fitted to the psd plots ünlüsoy and süzen 2020 17 p s d p o l y f a 1 a 2 k a 3 k 2 a 4 k 3 the double log curve of the correlation of template profiles psd and spatial frequency using third order polynomial fits are shown in fig 7 the fractal dimension can be calculated from the area difference by comparing the psd of the sample and template the interpolation algorithm is given by comparing the smallest area difference of psd between the sample and three templates the fractal dimension is calculated by the ratio of area differences as follows 18 d s d i δ a i d i d i 1 δ a i δ a i 1 δ a i 1 δ a i 1 d i δ a i d i d i 1 δ a i δ a i 1 δ a i 1 δ a i 1 where d s represents the fractal dimension of the sample profile and δ a i represents the smallest area difference between the psd of the sample and template the specific calculation process is 1 prepare template data of the coordinates of fracture height from synfrac and remove the mean and linear trend of the height data of the fractures 2 window the result with a 10 cosine tapered window in the form 19 w n w 0 5 0 5 cos 2 π n w 5 n 0 n w n 10 1 n 10 n w 9 n 10 0 5 0 5 cos 2 π n n w 5 n n w 9 n 10 where w n w is the window coefficient at the n w position of the sample profile 3 apply twice the fast fourier transform to get the psd of each template fit the data of each template using a third order polynomial function in double log space i e fig 7 4 select the sample profile with the same resolution accuracy as the template psd of the sample surface profile can be obtained and fitted by a third order polynomial function 5 calculate area interpolation between the psd of the template fracture and sample fracture choose the smallest area difference as the standard template for calculating the fractal dimension of the sample profile by its upper and lower templates using eq 18 the method is simple and solves the error caused by high order noise it can also be applied to rough walled fractures without conforming to the gaussian distribution fig 8 shows the fractal dimension of a 2 using the psd area interpolation the green curve represents the psd of the a 2 surface black shading represents the area interpolation of the power spectrum curve the result indicates that a 2 is the closest to the template with a fractal dimension of 2 3 fig 8a by comparing samples with the power spectrum templates of d 2 4 and d 2 2 fig 8b and c the fractal dimension of a 2 is obtained by using eq 18 and found to be 2 27 fractal dimension of the primary fracture affected by the first fifth order wavelet is also analysed as shown in table 3 the results show that the influence of the first order wavelet on the fractal dimension is almost negligible however the second order wavelet greatly reduces the fractal dimension of the decomposed fracture the fractal dimension is less than 2 0 when the surface is decomposed to fourth order as the fracture surface is decomposed to the fifth order the fractal dimension of the fracture decreases suddenly there is almost no correlation between the fifth order primary fracture and the original fracture 4 3 non linear analysis of flow field along a rough walled fracture in this section the fluid flow simulations are carried out for the surfaces of a 0 a 8 to analyse the flow non linearity influenced by the secondary roughness with lbm the non darcy flow characteristics are quantitatively assessed using the forchheimer s law zimmerman et al 2004 yin et al 2019 guo et al 2020a 20 p a f q b f q 2 where a f 12 μ w e h 3 and b f β ρ w 2 e h 2 and β is the forchheimer coefficient based on eq 20 the non linear effect factor reflecting the non linear strength of the flow field is defined as follow chen et al 2015 yin et al 2018 21 α b f q 2 a f q b f q 2 fig 9 shows the best fitted curve of the correlation between flow rate and pressure using forchheimer equation the forchheimer coefficient is listed in the rough walled fracture with different fractal dimensions by wavelet transform as shown in table 4 the results of b f indicate that the non linearity of flow field gradually decreases with increasing wavelet classification the sum of first fifth order wavelet can be the secondary roughness of the fracture with fractal dimensions of 1 2 and 1 6 using zou et al s criterion when the fractal dimension is 1 2 and 1 6 shown in fig 9a and b the difference of the flow rate is not large at a 0 a 5 which indicates that secondary roughness has little effect on the flow rate along fractures with a small fractal dimension however the flow rate variation influenced by first fifth order wavelet is much larger than the cases with small fractal dimension when the fractal dimension is 2 0 and 2 4 shown in fig 9c and d the secondary roughness causes obvious flow non linearity phenomenon in a fracture with a high fractal dimension furthermore as shown in fig 9a d when the surface is decomposed to a 6 a 8 the variation of the flow rate does not change much at each fractal dimension the main factors affecting the flow field are the aperture the primary roughness and the secondary roughness of the fracture when the fracture surface aperture is consistent and the high order wavelet is filtered out the influence of secondary roughness is almost ignored due to the relatively smooth surface the only variable of a 6 a 8 fracture surface is the primary roughness which represents the undulation of fracture surface the undulation of fractures with different fractal dimensions is similar at the sixth to eighth order therefore the flow pattern in a 6 a 8 is mainly dominated by the fracture undulant properties rather than the fracture secondary roughness furthermore the significant difference of flow rate is mainly caused by the first fourth order wavelet for the fracture with different fractal dimension such as the blue black green and pink curves shown in fig 9a d therefore the flow behavior is mainly affected by the wavelet from first order to the fourth order to further analyse the influence of wavelets at all levels on flow non linearity the non linear coefficients are compared in original fracture surface and eighth order primary surface fig 10 when the fractal dimension is 1 2 and 1 6 the non linear coefficient increases only with the pressure gradient and is not affected by the wavelet decomposition of the sub surface when the fractal dimension is 2 0 the non linear differences of the sub surfaces gradually appear there is a large non linear difference of the sub surfaces at all levels when the fractal dimension is 2 4 thus when the fractal dimension is less than 2 0 the secondary roughness has little effect on the flow non linearity the effect of secondary roughness on non linear flow behavior must be considered when the fractal dimension is higher than 2 0 therefore taking d 2 0 as the classification boundary the fourth order fracture surface is selected as the critical fracture surface refer to the case with d 2 4 in table 3 fig 10 also presents the mean non linear coefficient error α e between the original fracture surface a 0 and the fourth order primary fracture surface a 4 with different pressure gradients which is defined as 22 α e 1 n i 0 n α o α p α o where α o and α p represent the non linear coefficient of original fracture surface and fourth order primary fracture surface respectively the mean non linear coefficient error increases with the increase of the fractal dimension when the fractal dimension is 2 0 and 2 4 the mean non linear coefficient error reaches 2 15 and 23 0 the results indicate that secondary roughness has great influence on flow non linearity when the fractal dimension of original fracture is greater than 2 0 furthermore fig 11a shows an exponential relationship between primary roughness and flow rate the primary roughness barely changes while the flow rate changes significantly when the fractal dimension is higher than 2 0 which indicates that the flow behavior is mainly affected by the secondary roughness when the fractal dimension is less than 2 0 the primary roughness changes greatly which indicates that the fracture surface after the wavelet transform has lost the main property of the original fracture surface fig 11b shows the correlation between the high order wavelet and the difference of flow rate of two adjacent primary surfaces with wavelet transform when the fractal dimension is higher than 2 0 the flow difference has a linear relationship with first fourth order wavelet however the linear correlation is not valid when the fractal dimension is less than 2 0 therefore the first fourth order wavelet results in the linear trend of the flow rate difference according to the above analysis the classification criterion of secondary roughness is redefined as follow the classification of fracture surface is the fifth order primary surface according to the criterion in zou et al 2015 when the fractal dimension of the original fracture is less than 2 0 the classification of fracture surface is the first primary surface with fractal dimension less than 2 0 when the fractal dimension of the original fracture is larger than 2 0 the improved criterion is determined by the effect of high order roughness on the fracture fractal dimension and non linear flow behavior although the fifth order surface is the classification criterion surface of secondary roughness of fractures with fractal dimensions of 1 2 and 1 6 the flow rate difference between the original fracture surface and the fifth order fracture surface is less than 1 the flow rate changes by about 2 7 compared with the case considering the secondary roughness when the fractal dimension is 2 0 therefore the effect of secondary roughness can be ignored for the fractures with fractal dimension less than 2 0 furthermore for fractures with fractal dimension larger than 2 1 the four order surface can be selected as the classification criterion surface for fractures with fractal dimension from 2 0 to 2 1 it is recommended to take the third order surface as the classification criterion surface the suggested classification for the secondary roughness can be referred to table 3 4 4 piecewise equivalent cubic law model piecewise equivalent model is established to describe the relationship between flow behavior and primary and secondary roughness in this section fig 12 illustrates the local velocity contour of rough walled fracture after wavelet transform the rough walled surface gradually becomes smooth with the increase of surface classification when the fractal dimension is higher than 2 0 fig 12a and 12b numerious burrs are distributed along the fracture surface the uneven distribution of the velocity field is affected by the burrs and undulation of the fracture when the fractal dimension is less than 2 0 the fracture surface is relatively smooth which indicates that the flow field is just subject to the undulation along rough walled fracture when the fracture surface is decomposed to the sixth order fig 12d the flow field can be approximately regarded as a darcy flow and the velocity field is parabolically distributed along the aperture direction therefore the surface roughness characteristic should be divided into two parts undulation and burrs which are determined by the primary and the secondary roughness respectively to quantitatively describe the flow behavior an equivalent model learned from the form summarised by previous scholars wang et al 2015 2018 ju et al 2019 is expressed as 23 q w e m 3 12 μ δ p l 1 1 σ a x y the standard deviation of the fracture height can be regarded as the superposition of the high order wavelet of each level surface and the primary roughness of the last level surface 24 σ σ h a 1 σ h a 2 σ h a 3 σ h a j σ p a j when the fractal dimension is higher than 2 0 the surface a j is selected as the surface whose fractal dimension is just less than 2 0 based on the definition of secondary roughness eq 24 can be written as 25 σ σ s σ p a j d 2 0 σ σ p a j d 2 0 when the fractal dimension is less than 2 0 the effect of secondary roughness is ignored therefore a piecewise equivalent model is established to quantify the correlation between flow rate and secondary roughness 26 q w e m 3 12 μ δ p l 1 1 σ p e m x y 1 f σ s d 2 0 w e m 3 12 μ δ p l 1 1 σ p e m x y d 2 0 the above formula shows that the reduction of flow rate is affected by two aspects primary roughness and secondary roughness which are characterised by undulation and burrs furthermore the hydraulic aperture e h permeability k can be derived as 27 e h e m 1 σ p e m x y 3 f σ s 1 3 d 2 0 e m 1 σ p e m x y 3 d 2 0 28 k e m 2 12 1 σ p e m x 2 y 3 f σ s 2 3 d 2 0 e m 2 12 1 σ p e m x 2 y 3 d 2 0 the effect of secondary roughness is assumed as f σ s to determine the model coefficients and the form of f σ s the classical form proposed by renshaw is adopted by the numerical fitting method in this paper where x 2 from a commonly used value of the previous model renshaw 1995 y is defined as the piecewise coefficient which is calculated from the flow rate in the fracture with a fractal dimension of 2 0 fig 13a shows the change of piecewise coefficient with absolute roughness the piecewise coefficient shows the exponential decrease relationship with the increase of absolute roughness the relationship between f σ s and σ s can be established when the piecewise coefficient is determined the primary and secondary roughness is non dimensionalized by introducing the mechanical aperture and the characteristic length of secondary roughness l σ s 1 0 5 m by comparing the numerical results of different combinations of σ s and f σ s exponential relationship f σ s l σ s λ e η σ s l σ s is found to be a reasonable function where f σ s l σ s is defined as the secondary roughness intensity factor and η and λ are the fitted coefficient fig 14a shows the best fitted curve of f σ s l σ s and dimensionless secondary roughness using numerical results when the absolute roughness σ a 1 0 where f σ s l σ s is obtained by calculation of the numerical results by eq 26 the exponential function can describe the relationship between flow rate and secondary roughness the flow behavior along rough walled fractures is only discussed in the weak inertia region with r e 6 5 in this study wang et al 2015 zimmerman et al 2004 therefore the pressure difference has little effect on the fitted coefficient the fitted parameters are further analyzed with different absolute roughness fig 13b and c shows the change of fitted coefficient with absolute roughness when σ a is less than 1 18 λ is stable at 0 64 however when σ a is large λ becomes unstable η increases first and then reduces with the increase of absolute roughness both λ and η determine the strength of secondary roughness the reference values of parameters are shown in table 5 for the application of the equivalent model furthermore the validity of the equivalent model is further demonstrated by comparing with the model proposed by patir and cheng 1978 the patir model is an empirical formula obtained by theoretical derivation and numerical verification which has been widely validated zimmerman and bodvarsson 1996 zhang et al 2016 zhang and chai 2020 gao et al 2023 29 q w e m 3 12 μ δ p l 1 0 9 e 0 56 e m σ p fig 14b shows the flow rate versus pressure gradient using cubic law patir model renshaw model and the developed equivalent model for fractures with identical aperture and primary roughness the result of equivalent model with d 2 0 is used to compare with the patir model which ignores the influence of secondary roughness the error between the equivalent model and patir model is less than 1 5 the accuracy of the equivalent model is greatly improved compared with cubic law and renshaw model the results show that the mechanical aperture and primary roughness are not the only two factors to affect the flow rate the decrease of flow rate influenced by the secondary roughness is greater than that only considering primary roughness when the fractal dimension is 2 4 in this case the flow rate is mainly dominated by the effect of secondary roughness even if the primary roughness and the fracture aperture remain constant table 6 compares the hydraulic aperture and permeability with different models renshaw model and louis model consider only the primary roughness of the fracture the permeability is similar to that of the equivalent model with d 2 2 in fact the two models estimate only two cases of constant secondary roughness the permeability is also calculated using the rasouli model with the fractal dimension of 2 4 the transformation relationship between jrc and fractal dimension adopts the method proposed by li and huang 2015 and jrc is 13 1 in this study rasouli and hosseinian 2011 the results shows that rasouli model ignores the influence of the secondary roughness which leads to overestimate the hydraulic aperture and permeability of the fracture compared with the equivalent model in summary when the secondary roughness effect is obvious none of the above three models can effectively estimate the permeability characteristics of the fractures therefore the increase of secondary roughness greatly reduces the hydraulic aperture and permeability which indicates that the burrs on the fracture surface enhance flow non linearity resulting in the decrease of the flow rate the flow rate loss caused by secondary roughness and primary roughness is further analyzed with different fractal dimensions and mechanical apertures based on the equivalent model as shown in fig 14c and d δ q 1 δ q 2 and δ q 3 represent the flow rate loss by primary roughness secondary roughness and total flow rate loss respectively 30 δ q 1 w e m 3 12 μ δ p l 1 1 1 σ p e m x y δ q 2 w e m 3 12 μ δ p l 1 1 σ p e m x y 1 1 f σ s δ q 3 w e m 3 12 μ δ p l 1 1 1 σ p e m x y 1 f σ s when the fracture aperture remains constant δ q 1 is constant and δ q 2 increases with the increase of fractal dimension δ q 1 is larger than δ q 2 when the fractal dimension is up to 2 45 the secondary roughness dominants the change trend of the total flow rate when the fractal dimension remains constant both δ q 1 and δ q 2 increases with the increase of mechanical aperture however the growth rate of δ q 2 is less than that of δ q 1 with the increase of fracture aperture which indicates that the effect of secondary roughness is weakened compared with the primary roughness when the mechanical aperture increases in summary the equivalent model considers the influence of the primary and secondary roughness on the flow along the fracture with complex geometry compared with the cubic law which can be used in the fracture with high roughness and undulation the equivalent model reveals non linear flow mechanism influenced by the secondary roughness and describes the non linear flow behavior simply and accurately by incorporating the effect of fracture secondary roughness 5 conclusions and prospects in this study a piecewise equivalent model is developed to characterise the relationship between primary and secondary roughness and flow rate inside the fracture setting the fractal dimension 2 0 as the piecewise point the rough walled fracture influenced by the primary and secondary roughness is quantitatively evaluated based on the wavelet transform and the power spectrum method a classification criterion is then proposed to accurately capture the secondary roughness by investigating the non linear flow behavior along rough walled fractures using the lbm the main conclusions are summarised as follows the sum of first three to five order of the high order wavelet satisfies the criterion of the secondary roughness for most fracture surfaces current classification criterion of secondary roughness cannot achieve unique quantification of all original fracture surfaces with different fractal dimensions the improved classification criterion of secondary roughness is redefined as follow the classification criterion surface is the fifth order primary surface when the original fracture fractal dimension is less than 2 0 while the classification criterion surface is the first primary surface with fractal dimension less than 2 0 when the original fracture fractal dimension is larger than 2 0 the modified power spectrum method can effectively estimate the fractal dimension of the fracture without following the normal distribution the influence of high order wavelet on the fractal dimension of rough walled fracture surfaces is analysed the effect of the first order wavelet on the fractal dimension is almost negligible the wavelet from the second order to the fourth order greatly decreases the fracture roughness the fractal dimension of surface is less than 2 0 when the fracture surface profile is decomposed to the fourth order there is almost no correlation between the fifth order primary fracture and the original fracture higher fractal dimensions enhance the fracture secondary roughness which greatly increases the non linear behavior of the flow field however the effect of the secondary roughness on flow non linearity is negligible when the fractal dimension is less than 2 0 the decrease of flow rate is mainly caused by the primary roughness and secondary roughness and the secondary roughness significantly increases the non linearity of the flow field when the fractal dimension is larger than 2 0 a quantitative piecewise model considering the effect of secondary roughness is established to predict the equivalent flow behavior in a rough walled fracture it is indicated that the non linear flow pattern is influenced by both undulation and burrs in fractures with higher fractal dimension d 2 0 the variation of flow rates is greatly influenced by burrs and thus dominated by the secondary roughness even if fracture apertures and primary roughness are identical the increase of secondary roughness greatly reduces the hydraulic aperture and the permeability and the effect of secondary roughness is weakened compared to the primary roughness when the mechanical aperture of the fracture increases nevertheless when the absolute roughness is large the fitted coefficient of the equivalent model changes irregularly the influence of fitted coefficient on the secondary roughness intensity factor needs further study at large absolute roughness credit authorship contribution statement chunlei ma conceptualization methodology software data curation writing original draft visualization investigation yun chen supervision methodology validation writing review editing funding acquisition xiaoliang tong supervision validation reviewing funding acquisition guowei ma conceptualization methodology supervision declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements this research was supported by the national natural science foundation of china grant nos 52061160367 u1965204 52109121 science and technology program of hebei grant nos e2022202041 e2021202073 e2020050015 the support from the project of key technologies of seepage control system for large scale hydraulic projects was also gratefully appreciated 
