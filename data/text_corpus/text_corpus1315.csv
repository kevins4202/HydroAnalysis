index,text
6575,model selection and model averaging have become popular tools to address conceptual uncertainty in hydro geo logical modeling within the last two decades many different flavors of approaches and implementations have emerged which complicate an easy access to and a thorough understanding of the underlying principles with the many approaches and applications a variety of terms has been defined which easily leads to misunderstandings and confusion among the community here we review bayesian model selection bms and averaging bma as a rigorous statistical framework for model choice under uncertainty we aim to clarify the theoretical foundations of both methods their relationship to one another and to alternative approaches and implications of implementation choices we further build a bridge to bayesian model combination bmc which turns out to be bayesian averaging or selection of combined models bcma bcms concluding from our theoretical review we argue that the goal and the philosophical perspective of modeling should be main drivers when choosing to use the b c ms a toolbox and we offer guidance to identify the suitable approach for specific modeling goals with this review we hope to further strengthen the utility of bayesian methods in the face of conceptual uncertainty by directing their use into the right channels keywords multi modeling conceptual uncertainty model selection model averaging model combination bayes theorem information criteria hydrological modeling groundwater modeling 1 introduction all you really need to know for the moment is that the universe is a lot more complicated than you might think even if you start from a position of thinking it s pretty damn complicated in the first place adams 1979 in hydrosystem modeling we are confronted with several sources of errors and uncertainty input and output uncertainty due to noisy measurements and scale disparities parameter uncertainty due to sparse observations and heterogeneities and model structural uncertainty conceptual uncertainty due to simplification of the underlying physical processes e g renard et al 2010 clark et al 2011 refsgaard et al 2012 elshall and tsai 2014 the stochastification of hydrological models aims to translate these uncertainties into realistic uncertainty estimates for model predictions liu and gupta 2007 montanari and koutsoyiannis 2012 nearing et al 2016 recently multi model approaches to explicitly address conceptual uncertainty have gained renewed interest gupta et al 2012 e g based on bayesian probability theory draper 1995 or information theory gong et al 2013 multi modeling means to employ an ensemble of several distinct model candidates to simulate a certain phenomenon or occurrence e g burnham and anderson 2003 these models may differ in their conceptuality e g being physics based or data driven or in their level of detail e g scale or resolution but they all yield the same desired quantity as model output fields of application in hydro geo logy include among others groundwater modeling rojas et al 2010 reactive transport modeling lu et al 2013 soil plant atmosphere modeling wöhling et al 2015 and hydrological modeling marshall et al 2005 quantifying or at least approximating conceptual uncertainty not only allows to equip model results with uncertainty bounds but also to determine the information content in existing data with respect to model choice wöhling et al 2015 and to apply formal optimization procedures toward reduced uncertainty such as optimal design of monitoring kikuchi et al 2015 nowak and guthke 2016 pham and tsai 2016 although there seems to be consensus that following the method of multiple working hypotheses chamberlin 1890 may shield us from overconfident and potentially biased single model predictions clark et al 2011 there is less awareness that modeling results may vary dramatically with the chosen approach to multi modeling a considerate choice of methods is crucial but challenging within the last two decades many different flavors of multi model approaches and implementations have emerged which complicate an easy access to and a thorough understanding of the underlying principles with the many approaches and applications a variety of terms has been defined which easily leads to misunderstandings and confusion among the community and promotes ad hoc modeling decisions to counteract this development we review rigorous bayesian approaches to model selection and averaging which comprise the well known frameworks of bayesian model selection bms and bayesian model averaging bma but also extend to cross validation and model combination techniques more details and references are provided in section 2 we aim to clarify their theoretical foundations including their philosophical view on modeling their relationship to alternative non bayesian methods and implications of implementation choices we further contrast bms bma with bayesian model combination bmc which turns out to be bayesian averaging or selection of combined models to be defined below we argue that the goal and the philosophical perspective of modeling should be main drivers when choosing to use either such tool and we offer guidance to identify the suitable approach for a specific modeling goal this review intends to address modelers in science and practice with a basic background in statistics who strive a more fundamental understanding of model selection averaging and combination concepts in contrast to previous well received surveys and tutorials e g draper 1995 hoeting et al 1999 wasserman 2000 chipman et al 2001 monteith et al 2011 vehtari and ojanen 2012 we refrain from using equations or numerical examples for demonstration referring to gelfand and dey 1994 instead but aim to provide a compact version of the context necessary to understand and choose the most suitable model selection or averaging approach for a specific type of application since in hydrology we are typically confronted with difficult conceptual modeling decisions and limited data sets we focus on methods and theories that are relevant for such situations and avoid discussing unrealistic limiting cases e g infinite sample asymptotics as much as possible still our conclusions and recommendations are derived from a theoretical analysis that ensures validity over a wide range of situations and modeling disciplines 2 key concepts of bayesian multi modeling there is a theory which states that if ever anyone discovers exactly what the universe is for and why it is here it will instantly disappear and be replaced by something even more bizarre and inexplicable there is another theory which states that this has already happened adams 1979 2 1 classification of approaches working with a set of competing models hypotheses we can follow two intuitively distinct modeling approaches the winner take all perspective of model selection or the team of rivals perspective of model averaging fig 1 in the case of model selection the goal is typically to identify relevant hydrological processes among a set of alternative hypotheses wagener and gupta 2005 in the spirit of hypothesis testing or more pragmatically to reduce the considered model space in order to keep the computational effort for prediction low in this context it is crucial to focus on the primary purposes of modelling i e to generalize to future data or process identification both of which we will put into perspective in section 2 2 the team of rivals perspective ferré 2017 advertises model averaging in its most general form deliberately keep more than one model for inference and prediction for fear of missing something when constraining the model set to a single hypothesis model averaging allows to combine the best properties of different models with the hope to obtain a combined estimate that is more skillful and reliable than any of the individual models while the terms model averaging and model combination are sometimes used interchangeably as in the previous sentence we will provide a more careful definition and distinction below see section 2 3 ff clarifying which notions of model averaging exist regarding model weighting and uncertainty quantification a comprehensive overview of both model selection and averaging from a statistics point of view is offered by claeskens 2016 besides the intuitive classification of multi model approaches into selection and averaging the philosophical roots of the different approaches in each class call for their own classification each method has been derived in a specific mindset using specific assumptions and therefore only yields readily interpretable results in their respective realm probably the most fundamental issue regarding classification from the philosophical perspective of modeling is whether or not a true model exists either we do believe that the true underlying model i e the full mathematical specification of the modeled system can in principle be written down and be identified or we accept that the truth can only ever be approximated to some degree many authors have pointed this out before e g burnham and anderson 2003 claeskens 2016 but its importance for choosing the most appropriate statistical method to address model choice uncertainty cannot be overstated modeling of physical systems as done in hydrology normally does not allow for assuming that the true model is actually in the set of available models let alone that it is possible to formulate it ever the idea of constructing a model that fully represents the true system with all its details is illusory however depending on the relevance of system components on the modeled scale we might be able to at least set up what we call a quasi true model a quasi true model might e g not resolve the exact physics of molecular reactions behind declining concentrations but on the considered scale the assumed order of reactive decay might adequately reflect the system s lumped chemistry e g in a river segment however quasi true models shall not be confused with models that merely mimic the data in order to approximate the truth an example of a mimicking model would be using a power law instead of exponential decay the former approximates the latter while the latter is the actual solution to the quasi true process model described by an ordinary differential equation of first order for a certain range of data a power law with a constant exponent might yield a very close approximation to the quasi true exponential model and is therefore also generalizable to a certain degree beyond that range nonetheless a mimicking model is neither true nor quasi true because it does not reflect the underlying causal relation hence considering whether we operate potentially quasi true or approximatively mimicking models to achieve our modeling goal also pertains to using the adequate multi model approach after reviewing bayesian multi model concepts from a theoretical point of view in the remainder of section 2 and from a practical point of view in section 3 we will argue in section 4 that by including the philosophical modeling perspective it is possible to make a more informed and less ad hoc choice between bms bma bmc vs alternative methods that suits the individual modeling purpose 2 2 bayesian model selection 2 2 1 identification of a quasi true model via bms if we assume that there is a true model to be identified then we should adopt a statistical model selection method that fulfills exactly this job description bms raftery 1995 is one such method that promises to point the modeler to the true hypothesis if the true model is in the available set of models at least in the limit of infinite data set size this is referred to as consistent model selection shibata 1986 hurvich and tsai 1989 for a finite amount of data identification of the true model may not be possible because the candidate models may not be sufficiently well distinguishable through the blurred eyes of the data when in doubt i e in cases of similar model performance bms will prefer simpler models due to the built in principle of parsimony a k a occam s razor gull 1988 with growing data set size bms will identify the true complexity with increasing confidence as opposed to other model selection techniques see below this behavior of bms has been demonstrated on a case of hydrogeological model selection by schöniger et al 2015b as a useful side effect bms is able to detect model dis similarity if two models have been set up in presumably different ways but are general enough to look the same through the eyes of the data at hand bms will yield an undecisive ranking in bms probabilities are assigned to all model candidates that express the degree of plausibility of being the quasi true model these probabilities are referred to as model weights and are updated using bayesian model evidence bme the higher a model s bme compared to the other models the higher its updated probability to be the true model bme can be understood as a global measure of fit integrated over the whole parameter space of the model and is therefore also referred to as marginal likelihood the bme values can then be transferred to so called posterior odds i e model weight ratios or bayes factors kass and raftery 1995 which indicate how much more plausible a candidate model is compared to an alternative one the computational effort of evaluating bayesian model weights can become prohibitive in practical applications e g for non linear models large parameter spaces high model dimensionality or long model computing times this is why computationally efficient approximations via the kashyap information criterion kic kashyap 1982 or the bayesian information criterion bic schwarz 1978 are frequently used to estimate the logarithmic bme cf section 3 3 we will call both full bms and its approximations bms type identification methods in allusion to their philosophical quest to identify the true model 2 2 2 approximation of truth via bayesian cross validation if we do not believe in the notion of a true model our goal is to approximate the truth as closely as possible to make realistic predictions this assumption is the crucial distinction from bms in section 2 2 1 with additional observed data our picture of truth may even change and so should our selected model in order to catch up with our evolving level of knowledge this is what cross validation cv techniques and approximations thereof provide briefly some of the available data is held out as validation data while the remaining data is used for model calibration then the performance of each calibrated model in accurately and precisely predicting the validation data is evaluated and the models are rated accordingly this method directly evaluates the predictive capability of a model i e its ability to generalize to new data the key question is how closely should my model approximate the data i e mimic the truth to avoid underfitting of process details and overfitting of noise the purpose of model selection is then to find the model that strikes an optimal balance between lack of accuracy bias and lack of precision variance forster 2000 hastie et al 2009 this balance is meant when referring to generalizability of a model hastie et al 2009 in this spirit bayesian cv for model rating works probabilistically and targets at finding the model with highest predictive density for new data although bayesian cv is perfectly suited to select the model that provides the best balance it comes with large computational effort for repeated calibration and validation and suffers from instabilities when confronted with limited data set sizes e g gelman et al 2014 the akaike information criterion aic akaike 1973 offers a computationally undemanding alternative derived in the context of information theory the aic aims to minimize the expected information loss measured as kullback leibler divergence when approximating truth with candidate models and is shown to be asymptotically equivalent to cv stone 1977 limitations by restrictive assumptions of the aic can be overcome by using e g the corrected aic aicc hurvich and tsai 1989 the deviance ic dic spiegelhalter et al 2002 or the widely applicable ic waic watanabe 2010 we consider these methods to be bayesian model selection techniques because they act on posterior probability density functions pdfs of model parameters and or predictions even though they differ in their degree of accounting for prior knowledge yet they are not to be confused with bms because their model selection result does not allow for an interpretation of probability of being the true model we therefore call them cv type approximation methods in contrast to bms type identification methods keeping the typical information theoretic notion of no true model exists in mind ye et al 2008 it is clear that the aic or other cv type methods can never yield a rating on how good the approximation is on an absolute scale but only relative to other models burnham and anderson 2003 i e which model is closer to the unknown truth compared to others these methods guarantee to select the model that promises best predictions given the current data despite having an unidentifiable true model this property of still finding a best approximative model based on only limited data is called asymptotic efficiency shibata 1980 and is often contrasted to bms type consistency e g yang 2005 claeskens and hjort 2008 vrieze 2012 consistency and efficiency represent two kinds of optimality aho et al 2014 that cannot be combined in a single model selection method claeskens and hjort 2008 burnham and anderson 2004 because they are developed for deviating settings which is formally proven by yang 2005 despite this distinction there are studies where model weights are approximated via kic bic and additionally via actual information criteria ic such as the aic with results often being contradictory and therefore inconclusive e g poeter and anderson 2005 ye et al 2008 tsai and li 2008 singh et al 2010 foglia et al 2013 this does not come as a surprise because some of the used criteria work under deviating assumptions schöniger et al 2014 and pursue different objectives höge et al 2018 2 2 3 the role of model complexity it is quite confusing why the endeavor to identify or to approximate the truth should be completely different routes to take in model selection however keeping the notion of an either existing or non existing true model in mind it is important to understand that these two goals can only be pursued by respective model selection methods aho et al 2014 the difference between the two worlds becomes apparent when looking at their respective takes on the complexity of the truth to be modeled and the implications for over and underfitting höge et al 2018 and references therein for this it is important to apprehend every model selection procedure in theory to be sequentially iterated while including more data if there was more time money workforce etc with each iteration the applied method fulfills its intended identification or approximation purpose successively better as illustrated in fig 2 in practice we usually have a finite amount of data available and therefore obtain a snapshot of this underlying theoretic behavior model complexity in this context does not require a specific and comprehensive definition which by itself poses an unresolved problem see e g gell mann 1995 we can think of it as notion of number of parameters and functional terms their non linearity and interrelation degree of detail etc the target of bms type selection is the true model of fixed and finite complexity as to see in fig 2 all models that are more complex than the truth overfit and the ones that are not complex enough underfit in principle independently of data size however for small data set sizes bms type methods tend to prefer simpler models over a more complex true model in a conservative manner hurvich and tsai 1989 burnham and anderson 2003 it is therefore advised to assess the identifiability of a certain level of true model complexity through the eyes of the current data set schöniger et al 2015a guthke 2017 if none of the candidate models can be identified as the true model yet due to a too small sample size more informative data need to be collected first before ranking the model set with bms type methods if the truth was infinitely complex a true model could not even be hypothesized hence the target model of cv type selection is not a fixed one but depends on the data set size i e the model that catches up best with the data generating process given the currently available data under the implicit assumption that none of the models in the set is true cv type methods always try to find the model that is the closest approximation of the truth but also not too close that noise is fitted rather than the unknown truth especially complex models with many degrees of freedom tend to such overfitting because the information in the currently available data is insufficient to fully invert the model i e to constrain its degrees of freedom overfitting models cannot justify their complexity for approximating the unknown true model on the current data while underfitting models lag behind in their complexity for the current data cv methods prevent both by enforcing appropriate complexity in the validation step however cv type methods can also switch to more complex models that did overfit before when the data size grows with more informative data the risk of fitting noise declines while more and more of the real infinite complexity becomes visible see fig 2 then a more complex model might become a better approximation of the truth more information supports higher complexity this potential behavior however sometimes triggers cv type methods to overrate too complex models in their performance while trying to approach the truth more closely while guarding against overfitting at a certain data level cv type methods always question whether a more complex model would be more appropriate as soon as more data is included i e are prone to the risk of overfitting hurvich and tsai 1989 ng 1997 burnham and anderson 2003 the intention to mitigate this risk has lead to e g the development of the so called corrected aic or aicc sugiura 1978 that takes the data set size into account 2 3 bayesian model averaging 2 3 1 quantifying indecisiveness of bms via bma if evidence in the data seems insufficiently clear to uniquely identify the most true model via bms we can decide to keep potentially true models in the set and weight them according to their probability to be the true model this is the principle behind bma draper 1995 in its original form bma uses the exact same posterior model weights as determined in bms section 2 2 1 to combine the individual models predictions into a weighted average the fundamental set of equations can be found in e g hoeting et al 1999 and for completeness also in appendix a for applications of the original bma approach we will refer to it as full bma in hydro geo logy see e g schöniger et al 2014 schöniger et al 2015a wöhling et al 2015 schöniger et al 2015b liu et al 2016 zeng et al 2018 it is important to realize that both bms and bma assume that a true model exists is available and can be identified if there was just enough data as such the bma result can be considered to be a preliminary result on the way towards bms minka 2002 therefore despite appearing as a forecast driven team of rivals approach at first sight fig 1 bma is actually an intermediate snapshot on a winner take all race as long as the amount of data is insufficient for a clear identification of the true model the bma strategy is to keep all potentially true models in the set and weight them according to their probability to be the true model with more information coming in the weight of the most true candidate model will converge to 1 and all others will diminish this is illustrated for a two model scenario in fig 3 with model m1 being the quasi true model and the bma predictive pdf converging to the predictive pdf of model m1 for increasing amount of data the uncertainty about choosing the most plausible model in the set is reflected by the width of the bma predictive pdf and can be measured as e g bma variance hoeting et al 1999 or entropy zeng et al 2016 the progress of bma to bms while converging to the allegedly true model as displayed in fig 3 can be illustrated by a simple conceptual exemplification for an observed decline in concentration of a substance two experts might provide two plausible hypotheses the first expert hypothesizes the concentration decrease results from only microbial consumption m1 and the second expert claims that solely abiotic reactions cause the decline m2 each expert comes up with a model that contains the mathematic formulation of their respective process if bma was applied to both models it would prefer one over the other and would do so increasingly clearly with more included data from the decline bma assumes only one of the two models can be true and tries to identify it bma would not settle with the weights of the two processes like 75 biotic and 25 abiotic under growing data size even if this ratio represented what actually happened in reality 2 3 2 approximative methods confusingly labeled as bma besides its original meaning the term bma has been misleadingly used for a different form of pdf averaging as proposed by raftery et al 2005 this approach rests on the idea of mixing gaussian predictive pdfs and determining the mixing coefficients with the help of the expectation maximization em algorithm mclachlan and krishnan 2007 we therefore propose to refer to it as gaussian mixture model averaging gamma the crucial difference is that bma rests on the assumption that exactly one model has generated the data whereas gamma assumes that the data can be best described by a mixture pdf and aims to identify the true proportions of the candidate models pdfs also in analogy to bma where model weights might come from bms methods like kic or bic cf 2 2 1 model averaging is sometimes performed based on weights from cv type criteria like aic dic or waic this is occasionally called pseudo bma geisser and eddy 1979 yao et al 2017 however this is not equivalent to bayesian model averaging as denoted bma here and should not be confused model averaging based on aic and alike does not have a rigorous statistical foundation wasserman 2000 and hence there is no clear interpretation of results in a bma sense we strongly advise to use the term bma only for the original bayesian approach to model averaging as presented in hoeting et al 1999 or draper 1995 and to indicate any deviating approach with a clearly distinguishable term this should help reduce the confusion about the applicability and interpretation of bma in the hydrological community 2 4 bayesian model combination 2 4 1 pdf averaging vs forecast combination note that the bayesian multi model approaches discussed above act on the level of predictive pdfs recall fig 3 and not on individual realizations or on the best estimate forecast itself hence averaged quantities such as the expected value only have statistical meaning not a physical meaning the fact that the bma mean may be physically meaningless is a frequent criticism of bma which however results from a misunderstanding of what bma is by combination of individual models pdfs we do not obtain the pdf of an improved model but we obtain a multi model pdf that envelopes all forecast scenarios that seem plausible given the current state of knowledge understanding bma as an intermediate result on the way to bms avoids the misinterpretation of the bma mean indeed when choosing to apply bma we should naturally be interested in the full bma pdf instead most other model averaging techniques act on the level of forecasts and are therefore referred to as model combination approaches fig 1 this means that individual forecasts e g model realizations or best estimate forecasts are being combined between different models to obtain a new type of forecast that is beyond of what the individual models could achieve model combination aims at finding a composite model that is superior to the individual proposed models because individual structural deficits are ideally compensated within a good combination this is fundamentally different from bms or bma not only in the resulting mean but also in predictive uncertainty pdf averaging aims at obtaining an ideally fully encompassing typically wide and multi modal predictive pdf whereas the pdf of a combined model will be much more focused i e with a lower variance than the lowest individual variance under certain conditions see bates and granger 1969 further the pdf of a combined model will not reflect conceptual uncertainty about choosing one of the models in the set which emphasizes the different philosophies that exist under the hood of model averaging fig 1 a variety of model combination weighting schemes on forecast level has been proposed in order to produce a most accurate combined model for an unknown truth e g equal weights averaging ewa or bates granger model averaging bga with weights based on the forecast variance most averaging rules rely on simplex weights positive weights that sum to one granger and ramanathan 1984 have proposed to move away from this constraint in order to improve predictive performance of the combined estimator via bias correction granger ramanathan averaging gra further so called ensemble methods like bagging or boosting exist kim and ghahramani 2012 and references therein where combination implies mainly setting up model ensembles and applying distinct model training schemes in a way that the ensemble members mutually counteract bias or variance of forecasts a comparison of combination approaches for hydrological applications yet without consideration of their philosophical differences has been performed by e g diks and vrugt 2010 2 4 2 identification of a quasi true combined model via bcms it is important to understand that bma will converge to the individual model that is closest to the truth instead of converging to model weights such that the weighted combination of models is closest minka 2002 monteith et al 2011 we suppose that what many users actually seek when they choose bma is such an optimally weighted combination especially in situations when they do not believe that the quasi true model is in the set but is somehow encircled by the models in the ensemble in this situation a model space enrichment procedure that fills the inner space spanned by the proposed models is bound to be beneficial and an objective selection of the most suitable proposition in the enriched hypothesis space is even more helpful in this spirit we suggest to follow the argument of minka 2002 and apply bma to a new hypotheses space of stacked models we term this procedure bayesian combined model averaging bcma to highlight its relation to both model combination and bma monteith et al 2011 use the more elegant but less precise term bayesian model combination practically bcma means applying the equations for bma see appendix a not to the individual model alternatives but to forecast combinations of these individual models these combined models are predefined by the modeler and bcma weights them according to how well each of them matches the truth just like bma does this for individual models note that in the very same manner as bma is a preliminary step on the way to bms bcma can be understood as a preliminary result on the way to bayesian combined model selection bcms for an application of bcma and bcms refer to monteith et al 2011 and for proposed combination schemes in statistical modelling or hydrology that work in a similar fashion see kim and ghahramani 2012 or ajami et al 2007 respectively in the examples just referenced model weights for combination are not based on marginal likelihoods as in bma but are obtained by other means such as an em approach see section 2 3 2 or drawn from a distribution of weights in the former two examples monteith et al 2011 kim and ghahramani 2012 the combinations of statistical classification models often encountered in machine learning are then rated against one another regarding their probabilistic performance in the latter example ajami et al 2007 combined hydrological rainfall runoff models are rated within the bma framework like in bcma the difference between bma bms and bcma bcms becomes apparent when looking at the change in model weightings under growing data size this is illustrated for a simple two model setup in fig 4 like the biotic vs abiotic decay example from section 2 3 without any data indifferent uniform prior model weights are assigned to the two individual models m1 microbial and m2 abiotic in bma bms i e 50 each and for all a priori specified combinations of the two models in bcma bcms i e exemplary 5 combinations with 20 each referring to the conceptual example above each combined model consists of both the biotic and abiotic reaction terms but by different fractions which resembles that the concentration decrease is caused by both e g 25 microbial and 75 pure chemical decay once the individual or combined models face a small amount of data the model set member closest to the data gains strongest in weight and others gain less or lose model weight these weights represent the uncertainty in bma or bcma of an individual or combined model respectively to represent the truth given the current data under more and more additional informative data the weighting converges fully to the one most plausible member in the set bma turns into bms for an individual model and bcma turns into bcms for a combined model in a situation as visualized in fig 4 where the truth lays somewhere between m1 and m2 bma bms will tend towards the one single model in the set that appears to be most likely to have generated the data either the biotic or abiotic model but not a mixture identifying a truth consisting of combined models will only be pursued by bcma bcms where the combinations are a priori defined by the modeler and offered as candidates 2 4 3 approximative encompassing modeling by model combination above we discussed model combination as averaging on forecast level to obtain a more focused predictive pdf bcma see section 2 4 1 in contrast to that we discussed pdf averaging to obtain an envelope for several models that accounts for uncertainty before selecting only one of them bma see section 2 3 in addition there are bayesian approaches with the goal to construct an encompassing predictive pdf to account for predictive uncertainty piironen and vehtari 2017 but without seeking to identify a true model or more generally without converging to model selection gelman et al 2014 one way of building a fully encompassing predictive pdf from individual models is so called stacking which traditionally is a point forecast combination scheme recently it was refined to bayesian stacking of predictive pdfs yao et al 2017 in bayesian stacking a linear combination of individual predictive pdfs is obtained by optimizing the model weights such that the combination of models achieves highest predictive density while non simplex weights are plausible when applied to point forecasts like the mean prediction simplex weights are a natural constraint for pdf averaging yao et al 2017 this idea of combining individual models is juxtaposed to directly construct a fully encompassing model that contains all candidates as special cases gelman et al 2014 figuratively this general model is supposed to span an entire area of model space throughout calibration the specific model formulation to approximate the truth is supposed to reveal itself being informed by prior knowledge and data see e g fienen et al 2009 and smoothly transitioning between the special cases a prominent example from geostatistics is the matérn covariance function that has an additional shape parameter and the special cases of gaussian exponential or power law covariance are retrieved through specific values of this parameter this would be an example for so called nested models where one model is a generalization of others but the approach is likewise feasible for non nested models gelman et al 2014 however in fields like hydrology the broad range of models data driven lumped physics based etc complicates a fully encompassing formulation that would allow for meaningful interpretation in principle arbitrary models of any type could be interlinked within an encompassing model yet it remains questionable how insightful a continuous transition between e g data driven and physics based models in such a field can be and how the information contained in data can be taken up by and distributed between the appropriate parameters during calibration modelers sometimes have distinct ideas about which parts of the system have to be represented in a certain model type and which ones should not be and want these ideas to be tested in order to identify a quasi true model therefore while being a pragmatic approach to obtain predictive models fully encompassing models remain questionable for process identification a distinct situation where nested models can yet help to at least pursue process identification at the relevant scale is upscaling when assuming that small scale physics are well understood but the application at hand requires simpler coarse models due to e g computing time or data availability problems a cascade of increasingly upscaled or lumped models can be proposed in such cases the finest scale model is allegedly the all encompassing model but it is infeasible to apply then model selection like bms can be helpful to identify the most useful model scale which boils down to a selection of appropriate model complexity schöniger et al 2015a 3 practical issues and recent advances space it says is big really big you just won t believe how vastly hugely mindbogglingly big it is i mean you may think it s a long way down the road to the chemist s but that s just peanuts to space adams 1979 3 1 definition of the model space bms and bma are strictly valid only in a distinct hypothetical set up only if the considered set of models were a representative subset of the presumably infinite model space prior model weights would reflect a proper prior distribution over model space and only then the bayesian framework could be used to honestly assess and propagate structural uncertainty draper 1995 since this is usually not the case in hydrological applications we need to remind ourselves that bma can only be understood as an intermediate step towards model selection bma cannot quantify conceptual uncertainty per se because it cannot quantify the uncertainty about all the other conceptual model variants that exist outside of the considered set this would require a fundamentally different modeling approach as envisioned by nearing and gupta 2018 instead bma quantifies the recognized uncertainty about choosing the most plausible model in the set this uncertainty may narrow down to a single model too quickly cf fig 3 with growing data set size this issue becomes even more pressing when failing to account for model structural errors as will be discussed in section 3 2 for these reasons bma tends to fall short of a modeler s expectations with regard to predictive coverage domingos 2000 schöniger et al 2015b when it is employed under the realistic condition that the considered model set does not sample the hypothetical model space sufficiently well the representation of model space by the model set is indicated by the so called m closed m complete and m open settings bernardo and smith 1994 in line with the assumption of a completely sampled model space and as discussed in section 2 bms and bma work under the assumption that a true model exists and is part of the considered model set i e that the true model is identifiable this assumption is also referred to as the m closed view bernardo and smith 1994 vehtari and ojanen 2012 for practical purposes this view may be relaxed to the question of identifying a quasi true model among a set of alternatives while acknowledging that posterior model probabilities are only meaningful in the context of an available quasi true model as modelers we might sometimes be in a scenario where we are convinced that a true model exists and conceptually we fully understand it however due to limited resources time knowledge etc yao et al 2017 we are not able to add it to our model set exactly as we conceptualized it and therefore no quasi true model is available in this so called m complete view bernardo and smith 1994 le and clarke 2017 we also cannot identify this model but pursue optimal approximation via cv type methods alternatively bmc allows to approach the truth with combined models that enrich the part of model space that is spanned by the original model candidates in our set as cornerstones as opposed to bms bma which in this view only evaluates and ranks the cornerstones kim and ghahramani 2012 according to the m open view vehtari and ojanen 2012 in contrast the truth cannot be covered by the considered model space it is even questionable if a fixed and finite truth exists in this case methods for model selection and averaging of cv type still promise to maximize predictive coverage piironen and vehtari 2017 to decide whether extending the considered set of models with new competitors is more promising than a selection or combination of existing models the distance between the competing models and the truth as represented by the data needs to be assessed this can be evaluated with a statistical distance metric such as kullback leibler divergence approaches in this direction were proposed by e g abramowitz and gupta 2008 and schöniger et al 2015b further research along these lines is needed to provide a better characterization of the model set and model space for specific applications and therewith to support the choice of appropriate multi model methods 3 2 definition of the likelihood function bme is the core quantity for updating model weights in bms bma for individual models or in bcms bcma for combined models as marginalized likelihood bme is always conditional on the assumptions that define the likelihood function i e the mathematical description of the residuals between predictions and observations hence a proper definition of likelihood is crucial note that despite the default white noise or i i d errors assumption residuals typically show to be correlated heteroscedastic and non gaussian in hydrological applications schoups and vrugt 2010 although often treated in a lumped manner within the likelihood formulation random measurement errors should be treated separately from systematic errors that originate from structural deficits of the model these structural errors should be explicitly modeled for example by statistically describing biases in model forecasts del giudice et al 2013 then the likelihood function and ultimately bme reflect a bias corrected model s mismatch on an objective scale related to measurement error if model structural errors are not properly accounted for bayesian model weights may seem overly decisive lu et al 2013 schöniger et al 2014 because the flawed models are tested against the data in such a strict manner that ever so slight differences in performance will be decisive for model ranking in such common cases the interpretation of model weights as a degree of model plausibility becomes questionable in fact it seems hard to defend a multi model approach that does not account for structural errors in the individual models since the very motivation of multi modeling rests upon the fear that not all if any of the models are indeed entirely accurate hence adding an explicit error model e g a statistical model on top of the actual conceptual or physics based model is a form of model space enrichment see section 3 1 that decreases the extent to which the implicit assumption of bms bma true model is contained in the set is violated this implies two steps of modelling first constructing a process model and second adding an explicit error model to describe the residuals after letting the process model face data the likelihood function in the bms bma framework may then contain this error model opposed to that a recent approach promotes a modeling philosophy in which everything that causes residuals between model predictions and data including the measurement process is explicitly modelled altogether in one step for example a specified statistical error model is directly attached to a physics based model such that this so called mixed model as a whole is evaluated with regard to its plausibility in the light of data while the debatable definition of an appropriate likelihood function for measurement error is entirely avoided nearing et al 2016 3 3 evaluation of bme there is plenty of literature on the implementation of bma bms for fields where bme can be estimated comparably easily e g linear regression gaussian linear models and graphical models e g madigan and raftery 1994 hoeting et al 1999 wasserman 2000 burnham and anderson 2003 however in the field of hydro geo logy we typically deal with complex non linear models with non gaussian parameter distributions analytical solutions e g based on conjugate distributions are hence rarely applicable friel and wyse 2012 which necessitates mathematical approximations like the bic or kic schwarz 1978 kashyap 1982 or numerical bme integration raftery 1995 3 3 1 sampling based bme evaluation numerical sampling based integration methods employ e g nested sampling skilling 2004 gibbs sampling chib 1995 or various versions originating from importance sampling e g gelman and meng 1998 the idea behind importance sampling is to cover the parameter space regions of interest via so called importance distributions setting these importance distributions to either the prior or posterior distribution i e the endpoints of bayesian statistics yields the arithmetic mean estimator e g schöniger et al 2014 or harmonic mean estimator newton and raftery 1994 respectively conceptually other importance sampling based approaches are situated somewhere in between these two this includes recently developed or improved techniques like thermodynamic integration liu et al 2016 or estimators based on gaussian mixtures as importance distributions volpi et al 2017 benchmarking studies that compare various methods for estimating bme are for example friel and wyse 2012 schöniger et al 2014 liu et al 2016 brunetti et al 2017 and zeng et al 2018 alternative approaches employ surrogate models zeng et al 2018 mohamadi et al 2018 and references therein where bme is evaluated not for the original often expensive models but for cheap proxies further methods have been proposed which pursue the idea of bms bma but are able to avoid the evaluation of bme in specific model settings e g via trans dimensional reversible jump mcmc green 1995 3 3 2 approximation based bme evaluation as pointed out in the context of bms section 2 2 the computational effort of evaluating bme and hence bayesian model weights can numerically become prohibitive which necessitates the use of approximations like the kic or bic note that neither the kic nor the bic actually has an information theoretic background and as such could be considered misnomers burnham and anderson 2004 the kic is based on the laplace approximation and requires the posterior parameter distribution of the candidate model to be gaussian the posterior is then either evaluated at the maximum a posteriori estimate map or at the maximum likelihood estimate mle with kic map being a more reliable approximator than kic mle schöniger et al 2014 the bic can be derived by further simplification of the kic in the limit of infinite data set size bic behaves strongly consistently claeskens and hjort 2008 and will always try to identify a true model or quasi true model as sufficient representation of the truth burnham and anderson 2004 even if there is none in the set due to their ease of use these bme approximators are likewise applied for computationally efficient estimation of bme based posterior model weights in bma an example for this is maximum likelihood bma mlbma neuman 2003 ye et al 2004 neuman et al 2012 that is applicable in fields where computationally demanding models are employed like hydro geo logy this variant of bma is named for its use of the kic mle to approximate bme to increase accuracy schöniger et al 2014 recommend to perform kic map bma instead if full bma is unfeasible note that in approximation based bma the resulting weights represent at best a rough approximation of bayesian probability in the case of kic at worst they may lead to a completely opposite model ranking than full bma in the case of bic schöniger et al 2014 3 4 robustness of model ranking results the robustness of bayesian model weights depends on the uncertainty that exists in input driving and output calibration data schöniger et al 2015b have proposed a parametric bootstrap approach to visualize how these sources of uncertainty propagate to uncertainty in bayesian model weights analyzing the lack of robustness in model weights has been shown to shield us from potentially wrong decisions based on uncertain model ranking results such analyses further allow to prioritize data collection efforts to reduce predictive uncertainty in a most efficient manner by identifying the relative importance of parameter uncertainty within model uncertainty vs model choice uncertainty between model uncertainty 3 5 physical interpretation of predictions model combination in hydrology might be criticized for its lack of strict mass balance conservation cf section 2 4 1 this argument would prefer pdf averaging over actual model combination however giving conservation of potentially wrongly specified mass balances priority over increased predictive performance is certainly debatable in some contexts if the goal of a modeling study is to identify the importance of a specific physical process of course presumed to be generally valid physical laws should be respected by the model and should not be weakened unnecessarily however if the goal is to predict as accurately and reliably as possible it might prove more successful to allow for some more degrees of freedom accounting for errors that do not result from the physical laws but from wrongly specified boundary conditions oversimplification missing processes or other shortcomings it seems that no hydrological model is shielded against these types of errors and it remains an open question how to deal with them model combination approaches and statistical error treatment approaches cf section 3 2 might provide additional insight which in the end will help formulating better models that can safely rely on physical laws 3 6 available software readily available software packages for bayesian modeling and model evaluation exist pymc3 salvatier et al 2016 stan carpenter et al 2017 winbugs lunn et al 2000 jags plummer et al 2003 etc however most of them provide bayesian cv based rather than bme based model evaluation and ranking the ones that allow for bme evaluation are often sampling algorithms that provide bme as a side product emcee foreman mackey et al 2013 dream vrugt et al 2009 mrbayes huelsenbeck and ronquist 2001 etc a detailed evaluation and discussion of these software packages is beyond the scope of this study and we also do not want to give recommendations in choosing one over the other instead we wish to emphasize that understanding the limiting assumptions and scrutinizing their validity for the application case at hand should be of primary importance to the modeler available tools always come with the risk of being used as black box and this can lead to false conclusions in their own right such tools might still be helpful but we want to encourage the users to check whether their modeling task at hand can be treated by such software regarding the philosophical and methodical background discussed 4 guideline to identify the best suited multi model approach a towel it says is about the most massively useful thing an interstellar hitchhiker can have partly it has great practical value adams 1979 to not throw in the towel we propose a scheme for finding the most suitable multi modeling approach for any modeling task at hand starting from the philosophical perspective enables us to find a clear way through the model selection and averaging tree as depicted in fig 5 bms is our way to go if we are interested in process identification and the process is represented as identifiable target model in our set of models depending on the data availability this path will pass through bma which enables us to handle the uncertainty between all plausible hypotheses probabilistically until one reveals itself as best representation of the truth however if we think that this target model is not a single model but rather a combined model which is situated somewhere between these candidates in the set we can propose and select a corresponding combined model using bcma and ultimately bcms in fig 5 we see that all bms bma bcms and bcma are aiming at model identification by applying bcma bcms we avoid the uneasy assumption of having a quasi true model in the set immediately but still we aim to identify a best combined model that represents the truth if we have to deny the initial question about whether there is an identifiable true model or at least a quasi true model we can only seek to approximate this truth to obtain plausible predictions then our options are either to enrich model space via classic combination approaches ewa bga gra or to select a single model that promises the best while parsimonious approximation actual model combination approaches as in the former option estimate model weights such that the truth is approximated by a composite model ensemble again this is also different from bcma during which the combined models are a priori defined by the modeler and not meant to only approximate the truth but to allow for its identification the latter option to select a model of justified complexity remains as the way to approximate the truth when only one best model is desired this is achieved by cv and actual information theoretic ics thereby if no model can clearly outperform the others during the selection preliminary cv based weights can be assigned to the models to handle the uncertainty in selecting one however note that this is no actual model combination and therefore should also not be applied as such 5 summary and outlook don t panic adams 1979 numerous methods for model selection or averaging exist that are to some degree bayesian but modelers have to be very cautious whether this pertains to actual bms bma and from which philosophical point of view the respective method is valid bayesian model combination bcms bcma is often confused with bma but serves a different purpose our primary motivation for this review is therefore to create awareness when starting any modeling endeavor we urge to carefully consider the modeling objectives the statistical approach for treating conceptual uncertainty including implementation choices and the conclusions allowed to be drawn from such analyses we have discussed that 1 the philosophical perspectives of statistical methods to address conceptual uncertainty can be distinguished in process identification and truth approximation 2 this distinction relates to the question of whether we believe that the truth is contained in the considered model set or not 3 bms and bma are tailored to process identification because they assume that the quasi true model is contained in the set of candidates 4 for the above reason bma converges to bms with increasing evidence in the data 5 and hence while bma has often been considered a method to address conceptual model uncertainty it rather expresses the degree of uncertainty in fully converging towards the most recognized true model candidate 6 to increase predictive skill model combination on the level of forecasts may be more successful in many cases 7 bma bms can be applied to an enriched model space of combined models resulting in bcma bcms formally bma bms and bcma bcms are conducted the same way see appendix a however they differ in the type of models we define to be in the model set the discussion revealed various potential avenues for further research such as a systematic coverage of model space how can we assure that the considered set of models is an adequate representation of model space model combination probabilities are always conditional on that set which necessitates approaches to measure the currently covered model space and to assign consistent prior model probabilities the suitability of bma bms to compare models of vastly different complexity which range of model complexity may or should be present in the model set since bms bma implicitly follows the principle of parsimony any model in the set needs to be justifiable in the light of the data to be identifiable as the quasi true model schöniger et al 2015a a solid treatment of measurement and model structural errors in multi model approaches how can we adequately describe our assumptions about model specific structural errors and model independent measurement errors the appropriateness of commonly used likelihood functions is questionable and more realistic descriptions e g in the form of statistical error models are needed facilitating access of practitioners to the theoretical methods discussed here how can we promote the understanding and use of the discussed methods in practice available software mostly covers cv type model rating and attempts for generally applicable software are usually not designed for physics based models from hydrology yet the practical difficulty of identifying relevant differences in models how can we assure that the data set is informative for model identification via bma bms optimal design of experiments can help maximize the visibility of model structural differences nowak and guthke 2016 a guideline for model development can the knowledge gained in the evaluation of yet non true models be utilized for creating enhanced model candidates a structured scheme of how to isolate the strong parts of a model and transfer it to another model with complementary strong features would boost model development and system understanding overall we hope to further strengthen the utility of bayesian methods in the face of conceptual uncertainty with this review by directing their use into the right channels conflict of interest the authors declared that there is no conflict of interest acknowledgments the authors would like to thank the german research foundation dfg for financial support of the project within the research training group integrated hydrosystem modelling grk 1829 at the university of tübingen within the cluster of excellence in simulation technology exc 310 2 at the university of stuttgart and within the collaborative research centre campos sfb 1253 between both universities and their partner institutions further we thank jonghyun harry lee ahmed elshall and four anonymous reviewers for their constructive comments and suggestions that have helped to improve the manuscript appendix a equations in bayesian model averaging bma a model weight expresses the probability of a model to be the true one among n m distinct models each model m 1 2 n m is assigned a prior model probability p m m which is updated to the posterior model probability p m m d given data d via bayes theorem for models a 1 p m m d p d m m p m m m 1 n m p d m m p m m with bayesian model evidence p d m m a 2 p d m m p d m m θ m p θ m m m d θ m as likelihood p d m m θ m marginalized over the whole prior parameter distribution p θ m m m of parameters θ m of m m formally these equations are used likewise in bcma but each m m is then substituted by a combined model cm k with k 1 2 n cm 
6575,model selection and model averaging have become popular tools to address conceptual uncertainty in hydro geo logical modeling within the last two decades many different flavors of approaches and implementations have emerged which complicate an easy access to and a thorough understanding of the underlying principles with the many approaches and applications a variety of terms has been defined which easily leads to misunderstandings and confusion among the community here we review bayesian model selection bms and averaging bma as a rigorous statistical framework for model choice under uncertainty we aim to clarify the theoretical foundations of both methods their relationship to one another and to alternative approaches and implications of implementation choices we further build a bridge to bayesian model combination bmc which turns out to be bayesian averaging or selection of combined models bcma bcms concluding from our theoretical review we argue that the goal and the philosophical perspective of modeling should be main drivers when choosing to use the b c ms a toolbox and we offer guidance to identify the suitable approach for specific modeling goals with this review we hope to further strengthen the utility of bayesian methods in the face of conceptual uncertainty by directing their use into the right channels keywords multi modeling conceptual uncertainty model selection model averaging model combination bayes theorem information criteria hydrological modeling groundwater modeling 1 introduction all you really need to know for the moment is that the universe is a lot more complicated than you might think even if you start from a position of thinking it s pretty damn complicated in the first place adams 1979 in hydrosystem modeling we are confronted with several sources of errors and uncertainty input and output uncertainty due to noisy measurements and scale disparities parameter uncertainty due to sparse observations and heterogeneities and model structural uncertainty conceptual uncertainty due to simplification of the underlying physical processes e g renard et al 2010 clark et al 2011 refsgaard et al 2012 elshall and tsai 2014 the stochastification of hydrological models aims to translate these uncertainties into realistic uncertainty estimates for model predictions liu and gupta 2007 montanari and koutsoyiannis 2012 nearing et al 2016 recently multi model approaches to explicitly address conceptual uncertainty have gained renewed interest gupta et al 2012 e g based on bayesian probability theory draper 1995 or information theory gong et al 2013 multi modeling means to employ an ensemble of several distinct model candidates to simulate a certain phenomenon or occurrence e g burnham and anderson 2003 these models may differ in their conceptuality e g being physics based or data driven or in their level of detail e g scale or resolution but they all yield the same desired quantity as model output fields of application in hydro geo logy include among others groundwater modeling rojas et al 2010 reactive transport modeling lu et al 2013 soil plant atmosphere modeling wöhling et al 2015 and hydrological modeling marshall et al 2005 quantifying or at least approximating conceptual uncertainty not only allows to equip model results with uncertainty bounds but also to determine the information content in existing data with respect to model choice wöhling et al 2015 and to apply formal optimization procedures toward reduced uncertainty such as optimal design of monitoring kikuchi et al 2015 nowak and guthke 2016 pham and tsai 2016 although there seems to be consensus that following the method of multiple working hypotheses chamberlin 1890 may shield us from overconfident and potentially biased single model predictions clark et al 2011 there is less awareness that modeling results may vary dramatically with the chosen approach to multi modeling a considerate choice of methods is crucial but challenging within the last two decades many different flavors of multi model approaches and implementations have emerged which complicate an easy access to and a thorough understanding of the underlying principles with the many approaches and applications a variety of terms has been defined which easily leads to misunderstandings and confusion among the community and promotes ad hoc modeling decisions to counteract this development we review rigorous bayesian approaches to model selection and averaging which comprise the well known frameworks of bayesian model selection bms and bayesian model averaging bma but also extend to cross validation and model combination techniques more details and references are provided in section 2 we aim to clarify their theoretical foundations including their philosophical view on modeling their relationship to alternative non bayesian methods and implications of implementation choices we further contrast bms bma with bayesian model combination bmc which turns out to be bayesian averaging or selection of combined models to be defined below we argue that the goal and the philosophical perspective of modeling should be main drivers when choosing to use either such tool and we offer guidance to identify the suitable approach for a specific modeling goal this review intends to address modelers in science and practice with a basic background in statistics who strive a more fundamental understanding of model selection averaging and combination concepts in contrast to previous well received surveys and tutorials e g draper 1995 hoeting et al 1999 wasserman 2000 chipman et al 2001 monteith et al 2011 vehtari and ojanen 2012 we refrain from using equations or numerical examples for demonstration referring to gelfand and dey 1994 instead but aim to provide a compact version of the context necessary to understand and choose the most suitable model selection or averaging approach for a specific type of application since in hydrology we are typically confronted with difficult conceptual modeling decisions and limited data sets we focus on methods and theories that are relevant for such situations and avoid discussing unrealistic limiting cases e g infinite sample asymptotics as much as possible still our conclusions and recommendations are derived from a theoretical analysis that ensures validity over a wide range of situations and modeling disciplines 2 key concepts of bayesian multi modeling there is a theory which states that if ever anyone discovers exactly what the universe is for and why it is here it will instantly disappear and be replaced by something even more bizarre and inexplicable there is another theory which states that this has already happened adams 1979 2 1 classification of approaches working with a set of competing models hypotheses we can follow two intuitively distinct modeling approaches the winner take all perspective of model selection or the team of rivals perspective of model averaging fig 1 in the case of model selection the goal is typically to identify relevant hydrological processes among a set of alternative hypotheses wagener and gupta 2005 in the spirit of hypothesis testing or more pragmatically to reduce the considered model space in order to keep the computational effort for prediction low in this context it is crucial to focus on the primary purposes of modelling i e to generalize to future data or process identification both of which we will put into perspective in section 2 2 the team of rivals perspective ferré 2017 advertises model averaging in its most general form deliberately keep more than one model for inference and prediction for fear of missing something when constraining the model set to a single hypothesis model averaging allows to combine the best properties of different models with the hope to obtain a combined estimate that is more skillful and reliable than any of the individual models while the terms model averaging and model combination are sometimes used interchangeably as in the previous sentence we will provide a more careful definition and distinction below see section 2 3 ff clarifying which notions of model averaging exist regarding model weighting and uncertainty quantification a comprehensive overview of both model selection and averaging from a statistics point of view is offered by claeskens 2016 besides the intuitive classification of multi model approaches into selection and averaging the philosophical roots of the different approaches in each class call for their own classification each method has been derived in a specific mindset using specific assumptions and therefore only yields readily interpretable results in their respective realm probably the most fundamental issue regarding classification from the philosophical perspective of modeling is whether or not a true model exists either we do believe that the true underlying model i e the full mathematical specification of the modeled system can in principle be written down and be identified or we accept that the truth can only ever be approximated to some degree many authors have pointed this out before e g burnham and anderson 2003 claeskens 2016 but its importance for choosing the most appropriate statistical method to address model choice uncertainty cannot be overstated modeling of physical systems as done in hydrology normally does not allow for assuming that the true model is actually in the set of available models let alone that it is possible to formulate it ever the idea of constructing a model that fully represents the true system with all its details is illusory however depending on the relevance of system components on the modeled scale we might be able to at least set up what we call a quasi true model a quasi true model might e g not resolve the exact physics of molecular reactions behind declining concentrations but on the considered scale the assumed order of reactive decay might adequately reflect the system s lumped chemistry e g in a river segment however quasi true models shall not be confused with models that merely mimic the data in order to approximate the truth an example of a mimicking model would be using a power law instead of exponential decay the former approximates the latter while the latter is the actual solution to the quasi true process model described by an ordinary differential equation of first order for a certain range of data a power law with a constant exponent might yield a very close approximation to the quasi true exponential model and is therefore also generalizable to a certain degree beyond that range nonetheless a mimicking model is neither true nor quasi true because it does not reflect the underlying causal relation hence considering whether we operate potentially quasi true or approximatively mimicking models to achieve our modeling goal also pertains to using the adequate multi model approach after reviewing bayesian multi model concepts from a theoretical point of view in the remainder of section 2 and from a practical point of view in section 3 we will argue in section 4 that by including the philosophical modeling perspective it is possible to make a more informed and less ad hoc choice between bms bma bmc vs alternative methods that suits the individual modeling purpose 2 2 bayesian model selection 2 2 1 identification of a quasi true model via bms if we assume that there is a true model to be identified then we should adopt a statistical model selection method that fulfills exactly this job description bms raftery 1995 is one such method that promises to point the modeler to the true hypothesis if the true model is in the available set of models at least in the limit of infinite data set size this is referred to as consistent model selection shibata 1986 hurvich and tsai 1989 for a finite amount of data identification of the true model may not be possible because the candidate models may not be sufficiently well distinguishable through the blurred eyes of the data when in doubt i e in cases of similar model performance bms will prefer simpler models due to the built in principle of parsimony a k a occam s razor gull 1988 with growing data set size bms will identify the true complexity with increasing confidence as opposed to other model selection techniques see below this behavior of bms has been demonstrated on a case of hydrogeological model selection by schöniger et al 2015b as a useful side effect bms is able to detect model dis similarity if two models have been set up in presumably different ways but are general enough to look the same through the eyes of the data at hand bms will yield an undecisive ranking in bms probabilities are assigned to all model candidates that express the degree of plausibility of being the quasi true model these probabilities are referred to as model weights and are updated using bayesian model evidence bme the higher a model s bme compared to the other models the higher its updated probability to be the true model bme can be understood as a global measure of fit integrated over the whole parameter space of the model and is therefore also referred to as marginal likelihood the bme values can then be transferred to so called posterior odds i e model weight ratios or bayes factors kass and raftery 1995 which indicate how much more plausible a candidate model is compared to an alternative one the computational effort of evaluating bayesian model weights can become prohibitive in practical applications e g for non linear models large parameter spaces high model dimensionality or long model computing times this is why computationally efficient approximations via the kashyap information criterion kic kashyap 1982 or the bayesian information criterion bic schwarz 1978 are frequently used to estimate the logarithmic bme cf section 3 3 we will call both full bms and its approximations bms type identification methods in allusion to their philosophical quest to identify the true model 2 2 2 approximation of truth via bayesian cross validation if we do not believe in the notion of a true model our goal is to approximate the truth as closely as possible to make realistic predictions this assumption is the crucial distinction from bms in section 2 2 1 with additional observed data our picture of truth may even change and so should our selected model in order to catch up with our evolving level of knowledge this is what cross validation cv techniques and approximations thereof provide briefly some of the available data is held out as validation data while the remaining data is used for model calibration then the performance of each calibrated model in accurately and precisely predicting the validation data is evaluated and the models are rated accordingly this method directly evaluates the predictive capability of a model i e its ability to generalize to new data the key question is how closely should my model approximate the data i e mimic the truth to avoid underfitting of process details and overfitting of noise the purpose of model selection is then to find the model that strikes an optimal balance between lack of accuracy bias and lack of precision variance forster 2000 hastie et al 2009 this balance is meant when referring to generalizability of a model hastie et al 2009 in this spirit bayesian cv for model rating works probabilistically and targets at finding the model with highest predictive density for new data although bayesian cv is perfectly suited to select the model that provides the best balance it comes with large computational effort for repeated calibration and validation and suffers from instabilities when confronted with limited data set sizes e g gelman et al 2014 the akaike information criterion aic akaike 1973 offers a computationally undemanding alternative derived in the context of information theory the aic aims to minimize the expected information loss measured as kullback leibler divergence when approximating truth with candidate models and is shown to be asymptotically equivalent to cv stone 1977 limitations by restrictive assumptions of the aic can be overcome by using e g the corrected aic aicc hurvich and tsai 1989 the deviance ic dic spiegelhalter et al 2002 or the widely applicable ic waic watanabe 2010 we consider these methods to be bayesian model selection techniques because they act on posterior probability density functions pdfs of model parameters and or predictions even though they differ in their degree of accounting for prior knowledge yet they are not to be confused with bms because their model selection result does not allow for an interpretation of probability of being the true model we therefore call them cv type approximation methods in contrast to bms type identification methods keeping the typical information theoretic notion of no true model exists in mind ye et al 2008 it is clear that the aic or other cv type methods can never yield a rating on how good the approximation is on an absolute scale but only relative to other models burnham and anderson 2003 i e which model is closer to the unknown truth compared to others these methods guarantee to select the model that promises best predictions given the current data despite having an unidentifiable true model this property of still finding a best approximative model based on only limited data is called asymptotic efficiency shibata 1980 and is often contrasted to bms type consistency e g yang 2005 claeskens and hjort 2008 vrieze 2012 consistency and efficiency represent two kinds of optimality aho et al 2014 that cannot be combined in a single model selection method claeskens and hjort 2008 burnham and anderson 2004 because they are developed for deviating settings which is formally proven by yang 2005 despite this distinction there are studies where model weights are approximated via kic bic and additionally via actual information criteria ic such as the aic with results often being contradictory and therefore inconclusive e g poeter and anderson 2005 ye et al 2008 tsai and li 2008 singh et al 2010 foglia et al 2013 this does not come as a surprise because some of the used criteria work under deviating assumptions schöniger et al 2014 and pursue different objectives höge et al 2018 2 2 3 the role of model complexity it is quite confusing why the endeavor to identify or to approximate the truth should be completely different routes to take in model selection however keeping the notion of an either existing or non existing true model in mind it is important to understand that these two goals can only be pursued by respective model selection methods aho et al 2014 the difference between the two worlds becomes apparent when looking at their respective takes on the complexity of the truth to be modeled and the implications for over and underfitting höge et al 2018 and references therein for this it is important to apprehend every model selection procedure in theory to be sequentially iterated while including more data if there was more time money workforce etc with each iteration the applied method fulfills its intended identification or approximation purpose successively better as illustrated in fig 2 in practice we usually have a finite amount of data available and therefore obtain a snapshot of this underlying theoretic behavior model complexity in this context does not require a specific and comprehensive definition which by itself poses an unresolved problem see e g gell mann 1995 we can think of it as notion of number of parameters and functional terms their non linearity and interrelation degree of detail etc the target of bms type selection is the true model of fixed and finite complexity as to see in fig 2 all models that are more complex than the truth overfit and the ones that are not complex enough underfit in principle independently of data size however for small data set sizes bms type methods tend to prefer simpler models over a more complex true model in a conservative manner hurvich and tsai 1989 burnham and anderson 2003 it is therefore advised to assess the identifiability of a certain level of true model complexity through the eyes of the current data set schöniger et al 2015a guthke 2017 if none of the candidate models can be identified as the true model yet due to a too small sample size more informative data need to be collected first before ranking the model set with bms type methods if the truth was infinitely complex a true model could not even be hypothesized hence the target model of cv type selection is not a fixed one but depends on the data set size i e the model that catches up best with the data generating process given the currently available data under the implicit assumption that none of the models in the set is true cv type methods always try to find the model that is the closest approximation of the truth but also not too close that noise is fitted rather than the unknown truth especially complex models with many degrees of freedom tend to such overfitting because the information in the currently available data is insufficient to fully invert the model i e to constrain its degrees of freedom overfitting models cannot justify their complexity for approximating the unknown true model on the current data while underfitting models lag behind in their complexity for the current data cv methods prevent both by enforcing appropriate complexity in the validation step however cv type methods can also switch to more complex models that did overfit before when the data size grows with more informative data the risk of fitting noise declines while more and more of the real infinite complexity becomes visible see fig 2 then a more complex model might become a better approximation of the truth more information supports higher complexity this potential behavior however sometimes triggers cv type methods to overrate too complex models in their performance while trying to approach the truth more closely while guarding against overfitting at a certain data level cv type methods always question whether a more complex model would be more appropriate as soon as more data is included i e are prone to the risk of overfitting hurvich and tsai 1989 ng 1997 burnham and anderson 2003 the intention to mitigate this risk has lead to e g the development of the so called corrected aic or aicc sugiura 1978 that takes the data set size into account 2 3 bayesian model averaging 2 3 1 quantifying indecisiveness of bms via bma if evidence in the data seems insufficiently clear to uniquely identify the most true model via bms we can decide to keep potentially true models in the set and weight them according to their probability to be the true model this is the principle behind bma draper 1995 in its original form bma uses the exact same posterior model weights as determined in bms section 2 2 1 to combine the individual models predictions into a weighted average the fundamental set of equations can be found in e g hoeting et al 1999 and for completeness also in appendix a for applications of the original bma approach we will refer to it as full bma in hydro geo logy see e g schöniger et al 2014 schöniger et al 2015a wöhling et al 2015 schöniger et al 2015b liu et al 2016 zeng et al 2018 it is important to realize that both bms and bma assume that a true model exists is available and can be identified if there was just enough data as such the bma result can be considered to be a preliminary result on the way towards bms minka 2002 therefore despite appearing as a forecast driven team of rivals approach at first sight fig 1 bma is actually an intermediate snapshot on a winner take all race as long as the amount of data is insufficient for a clear identification of the true model the bma strategy is to keep all potentially true models in the set and weight them according to their probability to be the true model with more information coming in the weight of the most true candidate model will converge to 1 and all others will diminish this is illustrated for a two model scenario in fig 3 with model m1 being the quasi true model and the bma predictive pdf converging to the predictive pdf of model m1 for increasing amount of data the uncertainty about choosing the most plausible model in the set is reflected by the width of the bma predictive pdf and can be measured as e g bma variance hoeting et al 1999 or entropy zeng et al 2016 the progress of bma to bms while converging to the allegedly true model as displayed in fig 3 can be illustrated by a simple conceptual exemplification for an observed decline in concentration of a substance two experts might provide two plausible hypotheses the first expert hypothesizes the concentration decrease results from only microbial consumption m1 and the second expert claims that solely abiotic reactions cause the decline m2 each expert comes up with a model that contains the mathematic formulation of their respective process if bma was applied to both models it would prefer one over the other and would do so increasingly clearly with more included data from the decline bma assumes only one of the two models can be true and tries to identify it bma would not settle with the weights of the two processes like 75 biotic and 25 abiotic under growing data size even if this ratio represented what actually happened in reality 2 3 2 approximative methods confusingly labeled as bma besides its original meaning the term bma has been misleadingly used for a different form of pdf averaging as proposed by raftery et al 2005 this approach rests on the idea of mixing gaussian predictive pdfs and determining the mixing coefficients with the help of the expectation maximization em algorithm mclachlan and krishnan 2007 we therefore propose to refer to it as gaussian mixture model averaging gamma the crucial difference is that bma rests on the assumption that exactly one model has generated the data whereas gamma assumes that the data can be best described by a mixture pdf and aims to identify the true proportions of the candidate models pdfs also in analogy to bma where model weights might come from bms methods like kic or bic cf 2 2 1 model averaging is sometimes performed based on weights from cv type criteria like aic dic or waic this is occasionally called pseudo bma geisser and eddy 1979 yao et al 2017 however this is not equivalent to bayesian model averaging as denoted bma here and should not be confused model averaging based on aic and alike does not have a rigorous statistical foundation wasserman 2000 and hence there is no clear interpretation of results in a bma sense we strongly advise to use the term bma only for the original bayesian approach to model averaging as presented in hoeting et al 1999 or draper 1995 and to indicate any deviating approach with a clearly distinguishable term this should help reduce the confusion about the applicability and interpretation of bma in the hydrological community 2 4 bayesian model combination 2 4 1 pdf averaging vs forecast combination note that the bayesian multi model approaches discussed above act on the level of predictive pdfs recall fig 3 and not on individual realizations or on the best estimate forecast itself hence averaged quantities such as the expected value only have statistical meaning not a physical meaning the fact that the bma mean may be physically meaningless is a frequent criticism of bma which however results from a misunderstanding of what bma is by combination of individual models pdfs we do not obtain the pdf of an improved model but we obtain a multi model pdf that envelopes all forecast scenarios that seem plausible given the current state of knowledge understanding bma as an intermediate result on the way to bms avoids the misinterpretation of the bma mean indeed when choosing to apply bma we should naturally be interested in the full bma pdf instead most other model averaging techniques act on the level of forecasts and are therefore referred to as model combination approaches fig 1 this means that individual forecasts e g model realizations or best estimate forecasts are being combined between different models to obtain a new type of forecast that is beyond of what the individual models could achieve model combination aims at finding a composite model that is superior to the individual proposed models because individual structural deficits are ideally compensated within a good combination this is fundamentally different from bms or bma not only in the resulting mean but also in predictive uncertainty pdf averaging aims at obtaining an ideally fully encompassing typically wide and multi modal predictive pdf whereas the pdf of a combined model will be much more focused i e with a lower variance than the lowest individual variance under certain conditions see bates and granger 1969 further the pdf of a combined model will not reflect conceptual uncertainty about choosing one of the models in the set which emphasizes the different philosophies that exist under the hood of model averaging fig 1 a variety of model combination weighting schemes on forecast level has been proposed in order to produce a most accurate combined model for an unknown truth e g equal weights averaging ewa or bates granger model averaging bga with weights based on the forecast variance most averaging rules rely on simplex weights positive weights that sum to one granger and ramanathan 1984 have proposed to move away from this constraint in order to improve predictive performance of the combined estimator via bias correction granger ramanathan averaging gra further so called ensemble methods like bagging or boosting exist kim and ghahramani 2012 and references therein where combination implies mainly setting up model ensembles and applying distinct model training schemes in a way that the ensemble members mutually counteract bias or variance of forecasts a comparison of combination approaches for hydrological applications yet without consideration of their philosophical differences has been performed by e g diks and vrugt 2010 2 4 2 identification of a quasi true combined model via bcms it is important to understand that bma will converge to the individual model that is closest to the truth instead of converging to model weights such that the weighted combination of models is closest minka 2002 monteith et al 2011 we suppose that what many users actually seek when they choose bma is such an optimally weighted combination especially in situations when they do not believe that the quasi true model is in the set but is somehow encircled by the models in the ensemble in this situation a model space enrichment procedure that fills the inner space spanned by the proposed models is bound to be beneficial and an objective selection of the most suitable proposition in the enriched hypothesis space is even more helpful in this spirit we suggest to follow the argument of minka 2002 and apply bma to a new hypotheses space of stacked models we term this procedure bayesian combined model averaging bcma to highlight its relation to both model combination and bma monteith et al 2011 use the more elegant but less precise term bayesian model combination practically bcma means applying the equations for bma see appendix a not to the individual model alternatives but to forecast combinations of these individual models these combined models are predefined by the modeler and bcma weights them according to how well each of them matches the truth just like bma does this for individual models note that in the very same manner as bma is a preliminary step on the way to bms bcma can be understood as a preliminary result on the way to bayesian combined model selection bcms for an application of bcma and bcms refer to monteith et al 2011 and for proposed combination schemes in statistical modelling or hydrology that work in a similar fashion see kim and ghahramani 2012 or ajami et al 2007 respectively in the examples just referenced model weights for combination are not based on marginal likelihoods as in bma but are obtained by other means such as an em approach see section 2 3 2 or drawn from a distribution of weights in the former two examples monteith et al 2011 kim and ghahramani 2012 the combinations of statistical classification models often encountered in machine learning are then rated against one another regarding their probabilistic performance in the latter example ajami et al 2007 combined hydrological rainfall runoff models are rated within the bma framework like in bcma the difference between bma bms and bcma bcms becomes apparent when looking at the change in model weightings under growing data size this is illustrated for a simple two model setup in fig 4 like the biotic vs abiotic decay example from section 2 3 without any data indifferent uniform prior model weights are assigned to the two individual models m1 microbial and m2 abiotic in bma bms i e 50 each and for all a priori specified combinations of the two models in bcma bcms i e exemplary 5 combinations with 20 each referring to the conceptual example above each combined model consists of both the biotic and abiotic reaction terms but by different fractions which resembles that the concentration decrease is caused by both e g 25 microbial and 75 pure chemical decay once the individual or combined models face a small amount of data the model set member closest to the data gains strongest in weight and others gain less or lose model weight these weights represent the uncertainty in bma or bcma of an individual or combined model respectively to represent the truth given the current data under more and more additional informative data the weighting converges fully to the one most plausible member in the set bma turns into bms for an individual model and bcma turns into bcms for a combined model in a situation as visualized in fig 4 where the truth lays somewhere between m1 and m2 bma bms will tend towards the one single model in the set that appears to be most likely to have generated the data either the biotic or abiotic model but not a mixture identifying a truth consisting of combined models will only be pursued by bcma bcms where the combinations are a priori defined by the modeler and offered as candidates 2 4 3 approximative encompassing modeling by model combination above we discussed model combination as averaging on forecast level to obtain a more focused predictive pdf bcma see section 2 4 1 in contrast to that we discussed pdf averaging to obtain an envelope for several models that accounts for uncertainty before selecting only one of them bma see section 2 3 in addition there are bayesian approaches with the goal to construct an encompassing predictive pdf to account for predictive uncertainty piironen and vehtari 2017 but without seeking to identify a true model or more generally without converging to model selection gelman et al 2014 one way of building a fully encompassing predictive pdf from individual models is so called stacking which traditionally is a point forecast combination scheme recently it was refined to bayesian stacking of predictive pdfs yao et al 2017 in bayesian stacking a linear combination of individual predictive pdfs is obtained by optimizing the model weights such that the combination of models achieves highest predictive density while non simplex weights are plausible when applied to point forecasts like the mean prediction simplex weights are a natural constraint for pdf averaging yao et al 2017 this idea of combining individual models is juxtaposed to directly construct a fully encompassing model that contains all candidates as special cases gelman et al 2014 figuratively this general model is supposed to span an entire area of model space throughout calibration the specific model formulation to approximate the truth is supposed to reveal itself being informed by prior knowledge and data see e g fienen et al 2009 and smoothly transitioning between the special cases a prominent example from geostatistics is the matérn covariance function that has an additional shape parameter and the special cases of gaussian exponential or power law covariance are retrieved through specific values of this parameter this would be an example for so called nested models where one model is a generalization of others but the approach is likewise feasible for non nested models gelman et al 2014 however in fields like hydrology the broad range of models data driven lumped physics based etc complicates a fully encompassing formulation that would allow for meaningful interpretation in principle arbitrary models of any type could be interlinked within an encompassing model yet it remains questionable how insightful a continuous transition between e g data driven and physics based models in such a field can be and how the information contained in data can be taken up by and distributed between the appropriate parameters during calibration modelers sometimes have distinct ideas about which parts of the system have to be represented in a certain model type and which ones should not be and want these ideas to be tested in order to identify a quasi true model therefore while being a pragmatic approach to obtain predictive models fully encompassing models remain questionable for process identification a distinct situation where nested models can yet help to at least pursue process identification at the relevant scale is upscaling when assuming that small scale physics are well understood but the application at hand requires simpler coarse models due to e g computing time or data availability problems a cascade of increasingly upscaled or lumped models can be proposed in such cases the finest scale model is allegedly the all encompassing model but it is infeasible to apply then model selection like bms can be helpful to identify the most useful model scale which boils down to a selection of appropriate model complexity schöniger et al 2015a 3 practical issues and recent advances space it says is big really big you just won t believe how vastly hugely mindbogglingly big it is i mean you may think it s a long way down the road to the chemist s but that s just peanuts to space adams 1979 3 1 definition of the model space bms and bma are strictly valid only in a distinct hypothetical set up only if the considered set of models were a representative subset of the presumably infinite model space prior model weights would reflect a proper prior distribution over model space and only then the bayesian framework could be used to honestly assess and propagate structural uncertainty draper 1995 since this is usually not the case in hydrological applications we need to remind ourselves that bma can only be understood as an intermediate step towards model selection bma cannot quantify conceptual uncertainty per se because it cannot quantify the uncertainty about all the other conceptual model variants that exist outside of the considered set this would require a fundamentally different modeling approach as envisioned by nearing and gupta 2018 instead bma quantifies the recognized uncertainty about choosing the most plausible model in the set this uncertainty may narrow down to a single model too quickly cf fig 3 with growing data set size this issue becomes even more pressing when failing to account for model structural errors as will be discussed in section 3 2 for these reasons bma tends to fall short of a modeler s expectations with regard to predictive coverage domingos 2000 schöniger et al 2015b when it is employed under the realistic condition that the considered model set does not sample the hypothetical model space sufficiently well the representation of model space by the model set is indicated by the so called m closed m complete and m open settings bernardo and smith 1994 in line with the assumption of a completely sampled model space and as discussed in section 2 bms and bma work under the assumption that a true model exists and is part of the considered model set i e that the true model is identifiable this assumption is also referred to as the m closed view bernardo and smith 1994 vehtari and ojanen 2012 for practical purposes this view may be relaxed to the question of identifying a quasi true model among a set of alternatives while acknowledging that posterior model probabilities are only meaningful in the context of an available quasi true model as modelers we might sometimes be in a scenario where we are convinced that a true model exists and conceptually we fully understand it however due to limited resources time knowledge etc yao et al 2017 we are not able to add it to our model set exactly as we conceptualized it and therefore no quasi true model is available in this so called m complete view bernardo and smith 1994 le and clarke 2017 we also cannot identify this model but pursue optimal approximation via cv type methods alternatively bmc allows to approach the truth with combined models that enrich the part of model space that is spanned by the original model candidates in our set as cornerstones as opposed to bms bma which in this view only evaluates and ranks the cornerstones kim and ghahramani 2012 according to the m open view vehtari and ojanen 2012 in contrast the truth cannot be covered by the considered model space it is even questionable if a fixed and finite truth exists in this case methods for model selection and averaging of cv type still promise to maximize predictive coverage piironen and vehtari 2017 to decide whether extending the considered set of models with new competitors is more promising than a selection or combination of existing models the distance between the competing models and the truth as represented by the data needs to be assessed this can be evaluated with a statistical distance metric such as kullback leibler divergence approaches in this direction were proposed by e g abramowitz and gupta 2008 and schöniger et al 2015b further research along these lines is needed to provide a better characterization of the model set and model space for specific applications and therewith to support the choice of appropriate multi model methods 3 2 definition of the likelihood function bme is the core quantity for updating model weights in bms bma for individual models or in bcms bcma for combined models as marginalized likelihood bme is always conditional on the assumptions that define the likelihood function i e the mathematical description of the residuals between predictions and observations hence a proper definition of likelihood is crucial note that despite the default white noise or i i d errors assumption residuals typically show to be correlated heteroscedastic and non gaussian in hydrological applications schoups and vrugt 2010 although often treated in a lumped manner within the likelihood formulation random measurement errors should be treated separately from systematic errors that originate from structural deficits of the model these structural errors should be explicitly modeled for example by statistically describing biases in model forecasts del giudice et al 2013 then the likelihood function and ultimately bme reflect a bias corrected model s mismatch on an objective scale related to measurement error if model structural errors are not properly accounted for bayesian model weights may seem overly decisive lu et al 2013 schöniger et al 2014 because the flawed models are tested against the data in such a strict manner that ever so slight differences in performance will be decisive for model ranking in such common cases the interpretation of model weights as a degree of model plausibility becomes questionable in fact it seems hard to defend a multi model approach that does not account for structural errors in the individual models since the very motivation of multi modeling rests upon the fear that not all if any of the models are indeed entirely accurate hence adding an explicit error model e g a statistical model on top of the actual conceptual or physics based model is a form of model space enrichment see section 3 1 that decreases the extent to which the implicit assumption of bms bma true model is contained in the set is violated this implies two steps of modelling first constructing a process model and second adding an explicit error model to describe the residuals after letting the process model face data the likelihood function in the bms bma framework may then contain this error model opposed to that a recent approach promotes a modeling philosophy in which everything that causes residuals between model predictions and data including the measurement process is explicitly modelled altogether in one step for example a specified statistical error model is directly attached to a physics based model such that this so called mixed model as a whole is evaluated with regard to its plausibility in the light of data while the debatable definition of an appropriate likelihood function for measurement error is entirely avoided nearing et al 2016 3 3 evaluation of bme there is plenty of literature on the implementation of bma bms for fields where bme can be estimated comparably easily e g linear regression gaussian linear models and graphical models e g madigan and raftery 1994 hoeting et al 1999 wasserman 2000 burnham and anderson 2003 however in the field of hydro geo logy we typically deal with complex non linear models with non gaussian parameter distributions analytical solutions e g based on conjugate distributions are hence rarely applicable friel and wyse 2012 which necessitates mathematical approximations like the bic or kic schwarz 1978 kashyap 1982 or numerical bme integration raftery 1995 3 3 1 sampling based bme evaluation numerical sampling based integration methods employ e g nested sampling skilling 2004 gibbs sampling chib 1995 or various versions originating from importance sampling e g gelman and meng 1998 the idea behind importance sampling is to cover the parameter space regions of interest via so called importance distributions setting these importance distributions to either the prior or posterior distribution i e the endpoints of bayesian statistics yields the arithmetic mean estimator e g schöniger et al 2014 or harmonic mean estimator newton and raftery 1994 respectively conceptually other importance sampling based approaches are situated somewhere in between these two this includes recently developed or improved techniques like thermodynamic integration liu et al 2016 or estimators based on gaussian mixtures as importance distributions volpi et al 2017 benchmarking studies that compare various methods for estimating bme are for example friel and wyse 2012 schöniger et al 2014 liu et al 2016 brunetti et al 2017 and zeng et al 2018 alternative approaches employ surrogate models zeng et al 2018 mohamadi et al 2018 and references therein where bme is evaluated not for the original often expensive models but for cheap proxies further methods have been proposed which pursue the idea of bms bma but are able to avoid the evaluation of bme in specific model settings e g via trans dimensional reversible jump mcmc green 1995 3 3 2 approximation based bme evaluation as pointed out in the context of bms section 2 2 the computational effort of evaluating bme and hence bayesian model weights can numerically become prohibitive which necessitates the use of approximations like the kic or bic note that neither the kic nor the bic actually has an information theoretic background and as such could be considered misnomers burnham and anderson 2004 the kic is based on the laplace approximation and requires the posterior parameter distribution of the candidate model to be gaussian the posterior is then either evaluated at the maximum a posteriori estimate map or at the maximum likelihood estimate mle with kic map being a more reliable approximator than kic mle schöniger et al 2014 the bic can be derived by further simplification of the kic in the limit of infinite data set size bic behaves strongly consistently claeskens and hjort 2008 and will always try to identify a true model or quasi true model as sufficient representation of the truth burnham and anderson 2004 even if there is none in the set due to their ease of use these bme approximators are likewise applied for computationally efficient estimation of bme based posterior model weights in bma an example for this is maximum likelihood bma mlbma neuman 2003 ye et al 2004 neuman et al 2012 that is applicable in fields where computationally demanding models are employed like hydro geo logy this variant of bma is named for its use of the kic mle to approximate bme to increase accuracy schöniger et al 2014 recommend to perform kic map bma instead if full bma is unfeasible note that in approximation based bma the resulting weights represent at best a rough approximation of bayesian probability in the case of kic at worst they may lead to a completely opposite model ranking than full bma in the case of bic schöniger et al 2014 3 4 robustness of model ranking results the robustness of bayesian model weights depends on the uncertainty that exists in input driving and output calibration data schöniger et al 2015b have proposed a parametric bootstrap approach to visualize how these sources of uncertainty propagate to uncertainty in bayesian model weights analyzing the lack of robustness in model weights has been shown to shield us from potentially wrong decisions based on uncertain model ranking results such analyses further allow to prioritize data collection efforts to reduce predictive uncertainty in a most efficient manner by identifying the relative importance of parameter uncertainty within model uncertainty vs model choice uncertainty between model uncertainty 3 5 physical interpretation of predictions model combination in hydrology might be criticized for its lack of strict mass balance conservation cf section 2 4 1 this argument would prefer pdf averaging over actual model combination however giving conservation of potentially wrongly specified mass balances priority over increased predictive performance is certainly debatable in some contexts if the goal of a modeling study is to identify the importance of a specific physical process of course presumed to be generally valid physical laws should be respected by the model and should not be weakened unnecessarily however if the goal is to predict as accurately and reliably as possible it might prove more successful to allow for some more degrees of freedom accounting for errors that do not result from the physical laws but from wrongly specified boundary conditions oversimplification missing processes or other shortcomings it seems that no hydrological model is shielded against these types of errors and it remains an open question how to deal with them model combination approaches and statistical error treatment approaches cf section 3 2 might provide additional insight which in the end will help formulating better models that can safely rely on physical laws 3 6 available software readily available software packages for bayesian modeling and model evaluation exist pymc3 salvatier et al 2016 stan carpenter et al 2017 winbugs lunn et al 2000 jags plummer et al 2003 etc however most of them provide bayesian cv based rather than bme based model evaluation and ranking the ones that allow for bme evaluation are often sampling algorithms that provide bme as a side product emcee foreman mackey et al 2013 dream vrugt et al 2009 mrbayes huelsenbeck and ronquist 2001 etc a detailed evaluation and discussion of these software packages is beyond the scope of this study and we also do not want to give recommendations in choosing one over the other instead we wish to emphasize that understanding the limiting assumptions and scrutinizing their validity for the application case at hand should be of primary importance to the modeler available tools always come with the risk of being used as black box and this can lead to false conclusions in their own right such tools might still be helpful but we want to encourage the users to check whether their modeling task at hand can be treated by such software regarding the philosophical and methodical background discussed 4 guideline to identify the best suited multi model approach a towel it says is about the most massively useful thing an interstellar hitchhiker can have partly it has great practical value adams 1979 to not throw in the towel we propose a scheme for finding the most suitable multi modeling approach for any modeling task at hand starting from the philosophical perspective enables us to find a clear way through the model selection and averaging tree as depicted in fig 5 bms is our way to go if we are interested in process identification and the process is represented as identifiable target model in our set of models depending on the data availability this path will pass through bma which enables us to handle the uncertainty between all plausible hypotheses probabilistically until one reveals itself as best representation of the truth however if we think that this target model is not a single model but rather a combined model which is situated somewhere between these candidates in the set we can propose and select a corresponding combined model using bcma and ultimately bcms in fig 5 we see that all bms bma bcms and bcma are aiming at model identification by applying bcma bcms we avoid the uneasy assumption of having a quasi true model in the set immediately but still we aim to identify a best combined model that represents the truth if we have to deny the initial question about whether there is an identifiable true model or at least a quasi true model we can only seek to approximate this truth to obtain plausible predictions then our options are either to enrich model space via classic combination approaches ewa bga gra or to select a single model that promises the best while parsimonious approximation actual model combination approaches as in the former option estimate model weights such that the truth is approximated by a composite model ensemble again this is also different from bcma during which the combined models are a priori defined by the modeler and not meant to only approximate the truth but to allow for its identification the latter option to select a model of justified complexity remains as the way to approximate the truth when only one best model is desired this is achieved by cv and actual information theoretic ics thereby if no model can clearly outperform the others during the selection preliminary cv based weights can be assigned to the models to handle the uncertainty in selecting one however note that this is no actual model combination and therefore should also not be applied as such 5 summary and outlook don t panic adams 1979 numerous methods for model selection or averaging exist that are to some degree bayesian but modelers have to be very cautious whether this pertains to actual bms bma and from which philosophical point of view the respective method is valid bayesian model combination bcms bcma is often confused with bma but serves a different purpose our primary motivation for this review is therefore to create awareness when starting any modeling endeavor we urge to carefully consider the modeling objectives the statistical approach for treating conceptual uncertainty including implementation choices and the conclusions allowed to be drawn from such analyses we have discussed that 1 the philosophical perspectives of statistical methods to address conceptual uncertainty can be distinguished in process identification and truth approximation 2 this distinction relates to the question of whether we believe that the truth is contained in the considered model set or not 3 bms and bma are tailored to process identification because they assume that the quasi true model is contained in the set of candidates 4 for the above reason bma converges to bms with increasing evidence in the data 5 and hence while bma has often been considered a method to address conceptual model uncertainty it rather expresses the degree of uncertainty in fully converging towards the most recognized true model candidate 6 to increase predictive skill model combination on the level of forecasts may be more successful in many cases 7 bma bms can be applied to an enriched model space of combined models resulting in bcma bcms formally bma bms and bcma bcms are conducted the same way see appendix a however they differ in the type of models we define to be in the model set the discussion revealed various potential avenues for further research such as a systematic coverage of model space how can we assure that the considered set of models is an adequate representation of model space model combination probabilities are always conditional on that set which necessitates approaches to measure the currently covered model space and to assign consistent prior model probabilities the suitability of bma bms to compare models of vastly different complexity which range of model complexity may or should be present in the model set since bms bma implicitly follows the principle of parsimony any model in the set needs to be justifiable in the light of the data to be identifiable as the quasi true model schöniger et al 2015a a solid treatment of measurement and model structural errors in multi model approaches how can we adequately describe our assumptions about model specific structural errors and model independent measurement errors the appropriateness of commonly used likelihood functions is questionable and more realistic descriptions e g in the form of statistical error models are needed facilitating access of practitioners to the theoretical methods discussed here how can we promote the understanding and use of the discussed methods in practice available software mostly covers cv type model rating and attempts for generally applicable software are usually not designed for physics based models from hydrology yet the practical difficulty of identifying relevant differences in models how can we assure that the data set is informative for model identification via bma bms optimal design of experiments can help maximize the visibility of model structural differences nowak and guthke 2016 a guideline for model development can the knowledge gained in the evaluation of yet non true models be utilized for creating enhanced model candidates a structured scheme of how to isolate the strong parts of a model and transfer it to another model with complementary strong features would boost model development and system understanding overall we hope to further strengthen the utility of bayesian methods in the face of conceptual uncertainty with this review by directing their use into the right channels conflict of interest the authors declared that there is no conflict of interest acknowledgments the authors would like to thank the german research foundation dfg for financial support of the project within the research training group integrated hydrosystem modelling grk 1829 at the university of tübingen within the cluster of excellence in simulation technology exc 310 2 at the university of stuttgart and within the collaborative research centre campos sfb 1253 between both universities and their partner institutions further we thank jonghyun harry lee ahmed elshall and four anonymous reviewers for their constructive comments and suggestions that have helped to improve the manuscript appendix a equations in bayesian model averaging bma a model weight expresses the probability of a model to be the true one among n m distinct models each model m 1 2 n m is assigned a prior model probability p m m which is updated to the posterior model probability p m m d given data d via bayes theorem for models a 1 p m m d p d m m p m m m 1 n m p d m m p m m with bayesian model evidence p d m m a 2 p d m m p d m m θ m p θ m m m d θ m as likelihood p d m m θ m marginalized over the whole prior parameter distribution p θ m m m of parameters θ m of m m formally these equations are used likewise in bcma but each m m is then substituted by a combined model cm k with k 1 2 n cm 
6576,flash floods are among the most devastating natural hazards but the analyses of flash floods are hindered by the lack of hydro meteorological data in particular data issues are critical in ungauged basins this paper presents an investigation of flash flood modeling using coupled hydrological and hydrodynamic models specifically the topography based hydrological model topmodel is employed to obtain the flood hydrograph and the coupled 1d 2d hydrodynamic model mikeflood is used to simulate floodplain inundation the methods are tested in the pajiang river a mountainous basin in south china the results show that the topmodel effectively simulates the flood hydrograph and the nash sutcliffe coefficients of the flow rate are larger than 0 80 for both calibration and validation also mikeflood effectively simulates the flood inundation extent and flooding depth and the bias for simulated water level is 0 004 m further using socioeconomic and historical flood disaster datasets areas with high flash flood risk are identified a flash flood risk assessment shows a clear trend of change in the flood hazard and the spatial heterogeneity of the flood vulnerability is significant a sensitivity analysis of the flood vulnerability to the socioeconomic indicators indicates that cultivated land is the most sensitive indicators and the population and the construction areas have similar influences overall the coupled hydrological and hydrodynamic models prove effective and can be used in practical applications for flash flood risk mapping keywords topmodel mikeflood ungauged basins runoff simulation flash flood assessment 1 introduction flash floods are one of the most frequent challenging and devastating natural hazards worldwide and are distinguished from regular floods by their rapid occurrence on short timescales high frequency of the flood occurrence and destructive results require an ongoing improvement on detailed risk assessment and mapping of flash floods stefanidis and stathis 2013 generally detailed risk assessment of flash floods are hindered by 1 insufficient records such as streamflow and flood damage data at the site of interest viero et al 2014 2 low precision of the socioeconomic data in each grid dutta et al 2003 and 3 uncertainty in damage estimation due to the lack of data and the model structure glas et al 2016 these challenges substantially hinder the process of transition from traditional flood defense strategies to a flood risk management approach hegger et al 2014 therefore generating accurate flood modeling is one of the prime goals of scientists and governments the risk of flash floods is a combination of the natural hazard and vulnerability it is schematized in an equation risk hazard vulnerability wisner et al 2003 in flood risk assessment previous studies have used different types of models to assess flash floods these models are classified into three types generally they are multicriteria decision analysis mcda data driven models and process driven models respectively fernández and lutz 2010 siahkamari et al 2018 tehrany et al 2014 viero et al 2014 analytic hierarchy process ahp is the most widely used multicriteria decision analysis and has been applied to a wide range of scientific fields saaty 1994 stefanidis and stathis 2013 ahp is a structured technique used for analyzing complex parameter problems it defines the weight of each parameter after they are ranked by expert knowledge according to their relative importance kazakis et al 2015 with the help of geographic information systems gis ahp has proven successful in flood hazards analysis ahmad and verma 2018 khosravi et al 2016 rahmati et al 2016a but this method contains many biases and can be affected by the judgment of the experts chang et al 2008 tehrany et al 2013 mcda is mostly based on a linear assumption which is not appropriate for catchment studies catchment studies have nonlinear correlations between dynamics of flash floods and parameters such as complex terrain liu and de smedt 2005 rahmati et al 2016c the fundamental concepts of data driven models are to model nonlinear correlations between some input conditioning factors and an output darabi et al 2019 haghizadeh et al 2017 kanevski et al 2009 machine learning is the main source of techniques for the data driven modeling problem pradhan and lee 2010 tehrany et al 2013 the tested machine learning methods such as artificial neural networks anns ngo et al 2018 support vector machine svm tehrany et al 2015 decision tree dt tehrany et al 2013 and random forest rf rahmati and pourghasemi 2017 are capable to detect the flooded locations with relatively higher accuracy however machine learning methods are commonly considered as black box models which do not account for the dynamics of flash floods and may fail to capture the connectivity between river channels and floodplains rahmati et al 2016b zhao et al 2017 many efforts have been made to couple hydrologic and hydrodynamic models for detailed flood risk assessment knebl et al 2005 developed a regional scale flood modeling which consists of the hydrologic engineering center s hydrologic modeling system hec hms as well as hec s river analysis system hec ras bonnifait et al 2009 coupled topmodel with a 1d hydraulic model named carima for reconstructing the catastrophic flood event in the gard region france nguyen et al 2016 developed hiresflood uci by coupling the nws s hydrologic model hl rdhm with the hydraulic model brezo for flash flood modeling in baron fork at eldon oklahoma these mentioned hydrologic hydrodynamic models have applied in flood hazard assessment with very good results and are fundamental to the reliable assessment of flash flood vulnerability grimaldi et al 2013 viero et al 2014 in flood vulnerability assessment the economic loss to different land use features is calculated based on the simulated flood parameters obtained from the hydrologic hydrodynamic model it is an objective way to describe the consequences of a flood ballesteros cánovas et al 2013 dutta et al 2003 in this paper we develop a coupled model that combines topmodel beven and kirkby 1979 and mikeflood patro et al 2009 to map the flood inundation and assess the flash flood risk in ungauged basins in reality however there are a large number of ungauged basins lacking data such as precipitation flow and other social economic datasets when no sufficient flow records are available at the site of interest regionalization techniques may be applied to derive those estimates kokkonen et al 2003 regionalization can be defined as the transfer of information from one catchment to another this method can be classified into the hydrologic model based and data driven groups mcintyre et al 2005 razavi and coulibaly 2013 the hydrologic model based approaches e g disaggregation spatial proximity and physical similarity not only transfer lumped or distributed hydrological model parameters from nearby gauged basin to ungauged basins but also assume that the streamflow contribution from each ungauged sub catchment to the total catchment yield is proportional to the ratio of the catchment s topographic index mcintyre et al 2005 schreider et al 2002 sellami et al 2014 young 2006 the hydrologic model based approaches have been widely used for streamflow time series generation in data scarce and ungauged basins but they have been rarely expanded to the simulation of flood propagation processes for the data driven approaches e g ann regression based methods arma they consider the nonlinear relationship between flow and other meteorological factors chiang et al 2002 compared with data driven approaches hydrologic model based approaches can better capture the physical processes in a catchment and are an important tool to estimate the elements of the water cycle anselmo et al 1996 lan et al 2018 therefore in this paper we use hydrologic model based approaches to generate flow rate time series at the site of interest and mitigate the critical problem of the lack of streamflow data for the detailed hazard assessment of flash floods urban damage estimation is commonly based on applying the damage stage curve to cases where historic flood damage data is not available smith 1994 nevertheless the applicability the damage stage curve to rural damage estimation has not been duly verified due to lack of observations dutta et al 2003 moreover this method results in disadvantages due to the lack of accounting for the dynamics of flash floods and not considering the sensitivity of flood risk to these criteria glas et al 2016 merwade et al 2008 ouma and tateishi 2014 thus we propose a new flood loss estimation model it is a combination of simulated flood inundation parameters and socioeconomic features the sensitivity of the flood risk to these socioeconomic criteria is considered as well in flood risk management not only is information on the flood hazard desirable but also information on the consequences of a flood apel et al 2008 moel et al 2009 with the increased availability of hydrological hydrodynamic models and datasets of the natural environment there has been an increased focus on detailed flood hazard mapping anselmo et al 1996 mai and de smedt 2017 however the influence of different socioeconomic indicators on flood damage are still difficult to quantify precisely in ungauged basins moel et al 2009 comprehensive risk assessments that take into account the hazard and the vulnerability aspect of flood risk are gaining increasing attention in the fields of flood design and flood risk management apel et al 2008 as the challenges of determining the flood parameters in flood hazard assessments and the loss estimation in flood vulnerability assessments are mitigated flood risk mapping will become more precise flood risk mapping can serve as an informative tool for decision makers regional governments cannot only decide for themselves how strict flood zones are incorporated into the spatial planning policies but this information also raises the awareness of the general public about floods by making the data available online hegger et al 2014 insurance companies use the maps to determine different premiums for different flood zones therefore these maps help the insurance industry hudson 2017 as mentioned above flood risk mapping provides support for the transition from traditional flood defense strategies to a flood risk management approach therefore the objectives of this study are 1 to investigate the flood hazard assessment coupling topmodel and mikeflood 2 to estimate the flood vulnerability assessment using various water depths and durations 3 to conduct a risk assessment and sensitivity analysis based on a grid transformation of the inundation and damage results 2 methodology the developed framework of the current research is presented as a flowchart in fig 1 some of the most important features relevant to this research work are the following 1 the topmodel parameters of the total basin are optimized by the flood events with different peaks and return periods t based on the shuffled complex evolution sce ua algorithm 2 the topographic index ti values of each ungauged subbasin are obtained using gis methods and the rainfall events with different return periods are selected 3 the discharge processes in the ungauged subbasins is simulated by using the optimal parameter values of the total basin ti values of each subbasin precipitation and evaporation data 4 the discharge processes of each ungauged subbasins is selected as the upstream and point source boundary of mikeflood the stream network structures and cross sections are generalized and a flexible mesh of triangular elements is generated 5 the roughness coefficients are adjusted to bridge the gap between the observed and simulated water levels as well as the observed and simulated inundation 6 the maximum water depth and duration of the flood events with different return periods are simulated by coupling the topmodel and mikeflood 7 the flash flood hazard mapping is conducted by the linear combination of the cell values of the maximum water depth and duration 8 the flood loss distributions of each indicator with different maximum water depths and durations are estimated based on the area of the flood parameters and the flood vulnerability distribution is mapped 9 a sensitivity analysis is performed on the indicators that may potentially influence the risk assessment a detailed description of the above steps is given below 2 1 streamflow simulation topmodel a physically based semi distributed topohydrological model was first introduced by beven and kirkby 1979 and has been widely used for hydrological modeling bastola et al 2008 blazkova and beven 1997 in this model topography and soil characteristics are regarded as critical influencing factors of runoff jeziorska and niedzielski 2018 the precipitation data are spatially averaged for the subbasins and the total basin using the thiessen polygon method schumann 1998 the topography is quantitatively expressed by ti value section s1 1 computed by a single flow direction algorithm wolock and mccabe jr 1995 all points with the same ti value have the same characteristics of the hydrological response nash s instantaneous unit hydrograph nash 1957 and a linear reservoir method cheng 2010 are used for the runoff concentration calculation a shuffled complex evolution sce ua approach is a commonly used algorithm because it is open source and is the first algorithm aimed specifically at calibrating hydrological models duan et al 1994 the nash sutcliffe efficiency nse coefficient section s1 2 is used as a measure of agreement between the observed and simulated runoff nash and sutcliffe 1970 since there are several nse coefficient intervals to describe the model performance we use the general performance ratings table s1 recommended by moriasi et al 2007 the discharge processes of the upstream and point source boundary of the mike11 hydrodynamic model at the site of interest are unavailable they can be defined as ungauged basins sivapalan et al 2003 schreider et al 2002 assumed that the streamflow contribution from each ungauged subbasin to the total basin is proportional to the ratio of the basin s ti the ti values of each subbasin are computed based on a digital elevation model dem fig 2 jeziorska and niedzielski 2018 found that topmodel based on floods with single peaks and high return periods perform better most of flood control standard of the dike in the study area is under 5 years or above 20 years according to the result of pearson type iii distribution frequency and streamflow simulation rainfall events took place on 7 may 2010 t 20 years and 15 may 2013 t 5 years are taken as the input datasets to the hydrologic model fig s1 the rainfall and flood event are assumed the same return periods the precipitation ti values and the evaporation data as input datasets to the calibrated topmodel to simulate the discharge process of the ungauged basins for t 5 years and t 20 years 2 2 flash flood inundation simulation the hydrodynamic model mikeflood simulates the 1d flow in river channels mike11 and the 2d flow mike21 in floodplains patro et al 2009 the 1d models fail to provide information on the flow field whereas the 2d models require substantial computer time hence attempts have been made to couple the 1d river flow models with the 2d floodplain flow models viero et al 2014 the coupled 1d 2d models offer great advantages for real time simulations of flood events as shown in fig 3 this paper selects the outlet discharge processes of the gangxing gx xiaping xp and shunlian sl subbasins as the upstream boundary and the longxishui lxs longnanhe lnh and wudongshui wds subbasins as the point source boundary the main features of the ungauged subbasins are shown in table s3 the downstream boundary is the dmx according to the observed water levels the domain of the mike21 model is described by a mesh consisting of approximately 8583 nodes and 15 090 triangular elements and the area of the triangular element ranges from 548 to 25 123 m2 mikeflood computes the appropriate time step based on the courant number stability criterion león et al 1990 it is difficult to assign reliable initial conditions to the model for a mountainous river like the pajiang river thus a hot start method dhi 2017 is used by calculating the inflow time series of the first 24 h with a time step of 0 1 s and the results of the end time are assigned to the initial conditions based on the discharge process of the simulated hydrograph of upstream and point source boundary the mike11 model is coupled with topmodel and the mikeflood couples the mike11 model with mike21 model using a lateral link method dhi 2012 hence a coupled hydrologic and hydrodynamic model named tpmf is created the roughness coefficients are adjusted to bridge the gap between the observed and simulated water levels at cross sections where the elevation of the historic flood stage is available there are no common performance indices to assess the performance of the tpmf model patro et al 2009 used performance indices like coefficient of determination r2 percentage deviation in peak dev to assess the prediction performance of the mikeflood mai and de smedt 2017 applied nse to assess the prediction performance of the hec ras model this study applies flood stage error root mean square error rmse dev and bias section s1 2 to measure the agreement between the observed and simulated water levels the simulated flood inundation is validated by comparing the calculated and field measured maximum floodplain inundation 2 3 risk assessment of flash flood 2 3 1 hazard assessment ahp is a structured technique used for analyzing complex parameter problems it defines the weight of each parameter after they are ranked by expert knowledge according to their relative importance kazakis et al 2015 the results of the flood routing and propagation process simulated by the tpmf model are the maximum water depth and flood duration they are considered as the most important flood parameters and selected as the flood hazard indicators moel et al 2009 patro et al 2009 the selected parameters are not complex thus the maximum water depth and flood duration are multiplied by the same weight factor empirically to map the flood hazard distribution for t 5 years and t 20 years section s1 3 2 3 2 vulnerability and risk assessment the flood loss estimation depends on the indicators that are affected by the flash flood cultural damage ecological damage and indirect damage e g due to business disruption are still very difficult to quantify moel et al 2009 this paper tries to compute the loss rate of the vulnerability indicators such as cultivated land construction and population for different maximum water depths and durations these vulnerability indicators are multiplied by the same weight factor empirically to map the flood vulnerability distribution for t 5 years and t 20 years section s1 4 the flash flood assessment is a comprehensive of information on the flood hazard and the consequences of a flood apel et al 2008 maskrey 1989 believed that risk can be defined as risk hazard vulnerability wu et al 2015 have applied the equation to assess the flood risk in the huaihe river basin china and the result is reliable based on the tpmf integrated model a detailed flood risk assessment is conducted by maskrey s definition section s1 5 2 3 3 sensitivity analysis a geographical sensitivity analysis of an overlay based suitability analysis can indicate which parameter is the most determining the values of the output map lodwick et al 1990 in particular the problem related to the influence of each single parameter on the final vulnerability value is subject of research by other authors kazakis et al 2015 napolitano and fabbri 1996 sensitivity analysis in this paper applies the map removal sensitivity measure defined by lodwick et al 1990 it represents the sensitivity associated with removing one or more indicators from a suitability analysis section s1 6 3 study area and data in this study we conduct a case study in the upper pajiang river basin pjrb this basin which has a drainage area of about 468 km2 and drains at the damiaoxia gauge station dmx is located in fogang county qingyuan city china fig 2 the pajiang river runs from its origin dongtian candle to the county border and eventually meets the beijiang river in qingxin county the estimated annual precipitation ranges from 2000 to 2200 mm due to the summer monsoon the precipitation is unevenly distributed throughout the year and more than 70 of the annual precipitation falls from april to september as a mountainous river the mean slope of the upper pajiang river is 4 82 and the maximum hydrologic distance between the outlet and the basin divide is 41 6 km the slope drops greatly and the river drainage may not have the ability to contain outflow after stormwater events in the flood season as a result flash floods with large peak flows and short durations occur frequently in this area basin wide floods which occurred in 1988 1993 and 2013 were the most destructive disaster events in the historical record the flood event of 15 may 2013 with a return period above 20 years caused 11 deaths and a direct economic loss of 1 17 billion yuan the selected hydrological model domain contains the upper pjrb and the selected hydrodynamic modeling domain with an area of 91 87 km2 is located in the proximity of the mainstream where the elevation of the floodplain is less than 200 m fig 2 hourly streamflow and climatic data of thirty flood events from 1971 to 2013 are used 25 events are used for calibration and the remainder is utilized to verify the developed model 4 results 4 1 streamflow simulation in ungauged basins based on topmodel in the calibration periods the nse coefficients of the 25 flood events range from 0 71 to 0 96 and the mean nse is 0 85 in the validation periods the nse coefficients of the five flood events range from 0 72 to 0 93 and the mean nse is 0 86 we use 20 of the flood peak discharge as the cutoff for the occurrence of flood events the flood peak discharge of three flood events is out of the threshold in the calibration period and no flood events in the validation period in the calibration and validation periods generally the simulated flood peak discharge values are lower than the observed ones the calibrated parameter values are shown in table s4 the nse coefficients of the different return periods are compared to evaluate the influence of floods with different return periods on the model performance it is observed that the higher the return periods the better the model performance is to evaluate the influence of floods with different peak types on the model performance we compare the simulated and observed discharge values of the major flood events with different return periods and peak types fig 4 overall the performance of the topmodel based on floods with single or multiple peaks is very good and floods with single peaks and high return periods perform better the results of the model performance indicate that the topmodel is suitable for runoff simulations the precipitation for t 20 years and t 5 years is independently selected as input data to the calibrated topmodel and the discharge processes of the previous six ungauged basins are simulated fig 5 shows that in the same subbasin the discharge processes differ for different return periods the higher the return period the heavier the peak discharge is in the same return period the discharge processes differ for different drainage areas as well the larger the drainage area the heavier the peak discharge is the peak discharge increases with increasing drainage area the results illustrate that the discharge processes derived from the topmodel are in close agreement with the actual hydrological processes 4 2 flash flood inundation simulation based on tpmf model 4 2 1 performance of the tpmf model the average dev and flood stage error between the flood stage and the simulated peak water level of the main cross section are 0 55 0 73 m respectively the rmse and bias are 0 98 and 0 004 m respectively table s5 fig 6 shows a comparison of the extents of the simulated and measured floodplain inundations the field investigation has shown that the middle reach of the mainstream suffered the most severe damage during the flood events of 15 may 2013 the inundation derived from the tpmf model is in agreement with the field data the results of the water level in the channel and the floodplain inundation illustrate that the simulated inundation agrees well with the observed data therefore the tpmf model performance is satisfactory nevertheless the model performance is less effective in tributaries such as the chengjing river and yaodong river the simulated inundation extent is smaller than the measured extent 4 2 2 simulation of flood routing and propagation process in different return periods the simulated inundation maps showing the water depth and extent of inundation in different time steps are presented in fig 7 in terms of the temporal change trend the inundation area of the two historic flood events increases rapidly after the 48th hour however the area decreases between the 48th to 72nd hour and changes little after the 72nd hour in terms of the spatial change trend the vicinity of the stream is affected severely by the floods as the time step increases the inundation areas near the mainstream and tributaries increase gradually and the changes in the inundation area are more significant close to the mainstream than in the tributaries in the same time step the influences of the flood are different for different reaches the average water depth decreases gradually with increasing distance from the stream the inundation area is larger and the average water depth is deeper near the mainstream than near the tributaries at the end of the simulation the maximum water depth and duration above the threshold are computed the inundation area average water depth and average duration increase gradually with an increase in the return period the most susceptible region is in the middle reach of the upper pjr fig 8 and table s6 4 3 risk assessment based on tpmf model 4 3 1 risk assessment of flash flood the hazard and vulnerability potential are classified into the four classes of low moderate high and very high using a natural break classification the flash flood hazard decreases from northeast to southwest and with increasing distance from the stream figs s2 s3 unlike the flash flood hazard the spatial heterogeneity of the flash flood vulnerability is significant in general the flash flood vulnerability is higher in the downstream than the upstream area figs s4 s5 the flood risk maps based on the hazard and vulnerability assessment for t 5 years and t 20 years are presented in figs 9 and 10 four classes of flood risk are shown i e low moderate high and very high and the areas of the flood risk classes for different return periods are presented the threshold of the flash flood risk is lower for t 5 years than for t 20 years due to the influence of the flash flood vulnerability table s7 the spatial heterogeneity of the flash flood risk is apparent however in general the flash flood risk decreases from northeast to southwest for flash floods with t 5 years a majority of the inundations are in the high and very high classes with area ratios of 35 69 and 44 46 respectively for flash floods with t 20 years the inundations in the very high class are located in the middle to upper reaches and 86 16 of the total study area is in the high to very high flash flood risk class 4 3 2 sensitivity analysis table s8 shows the results of the sensitivity analysis the most sensitive indicator of floods is cultivated land approximately with 260 of sensitivity and 43 of variation index for both t 5 years and t 20 years the new vulnerability index is calculated without considering the parameter cultivated land population and construction respectively comparison between the new vulnerability index and the initial one shows that the cultivated land has a bigger influence in the middle down reach fig 11 the difference of sensitivity and variation index between population and construction is not distinct fig 12 the vulnerability variation index of the cultivated land increases with increasing levels of the risk classes while that of population and construction decreases for t 5 years for t 20 years the vulnerability variation index of the cultivated land decreases from high to very high risk while that of population and construction increases generally the sensitivity of flood risk to the socioeconomic indicators differs for different risk classes 5 discussions 5 1 flash flood hazard analysis the proposed tpmf model for the assessment of flash flood can be a useful tool for the mitigation of the devastating impact of floods moreover the performance of the model and sensitivity analysis can support the analysis the average dev and flood stage error between the flood stage and the simulated peak water level of the main cross section are 0 55 0 73 m respectively patro et al 2009 applied mikeflood and remote sensing data to simulate flood inundation and the average dev in peak is 0 98 bonnifait et al 2009 observed an underestimation of maximum water levels by about 1 5 to 2 m all along the stream when coupled topmodel with a 1d hydraulic model named carima nguyen et al 2016 developed hiresflood uci by coupling the hl rdhm hydrologic model with the brezo hydraulic model for flash flood modeling and the flood stage error varied from 0 19 to 0 82 m the aforementioned hydrologic and hydrodynamic models are calibrated by available flood records the hydrograph values may need to be calibrated to produce the smallest overall prediction error for different hydrological settings of different periods and subbasins but in data scarce subbasins of the pjrb it is impractical to calibrate the hydrograph of each subbasin accordingly the proposed discharge simulation technique in ungauged basins is available for providing discharge upstream conditions for the hydraulic model and the proposed tpmf model can be a valuable tool to estimate the depth and duration where flood hazard is high very high the flood inundation simulation has revealed the importance of mainstream in flood events it is necessary to be included in flood prevention plans however records of historical flood events support the importance of both mainstream and tributaries in flood events as shown in fig 6 the simulated inundation extent is smaller than the measured extent especially in tributaries compared with the mainstream the hydrograph of tributaries and rivulets are low flows and water level usually the acceptable performance of the model for extreme events contributes to a good statistical performance for the entire hydrograph but fails to match the observed hydrograph during low flows and water level this interpretation has also been supported in mai and de smedt 2017 according to mai and de smedt 2017 combining the hydrologic model wetspa and the hec ras hydraulic model predicts peak flows and water levels more accurately than other flows similarly bonnifait et al 2009 observed an underestimation of maximum water levels by about 1 5 to 2 m all along the stream when coupled topmodel with a 1d hydraulic model named carima and this result may be partly explained by the underestimation of the hydrological simulations also jeziorska and niedzielski 2018 found that the topmodel seems to provide a superior fit to the large peaks rather than to other hydrological situations mignot et al 2006 found that topographical irregularities and storage areas have a larger influence on the flow for less extreme flood using 2d shallow water equations in this paper the hydrograph in the ungauged subbasins is simulated by precipitation and ti values the precipitation data are spatially averaged which may result in the attenuation of the peak precipitation as shown in fig 3 60 of the ti values are in the range of 4 6 implying that low ti values comprise the majority of total ti values and there are few saturated source areas lin et al 2010 low peak precipitation and ti values may be the most likely reasons for underestimating peak discharges although this discrepancy occurs in simulation for low flows its impact on the overall efficiency measure is rather small jeziorska and niedzielski 2018 5 2 flash flood vulnerability and sensitivity analysis the loss rate of each vulnerability indicator is computed based on the extent of the maximum water depth and duration figs s4 s5 illustrates the significant spatial heterogeneity of the flash flood vulnerability this is consistent with the randomness non identity and fuzziness of the spatial distribution of the flood losses the raster of the flood vulnerability distribution is obtained by overlaying the indicators with the same weight nevertheless the inherent socio economic attributes of the population construction and cultivated land the ability to handle flood disasters is variable roy and blaschke 2015 concluded that a linear combination method with the same weight may not represent the large variations of the indicators and may have resulted in an adverse effect on the accuracy of the flood vulnerability assessment the most sensitive indicator of flood vulnerability assessment is cultivated land for both t 5 years and t 20 years but the sensitivity of vulnerability to the population and construction increase from high to very high risk glas et al 2016 found that the location of the buildings provided the best result when calculating the total damage whereas the impact of the crops was very small fig s6 shows that the cultivated land is an important part of the land use type in the pjrb and mainly located on either side of the river in low lying and flood prone areas when a flood with a return period greater than 20 years occurs the construction areas and the population start to be affected construction and population have greater socio economic value than cultivated land consequently the sensitivity of vulnerability to the population and construction increases in summary the main advantage of the proposed tpmf model is its ability to provide overall assessment of flash flood risk although it is less accuracy to estimate the low flows in tributaries this approach mitigates the critical problem of the lack of socioeconomic data and the uncertainty in damage estimation in the detailed risk assessment of flash floods 6 conclusions in this study we couple topmodel and mikeflood to simulate the floodplain inundation of a mountainous river basin in south china further risk assessment and sensitivity analysis of flash floods are conducted in ungauged basins the following conclusions are drawn 1 the flood routing and propagation processes simulated by the tpmf model are closely related to the local topography the nse values are larger than 0 8 in the calibration and validation periods and the overall performance of the topmodel based on single peak or multi peak flood events is satisfactory the model performance is better for floods with single peaks and high return periods in the tpmf model the average dev and flood stage error between the flood stage and the simulated peak water level of the main cross section are 0 55 0 73 m indicating a satisfactory fit the inundation extent increases rapidly after the 48th hour however the area decreases slowly between the 48th to 72nd hour and changes little after the 72nd hour in terms of the spatial change trend the vicinity of the stream is affected severely by floods which illustrates that the flood routing and propagation processes closely agree with the actual hydrodynamic process 2 the integration of the socioeconomic data and historical flood disaster data into the tpmf model demonstrate that areas with a high risk of flash floods are concentrated in the upper reaches and on both sides of the river the spatial distribution trend of the flash flood hazard is apparent and the spatial heterogeneity of the flash flood vulnerability is significant the most sensitive indicator of flood vulnerability assessment is cultivated land in different return periods but the sensitivity of vulnerability to the population and construction increase from high to very high risk 3 the good agreement between the predicted and measured spatial patterns of the inundated areas indicates that the tpmf model exhibits good suitability for practical applications future research objectives include the integration of mike with another programming language such as dhi matlab so that the optimal parameters of mike can be calibrated automatically more detailed information on the overbank cross sections and the distribution of flood events derived from hyperspectral and high resolution data will significantly improve the hydrodynamic simulation results and the damage estimation another research emphasis is data assimilation by integrating small scale inundation extent data with large scale socio economic data to minimize the difficulties of flash flood risk assessment declaration of interest statement the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the national natural science foundation of china 51779279 51822908 supported by the national key r d program of china 2017yfc0405900 open research foundation of dynamics and the associated process control key laboratory in the pearl river estuary of ministry of water resources 2017kj12 baiqianwan project s young talents plan of special support program in guangdong province 42150001 and the fundamental research funds for the central universities 20187614031620001 are gratefully acknowledged we appreciate the constructive comments and suggestions from the editors the associate editor and the three anonymous reviewers the dem with a cell size of 30 30 m is obtained from the geospatial data cloud http www gscloud cn hourly precipitation and discharge data river network and cross section data are provided by the guangzhou branch of the guangdong provincial bureau of hydrology gzbh in gunagzhou china the evaporation data are obtained from the china meteorological data network http data cma cn author agreement we confirm that the manuscript has been read and approved by all named authors and that there are no other persons who satisfied the criteria for authorship but are not listed we further confirm that the order of authors listed in the manuscript has been approved by all of us appendix a supplementary material supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 03 002 appendix a supplementary material the following are the supplementary data to this article supplementary data 1 
6576,flash floods are among the most devastating natural hazards but the analyses of flash floods are hindered by the lack of hydro meteorological data in particular data issues are critical in ungauged basins this paper presents an investigation of flash flood modeling using coupled hydrological and hydrodynamic models specifically the topography based hydrological model topmodel is employed to obtain the flood hydrograph and the coupled 1d 2d hydrodynamic model mikeflood is used to simulate floodplain inundation the methods are tested in the pajiang river a mountainous basin in south china the results show that the topmodel effectively simulates the flood hydrograph and the nash sutcliffe coefficients of the flow rate are larger than 0 80 for both calibration and validation also mikeflood effectively simulates the flood inundation extent and flooding depth and the bias for simulated water level is 0 004 m further using socioeconomic and historical flood disaster datasets areas with high flash flood risk are identified a flash flood risk assessment shows a clear trend of change in the flood hazard and the spatial heterogeneity of the flood vulnerability is significant a sensitivity analysis of the flood vulnerability to the socioeconomic indicators indicates that cultivated land is the most sensitive indicators and the population and the construction areas have similar influences overall the coupled hydrological and hydrodynamic models prove effective and can be used in practical applications for flash flood risk mapping keywords topmodel mikeflood ungauged basins runoff simulation flash flood assessment 1 introduction flash floods are one of the most frequent challenging and devastating natural hazards worldwide and are distinguished from regular floods by their rapid occurrence on short timescales high frequency of the flood occurrence and destructive results require an ongoing improvement on detailed risk assessment and mapping of flash floods stefanidis and stathis 2013 generally detailed risk assessment of flash floods are hindered by 1 insufficient records such as streamflow and flood damage data at the site of interest viero et al 2014 2 low precision of the socioeconomic data in each grid dutta et al 2003 and 3 uncertainty in damage estimation due to the lack of data and the model structure glas et al 2016 these challenges substantially hinder the process of transition from traditional flood defense strategies to a flood risk management approach hegger et al 2014 therefore generating accurate flood modeling is one of the prime goals of scientists and governments the risk of flash floods is a combination of the natural hazard and vulnerability it is schematized in an equation risk hazard vulnerability wisner et al 2003 in flood risk assessment previous studies have used different types of models to assess flash floods these models are classified into three types generally they are multicriteria decision analysis mcda data driven models and process driven models respectively fernández and lutz 2010 siahkamari et al 2018 tehrany et al 2014 viero et al 2014 analytic hierarchy process ahp is the most widely used multicriteria decision analysis and has been applied to a wide range of scientific fields saaty 1994 stefanidis and stathis 2013 ahp is a structured technique used for analyzing complex parameter problems it defines the weight of each parameter after they are ranked by expert knowledge according to their relative importance kazakis et al 2015 with the help of geographic information systems gis ahp has proven successful in flood hazards analysis ahmad and verma 2018 khosravi et al 2016 rahmati et al 2016a but this method contains many biases and can be affected by the judgment of the experts chang et al 2008 tehrany et al 2013 mcda is mostly based on a linear assumption which is not appropriate for catchment studies catchment studies have nonlinear correlations between dynamics of flash floods and parameters such as complex terrain liu and de smedt 2005 rahmati et al 2016c the fundamental concepts of data driven models are to model nonlinear correlations between some input conditioning factors and an output darabi et al 2019 haghizadeh et al 2017 kanevski et al 2009 machine learning is the main source of techniques for the data driven modeling problem pradhan and lee 2010 tehrany et al 2013 the tested machine learning methods such as artificial neural networks anns ngo et al 2018 support vector machine svm tehrany et al 2015 decision tree dt tehrany et al 2013 and random forest rf rahmati and pourghasemi 2017 are capable to detect the flooded locations with relatively higher accuracy however machine learning methods are commonly considered as black box models which do not account for the dynamics of flash floods and may fail to capture the connectivity between river channels and floodplains rahmati et al 2016b zhao et al 2017 many efforts have been made to couple hydrologic and hydrodynamic models for detailed flood risk assessment knebl et al 2005 developed a regional scale flood modeling which consists of the hydrologic engineering center s hydrologic modeling system hec hms as well as hec s river analysis system hec ras bonnifait et al 2009 coupled topmodel with a 1d hydraulic model named carima for reconstructing the catastrophic flood event in the gard region france nguyen et al 2016 developed hiresflood uci by coupling the nws s hydrologic model hl rdhm with the hydraulic model brezo for flash flood modeling in baron fork at eldon oklahoma these mentioned hydrologic hydrodynamic models have applied in flood hazard assessment with very good results and are fundamental to the reliable assessment of flash flood vulnerability grimaldi et al 2013 viero et al 2014 in flood vulnerability assessment the economic loss to different land use features is calculated based on the simulated flood parameters obtained from the hydrologic hydrodynamic model it is an objective way to describe the consequences of a flood ballesteros cánovas et al 2013 dutta et al 2003 in this paper we develop a coupled model that combines topmodel beven and kirkby 1979 and mikeflood patro et al 2009 to map the flood inundation and assess the flash flood risk in ungauged basins in reality however there are a large number of ungauged basins lacking data such as precipitation flow and other social economic datasets when no sufficient flow records are available at the site of interest regionalization techniques may be applied to derive those estimates kokkonen et al 2003 regionalization can be defined as the transfer of information from one catchment to another this method can be classified into the hydrologic model based and data driven groups mcintyre et al 2005 razavi and coulibaly 2013 the hydrologic model based approaches e g disaggregation spatial proximity and physical similarity not only transfer lumped or distributed hydrological model parameters from nearby gauged basin to ungauged basins but also assume that the streamflow contribution from each ungauged sub catchment to the total catchment yield is proportional to the ratio of the catchment s topographic index mcintyre et al 2005 schreider et al 2002 sellami et al 2014 young 2006 the hydrologic model based approaches have been widely used for streamflow time series generation in data scarce and ungauged basins but they have been rarely expanded to the simulation of flood propagation processes for the data driven approaches e g ann regression based methods arma they consider the nonlinear relationship between flow and other meteorological factors chiang et al 2002 compared with data driven approaches hydrologic model based approaches can better capture the physical processes in a catchment and are an important tool to estimate the elements of the water cycle anselmo et al 1996 lan et al 2018 therefore in this paper we use hydrologic model based approaches to generate flow rate time series at the site of interest and mitigate the critical problem of the lack of streamflow data for the detailed hazard assessment of flash floods urban damage estimation is commonly based on applying the damage stage curve to cases where historic flood damage data is not available smith 1994 nevertheless the applicability the damage stage curve to rural damage estimation has not been duly verified due to lack of observations dutta et al 2003 moreover this method results in disadvantages due to the lack of accounting for the dynamics of flash floods and not considering the sensitivity of flood risk to these criteria glas et al 2016 merwade et al 2008 ouma and tateishi 2014 thus we propose a new flood loss estimation model it is a combination of simulated flood inundation parameters and socioeconomic features the sensitivity of the flood risk to these socioeconomic criteria is considered as well in flood risk management not only is information on the flood hazard desirable but also information on the consequences of a flood apel et al 2008 moel et al 2009 with the increased availability of hydrological hydrodynamic models and datasets of the natural environment there has been an increased focus on detailed flood hazard mapping anselmo et al 1996 mai and de smedt 2017 however the influence of different socioeconomic indicators on flood damage are still difficult to quantify precisely in ungauged basins moel et al 2009 comprehensive risk assessments that take into account the hazard and the vulnerability aspect of flood risk are gaining increasing attention in the fields of flood design and flood risk management apel et al 2008 as the challenges of determining the flood parameters in flood hazard assessments and the loss estimation in flood vulnerability assessments are mitigated flood risk mapping will become more precise flood risk mapping can serve as an informative tool for decision makers regional governments cannot only decide for themselves how strict flood zones are incorporated into the spatial planning policies but this information also raises the awareness of the general public about floods by making the data available online hegger et al 2014 insurance companies use the maps to determine different premiums for different flood zones therefore these maps help the insurance industry hudson 2017 as mentioned above flood risk mapping provides support for the transition from traditional flood defense strategies to a flood risk management approach therefore the objectives of this study are 1 to investigate the flood hazard assessment coupling topmodel and mikeflood 2 to estimate the flood vulnerability assessment using various water depths and durations 3 to conduct a risk assessment and sensitivity analysis based on a grid transformation of the inundation and damage results 2 methodology the developed framework of the current research is presented as a flowchart in fig 1 some of the most important features relevant to this research work are the following 1 the topmodel parameters of the total basin are optimized by the flood events with different peaks and return periods t based on the shuffled complex evolution sce ua algorithm 2 the topographic index ti values of each ungauged subbasin are obtained using gis methods and the rainfall events with different return periods are selected 3 the discharge processes in the ungauged subbasins is simulated by using the optimal parameter values of the total basin ti values of each subbasin precipitation and evaporation data 4 the discharge processes of each ungauged subbasins is selected as the upstream and point source boundary of mikeflood the stream network structures and cross sections are generalized and a flexible mesh of triangular elements is generated 5 the roughness coefficients are adjusted to bridge the gap between the observed and simulated water levels as well as the observed and simulated inundation 6 the maximum water depth and duration of the flood events with different return periods are simulated by coupling the topmodel and mikeflood 7 the flash flood hazard mapping is conducted by the linear combination of the cell values of the maximum water depth and duration 8 the flood loss distributions of each indicator with different maximum water depths and durations are estimated based on the area of the flood parameters and the flood vulnerability distribution is mapped 9 a sensitivity analysis is performed on the indicators that may potentially influence the risk assessment a detailed description of the above steps is given below 2 1 streamflow simulation topmodel a physically based semi distributed topohydrological model was first introduced by beven and kirkby 1979 and has been widely used for hydrological modeling bastola et al 2008 blazkova and beven 1997 in this model topography and soil characteristics are regarded as critical influencing factors of runoff jeziorska and niedzielski 2018 the precipitation data are spatially averaged for the subbasins and the total basin using the thiessen polygon method schumann 1998 the topography is quantitatively expressed by ti value section s1 1 computed by a single flow direction algorithm wolock and mccabe jr 1995 all points with the same ti value have the same characteristics of the hydrological response nash s instantaneous unit hydrograph nash 1957 and a linear reservoir method cheng 2010 are used for the runoff concentration calculation a shuffled complex evolution sce ua approach is a commonly used algorithm because it is open source and is the first algorithm aimed specifically at calibrating hydrological models duan et al 1994 the nash sutcliffe efficiency nse coefficient section s1 2 is used as a measure of agreement between the observed and simulated runoff nash and sutcliffe 1970 since there are several nse coefficient intervals to describe the model performance we use the general performance ratings table s1 recommended by moriasi et al 2007 the discharge processes of the upstream and point source boundary of the mike11 hydrodynamic model at the site of interest are unavailable they can be defined as ungauged basins sivapalan et al 2003 schreider et al 2002 assumed that the streamflow contribution from each ungauged subbasin to the total basin is proportional to the ratio of the basin s ti the ti values of each subbasin are computed based on a digital elevation model dem fig 2 jeziorska and niedzielski 2018 found that topmodel based on floods with single peaks and high return periods perform better most of flood control standard of the dike in the study area is under 5 years or above 20 years according to the result of pearson type iii distribution frequency and streamflow simulation rainfall events took place on 7 may 2010 t 20 years and 15 may 2013 t 5 years are taken as the input datasets to the hydrologic model fig s1 the rainfall and flood event are assumed the same return periods the precipitation ti values and the evaporation data as input datasets to the calibrated topmodel to simulate the discharge process of the ungauged basins for t 5 years and t 20 years 2 2 flash flood inundation simulation the hydrodynamic model mikeflood simulates the 1d flow in river channels mike11 and the 2d flow mike21 in floodplains patro et al 2009 the 1d models fail to provide information on the flow field whereas the 2d models require substantial computer time hence attempts have been made to couple the 1d river flow models with the 2d floodplain flow models viero et al 2014 the coupled 1d 2d models offer great advantages for real time simulations of flood events as shown in fig 3 this paper selects the outlet discharge processes of the gangxing gx xiaping xp and shunlian sl subbasins as the upstream boundary and the longxishui lxs longnanhe lnh and wudongshui wds subbasins as the point source boundary the main features of the ungauged subbasins are shown in table s3 the downstream boundary is the dmx according to the observed water levels the domain of the mike21 model is described by a mesh consisting of approximately 8583 nodes and 15 090 triangular elements and the area of the triangular element ranges from 548 to 25 123 m2 mikeflood computes the appropriate time step based on the courant number stability criterion león et al 1990 it is difficult to assign reliable initial conditions to the model for a mountainous river like the pajiang river thus a hot start method dhi 2017 is used by calculating the inflow time series of the first 24 h with a time step of 0 1 s and the results of the end time are assigned to the initial conditions based on the discharge process of the simulated hydrograph of upstream and point source boundary the mike11 model is coupled with topmodel and the mikeflood couples the mike11 model with mike21 model using a lateral link method dhi 2012 hence a coupled hydrologic and hydrodynamic model named tpmf is created the roughness coefficients are adjusted to bridge the gap between the observed and simulated water levels at cross sections where the elevation of the historic flood stage is available there are no common performance indices to assess the performance of the tpmf model patro et al 2009 used performance indices like coefficient of determination r2 percentage deviation in peak dev to assess the prediction performance of the mikeflood mai and de smedt 2017 applied nse to assess the prediction performance of the hec ras model this study applies flood stage error root mean square error rmse dev and bias section s1 2 to measure the agreement between the observed and simulated water levels the simulated flood inundation is validated by comparing the calculated and field measured maximum floodplain inundation 2 3 risk assessment of flash flood 2 3 1 hazard assessment ahp is a structured technique used for analyzing complex parameter problems it defines the weight of each parameter after they are ranked by expert knowledge according to their relative importance kazakis et al 2015 the results of the flood routing and propagation process simulated by the tpmf model are the maximum water depth and flood duration they are considered as the most important flood parameters and selected as the flood hazard indicators moel et al 2009 patro et al 2009 the selected parameters are not complex thus the maximum water depth and flood duration are multiplied by the same weight factor empirically to map the flood hazard distribution for t 5 years and t 20 years section s1 3 2 3 2 vulnerability and risk assessment the flood loss estimation depends on the indicators that are affected by the flash flood cultural damage ecological damage and indirect damage e g due to business disruption are still very difficult to quantify moel et al 2009 this paper tries to compute the loss rate of the vulnerability indicators such as cultivated land construction and population for different maximum water depths and durations these vulnerability indicators are multiplied by the same weight factor empirically to map the flood vulnerability distribution for t 5 years and t 20 years section s1 4 the flash flood assessment is a comprehensive of information on the flood hazard and the consequences of a flood apel et al 2008 maskrey 1989 believed that risk can be defined as risk hazard vulnerability wu et al 2015 have applied the equation to assess the flood risk in the huaihe river basin china and the result is reliable based on the tpmf integrated model a detailed flood risk assessment is conducted by maskrey s definition section s1 5 2 3 3 sensitivity analysis a geographical sensitivity analysis of an overlay based suitability analysis can indicate which parameter is the most determining the values of the output map lodwick et al 1990 in particular the problem related to the influence of each single parameter on the final vulnerability value is subject of research by other authors kazakis et al 2015 napolitano and fabbri 1996 sensitivity analysis in this paper applies the map removal sensitivity measure defined by lodwick et al 1990 it represents the sensitivity associated with removing one or more indicators from a suitability analysis section s1 6 3 study area and data in this study we conduct a case study in the upper pajiang river basin pjrb this basin which has a drainage area of about 468 km2 and drains at the damiaoxia gauge station dmx is located in fogang county qingyuan city china fig 2 the pajiang river runs from its origin dongtian candle to the county border and eventually meets the beijiang river in qingxin county the estimated annual precipitation ranges from 2000 to 2200 mm due to the summer monsoon the precipitation is unevenly distributed throughout the year and more than 70 of the annual precipitation falls from april to september as a mountainous river the mean slope of the upper pajiang river is 4 82 and the maximum hydrologic distance between the outlet and the basin divide is 41 6 km the slope drops greatly and the river drainage may not have the ability to contain outflow after stormwater events in the flood season as a result flash floods with large peak flows and short durations occur frequently in this area basin wide floods which occurred in 1988 1993 and 2013 were the most destructive disaster events in the historical record the flood event of 15 may 2013 with a return period above 20 years caused 11 deaths and a direct economic loss of 1 17 billion yuan the selected hydrological model domain contains the upper pjrb and the selected hydrodynamic modeling domain with an area of 91 87 km2 is located in the proximity of the mainstream where the elevation of the floodplain is less than 200 m fig 2 hourly streamflow and climatic data of thirty flood events from 1971 to 2013 are used 25 events are used for calibration and the remainder is utilized to verify the developed model 4 results 4 1 streamflow simulation in ungauged basins based on topmodel in the calibration periods the nse coefficients of the 25 flood events range from 0 71 to 0 96 and the mean nse is 0 85 in the validation periods the nse coefficients of the five flood events range from 0 72 to 0 93 and the mean nse is 0 86 we use 20 of the flood peak discharge as the cutoff for the occurrence of flood events the flood peak discharge of three flood events is out of the threshold in the calibration period and no flood events in the validation period in the calibration and validation periods generally the simulated flood peak discharge values are lower than the observed ones the calibrated parameter values are shown in table s4 the nse coefficients of the different return periods are compared to evaluate the influence of floods with different return periods on the model performance it is observed that the higher the return periods the better the model performance is to evaluate the influence of floods with different peak types on the model performance we compare the simulated and observed discharge values of the major flood events with different return periods and peak types fig 4 overall the performance of the topmodel based on floods with single or multiple peaks is very good and floods with single peaks and high return periods perform better the results of the model performance indicate that the topmodel is suitable for runoff simulations the precipitation for t 20 years and t 5 years is independently selected as input data to the calibrated topmodel and the discharge processes of the previous six ungauged basins are simulated fig 5 shows that in the same subbasin the discharge processes differ for different return periods the higher the return period the heavier the peak discharge is in the same return period the discharge processes differ for different drainage areas as well the larger the drainage area the heavier the peak discharge is the peak discharge increases with increasing drainage area the results illustrate that the discharge processes derived from the topmodel are in close agreement with the actual hydrological processes 4 2 flash flood inundation simulation based on tpmf model 4 2 1 performance of the tpmf model the average dev and flood stage error between the flood stage and the simulated peak water level of the main cross section are 0 55 0 73 m respectively the rmse and bias are 0 98 and 0 004 m respectively table s5 fig 6 shows a comparison of the extents of the simulated and measured floodplain inundations the field investigation has shown that the middle reach of the mainstream suffered the most severe damage during the flood events of 15 may 2013 the inundation derived from the tpmf model is in agreement with the field data the results of the water level in the channel and the floodplain inundation illustrate that the simulated inundation agrees well with the observed data therefore the tpmf model performance is satisfactory nevertheless the model performance is less effective in tributaries such as the chengjing river and yaodong river the simulated inundation extent is smaller than the measured extent 4 2 2 simulation of flood routing and propagation process in different return periods the simulated inundation maps showing the water depth and extent of inundation in different time steps are presented in fig 7 in terms of the temporal change trend the inundation area of the two historic flood events increases rapidly after the 48th hour however the area decreases between the 48th to 72nd hour and changes little after the 72nd hour in terms of the spatial change trend the vicinity of the stream is affected severely by the floods as the time step increases the inundation areas near the mainstream and tributaries increase gradually and the changes in the inundation area are more significant close to the mainstream than in the tributaries in the same time step the influences of the flood are different for different reaches the average water depth decreases gradually with increasing distance from the stream the inundation area is larger and the average water depth is deeper near the mainstream than near the tributaries at the end of the simulation the maximum water depth and duration above the threshold are computed the inundation area average water depth and average duration increase gradually with an increase in the return period the most susceptible region is in the middle reach of the upper pjr fig 8 and table s6 4 3 risk assessment based on tpmf model 4 3 1 risk assessment of flash flood the hazard and vulnerability potential are classified into the four classes of low moderate high and very high using a natural break classification the flash flood hazard decreases from northeast to southwest and with increasing distance from the stream figs s2 s3 unlike the flash flood hazard the spatial heterogeneity of the flash flood vulnerability is significant in general the flash flood vulnerability is higher in the downstream than the upstream area figs s4 s5 the flood risk maps based on the hazard and vulnerability assessment for t 5 years and t 20 years are presented in figs 9 and 10 four classes of flood risk are shown i e low moderate high and very high and the areas of the flood risk classes for different return periods are presented the threshold of the flash flood risk is lower for t 5 years than for t 20 years due to the influence of the flash flood vulnerability table s7 the spatial heterogeneity of the flash flood risk is apparent however in general the flash flood risk decreases from northeast to southwest for flash floods with t 5 years a majority of the inundations are in the high and very high classes with area ratios of 35 69 and 44 46 respectively for flash floods with t 20 years the inundations in the very high class are located in the middle to upper reaches and 86 16 of the total study area is in the high to very high flash flood risk class 4 3 2 sensitivity analysis table s8 shows the results of the sensitivity analysis the most sensitive indicator of floods is cultivated land approximately with 260 of sensitivity and 43 of variation index for both t 5 years and t 20 years the new vulnerability index is calculated without considering the parameter cultivated land population and construction respectively comparison between the new vulnerability index and the initial one shows that the cultivated land has a bigger influence in the middle down reach fig 11 the difference of sensitivity and variation index between population and construction is not distinct fig 12 the vulnerability variation index of the cultivated land increases with increasing levels of the risk classes while that of population and construction decreases for t 5 years for t 20 years the vulnerability variation index of the cultivated land decreases from high to very high risk while that of population and construction increases generally the sensitivity of flood risk to the socioeconomic indicators differs for different risk classes 5 discussions 5 1 flash flood hazard analysis the proposed tpmf model for the assessment of flash flood can be a useful tool for the mitigation of the devastating impact of floods moreover the performance of the model and sensitivity analysis can support the analysis the average dev and flood stage error between the flood stage and the simulated peak water level of the main cross section are 0 55 0 73 m respectively patro et al 2009 applied mikeflood and remote sensing data to simulate flood inundation and the average dev in peak is 0 98 bonnifait et al 2009 observed an underestimation of maximum water levels by about 1 5 to 2 m all along the stream when coupled topmodel with a 1d hydraulic model named carima nguyen et al 2016 developed hiresflood uci by coupling the hl rdhm hydrologic model with the brezo hydraulic model for flash flood modeling and the flood stage error varied from 0 19 to 0 82 m the aforementioned hydrologic and hydrodynamic models are calibrated by available flood records the hydrograph values may need to be calibrated to produce the smallest overall prediction error for different hydrological settings of different periods and subbasins but in data scarce subbasins of the pjrb it is impractical to calibrate the hydrograph of each subbasin accordingly the proposed discharge simulation technique in ungauged basins is available for providing discharge upstream conditions for the hydraulic model and the proposed tpmf model can be a valuable tool to estimate the depth and duration where flood hazard is high very high the flood inundation simulation has revealed the importance of mainstream in flood events it is necessary to be included in flood prevention plans however records of historical flood events support the importance of both mainstream and tributaries in flood events as shown in fig 6 the simulated inundation extent is smaller than the measured extent especially in tributaries compared with the mainstream the hydrograph of tributaries and rivulets are low flows and water level usually the acceptable performance of the model for extreme events contributes to a good statistical performance for the entire hydrograph but fails to match the observed hydrograph during low flows and water level this interpretation has also been supported in mai and de smedt 2017 according to mai and de smedt 2017 combining the hydrologic model wetspa and the hec ras hydraulic model predicts peak flows and water levels more accurately than other flows similarly bonnifait et al 2009 observed an underestimation of maximum water levels by about 1 5 to 2 m all along the stream when coupled topmodel with a 1d hydraulic model named carima and this result may be partly explained by the underestimation of the hydrological simulations also jeziorska and niedzielski 2018 found that the topmodel seems to provide a superior fit to the large peaks rather than to other hydrological situations mignot et al 2006 found that topographical irregularities and storage areas have a larger influence on the flow for less extreme flood using 2d shallow water equations in this paper the hydrograph in the ungauged subbasins is simulated by precipitation and ti values the precipitation data are spatially averaged which may result in the attenuation of the peak precipitation as shown in fig 3 60 of the ti values are in the range of 4 6 implying that low ti values comprise the majority of total ti values and there are few saturated source areas lin et al 2010 low peak precipitation and ti values may be the most likely reasons for underestimating peak discharges although this discrepancy occurs in simulation for low flows its impact on the overall efficiency measure is rather small jeziorska and niedzielski 2018 5 2 flash flood vulnerability and sensitivity analysis the loss rate of each vulnerability indicator is computed based on the extent of the maximum water depth and duration figs s4 s5 illustrates the significant spatial heterogeneity of the flash flood vulnerability this is consistent with the randomness non identity and fuzziness of the spatial distribution of the flood losses the raster of the flood vulnerability distribution is obtained by overlaying the indicators with the same weight nevertheless the inherent socio economic attributes of the population construction and cultivated land the ability to handle flood disasters is variable roy and blaschke 2015 concluded that a linear combination method with the same weight may not represent the large variations of the indicators and may have resulted in an adverse effect on the accuracy of the flood vulnerability assessment the most sensitive indicator of flood vulnerability assessment is cultivated land for both t 5 years and t 20 years but the sensitivity of vulnerability to the population and construction increase from high to very high risk glas et al 2016 found that the location of the buildings provided the best result when calculating the total damage whereas the impact of the crops was very small fig s6 shows that the cultivated land is an important part of the land use type in the pjrb and mainly located on either side of the river in low lying and flood prone areas when a flood with a return period greater than 20 years occurs the construction areas and the population start to be affected construction and population have greater socio economic value than cultivated land consequently the sensitivity of vulnerability to the population and construction increases in summary the main advantage of the proposed tpmf model is its ability to provide overall assessment of flash flood risk although it is less accuracy to estimate the low flows in tributaries this approach mitigates the critical problem of the lack of socioeconomic data and the uncertainty in damage estimation in the detailed risk assessment of flash floods 6 conclusions in this study we couple topmodel and mikeflood to simulate the floodplain inundation of a mountainous river basin in south china further risk assessment and sensitivity analysis of flash floods are conducted in ungauged basins the following conclusions are drawn 1 the flood routing and propagation processes simulated by the tpmf model are closely related to the local topography the nse values are larger than 0 8 in the calibration and validation periods and the overall performance of the topmodel based on single peak or multi peak flood events is satisfactory the model performance is better for floods with single peaks and high return periods in the tpmf model the average dev and flood stage error between the flood stage and the simulated peak water level of the main cross section are 0 55 0 73 m indicating a satisfactory fit the inundation extent increases rapidly after the 48th hour however the area decreases slowly between the 48th to 72nd hour and changes little after the 72nd hour in terms of the spatial change trend the vicinity of the stream is affected severely by floods which illustrates that the flood routing and propagation processes closely agree with the actual hydrodynamic process 2 the integration of the socioeconomic data and historical flood disaster data into the tpmf model demonstrate that areas with a high risk of flash floods are concentrated in the upper reaches and on both sides of the river the spatial distribution trend of the flash flood hazard is apparent and the spatial heterogeneity of the flash flood vulnerability is significant the most sensitive indicator of flood vulnerability assessment is cultivated land in different return periods but the sensitivity of vulnerability to the population and construction increase from high to very high risk 3 the good agreement between the predicted and measured spatial patterns of the inundated areas indicates that the tpmf model exhibits good suitability for practical applications future research objectives include the integration of mike with another programming language such as dhi matlab so that the optimal parameters of mike can be calibrated automatically more detailed information on the overbank cross sections and the distribution of flood events derived from hyperspectral and high resolution data will significantly improve the hydrodynamic simulation results and the damage estimation another research emphasis is data assimilation by integrating small scale inundation extent data with large scale socio economic data to minimize the difficulties of flash flood risk assessment declaration of interest statement the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the national natural science foundation of china 51779279 51822908 supported by the national key r d program of china 2017yfc0405900 open research foundation of dynamics and the associated process control key laboratory in the pearl river estuary of ministry of water resources 2017kj12 baiqianwan project s young talents plan of special support program in guangdong province 42150001 and the fundamental research funds for the central universities 20187614031620001 are gratefully acknowledged we appreciate the constructive comments and suggestions from the editors the associate editor and the three anonymous reviewers the dem with a cell size of 30 30 m is obtained from the geospatial data cloud http www gscloud cn hourly precipitation and discharge data river network and cross section data are provided by the guangzhou branch of the guangdong provincial bureau of hydrology gzbh in gunagzhou china the evaporation data are obtained from the china meteorological data network http data cma cn author agreement we confirm that the manuscript has been read and approved by all named authors and that there are no other persons who satisfied the criteria for authorship but are not listed we further confirm that the order of authors listed in the manuscript has been approved by all of us appendix a supplementary material supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 03 002 appendix a supplementary material the following are the supplementary data to this article supplementary data 1 
6577,groundwater well can be used as a seismograph or as strainmeter since the well water level is sensitive to seismic waves and other crustal strains through mechanical coupling between rock and groundwater the use of the groundwater level in monitoring tectonic strain however is hindered by the enigmatic response discrepancy among wells different responses may be recorded by even closely clustered wells therefore understanding the factors that control the sensitivity of well aquifer systems in response to different periodic loadings is important in this paper we analyzed two closely located wells that show no response to seismic waves and respond differently to earth tides and barometric pressure we discuss these wells sensitivity of response to periodic loadings by using the spectrum barometric response function tidal response and slug test methods our results show that the different response characteristics are due to the differences in permeability and confinement of the well aquifer systems well aquifer systems with a low permeability cannot record coseismic responses even if they can record clear tidal and barometric signals in addition periodic loading signals in shallow aquifers will be easily masked by the interference from the surface for example rainfall or pumping which diminishes the signal to noise ratio of the tectonic activity thus the hydrogeological and tectonic settings are the two key factors that should be considered in choosing a suitable monitoring well location for tectonic strain monitoring keywords water level well aquifer system periodic loading tidal response barometric response 1 introduction groundwater is one of the most sensitive media responding to crustal stress and solid deformation groundwater can be regarded as a strainmeter when the well aquifer system is well confined rexin et al 1962 hydrogeologists always analyze the influence of periodic loadings on the water level to study the mechanisms of water level response to crustal strain and stress naturally the most common periodical loadings are earth tides seismic waves and atmospheric fluctuations these periodical loadings provide a convenient way to study the dynamic response mechanisms of well aquifer systems roeloffs 1996 elkhoury et al 2006 shi and wang 2013 yan et al 2016 and obtain the aquifer s hydrogeological parameters xue et al 2013 kinoshita et al 2015 manga et al 2016 shi and wang 2016 meanwhile groundwater is also used as a criterion to determine whether a monitoring well can detect a precursor according to its sensitivity to earth tides roeloffs 1988 wang and manga 2010 in open wells tapping an artesian aquifer the water level responds to pressure head disturbances caused by the dilation of the aquifer hsieh et al 1987 of particular interest are harmonic disturbances that may be caused by seismic waves or earth tides based on darcy s law and theis s equation theis 1935 the characteristics of the water level response to periodical dynamic loads were first studied by cooper et al 1965 they took an example of earthquake induced fluctuations in the water level to indicate the amplification of the water level they noted that the amplitude response was closely related with the height of the water column the radius of the well and the hydraulic parameters of the aquifer the results showed that the steady fluctuation of the water level in a well occurs at the same frequency as the harmonic pressure head disturbance in the aquifer however the amplitude of the response is generally different from that of the disturbance and there is also a shift in phase based on this finding hsieh et al 1987 analyzed the characteristics of water level responses to earth tides and established a method for estimating the transmissivity of an aquifer by using the amplitude and phase shift of the water level response similarly sun et al 2015 2018 combined a high sampling rate water level record and seismic wave data and created a method to estimate the aquifer transmissivity by water level responses to the seismic waves the response of water level in well to earth tides and seismic waves indicate that wells can be used to detect crustal strains the vertical groundwater flow between the well intake and the water table can substantially attenuate this sensitivity the attenuation of strain sensitivity as a function of frequency can be inferred from the response of water wells to atmospheric loading lai et al 2013 rojstaczer 1988 unlike seismic waves and earth tides the atmospheric fluctuations not only have fixed periods such as the same periodic components as the tides semidiurnal and diurnal waves but also have minute month and even annual periods because of the significant effects caused by temperature water level fluctuations in response to seismic waves earth tides and atmospheric loading have long been studied in wells despite the apparent sensitivity of wells to strain and a well established theory that quantitatively describes the interaction between pore pressure and elastic rock deformation biot 1941 cooper et al 1965 the use of water wells to monitor tectonic strain has been limited in extent the lack of use of water wells as strainmeters is due to some obvious problems that can be separated into two broad classes those related strictly to the bulk material properties of the rock near the well and those that are due primarily to groundwater flow rojstaczer 1988 the response characteristics of water level to periodic loadings are closely related to well aquifer system properties such as well conditions and aquifer properties meanwhile some external factors such as rainfall and pumping also affect the sensitivity response to the crustal strain therefore due to the differences between well structures depth diameter observation layer and aquifer hydrological properties permeability specific storage aquifer thickness the sensitivity of well aquifer systems responses to crustal strain will be different even if their spatial locations are adjacent in this paper we analyze the water level records of two wells located at the same station in huize china approximately 10 m apart we discuss the sensitivity of these different wells responses to periodic loadings by using tidal response barometric response function and slug test methods to study their different response characteristics and mechanisms 2 hydrogeological setting and observation huize station is located at the margin of the nagu lake basin 18 km west of huize county china at 26 52 n and 103 15 e and at an elevation of 2005 m above mean sea level the station is 8 km east of the xiaojiang fault and belongs to the southeastern portion of the sichuan yunnan rhombic block fig 1 a the major source of groundwater recharge in the nagu basin originates from the carbonated karst water in the mountainous area northeast of huize station where a moderately developed karst underground river exists fig b groundwater mainly presents as crack confined water around this station and the water abundance is weak in the loose porous layer with a single well inflow of less than 100 m3 d and an underground river flow less than 100 l s qiao et al 2005 song and li 2018 there are two observation wells in huize station one ow was drilled in december 1981 and another nw was drilled in december 2017 their observation depths and layers are completely different although they are just 10 m apart at the ground fig 1c the clay layer occurs from the surface to a depth of 34 m with a thickness of 0 15 cm brown clay aquiclude at a depth of 11 m 19 m two aquifers exist at depths from 28 42 m to 30 83 m and 34 06 m to 40 87 m respectively at depths from 34 m to 100 07 m a weathered basalt layer is present with an aquifer at depths from 66 29 m to 69 58 m which is the primary observation layer for the ow well dense basalt exists at depths from 100 07 m to 125 m which functions as a good aquiclude the nw well s main observation layer is below a depth of 125 m and its basalt developed many fissures with relatively good water yield properties the total depth of the ow well is 103 15 m and its cashing depth is 87 7 m a screen filter was installed at depths from 34 06 to 87 80 m and the observation zone is from a 34 06 m depth to the bottom the water level is between 40 50 m below the surface the nw well depth is 435 38 m and its cashing depth is 125 03 m a screen filter was installed below this depth and the observation section is also in this zone the water level depth is between 65 70 m in addition to these water level observation wells huize station also observes meteorological three elements i e air pressure temperature and precipitation water level gauges used in the two wells are swy ii type digital water level meter which was developed and produced by the institute of crustal dynamics china earthquake administration the water level sensor has a range of 0 50 m resolution of 1 mm sampling interval of 1 s precision of 0 02 and accuracy of 0 05 f s in this paper the sampling interval of water level data is 1 s for slug test and 1 min for others 3 water level response to periodic loading the dynamic water level changes are completely different between the ow and nw wells due to the differences in the well structure and observation layers between the ow and nw wells fig 2 shows the water levels of the two wells precipitation and barometric pressure records in huize obviously the ow well with shallower observation layer 66 29 69 58 m is more disturbed with a larger water level fluctuation and no apparent regularity fig 2a and the nw well with deeper observation layer 125 435 m is steady with regular changes of water level fig 2b although the water level of both wells showed a trend of rising it was less affected by the local precipitation to get a better view of periodic changes in water level and to compare them to the pressure changes the data recorded during 2017 12 29 2018 1 27 28 days were selected and the trend in observed data has been removed by the method of zero phase digital filtering gustafsson 1996 the detrended data of water level in ow well shows a vague periodic change fig 2c and recognizing the dominant frequency in the frequency domain of fig 2 d is difficult unlike the ow well the water level changes in nw well fig 2e are more sensitive to the barometric pressure fig 2g combining fig 2 f and h we can identify that the water level and barometric pressure have the same major frequency components such as diurnal wave k1 and semidiurnal wave s2 fig 2 f also shows that the water level of the nw well is sensitive to periodic loadings except for the obvious amplitudes of the k1 and s2 waves that are consistent with the barometric fluctuations there is noticeable amplitude at the m2 wave that is consistent with the earth tides frequency this shows that the nw well is also sensitive to earth tides to this end we use the regression deconvolution method rasmussen and crawford 1997 to correct the water level and the method can estimate the barometric efficiency be to form an ordinary least squares regression equation between barometric pressure b and water level w as w beb where both w and b have been detrended and the means have been subtracted to remove the intercept the barometric corrected water level can be obtained by removing the barometric influence given an estimate of the barometric efficiency we obtain the corrected or residual water level as w t beb t at each time t as shown in fig 3 a the corrected water level with be 9 22 mm hpa is close to the shape of the theoretical body strain fig 3c which was calculated by the liu s method with an elliptical and rotating model liu and li 1986 the water level of the nw well is sensitive to response to the crustal strain and the response sensitivity is on the order of 10 8 mm according to fig 3 b and d we see that in the primary frequencies of the volumetric strain for the diurnal waves k1 o1 and semidiurnal waves s2 m2 n2 the water level response also shows higher amplitude among all of the response frequencies the highest amplitude is the m2 wave from these water level analyses of the ow and nw wells we observe that due to the shallow observation layer the ow well was seriously disturbed from the earth surface and its water level contains many noise signals that mask the response to the periodic loadings e g earth tides and barometric pressure however the nw well has a deeper observation layer and its water level reflects periodic loadings and shows more clearly response to crustal strain in addition to analyzing the fixed periods responses of the water level for diurnal waves k1 o1 and semidiurnal waves s2 m2 n2 we also attempt to analyze the full frequency response characteristics of the two wells to different periodic loadings by response transfer function method rojstaczer 1988 noted that there are four imbalances in pressure potential due to the step change in atmospheric load that induce fluid flow 1 vertical air flow induced by the pressure imbalance between the earth s surface and the water table 2 vertical groundwater flow induced by the pressure potential imbalance between the water table and the confining layer 3 vertical groundwater flow induced by the pressure potential imbalance between the confining layer and the aquifer and 4 lateral groundwater flow induced by the pressure potential imbalance between the open water well and the aquifer all four imbalances induced by the step load will be established instantaneously the response of water level to barometric pressure is described by 1 b e ω x 0 ρ g a p 0 a s 0 ρ g a where be is the barometric efficiency related to the loading frequency ω as shown in fig 4 a and x 0 denote the amplitude of the atmospheric load and the water level fluctuation in the well p 0 is the pore pressure of the aquifer and s 0 is the drawdown at the well ρ is the density of water 1000 kg m3 and g is the acceleration of gravity 9 81 m s2 eq 1 defined as model a in this paper describes the ratio of the amplitude of the water level fluctuation to the amplitude of the atmospheric load regarding equivalent water level the aquifer is pressurized instantaneously via grain to grain contact due to the change in surface load and the pressure is changed by the loading efficiency γ in aquifer which incorporates the influence of horizontal deformation the deeper the aquifer the smaller the value of γ rojstaczer 1988 under conditions where the confining layer has zero permeability and the aquifer transmissivity is high p 0 would be equal to aγ and the barometric efficiency be can be described by 2 b e ω a γ 1 s 0 ρ g a we defined eq 2 as model b however under conditions where the confining layer has a finite permeability and the aquifer transmissivity is low the barometric efficiency will be a strong function of frequency in the low frequency band due to the four aforementioned imbalances the response is distinguished by an increasing attenuation with the decreasing frequency in the high frequency band the well is isolated from water table and unsaturated zone influences as a result aquifer pressure p 0 is a constant and the dimensionless frequency is effectively infinite the barometric efficiency of the response can be described by model b because water table influences are negligible the solution given by model b is nearly identical to the solution given by cooper et al 1965 and hsieh et al 1987 for the steady state response of a well that taps a confined aquifer to the periodic deformation at frequencies where inertial effects are insignificant rojstaczer 1988 used three dimensionless frequency parameters rω qω and wω corresponding to the pressure transfer degree in aeration zone partial confining layer and aquifer respectively for model a and b rω is assumed to be much less than qw rω qω 10 4 while plotting the theoretical be ω curve we just assigned the values of the storage coefficient s loading efficiency γ and qω wω or qω for model a and b respectively to compare the water level response with the theoretical curve we use the transfer function to determine the actual be ω curve rojstaczer 1988 3 bb b t t b t t hb h t bw t w where bb and tt denote the power spectra of the barometric pressure and earth tides respectively bt and tb denote the cross spectrum and complex conjugate of the cross spectrum respectively between the barometric pressure and earth tides bw and tw denote the cross spectra between barometric pressure and earth tides respectively and hb and ht denote the transfer function between the water level and barometric pressure and the water level and earth tides respectively based on the recorded database and the method of transfer function we can obtain the actual response be ω curve of the target well which can be fitted with the theoretically characteristic curve after this fitting not only can we identify the characteristics of the aquifer system but the permeability of the aquifer can also be obtained with appropriate basic parameters fig 5 shows the calculated transfer function of the responses of wells ow and nw to barometric and theoretical volumetric tide loading during 2018 1 1 2018 6 31 fig 5 a shows that the transfer function values of the ow well to the barometric pressure and the earth tides are very discrete and there is no discernible regularity thus indicating that the well s sensitivity to them is weak in comparison as shown in fig 5b the nw well is more sensitive in response to the barometric pressure and the earth tides and the barometric pressure transfer function is in complete agreement with the theoretical model in the low frequency band less than 100 cpd the actual value agrees with model a with qω wω 104 γ 0 05 s 5 10 4 while in the high frequency band greater than 100 cpd the actual value is approximately the same as model b with qω 10 γ 0 05 s 5 10 4 but with fluctuations this phenomenon may be due to the exchange of water between the horizontal well and the aquifer during high frequency atmospheric loading as well as the partial vertical flow of the aquifer overall the response characteristics to periodic loadings for the ow and nw wells are entirely different due to the differences of their well aquifer systems the ow well is significantly affected by the shallow interference perhaps rainfall river discharge or pumping and the entry of the interference signal masked the well water level response to the crustal strain in contrast the nw well is almost unaffected by the shallow disturbance and the dynamic changes of the water level are wholly controlled by the loading of the barometric pressure and the earth tides indicating that the nw well s water level response sensitivity to the crustal strain is very high 4 hydraulic parameters of aquifer in the absence of external disturbances such as rainfall and pumping the sensitivity of the water level s response to the crustal strain is closely related to the hydrological parameters of the well aquifer system cooper et al 1965 rojstaczer 1988 especially the permeability hydraulic conductivity or transmissivity we estimate the hydrological parameters for the ow and nw wells 4 1 estimating aquifer transmissivity by slug test slug test is a relatively convenient and rapid field method to obtain hydrological parameters the permeability of an aquifer near a well can be estimated from the rate of rise drop of the water level in the well after a specific volume of water is suddenly removed added bouwer 1989 bouwer and rice 1976 matsumoto and shigematsu 2018 the change in water level in a well of a finite diameter after a known volume of water is suddenly injected or withdrawn obeys the following rules cooper et al 1967 4 h h 0 8 α π 2 0 e β u 2 α d u u δ u δ u u j 0 u 2 α j 1 u 2 u y 0 u 2 α y 1 u 2 β t t r c 2 α r w 2 s r c 2 where rc is the casing radius rw is the horizontal distance between the well center and original aquifer s is storage coefficient t is transmissivity h 0 and h are the water head at times 0 and t j 0 and j 1 are the zero order and first order bessel function of first kind respectively and y 0 and y 1 are the zero order and first order bessel function of second kind respectively we applied this method twice to the ow and nw wells once rapidly injecting approximately 50 l water into the wells and once instantaneously withdrawing approximately 1 l water from the wells the transmissivity is then estimated from the water level recovery fig 6 based on the characteristics of these two wells rc and rw are set to 54 mm and 84 mm respectively for the ow well and 54 mm and 64 mm respectively for the nw well because the storage coefficient has a slight influence on the water level recovery of the slug test when the order of magnitude is determined cooper et al 1967 the aquifer storage coefficient of these wells is taken as 10 4 refer to the calculated value hereafter as shown in fig 6 using the water level recovery record of the injection test to fit with the type curve based on the eq 4 we obtain the well aquifer transmissivity as 0 28 m2 d and 1 28 m2 d for the ow and nw wells respectively meanwhile after using the record of the withdrawal test the transmissivity of the aquifers in the ow and nw wells are 0 20 m2 d and 1 41 m2 d respectively hence the aquifer transmissivity of the nw well is higher than that of the ow well but these values are still small compared with other common aquifers 4 2 estimating aquifer properties from tidal response fig 3 shows that the water level of the nw well is sensitive in response to the earth tides hydrological parameters could also be estimated from the water level response to the earth tides bredehoeft 1967 hsieh et al 1987 due to the non apparent tidal response in the ow well we only estimated aquifer parameters from the nw well 4 2 1 estimating aquifer specific storage ss the relationship among the specific storage ss the second tidal potential w 2 and the hydraulic head h are as follows bredehoeft 1967 5 s s 1 2 v 1 v 2 h 6 l ag d w 2 dh where v is poisson s ratio h and l are terms of love numbers at the surface a is the mean earth radius and g is gravity acceleration using analysis methods by marine 1975 the values calculated from the square brackets in eq 5 will always be a constant for general aquifer materials or approximately 7 88 10 9s2 m2 v 0 27 h 0 6 l 0 07 a 6 371 10 6m g 9 8 m s2 the amplitude of w 2 is a function of the tidal component period τ and latitude θ which is denoted as a2 τ θ the distinct hydraulic head caused by different periodic components is taken as ah τ thus rhoads and robinson 1979 6 s s 7 88 10 9 s 2 m 2 a 2 τ θ a h τ a 2 τ θ g k m b f θ where km is a constant which is related to the earth lunar mass the distance between the earth and moon and the earth radius is approximately equal to 0 537 m doodson and warburg 1941 b is a constant related to the tidal wave component period f θ is a function of latitude θ for the m2 wave b 0 908 f θ 0 5cos2 θ ah τ can be obtained by the earth tides harmonic analysis the estimated specific storage of nw well aquifer system is ranged from 1 6 to 1 8 m 1 as shown in table 1 4 2 2 estimating aquifer transmissivity t hsieh et al 1987 derived a quantitative relationship among the response ratio and phase shift as well as the properties of the well aquifer system by the pressure disturbed model of a confined well aquifer 7 λ e 2 f 2 1 2 η arctan f e where λ is the response ratio η is phase shift e f are respectively as 8 e 1 ω r c 2 2 t k e i α w f ω r c 2 2 t k e r α w α w ω s t 1 2 r w where rw is the unscreened open well radius ω is the tidal frequency ker and kei are the real part and imaginary part of zero order kelvin functions respectively s is the aquifer storage coefficient which is equal to the specific storage multiplied by the aquifer equivalent thickness hence based on the tidal response characteristics of the well water level the response amplitude and phase shift of the fixed periodic waves are obtained by the harmonic analysis method venedikov et al 2003 and the specific storage and transmissivity of the target well aquifer system can be determined since the nw well has the most apparent response to the m2 wave we used the m2 wave s information to estimate the aquifer properties barometric components in the water level of the nw well are removed through the regression deconvolution method rasmussen and crawford 1997 we then estimated parameters by using the whole data 2018 1 1 2018 7 31 and monthly data respectively the amplitude response is used to estimate the specific storage and the phase shift is used to estimate the transmissivity we estimated the transmissivity by assigning the aquifer thickness d to 310 m and using the values of specific storage obtained previously where s ssd as shown in table 1 the transmissivity is range from 0 12 to 0 44 m2 d we compare the aquifer transmissivity calculated from the slug test with the well response to tides and find that the former is slightly higher than the latter this finding may be related to the spatial extent of water flow between well and aquifer which means the result merely reflects the transmissivity of the nearby well from the slug test while presenting more widely in aquifer since the response of the earth tides 5 discussion in a closed well the water pressure changes are negative proportional to strain in the radial direction and volumetric strain and are proportional to vertical strain kitagawa et al 2011 although the water level of the nw well is susceptible to the crustal strain finding any coseismic changes water level oscillation caused by far field earthquakes since the observation is difficult this is also true for the ow well the phenomenon can be explained by the wellbore storage effects which is the term used to describe a lag of piezometer water level behind aquifer pressure resulting from the need for water to flow into the borehole in order to equilibrate water level with aquifer pressure wellbore storage effects increase as the transmissivity of the aquifer decreases roeloffs 1996 so the reason why the nw well has no coseismic response may be related to the lesser permeability of the aquifer in order to explain it quantitatively we estimated the amplification factor λ for the effect of well water level on periodic loadings by using the model created by cooper et al 1965 9 λ 1 π r w 2 t τ k e i α w 4 π 2 h e τ 2 g 2 π r w 2 t τ k e r α w 2 1 2 where τ is the period ω is the frequency and h e is the effective height of water he hw 3d 8 cooper et al 1965 in which d is the aquifer thickness and hw is the height of water column in well casing the order of magnitude ranges from 10 1 to 100 m2 d for the transmissivity of nw well estimated from the slug test and the well response to tides fig 7 a read from fig 1c we assigned d 310 m and hw 56 m to calculate the amplification factor of the nw well for different periodic loadings according to formula 9 the results were shown in fig 7b in the range of the seismic waves period range several seconds to tens of seconds due to the slight transmissivity the amplification factor of the nw well on the pore pressure caused by the seismic wave is very small at only 10 2 10 3 times and can be ignored however in the range of the earth tides period hours to one day the amplification factor is close to 1 therefore the nw well can record changes in earth tides but no coseismic fluctuations are found the main reason is that the aquifer has a weak permeability as does the ow well consequently the permeability of the aquifer system determines the degree of response of the well water level to the periodic loadings which is the critical parameter affecting the sensitivity of the response to the crustal strain whereas in a well aquifer system whether there is external interference affecting this crucial parameter and thus playing a decisive role should be considered though the aquifer transmissivity of the ow and nw wells have a similar order of the magnitude their responses of the well water level to the periodic loadings is entirely different the nw well is sensitive but the ow well has almost no response the principal reason is that the observation aquifer of the ow well is shallow and is affected too much by the surface water or human activities perhaps rainfall river discharge or pumping interferes with the normal water level dynamics after this interference information is superimposed on the well water level the well water level changes caused by the weak crustal strain will be masked the hydrogen and oxygen isotope data of ow well and nw well also showed different features fig 8 δ2h and δ18o are relatively depleted in the nw well but enriched in the ow moreover river water and residential well water show similar isotope values and the ow well falling much more close to the surface water than the nw well such differences may indicate that the different origin of these water the most depleted δ2h and δ18o in nw well indicate that it may recharge from much higher altitude than the ow well the isotope value of ow well is close to the surface water river water and residential well water indicating it may receive partially recharge from the local surface water thus from the viewpoint of isotope data we suppose that the water level in ow well is affected by the surface water however the disturbance signal in the water level does not always exist and it may weaken or disappear during some periods when the interference signal disappears the response of the water level to the crustal strain becomes clearer as shown in fig 9 we used the relatively stable time period 2017 6 15 2017 9 15 to calculate the ow well water level response to the barometric pressure and earth tides the trend in observed data fig 9a has been removed by the method of zero phase digital filtering with all zero filter gustafsson 1996 and the filter order was set as 1440 the primary frequencies of the detrended water level for the diurnal wave k1 and semidiurnal waves m2 s2 has been clearly displayed fig 9b compared with fig 5a the response characteristics to the periodic loadings are obviously improved during this period as shown in fig 9c the barometric response function especially shows the shallow aquifer habits and closes to the type curve of model a with qω wω 104 γ 0 55 s 5 10 4 but it has some differences compared with the deeper aquifer type curve of the nw well with γ 0 05 therefore from the perspective of observing the crustal strain the nw well is substantially more representative than the ow well because the observation aquifer of the nw well is deeper and more sensitive to periodic loadings additionally from the barometric pressure response transfer function of the nw well and ow well water level in figs 5 and 9 there is not only an existing water flow between the well and the aquifer but they also have a vertical water exchange in the aquifer both are consistent with model a which presents the water flow and pressure transfer between the confining layer and the aquifer we used the purely confined model proposed by cooper et al 1967 to estimate the aquifer transmissivity in the slug test which assumes that the water flow is exchanged only between the well and the aquifer if there is a vertical water flow the model created by bouwer 1989 can be used the relationship between water recovery corresponding to time is 10 t d r c 2 ln r e r w 2 l 1 t ln y 0 y t where d is the vertical distance between the lower confining bed and the water table l is the length of screen or open section of the well y 0 and yt are the vertical distances between the water table at equilibrium and the water level in well at times 0 and t respectively and r e is the effective radius defined by bouwer and rice 1976 as 11 ln r e r w 1 1 ln h v r w c l r w 1 where hv is the vertical distance between the bottom of well and the water table and c is a dimensionless parameter that is a function of l rw combining eqs 10 and 11 we give appropriate parameters based on the structure of the well in fig 1c and we can estimate the transmissivity of the ow and nw wells by using the water level recovery data obtained by slug test respectively fig 10 in comparing the results with the purely confined model illustrated by cooper et al 1967 our model s results show that the transmissivity of the nw well is smaller while the ow well is approximately the same tab 2 overall no matter which method is used the transmissivity of the nw and ow wells is roughly at a 10 1 100 m2 d order of magnitude see table 2 if the aquifer transmissivity is relatively low then well water level changes can be smaller than aquifer head changes and can lag behind them on the other hand if aquifer transmissivity is relatively high then the well aquifer system may have a resonant frequency near which the well water level can amplify the aquifer pressure changes by as much as three orders of magnitude roeloffs 1996 the transmissivity of ow and nw well aquifer is about 10 1 100 m2 d which directly causes the water levels has no amplification of seismic waves as shown fig 7b consequently we posit that the sensitivity of the well aquifer system response to different periodic loadings depends on its well structure and aquifer permeability the aquifer system s structure is the primary controlling factor in its sensitivity response to barometric pressure and earth tides and the permeability of the aquifer determines whether the well aquifer system can detect seismic waves 6 conclusions in this paper we analyze the response characteristics of the water level to the periodic loadings of the ow and nw wells in the same station in huize county china though the distance between these two wells is less than 10 m the structure of the well and the observation layer are entirely different we compare these two wells sensitivities to the periodic loadings by using the spectrum analysis barometric transfer function and slug test methods two wells show different response characteristics to different periodic loadings and the view of hydrogeological and tectonic settings control the response sensitivity was supported our conclusions are as follows 1 due to the discrepancies between the well structure and the observation layer the water level response to the periodic loadings will be different even at the same station correspondingly the wells will respond differently to the crustal strain 2 the water level sensitivity to the crustal strain response is closely related to the permeability of the observed aquifer as presented by roeloffs 1996 if the permeability is low a coseismic response caused by seismic waves might not be detected even if the well responds well to the barometric pressure or earth tides and the nw well in this paper obeys the theory 3 as shown in the ow well in addition to permeability if the aquifer is shallow or near the surface it will be easily disturbed by the surface water and causes some interference such that the response of the well aquifer system to the crustal strain will be masked acknowledgements the original water level and barometric pressure data supporting this article are from the monitoring center of the yunnan earthquake agency china these data were extremely important for the study and the knowledge gained we thank anonymous reviewers for constructive reviews and the helps provided by the editor this research was supported by the national key r d program of china 2017yfc1500502 the national natural science foundation of china 41502239 u1602233 41602266 the research grant from institute of crustal dynamics china earthquake administration zdj2014 05 zdj2014 13 declarations of interest none 
6577,groundwater well can be used as a seismograph or as strainmeter since the well water level is sensitive to seismic waves and other crustal strains through mechanical coupling between rock and groundwater the use of the groundwater level in monitoring tectonic strain however is hindered by the enigmatic response discrepancy among wells different responses may be recorded by even closely clustered wells therefore understanding the factors that control the sensitivity of well aquifer systems in response to different periodic loadings is important in this paper we analyzed two closely located wells that show no response to seismic waves and respond differently to earth tides and barometric pressure we discuss these wells sensitivity of response to periodic loadings by using the spectrum barometric response function tidal response and slug test methods our results show that the different response characteristics are due to the differences in permeability and confinement of the well aquifer systems well aquifer systems with a low permeability cannot record coseismic responses even if they can record clear tidal and barometric signals in addition periodic loading signals in shallow aquifers will be easily masked by the interference from the surface for example rainfall or pumping which diminishes the signal to noise ratio of the tectonic activity thus the hydrogeological and tectonic settings are the two key factors that should be considered in choosing a suitable monitoring well location for tectonic strain monitoring keywords water level well aquifer system periodic loading tidal response barometric response 1 introduction groundwater is one of the most sensitive media responding to crustal stress and solid deformation groundwater can be regarded as a strainmeter when the well aquifer system is well confined rexin et al 1962 hydrogeologists always analyze the influence of periodic loadings on the water level to study the mechanisms of water level response to crustal strain and stress naturally the most common periodical loadings are earth tides seismic waves and atmospheric fluctuations these periodical loadings provide a convenient way to study the dynamic response mechanisms of well aquifer systems roeloffs 1996 elkhoury et al 2006 shi and wang 2013 yan et al 2016 and obtain the aquifer s hydrogeological parameters xue et al 2013 kinoshita et al 2015 manga et al 2016 shi and wang 2016 meanwhile groundwater is also used as a criterion to determine whether a monitoring well can detect a precursor according to its sensitivity to earth tides roeloffs 1988 wang and manga 2010 in open wells tapping an artesian aquifer the water level responds to pressure head disturbances caused by the dilation of the aquifer hsieh et al 1987 of particular interest are harmonic disturbances that may be caused by seismic waves or earth tides based on darcy s law and theis s equation theis 1935 the characteristics of the water level response to periodical dynamic loads were first studied by cooper et al 1965 they took an example of earthquake induced fluctuations in the water level to indicate the amplification of the water level they noted that the amplitude response was closely related with the height of the water column the radius of the well and the hydraulic parameters of the aquifer the results showed that the steady fluctuation of the water level in a well occurs at the same frequency as the harmonic pressure head disturbance in the aquifer however the amplitude of the response is generally different from that of the disturbance and there is also a shift in phase based on this finding hsieh et al 1987 analyzed the characteristics of water level responses to earth tides and established a method for estimating the transmissivity of an aquifer by using the amplitude and phase shift of the water level response similarly sun et al 2015 2018 combined a high sampling rate water level record and seismic wave data and created a method to estimate the aquifer transmissivity by water level responses to the seismic waves the response of water level in well to earth tides and seismic waves indicate that wells can be used to detect crustal strains the vertical groundwater flow between the well intake and the water table can substantially attenuate this sensitivity the attenuation of strain sensitivity as a function of frequency can be inferred from the response of water wells to atmospheric loading lai et al 2013 rojstaczer 1988 unlike seismic waves and earth tides the atmospheric fluctuations not only have fixed periods such as the same periodic components as the tides semidiurnal and diurnal waves but also have minute month and even annual periods because of the significant effects caused by temperature water level fluctuations in response to seismic waves earth tides and atmospheric loading have long been studied in wells despite the apparent sensitivity of wells to strain and a well established theory that quantitatively describes the interaction between pore pressure and elastic rock deformation biot 1941 cooper et al 1965 the use of water wells to monitor tectonic strain has been limited in extent the lack of use of water wells as strainmeters is due to some obvious problems that can be separated into two broad classes those related strictly to the bulk material properties of the rock near the well and those that are due primarily to groundwater flow rojstaczer 1988 the response characteristics of water level to periodic loadings are closely related to well aquifer system properties such as well conditions and aquifer properties meanwhile some external factors such as rainfall and pumping also affect the sensitivity response to the crustal strain therefore due to the differences between well structures depth diameter observation layer and aquifer hydrological properties permeability specific storage aquifer thickness the sensitivity of well aquifer systems responses to crustal strain will be different even if their spatial locations are adjacent in this paper we analyze the water level records of two wells located at the same station in huize china approximately 10 m apart we discuss the sensitivity of these different wells responses to periodic loadings by using tidal response barometric response function and slug test methods to study their different response characteristics and mechanisms 2 hydrogeological setting and observation huize station is located at the margin of the nagu lake basin 18 km west of huize county china at 26 52 n and 103 15 e and at an elevation of 2005 m above mean sea level the station is 8 km east of the xiaojiang fault and belongs to the southeastern portion of the sichuan yunnan rhombic block fig 1 a the major source of groundwater recharge in the nagu basin originates from the carbonated karst water in the mountainous area northeast of huize station where a moderately developed karst underground river exists fig b groundwater mainly presents as crack confined water around this station and the water abundance is weak in the loose porous layer with a single well inflow of less than 100 m3 d and an underground river flow less than 100 l s qiao et al 2005 song and li 2018 there are two observation wells in huize station one ow was drilled in december 1981 and another nw was drilled in december 2017 their observation depths and layers are completely different although they are just 10 m apart at the ground fig 1c the clay layer occurs from the surface to a depth of 34 m with a thickness of 0 15 cm brown clay aquiclude at a depth of 11 m 19 m two aquifers exist at depths from 28 42 m to 30 83 m and 34 06 m to 40 87 m respectively at depths from 34 m to 100 07 m a weathered basalt layer is present with an aquifer at depths from 66 29 m to 69 58 m which is the primary observation layer for the ow well dense basalt exists at depths from 100 07 m to 125 m which functions as a good aquiclude the nw well s main observation layer is below a depth of 125 m and its basalt developed many fissures with relatively good water yield properties the total depth of the ow well is 103 15 m and its cashing depth is 87 7 m a screen filter was installed at depths from 34 06 to 87 80 m and the observation zone is from a 34 06 m depth to the bottom the water level is between 40 50 m below the surface the nw well depth is 435 38 m and its cashing depth is 125 03 m a screen filter was installed below this depth and the observation section is also in this zone the water level depth is between 65 70 m in addition to these water level observation wells huize station also observes meteorological three elements i e air pressure temperature and precipitation water level gauges used in the two wells are swy ii type digital water level meter which was developed and produced by the institute of crustal dynamics china earthquake administration the water level sensor has a range of 0 50 m resolution of 1 mm sampling interval of 1 s precision of 0 02 and accuracy of 0 05 f s in this paper the sampling interval of water level data is 1 s for slug test and 1 min for others 3 water level response to periodic loading the dynamic water level changes are completely different between the ow and nw wells due to the differences in the well structure and observation layers between the ow and nw wells fig 2 shows the water levels of the two wells precipitation and barometric pressure records in huize obviously the ow well with shallower observation layer 66 29 69 58 m is more disturbed with a larger water level fluctuation and no apparent regularity fig 2a and the nw well with deeper observation layer 125 435 m is steady with regular changes of water level fig 2b although the water level of both wells showed a trend of rising it was less affected by the local precipitation to get a better view of periodic changes in water level and to compare them to the pressure changes the data recorded during 2017 12 29 2018 1 27 28 days were selected and the trend in observed data has been removed by the method of zero phase digital filtering gustafsson 1996 the detrended data of water level in ow well shows a vague periodic change fig 2c and recognizing the dominant frequency in the frequency domain of fig 2 d is difficult unlike the ow well the water level changes in nw well fig 2e are more sensitive to the barometric pressure fig 2g combining fig 2 f and h we can identify that the water level and barometric pressure have the same major frequency components such as diurnal wave k1 and semidiurnal wave s2 fig 2 f also shows that the water level of the nw well is sensitive to periodic loadings except for the obvious amplitudes of the k1 and s2 waves that are consistent with the barometric fluctuations there is noticeable amplitude at the m2 wave that is consistent with the earth tides frequency this shows that the nw well is also sensitive to earth tides to this end we use the regression deconvolution method rasmussen and crawford 1997 to correct the water level and the method can estimate the barometric efficiency be to form an ordinary least squares regression equation between barometric pressure b and water level w as w beb where both w and b have been detrended and the means have been subtracted to remove the intercept the barometric corrected water level can be obtained by removing the barometric influence given an estimate of the barometric efficiency we obtain the corrected or residual water level as w t beb t at each time t as shown in fig 3 a the corrected water level with be 9 22 mm hpa is close to the shape of the theoretical body strain fig 3c which was calculated by the liu s method with an elliptical and rotating model liu and li 1986 the water level of the nw well is sensitive to response to the crustal strain and the response sensitivity is on the order of 10 8 mm according to fig 3 b and d we see that in the primary frequencies of the volumetric strain for the diurnal waves k1 o1 and semidiurnal waves s2 m2 n2 the water level response also shows higher amplitude among all of the response frequencies the highest amplitude is the m2 wave from these water level analyses of the ow and nw wells we observe that due to the shallow observation layer the ow well was seriously disturbed from the earth surface and its water level contains many noise signals that mask the response to the periodic loadings e g earth tides and barometric pressure however the nw well has a deeper observation layer and its water level reflects periodic loadings and shows more clearly response to crustal strain in addition to analyzing the fixed periods responses of the water level for diurnal waves k1 o1 and semidiurnal waves s2 m2 n2 we also attempt to analyze the full frequency response characteristics of the two wells to different periodic loadings by response transfer function method rojstaczer 1988 noted that there are four imbalances in pressure potential due to the step change in atmospheric load that induce fluid flow 1 vertical air flow induced by the pressure imbalance between the earth s surface and the water table 2 vertical groundwater flow induced by the pressure potential imbalance between the water table and the confining layer 3 vertical groundwater flow induced by the pressure potential imbalance between the confining layer and the aquifer and 4 lateral groundwater flow induced by the pressure potential imbalance between the open water well and the aquifer all four imbalances induced by the step load will be established instantaneously the response of water level to barometric pressure is described by 1 b e ω x 0 ρ g a p 0 a s 0 ρ g a where be is the barometric efficiency related to the loading frequency ω as shown in fig 4 a and x 0 denote the amplitude of the atmospheric load and the water level fluctuation in the well p 0 is the pore pressure of the aquifer and s 0 is the drawdown at the well ρ is the density of water 1000 kg m3 and g is the acceleration of gravity 9 81 m s2 eq 1 defined as model a in this paper describes the ratio of the amplitude of the water level fluctuation to the amplitude of the atmospheric load regarding equivalent water level the aquifer is pressurized instantaneously via grain to grain contact due to the change in surface load and the pressure is changed by the loading efficiency γ in aquifer which incorporates the influence of horizontal deformation the deeper the aquifer the smaller the value of γ rojstaczer 1988 under conditions where the confining layer has zero permeability and the aquifer transmissivity is high p 0 would be equal to aγ and the barometric efficiency be can be described by 2 b e ω a γ 1 s 0 ρ g a we defined eq 2 as model b however under conditions where the confining layer has a finite permeability and the aquifer transmissivity is low the barometric efficiency will be a strong function of frequency in the low frequency band due to the four aforementioned imbalances the response is distinguished by an increasing attenuation with the decreasing frequency in the high frequency band the well is isolated from water table and unsaturated zone influences as a result aquifer pressure p 0 is a constant and the dimensionless frequency is effectively infinite the barometric efficiency of the response can be described by model b because water table influences are negligible the solution given by model b is nearly identical to the solution given by cooper et al 1965 and hsieh et al 1987 for the steady state response of a well that taps a confined aquifer to the periodic deformation at frequencies where inertial effects are insignificant rojstaczer 1988 used three dimensionless frequency parameters rω qω and wω corresponding to the pressure transfer degree in aeration zone partial confining layer and aquifer respectively for model a and b rω is assumed to be much less than qw rω qω 10 4 while plotting the theoretical be ω curve we just assigned the values of the storage coefficient s loading efficiency γ and qω wω or qω for model a and b respectively to compare the water level response with the theoretical curve we use the transfer function to determine the actual be ω curve rojstaczer 1988 3 bb b t t b t t hb h t bw t w where bb and tt denote the power spectra of the barometric pressure and earth tides respectively bt and tb denote the cross spectrum and complex conjugate of the cross spectrum respectively between the barometric pressure and earth tides bw and tw denote the cross spectra between barometric pressure and earth tides respectively and hb and ht denote the transfer function between the water level and barometric pressure and the water level and earth tides respectively based on the recorded database and the method of transfer function we can obtain the actual response be ω curve of the target well which can be fitted with the theoretically characteristic curve after this fitting not only can we identify the characteristics of the aquifer system but the permeability of the aquifer can also be obtained with appropriate basic parameters fig 5 shows the calculated transfer function of the responses of wells ow and nw to barometric and theoretical volumetric tide loading during 2018 1 1 2018 6 31 fig 5 a shows that the transfer function values of the ow well to the barometric pressure and the earth tides are very discrete and there is no discernible regularity thus indicating that the well s sensitivity to them is weak in comparison as shown in fig 5b the nw well is more sensitive in response to the barometric pressure and the earth tides and the barometric pressure transfer function is in complete agreement with the theoretical model in the low frequency band less than 100 cpd the actual value agrees with model a with qω wω 104 γ 0 05 s 5 10 4 while in the high frequency band greater than 100 cpd the actual value is approximately the same as model b with qω 10 γ 0 05 s 5 10 4 but with fluctuations this phenomenon may be due to the exchange of water between the horizontal well and the aquifer during high frequency atmospheric loading as well as the partial vertical flow of the aquifer overall the response characteristics to periodic loadings for the ow and nw wells are entirely different due to the differences of their well aquifer systems the ow well is significantly affected by the shallow interference perhaps rainfall river discharge or pumping and the entry of the interference signal masked the well water level response to the crustal strain in contrast the nw well is almost unaffected by the shallow disturbance and the dynamic changes of the water level are wholly controlled by the loading of the barometric pressure and the earth tides indicating that the nw well s water level response sensitivity to the crustal strain is very high 4 hydraulic parameters of aquifer in the absence of external disturbances such as rainfall and pumping the sensitivity of the water level s response to the crustal strain is closely related to the hydrological parameters of the well aquifer system cooper et al 1965 rojstaczer 1988 especially the permeability hydraulic conductivity or transmissivity we estimate the hydrological parameters for the ow and nw wells 4 1 estimating aquifer transmissivity by slug test slug test is a relatively convenient and rapid field method to obtain hydrological parameters the permeability of an aquifer near a well can be estimated from the rate of rise drop of the water level in the well after a specific volume of water is suddenly removed added bouwer 1989 bouwer and rice 1976 matsumoto and shigematsu 2018 the change in water level in a well of a finite diameter after a known volume of water is suddenly injected or withdrawn obeys the following rules cooper et al 1967 4 h h 0 8 α π 2 0 e β u 2 α d u u δ u δ u u j 0 u 2 α j 1 u 2 u y 0 u 2 α y 1 u 2 β t t r c 2 α r w 2 s r c 2 where rc is the casing radius rw is the horizontal distance between the well center and original aquifer s is storage coefficient t is transmissivity h 0 and h are the water head at times 0 and t j 0 and j 1 are the zero order and first order bessel function of first kind respectively and y 0 and y 1 are the zero order and first order bessel function of second kind respectively we applied this method twice to the ow and nw wells once rapidly injecting approximately 50 l water into the wells and once instantaneously withdrawing approximately 1 l water from the wells the transmissivity is then estimated from the water level recovery fig 6 based on the characteristics of these two wells rc and rw are set to 54 mm and 84 mm respectively for the ow well and 54 mm and 64 mm respectively for the nw well because the storage coefficient has a slight influence on the water level recovery of the slug test when the order of magnitude is determined cooper et al 1967 the aquifer storage coefficient of these wells is taken as 10 4 refer to the calculated value hereafter as shown in fig 6 using the water level recovery record of the injection test to fit with the type curve based on the eq 4 we obtain the well aquifer transmissivity as 0 28 m2 d and 1 28 m2 d for the ow and nw wells respectively meanwhile after using the record of the withdrawal test the transmissivity of the aquifers in the ow and nw wells are 0 20 m2 d and 1 41 m2 d respectively hence the aquifer transmissivity of the nw well is higher than that of the ow well but these values are still small compared with other common aquifers 4 2 estimating aquifer properties from tidal response fig 3 shows that the water level of the nw well is sensitive in response to the earth tides hydrological parameters could also be estimated from the water level response to the earth tides bredehoeft 1967 hsieh et al 1987 due to the non apparent tidal response in the ow well we only estimated aquifer parameters from the nw well 4 2 1 estimating aquifer specific storage ss the relationship among the specific storage ss the second tidal potential w 2 and the hydraulic head h are as follows bredehoeft 1967 5 s s 1 2 v 1 v 2 h 6 l ag d w 2 dh where v is poisson s ratio h and l are terms of love numbers at the surface a is the mean earth radius and g is gravity acceleration using analysis methods by marine 1975 the values calculated from the square brackets in eq 5 will always be a constant for general aquifer materials or approximately 7 88 10 9s2 m2 v 0 27 h 0 6 l 0 07 a 6 371 10 6m g 9 8 m s2 the amplitude of w 2 is a function of the tidal component period τ and latitude θ which is denoted as a2 τ θ the distinct hydraulic head caused by different periodic components is taken as ah τ thus rhoads and robinson 1979 6 s s 7 88 10 9 s 2 m 2 a 2 τ θ a h τ a 2 τ θ g k m b f θ where km is a constant which is related to the earth lunar mass the distance between the earth and moon and the earth radius is approximately equal to 0 537 m doodson and warburg 1941 b is a constant related to the tidal wave component period f θ is a function of latitude θ for the m2 wave b 0 908 f θ 0 5cos2 θ ah τ can be obtained by the earth tides harmonic analysis the estimated specific storage of nw well aquifer system is ranged from 1 6 to 1 8 m 1 as shown in table 1 4 2 2 estimating aquifer transmissivity t hsieh et al 1987 derived a quantitative relationship among the response ratio and phase shift as well as the properties of the well aquifer system by the pressure disturbed model of a confined well aquifer 7 λ e 2 f 2 1 2 η arctan f e where λ is the response ratio η is phase shift e f are respectively as 8 e 1 ω r c 2 2 t k e i α w f ω r c 2 2 t k e r α w α w ω s t 1 2 r w where rw is the unscreened open well radius ω is the tidal frequency ker and kei are the real part and imaginary part of zero order kelvin functions respectively s is the aquifer storage coefficient which is equal to the specific storage multiplied by the aquifer equivalent thickness hence based on the tidal response characteristics of the well water level the response amplitude and phase shift of the fixed periodic waves are obtained by the harmonic analysis method venedikov et al 2003 and the specific storage and transmissivity of the target well aquifer system can be determined since the nw well has the most apparent response to the m2 wave we used the m2 wave s information to estimate the aquifer properties barometric components in the water level of the nw well are removed through the regression deconvolution method rasmussen and crawford 1997 we then estimated parameters by using the whole data 2018 1 1 2018 7 31 and monthly data respectively the amplitude response is used to estimate the specific storage and the phase shift is used to estimate the transmissivity we estimated the transmissivity by assigning the aquifer thickness d to 310 m and using the values of specific storage obtained previously where s ssd as shown in table 1 the transmissivity is range from 0 12 to 0 44 m2 d we compare the aquifer transmissivity calculated from the slug test with the well response to tides and find that the former is slightly higher than the latter this finding may be related to the spatial extent of water flow between well and aquifer which means the result merely reflects the transmissivity of the nearby well from the slug test while presenting more widely in aquifer since the response of the earth tides 5 discussion in a closed well the water pressure changes are negative proportional to strain in the radial direction and volumetric strain and are proportional to vertical strain kitagawa et al 2011 although the water level of the nw well is susceptible to the crustal strain finding any coseismic changes water level oscillation caused by far field earthquakes since the observation is difficult this is also true for the ow well the phenomenon can be explained by the wellbore storage effects which is the term used to describe a lag of piezometer water level behind aquifer pressure resulting from the need for water to flow into the borehole in order to equilibrate water level with aquifer pressure wellbore storage effects increase as the transmissivity of the aquifer decreases roeloffs 1996 so the reason why the nw well has no coseismic response may be related to the lesser permeability of the aquifer in order to explain it quantitatively we estimated the amplification factor λ for the effect of well water level on periodic loadings by using the model created by cooper et al 1965 9 λ 1 π r w 2 t τ k e i α w 4 π 2 h e τ 2 g 2 π r w 2 t τ k e r α w 2 1 2 where τ is the period ω is the frequency and h e is the effective height of water he hw 3d 8 cooper et al 1965 in which d is the aquifer thickness and hw is the height of water column in well casing the order of magnitude ranges from 10 1 to 100 m2 d for the transmissivity of nw well estimated from the slug test and the well response to tides fig 7 a read from fig 1c we assigned d 310 m and hw 56 m to calculate the amplification factor of the nw well for different periodic loadings according to formula 9 the results were shown in fig 7b in the range of the seismic waves period range several seconds to tens of seconds due to the slight transmissivity the amplification factor of the nw well on the pore pressure caused by the seismic wave is very small at only 10 2 10 3 times and can be ignored however in the range of the earth tides period hours to one day the amplification factor is close to 1 therefore the nw well can record changes in earth tides but no coseismic fluctuations are found the main reason is that the aquifer has a weak permeability as does the ow well consequently the permeability of the aquifer system determines the degree of response of the well water level to the periodic loadings which is the critical parameter affecting the sensitivity of the response to the crustal strain whereas in a well aquifer system whether there is external interference affecting this crucial parameter and thus playing a decisive role should be considered though the aquifer transmissivity of the ow and nw wells have a similar order of the magnitude their responses of the well water level to the periodic loadings is entirely different the nw well is sensitive but the ow well has almost no response the principal reason is that the observation aquifer of the ow well is shallow and is affected too much by the surface water or human activities perhaps rainfall river discharge or pumping interferes with the normal water level dynamics after this interference information is superimposed on the well water level the well water level changes caused by the weak crustal strain will be masked the hydrogen and oxygen isotope data of ow well and nw well also showed different features fig 8 δ2h and δ18o are relatively depleted in the nw well but enriched in the ow moreover river water and residential well water show similar isotope values and the ow well falling much more close to the surface water than the nw well such differences may indicate that the different origin of these water the most depleted δ2h and δ18o in nw well indicate that it may recharge from much higher altitude than the ow well the isotope value of ow well is close to the surface water river water and residential well water indicating it may receive partially recharge from the local surface water thus from the viewpoint of isotope data we suppose that the water level in ow well is affected by the surface water however the disturbance signal in the water level does not always exist and it may weaken or disappear during some periods when the interference signal disappears the response of the water level to the crustal strain becomes clearer as shown in fig 9 we used the relatively stable time period 2017 6 15 2017 9 15 to calculate the ow well water level response to the barometric pressure and earth tides the trend in observed data fig 9a has been removed by the method of zero phase digital filtering with all zero filter gustafsson 1996 and the filter order was set as 1440 the primary frequencies of the detrended water level for the diurnal wave k1 and semidiurnal waves m2 s2 has been clearly displayed fig 9b compared with fig 5a the response characteristics to the periodic loadings are obviously improved during this period as shown in fig 9c the barometric response function especially shows the shallow aquifer habits and closes to the type curve of model a with qω wω 104 γ 0 55 s 5 10 4 but it has some differences compared with the deeper aquifer type curve of the nw well with γ 0 05 therefore from the perspective of observing the crustal strain the nw well is substantially more representative than the ow well because the observation aquifer of the nw well is deeper and more sensitive to periodic loadings additionally from the barometric pressure response transfer function of the nw well and ow well water level in figs 5 and 9 there is not only an existing water flow between the well and the aquifer but they also have a vertical water exchange in the aquifer both are consistent with model a which presents the water flow and pressure transfer between the confining layer and the aquifer we used the purely confined model proposed by cooper et al 1967 to estimate the aquifer transmissivity in the slug test which assumes that the water flow is exchanged only between the well and the aquifer if there is a vertical water flow the model created by bouwer 1989 can be used the relationship between water recovery corresponding to time is 10 t d r c 2 ln r e r w 2 l 1 t ln y 0 y t where d is the vertical distance between the lower confining bed and the water table l is the length of screen or open section of the well y 0 and yt are the vertical distances between the water table at equilibrium and the water level in well at times 0 and t respectively and r e is the effective radius defined by bouwer and rice 1976 as 11 ln r e r w 1 1 ln h v r w c l r w 1 where hv is the vertical distance between the bottom of well and the water table and c is a dimensionless parameter that is a function of l rw combining eqs 10 and 11 we give appropriate parameters based on the structure of the well in fig 1c and we can estimate the transmissivity of the ow and nw wells by using the water level recovery data obtained by slug test respectively fig 10 in comparing the results with the purely confined model illustrated by cooper et al 1967 our model s results show that the transmissivity of the nw well is smaller while the ow well is approximately the same tab 2 overall no matter which method is used the transmissivity of the nw and ow wells is roughly at a 10 1 100 m2 d order of magnitude see table 2 if the aquifer transmissivity is relatively low then well water level changes can be smaller than aquifer head changes and can lag behind them on the other hand if aquifer transmissivity is relatively high then the well aquifer system may have a resonant frequency near which the well water level can amplify the aquifer pressure changes by as much as three orders of magnitude roeloffs 1996 the transmissivity of ow and nw well aquifer is about 10 1 100 m2 d which directly causes the water levels has no amplification of seismic waves as shown fig 7b consequently we posit that the sensitivity of the well aquifer system response to different periodic loadings depends on its well structure and aquifer permeability the aquifer system s structure is the primary controlling factor in its sensitivity response to barometric pressure and earth tides and the permeability of the aquifer determines whether the well aquifer system can detect seismic waves 6 conclusions in this paper we analyze the response characteristics of the water level to the periodic loadings of the ow and nw wells in the same station in huize county china though the distance between these two wells is less than 10 m the structure of the well and the observation layer are entirely different we compare these two wells sensitivities to the periodic loadings by using the spectrum analysis barometric transfer function and slug test methods two wells show different response characteristics to different periodic loadings and the view of hydrogeological and tectonic settings control the response sensitivity was supported our conclusions are as follows 1 due to the discrepancies between the well structure and the observation layer the water level response to the periodic loadings will be different even at the same station correspondingly the wells will respond differently to the crustal strain 2 the water level sensitivity to the crustal strain response is closely related to the permeability of the observed aquifer as presented by roeloffs 1996 if the permeability is low a coseismic response caused by seismic waves might not be detected even if the well responds well to the barometric pressure or earth tides and the nw well in this paper obeys the theory 3 as shown in the ow well in addition to permeability if the aquifer is shallow or near the surface it will be easily disturbed by the surface water and causes some interference such that the response of the well aquifer system to the crustal strain will be masked acknowledgements the original water level and barometric pressure data supporting this article are from the monitoring center of the yunnan earthquake agency china these data were extremely important for the study and the knowledge gained we thank anonymous reviewers for constructive reviews and the helps provided by the editor this research was supported by the national key r d program of china 2017yfc1500502 the national natural science foundation of china 41502239 u1602233 41602266 the research grant from institute of crustal dynamics china earthquake administration zdj2014 05 zdj2014 13 declarations of interest none 
6578,karst aquifers are geologic media that consist of enlarged solution conduits the geometry of such conduits is very difficult to delineate and its hydraulic parameters such as hydraulic conductivity k and specific storage ss are difficult to estimate hydraulic tomography ht shown to successfully map the hydraulic heterogeneity of porous and fractured rock aquifers may also be effective in karst aquifers in this study we utilize two synthetic karst aquifers with the branchwork and network conduit patterns bcp and ncp to explore the ability of ht to map conduit patterns and their hydraulic parameters prior to the ht analysis forward models are built to study groundwater flow through synthetic karst aquifers to generate data and to gain insights into flow patterns forward modeling results reveal that 1 flow rates at the outlet of the bcp model is higher than the ncp model even when the same water injection rates are applied at the wells 2 the location of the injection well has a significant impact on the flow rate at the outlet and 3 flow lines mainly follow the conduit pattern these results suggest that the accurate mapping of karst networks is critical next the ht analysis of these data is conducted using a set of six models bcp and ncp to map the heterogeneity of different conduit systems under steady state and transient flow conditions inverse modeling results reveal that the estimated k maps become more reliable as the number of injection tests and the corresponding monitoring data are increased in the ht analysis moreover the results of the bcp model is closer to the original conduit pattern in comparison to the ncp model a comparative study of three inverse models with different number of observation wells and assuming the availability of geological data has shown that a more reliable pattern of the conduits can be mapped by adding geological information even when a small number of observation wells are available overall our results confirm the feasibility of ht in detecting major conduits as well as being able to differentiate between the bcp and ncp network types keywords karst aquifers subsurface heterogeneity hydraulic tomography inverse modeling 1 introduction worldwide the importance of karst aquifers is well known and they play an important role in the water supply for industrial agricultural and drinking water purposes it has been estimated that 25 of the world s population relies on karst water resources ford and williams 2013 nevertheless the study of karst aquifers is very difficult and uncertain due to their inherent heterogeneity due to the heterogeneity in hydraulic conductivity k and specific storage ss karst aquifers consisting of enlarged solution conduits have been idealized as equivalent porous media consisting of double or triple porosities abusaada and sauter 2013 larocque et al 1999 scanlon et al 2003 although such media do not explicitly consider the location and patterns of conduits important for rapid groundwater flow and contaminant transport alternatively some researchers have treated karst aquifers as a pipe network with non darcian flow taking place mohammadi et al 2018b xu and hu 2017 although the network geometry is difficult to map and hence is quite detecting the conduit pattern and their location in a karst limestone is very challenging in applied and theoretical karst studies the superposition of geological e g lithological and structural elements environmental e g acidity and temperature of water and hydraulic factors e g groundwater velocity and flow rate result in extremely complicated processes of cave development in addition the development of karst is an evolutionary process which may be active through the geological history of a limestone from diagenesis and deep burial with mainly confined flow system to surface exposure with dominant unconfined groundwater flow klimchouk and ford 2000 although field observations and entering karst caves for mapping the conduits are the most direct methods these approaches may fail due to the existence of unknown and inaccessible conduits indirect methods such as artificial tracing green et al 2012 li 2012 mohammadi et al 2007 smart 1988 smart and ford 1986 geophysical approaches chalikakis et al 2011 mcgrath et al 2002 šumanovac and weisser 2001 and pumping pressure tests giese et al 2018 mace 1997 maréchal et al 2008 have been utilized to explore conduit locations and their organization in terms of modeling two mathematical approaches i e lumped and distributed have been developed to idealize the heterogeneity of karst aquifers and estimate the karst water resources based on the obtained data by the above direct and indirect methods lumped models attempt to highlight the role of known physical processes e g recharge storage and discharge that affect groundwater flow in karst aquifers by assuming convenient modeling elements such as reservoir s and their interactions chang et al 2017 hartmann et al 2014 jukic and denic jukic 2008 martínez santos and andreu 2010 mohammadi and shoja 2014 distributed models include approaches such as equivalent porous medium epm discrete conduit continuum dcc and stochastic continuum sc modeling approaches the epm models simplify a karst aquifer to a porous medium by assuming spatial averages for hydraulic parameters of karst aquifers abusaada and sauter 2013 ghasemizadeh et al 2015 rodríguez et al 2013 scanlon et al 2003 the dcc models handle the heterogeneity of karst aquifers by considering a network of conduits which simulate groundwater flow and solute transport in the matrix and conduits as well as their exchange liedl et al 2003 reimann et al 2014 xu et al 2015 the sc models assume that the hydraulic parameters are treated as a correlated random fields different stochastic algorithms have been utilized in generating the spatial heterogeneity of conduits and fractures borghi et al 2016 jaquet et al 2004 langevin 2003 pardo igúzquiza et al 2012 generally the available hydrodynamic data for the forward modeling of karst aquifers based on the above mentioned approaches are typically insufficient and uncertain inverse modeling techniques are widely used for the characterization of karst aquifer heterogeneity inverse methods have been used to find a set of aquifer parameters which minimize the differences between the output of forward models e g head and or flow rate data and observed data inversion techniques include a forward solution of the flow equation and an algorithm for seeking unknown parameters of the aquifer such as k and ss fischer et al 2017a yeh 1986 three categories of models have been implemented for forward and inverse modeling of karst aquifers hartmann et al 2014 teutsch and sauter 1998 worthington and soley 2017 in the first and simplest category a wide variety of inversion techniques have been used for inverse modeling of fractured or karst aquifers assuming an effective parameter approach dörfliger et al 2009 larocque et al 1999 scanlon et al 2003 fischer et al 2017a the second category includes consideration of discrete karst conduits assari and mohammadi 2017 fischer et al 2017b giese et al 2018 jaquet et al 2004 mohammadi et al 2018b reimann et al 2014 inverse models of discrete karst conduit aquifers rely on deterministic borghi et al 2016 fischer et al 2018 or stochastic collon et al 2017 le coz et al 2017 li et al 2014 techniques to generate a network of conduits in the modeling domain in the third category the fractured or karst aquifer is represented by a 2d or 3d highly heterogeneous stochastic continuum dörfliger et al 2009 illman 2014 larocque et al 1999 lavenue and de marsily 2001 scanlon et al 2003 wang et al 2016 in this study the sequential successive linear estimator ssle t c jim yeh and liu 2000 zhu and yeh 2005 a geostatistical inverse modeling approach that utilizes pumping test data known as hydraulic tomography ht is used for inverse modeling over the last two decades ht has been successfully utilized for mapping the spatial heterogeneity in k and ss of aquifers conceptually ht is analogous to geophysical tomography but relies on pumping test data even though ht has been developed for porous media it has been successfully implemented in fractured media hao et al 2008 illman et al 2009 zha et al 2015 zhao and illman 2018 and research on ht in karst has begun wang et al 2017 fischer et al 2018 2017b the feasibility and ability of ht in estimating the k and ss of fractured media has been tested at both the laboratory berg and illman 2011 sharmeen et al 2012 and field scales illman et al 2009 zha et al 2016 2015 moreover ht has been successfully implemented for the reconstruction of assumed parameters in synthetic models fischer et al 2018a hao et al 2008 ni and yeh 2008 zha et al 2018 2016 in this study for the first time we utilize ht to examine the feasibility of mapping the karst network type and its heterogeneity of two conduit patterns with synthetic karst models different from previous studies here we examine two common patterns of karst conduit branchwork and network conduit patterns bcp and ncp for designing the synthetic models as these patterns are often selected for karst aquifer modeling borghi et al 2012 collon drouaillet et al 2012 florea and wicks 2001 jouves et al 2017 mohammadi et al 2018a a set of synthetic models are used to explore the ability of ht in detecting conduit patterns and their organization the conduits are idealized as zones of high and low values of k and ss respectively differences between conduits and surrounding matrix in terms of k and ss range between three to four orders of magnitude the synthetic models bcp and ncp are designed based on the known and frequently encountered patterns of karst conduits this study is a first attempt to detect karst conduit patterns by implementing ht under steady and transient conditions we used injection instead of pumping wells as well as two known patterns of karst conduits sinkholes are idealized as injection wells in our ht study this research attempts to answer the following questions 1 is it possible to detect the conduit pattern and their heterogeneity using ht based on the variation of head and flow rate data in response to a few injection events and 2 is ht able to differentiate between bcp and ncp 2 theoretical background the sequential successive linear estimator ssle forming the backbone of ht was described in detail for steady state groundwater flow by yeh and liu 2000 and for the transient case by zhu and yeh 2005 the following general groundwater flow equation is solved zhu and yeh 2005 1 k x h q x p s s x h t subject to the following initial and boundary conditions 2 h t 0 h 0 h γ 1 h 1 a n d k x h n γ 2 q where k x is the saturated hydraulic conductivity l t 1 h denotes the total hydraulic head l s s is the specific storage l 1 q x p is the pumping injection rate l 3 t 1 x is the spatial coordinate l t is time t h 0 represents the initial hydraulic head h 1 is a constant head l at boundary γ 1 q is the specific discharge l t at neumann boundary γ 2 and n is a unit vector normal to γ 2 a two dimensional solution of the eq 1 subject to eq 2 under steady and transient conditions is used in this study yeh et al 1993 this solution produces spatial and temporal distributions of h which is used by the inverse algorithm to estimate the aquifer heterogeneity in this study the sequential successive linear estimator ssle is selected to characterize the heterogeneity of the modeling domain in terms of k and ss distributions or tomograms from now on as stochastic processes zhu and yeh 2005 the ssle algorithm is an improved version of the successive linear estimator sle previously developed by hughson and yeh 2000 yeh et al 1996 yeh and zhang 1996 the sle concept is sequentially used by the ssle algorithm to update the covariance and cross covariance between the h k and s s during the estimation process zhu and yeh 2005 the ssle approach was explained in detail by hanna and yeh 1998 yeh and liu 2000 and zhu and yeh 2005 3 methodology 3 1 synthetic models the synthetic models are selected based on the mechanism of development of bcp and ncp in a fractured limestone several geological features i e fractures and bedding planes are important in the creation of the final pattern and organization of karst conduits fractures are widely distributed in limestone terrains especially in areas with a history of active tectonics these features act as initially weak surfaces through which water preferentially flows through enlargement and widening of fractures take place due to solution processes when water flows through the fractures the orientation and pattern of the passages in limestone caves are mainly controlled by geologic structure and ambient groundwater flow based on the interpretation of over five hundred cave maps palmer 1991 introduced a classification for limestone cave patterns based on the interaction between types of dominant porosity i e inter granular bedding planes and fractures and recharge i e concentrated diffuse and hypogene the coupling of porosity types and recharge modes create more than ten known cave patterns bcp and ncp i e maze cave system are results of karst development if concentrated i e sinkhole recharge and diffuse recharge take place through fracture porosity respectively fig 1 in addition to bcp and ncp with a combined relative frequency of 74 in limestone caves other patterns e g anastomotic spongework ramiform and single passage caves with a frequency of 26 have been mapped palmer 1991 bcp is more frequent and include 57 of the studied caves palmer 1991 and 80 of known mapped caves ford and williams 2013 they typically form in karst terrains where point recharge takes place through sparse fractures that are well connected a bcp is characterized by several passages as tributaries of a river where they rarely provide a closed loop palmer 1991 generally toward the outlet of a cave system i e spring conduits in a bcp are fewer in number and larger in diameter on the other hand ncp with a 17 frequency are developed as an angular array fig 1 where closed loops are common palmer 1991 these networks tend to form in karst terrains where diffuse recharge takes place through a large number of fractures recent studies have introduced different settings leading to the development of ncp including stripe karst skoglund et al 2010 and transitioning from confined to unconfined karst settings perne et al 2014 synthetic karst networks i e bcp and ncp models are utilized for the synthetic ht studies the synthetic models include a heterogeneous media with wide range of k where high k and low ss are assigned to the developed conduits the k of the matrix and the conduit zone was assumed to be uniform with initial values of 0 01 and 5 c m s 1 respectively fig 2 initial values of 0 01 and 0 0001 cm 1 were assigned to the ss of the matrix and the conduit zones respectively in addition to the matrix and conduit zones a transitional zone with a k of 0 1 c m s 1 and ss of 0 005 cm 1 is assumed where the matrix changes to conduit since the conduit cells have the same width i e same size larger conduits are simulated by assigning two or three rows of the conduit cells general groundwater flow is assumed to take place from the left to the right side sinkholes are idealized as five injection points where focussed recharge takes place fig 2 laminar groundwater flow in both materials is assumed to take place following darcy s law in order for a reliable comparison of the bcp and ncp karst synthetic models the same conditions are assumed for karst development in the models two criteria average tortuosity and conduit fraction i e conduit density are specified to be the same for the synthetic models average tortuosity τ is computed based on the following equation ronayne 2013 3 τ 1 b i 1 b l i r i where b is the total number of paths between the sinkholes to the outlet r i and l i are the straight line and groundwater flow path distances between the i th sinkhole to the outlet conceptually tortuosity is defined as a ratio of groundwater flow path distance to straight line distance traveled by a particle between two points along a flow line a tortuosity of 1 represents the movement of water in a straight path such as through a pipe tortuosity values larger than 1 represent sinuous paths of groundwater flow although a wide range of 1 to more than 3 has been suggested for tortuosity in karst aquifers assari and mohammadi 2017 field 2002 worthington 1991 typical values of tortuosity are between 1 3 and 1 7 field 2002 morales et al 2010 the conduit fraction f c is computed using the following equation pardo iguzquiza et al 2011 4 f c a c a t 100 where a c and a t are the areas of the conduit and the total model domain respectively f c is estimated for all types of conduits theoretically f c ranges from zero for a non conduit domain to 100 if the model domain is considered to be inside a large conduit although areal coverage of the conduits in several cases has been estimated to be less than 1 worthington 2014 a wide range of up to 50 has been reported for total voids i e primary and secondary porosity of karst aquifers ford and williams 2013 assuming the presence of five sinkholes as concentrated recharge points in the synthetic models fig 2 and assigning 5 for b in eq 3 a value of 1 15 and 1 20 for τ and a value of 27 8 and 28 2 for f c are determined for the bcp and ncp models respectively table 1 accordingly the synthetic bcp and ncp models are similar in terms of the number and spatial distribution of conduits even though their organization and patterns are different 3 2 designed experiments vsaft2 available at www hwr arizona edu yeh is used as a numerical code for forward and inverse modeling in this study in order to produce the necessary data for inverse modeling a two dimensional synthetic model with dimensions of 60 cm 30 cm was assumed for both the bcp and ncp configurations fig 2 a rectangular finite element grid was considered to discretize the model domain into 1 800 elements consisting of 1 cm 1 cm grid blocks a no flow boundary was assigned to the top and bottom as well as to a portion of the right boundary fig 2 constant heads of 100 cm and 95 cm are assigned to the left and a portion of the right boundary to establish horizontal groundwater flow from the left to the right in order to simulate concentrated flow toward the right boundary i e assuming that a spring located on the right boundary discharges water from the model part of the right boundary is specified to be a constant head boundary while the remaining portion is treated to be no flow boundaries fig 2 five injection wells are placed to represent sinkholes that are assumed to recharge the karst aquifer fig 2 the sinkholes i e recharge wells are located at the conduit cells while 16 observation wells are placed in both matrix and conduit cells to monitor the corresponding groundwater level fluctuations fig 2 an observation well placed close to the right constant boundary monitors the flow rate discharging from the models fig 2 in order to evaluate the effect of recharge wells five runs of forward modeling of the synthetic models are conducted under both steady and transient flow conditions for each forward run a flow rate of 5 c m 3 s 1 is applied at one of the injection wells and the corresponding head and flow rate variation at the observation wells are obtained the same procedure is considered for both bcp and ncp synthetic models two sets of inverse models are considered the first set compares the ability of ht to map the conduits under steady and transient flow conditions the second set focuses on detecting different conduit patterns retrieving the conduit location and the reliability of the estimated conduit pattern by using different set of available forward modeling data the results are then assessed through standard metrics such as the correlation coefficient r the mean absolute error l1 and mean square error l2 which are defined as 5 r 1 n i 1 n z i μ z z i μ z 1 n i 1 n z i μ z 2 1 n i 1 n z i μ z 2 6 l 1 1 n i 1 n z i z i 7 l 2 1 n i 1 n z i z i 2 where z i and z i represent the i th reference and simulated parameters e g pressure head and flow rate μ z and μ z are the mean values of the reference and simulated parameters respectively and n is the total number of parameters 4 results and discussion 4 1 forward modeling the responses to water injection through sinkholes in terms of the fluctuation of pressure head and the flow rate under steady and transient flow conditions are obtained for the bcp and ncp models in order to study the behavior of the synthetic karst models when we have different injection wells recharging for different conduit patterns the time variation of flow rate at the outlet of the models i e spring and the map of groundwater flow direction are compared under the same hydraulic conditions figs 3 and 4 the temporal variation of flow rate at the outlet of the models when all five injection wells are active with a rate of 5 c m 3 s 1 over 45 s is presented in fig 3 initially groundwater flow is due to the constant head boundary conditions of 100 and 95 cm at left and right boundaries respectively the flow rate at the outlet at early time prior to the activation of injection wells is 0 02 c m 3 s 1 for both bcp and ncp models then a flow rate of 5 c m 3 s 1 is applied sequentially at each sinkhole injection wells s1 to s5 in fig 2 for a period of 45 s and then the rate is shut off injection times at wells 1 5 with a rate of 5 c m 3 s 1 is from 45 to 90 145 to 190 245 to 290 345 to 390 and 445 to 490 s respectively injection well 1 s1 in fig 2 is located furthest from the outlet although there is a direct conduit path between s1 and the outlet in both models the flow rate values at the outlet of bcp 0 8 c m 3 s 1 are larger than ncp 0 6 c m 3 s 1 due to the convergence of the conduit pattern generally the peak flow rate at the outlet of the bcp model is higher than the ncp model even if the same rate of injection is applied at the wells the peak flow rate in response to the activation of injection wells can be affected due to increasing of water pressure and hydraulic gradient induced by previously active injection wells therefore despite the 55 sec interval between the shutdown of an injection well and activation of the subsequent injection well the memory of hydraulic head changes caused by the previously active injection well has not disappeared and the flow rate at the outlet does not approach the background value fig 3 in addition the location of the injection well has a significant effect on the flow rate at the outlet for example the third peak in fig 3 shows the contribution of injection well s3 fig 2 in discharging groundwater flow it appears that the available flow path for conveying the recharge water from s3 to the outlet is more efficient for the bcp in comparison to the ncp model due to the observed flow rate of 1 5 and 0 8 c m 3 s 1 for the bcp and ncp models respectively under an equal injection rate of 5 c m 3 s 1 therefore it is seen that variation of flow rate at the outlet is related to the distance of injection well to the outlet as well as the pattern of connection between conduits i e high k cells this behaviour of the synthetic models is comparable to field scale karst aquifers for which the spring discharge is mainly controlled by the location of the recharge points and the degree i e type of karst development after shutting off the injection wells there is no recharge thus the flow rate decreases it takes a long time for the recharge water to drain through the conduits and discharge through the outlet the flow approaches 0 05 c m 3 s 1 after 1 200 s slightly larger than the background rate of 0 02 c m 3 s 1 from fig 3 it is evident that the flow rate approaches the background value at very large times the recession part of the flow rate curve after 600 s in fig 3 has a similar appearance for both the studied bcp and ncp models it seems that the flow is completely matrix dominated and the geometry of conduits does not have significant impact on the flow rates when the injection pulses have been switched off the similar tailing behavior of bcp and ncp is related to 1 having the same injection conditions in terms of location and injection rate 2 the approximately equal area of low k zones present in both bcp and ncp model domains and 3 the consideration for a similar high k zone close to the outlet for both bcp and ncp maps of groundwater flow showing flow rate vectors for both models are presented in fig 4 these maps are prepared based on head data at the observation wells under steady state flow conditions when all the injection wells are active at a rate of 5 c m 3 s 1 it is evident from fig 4 that flow takes primarily through the conduits i e high k zones groundwater flows at higher velocity and the flow rate in the conduit concentrates toward the outlet of the models fig 4 the bcp model shows a flow rate of more than 12 c m 3 s 1 at the main outlet channel due to the higher capacity of groundwater movement the lower flow rate of 0 5 c m 3 s 1 is computed at the conduit near the left boundary but as more water enters through the injection wells the flow rate in the conduit zone approaches a larger value approaching 12 c m 3 s 1 at the right boundary this is consistent with the hydrogeology of a karst aquifer in which the conduits become larger towards the outlet as it carries more water 4 2 inverse modeling inverse modeling is conducted with five different models to assess the ability of ht to map aquifer heterogeneity i e different conduit systems based on the activation of one or more injection wells under steady state and transient flow conditions initial estimates of the two dimensional fields of k and ss are created based on given unconditional mean variance and correlation lengths of k and ss xiang et al 2009 the exponential covariance model is adopted for the k and ss fields for both low k and high k zones i e matrix and conduit zones in fig 2 respectively for all cases the initial mean of k and ss along x and y directions are set as 0 1 c m s 1 and 0 01 cm 1 for the low k zone and 4 5 c m s 1 and 0 0001 cm 1 for the high k zone the variance of ln k and ln ss are initially set as 8 7 and 0 7 for the low k and high k zones respectively while the correlation lengths in the x and y directions are set as 15 and 5 m for the low k zone and 10 and 1 m for the high k zone injection wells are successively added to inverse models 1 to 5 for example inverse model 1 consists of a single injection well while inverse model 3 has three injection wells each injection well is activated over a different stress period the location and injection rate a rate of 5 c m 3 s 1 of the injection wells are the same for both the bcp and ncp models fig 2 the steady and transient head values from the 16 observation wells obtained by forward models of bcp and ncp are used for steady and transient inversions respectively one record of head per observation well obtained through forward simulations is utilized for steady state ht in contrast 48 records of head per observation well are available for transient ht transient forward simulations are executed over a simulation time of 720 s with time steps of 15 s results are presented as tomograms of k and ss separately for steady and transient inversions results are evaluated through scatterplots of estimated versus reference values of hydraulic parameters as well as scatterplots of pressure head and flow rate for both steady and transient ht 4 2 1 steady state hydraulic tomography ssht the effect of the number of injection wells the mean and variance of the estimated k by each of the five models from both of bcp and ncp models are presented in table s1 of the supplementary information the arithmetic mean of estimated k by various ssht models ranges from 0 05 to 0 34 c m s 1 and from 0 08 to 0 57 c m s 1 in bcp and ncp models respectively table s1 reveals that the estimated values of k by the bcp models are closer to the reference values in comparison to the ncp models assuming an equal number of activated injection wells the results of the inverse models from both the bcp and ncp cases are evaluated in terms of their k tomograms fig 5 the number of activated injection wells increases from the top to the bottom in fig 5 the estimated k tomograms become more differentiable between the bcp and ncp models as the number of activated injection wells increases however the bcp model remains closer to the reference pattern fig 2 in comparison to the ncp model in terms of the computed k values at each pixel examination of the scatterplot in fig s1 of the supplementary information shows that the estimated k values improve by increasing the number of data from injection wells the correlation coefficient computed as part of the scatterplot of k shows a slight improvement in the estimated k when the number of injection wells is increased fig s1 however differences between the estimated and observed head are negligible even with one active injection well fig s2 it can be stated that although the difference between simulated and reference head is negligible the estimated k values are still quite different from the reference values this problem arises because the inverse problem is often ill posed yeh 1986 the non uniqueness of the estimated values is a common drawback with inverse modeling the estimated values of k were selected by ssle to best match the head data at a limited number of observation wells as an ill posed problem different sets of spatial distributions of k even with unreasonable values of k i e different solution of inverse problem may be estimated given the limited number of head data carrera and neuman 1986 poeter and hill 1997 in addition the diffusive nature of head propagation may cause the small differences between the correlation coefficients of scatterplots in figs s1 and s2 of the supplementary information therefore we prefer to compare the spatial patterns of the estimated k with the reference conduit pattern instead of scatterplots 4 2 2 transient hydraulic tomography tht the effect of the number of injection wells table s2 of the supplementary information contains elementary statistics of k and ss computed for each of the five models of both bcp and ncp table s2 reveals that the bcp models yield a range of arithmetic mean k values from 0 64 to 9 34 c m s 1 and arithmetic mean ss values ranging from 0 002 to 0 003 cm 1 for the ncp models mean values of k and ss are estimated to be in the range of 0 06 to 3 1 c m s 1 and 0 0004 to 0 01 cm 1 respectively although the estimated k and ss fig s3 values for each pixel show an overestimation in comparison to the reference model for both the bcp and ncp models the k tomograms show an acceptable spatial pattern of the conduit fig 6 while the correspondence is less clear for the ss tomogram fig 7 branches of the conduits in the estimated tomograms of k and ss figs 6 and 7 can be identified by zones consisting of high k and low ss values the negative correlation between k and ss for conduits identified in this study was also observed by illman et al 2009 and sharmeen et al 2012 the conduit pattern based on the k tomogram fig 6 becomes gradually clearer and similar to the reference pattern when the number of injection wells increases in particular the conduit pattern is similar to the reference pattern when data from 4 or 5 injection wells are utilized for tht analysis especially for the bcp model fig 6 on the other hand the improvement to the ss tomogram is not as evident compared to the k tomogram fig 7 the scatterplot of the simulated versus reference k and ss fig s3 shows a large scatter and some bias the values of correlation coefficient between the estimated and reference k and ss fig s3 show little improvement when the number of active injection wells is increased in particular the correlation coefficient increases from 0 01 to 0 05 for k and 0 06 to 0 09 for ss for the bcp model similarly the correlation coefficient increases from 0 01 to 0 02 for k and 0 03 to 0 07 for ss in terms of the ncp model however the scatterplots of the estimated and reference heads fig s4 reveal a very high correlation it seems that even though the values of estimated and reference heads are similar the non uniqueness of the inverse model still causes a difference between the estimated and reference k and ss values illman et al 2010 since head data is available at only 16 observation locations not at all estimated locations for k and ss the inverse problem is ill posed therefore the estimation error of head data at available points may be minimized by numerous sets of k and ss fields carrera and neuman 1986 poeter and hill 1997 some of the estimated k and ss values may be small or large and even unreasonable in comparison to the reference values although the inverse model does not accurately estimate the k and ss values at some cells the model is still able to capture the general structure of the reference case with the spatial pattern of the estimated high k and low ss values corresponding with the reference conduit pattern figs 6 and 7 since this study was conducted to reconstruct the conduit pattern through the ht approach the results seem to be reasonable obviously improved estimates of k and ss could be obtained by increasing of the number of observation and injection wells however it should be noted that under real and practical conditions only a limited number of observation and injection wells are typically available furthermore if we are able to retrieve even a general conduit pattern in a karst aquifer through a limited number of observation wells one could utilize the tomograms and the estimation variance map to plan for additional site investigations including drilling of wells to refine the estimates using this iterative approach a more accurate map of the conduits should be identified through the addition of new data 4 3 comparison of results from ssht and tht a comparison between k tomograms of the ssht and tht assuming bcp and ncp models with 5 injection and 16 observation wells is presented in fig s5 of the supplementary information the k tomogram from tht can be a better indicator of aquifer heterogeneity due to the large numbers of head records at the observation wells in particular tht is run based on 24 temporal records of head per observation well while only one record of head per observation well is used in ssht it seems that temporal and spatial variation of head could reconstruct a more reliable image of aquifer heterogeneity i e conduit pattern major flow paths in the assumed synthetic bcp and ncp models follow the conduit patterns fig 4 therefore temporal and spatial variation of head and flow rate data at an observation point are influenced by the contributions of different flow paths however the temporal variation of head and flow rate data are not available under steady state conditions and the inverse model converges based on limited spatial records of head and flow rate generally we found that the behaviour of the inverse models under transient conditions is more acceptable than under steady state conditions in addition the bcp is more accurately mapped compared to the reference pattern in comparison to ncp fig s5 the similarity between the spatial distribution of estimated high k zones with the reference map of the conduits and higher correlation coefficient between the estimated and reference parameters i e k and water level buildup by the bcp model show a better behavior of the inverse model results for bcp in comparison to ncp it seems that since the specific pattern of heterogeneity i e assigned k field in the bcp has a specific variation along the flow direction from the left to the right boundaries i e the number of conduits is reduced meanwhile the conduits become larger in size the result of tht in mapping of heterogeneity is more accurate in comparison to the ncp inverse model without a specific spatial trend of k field 4 4 transient hydraulic tomography tht the effect of number of observation wells and geological information the results of ht have been extensively improved through the integration with other data such as from geological surveys zhao et al 2016 luo et al 2017 zhao and illman 2017 2018 zha et al 2017 flowmeter surveys zha et al 2015 2014 tracer tests illman et al 2010 zha et al 2014 zhu et al 2009 and geophysical surveys day lewis et al 2006 yeh et al 2002 in order to examine the role of geological data on the estimation of a karst conduit patterns an additional set of inverse simulations were performed these models attempt to evaluate the reliability of the designed synthetic modeling approach in reconstructing the conduit pattern we utilize tht over ssht because our earlier results reveal that the former can map karst conduit better than the latter due to the use of transient data for inversions case 1 initially assumes that the aquifer is homogeneous without any prior geological information about the development of karst case 1 includes 5 injection and 16 observation wells initial guesses for mean values of 0 1 c m s 1 and 0 001 cm 1 are set for k and ss respectively while corresponding variances of 13 8 and correlation lengths of 15 and 5 m in x and y directions respectively are set for both k and ss the inversion model vsaft2 starts with initial values of k and ss as a homogeneous material in case 1 and updates them during each iteration to find mean heterogeneous fields of k and ss conditioned to head data obtained at the 16 observation wells yeh and liu 2000 a mean value of 19 61 c m s 1 and 0 67 cm 1 in the bcp model and 17 12 c m s 1 and 0 03 cm 1 in the ncp model are estimated with tht for k and ss respectively these values are considerably different from the reference values table s3 even though the convergence criteria l1 and l2 in figs 8 and 9 and correlation coefficient between simulated and reference head are acceptable for both bcp and ncp models the estimated values of k and ss show different descriptive statistics table s3 however the estimated spatial heterogeneity reveals different conduit patterns for the bcp and ncp models figs 8 9 and s6 case 2 is treated as a two material inverse model with 5 injection and 12 observation wells we assigned a higher value of k and a lower value of ss in the cells around the injection wells and the outlet of the model as a new material type the injection wells and the outlet of the model simulate hypothetical locations of sinkholes and spring in a karst aquifer respectively generally the potential for development of karst i e high k and low ss around the location of sinkholes and springs is high in a karst aquifer therefore a value of 4 5 c m s 1 and 0 0001 cm 1 for the mean values of k and ss were assigned to these cells moreover a variance of 13 8 and correlation lengths of 15 and 5 m in x and y directions respectively are initially set for k and ss respectively the remaining cells had the same values as in case 1 the results show that the range of estimated values of k and ss have been reduced in particular a mean value of 1 38 and 7 71 c m s 1 for k and 0 4 and 0 1 cm 1 for ss are estimated by tht for the bcp and ncp models respectively meanwhile the l1 and l2 norms as well as the correlation between simulated and reference heads are acceptable in addition the k and ss tomograms present a clearer image of the reference conduit pattern especially for the bcp model figs 8 9 and s6 case 3 included two material types in which more cells were assigned as a material with higher k and lower ss for this case we assumed a pattern of perpendicular fracture set as the initial plausible pattern e g left column of fig 1 that could lead to the development of conduit patterns fig 1 the mean and variance of k and ss for the two materials in case 3 are the same as in case 2 however case 3 contains 5 injection and 8 observation wells results reveal that the estimated values of k and ss have become closer to the reference values table s3 a mean value of 0 74 c m s 1 and 0 08 cm 1 for the bcp model and 3 13 c m s 1 and 0 04 cm 1 for the ncp model are estimated for k and ss respectively furthermore the estimated spatial pattern of conduits based on the k and ss tomograms is very comparable with the reference bcp and ncp likewise the correlation between estimated and reference heads confirms this similarity figs 8 9 and s6 4 5 effect of considering geological based information the results reveal that assuming a heterogeneous material without assigning prior geological information to the models case 1 reaches an acceptable conduit pattern for bcp in comparison to ncp fig s7 in case 2 when the model treated the domain as two materials with a high k close to the injection wells the estimated conduit pattern tends to be similar to the reference bcp and ncp fig s7 it is noteworthy that bcp has been reproduced better than ncp when reducing the number of observation wells to 8 in case 3 and assuming a fracture set as prior information in the inverse model the results show that although the ncp model is more similar to the reference conduit pattern the reproduced conduit pattern by the bcp model is not very different between cases 2 and 3 fig s7 table s3 reveals that from case 1 to case 3 the mean values of k and ss reduced from 19 61 to 0 79 c m s 1 and from 0 67 to 0 004 cm 1 that are comparable to the reference values of 0 17 c m s 1 and 0 007 cm 1 generally the reliability of the estimated conduit pattern from case 1 to case 3 is improved due to superimposing geological data by assigning high k and low ss to the cells around the injection wells in case 2 or to a set of cells as a fracture set in case 3 however the number of observation wells has been reduced from 16 in case 1 to 8 in case 3 this finding suggests that despite the low density of observation wells in karst terrains considering precise geological data could extensively help in mapping complex karst terrains this finding is very important because generally spring discharge and recharge rate at the sinkholes and sinking streams i e injection wells in the synthetic models are available or it is often possible to measure in karst areas in contrast high density of observation wells is rare in karst areas 4 6 model validation in order to validate the performance of the models three new injection wells p1 p2 and p3 in fig 2 are activated in the reference models as well as the three comparative cases introduced in section 4 4 the forward models of bcp and ncp fig 2 are considered as reference models the location of the injection wells and their rate of injection are the same for the bcp and ncp models the estimated k and ss in the last iteration of the inverse models of the 3 cases figs 8 and 9 are assigned to the synthetic models for validation purposes the results of the models as simulated values are compared with the reference models as observed values in terms of flow rate at the outlet of each model and pressure head at three selected observation wells validation of the models took place under two scenarios of injection at p1 p3 1 the injection wells were sequentially activated with a rate of 5 c m 3 s 1 for a period of 45 s and 2 the injection wells were simultaneously activated with a rate of 5 c m 3 s 1 for the entire period of simulation in fig 10 pressure head and flow rate at the outlet of the reference model i e w17 are compared with three cases under sequential activation of the injection wells w17 is located close to the outlet of the synthetic models fig 2 based on the results from two specific realizations patterns of conduit i e bcp and ncp responses of the bcp to the activation of the injection wells is higher than ncp in terms of pressure head and flow rate at the outlet fig 10 maximum values of the simulated pressure head and flow rate for bcp are 96 1 cm and 1 7 c m 3 s 1 respectively while these values are 95 7 and 1 2 c m 3 s 1 for the ncp model although the injection wells are activated at different locations the ncp models show approximately the same response at the outlet in contrast the bcp models show different response to the activation of injection wells the different responses of bcp and ncp in terms of pressure head and flow rate are related to the specific pattern of connected conduits to create preferential flow paths toward the outlet in bcp in comparison to ncp the results of cases 2 and 3 are closer to the reference model for both the bcp and ncp models in fig s10 of the supplementary information simulated values of pressure head at the three observation wells are compared under continuous injection of the injection wells one of the selected observation wells w10 is located in the matrix and two w7 and w12 are placed in the conduit fig 2 visually the simulated pressure head for the bcp models is closer to the reference model in comparison to ncp at all observation points in addition the buildup of groundwater level in response to the injection wells in ncp is higher than bcp however since w10 is located in a low k zone i e matrix pressure head is higher than w7 and w12 which are located in the high k zones i e conduit in both bcp and ncp models the pressure head at the observation wells is affected by 1 the constant head boundaries at the right and left sides of the synthetic models fig 2 2 the spatial distributions of the k and ss and 3 the location of injection wells therefore the transient responses of w7 w10 and w12 are different in the bcp and ncp models in terms of the ncp model the estimated pressure head for case 2 is the closest to the reference model however all cases of bcp models estimate pressure head that are very close to the reference model fig s8 these results suggest that the tht yield significantly better results in terms of pressure head predictions for bcp than ncp predictions of flow rates are next assessed in fig s9 transient responses of flow rate in the selected observation wells are mainly affected by the values of k and ss around the location of the wells since w10 is located in the matrix consisting of low k material the flow rate is small and shows negligible temporal variation less than 0 03 c m 3 s 1 in comparison to w7 and w12 that are located within a conduit of a high k material with a maximum flow rate of 3 and 6 c m 3 s 1 respectively fig s9 although the estimation of flow rates at w7 and w12 in general reveals an underestimation relative to the reference model flow rate remains more reliable at w10 for both bcp and ncp models generally the flow rate values in the bcp models is higher than the ncp models due to the higher organization of the conduits i e high k zones toward the outlet of the model as a result improvement of cases 2 and 3 that include additional geological information in terms of the estimation of flow rate is visible the reference and simulated pressure head and flow rate data obtained through the validation exercise are compared in scatter plots figs 11 and 12 in which the 45 degree solid line indicates a perfect match the values of pressure head and flow rate at times of 10 30 80 160 and 750 s at all 17 observation wells are selected for comparison purposes best fit lines dashed of the pressure head reveals a better performance of the estimation in the three cases of bcp in comparison to ncp models in addition an overestimation of pressure head is obvious in the ncp models although the performance of the estimated pressure head based on l1 l2 and correlation coefficient is acceptable for all three cases case 2 still yields smaller values of l1 and l2 and a higher value of the correlation coefficient however the smaller values of l1 and l2 belongs to bcp for all three cases in comparison to ncp models fig 11 the improvement in the prediction of flow rates in terms of l1 and l2 is obvious from cases 1 to 3 in both bcp and ncp models l1 and l2 values decrease from 0 79 and 1 91 to 0 45 and 0 74 in bcp and from 0 94 and 2 64 to 0 64 and 1 13 in ncp models respectively meanwhile the correlation coefficient also indicates the improvement of predictions from cases 1 to 3 and increase from 0 87 to 0 91 in bcp and from 0 62 to 0 91 in ncp models respectively the lowest values of l1 and l2 and the highest value of correlation coefficient belong to case 3 for both bcp and ncp models fig 12 however scatterplots of pressure head show that the differences between l1 and l2 in the 3 cases is negligible it seems that even though the prediction performance of pressure head is acceptable for the three cases case 3 that includes more geological information results in the highest performance for estimating the flow rates for both bcp and ncp models 5 conclusions in this study the potential of ht in mapping the heterogeneity of two synthetic karst models with different yet representative patterns is investigated under steady state and transient conditions three elements of a typical karst aquifer that is the presence of sinkholes a spring and the conduit pattern i e heterogeneity are simulated through injection wells considering a small portion of no flow boundary as constant head boundary and high k and low ss zones respectively several combinations of injection and observation wells are examined to produce heterogeneity maps via ssht and tht we examined the role of karst hydrogeological knowledge such as higher potential for karst development around the location of sinkholes and spring to improve ht in mapping heterogeneity the validity of the estimated k and ss fields i e k and ss tomograms are tested by utilizing additional data from new injection wells not used in the calibration effort and comparing the results with the reference models five injection wells in the synthetic models were activated to cause temporal and spatial perturbations in head and flow rate which were recorded at several observation wells under steady and transient states these data were collected for both the bcp and ncp synthetic models k and ss tomograms were evaluated on the basis of the estimated heterogeneity or conduit pattern the results revealed the ability of ht in reproducing the conduit patterns in particular tht in bcp models show reliable results in comparison to ncp models in addition the effect of including geological data as prior information for inverse modeling in improving the accuracy of k tomograms was examined since drilling of observation wells in karst terrains is expensive this research proves that adding geological data e g data on fracture set and or knowing general trend of karst development can be very valuable and could compensate for the lack of a sufficient density of observation wells it seems that collecting complementary data from tracer and geophysical tests and utilizing these data in inverse models could potentially improve the estimation of conduit pattern since mapping the karst conduit location and its pattern is one of the most important and difficult aspect of karst studies it is envisioned that the integration of geological hydrogeological and geophysical data with ht would be beneficial in the future in order to obtain more refined maps of karst heterogeneity including improved hydraulic parameter estimates one should include both pressure head and flow rate in ht zha et al 2014 declaration of interests none acknowledgments the first author would like to acknowledge shiraz university for financial support during sabbatical leave at university of waterloo the second author acknowledges the support from natural sciences and engineering research council of canada nserc through the discovery grant which made this collaboration with the first author possible the authors thank prof franci gabrovšek and an anonymous reviewer for their constructive comments and suggestions which improved the manuscript appendix a supplementary material supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 02 044 appendix a supplementary material the following are the supplementary data to this article supplementary data 1 
6578,karst aquifers are geologic media that consist of enlarged solution conduits the geometry of such conduits is very difficult to delineate and its hydraulic parameters such as hydraulic conductivity k and specific storage ss are difficult to estimate hydraulic tomography ht shown to successfully map the hydraulic heterogeneity of porous and fractured rock aquifers may also be effective in karst aquifers in this study we utilize two synthetic karst aquifers with the branchwork and network conduit patterns bcp and ncp to explore the ability of ht to map conduit patterns and their hydraulic parameters prior to the ht analysis forward models are built to study groundwater flow through synthetic karst aquifers to generate data and to gain insights into flow patterns forward modeling results reveal that 1 flow rates at the outlet of the bcp model is higher than the ncp model even when the same water injection rates are applied at the wells 2 the location of the injection well has a significant impact on the flow rate at the outlet and 3 flow lines mainly follow the conduit pattern these results suggest that the accurate mapping of karst networks is critical next the ht analysis of these data is conducted using a set of six models bcp and ncp to map the heterogeneity of different conduit systems under steady state and transient flow conditions inverse modeling results reveal that the estimated k maps become more reliable as the number of injection tests and the corresponding monitoring data are increased in the ht analysis moreover the results of the bcp model is closer to the original conduit pattern in comparison to the ncp model a comparative study of three inverse models with different number of observation wells and assuming the availability of geological data has shown that a more reliable pattern of the conduits can be mapped by adding geological information even when a small number of observation wells are available overall our results confirm the feasibility of ht in detecting major conduits as well as being able to differentiate between the bcp and ncp network types keywords karst aquifers subsurface heterogeneity hydraulic tomography inverse modeling 1 introduction worldwide the importance of karst aquifers is well known and they play an important role in the water supply for industrial agricultural and drinking water purposes it has been estimated that 25 of the world s population relies on karst water resources ford and williams 2013 nevertheless the study of karst aquifers is very difficult and uncertain due to their inherent heterogeneity due to the heterogeneity in hydraulic conductivity k and specific storage ss karst aquifers consisting of enlarged solution conduits have been idealized as equivalent porous media consisting of double or triple porosities abusaada and sauter 2013 larocque et al 1999 scanlon et al 2003 although such media do not explicitly consider the location and patterns of conduits important for rapid groundwater flow and contaminant transport alternatively some researchers have treated karst aquifers as a pipe network with non darcian flow taking place mohammadi et al 2018b xu and hu 2017 although the network geometry is difficult to map and hence is quite detecting the conduit pattern and their location in a karst limestone is very challenging in applied and theoretical karst studies the superposition of geological e g lithological and structural elements environmental e g acidity and temperature of water and hydraulic factors e g groundwater velocity and flow rate result in extremely complicated processes of cave development in addition the development of karst is an evolutionary process which may be active through the geological history of a limestone from diagenesis and deep burial with mainly confined flow system to surface exposure with dominant unconfined groundwater flow klimchouk and ford 2000 although field observations and entering karst caves for mapping the conduits are the most direct methods these approaches may fail due to the existence of unknown and inaccessible conduits indirect methods such as artificial tracing green et al 2012 li 2012 mohammadi et al 2007 smart 1988 smart and ford 1986 geophysical approaches chalikakis et al 2011 mcgrath et al 2002 šumanovac and weisser 2001 and pumping pressure tests giese et al 2018 mace 1997 maréchal et al 2008 have been utilized to explore conduit locations and their organization in terms of modeling two mathematical approaches i e lumped and distributed have been developed to idealize the heterogeneity of karst aquifers and estimate the karst water resources based on the obtained data by the above direct and indirect methods lumped models attempt to highlight the role of known physical processes e g recharge storage and discharge that affect groundwater flow in karst aquifers by assuming convenient modeling elements such as reservoir s and their interactions chang et al 2017 hartmann et al 2014 jukic and denic jukic 2008 martínez santos and andreu 2010 mohammadi and shoja 2014 distributed models include approaches such as equivalent porous medium epm discrete conduit continuum dcc and stochastic continuum sc modeling approaches the epm models simplify a karst aquifer to a porous medium by assuming spatial averages for hydraulic parameters of karst aquifers abusaada and sauter 2013 ghasemizadeh et al 2015 rodríguez et al 2013 scanlon et al 2003 the dcc models handle the heterogeneity of karst aquifers by considering a network of conduits which simulate groundwater flow and solute transport in the matrix and conduits as well as their exchange liedl et al 2003 reimann et al 2014 xu et al 2015 the sc models assume that the hydraulic parameters are treated as a correlated random fields different stochastic algorithms have been utilized in generating the spatial heterogeneity of conduits and fractures borghi et al 2016 jaquet et al 2004 langevin 2003 pardo igúzquiza et al 2012 generally the available hydrodynamic data for the forward modeling of karst aquifers based on the above mentioned approaches are typically insufficient and uncertain inverse modeling techniques are widely used for the characterization of karst aquifer heterogeneity inverse methods have been used to find a set of aquifer parameters which minimize the differences between the output of forward models e g head and or flow rate data and observed data inversion techniques include a forward solution of the flow equation and an algorithm for seeking unknown parameters of the aquifer such as k and ss fischer et al 2017a yeh 1986 three categories of models have been implemented for forward and inverse modeling of karst aquifers hartmann et al 2014 teutsch and sauter 1998 worthington and soley 2017 in the first and simplest category a wide variety of inversion techniques have been used for inverse modeling of fractured or karst aquifers assuming an effective parameter approach dörfliger et al 2009 larocque et al 1999 scanlon et al 2003 fischer et al 2017a the second category includes consideration of discrete karst conduits assari and mohammadi 2017 fischer et al 2017b giese et al 2018 jaquet et al 2004 mohammadi et al 2018b reimann et al 2014 inverse models of discrete karst conduit aquifers rely on deterministic borghi et al 2016 fischer et al 2018 or stochastic collon et al 2017 le coz et al 2017 li et al 2014 techniques to generate a network of conduits in the modeling domain in the third category the fractured or karst aquifer is represented by a 2d or 3d highly heterogeneous stochastic continuum dörfliger et al 2009 illman 2014 larocque et al 1999 lavenue and de marsily 2001 scanlon et al 2003 wang et al 2016 in this study the sequential successive linear estimator ssle t c jim yeh and liu 2000 zhu and yeh 2005 a geostatistical inverse modeling approach that utilizes pumping test data known as hydraulic tomography ht is used for inverse modeling over the last two decades ht has been successfully utilized for mapping the spatial heterogeneity in k and ss of aquifers conceptually ht is analogous to geophysical tomography but relies on pumping test data even though ht has been developed for porous media it has been successfully implemented in fractured media hao et al 2008 illman et al 2009 zha et al 2015 zhao and illman 2018 and research on ht in karst has begun wang et al 2017 fischer et al 2018 2017b the feasibility and ability of ht in estimating the k and ss of fractured media has been tested at both the laboratory berg and illman 2011 sharmeen et al 2012 and field scales illman et al 2009 zha et al 2016 2015 moreover ht has been successfully implemented for the reconstruction of assumed parameters in synthetic models fischer et al 2018a hao et al 2008 ni and yeh 2008 zha et al 2018 2016 in this study for the first time we utilize ht to examine the feasibility of mapping the karst network type and its heterogeneity of two conduit patterns with synthetic karst models different from previous studies here we examine two common patterns of karst conduit branchwork and network conduit patterns bcp and ncp for designing the synthetic models as these patterns are often selected for karst aquifer modeling borghi et al 2012 collon drouaillet et al 2012 florea and wicks 2001 jouves et al 2017 mohammadi et al 2018a a set of synthetic models are used to explore the ability of ht in detecting conduit patterns and their organization the conduits are idealized as zones of high and low values of k and ss respectively differences between conduits and surrounding matrix in terms of k and ss range between three to four orders of magnitude the synthetic models bcp and ncp are designed based on the known and frequently encountered patterns of karst conduits this study is a first attempt to detect karst conduit patterns by implementing ht under steady and transient conditions we used injection instead of pumping wells as well as two known patterns of karst conduits sinkholes are idealized as injection wells in our ht study this research attempts to answer the following questions 1 is it possible to detect the conduit pattern and their heterogeneity using ht based on the variation of head and flow rate data in response to a few injection events and 2 is ht able to differentiate between bcp and ncp 2 theoretical background the sequential successive linear estimator ssle forming the backbone of ht was described in detail for steady state groundwater flow by yeh and liu 2000 and for the transient case by zhu and yeh 2005 the following general groundwater flow equation is solved zhu and yeh 2005 1 k x h q x p s s x h t subject to the following initial and boundary conditions 2 h t 0 h 0 h γ 1 h 1 a n d k x h n γ 2 q where k x is the saturated hydraulic conductivity l t 1 h denotes the total hydraulic head l s s is the specific storage l 1 q x p is the pumping injection rate l 3 t 1 x is the spatial coordinate l t is time t h 0 represents the initial hydraulic head h 1 is a constant head l at boundary γ 1 q is the specific discharge l t at neumann boundary γ 2 and n is a unit vector normal to γ 2 a two dimensional solution of the eq 1 subject to eq 2 under steady and transient conditions is used in this study yeh et al 1993 this solution produces spatial and temporal distributions of h which is used by the inverse algorithm to estimate the aquifer heterogeneity in this study the sequential successive linear estimator ssle is selected to characterize the heterogeneity of the modeling domain in terms of k and ss distributions or tomograms from now on as stochastic processes zhu and yeh 2005 the ssle algorithm is an improved version of the successive linear estimator sle previously developed by hughson and yeh 2000 yeh et al 1996 yeh and zhang 1996 the sle concept is sequentially used by the ssle algorithm to update the covariance and cross covariance between the h k and s s during the estimation process zhu and yeh 2005 the ssle approach was explained in detail by hanna and yeh 1998 yeh and liu 2000 and zhu and yeh 2005 3 methodology 3 1 synthetic models the synthetic models are selected based on the mechanism of development of bcp and ncp in a fractured limestone several geological features i e fractures and bedding planes are important in the creation of the final pattern and organization of karst conduits fractures are widely distributed in limestone terrains especially in areas with a history of active tectonics these features act as initially weak surfaces through which water preferentially flows through enlargement and widening of fractures take place due to solution processes when water flows through the fractures the orientation and pattern of the passages in limestone caves are mainly controlled by geologic structure and ambient groundwater flow based on the interpretation of over five hundred cave maps palmer 1991 introduced a classification for limestone cave patterns based on the interaction between types of dominant porosity i e inter granular bedding planes and fractures and recharge i e concentrated diffuse and hypogene the coupling of porosity types and recharge modes create more than ten known cave patterns bcp and ncp i e maze cave system are results of karst development if concentrated i e sinkhole recharge and diffuse recharge take place through fracture porosity respectively fig 1 in addition to bcp and ncp with a combined relative frequency of 74 in limestone caves other patterns e g anastomotic spongework ramiform and single passage caves with a frequency of 26 have been mapped palmer 1991 bcp is more frequent and include 57 of the studied caves palmer 1991 and 80 of known mapped caves ford and williams 2013 they typically form in karst terrains where point recharge takes place through sparse fractures that are well connected a bcp is characterized by several passages as tributaries of a river where they rarely provide a closed loop palmer 1991 generally toward the outlet of a cave system i e spring conduits in a bcp are fewer in number and larger in diameter on the other hand ncp with a 17 frequency are developed as an angular array fig 1 where closed loops are common palmer 1991 these networks tend to form in karst terrains where diffuse recharge takes place through a large number of fractures recent studies have introduced different settings leading to the development of ncp including stripe karst skoglund et al 2010 and transitioning from confined to unconfined karst settings perne et al 2014 synthetic karst networks i e bcp and ncp models are utilized for the synthetic ht studies the synthetic models include a heterogeneous media with wide range of k where high k and low ss are assigned to the developed conduits the k of the matrix and the conduit zone was assumed to be uniform with initial values of 0 01 and 5 c m s 1 respectively fig 2 initial values of 0 01 and 0 0001 cm 1 were assigned to the ss of the matrix and the conduit zones respectively in addition to the matrix and conduit zones a transitional zone with a k of 0 1 c m s 1 and ss of 0 005 cm 1 is assumed where the matrix changes to conduit since the conduit cells have the same width i e same size larger conduits are simulated by assigning two or three rows of the conduit cells general groundwater flow is assumed to take place from the left to the right side sinkholes are idealized as five injection points where focussed recharge takes place fig 2 laminar groundwater flow in both materials is assumed to take place following darcy s law in order for a reliable comparison of the bcp and ncp karst synthetic models the same conditions are assumed for karst development in the models two criteria average tortuosity and conduit fraction i e conduit density are specified to be the same for the synthetic models average tortuosity τ is computed based on the following equation ronayne 2013 3 τ 1 b i 1 b l i r i where b is the total number of paths between the sinkholes to the outlet r i and l i are the straight line and groundwater flow path distances between the i th sinkhole to the outlet conceptually tortuosity is defined as a ratio of groundwater flow path distance to straight line distance traveled by a particle between two points along a flow line a tortuosity of 1 represents the movement of water in a straight path such as through a pipe tortuosity values larger than 1 represent sinuous paths of groundwater flow although a wide range of 1 to more than 3 has been suggested for tortuosity in karst aquifers assari and mohammadi 2017 field 2002 worthington 1991 typical values of tortuosity are between 1 3 and 1 7 field 2002 morales et al 2010 the conduit fraction f c is computed using the following equation pardo iguzquiza et al 2011 4 f c a c a t 100 where a c and a t are the areas of the conduit and the total model domain respectively f c is estimated for all types of conduits theoretically f c ranges from zero for a non conduit domain to 100 if the model domain is considered to be inside a large conduit although areal coverage of the conduits in several cases has been estimated to be less than 1 worthington 2014 a wide range of up to 50 has been reported for total voids i e primary and secondary porosity of karst aquifers ford and williams 2013 assuming the presence of five sinkholes as concentrated recharge points in the synthetic models fig 2 and assigning 5 for b in eq 3 a value of 1 15 and 1 20 for τ and a value of 27 8 and 28 2 for f c are determined for the bcp and ncp models respectively table 1 accordingly the synthetic bcp and ncp models are similar in terms of the number and spatial distribution of conduits even though their organization and patterns are different 3 2 designed experiments vsaft2 available at www hwr arizona edu yeh is used as a numerical code for forward and inverse modeling in this study in order to produce the necessary data for inverse modeling a two dimensional synthetic model with dimensions of 60 cm 30 cm was assumed for both the bcp and ncp configurations fig 2 a rectangular finite element grid was considered to discretize the model domain into 1 800 elements consisting of 1 cm 1 cm grid blocks a no flow boundary was assigned to the top and bottom as well as to a portion of the right boundary fig 2 constant heads of 100 cm and 95 cm are assigned to the left and a portion of the right boundary to establish horizontal groundwater flow from the left to the right in order to simulate concentrated flow toward the right boundary i e assuming that a spring located on the right boundary discharges water from the model part of the right boundary is specified to be a constant head boundary while the remaining portion is treated to be no flow boundaries fig 2 five injection wells are placed to represent sinkholes that are assumed to recharge the karst aquifer fig 2 the sinkholes i e recharge wells are located at the conduit cells while 16 observation wells are placed in both matrix and conduit cells to monitor the corresponding groundwater level fluctuations fig 2 an observation well placed close to the right constant boundary monitors the flow rate discharging from the models fig 2 in order to evaluate the effect of recharge wells five runs of forward modeling of the synthetic models are conducted under both steady and transient flow conditions for each forward run a flow rate of 5 c m 3 s 1 is applied at one of the injection wells and the corresponding head and flow rate variation at the observation wells are obtained the same procedure is considered for both bcp and ncp synthetic models two sets of inverse models are considered the first set compares the ability of ht to map the conduits under steady and transient flow conditions the second set focuses on detecting different conduit patterns retrieving the conduit location and the reliability of the estimated conduit pattern by using different set of available forward modeling data the results are then assessed through standard metrics such as the correlation coefficient r the mean absolute error l1 and mean square error l2 which are defined as 5 r 1 n i 1 n z i μ z z i μ z 1 n i 1 n z i μ z 2 1 n i 1 n z i μ z 2 6 l 1 1 n i 1 n z i z i 7 l 2 1 n i 1 n z i z i 2 where z i and z i represent the i th reference and simulated parameters e g pressure head and flow rate μ z and μ z are the mean values of the reference and simulated parameters respectively and n is the total number of parameters 4 results and discussion 4 1 forward modeling the responses to water injection through sinkholes in terms of the fluctuation of pressure head and the flow rate under steady and transient flow conditions are obtained for the bcp and ncp models in order to study the behavior of the synthetic karst models when we have different injection wells recharging for different conduit patterns the time variation of flow rate at the outlet of the models i e spring and the map of groundwater flow direction are compared under the same hydraulic conditions figs 3 and 4 the temporal variation of flow rate at the outlet of the models when all five injection wells are active with a rate of 5 c m 3 s 1 over 45 s is presented in fig 3 initially groundwater flow is due to the constant head boundary conditions of 100 and 95 cm at left and right boundaries respectively the flow rate at the outlet at early time prior to the activation of injection wells is 0 02 c m 3 s 1 for both bcp and ncp models then a flow rate of 5 c m 3 s 1 is applied sequentially at each sinkhole injection wells s1 to s5 in fig 2 for a period of 45 s and then the rate is shut off injection times at wells 1 5 with a rate of 5 c m 3 s 1 is from 45 to 90 145 to 190 245 to 290 345 to 390 and 445 to 490 s respectively injection well 1 s1 in fig 2 is located furthest from the outlet although there is a direct conduit path between s1 and the outlet in both models the flow rate values at the outlet of bcp 0 8 c m 3 s 1 are larger than ncp 0 6 c m 3 s 1 due to the convergence of the conduit pattern generally the peak flow rate at the outlet of the bcp model is higher than the ncp model even if the same rate of injection is applied at the wells the peak flow rate in response to the activation of injection wells can be affected due to increasing of water pressure and hydraulic gradient induced by previously active injection wells therefore despite the 55 sec interval between the shutdown of an injection well and activation of the subsequent injection well the memory of hydraulic head changes caused by the previously active injection well has not disappeared and the flow rate at the outlet does not approach the background value fig 3 in addition the location of the injection well has a significant effect on the flow rate at the outlet for example the third peak in fig 3 shows the contribution of injection well s3 fig 2 in discharging groundwater flow it appears that the available flow path for conveying the recharge water from s3 to the outlet is more efficient for the bcp in comparison to the ncp model due to the observed flow rate of 1 5 and 0 8 c m 3 s 1 for the bcp and ncp models respectively under an equal injection rate of 5 c m 3 s 1 therefore it is seen that variation of flow rate at the outlet is related to the distance of injection well to the outlet as well as the pattern of connection between conduits i e high k cells this behaviour of the synthetic models is comparable to field scale karst aquifers for which the spring discharge is mainly controlled by the location of the recharge points and the degree i e type of karst development after shutting off the injection wells there is no recharge thus the flow rate decreases it takes a long time for the recharge water to drain through the conduits and discharge through the outlet the flow approaches 0 05 c m 3 s 1 after 1 200 s slightly larger than the background rate of 0 02 c m 3 s 1 from fig 3 it is evident that the flow rate approaches the background value at very large times the recession part of the flow rate curve after 600 s in fig 3 has a similar appearance for both the studied bcp and ncp models it seems that the flow is completely matrix dominated and the geometry of conduits does not have significant impact on the flow rates when the injection pulses have been switched off the similar tailing behavior of bcp and ncp is related to 1 having the same injection conditions in terms of location and injection rate 2 the approximately equal area of low k zones present in both bcp and ncp model domains and 3 the consideration for a similar high k zone close to the outlet for both bcp and ncp maps of groundwater flow showing flow rate vectors for both models are presented in fig 4 these maps are prepared based on head data at the observation wells under steady state flow conditions when all the injection wells are active at a rate of 5 c m 3 s 1 it is evident from fig 4 that flow takes primarily through the conduits i e high k zones groundwater flows at higher velocity and the flow rate in the conduit concentrates toward the outlet of the models fig 4 the bcp model shows a flow rate of more than 12 c m 3 s 1 at the main outlet channel due to the higher capacity of groundwater movement the lower flow rate of 0 5 c m 3 s 1 is computed at the conduit near the left boundary but as more water enters through the injection wells the flow rate in the conduit zone approaches a larger value approaching 12 c m 3 s 1 at the right boundary this is consistent with the hydrogeology of a karst aquifer in which the conduits become larger towards the outlet as it carries more water 4 2 inverse modeling inverse modeling is conducted with five different models to assess the ability of ht to map aquifer heterogeneity i e different conduit systems based on the activation of one or more injection wells under steady state and transient flow conditions initial estimates of the two dimensional fields of k and ss are created based on given unconditional mean variance and correlation lengths of k and ss xiang et al 2009 the exponential covariance model is adopted for the k and ss fields for both low k and high k zones i e matrix and conduit zones in fig 2 respectively for all cases the initial mean of k and ss along x and y directions are set as 0 1 c m s 1 and 0 01 cm 1 for the low k zone and 4 5 c m s 1 and 0 0001 cm 1 for the high k zone the variance of ln k and ln ss are initially set as 8 7 and 0 7 for the low k and high k zones respectively while the correlation lengths in the x and y directions are set as 15 and 5 m for the low k zone and 10 and 1 m for the high k zone injection wells are successively added to inverse models 1 to 5 for example inverse model 1 consists of a single injection well while inverse model 3 has three injection wells each injection well is activated over a different stress period the location and injection rate a rate of 5 c m 3 s 1 of the injection wells are the same for both the bcp and ncp models fig 2 the steady and transient head values from the 16 observation wells obtained by forward models of bcp and ncp are used for steady and transient inversions respectively one record of head per observation well obtained through forward simulations is utilized for steady state ht in contrast 48 records of head per observation well are available for transient ht transient forward simulations are executed over a simulation time of 720 s with time steps of 15 s results are presented as tomograms of k and ss separately for steady and transient inversions results are evaluated through scatterplots of estimated versus reference values of hydraulic parameters as well as scatterplots of pressure head and flow rate for both steady and transient ht 4 2 1 steady state hydraulic tomography ssht the effect of the number of injection wells the mean and variance of the estimated k by each of the five models from both of bcp and ncp models are presented in table s1 of the supplementary information the arithmetic mean of estimated k by various ssht models ranges from 0 05 to 0 34 c m s 1 and from 0 08 to 0 57 c m s 1 in bcp and ncp models respectively table s1 reveals that the estimated values of k by the bcp models are closer to the reference values in comparison to the ncp models assuming an equal number of activated injection wells the results of the inverse models from both the bcp and ncp cases are evaluated in terms of their k tomograms fig 5 the number of activated injection wells increases from the top to the bottom in fig 5 the estimated k tomograms become more differentiable between the bcp and ncp models as the number of activated injection wells increases however the bcp model remains closer to the reference pattern fig 2 in comparison to the ncp model in terms of the computed k values at each pixel examination of the scatterplot in fig s1 of the supplementary information shows that the estimated k values improve by increasing the number of data from injection wells the correlation coefficient computed as part of the scatterplot of k shows a slight improvement in the estimated k when the number of injection wells is increased fig s1 however differences between the estimated and observed head are negligible even with one active injection well fig s2 it can be stated that although the difference between simulated and reference head is negligible the estimated k values are still quite different from the reference values this problem arises because the inverse problem is often ill posed yeh 1986 the non uniqueness of the estimated values is a common drawback with inverse modeling the estimated values of k were selected by ssle to best match the head data at a limited number of observation wells as an ill posed problem different sets of spatial distributions of k even with unreasonable values of k i e different solution of inverse problem may be estimated given the limited number of head data carrera and neuman 1986 poeter and hill 1997 in addition the diffusive nature of head propagation may cause the small differences between the correlation coefficients of scatterplots in figs s1 and s2 of the supplementary information therefore we prefer to compare the spatial patterns of the estimated k with the reference conduit pattern instead of scatterplots 4 2 2 transient hydraulic tomography tht the effect of the number of injection wells table s2 of the supplementary information contains elementary statistics of k and ss computed for each of the five models of both bcp and ncp table s2 reveals that the bcp models yield a range of arithmetic mean k values from 0 64 to 9 34 c m s 1 and arithmetic mean ss values ranging from 0 002 to 0 003 cm 1 for the ncp models mean values of k and ss are estimated to be in the range of 0 06 to 3 1 c m s 1 and 0 0004 to 0 01 cm 1 respectively although the estimated k and ss fig s3 values for each pixel show an overestimation in comparison to the reference model for both the bcp and ncp models the k tomograms show an acceptable spatial pattern of the conduit fig 6 while the correspondence is less clear for the ss tomogram fig 7 branches of the conduits in the estimated tomograms of k and ss figs 6 and 7 can be identified by zones consisting of high k and low ss values the negative correlation between k and ss for conduits identified in this study was also observed by illman et al 2009 and sharmeen et al 2012 the conduit pattern based on the k tomogram fig 6 becomes gradually clearer and similar to the reference pattern when the number of injection wells increases in particular the conduit pattern is similar to the reference pattern when data from 4 or 5 injection wells are utilized for tht analysis especially for the bcp model fig 6 on the other hand the improvement to the ss tomogram is not as evident compared to the k tomogram fig 7 the scatterplot of the simulated versus reference k and ss fig s3 shows a large scatter and some bias the values of correlation coefficient between the estimated and reference k and ss fig s3 show little improvement when the number of active injection wells is increased in particular the correlation coefficient increases from 0 01 to 0 05 for k and 0 06 to 0 09 for ss for the bcp model similarly the correlation coefficient increases from 0 01 to 0 02 for k and 0 03 to 0 07 for ss in terms of the ncp model however the scatterplots of the estimated and reference heads fig s4 reveal a very high correlation it seems that even though the values of estimated and reference heads are similar the non uniqueness of the inverse model still causes a difference between the estimated and reference k and ss values illman et al 2010 since head data is available at only 16 observation locations not at all estimated locations for k and ss the inverse problem is ill posed therefore the estimation error of head data at available points may be minimized by numerous sets of k and ss fields carrera and neuman 1986 poeter and hill 1997 some of the estimated k and ss values may be small or large and even unreasonable in comparison to the reference values although the inverse model does not accurately estimate the k and ss values at some cells the model is still able to capture the general structure of the reference case with the spatial pattern of the estimated high k and low ss values corresponding with the reference conduit pattern figs 6 and 7 since this study was conducted to reconstruct the conduit pattern through the ht approach the results seem to be reasonable obviously improved estimates of k and ss could be obtained by increasing of the number of observation and injection wells however it should be noted that under real and practical conditions only a limited number of observation and injection wells are typically available furthermore if we are able to retrieve even a general conduit pattern in a karst aquifer through a limited number of observation wells one could utilize the tomograms and the estimation variance map to plan for additional site investigations including drilling of wells to refine the estimates using this iterative approach a more accurate map of the conduits should be identified through the addition of new data 4 3 comparison of results from ssht and tht a comparison between k tomograms of the ssht and tht assuming bcp and ncp models with 5 injection and 16 observation wells is presented in fig s5 of the supplementary information the k tomogram from tht can be a better indicator of aquifer heterogeneity due to the large numbers of head records at the observation wells in particular tht is run based on 24 temporal records of head per observation well while only one record of head per observation well is used in ssht it seems that temporal and spatial variation of head could reconstruct a more reliable image of aquifer heterogeneity i e conduit pattern major flow paths in the assumed synthetic bcp and ncp models follow the conduit patterns fig 4 therefore temporal and spatial variation of head and flow rate data at an observation point are influenced by the contributions of different flow paths however the temporal variation of head and flow rate data are not available under steady state conditions and the inverse model converges based on limited spatial records of head and flow rate generally we found that the behaviour of the inverse models under transient conditions is more acceptable than under steady state conditions in addition the bcp is more accurately mapped compared to the reference pattern in comparison to ncp fig s5 the similarity between the spatial distribution of estimated high k zones with the reference map of the conduits and higher correlation coefficient between the estimated and reference parameters i e k and water level buildup by the bcp model show a better behavior of the inverse model results for bcp in comparison to ncp it seems that since the specific pattern of heterogeneity i e assigned k field in the bcp has a specific variation along the flow direction from the left to the right boundaries i e the number of conduits is reduced meanwhile the conduits become larger in size the result of tht in mapping of heterogeneity is more accurate in comparison to the ncp inverse model without a specific spatial trend of k field 4 4 transient hydraulic tomography tht the effect of number of observation wells and geological information the results of ht have been extensively improved through the integration with other data such as from geological surveys zhao et al 2016 luo et al 2017 zhao and illman 2017 2018 zha et al 2017 flowmeter surveys zha et al 2015 2014 tracer tests illman et al 2010 zha et al 2014 zhu et al 2009 and geophysical surveys day lewis et al 2006 yeh et al 2002 in order to examine the role of geological data on the estimation of a karst conduit patterns an additional set of inverse simulations were performed these models attempt to evaluate the reliability of the designed synthetic modeling approach in reconstructing the conduit pattern we utilize tht over ssht because our earlier results reveal that the former can map karst conduit better than the latter due to the use of transient data for inversions case 1 initially assumes that the aquifer is homogeneous without any prior geological information about the development of karst case 1 includes 5 injection and 16 observation wells initial guesses for mean values of 0 1 c m s 1 and 0 001 cm 1 are set for k and ss respectively while corresponding variances of 13 8 and correlation lengths of 15 and 5 m in x and y directions respectively are set for both k and ss the inversion model vsaft2 starts with initial values of k and ss as a homogeneous material in case 1 and updates them during each iteration to find mean heterogeneous fields of k and ss conditioned to head data obtained at the 16 observation wells yeh and liu 2000 a mean value of 19 61 c m s 1 and 0 67 cm 1 in the bcp model and 17 12 c m s 1 and 0 03 cm 1 in the ncp model are estimated with tht for k and ss respectively these values are considerably different from the reference values table s3 even though the convergence criteria l1 and l2 in figs 8 and 9 and correlation coefficient between simulated and reference head are acceptable for both bcp and ncp models the estimated values of k and ss show different descriptive statistics table s3 however the estimated spatial heterogeneity reveals different conduit patterns for the bcp and ncp models figs 8 9 and s6 case 2 is treated as a two material inverse model with 5 injection and 12 observation wells we assigned a higher value of k and a lower value of ss in the cells around the injection wells and the outlet of the model as a new material type the injection wells and the outlet of the model simulate hypothetical locations of sinkholes and spring in a karst aquifer respectively generally the potential for development of karst i e high k and low ss around the location of sinkholes and springs is high in a karst aquifer therefore a value of 4 5 c m s 1 and 0 0001 cm 1 for the mean values of k and ss were assigned to these cells moreover a variance of 13 8 and correlation lengths of 15 and 5 m in x and y directions respectively are initially set for k and ss respectively the remaining cells had the same values as in case 1 the results show that the range of estimated values of k and ss have been reduced in particular a mean value of 1 38 and 7 71 c m s 1 for k and 0 4 and 0 1 cm 1 for ss are estimated by tht for the bcp and ncp models respectively meanwhile the l1 and l2 norms as well as the correlation between simulated and reference heads are acceptable in addition the k and ss tomograms present a clearer image of the reference conduit pattern especially for the bcp model figs 8 9 and s6 case 3 included two material types in which more cells were assigned as a material with higher k and lower ss for this case we assumed a pattern of perpendicular fracture set as the initial plausible pattern e g left column of fig 1 that could lead to the development of conduit patterns fig 1 the mean and variance of k and ss for the two materials in case 3 are the same as in case 2 however case 3 contains 5 injection and 8 observation wells results reveal that the estimated values of k and ss have become closer to the reference values table s3 a mean value of 0 74 c m s 1 and 0 08 cm 1 for the bcp model and 3 13 c m s 1 and 0 04 cm 1 for the ncp model are estimated for k and ss respectively furthermore the estimated spatial pattern of conduits based on the k and ss tomograms is very comparable with the reference bcp and ncp likewise the correlation between estimated and reference heads confirms this similarity figs 8 9 and s6 4 5 effect of considering geological based information the results reveal that assuming a heterogeneous material without assigning prior geological information to the models case 1 reaches an acceptable conduit pattern for bcp in comparison to ncp fig s7 in case 2 when the model treated the domain as two materials with a high k close to the injection wells the estimated conduit pattern tends to be similar to the reference bcp and ncp fig s7 it is noteworthy that bcp has been reproduced better than ncp when reducing the number of observation wells to 8 in case 3 and assuming a fracture set as prior information in the inverse model the results show that although the ncp model is more similar to the reference conduit pattern the reproduced conduit pattern by the bcp model is not very different between cases 2 and 3 fig s7 table s3 reveals that from case 1 to case 3 the mean values of k and ss reduced from 19 61 to 0 79 c m s 1 and from 0 67 to 0 004 cm 1 that are comparable to the reference values of 0 17 c m s 1 and 0 007 cm 1 generally the reliability of the estimated conduit pattern from case 1 to case 3 is improved due to superimposing geological data by assigning high k and low ss to the cells around the injection wells in case 2 or to a set of cells as a fracture set in case 3 however the number of observation wells has been reduced from 16 in case 1 to 8 in case 3 this finding suggests that despite the low density of observation wells in karst terrains considering precise geological data could extensively help in mapping complex karst terrains this finding is very important because generally spring discharge and recharge rate at the sinkholes and sinking streams i e injection wells in the synthetic models are available or it is often possible to measure in karst areas in contrast high density of observation wells is rare in karst areas 4 6 model validation in order to validate the performance of the models three new injection wells p1 p2 and p3 in fig 2 are activated in the reference models as well as the three comparative cases introduced in section 4 4 the forward models of bcp and ncp fig 2 are considered as reference models the location of the injection wells and their rate of injection are the same for the bcp and ncp models the estimated k and ss in the last iteration of the inverse models of the 3 cases figs 8 and 9 are assigned to the synthetic models for validation purposes the results of the models as simulated values are compared with the reference models as observed values in terms of flow rate at the outlet of each model and pressure head at three selected observation wells validation of the models took place under two scenarios of injection at p1 p3 1 the injection wells were sequentially activated with a rate of 5 c m 3 s 1 for a period of 45 s and 2 the injection wells were simultaneously activated with a rate of 5 c m 3 s 1 for the entire period of simulation in fig 10 pressure head and flow rate at the outlet of the reference model i e w17 are compared with three cases under sequential activation of the injection wells w17 is located close to the outlet of the synthetic models fig 2 based on the results from two specific realizations patterns of conduit i e bcp and ncp responses of the bcp to the activation of the injection wells is higher than ncp in terms of pressure head and flow rate at the outlet fig 10 maximum values of the simulated pressure head and flow rate for bcp are 96 1 cm and 1 7 c m 3 s 1 respectively while these values are 95 7 and 1 2 c m 3 s 1 for the ncp model although the injection wells are activated at different locations the ncp models show approximately the same response at the outlet in contrast the bcp models show different response to the activation of injection wells the different responses of bcp and ncp in terms of pressure head and flow rate are related to the specific pattern of connected conduits to create preferential flow paths toward the outlet in bcp in comparison to ncp the results of cases 2 and 3 are closer to the reference model for both the bcp and ncp models in fig s10 of the supplementary information simulated values of pressure head at the three observation wells are compared under continuous injection of the injection wells one of the selected observation wells w10 is located in the matrix and two w7 and w12 are placed in the conduit fig 2 visually the simulated pressure head for the bcp models is closer to the reference model in comparison to ncp at all observation points in addition the buildup of groundwater level in response to the injection wells in ncp is higher than bcp however since w10 is located in a low k zone i e matrix pressure head is higher than w7 and w12 which are located in the high k zones i e conduit in both bcp and ncp models the pressure head at the observation wells is affected by 1 the constant head boundaries at the right and left sides of the synthetic models fig 2 2 the spatial distributions of the k and ss and 3 the location of injection wells therefore the transient responses of w7 w10 and w12 are different in the bcp and ncp models in terms of the ncp model the estimated pressure head for case 2 is the closest to the reference model however all cases of bcp models estimate pressure head that are very close to the reference model fig s8 these results suggest that the tht yield significantly better results in terms of pressure head predictions for bcp than ncp predictions of flow rates are next assessed in fig s9 transient responses of flow rate in the selected observation wells are mainly affected by the values of k and ss around the location of the wells since w10 is located in the matrix consisting of low k material the flow rate is small and shows negligible temporal variation less than 0 03 c m 3 s 1 in comparison to w7 and w12 that are located within a conduit of a high k material with a maximum flow rate of 3 and 6 c m 3 s 1 respectively fig s9 although the estimation of flow rates at w7 and w12 in general reveals an underestimation relative to the reference model flow rate remains more reliable at w10 for both bcp and ncp models generally the flow rate values in the bcp models is higher than the ncp models due to the higher organization of the conduits i e high k zones toward the outlet of the model as a result improvement of cases 2 and 3 that include additional geological information in terms of the estimation of flow rate is visible the reference and simulated pressure head and flow rate data obtained through the validation exercise are compared in scatter plots figs 11 and 12 in which the 45 degree solid line indicates a perfect match the values of pressure head and flow rate at times of 10 30 80 160 and 750 s at all 17 observation wells are selected for comparison purposes best fit lines dashed of the pressure head reveals a better performance of the estimation in the three cases of bcp in comparison to ncp models in addition an overestimation of pressure head is obvious in the ncp models although the performance of the estimated pressure head based on l1 l2 and correlation coefficient is acceptable for all three cases case 2 still yields smaller values of l1 and l2 and a higher value of the correlation coefficient however the smaller values of l1 and l2 belongs to bcp for all three cases in comparison to ncp models fig 11 the improvement in the prediction of flow rates in terms of l1 and l2 is obvious from cases 1 to 3 in both bcp and ncp models l1 and l2 values decrease from 0 79 and 1 91 to 0 45 and 0 74 in bcp and from 0 94 and 2 64 to 0 64 and 1 13 in ncp models respectively meanwhile the correlation coefficient also indicates the improvement of predictions from cases 1 to 3 and increase from 0 87 to 0 91 in bcp and from 0 62 to 0 91 in ncp models respectively the lowest values of l1 and l2 and the highest value of correlation coefficient belong to case 3 for both bcp and ncp models fig 12 however scatterplots of pressure head show that the differences between l1 and l2 in the 3 cases is negligible it seems that even though the prediction performance of pressure head is acceptable for the three cases case 3 that includes more geological information results in the highest performance for estimating the flow rates for both bcp and ncp models 5 conclusions in this study the potential of ht in mapping the heterogeneity of two synthetic karst models with different yet representative patterns is investigated under steady state and transient conditions three elements of a typical karst aquifer that is the presence of sinkholes a spring and the conduit pattern i e heterogeneity are simulated through injection wells considering a small portion of no flow boundary as constant head boundary and high k and low ss zones respectively several combinations of injection and observation wells are examined to produce heterogeneity maps via ssht and tht we examined the role of karst hydrogeological knowledge such as higher potential for karst development around the location of sinkholes and spring to improve ht in mapping heterogeneity the validity of the estimated k and ss fields i e k and ss tomograms are tested by utilizing additional data from new injection wells not used in the calibration effort and comparing the results with the reference models five injection wells in the synthetic models were activated to cause temporal and spatial perturbations in head and flow rate which were recorded at several observation wells under steady and transient states these data were collected for both the bcp and ncp synthetic models k and ss tomograms were evaluated on the basis of the estimated heterogeneity or conduit pattern the results revealed the ability of ht in reproducing the conduit patterns in particular tht in bcp models show reliable results in comparison to ncp models in addition the effect of including geological data as prior information for inverse modeling in improving the accuracy of k tomograms was examined since drilling of observation wells in karst terrains is expensive this research proves that adding geological data e g data on fracture set and or knowing general trend of karst development can be very valuable and could compensate for the lack of a sufficient density of observation wells it seems that collecting complementary data from tracer and geophysical tests and utilizing these data in inverse models could potentially improve the estimation of conduit pattern since mapping the karst conduit location and its pattern is one of the most important and difficult aspect of karst studies it is envisioned that the integration of geological hydrogeological and geophysical data with ht would be beneficial in the future in order to obtain more refined maps of karst heterogeneity including improved hydraulic parameter estimates one should include both pressure head and flow rate in ht zha et al 2014 declaration of interests none acknowledgments the first author would like to acknowledge shiraz university for financial support during sabbatical leave at university of waterloo the second author acknowledges the support from natural sciences and engineering research council of canada nserc through the discovery grant which made this collaboration with the first author possible the authors thank prof franci gabrovšek and an anonymous reviewer for their constructive comments and suggestions which improved the manuscript appendix a supplementary material supplementary data to this article can be found online at https doi org 10 1016 j jhydrol 2019 02 044 appendix a supplementary material the following are the supplementary data to this article supplementary data 1 
6579,determining the spatiotemporal dynamics of surface water in a heterogeneous floodplain is difficult especially for its surrounding isolated lakes seasonal inundation patterns of these isolated lakes can be misestimated in a hydrodynamic model due to their short and erratic appearances a surface water time series of poyang lake having an 8 day revisit frequency with a 30 m spatial resolution from 2000 to 2016 was conducted for the first time this study was produced with a modified hierarchical spatiotemporal adaptive fusion model hstafm by integrating both landsat and modis data discrepancies between model based surface water and remotely sensed surface water were evaluated and possible causes were discussed results show that the modified hstafm can better detect the water features of a floodplain thereby providing more detailed information in an seasonal isolated lake system than the modis mod13q1 product with the fusion product we found that poyang lake evidently shrank after experiencing a longer low water period after the impoundment of the three gorges dam a large proportion of these discrepancies averaging 36 between model based and remotely sensed surface water distributed in seasonal isolated lakes mainly occurred during high water level periods uncertainties in the hydrodynamic model might attribute to smaller defined lake boundaries bathymetric variations human disturbance and unconsidered groundwater recharge discharge these results provide a new insight into the temporally continuous and spatially dynamic assessment of simulated surface water which is essential for the future improvement in the hydrodynamic model keywords surface water inundation pattern remote sensing hydrodynamic model seasonal isolated lake poyang lake 1 introduction wetlands and other periodically inundated floodplains constitute a diverse ecosystem which plays an important role in maintaining regional ecological and environmental functions including the hydrological cycle biodiversity conservation water quality protection and flood control acreman and ferguson 2010 selwood et al 2017 unfortunately terrestrial water bodies are undergoing a loss of biological diversity worldwide which has been endangering the future of these ecosystems davidson 2014 winemiller et al 2016 zhang et al 2017c in general the timing extent duration and variability of inundation affects vegetation development wetland dynamics and ecosystem function maintenance casanova and brock 2000 toogood and joyce 2009 zhang et al 2012 tan et al 2016 as a result an accurate characterization of spatiotemporal variations in surface water is essential mapping surface water changes has benefited a lot from the accessibility of free remote sensing images improved computational capacities and advanced image classification techniques such studies have largely filled the data gap over large remote and poorly gauged areas bates et al 2014 kuenzer et al 2015 pekel et al 2016 klein et al 2017 due to the lack of long term decades synthetic aperture radar sar data optical sensors such as landsat and moderate resolution imaging spectroradiometer modis are commonly used to capture surface water dynamics for large shallow water bodies over multi decadal time periods jones et al 2009 hu et al 2015 tulbure et al 2016 huang et al 2018 at a 30 m resolution landsat sensors onboard landsat 4 5 7 and 8 have accumulated a large data pool since the 1980s and have been widely used to monitor long term surface water dynamics on both a global scale yamazaki et al 2015 pekel et al 2016 klein et al 2017 and on regional scales rodrigues et al 2012 mueller et al 2016 jones et al 2017 however the 16 day satellite revisit cycle is generally not capable of detecting rapid surface changes especially during wet and flood seasons frazier and page 2009 jung et al 2011 in addition terra and aqua modis sensors have been used to study lake water variations feng et al 2012 khandelwal et al 2017 lu et al 2017 and floodplain inundation islam et al 2010 huang et al 2014 heimhuber et al 2018 due to their daily availability and broad spatial coverage however the coarse modis resolution 500 m modis data compared to 30 m landsat data is generally insufficient for capturing fragmented water bodies and inundation patterns within heterogeneous landscapes chen et al 2013 despite recent advances there seems to be a trade off between high spatial resolution and high temporal resolution for most current surface water records one feasible solution is to generate synthetic data through a combination of multi sensor satellite data such as those from landsat and modis weng et al 2014 gao et al 2015 wu et al 2016 chen et al 2017 zhao et al 2018 this solution can be realized using both indirect and direct strategies the indirect strategy is to decompose coarse scale modis observations using fine scale landsat observations based on this strategy fusion models include the spatial and temporal adaptive reflectance fusion model starfm gao et al 2006 the semi physical fusion approach roy et al 2008 the spatial temporal adaptive algorithm for mapping reflectance change staarch hilker et al 2009a 2009b the enhanced starfm estarfm zhu et al 2010 the spatial temporal data fusion approach stdfa wu et al 2012 the multi temporal fusion method amoros lopez et al 2013 and the improved stdfa istdfa wu et al 2016 2018 these models use the relationship of reflectance between modis and landsat data to predict landsat like reflectance at prediction dates the limitations include 1 lacking a pair of cloud free images in humid regions michishita et al 2015 and 2 assumption of no land cover change or only a linear change during the prediction period these assumptions may not hold over rapidly changing land surfaces the direct strategy is to downscale normalized difference vegetation index ndvi data which generally requires a fine resolution auxiliary land cover use database for pixel unmixing such models include the weighted linear mixing model wlm busetto et al 2008 the spatial and temporal adaptive vegetation index fusion model stavfm meng et al 2013 the automated compound smoother rmmeh jin and xu 2013 and the ndvi linear mixing growth model ndvi lmgm rao et al 2015 these models are claimed to support the study of land surface dynamics in heterogeneous landscapes much like indirect methods the direct methods also assume identical ndvi values for the same land use cover type this assumption may not hold within large areas where spectral heterogeneity is common even for the same land use cover type therefore an improved spatiotemporal fusion technique is required to address the above limitations for a seasonally inundated floodplain in the middle of the yangtze river basin poyang lake hosts the most predominant ecosystem with free flow connections to the yangtze river and provides an ideal study site sun et al 2012 lai et al 2014b dominated by gradual morphological changes the extensive floodplain basin located on the flat landscape of the lake was segmented by complex levee systems and river channels many ecologically and economically important seasonal isolated lakes sub lakes occur in these gradual floodplains along the western and southern lake offshores these seasonal isolated lakes expand to a large surface water area during the flood period and shrink to a small area during the dry period shankman and liang 2003 feng et al 2012 when and where inundation occurs directly affects the wetland aquatic ecological conditions as well as irrigation and drinking water availability williamson et al 2009 ariztegui et al 2010 du et al 2011 numerical modeling offers insights into inundation patterns physically based numerical models such as mike 21 li et al 2014 zhang et al 2014 yao et al 2018 the coupled 1d and 2d hydrodynamic analysis model for the middle yangtze river cham yangtze lai et al 2014b efdc wang et al 2015 delft3d flow zhang et al 2017a and gms lan et al 2015 have been established to simulate the spatial distribution and temporal patterns of inundation at a large spatial scale despite the availability of digital elevation models plus extensive field surveys carried out in 1998 and 2010 the knowledge of this floodplain topography is still insufficient to model flood propagation for seasonal isolated lakes it is difficult to determine boundary conditions due to the complex flow patterns and limited in situ observations yao et al 2018 a number of pioneering studies have attempted to document the landscape changes of poyang lake using landsat thematic mapper tm enhanced thematic mapper plus etm operational land imager oli data hui et al 2008 liu et al 2013 han et al 2015 modis data feng et al 2012 2013 wu and liu 2015 cai et al 2016 envisat data andreoli et al 2007 xu et al 2010 liao et al 2013 china s huanjing hj data zhao et al 2011 zhang et al 2015 and microsatellite data dronova et al 2011 however it remains a challenge to capture rapid surface water changes with a fine spatiotemporal resolution primarily due to the constraints on data availability to address the above limitations chen et al 2017 proposed a hierarchical spatiotemporal adaptive fusion model hstafm to produce synthetic ndvis for surface water mapping the fused ndvis 2000 2014 had a 16 day revisit frequency at a 30 m spatial resolution comparisons based on observed images show that hstafm is more accurate than starfm and fsdaf chen et al 2018 but hstafm still requires validation over small seasonal isolated lakes and narrow river channels in addition the discrepancies between model and remote sensing derived surface water areas can be considerably large despite li et al 2014 and zhang et al 2017b who have shown good correlation between them the knowledge gap of when where and how model remote sensing differences exist hampers their usage in various applications our study addresses this gap by integrating the novel time series of landsat and modis data based on fulfilling the following two objectives 1 to reconstruct the surface water series at 30 m and 8 day resolutions from 2000 to 2016 used for monitoring the rapid surface water changes of poyang lake especially during rising and recession periods 2 to investigate the discrepancies between and the uncertainties for surface water area derived from hydrodynamic modeling and remote sensing especially in seasonal isolated lakes the method in this study provides a novel perspective for capturing surface water dynamics it is expected to accommodate similar systems comprised of extensive floodplains and considerable water level fluctuations 2 study area poyang lake is the largest lake in the yangtze river basin controlled by an eastern asian monsoon considerable seasonal water level variations 10 m create an extensive wetland ecosystem across a total area of approximately 3000 km2 feng et al 2013 a complex levee system about 6400 km has been built around the lake for flood protection shankman et al 2006 leading to more than 77 seasonal isolated lakes during dry seasons when the lake is divided into many connected and disconnected segments separated by exposed floodplains fig 1 these lakes range from 1 km2 to over 71 km2 in size with a total surface water area of 767 km2 and an average size of 10 km2 for the poyang lake national nature reserve plnnr and the nanji wetland national nature reserve nwnnr 80 of the area is covered by these seasonal isolated lakes which provide a wide range of foraging options for more than 80 of the migrating birds water levels of seasonal isolated lakes increase as a result of continual inflow from five upstream tributaries xiushui ganjiang fuhe xinjiang and raohe and from the main lake during late spring and early summer may and june most seasonal isolated lakes merge into one single water body when the yangtze river reaches its highest water level from july to early september in late september poyang lake starts receding again and most seasonal isolated lakes are disconnected and have differing water levels throughout the winter from december to february the timing and duration of floodplain exposure and inundation affects the wetland plant senescence and regeneration cycle water quality and waterbird survival nishihiro et al 2004 liu et al 2006 raulings et al 2010 in recent years poyang lake has experienced significant changes due to intensive anthropogenic activity and climate change construction of the three gorges dam tgd which began to impound water in 2003 arguably has had the largest impact on flow and sediment regimes fang et al 2012 since the tgd came into operation the inundation pattern and the distribution of wetland habitats in floodplains has changed dramatically guo et al 2012 zhao et al 2013 mei et al 2016 which in turn has had detrimental impacts on its ecological function as a habitat for fish turvey et al 2010 and waterbirds barzen et al 2009 in addition liu et al 2013 revealed that the lake change was a synthetic result of precipitation evapotranspiration and outflow discharge however the natural dynamics of surface water in numerous temporal and widespread seasonal isolated lakes remains unclear 3 methods 3 1 modis data and pre processing the 250 m resolution 8 day composited mod13q1 data tile no h28v06 in 2000 2016 were obtained from the national aeronautics and space administration nasa earth observing system data and information system eosdis url https lpdaac usgs gov data access data pool a constrained view angle maximum value composite cv mvc and a bidirectional reflectance distribution function brdf algorithm have been designed to filter out the effects of instrument calibration sun angle differences terrain cloud shadows and atmospheric conditions jin and xu 2013 michishita et al 2014 furthermore the running median mean value maximum operation end point processing and hanning rmmeh smoothing method jin and xu 2013 was adopted to reduce the residual noise of the ndvi profile and to reconstruct a high quality ndvi time series the dataset was then blended with landsat images to produce a dense and fine resolution ndvi time series fig 2 shows all the modis and landsat data used in our study 3 2 landsat data and pre processing a total of 129 cloud free landsat tm etm and oli at a spatial resolution of 30 m images were downloaded from the united states geological survey usgs url http www usgs gov these images were first converted to top of atmosphere toa radiance using radiometric calibration coefficients in the metadata files then the flaash fast line of sight atmospheric analysis of spectral hypercubes module embedded in envi 5 1 software was used to generate surface reflectance data to fill the gaps present in etm images after may 31 2003 the triangulation method developed by scaramuzza et al 2004 was applied finally landsat ndvi and normalized difference water index ndwi images were calculated using the reflectance data 3 3 hydrodynamic modeling data and pre processing in this study remote sensing based surface water results were compared to model based results the inundation depth of poyang lake 2000 2012 was simulated by the mike 21 model dhi 2007 which is a 2d depth averaged hydrodynamic model implemented previously by li et al 2014 the model defines a total lake area of 3124 km2 according to historical lake surface areas during periods with high water levels a 2d grid system with an unstructured triangular grid was adopted to capture the complex bathymetry of poyang lake surveyed in 1998 and updated in 2000 the edge length of mesh elements varied from 70 to 1500 m resulting in a total of 20 450 triangular elements the time step was set to 5 s to limit the courant friedrich levy cfl number for a stable solution because groundwater observations and geological data are not available no attempts were made to simulate groundwater dynamics a detailed description of the model can be found in our former studies tan et al 2016 zhang et al 2017b finally daily time series data were averaged to 16 day jan 1 2000 to jun 26 2002 and 8 day jun 27 2002 to dec 31 2016 time steps 3 4 generating synthetic 8 day landsat ndvi images using hstafm the hstafm model proposed by chen et al 2017 was applied in this study compared to the starfm like fusion models the featured improvement of hstafm is the introduction of an initial prediction and integration of that initial prediction into a hierarchical strategy for selecting similar pixels the improvement can best capture land surface changes within the limited available prior posterior landsat modis image pair and targeted modis image four steps were followed to implement the hstafm approach first modis data were reprojected and resampled according to landsat images second an initial predicted fine resolution image was produced using a direct multiplier method third a hierarchical similar pixels scheme was used to identify both prior or posterior and predicted dates fourth a weight wij eq 1 was assigned to each similar pixel based on i the spectral difference sij between ndvi of the base landsat modis image pair and ii the spatial euclidean distance dij between the neighbor and the central pixel fifth the ndvi value of the central pixel was computed with the algorithm characterized in eq 1 1 f 2 x ω 2 y ω 2 b i 1 ω j 1 ω w ij p ij l 1 x i y j b m 2 x i y j b m 1 x i y j b 2 w ij 1 s ij d ij i 1 ω j 1 ω 1 s ij d ij 3 s ij p ij l x i y j l x ω 2 y ω 2 i 1 ω j 1 ω p ij l x i y j l x ω 2 y ω 2 4 d ij 1 x i x ω 2 2 y j y ω 2 2 1 ω where ω denotes the moving window size wij is the combined weight determined by the spectral and distance differences according to eqs 2 4 and p is the similar pixel set represented by a binary matrix a more detailed description of the hstafm algorithm can be found in two previous papers by chen et al 2017 2018 in this study contemporary modis and landsat images observed in the same year with the most similar land surface information were chosen to make up a pair the water level data at the duchang station served as a reference if the contemporary criterion was not met another landsat image with the closest water level and phenological period in the previous or next year was chosen as an alternative 3 5 surface water extraction using the jenks natural breaks method the poyang lake is mainly covered by dense vegetation tan et al 2016 therefore the fused ndvis were classified as water and non water classes in this study the jenks natural breaks method also called the jenks optimization method is a data clustering method used to find existing groups of values which are then joined together to exploit natural gaps in the data jenks 1977 class breaks are identified based on their ability to best group similar values and maximize the differences between classes the input data are divided into classes and their boundaries are set where relatively large differences exist between data values carr et al 2002 three main steps were followed to implement the jenks 1967 1977 natural breaks step 1 calculate the sum of squared deviations for array mean sdam step 2 for each range combination calculate the sum of squared deviations for class means sdcm all and find the smallest one sdcm all is much like sdam but uses class means and deviations figure out the smallest sdcm all so its best ranges minimize the variations within classes step 3 calculate a goodness of variance fit gvf defined as sdam scdm sdam gvf ranges from 1 perfect fit to 0 awful fit the higher sdcm all more variation within classes results in a lower gvf 4 results 4 1 blended surface water series and its accuracy based on the fused ndvis surface water mapping was performed using jenks natural breaks method to demonstrate the performance of this surface water result visual interpretation was performed in the plnnr and nwnnr for different hydrological periods using a uniform sampling strategy fig 3 samples without an available landsat image on oct 31 2016 day of year doy 305 were replaced by images obtained on dec 16 2016 doy 353 the blended 30 m surface water series accurately discerns small water bodies in the two national reserves because of the coarse modis spatial resolution shallow water can be misclassified and confused with muddy sediment as shown in fig 3 the surface water derived from fused ndvis performs better in detecting the boundaries of small seasonal isolated lakes meanwhile the spatial continuity of the narrow rivers was maintained on the blended surface water maps especially during low water level periods surface water area data derived from fused ndvis show a high correlation with those derived from landsat ndwi r2 0 92 demonstrating that the former has a similar accuracy compared with the high spatial resolution images fig 4 the root mean square error rmse between the two series is 208 km2 and the mean absolute error mae is 136 km2 poyang lake and its seasonal isolated lakes show strong seasonality in their surface water areas the annual maximum surface water 3153 km2 of the whole lake occurred on jul 19 2016 doy 201 and the minimum surface water 555 km2 occurred on jan 17 2004 doy 017 for all seasonal isolated lakes the maximum surface water 760 km2 was found on jul 27 2016 doy 209 and the minimum surface water 59 km2 was found on jan 1 2004 doy 001 the annual maximum minimum surface water ratio ranged from 2 0 to 4 9 for the whole lake and from 2 8 to 9 2 for all seasonal isolated lakes specifically the whole poyang lake showed a mean surface water area of 2105 595 km2 in the pre tgd period 2000 2002 hydrological year which decreased by 389 km2 to 1716 716 km2 during the post tgd period 2003 2016 hydrological year see fig 5 the difference between the two periods is statistically significant anvoa p 0 05 the striking results of the significant decrease in surface water areas after the impoundment of tgd were consistent with former studies feng et al 2013 in comparison although a decrease of surface water area from 364 190 km2 to 288 173 km2 was also found in seasonal isolated lakes after the impoundment of tgd the differences between the two periods were not significant p 0 05 the decreasing trends in the long term surface water area for both the whole poyang lake and seasonal isolated lakes were further revealed by variations in the duration of the low water period during each year fig 6 the duration of the low water period was derived from the number of surface water map with areas that were below the mean surface water area of the dry season from december to february between 2000 and 2015 approximately the much shorter duration of low water periods ranged from 0 to 8 days 1 8 days for the whole lake and 0 to 24 days for the seasonal isolated lakes during the pre tgd period leading to a general but not significant trends 3 12 days per year and 0 16 days per year respectively in the long term variations to be specific the longest low water periods were observed in the hydrological year of 2007 for both the whole lake 144 days and seasonal isolated lakes 144 days notably the moderate shrinkage of the whole poyang lake and seasonal isolated lakes was mainly caused by the earlier dry season starting dates meanwhile the serious shrinkage of the lake was caused by both the earlier starting dates and the later ending dates of the dry seasons 4 2 comparison of remotely sensed and model based surface water in this study a spatiotemporal comparison of surface water derived from a hydrodynamic model mike 21 with fused ndvis from the hstafm was tested to better identify errors and hence make improvements to either or both of these datasets temporal profiles of remotely sensed surface water areas and model based surface water areas from 2000 to 2012 are displayed in fig 7 however the model based surface water profile after 2012 is not shown in fig 7 due to lack of basic data the temporal variation and changing trend for the whole lake and seasonal isolated lakes achieved a general high consistency between these two datasets suggesting that both methods could successfully capture the inundation dynamics of the study area most of the time however the consistency varied from year to year for the whole poyang lake the most significant discrepancy of the surface water area appeared in 2008 with an average difference of 364 km2 meanwhile the minimum difference was 164 km2 on average which was detected in 2010 like the whole poyang lake the maximum and minimum surface water differences of seasonal isolated lakes between these two datasets were also detected in 2008 128 km2 and 2011 51 km2 for the whole period area differences between remotely sensed surface water and model based surface water ranged from 2 km2 to 1045 km2 on average 264 km2 for the whole lake and from 0 km2 to 412 km2 on average 83 km2 for seasonal isolated lakes in addition the relationship between remotely sensed surface water and model based surface water for the whole lake r2 0 85 is better than that for seasonal isolated lakes r2 0 75 as shown in fig 7b and fig 7c respectively in the whole poyang lake an overall 36 difference between remotely sensed surface water and model based surface water was attributed to estimation errors for seasonal isolated lakes notably the maximum areas of the model based surface water were constant during different years for both the whole lake 3124 km2 and seasonal isolated lakes 752 km2 an annual bias estimation for seasonal isolated lakes is further recognized in fig 8 which shows the monthly average differences between areas of remotely sensed surface water and areas of model based surface water varying from 1 km2 to 164 km2 fig 8a the model remote sensing discrepancies were much larger in july 164 km2 august 129 km2 and september 153 km2 than in other months on the seasonal variations the average area of model based surface water 481 km2 was much larger than the remotely sensed surface water 756 km2 fig 8b contrary to the considerable discrepancy during the flood period the remotely sensed surface water and model based surface water matched each other well during rising periods with a difference in the average surface water area of 17 km2 recession periods with a difference in the average surface water area of 63 km2 and dry periods with a difference in the average surface water area of 10 km2 in previous studies on poyang lake and the middle and lower reaches of the yangtze river 2006 was frequently analyzed as a typical drought year dai et al 2008 li et al 2017 and 2010 was recognized as a typical flood year feng et al 2012 fig 9 highlights the relationship between remotely sensed surface water areas and model based surface water areas in typical years as shown in fig 9 the surface water areas derived from model achieved the best consistency to the surface water areas derived from remote sensing in a typical flood year with a coefficient of correlation r2 0 92 which was better than the consistency achieved in the normal year r2 0 86 and in the typical drought year r2 0 66 in fig 9b most points distributed above the 1 1 line indicate that the hydrodynamic model always overestimates the surface water area in a drought year especially during the flood period 5 discussion large lakes like the poyang lake typically form wide floodplains occupied by seasonal isolated lakes streams rivers and wetlands the correlation between the inundation area of a lake or wetland and nearby river water level fluctuations can be presumed indicative of hydrological connectivity between the river main stem and its surrounding floodplain hydrological connectivity is important not only for the floodplain morphogenesis and system maintenance but for nutrient recycling among biotas monitoring the hydrological connectivity demands more continuous both spatial and temporal records of inundation dynamics taking place over the floodplain as a cost effective approach spatiotemporal fusion techniques have proved useful in monitoring the dynamics of phenology however some emerging problems remain for example the starfm like methods cannot deal with transient disturbance or land cover change situations because these methods require the primary assumption that the land cover type does not change between the baseline and predicted dates chen et al 2018 in this study the modified hstafm method can better capture the general change between the prior posterior and predicted dates and it offers an efficient tool to monitor the dynamics of deltaic systems especially of small seasonal isolated lakes and narrow river channels in addition this paper investigates in more detail the discrepancies between remotely sensed and model based surface water series the authors deemed that the hydrodynamic modeling approach appears to be limited in this case study especially in the simulation of seasonal isolated lakes karim et al 2011 estimated floodplain inundation in the fitzroy river catchment using hydrodynamic modeling and remote sensing they found that simulated inundations are large compared with modis detected inundations and the difference is larger at the later flood stage the author proposed that potential sources of model error are 1 the use of large grids to reproduce steam channels 2 implementation of a steady state initial condition and 3 use of a simplified land use map to estimate land surface roughness the first limitation also exists in this study considering the model s stability and efficiency the edge length of mesh elements varied from 70 to 1500 m for different lake topographies at different scales these elements were coarser on flat alluvial plains and finer in deep rivers and channels compared to remote sensing data hydrodynamic models usually have coarse meshes which cannot effectively capture the terrain differences and bathymetric changes these simulation uncertainties were amplified in the simulations of low gradient floodplains in the southern and western shores of poyang lake where most seasonal isolated lakes are distributed besides since 2000 the bed elevation of the hukou outflow channel has decreased significantly due to intensive illegal sand mining activities de leeuw et al 2010 lai et al 2014a lai et al 2014a proved that despite the sand mining activities which usually occurred in the northern channels the discharge ability of poyang lake into the yangtze river during the dry season increased 1 5 2 times due to bathymetric changes yao et al 2018 proposed that the bed erosion of the northern outlet channel averaged 3 m resulting in a decrease in the water level by 1 2 2 m in the most significant influence areas hydrodynamic models based on historical digital elevation models dems may have produced deviations in simulations years ago or later as for the second limitation in karim s model the initial conditions in this model were obtained by interpolating the observed surface water elevations at the five gauging stations additionally based on the object oriented classification spatially varying manning s roughness coefficients were derived from the empirical parameters of landcover types with high spatial resolution thus the third limitation in karim s model cannot be a critical factor causing simulation error in this study given the considerations discussed above there are some other limitations causing simulation uncertainties that need attention first the model boundary in this study is not the same as the real one poyang lake is historically a region of significant floods extensive levee building began after the 1954 floods before 1950 the total length of levees in jiangxi was about 3100 km the levee system was made higher and wider in the 1950s in the 1970s land was reclaimed from the poyang lake but after the 1998 flood farmlands destroyed by war and floods during the late 1800s and early 1900s were returned to the lake there are now 6400 km of levees that provide protection to 10 000 km2 of farmland and to a population of about 10 million people who live in the low lying areas at the margins of poyang lake shankman and liang 2003 the levees have been continuously improved a complex levee system and its dramatic changes make it difficult to confirm the actual boundary of poyang lake which is important when modeling in this study an area of 3124 km2 was determined by examining the historic flood event in 1998 which was smaller than the poyang lake area described in other literature shankman et al 2012 because the northern channel connected to the yangtze river is poyang lake s only outlet the discharge capability of the lake is constrained by the locking effect of the yangtze river continuous rising of the high water level will result in rising depths of inundation regions but the surface water area will remain static after reaching its maximum value this may account for the stable surface water that occurred in the model during the flood period second the inundation patterns of some seasonal isolated lakes are disturbed by human activities to the best of our knowledge at least 22 seasonal isolated lakes have water levels that are manipulated by independent control gates their elevations at central points and at bottom elevations of control gates are listed in table 1 these lakes have been contracted to local fishermen for fishing and planting control gates are usually closed in the early recession period to maintain water for fish growth and then opened in january for harvest however the closed and open time of the control gates differ it is almost impossible to accurately simulate the inundation patterns of all seasonal isolated lakes without any regulation information record third subsurface hydrological processes are not considered in this model one of our former studies li et al 2018 revealed that the groundwater level variations of a typical wetland in plnnr were sensitive to the lake water level fluctuations with a correlation coefficient varying from 0 93 to 0 99 between them based on the records of more than 40 groundwater observation wells in poyang lake basin lan et al 2015 confirmed that about 53 3 correlation coefficients between groundwater levels and nearby river levels were larger than 0 90 and 93 3 of these correlation coefficients were larger than 0 80 suggesting the presence of a good underground hydrological connectivity however for seasonal isolated lakes the groundwater discharge recharge has been neglected in hydrodynamic models which only consider the impact of precipitation and evaporation when these lakes disconnect from each other or the main lake this insufficiency of the hydrodynamic model may cause an underestimation of surface water for seasonal isolated lakes during a water rising period and an overestimation during a water recession period although the remote sensing approach performed better than the hydrodynamic modeling approach in this study the abilities and challenges of using optical remote sensing especially the fusion model needs further discussion three significant limitations in the remote sensing approach can be summarized as follows first water under a vegetation canopy is easily misclassified a challenge for remote sensing of inundated floodplains is the capacity to identify water under vegetation canopies including both emerging and floating macrophyte canopies flooded vegetation and soil formed a transition area of mixed pixels between water flooded wetland vegetation dry land vegetation and bare ground soil occurring on the edges of open water optical remote sensing technologies provide a wide coverage at a range of spatial and spectral resolutions but have limitations when covering tropical floodplains because the surface water is often obscured by dense overlying vegetation to understand spatiotemporal inundation patterns of floodplains one must first understand the development of a time series of inter annual and seasonal maps of variability in flooded standing vegetation thus longer wavelength l band synthetic aperture radar sar data have become the preferred data source because of the capacity of l band data to detect water under some canopies evans et al 2010 ward et al 2014 the fusion of microwave l band sar and optical landsat tm 5 satellite data provides us with a new insight to capturing the complex seasonal dynamics of floodplain aquatic vegetation cover and inundation second discrepancies exist in landsat modis pairs limitations remain in the hstafm method including errors produced by minor changes occurring at sub pixel resolution and errors introduced by the predicted images in selecting final similar pixels this problem was clarified by chen et al 2018 in their fusion model the accuracy of the final predicted image depended on the accuracy of the initial predicted ndvi value with landsat like spatial resolution at the target date meanwhile the initial predicted fine resolution ndvi l2 was calculated from the observed modis ndvi value at the target date m2 and the observed ndvi value of modis m1 and landsat l1 at a prior or posterior date by l 2 l 1 m 2 m 1 assuming that l1 was consistent with m1 in this study despite variations water levels and phenology were considered to reduce the difference between l1 and m1 as far as possible however land cover changes still exist due to the lack of available landsat images the discrepancy between m1 and l1 which was observed in different years may lead to an aggregation of change detection biases in the final prediction other optical data with fine spatial resolution such as sentinal 2 and gaofen 3 may be helpful to fill the gap of data shortages especially in recent years third the best threshold for index varies in space and time temporal dynamic thresholds were adopted for determining the optimal value in order to distinguish water bodies and non water bodies from differing conditions or sensor differences however an ideal single threshold for water or vegetation indices is difficult to determine because the spectral signature of water varies both in time and space tulbure et al 2016 therefore threshold definition in one image must also be considered in the context of intrinsic water characteristics and the land cover complexity of distinct regions as discussed above uncertainties in hydrodynamic modeling and remote sensing inhibit its use in flood mapping interest in integrating these two fields has increased with the availability of remote sensing data which are freely available near global and frequent for example karim et al 2011 found that the availability of remotely sensed flood inundation maps was very useful for model calibration moreover ticehurst et al 2015 improved the mapping of flood events based on the daily modis open water likelihood algorithm by successfully using information from the hydrodynamic model we believe in the near future the combination of hydrodynamic modeling and remote sensing will provide a useful way for assessing flood discharge and the duration and frequency of wetland connectivity for broad scale water resource assessment 6 conclusions a remotely sensed surface water series with an 8 day revisit frequency at a 30 m spatial resolution from 2000 to 2016 was first produced in poyang lake using the modified hstafm method a total of 129 surface water maps extracted from cloud free landsat ndwis were applied to evaluate the performance of the fusion method verifying that it can be reasonably employed in monitoring rapidly varying floodplains with the analysis of the blended surface water series we found that the annual average area of surface water decreased evidently after the operation of the tgd both for the whole poyang lake and seasonal isolated lakes the duration of the low water period increased between 2000 and 2015 with a changing rate that was higher for the whole lake than for seasonal isolated lakes as a major step forward this research quantifies the temporal and spatial variations of discrepancies between model based surface water and remotely sensed surface water a relatively higher correlation between these two datasets was found for the whole lake especially during low water level periods on average overall discrepancies of 36 were found in regions covered by seasonal isolated lakes and occurred more often during the flood period moreover it was found that the simulated surface water matched the observed surface water best in the typical flood year and worst in the typical drought year constraint of the designed model boundaries and influences of varied bathymetry might limit the simulation accuracy manipulation of seasonal isolated lakes could proportionally contribute to subsequent simulation uncertainties additionally groundwater discharge and recharge should be taken into consideration in further simulations this work presented the temporally continuous and spatially dynamic surface water of poyang lake providing new insight into the uncertainties of the hydrodynamic model and remote sensing given the rapid changes of the inundation pattern in poyang lake the 8 day interval of the remotely sensed surface water data collection still seems coarse despite lots of uncertainties identified while applying the hydrodynamic model during this study it remains a powerful tool for identifying continuous spatiotemporal changes in complex floodplain systems declaration of interest statement we declare that we have no financial and personal relationships with other people or organizations that can inappropriately influence our work there is no professional or other personal interest of any nature or kind in any product service and or company that could be construed as influencing the position presented in or the review of the manuscript entitled mapping inundation dynamics in a heterogeneous floodplain insights from integrating observation and modelling approach hydrol29903 acknowledgements we appreciate bin chen for providing the fusion model code and for his willingness to share his knowledge and expertise throughout this research we also gratefully acknowledge the professional english editing of xingwang fan in improving this article funding this work is supported by the national natural science foundation of china 41801080 and 41601031 the natural science foundation of jiangsu and jiangxi province bk20181103 and 20181bab213011 and the science foundation of nanjing institute of geography and limnology chinese academy of sciences niglas2017qd05 
6579,determining the spatiotemporal dynamics of surface water in a heterogeneous floodplain is difficult especially for its surrounding isolated lakes seasonal inundation patterns of these isolated lakes can be misestimated in a hydrodynamic model due to their short and erratic appearances a surface water time series of poyang lake having an 8 day revisit frequency with a 30 m spatial resolution from 2000 to 2016 was conducted for the first time this study was produced with a modified hierarchical spatiotemporal adaptive fusion model hstafm by integrating both landsat and modis data discrepancies between model based surface water and remotely sensed surface water were evaluated and possible causes were discussed results show that the modified hstafm can better detect the water features of a floodplain thereby providing more detailed information in an seasonal isolated lake system than the modis mod13q1 product with the fusion product we found that poyang lake evidently shrank after experiencing a longer low water period after the impoundment of the three gorges dam a large proportion of these discrepancies averaging 36 between model based and remotely sensed surface water distributed in seasonal isolated lakes mainly occurred during high water level periods uncertainties in the hydrodynamic model might attribute to smaller defined lake boundaries bathymetric variations human disturbance and unconsidered groundwater recharge discharge these results provide a new insight into the temporally continuous and spatially dynamic assessment of simulated surface water which is essential for the future improvement in the hydrodynamic model keywords surface water inundation pattern remote sensing hydrodynamic model seasonal isolated lake poyang lake 1 introduction wetlands and other periodically inundated floodplains constitute a diverse ecosystem which plays an important role in maintaining regional ecological and environmental functions including the hydrological cycle biodiversity conservation water quality protection and flood control acreman and ferguson 2010 selwood et al 2017 unfortunately terrestrial water bodies are undergoing a loss of biological diversity worldwide which has been endangering the future of these ecosystems davidson 2014 winemiller et al 2016 zhang et al 2017c in general the timing extent duration and variability of inundation affects vegetation development wetland dynamics and ecosystem function maintenance casanova and brock 2000 toogood and joyce 2009 zhang et al 2012 tan et al 2016 as a result an accurate characterization of spatiotemporal variations in surface water is essential mapping surface water changes has benefited a lot from the accessibility of free remote sensing images improved computational capacities and advanced image classification techniques such studies have largely filled the data gap over large remote and poorly gauged areas bates et al 2014 kuenzer et al 2015 pekel et al 2016 klein et al 2017 due to the lack of long term decades synthetic aperture radar sar data optical sensors such as landsat and moderate resolution imaging spectroradiometer modis are commonly used to capture surface water dynamics for large shallow water bodies over multi decadal time periods jones et al 2009 hu et al 2015 tulbure et al 2016 huang et al 2018 at a 30 m resolution landsat sensors onboard landsat 4 5 7 and 8 have accumulated a large data pool since the 1980s and have been widely used to monitor long term surface water dynamics on both a global scale yamazaki et al 2015 pekel et al 2016 klein et al 2017 and on regional scales rodrigues et al 2012 mueller et al 2016 jones et al 2017 however the 16 day satellite revisit cycle is generally not capable of detecting rapid surface changes especially during wet and flood seasons frazier and page 2009 jung et al 2011 in addition terra and aqua modis sensors have been used to study lake water variations feng et al 2012 khandelwal et al 2017 lu et al 2017 and floodplain inundation islam et al 2010 huang et al 2014 heimhuber et al 2018 due to their daily availability and broad spatial coverage however the coarse modis resolution 500 m modis data compared to 30 m landsat data is generally insufficient for capturing fragmented water bodies and inundation patterns within heterogeneous landscapes chen et al 2013 despite recent advances there seems to be a trade off between high spatial resolution and high temporal resolution for most current surface water records one feasible solution is to generate synthetic data through a combination of multi sensor satellite data such as those from landsat and modis weng et al 2014 gao et al 2015 wu et al 2016 chen et al 2017 zhao et al 2018 this solution can be realized using both indirect and direct strategies the indirect strategy is to decompose coarse scale modis observations using fine scale landsat observations based on this strategy fusion models include the spatial and temporal adaptive reflectance fusion model starfm gao et al 2006 the semi physical fusion approach roy et al 2008 the spatial temporal adaptive algorithm for mapping reflectance change staarch hilker et al 2009a 2009b the enhanced starfm estarfm zhu et al 2010 the spatial temporal data fusion approach stdfa wu et al 2012 the multi temporal fusion method amoros lopez et al 2013 and the improved stdfa istdfa wu et al 2016 2018 these models use the relationship of reflectance between modis and landsat data to predict landsat like reflectance at prediction dates the limitations include 1 lacking a pair of cloud free images in humid regions michishita et al 2015 and 2 assumption of no land cover change or only a linear change during the prediction period these assumptions may not hold over rapidly changing land surfaces the direct strategy is to downscale normalized difference vegetation index ndvi data which generally requires a fine resolution auxiliary land cover use database for pixel unmixing such models include the weighted linear mixing model wlm busetto et al 2008 the spatial and temporal adaptive vegetation index fusion model stavfm meng et al 2013 the automated compound smoother rmmeh jin and xu 2013 and the ndvi linear mixing growth model ndvi lmgm rao et al 2015 these models are claimed to support the study of land surface dynamics in heterogeneous landscapes much like indirect methods the direct methods also assume identical ndvi values for the same land use cover type this assumption may not hold within large areas where spectral heterogeneity is common even for the same land use cover type therefore an improved spatiotemporal fusion technique is required to address the above limitations for a seasonally inundated floodplain in the middle of the yangtze river basin poyang lake hosts the most predominant ecosystem with free flow connections to the yangtze river and provides an ideal study site sun et al 2012 lai et al 2014b dominated by gradual morphological changes the extensive floodplain basin located on the flat landscape of the lake was segmented by complex levee systems and river channels many ecologically and economically important seasonal isolated lakes sub lakes occur in these gradual floodplains along the western and southern lake offshores these seasonal isolated lakes expand to a large surface water area during the flood period and shrink to a small area during the dry period shankman and liang 2003 feng et al 2012 when and where inundation occurs directly affects the wetland aquatic ecological conditions as well as irrigation and drinking water availability williamson et al 2009 ariztegui et al 2010 du et al 2011 numerical modeling offers insights into inundation patterns physically based numerical models such as mike 21 li et al 2014 zhang et al 2014 yao et al 2018 the coupled 1d and 2d hydrodynamic analysis model for the middle yangtze river cham yangtze lai et al 2014b efdc wang et al 2015 delft3d flow zhang et al 2017a and gms lan et al 2015 have been established to simulate the spatial distribution and temporal patterns of inundation at a large spatial scale despite the availability of digital elevation models plus extensive field surveys carried out in 1998 and 2010 the knowledge of this floodplain topography is still insufficient to model flood propagation for seasonal isolated lakes it is difficult to determine boundary conditions due to the complex flow patterns and limited in situ observations yao et al 2018 a number of pioneering studies have attempted to document the landscape changes of poyang lake using landsat thematic mapper tm enhanced thematic mapper plus etm operational land imager oli data hui et al 2008 liu et al 2013 han et al 2015 modis data feng et al 2012 2013 wu and liu 2015 cai et al 2016 envisat data andreoli et al 2007 xu et al 2010 liao et al 2013 china s huanjing hj data zhao et al 2011 zhang et al 2015 and microsatellite data dronova et al 2011 however it remains a challenge to capture rapid surface water changes with a fine spatiotemporal resolution primarily due to the constraints on data availability to address the above limitations chen et al 2017 proposed a hierarchical spatiotemporal adaptive fusion model hstafm to produce synthetic ndvis for surface water mapping the fused ndvis 2000 2014 had a 16 day revisit frequency at a 30 m spatial resolution comparisons based on observed images show that hstafm is more accurate than starfm and fsdaf chen et al 2018 but hstafm still requires validation over small seasonal isolated lakes and narrow river channels in addition the discrepancies between model and remote sensing derived surface water areas can be considerably large despite li et al 2014 and zhang et al 2017b who have shown good correlation between them the knowledge gap of when where and how model remote sensing differences exist hampers their usage in various applications our study addresses this gap by integrating the novel time series of landsat and modis data based on fulfilling the following two objectives 1 to reconstruct the surface water series at 30 m and 8 day resolutions from 2000 to 2016 used for monitoring the rapid surface water changes of poyang lake especially during rising and recession periods 2 to investigate the discrepancies between and the uncertainties for surface water area derived from hydrodynamic modeling and remote sensing especially in seasonal isolated lakes the method in this study provides a novel perspective for capturing surface water dynamics it is expected to accommodate similar systems comprised of extensive floodplains and considerable water level fluctuations 2 study area poyang lake is the largest lake in the yangtze river basin controlled by an eastern asian monsoon considerable seasonal water level variations 10 m create an extensive wetland ecosystem across a total area of approximately 3000 km2 feng et al 2013 a complex levee system about 6400 km has been built around the lake for flood protection shankman et al 2006 leading to more than 77 seasonal isolated lakes during dry seasons when the lake is divided into many connected and disconnected segments separated by exposed floodplains fig 1 these lakes range from 1 km2 to over 71 km2 in size with a total surface water area of 767 km2 and an average size of 10 km2 for the poyang lake national nature reserve plnnr and the nanji wetland national nature reserve nwnnr 80 of the area is covered by these seasonal isolated lakes which provide a wide range of foraging options for more than 80 of the migrating birds water levels of seasonal isolated lakes increase as a result of continual inflow from five upstream tributaries xiushui ganjiang fuhe xinjiang and raohe and from the main lake during late spring and early summer may and june most seasonal isolated lakes merge into one single water body when the yangtze river reaches its highest water level from july to early september in late september poyang lake starts receding again and most seasonal isolated lakes are disconnected and have differing water levels throughout the winter from december to february the timing and duration of floodplain exposure and inundation affects the wetland plant senescence and regeneration cycle water quality and waterbird survival nishihiro et al 2004 liu et al 2006 raulings et al 2010 in recent years poyang lake has experienced significant changes due to intensive anthropogenic activity and climate change construction of the three gorges dam tgd which began to impound water in 2003 arguably has had the largest impact on flow and sediment regimes fang et al 2012 since the tgd came into operation the inundation pattern and the distribution of wetland habitats in floodplains has changed dramatically guo et al 2012 zhao et al 2013 mei et al 2016 which in turn has had detrimental impacts on its ecological function as a habitat for fish turvey et al 2010 and waterbirds barzen et al 2009 in addition liu et al 2013 revealed that the lake change was a synthetic result of precipitation evapotranspiration and outflow discharge however the natural dynamics of surface water in numerous temporal and widespread seasonal isolated lakes remains unclear 3 methods 3 1 modis data and pre processing the 250 m resolution 8 day composited mod13q1 data tile no h28v06 in 2000 2016 were obtained from the national aeronautics and space administration nasa earth observing system data and information system eosdis url https lpdaac usgs gov data access data pool a constrained view angle maximum value composite cv mvc and a bidirectional reflectance distribution function brdf algorithm have been designed to filter out the effects of instrument calibration sun angle differences terrain cloud shadows and atmospheric conditions jin and xu 2013 michishita et al 2014 furthermore the running median mean value maximum operation end point processing and hanning rmmeh smoothing method jin and xu 2013 was adopted to reduce the residual noise of the ndvi profile and to reconstruct a high quality ndvi time series the dataset was then blended with landsat images to produce a dense and fine resolution ndvi time series fig 2 shows all the modis and landsat data used in our study 3 2 landsat data and pre processing a total of 129 cloud free landsat tm etm and oli at a spatial resolution of 30 m images were downloaded from the united states geological survey usgs url http www usgs gov these images were first converted to top of atmosphere toa radiance using radiometric calibration coefficients in the metadata files then the flaash fast line of sight atmospheric analysis of spectral hypercubes module embedded in envi 5 1 software was used to generate surface reflectance data to fill the gaps present in etm images after may 31 2003 the triangulation method developed by scaramuzza et al 2004 was applied finally landsat ndvi and normalized difference water index ndwi images were calculated using the reflectance data 3 3 hydrodynamic modeling data and pre processing in this study remote sensing based surface water results were compared to model based results the inundation depth of poyang lake 2000 2012 was simulated by the mike 21 model dhi 2007 which is a 2d depth averaged hydrodynamic model implemented previously by li et al 2014 the model defines a total lake area of 3124 km2 according to historical lake surface areas during periods with high water levels a 2d grid system with an unstructured triangular grid was adopted to capture the complex bathymetry of poyang lake surveyed in 1998 and updated in 2000 the edge length of mesh elements varied from 70 to 1500 m resulting in a total of 20 450 triangular elements the time step was set to 5 s to limit the courant friedrich levy cfl number for a stable solution because groundwater observations and geological data are not available no attempts were made to simulate groundwater dynamics a detailed description of the model can be found in our former studies tan et al 2016 zhang et al 2017b finally daily time series data were averaged to 16 day jan 1 2000 to jun 26 2002 and 8 day jun 27 2002 to dec 31 2016 time steps 3 4 generating synthetic 8 day landsat ndvi images using hstafm the hstafm model proposed by chen et al 2017 was applied in this study compared to the starfm like fusion models the featured improvement of hstafm is the introduction of an initial prediction and integration of that initial prediction into a hierarchical strategy for selecting similar pixels the improvement can best capture land surface changes within the limited available prior posterior landsat modis image pair and targeted modis image four steps were followed to implement the hstafm approach first modis data were reprojected and resampled according to landsat images second an initial predicted fine resolution image was produced using a direct multiplier method third a hierarchical similar pixels scheme was used to identify both prior or posterior and predicted dates fourth a weight wij eq 1 was assigned to each similar pixel based on i the spectral difference sij between ndvi of the base landsat modis image pair and ii the spatial euclidean distance dij between the neighbor and the central pixel fifth the ndvi value of the central pixel was computed with the algorithm characterized in eq 1 1 f 2 x ω 2 y ω 2 b i 1 ω j 1 ω w ij p ij l 1 x i y j b m 2 x i y j b m 1 x i y j b 2 w ij 1 s ij d ij i 1 ω j 1 ω 1 s ij d ij 3 s ij p ij l x i y j l x ω 2 y ω 2 i 1 ω j 1 ω p ij l x i y j l x ω 2 y ω 2 4 d ij 1 x i x ω 2 2 y j y ω 2 2 1 ω where ω denotes the moving window size wij is the combined weight determined by the spectral and distance differences according to eqs 2 4 and p is the similar pixel set represented by a binary matrix a more detailed description of the hstafm algorithm can be found in two previous papers by chen et al 2017 2018 in this study contemporary modis and landsat images observed in the same year with the most similar land surface information were chosen to make up a pair the water level data at the duchang station served as a reference if the contemporary criterion was not met another landsat image with the closest water level and phenological period in the previous or next year was chosen as an alternative 3 5 surface water extraction using the jenks natural breaks method the poyang lake is mainly covered by dense vegetation tan et al 2016 therefore the fused ndvis were classified as water and non water classes in this study the jenks natural breaks method also called the jenks optimization method is a data clustering method used to find existing groups of values which are then joined together to exploit natural gaps in the data jenks 1977 class breaks are identified based on their ability to best group similar values and maximize the differences between classes the input data are divided into classes and their boundaries are set where relatively large differences exist between data values carr et al 2002 three main steps were followed to implement the jenks 1967 1977 natural breaks step 1 calculate the sum of squared deviations for array mean sdam step 2 for each range combination calculate the sum of squared deviations for class means sdcm all and find the smallest one sdcm all is much like sdam but uses class means and deviations figure out the smallest sdcm all so its best ranges minimize the variations within classes step 3 calculate a goodness of variance fit gvf defined as sdam scdm sdam gvf ranges from 1 perfect fit to 0 awful fit the higher sdcm all more variation within classes results in a lower gvf 4 results 4 1 blended surface water series and its accuracy based on the fused ndvis surface water mapping was performed using jenks natural breaks method to demonstrate the performance of this surface water result visual interpretation was performed in the plnnr and nwnnr for different hydrological periods using a uniform sampling strategy fig 3 samples without an available landsat image on oct 31 2016 day of year doy 305 were replaced by images obtained on dec 16 2016 doy 353 the blended 30 m surface water series accurately discerns small water bodies in the two national reserves because of the coarse modis spatial resolution shallow water can be misclassified and confused with muddy sediment as shown in fig 3 the surface water derived from fused ndvis performs better in detecting the boundaries of small seasonal isolated lakes meanwhile the spatial continuity of the narrow rivers was maintained on the blended surface water maps especially during low water level periods surface water area data derived from fused ndvis show a high correlation with those derived from landsat ndwi r2 0 92 demonstrating that the former has a similar accuracy compared with the high spatial resolution images fig 4 the root mean square error rmse between the two series is 208 km2 and the mean absolute error mae is 136 km2 poyang lake and its seasonal isolated lakes show strong seasonality in their surface water areas the annual maximum surface water 3153 km2 of the whole lake occurred on jul 19 2016 doy 201 and the minimum surface water 555 km2 occurred on jan 17 2004 doy 017 for all seasonal isolated lakes the maximum surface water 760 km2 was found on jul 27 2016 doy 209 and the minimum surface water 59 km2 was found on jan 1 2004 doy 001 the annual maximum minimum surface water ratio ranged from 2 0 to 4 9 for the whole lake and from 2 8 to 9 2 for all seasonal isolated lakes specifically the whole poyang lake showed a mean surface water area of 2105 595 km2 in the pre tgd period 2000 2002 hydrological year which decreased by 389 km2 to 1716 716 km2 during the post tgd period 2003 2016 hydrological year see fig 5 the difference between the two periods is statistically significant anvoa p 0 05 the striking results of the significant decrease in surface water areas after the impoundment of tgd were consistent with former studies feng et al 2013 in comparison although a decrease of surface water area from 364 190 km2 to 288 173 km2 was also found in seasonal isolated lakes after the impoundment of tgd the differences between the two periods were not significant p 0 05 the decreasing trends in the long term surface water area for both the whole poyang lake and seasonal isolated lakes were further revealed by variations in the duration of the low water period during each year fig 6 the duration of the low water period was derived from the number of surface water map with areas that were below the mean surface water area of the dry season from december to february between 2000 and 2015 approximately the much shorter duration of low water periods ranged from 0 to 8 days 1 8 days for the whole lake and 0 to 24 days for the seasonal isolated lakes during the pre tgd period leading to a general but not significant trends 3 12 days per year and 0 16 days per year respectively in the long term variations to be specific the longest low water periods were observed in the hydrological year of 2007 for both the whole lake 144 days and seasonal isolated lakes 144 days notably the moderate shrinkage of the whole poyang lake and seasonal isolated lakes was mainly caused by the earlier dry season starting dates meanwhile the serious shrinkage of the lake was caused by both the earlier starting dates and the later ending dates of the dry seasons 4 2 comparison of remotely sensed and model based surface water in this study a spatiotemporal comparison of surface water derived from a hydrodynamic model mike 21 with fused ndvis from the hstafm was tested to better identify errors and hence make improvements to either or both of these datasets temporal profiles of remotely sensed surface water areas and model based surface water areas from 2000 to 2012 are displayed in fig 7 however the model based surface water profile after 2012 is not shown in fig 7 due to lack of basic data the temporal variation and changing trend for the whole lake and seasonal isolated lakes achieved a general high consistency between these two datasets suggesting that both methods could successfully capture the inundation dynamics of the study area most of the time however the consistency varied from year to year for the whole poyang lake the most significant discrepancy of the surface water area appeared in 2008 with an average difference of 364 km2 meanwhile the minimum difference was 164 km2 on average which was detected in 2010 like the whole poyang lake the maximum and minimum surface water differences of seasonal isolated lakes between these two datasets were also detected in 2008 128 km2 and 2011 51 km2 for the whole period area differences between remotely sensed surface water and model based surface water ranged from 2 km2 to 1045 km2 on average 264 km2 for the whole lake and from 0 km2 to 412 km2 on average 83 km2 for seasonal isolated lakes in addition the relationship between remotely sensed surface water and model based surface water for the whole lake r2 0 85 is better than that for seasonal isolated lakes r2 0 75 as shown in fig 7b and fig 7c respectively in the whole poyang lake an overall 36 difference between remotely sensed surface water and model based surface water was attributed to estimation errors for seasonal isolated lakes notably the maximum areas of the model based surface water were constant during different years for both the whole lake 3124 km2 and seasonal isolated lakes 752 km2 an annual bias estimation for seasonal isolated lakes is further recognized in fig 8 which shows the monthly average differences between areas of remotely sensed surface water and areas of model based surface water varying from 1 km2 to 164 km2 fig 8a the model remote sensing discrepancies were much larger in july 164 km2 august 129 km2 and september 153 km2 than in other months on the seasonal variations the average area of model based surface water 481 km2 was much larger than the remotely sensed surface water 756 km2 fig 8b contrary to the considerable discrepancy during the flood period the remotely sensed surface water and model based surface water matched each other well during rising periods with a difference in the average surface water area of 17 km2 recession periods with a difference in the average surface water area of 63 km2 and dry periods with a difference in the average surface water area of 10 km2 in previous studies on poyang lake and the middle and lower reaches of the yangtze river 2006 was frequently analyzed as a typical drought year dai et al 2008 li et al 2017 and 2010 was recognized as a typical flood year feng et al 2012 fig 9 highlights the relationship between remotely sensed surface water areas and model based surface water areas in typical years as shown in fig 9 the surface water areas derived from model achieved the best consistency to the surface water areas derived from remote sensing in a typical flood year with a coefficient of correlation r2 0 92 which was better than the consistency achieved in the normal year r2 0 86 and in the typical drought year r2 0 66 in fig 9b most points distributed above the 1 1 line indicate that the hydrodynamic model always overestimates the surface water area in a drought year especially during the flood period 5 discussion large lakes like the poyang lake typically form wide floodplains occupied by seasonal isolated lakes streams rivers and wetlands the correlation between the inundation area of a lake or wetland and nearby river water level fluctuations can be presumed indicative of hydrological connectivity between the river main stem and its surrounding floodplain hydrological connectivity is important not only for the floodplain morphogenesis and system maintenance but for nutrient recycling among biotas monitoring the hydrological connectivity demands more continuous both spatial and temporal records of inundation dynamics taking place over the floodplain as a cost effective approach spatiotemporal fusion techniques have proved useful in monitoring the dynamics of phenology however some emerging problems remain for example the starfm like methods cannot deal with transient disturbance or land cover change situations because these methods require the primary assumption that the land cover type does not change between the baseline and predicted dates chen et al 2018 in this study the modified hstafm method can better capture the general change between the prior posterior and predicted dates and it offers an efficient tool to monitor the dynamics of deltaic systems especially of small seasonal isolated lakes and narrow river channels in addition this paper investigates in more detail the discrepancies between remotely sensed and model based surface water series the authors deemed that the hydrodynamic modeling approach appears to be limited in this case study especially in the simulation of seasonal isolated lakes karim et al 2011 estimated floodplain inundation in the fitzroy river catchment using hydrodynamic modeling and remote sensing they found that simulated inundations are large compared with modis detected inundations and the difference is larger at the later flood stage the author proposed that potential sources of model error are 1 the use of large grids to reproduce steam channels 2 implementation of a steady state initial condition and 3 use of a simplified land use map to estimate land surface roughness the first limitation also exists in this study considering the model s stability and efficiency the edge length of mesh elements varied from 70 to 1500 m for different lake topographies at different scales these elements were coarser on flat alluvial plains and finer in deep rivers and channels compared to remote sensing data hydrodynamic models usually have coarse meshes which cannot effectively capture the terrain differences and bathymetric changes these simulation uncertainties were amplified in the simulations of low gradient floodplains in the southern and western shores of poyang lake where most seasonal isolated lakes are distributed besides since 2000 the bed elevation of the hukou outflow channel has decreased significantly due to intensive illegal sand mining activities de leeuw et al 2010 lai et al 2014a lai et al 2014a proved that despite the sand mining activities which usually occurred in the northern channels the discharge ability of poyang lake into the yangtze river during the dry season increased 1 5 2 times due to bathymetric changes yao et al 2018 proposed that the bed erosion of the northern outlet channel averaged 3 m resulting in a decrease in the water level by 1 2 2 m in the most significant influence areas hydrodynamic models based on historical digital elevation models dems may have produced deviations in simulations years ago or later as for the second limitation in karim s model the initial conditions in this model were obtained by interpolating the observed surface water elevations at the five gauging stations additionally based on the object oriented classification spatially varying manning s roughness coefficients were derived from the empirical parameters of landcover types with high spatial resolution thus the third limitation in karim s model cannot be a critical factor causing simulation error in this study given the considerations discussed above there are some other limitations causing simulation uncertainties that need attention first the model boundary in this study is not the same as the real one poyang lake is historically a region of significant floods extensive levee building began after the 1954 floods before 1950 the total length of levees in jiangxi was about 3100 km the levee system was made higher and wider in the 1950s in the 1970s land was reclaimed from the poyang lake but after the 1998 flood farmlands destroyed by war and floods during the late 1800s and early 1900s were returned to the lake there are now 6400 km of levees that provide protection to 10 000 km2 of farmland and to a population of about 10 million people who live in the low lying areas at the margins of poyang lake shankman and liang 2003 the levees have been continuously improved a complex levee system and its dramatic changes make it difficult to confirm the actual boundary of poyang lake which is important when modeling in this study an area of 3124 km2 was determined by examining the historic flood event in 1998 which was smaller than the poyang lake area described in other literature shankman et al 2012 because the northern channel connected to the yangtze river is poyang lake s only outlet the discharge capability of the lake is constrained by the locking effect of the yangtze river continuous rising of the high water level will result in rising depths of inundation regions but the surface water area will remain static after reaching its maximum value this may account for the stable surface water that occurred in the model during the flood period second the inundation patterns of some seasonal isolated lakes are disturbed by human activities to the best of our knowledge at least 22 seasonal isolated lakes have water levels that are manipulated by independent control gates their elevations at central points and at bottom elevations of control gates are listed in table 1 these lakes have been contracted to local fishermen for fishing and planting control gates are usually closed in the early recession period to maintain water for fish growth and then opened in january for harvest however the closed and open time of the control gates differ it is almost impossible to accurately simulate the inundation patterns of all seasonal isolated lakes without any regulation information record third subsurface hydrological processes are not considered in this model one of our former studies li et al 2018 revealed that the groundwater level variations of a typical wetland in plnnr were sensitive to the lake water level fluctuations with a correlation coefficient varying from 0 93 to 0 99 between them based on the records of more than 40 groundwater observation wells in poyang lake basin lan et al 2015 confirmed that about 53 3 correlation coefficients between groundwater levels and nearby river levels were larger than 0 90 and 93 3 of these correlation coefficients were larger than 0 80 suggesting the presence of a good underground hydrological connectivity however for seasonal isolated lakes the groundwater discharge recharge has been neglected in hydrodynamic models which only consider the impact of precipitation and evaporation when these lakes disconnect from each other or the main lake this insufficiency of the hydrodynamic model may cause an underestimation of surface water for seasonal isolated lakes during a water rising period and an overestimation during a water recession period although the remote sensing approach performed better than the hydrodynamic modeling approach in this study the abilities and challenges of using optical remote sensing especially the fusion model needs further discussion three significant limitations in the remote sensing approach can be summarized as follows first water under a vegetation canopy is easily misclassified a challenge for remote sensing of inundated floodplains is the capacity to identify water under vegetation canopies including both emerging and floating macrophyte canopies flooded vegetation and soil formed a transition area of mixed pixels between water flooded wetland vegetation dry land vegetation and bare ground soil occurring on the edges of open water optical remote sensing technologies provide a wide coverage at a range of spatial and spectral resolutions but have limitations when covering tropical floodplains because the surface water is often obscured by dense overlying vegetation to understand spatiotemporal inundation patterns of floodplains one must first understand the development of a time series of inter annual and seasonal maps of variability in flooded standing vegetation thus longer wavelength l band synthetic aperture radar sar data have become the preferred data source because of the capacity of l band data to detect water under some canopies evans et al 2010 ward et al 2014 the fusion of microwave l band sar and optical landsat tm 5 satellite data provides us with a new insight to capturing the complex seasonal dynamics of floodplain aquatic vegetation cover and inundation second discrepancies exist in landsat modis pairs limitations remain in the hstafm method including errors produced by minor changes occurring at sub pixel resolution and errors introduced by the predicted images in selecting final similar pixels this problem was clarified by chen et al 2018 in their fusion model the accuracy of the final predicted image depended on the accuracy of the initial predicted ndvi value with landsat like spatial resolution at the target date meanwhile the initial predicted fine resolution ndvi l2 was calculated from the observed modis ndvi value at the target date m2 and the observed ndvi value of modis m1 and landsat l1 at a prior or posterior date by l 2 l 1 m 2 m 1 assuming that l1 was consistent with m1 in this study despite variations water levels and phenology were considered to reduce the difference between l1 and m1 as far as possible however land cover changes still exist due to the lack of available landsat images the discrepancy between m1 and l1 which was observed in different years may lead to an aggregation of change detection biases in the final prediction other optical data with fine spatial resolution such as sentinal 2 and gaofen 3 may be helpful to fill the gap of data shortages especially in recent years third the best threshold for index varies in space and time temporal dynamic thresholds were adopted for determining the optimal value in order to distinguish water bodies and non water bodies from differing conditions or sensor differences however an ideal single threshold for water or vegetation indices is difficult to determine because the spectral signature of water varies both in time and space tulbure et al 2016 therefore threshold definition in one image must also be considered in the context of intrinsic water characteristics and the land cover complexity of distinct regions as discussed above uncertainties in hydrodynamic modeling and remote sensing inhibit its use in flood mapping interest in integrating these two fields has increased with the availability of remote sensing data which are freely available near global and frequent for example karim et al 2011 found that the availability of remotely sensed flood inundation maps was very useful for model calibration moreover ticehurst et al 2015 improved the mapping of flood events based on the daily modis open water likelihood algorithm by successfully using information from the hydrodynamic model we believe in the near future the combination of hydrodynamic modeling and remote sensing will provide a useful way for assessing flood discharge and the duration and frequency of wetland connectivity for broad scale water resource assessment 6 conclusions a remotely sensed surface water series with an 8 day revisit frequency at a 30 m spatial resolution from 2000 to 2016 was first produced in poyang lake using the modified hstafm method a total of 129 surface water maps extracted from cloud free landsat ndwis were applied to evaluate the performance of the fusion method verifying that it can be reasonably employed in monitoring rapidly varying floodplains with the analysis of the blended surface water series we found that the annual average area of surface water decreased evidently after the operation of the tgd both for the whole poyang lake and seasonal isolated lakes the duration of the low water period increased between 2000 and 2015 with a changing rate that was higher for the whole lake than for seasonal isolated lakes as a major step forward this research quantifies the temporal and spatial variations of discrepancies between model based surface water and remotely sensed surface water a relatively higher correlation between these two datasets was found for the whole lake especially during low water level periods on average overall discrepancies of 36 were found in regions covered by seasonal isolated lakes and occurred more often during the flood period moreover it was found that the simulated surface water matched the observed surface water best in the typical flood year and worst in the typical drought year constraint of the designed model boundaries and influences of varied bathymetry might limit the simulation accuracy manipulation of seasonal isolated lakes could proportionally contribute to subsequent simulation uncertainties additionally groundwater discharge and recharge should be taken into consideration in further simulations this work presented the temporally continuous and spatially dynamic surface water of poyang lake providing new insight into the uncertainties of the hydrodynamic model and remote sensing given the rapid changes of the inundation pattern in poyang lake the 8 day interval of the remotely sensed surface water data collection still seems coarse despite lots of uncertainties identified while applying the hydrodynamic model during this study it remains a powerful tool for identifying continuous spatiotemporal changes in complex floodplain systems declaration of interest statement we declare that we have no financial and personal relationships with other people or organizations that can inappropriately influence our work there is no professional or other personal interest of any nature or kind in any product service and or company that could be construed as influencing the position presented in or the review of the manuscript entitled mapping inundation dynamics in a heterogeneous floodplain insights from integrating observation and modelling approach hydrol29903 acknowledgements we appreciate bin chen for providing the fusion model code and for his willingness to share his knowledge and expertise throughout this research we also gratefully acknowledge the professional english editing of xingwang fan in improving this article funding this work is supported by the national natural science foundation of china 41801080 and 41601031 the natural science foundation of jiangsu and jiangxi province bk20181103 and 20181bab213011 and the science foundation of nanjing institute of geography and limnology chinese academy of sciences niglas2017qd05 
