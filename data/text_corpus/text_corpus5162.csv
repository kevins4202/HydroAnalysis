index,text
25810,to aid decision making about environmental systems under deep uncertainty robustness metrics are commonly used to represent system performance over a number of scenarios however there are many robustness metrics and many ways of generating scenarios making it difficult to know which to choose in order to quantify system robustness and to make robust decisions to address this shortcoming we introduce a generic guidance framework to assist with the identification of the most robust decision alternatives as well as the rapid robustness analysis producing intelligent decisions software package which is a consistent and easy to use implementation of the framework we illustrate the framework and software package on a hypothetical lake pollution problem known as the lake problem showing how the framework and software package apply to several situations where decision makers may or may not know which scenarios or robustness metrics to use keywords deep uncertainty decision making under uncertainty robustness scenarios 1 introduction the long term planning of environmental systems presents major challenges as it requires decisions to be made despite significant uncertainty about the future state of the world frequently decision makers are operating at the level of deep uncertainty which refers to when deterministic and probabilistic approaches are insufficient for representing future states and the consideration of multiple plausible futures scenarios is required bradfield et al 2005 herman et al 2014 kwakkel et al 2010 kwakkel and haasnoot 2019 lempert 2003 little et al 2018 maier et al 2016 schwarz 1991 van der heijden 1996 varum and melo 2010 walker et al 2013 wright and cairns 2011 implicit in the deep uncertainty paradigm is that probabilities cannot be placed on these scenarios and therefore traditional risk based performance metrics such as reliability vulnerability resilience or expected value cannot be used to quantify the overall level of system performance across all scenarios maier et al 2016 rather deep uncertainty requires robustness metrics to be used which aim to quantify the relative level or variation of system performance across all or targeted scenarios bartholomew and kwakkel 2020 giudici et al 2020 herman et al 2015 kwakkel and haasnoot 2019 lempert 2003 maier et al 2016 mcphail et al 2018 as with traditional performance metrics in deterministic and probabilistic paradigms decision makers aim to choose a solution that has maximal performance robustness or a solution that has an appropriate tradeoff between performance metrics e g robustness vs cost there is a multitude of approaches to quantify system robustness generally by treating the scenarios as a distribution and making implicit probabilistic assumptions including i expected value metrics wald 1951 which indicate an expected level of performance across a range of scenarios ii metrics of higher order moments such as variance and skew e g kwakkel et al 2016a which provide information on how the expected level of performance varies across multiple scenarios iii regret based metrics savage 1951 where the regret of a decision alternative is defined as the difference between the performance of the selected option for a particular plausible condition and the performance of the best possible option for that condition and iv satisficing metrics simon 1956 which identify the range of scenarios that have acceptable performance relative to a threshold a common conclusion from recent research is that different robustness metrics can sometimes lead to decision alternatives being ranked differently making it difficult to determine which decision alternatives are most robust borgomeo et al 2018 drouet et al 2015 giuliani and castelletti 2016 hall et al 2012 herman et al 2015 kwakkel et al 2016a lempert and collins 2007 mcphail et al 2018 roach et al 2016 for example a case study by kwakkel et al 2016a on the transition of the european energy system towards a more sustainable future concluded that there is no clearly superior single robustness metric case specific consideration and system characteristics affect the merits of the various robustness measures this implies that an analyst has to choose carefully which robustness measure is being used and assess its appropriateness given that robustness metrics are calculated over a set of scenarios the choice of scenarios that are used in this calculation can also have an impact on the robustness value obtained in addition to the choice of robustness metric mcphail et al 2018 2020 a common categorization of scenarios is given by b√∂rjeson et al 2006 including the following three types predictive scenarios where the aim is to determine what will happen for example the future state of the world could be based on some future trajectory or change in trajectory due to some event explorative scenarios where the aim is to determine what could happen generally this is done by framing the future in terms of the uncertainties that have the largest effects on system performance but the future can also be unframed maier et al 2016 and normative scenarios where the aim is to determine how can a specific future be realized this is generally focused on interesting future outcomes or failure points for decision alternatives each of these types of scenarios can be created in different ways for example a set of scenarios for a particular problem could be created in a largely qualitative manner through a participatory process with stakeholders with the aim of producing generalizable scenarios e g wada et al 2019 while a different set of scenarios for the same problem could be created through a largely quantitative process by varying the inputs to the system model of interest e g using an approach such as latin hypercube sampling lhs culley et al 2016 2019 hadka et al 2015 hall et al 2012 herman et al 2015 kasprzyk et al 2013 kwakkel 2017 kwakkel et al 2015 2016b mcphail et al 2018 quinn et al 2017 2018 singh et al 2015 trindade et al 2017 watson and kasprzyk 2017 weaver et al 2013 zeff et al 2014 each of these approaches can lead to vastly different scenarios being produced shepherd et al 2018 for example a participatory approach will generally result in a small number of scenarios in targeted regions of the uncertain inputs space while quantitative approaches e g lhs of scenarios would lead to a large number of scenarios with even coverage of the space recent studies have shown that as is the case for the use of different robustness metrics the use of different sets of scenarios can also result in different robustness values of decision alternatives mcphail et al 2020 quinn et al 2020 reis and shortridge 2020 adding further uncertainty to the way the robustness of decision alternatives is quantified in order to assist analysts and decision makers in performing appropriate robustness analyses mcphail et al 2018 and mcphail et al 2020 developed generalizable quantitative approaches to assessing the sensitivity of the absolute and relative robustness of decision alternatives e g designs policies to the selection of robustness metrics and scenarios respectively however there is still a lack of a holistic procedure that provides guidance to analysts on the best way to identify which of the available decision alternatives is likely to be the most robust consequently the overarching aim of this paper is to develop a generic guidance framework to help identify the robustness of decision alternatives this paper also introduces the rapid robustness analysis producing intelligent decisions software package implementing the proposed guidance framework in a consistent and user friendly manner that enables the most robust decision alternatives to be identified for a given problem the software package complements existing software packages in this robust decision making space including the exploratory modelling em workbench kwakkel 2017 and rhodium hadjimichael et al 2020 we illustrate the guidance framework and software package on a hypothetical lake pollution problem known as the lake problem as it is a simple and well represented case study in the literature carpenter et al 1999 eker and kwakkel 2018 hadka et al 2015 kwakkel 2017 lempert and collins 2007 quinn et al 2017 singh et al 2015 ward et al 2015 consequently the specific objectives of this paper are to 1 develop a generic guidance framework to help identify the most robust decision alternatives for a given decision context 2 describe a software package that enables the guidance framework to be implemented in a consistent and user friendly manner and 3 illustrate the application of the framework and software package on the lake problem the remainder of this paper is organized as follows section 2 introduces the guidance framework for analyzing the robustness of a set of decision alternatives including how to create a custom robustness metric and how to assess the impact of the selection of scenarios and choice of robustness metric section 3 introduces a software package that can be used to implement this guidance and quantitatively and visually assess the impact of the choice of scenarios and robustness metric on the robustness values and rankings of decision alternatives section 4 introduces the lake problem and provides a simple illustration of how the guidance and software package can be applied to an environmental model and conclusions are presented in section 5 2 guidance framework for identifying the most robust decision alternatives at the heart of the proposed framework for assisting with the identification of the robustness of decision alternatives is the calculation of different robustness metrics the calculation of these metrics requires scenarios decision alternatives i e plans policies solutions and one or more quantitative metrics e g reliability or vulnerability which can be used to determine the level of performance of each decision alternative in each individual scenario herman et al 2015 mcphail et al 2018 fig 1 shows the processes through which these three inputs are used to calculate the robustness of each decision alternative i e the system performance across all scenarios calculation of robustness consists of two main steps 1 the use of a system model to calculate each decision alternative s performance in each scenario followed by 2 the combination of these performance values in order to calculate a single robustness value while these steps are identical for each robustness metric different robustness metrics require the selection of different options at each of one of three transformations 1 performance value transformation 2 scenario subset selection and 3 aggregation of performance values mcphail et al 2018 fig 1 at the first transformation the options are whether to use the raw values of system performance or whether to alter these values using regret or satisficing transforms at the second transformation the choice is which subset of the available scenarios to use in the calculation of the robustness metric at the third transformation the options are whether to combine the transformed performance values over the selected scenarios using a measure of the level of performance such as the mean or a measure of variability in performance such as the standard deviation the proposed guidance framework for assisting with the identification of the most robust decision alternatives is given in fig 2 the framework is designed to be as generic as possible catering to the level of knowledge of the decision makers including the following situations where the most appropriate robustness metric for a particular problem is already fixed or pre selected section 2 1 where a range of robustness metrics are to be considered e g where decision makers are either interested in understanding multiple aspects of robustness via different robustness metrics or cannot decide on which robustness metric is most appropriate from some set of robustness metrics section 2 2 or where the most appropriate robustness metric is yet to be determined based on the different attributes of the decision context i e the properties of the problem such as system thresholds and the preferences of the decision maker s e g preferred levels of risk aversion section 2 3 the framework also caters to situations where the scenarios under which system performance is to be calculated are already selected and situations where the influence of different sets of scenarios on the robustness of decision alternatives is to be considered e g situations where one wishes to know the sensitivity of a particular decision outcome to the selection of scenarios for analysis it should be noted that the proposed framework assumes that the decision alternatives to be considered have already been selected and that the relevant performance metrics for these decision alternatives have been calculated 2 1 robustness metric is already pre selected the process of identifying the decision alternative that has the highest relative robustness commences with the candidate set of decision alternatives for which the relative robustness is to be calculated the first decision point in this process is whether the robustness metric to be used in the assessment has been pre selected fig 2 box 2 if an appropriate metric has already been selected the next decision point in determining the robustness of decision alternatives is whether the set of scenarios to be used to determine the performance of the decision alternatives under consideration is fixed pre selected or not fig 2 box 6 if the set of scenarios is pre selected the robustness of each decision alternative can be calculated by combining its performance over the selected scenarios with the aid of the selected robustness metric then the alternative with the highest robustness value can be selected fig 2 boxes 10 and 15 if it is not clear which scenarios should be used for the robustness calculation the sensitivity of the relative robustness values of the different decision alternatives can be determined for different user defined scenario sets using the approach of mcphail et al 2020 fig 2 box 13 visualizations of the relative ranking of the decision alternatives can be used to determine using human judgement whether the choice of candidate scenario set matters fig 2 box 14 as illustrated in mcphail et al 2020 if the choice of candidate scenarios does not matter because the visualizations indicate that the decision alternatives are ranked similarly regardless of which scenarios are selected then the decision alternative that is considered most robust can be easily selected fig 2 box 15 however if the choice of scenarios does affect the relative robustness of the decision alternatives of interest then depending on the degree of sensitivity of the relative robustness of the different decision alternatives to the selected scenario sets some degree of judgement will be required to determine which decision alternative is considered most robust or which decision alternatives have an acceptable level of robustness fig 2 box 16 or it might be concluded that it is not possible to identify which decision alternative is most robust note that in the situation where a robustness metric is known or pre selected it may still be useful to consider the pathways through fig 2 where the robustness metric is not known this would provide extra information about the system and the impact of the selected robustness metric on the robustness and rankings as described below 2 2 there is a range of robustness metrics under consideration if the robustness metric to be used is not known or pre selected the key decision point is whether there is a known or pre selected set of alternative robustness metrics to be considered in the analysis fig 2 box 3 if there is a pre selected set of robustness metrics for consideration the next decision point is whether there is a fixed pre selected set of scenarios or not fig 2 box 5 if there is a pre selected set of scenarios the stability of the relative robustness of the decision alternatives under consideration can be calculated for the selected robustness metrics over the selected scenarios using the approach of mcphail et al 2018 fig 2 box 12 visualizations of the relative ranking of the decision alternatives can be used to determine whether the choice of candidate robustness metrics matters fig 2 box 14 as illustrated in mcphail et al 2020 and further discussed in sections 3 and 4 if the choice of robustness metrics does not matter because the visualizations indicate that the decision alternatives are ranked similarly regardless of which robustness metric is used then the decision alternative that is considered most robust can be selected easily fig 2 box 15 however if the robustness metric does affect the relative robustness of the decision alternatives of interest then depending on the degree to which this has an effect some degree of judgement will be required to determine which alternative is most robust or which decision alternatives have an acceptable level of robustness and it is recommended that the process for identifying the most appropriate robustness metric for the decision context under consideration introduced in fig 3 and discussed below be applied and that the analysis be repeated for the selected robustness metric fig 2 box 16 if the set of scenarios to be used are not fixed or pre selected the sensitivity of the relative robustness values of the different decision alternatives to the different user defined scenario sets and robustness values can be determined using the approach of mcphail et al 2020 fig 2 box 11 again the visualizations as illustrated in mcphail et al 2020 and further discussed in sections 3 and 4 allow the decision maker to see whether the candidate sets of scenarios and candidate robustness metrics have a significant effect on relative robustness fig 2 box 14 if the selection of scenarios and robustness metrics has an insignificant effect on the rankings the most robust decision alternative can be selected easily fig 2 box 15 however if scenario and robustness metric selection have an effect on relative robustness then depending on the degree to which this is the case some degree of judgement will be required to determine which decision alternative is most robust or which decision alternatives have acceptable levels of robustness and it is recommended that the most appropriate robustness metric is used to help determine this fig 2 box 16 2 3 the robustness metric s are yet to be determined if the set of alternative robustness metrics to be considered in the analysis is yet to be determined fig 2 box 3 the most appropriate robustness metric to be used for each individual performance metric can be determined by selecting the most appropriate options at each of the three transformations in fig 1 with the aid of the guidance in fig 3 and the corresponding equations in table 1 fig 2 box 4 it should be noted that this guidance and corresponding equations can be used to derive many of the established robustness metrics but other pathways through the guidance frameworks may lead to novel robustness metrics the first step in this process is to determine whether there is a meaningful performance threshold in the problem under consideration for example in a water supply system the sustainable yield must be greater than demand and thus the required demand becomes a constraint for the problem in this case the question then becomes whether solutions can be assessed using a pass or fail criterion or whether the magnitude of the failure is important in the previous example a water supply system would be deemed to fail if demand was greater than the sustainable yield so all decision alternatives could be classified as passing or failing in each scenario alternatively a decision maker looking at a water supply system could choose to set a threshold as the point where supply is low enough to cause water restrictions in which case the magnitude of failure does matter since less water would mean greater water restrictions if there is no performance threshold then the question is whether the aim is to maximize performance or avoid making the wrong decision by avoiding making the wrong decision we are referring to some decision makers who may have a desire to avoid selecting decision alternatives if there is a potential that with hindsight the decision maker could be criticized for having made the wrong decision even if at the time of making the decision it appeared to be a reasonable option with the available information for example many publicly owned water authorities face intense public scrutiny and for that reason some decision makers may want to avoid making decisions e g large capital expenditure projects such as a desalination plant for water security that could be perceived to be wrong after the fact e g an unnecessary expenditure because climate change or population growth eventuates to be less than expected decision makers in this situation may prefer to choose a decision alternative that is not the best in any single scenario but is never far from the best decision alternative in extreme good or bad scenarios the next step in fig 3 is to determine whether it is most important to get an indication of the level of performance or the range or variability of performance across multiple plausible futures generally the former is of greatest importance but the latter may also be important as an additional robustness metric given that decision makers would generally prefer to know the precise outcome of a particular decision rather than a highly uncertain outcome nevertheless if the range of performance is considered important it would generally be considered as a secondary metric to be used in addition to a robustness metric that indicates the level of performance for example in a water supply system it would be most important for decision makers to have an indication of how much water each decision alternative will supply but as an additional metric the decision makers may opt to choose a decision alternative with a slightly lower performance if the range of performance values is smaller across the different scenarios since they would have greater confidence in the outcome of their decision regardless of which scenario is realized in this case decision makers could consider both robustness metrics in their decision making in the case where an indication of the level of performance is chosen as being most important this is based on the level of risk tolerance or risk aversion required for the problem or preferred by the decision maker often a high level of risk aversion is warranted when the consequences of failure are very high for example the design of a water supply system would require a high level of risk aversion in contrast the level of risk aversion associated with the design of a stormwater system for a road in a remote area would generally be considerably less alternatively the level of risk aversion may also be a matter of personal preference with some decision makers being more tolerant of risk than others or it may be a matter of regulation where government set some minimum level of risk aversion this scale of risk aversion and risk tolerance can be represented in the selection of an appropriate robustness metric by choosing a percentile between 0 and 100 with 0 reflecting the worst case scenario extreme risk aversion for each decision alternative i e 0 of scenarios have worse performance and 100 reflecting the best case scenario extreme risk tolerance it must be noted that unlike a probabilistic assessment of level of performance percentiles that are used for robustness metrics are reflective of relative not absolute risk for example the 50th percentile does not reflect the median level of performance that can be expected in future however it does represent a level of performance that is worse than the 90th percentile and therefore is more risk averse than selecting the 90th percentile once the most appropriate custom robustness metric has been determined based on the attributes of the decision context the properties of the problem and the preferences of the decision maker with the aid of the process in fig 3 the next decision point is whether the scenarios under which the performance of the decision alternatives under consideration should be evaluated are known or not fig 2 box 5 from here the same process is followed as if the robustness metric was known in advance as described above leading to a scenario analysis fig 2 box 13 if the scenarios are unknown and the selection of the most robust decision alternative or decision alternatives of acceptable levels of robustness if the scenarios are known fig 2 boxes 10 and 15 as with the selection of a performance metric in any problem including deterministic and probabilistic problems it is entirely possible that decision makers will not be able to agree on which metric to use i e in the case of selecting a robustness metric which transformations are most appropriate for creating a robustness metric decision makers may choose to use multiple metrics to consider multiple points of view such as using both reliability and vulnerability to measure performance in a probabilistic uncertainty problem 3 the rapid software package the rapid robustness analysis producing intelligent decisions python software package implements the generic guidance framework introduced in fig 2 in a user friendly and consistent manner including functionality to guide the user through the process of creating a custom robustness metric as described in fig 3 rapid is implemented in python which is being used increasingly for scientific modelling because it is a high level general purpose and open source programming language with an emphasis on code readability it also has a very large standard library and a significant repository of third party python packages the fact that the rapid package is written in python also makes it easier for it to interact with many other software packages including two packages for robust decision making analysis the exploratory modelling em workbench kwakkel 2017 and rhodium hadjimichael et al 2020 which are also written in python as the em workbench includes functionality for the generation of decision alternatives i e policy options solutions etc the generation of scenarios i e states of the world plausible futures and vulnerability analyses including scenario discovery feature scoring and sensitivity analyses the em workbench can be used for the creation of all of the inputs needed for the generic guidance framework fig 2 implemented by the rapid software package the gap in the em workbench that the rapid software package fills is to provide simple building blocks for robustness metrics that allow robustness metrics to be constructed in a consistent manner that corresponds to the guidance framework introduced in this paper this also conforms to software best practices such as the unix philosophy which emphasizes smaller more modular software packages rather than one large software package as shown in fig 4 the processes from the guidance framework are implemented across two sub packages metrics and analysis colored purple and green respectively in fig 4 the sub package metrics contains functions implementing each of the three transformations required for the calculation of robustness metrics fig 1 see table 2 for available options at each of the three transformations this enables user defined custom robustness metrics to be developed see table 2 for available options at each of the three transformations including those obtained by following the process outlined in fig 3 either by manually selecting the transformations and combining them using the custom r metric function or by interacting with the guidance helper function guidance to r which steps through the process in fig 3 a number of commonly used robustness metrics have also been pre programmed see table 3 for these metrics as well as the corresponding choices at each of the three transformations these robustness metrics can then be used to calculate the robustness values for given decision alternatives scenarios and performance metrics as highlighted in fig 1 the analysis sub package colored green in fig 4 contains the quantitative methods and visualizations for assessing the sensitivity of the relative robustness values of different decision alternatives to the choice of robustness metrics and or scenario sets for the assessment of the impact of scenario selection on robustness values the software package uses the approach outlined by mcphail et al 2020 that is the software package calculates the difference in robustness values when the robustness is calculated using two different sets of scenarios first for each decision alternative l i one can calculate robustness r using one set of scenarios s a then calculate the robustness again with a second set of scenarios s b and finally compare the relative difference between the two robustness values we use the average relative difference Œ¥ across all n decision alternatives Œ¥ i 1 n r l i s a r l i s b r l i s a r l i s b 2 n 100 similarly for the assessment of the impact that scenario selection has on the rankings of the decision alternatives we follow mcphail et al 2020 using kendall s tau b ranking correlation to determine the difference in rankings when robustness is calculated using two different sets of scenarios kendall s tau b ranking has a range between 1 and 1 inclusive where 1 indicates that all decision alternatives have opposite rankings 1 indicates that the rankings are exactly the same and 0 implies that there is no correlation between the rankings specifically kendall s tau b metric is used to compare two sets of robustness values one calculated using a set of scenarios s a and the other calculated using a different set of scenarios s b r l 1 s a r l 2 s a r l n s a r l 1 s b r l 2 s b r l n s b similarly kendall s tau b ranking can be used to assess the difference in rankings when robustness is calculated using two different robustness metrics rather than two different sets of scenarios as considered above as recommended by mcphail et al 2018 as a quantitative alternative to the comparison of robustness metrics using visual methods such as parallel axes plots giuliani and castelletti 2016 specifically kendall s tau b metric is used to compare two sets of robustness values one calculated using a robustness metric r 1 and the other calculated using a different robustness metric r 2 r 1 l 1 s r 1 l 2 s r 1 l n s r 2 l 1 s r 2 l 2 s r 2 l n s note that since we are comparing different robustness metrics they can be in different scales or units therefore the relative difference in robustness values cannot be calculated unlike when assessing the impact of scenario selections on robustness values where a single robustness metric is used and therefore the values can be compared directly the structure of the two sub packages mentioned above i e metrics and analysis is as follows metrics a sub package containing functions for each of the three robustness metric transformations common metrics from the literature functions to help build custom robustness metrics and a helper function which asks the user the questions from the guidance provided in section 2 this sub package is structured as o transforms a sub package split into the three transformations t1 t2 t3 as three separate modules the t1 t2 and t3 sub packages which implement the transformations listed in table 2 note that if the aim is to minimize the performance value e g if cost is the measure of performance the sign of the performance values is inverted in all t1 functions because this ensures that the value of all robustness metrics is maximized o common metrics a sub package that calculates the following 11 commonly used robustness metrics mcphail et al 2018 maximin maximax hurwicz s optimism pessimism rule laplace s principle of insufficient reason minimax regret percentile minimax regret mean variance undesirable deviations percentile based skew percentile based kurtosis and starr s domain criterion implementing the three transformations from the transforms sub package as listed in table 3 o custom metrics a module that includes a function custom r metric for creating a custom robustness metric composed of three transformations from the transforms sub package and also provides a helper function for stepping users through the flowchart in fig 3 to create a custom robustness metric that is most appropriate for the decision context under consideration the guidance to r function this helper function asks questions of the user and uses the responses to create the resulting custom robustness metric using the custom r metric function analysis a sub package that enables the influence of different sets of scenarios and robustness metrics on the robustness values and rankings to be determined the scenarios similarity and robustness similarity functions respectively this module also produces plots to visualize the influence that the scenarios and robustness metrics have including i the delta plot function for plotting the relative difference in robustness values i e the deltas caused by different scenario selections or robustness metrics and ii the tau plot function for plotting the ranking similarity i e the kendall s tau b correlation from different robustness metrics both functions explained in more detail above a number of examples using the software package are also contained within the package including a multi objective robust optimization of the lake problem also explored in section 4 a common hypothetical environmental modelling problem used in the environmental systems modelling literature 4 the lake problem 4 1 background the examples directory in the rapid package includes the lake problem as an example of common usage of the package the lake problem is a hypothetical stylized model that is well represented in the literature carpenter et al 1999 eker and kwakkel 2018 hadka et al 2015 kwakkel 2017 lempert and collins 2007 mcphail et al 2020 quinn et al 2017 singh et al 2015 ward et al 2015 and represents a city that must decide the amount of pollution that it releases into a lake there are four competing objectives 1 the average concentration of phosphorous in the lake 2 the frequency of pollution levels exceeding a critical threshold i e the reliability 3 the economic benefit i e economic utility of polluting the lake and 4 a penalty for if the change in level of pollution is too high from year to year i e a measure of inertia of the pollution to help achieve more realistic and appropriate solutions both deep and stochastic uncertainties are present for the natural inflows of pollution into the lake the natural removal and recycling rates of pollution in the lake and the discount rate for the economic benefits to illustrate the generic guidance framework on the lake problem we follow several different pathways through the framework fig 2 including the situations where 1 section 4 2 the robustness metric is unknown and there are no candidate robustness metrics under consideration the method for generating the scenarios is known 2 section 4 3 the robustness metric is unknown and there are no candidate robustness metrics under consideration there are multiple candidate sets of scenarios 3 section 4 4 the robustness metric is unknown however there are multiple candidate robustness metrics the method for generating the scenarios is known 4 2 no candidate robustness metrics but scenario generation method known following the guidance framework we consider a situation in which we aim to use an optimization process fig 4 box 18 to determine a set of robust decision alternatives in this situation we also assume that the robustness metric is unknown fig 4 box 2 and that there are no candidate robustness metrics fig 4 box 3 leading to box 4 in fig 4 here we deviate from the em workbench kwakkel 2017 example of the lake problem which used standard robustness metrics for each of the objectives in our example we create a custom robustness metric by following the guidelines in fig 3 note that the creation of these custom robustness metrics is illustrative of how to follow the guidance and uses many assumptions about decision maker preferences that are not present in previous formulations of the lake problem also note that we have created one robustness metric for each of the four lake problem performance metrics but this need not be the case first for the average concentration of the phosphorous in the lake we decide that there is no meaningful threshold note that some studies have created a threshold for this objective and that we are most interested in making the best decision which gives us the identity transform for t1 we are looking for an indication of the level of performance leading to the identity transform for t3 and are relatively risk averse so the 25th percentile is used for t2 also see summary in table 4 for the reliability we assume a situation where a requirement for the project is a minimum of 80 reliability i e a decision alternative performs satisfactorily in an individual scenario only if pollution remains below the critical threshold for 80 of the time and that this requirement should be met in as many scenarios as possible thus the t1 transformation is the satisficing transform and the t3 transformation is the mean it is also decided that the aim is to understand what percentage of all scenarios under consideration have acceptable performance and so all scenarios are selected for t2 for the economic utility it is assumed that a level of 0 75 is required the economic utility is dimensionless in this study but 0 75 represents some minimum economic benefit that must be achieved and that any level lower than this will have significant consequences therefore the satisficing regret transform is used since this can accommodate the threshold of 0 75 and penalizes decision alternatives in each scenario that fail to achieve this the level of performance i e the level of potential regret is most important and therefore the identity transform is used for t3 it is also assumed that the decision maker has a moderate level of risk aversion for this objective and t2 is the 50th percentile of performance i e regret the inertia is a measure of how much the decision alternative options vary from year to year it is preferred that there are no significant changes in the level of pollution from one year to the next we are not using a specific threshold for this although some other studies have and the objective of the decision maker is to make the best decision regarding the level of performance level of inertia therefore the identity transform is chosen for t1 and t3 again the level of risk aversion is moderate for this objective and thus the 50th percentile is chosen for t2 returning to the overarching guidance framework for robustness analysis figs 2 and 4 now that we have the robustness metrics fig 4 box 4 and the scenarios are known fig 4 box 6 we can calculate robustness using the selected scenarios and selected custom robustness metrics fig 4 box 10 to illustrate this with the rapid software package we build upon an example of the lake problem that is included in the em workbench kwakkel 2017 with the following methodology 1 using the em workbench we formulate the model e g uncertain parameters objectives etc 2 using the rapid package we create the custom robustness metrics defined above in table 4 3 using the em workbench we formulate an optimization problem with the formulated model from step 1 and custom robustness metrics from step 2 4 using the em workbench we run the optimization to determine the most robust decision alternatives note that this means we begin with random decision alternatives in fig 4 box1 but then the em workbench refines these decision alternatives using the feedback loop with box 18 for step 1 the lake problem was specified in the same manner as in the em workbench example i e the uncertain parameters options for the decision alternatives and the performance objectives were defined in the same way using the em workbench functionality for defining a model image 1 for step 2 the custom robustness metrics defined in table 4 were first specified using the rapid package and then put into the form required for the em workbench note that when defining these custom metrics it was possible to use any combination of the three robustness metric transformations from the guidance for decision makers fig 3 and defined in table 1 these metrics can be defined using code as shown or can also be created using the metrics guidance to r function this function asks the user the questions from the flow chart in fig 3 guiding them to the creation of the robustness metric best suited for the problem that can then be used in subsequent analyses as shown in fig 5 the output from the metrics guidance to r function is the same as the output from the metrics custom r metric function in the example code image 4 as per the em workbench example which uses many objective robust optimization moro once the model has been formulated and the robustness metrics have been defined the next step is to use the em workbench to create a set of scenarios formulate an optimization problem and then run that optimization problem to find optimally robust decision alternatives this corresponds to the loop formed by box 18 in fig 4 the results found from this process are shown in fig 6 note again that the robustness metric transformations from the rapid software package ensure that a higher robustness value is always better e g we seek to minimize vulnerability but the sign for the robustness metric for vulnerability is switched so that we are aiming to maximize the robustness value the pareto front fig 6 shows expected relationships between objectives for example better vulnerability also results in better reliability but a worse result for the economic utility the relationship between the inertia and the other three objectives is weaker image 5 in this example of following the guidance framework figs 2 and 4 we showed that with no known robustness metric or set of candidate robustness metrics we could create a set of custom robustness metrics that were best suited to the problem table 4 using the guidance for creating a custom robustness metric fig 3 to determine the appropriate robustness metric transformations from table 2 we then created these robustness metrics in a systematic manner using the rapid software package and used these newly created robustness metrics in conjunction with another software package the em workbench to run a robust optimization and develop a pareto front of optimal decision alternatives 4 3 no candidate robustness metrics and multiple candidate scenario sets again following the guidance framework we use the optimal decision alternatives from the previous section and assume a situation in which the robustness metric is unknown fig 4 box 2 and there are no candidate robustness metrics fig 4 box 3 leading to box 4 in fig 4 here we create custom robustness metrics as per section 4 2 leading to the robustness metrics in table 4 unlike in section 4 2 in this section we consider a situation where there are multiple candidate sets of scenarios fig 4 box 6 e g in order to increase the diversity of considered uncertainties xexakis et al 2020 this situation could occur where the decision makers have identified different sets of scenarios that could all be appropriate for the problem or the situation where different decision makers create different sets of scenarios for the problem we note that while we will refer to the decision alternatives from the previous section as optimal they were optimal for the scenarios and robustness metrics in the previous section and for the specific formulation of this optimization problem maier et al 2018 they may not be optimal in this proceeding section different sets of scenarios correspond to different sets of points within the space of uncertain model inputs mcphail et al 2020 because these points are inputs to the calculation of robustness see fig 1 different sets of scenarios can lead to differences in robustness as a simplified illustration of this we create five candidate sets of 20 scenarios where each set is sampled from the uncertain variable space using the em workbench package with latin hypercube sampling we then evaluate the optimal decision alternatives from section 4 2 in all 100 scenarios using the em workbench package and calculate the robustness for all 5 scenario sets and all decision alternatives using the custom robustness metrics created in section 4 2 using the rapid package fig 4 box 9 note that for simplicity we only focus on the vulnerability objective from here on the same analysis could be applied to each of the four objectives image 6 image 2 returning to the robustness analysis guidance framework this brings us to box 13 in fig 4 where we use the analysis module of the rapid package to evaluate the relative difference in robustness values and the kendall s tau b rank correlation for determining the ranking similarity as described in section 3 the analysis module also enables us to visualize the influence of the scenarios by creating heatmaps that show all combinations of candidate sets of scenarios see fig 7 a and b the diagonal of the heatmaps is each candidate scenario set compared to itself and therefore the relative difference is 0 indicated by purple in fig 7 a and the ranking correlation is 1 indicated by blue in fig 7 b as expected from fig 7 a we can see that for the other comparisons of the scenario sets the relative difference in robustness values is very high in general indicated by mostly orange squares 30 difference in robustness values however there are some cases e g scenario sets 1 and 5 and scenario sets 4 and 5 that are more similar than the rest indicated by the green note that despite a high difference in robustness values fig 7 b indicates that the rankings of the decision alternatives are very stable consistent with mcphail et al 2020 given that all five candidate sets of scenarios were sampled using latin hypercube sampling it is interesting that the relative difference in robustness is so high in fig 7 a if the robustness values were important for the decision making process it would be difficult to be sure of the actual robustness values because the values would depend on which set of scenarios is being considered leading to fig 4 box 16 there are many reasons why the relative difference could be high including dissimilarity in the coverage of the scenario space and discontinuities in performance space mcphail et al 2020 in this example it is likely to be the former of these reasons because the number of scenarios in each set is small running the same code as above but with a larger number of scenarios 100 scenarios per set rather than 20 scenarios per set in fig 7 a we produce the heatmap shown in fig 7 c with the larger number of scenarios the relative difference is significantly lower in general likely due to a more similar coverage of the scenario space indicated by the greater number of blue and green squares and the smaller number of orange squares in this case we move from box 14 to boxes 15 and 17 in fig 4 being able to accurately determine the robustness of the decision alternatives note that a greater number of scenarios will not always allow a decision maker to accurately determine the robustness of the decision alternatives as indicated by the comparisons of scenario sets 4 and 5 in fig 7 c in this case we move from box 14 to box 16 in which case we need to use human judgement to determine which decision alternatives are the most robust alternatively if we are simply interested in the rankings of the solutions see fig 7 b then we would be able to move from box 14 to boxes 15 and 17 without increasing the number of scenarios assuming that we judge the kendall s tau b values approximately in the range between 0 7 and 1 0 to be sufficiently high for our purposes in this second example of following the guidance framework figs 2 and 4 we showed that with multiple candidate sets of scenarios we could use the rapid software package to evaluate the influence these candidate sets of scenarios had on both the robustness values and rankings using the visualizations produced by the software package we were then able to determine that the relative robustness values of different decision alternatives was not substantially affected by the different scenario sets fig 7 b giving confidence to decision makers and enabling the most robust decision alternative to be identified 4 4 multiple candidate robustness metrics and a known set of scenarios in this situation we assume that the robustness metric is unknown fig 4 box 2 but that there are multiple candidate robustness metrics fig 4 box 5 and that the set of scenarios is known leading to box 8 in fig 4 note that if there were multiple candidate sets of scenarios the analysis would be a combination of the following method and the method in section 4 3 we create the candidate robustness metrics using the rapid software package retaining the original robustness metric for the vulnerability determined in section 4 2 table 4 using the metrics custom r metric module and four traditional robustness metrics as the other candidate metrics including the maximax laplace s principle of insufficient reason minimax regret and percentile based kurtosis robustness metrics all included in the metrics common metrics module as with the previous examples these metrics were calculated evaluated this time across a known set of 100 scenarios sampled using latin hypercube sampling and visualized using the rapid package see fig 7 d image 3 in the visualization of the similarity in rankings fig 7 d the diagonal shows full ranking similarity a value of 1 indicated by blue because that is where each robustness metric is being compared to itself most of the metrics also show high levels of ranking similarity with each other with the exception of the percentile based kurtosis metric which shows a slight negative correlation with all other metrics indicated by the slightly red squares this potentially leads us from box 14 to box 16 in fig 4 because it is unknown which ranking is the one that we should follow the rankings provided by the percentile based kurtosis or the rankings provided by the rest of the metrics again using our judgement we decide that the percentile based kurtosis does not reflect the needs of the decision makers as much as the other robustness metrics do because the t3 transformation does not reflect the need to get an indication of the level of performance as explained by fig 3 and by mcphail et al 2018 also since all of the other candidate solutions generally agree with the custom robustness metric it follows that we can rely on this custom metric to determine which decision alternative is most robust fig 4 box 16 in this final illustration of using the guidance framework figs 2 and 4 and rapid software package we showed that with multiple candidate robustness metrics we can use the software package to evaluate the influence these robustness metrics have on the rankings of the decision alternatives using the visualizations produced by the software package we were then able to determine whether or not the influence was sufficiently large to affect these rankings all three of the simple examples considered show that the rapid package is easy to use and can be used in conjunction with related software packages such as the em workbench they also show that the rapid package is a practical tool for systematically following the guidance framework in figs 2 and 4 the guidance for creating robustness metrics in fig 3 shown in section 4 2 assessing the influence of candidate sets of scenarios on the robustness values and rankings shown in section 4 3 and assessing the influence of candidate robustness metrics on the robustness rankings of decision alternatives shown in section 4 4 note that since this is a multi objective problem there is no single most robust decision alternative and decision makers can only narrow down the choice of decision alternatives to those that represent the best trade offs between the four objectives it is likely that the decision of a final decision alternative would require further analysis by other potentially more senior decision makers to determine which trade off represents the best strategic choice 5 summary and conclusions robustness is important in the long term planning of environmental systems however there is a variety of metrics that can be used to calculate the robustness of a set of decision alternatives and recent research has shown that the choice of metric can affect the ranking of decision alternatives similarly there is a variety of approaches to selecting or generating scenarios which are an input to the calculation of robustness and the chosen approach has also been shown to have an effect on the robustness values and rankings of decision alternatives despite the uncertainty associated with the selection of scenarios and robustness metrics when determining the rankings of decision alternatives under deep uncertainty no guidance exists for decision makers on which choices to make as a response to this need for guidance this paper proposes a generic guidance framework to assist decision makers in the identification of robust decision alternatives fig 2 this framework caters to a variety of situations where the scenarios and or robustness metrics are known or not known the framework includes guidance on how to create a custom robustness metric for the problem at hand fig 3 based on the attributes of the problem e g the presence of performance thresholds tipping points or the objectives of the problem as well as the preferences of the decision maker e g the level of risk aversion the output from the guidance for the creation of a custom robustness metric is three robustness metric transformations table 1 which form the robustness metric when combined fig 1 the overarching guidance framework also identifies situations where quantitative analyses can be used to determine the influence that the selection of scenarios and or the choice of robustness metric has on the rankings of decision alternatives this paper also introduces an open source software package the rapid robustness analysis producing intelligent decisions package to assist with the consistency and ease of use of implementing the guidance framework see fig 4 the software package includes a module for the creation of custom robustness metrics using a wide range of robustness metric transformations table 2 including a function that leads the user through the guidance of how to create the robustness metric most suited for the problem at hand fig 3 it also includes a variety of commonly used traditional robustness metrics from the literature table 3 the software package also contains a module for the calculation and visualization of the impact of the selection of scenarios and choice of robustness metric on robustness values and rankings to illustrate the implementation of the guidance framework and rapid software package we consider the lake problem a hypothetical lake pollution problem commonly used in literature we use the guidance in fig 3 to create custom robustness metrics for the lake problem based on hypothetical problem attributes and decision maker preferences table 4 in conjunction with the em workbench kwakkel 2017 we use these robustness metrics as objectives in a robust optimization to create a set of robust decision alternatives as an example of the utility of the guidance framework and software package we use these optimal decision alternatives to consider a situation where there are multiple sets of scenarios under consideration using the rapid software package we visualize the impact of these different sets of scenarios showing that the robustness values are affected fig 7 a but rankings of the decision alternatives are not fig 7 b providing confidence to decision makers that the most robust decision alternative has been identified we also show that when using a larger set of scenarios the impact of the set of scenarios on the robustness values is greatly decreased fig 7 c in another example to highlight the utility of the guidance framework and software package we consider a situation where there is a variety of candidate robustness metrics we use the framework and software package to visualize the impact of the choice of robustness metric fig 7 d showing that most of the metrics agree on the rankings of the decision alternatives again providing confidence to decision makers that the most robust solution has been identified this guidance framework and software package assist decision makers in the identification of robust decision alternatives it does so in a systematic way and the software package increases the consistency and ease of use of implementing the guidance the guidance framework and software package are generic and cater to a wide variety of circumstances where the robustness metrics and or scenarios may or may not be known greatly increasing the accessibility of robustness analyses and techniques to decision makers after identifying the most robust decision alternatives or those decision alternatives that represent the best trade offs across multiple objectives decision makers are able to then explore those decision alternatives in more detail with these selected decision alternatives decision makers can better understand what makes some decision alternatives more robust explain to other stakeholders what it is about these decisions that makes them robust based on the information obtained from the guidance framework software availability the lake model is widely available on github in multiple repositories including in the emaworkbench https github com quaquel emaworkbench the rapid robustness analysis producing intelligent decisions software package is available on github https github com cameronmcphail rapid and in the python package index pypi https pypi org project rapidrobustness archived at doi org 10 5281 zenodo 4171495 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements thanks is given to sa water corporation australia who support the research of cameron mcphail through water research australia and thanks is also given to water research australia the authors would also like to thank andrea castelletti and matteo giuliani both from politecnico di milano for their important conceptual contributions to this research 
25810,to aid decision making about environmental systems under deep uncertainty robustness metrics are commonly used to represent system performance over a number of scenarios however there are many robustness metrics and many ways of generating scenarios making it difficult to know which to choose in order to quantify system robustness and to make robust decisions to address this shortcoming we introduce a generic guidance framework to assist with the identification of the most robust decision alternatives as well as the rapid robustness analysis producing intelligent decisions software package which is a consistent and easy to use implementation of the framework we illustrate the framework and software package on a hypothetical lake pollution problem known as the lake problem showing how the framework and software package apply to several situations where decision makers may or may not know which scenarios or robustness metrics to use keywords deep uncertainty decision making under uncertainty robustness scenarios 1 introduction the long term planning of environmental systems presents major challenges as it requires decisions to be made despite significant uncertainty about the future state of the world frequently decision makers are operating at the level of deep uncertainty which refers to when deterministic and probabilistic approaches are insufficient for representing future states and the consideration of multiple plausible futures scenarios is required bradfield et al 2005 herman et al 2014 kwakkel et al 2010 kwakkel and haasnoot 2019 lempert 2003 little et al 2018 maier et al 2016 schwarz 1991 van der heijden 1996 varum and melo 2010 walker et al 2013 wright and cairns 2011 implicit in the deep uncertainty paradigm is that probabilities cannot be placed on these scenarios and therefore traditional risk based performance metrics such as reliability vulnerability resilience or expected value cannot be used to quantify the overall level of system performance across all scenarios maier et al 2016 rather deep uncertainty requires robustness metrics to be used which aim to quantify the relative level or variation of system performance across all or targeted scenarios bartholomew and kwakkel 2020 giudici et al 2020 herman et al 2015 kwakkel and haasnoot 2019 lempert 2003 maier et al 2016 mcphail et al 2018 as with traditional performance metrics in deterministic and probabilistic paradigms decision makers aim to choose a solution that has maximal performance robustness or a solution that has an appropriate tradeoff between performance metrics e g robustness vs cost there is a multitude of approaches to quantify system robustness generally by treating the scenarios as a distribution and making implicit probabilistic assumptions including i expected value metrics wald 1951 which indicate an expected level of performance across a range of scenarios ii metrics of higher order moments such as variance and skew e g kwakkel et al 2016a which provide information on how the expected level of performance varies across multiple scenarios iii regret based metrics savage 1951 where the regret of a decision alternative is defined as the difference between the performance of the selected option for a particular plausible condition and the performance of the best possible option for that condition and iv satisficing metrics simon 1956 which identify the range of scenarios that have acceptable performance relative to a threshold a common conclusion from recent research is that different robustness metrics can sometimes lead to decision alternatives being ranked differently making it difficult to determine which decision alternatives are most robust borgomeo et al 2018 drouet et al 2015 giuliani and castelletti 2016 hall et al 2012 herman et al 2015 kwakkel et al 2016a lempert and collins 2007 mcphail et al 2018 roach et al 2016 for example a case study by kwakkel et al 2016a on the transition of the european energy system towards a more sustainable future concluded that there is no clearly superior single robustness metric case specific consideration and system characteristics affect the merits of the various robustness measures this implies that an analyst has to choose carefully which robustness measure is being used and assess its appropriateness given that robustness metrics are calculated over a set of scenarios the choice of scenarios that are used in this calculation can also have an impact on the robustness value obtained in addition to the choice of robustness metric mcphail et al 2018 2020 a common categorization of scenarios is given by b√∂rjeson et al 2006 including the following three types predictive scenarios where the aim is to determine what will happen for example the future state of the world could be based on some future trajectory or change in trajectory due to some event explorative scenarios where the aim is to determine what could happen generally this is done by framing the future in terms of the uncertainties that have the largest effects on system performance but the future can also be unframed maier et al 2016 and normative scenarios where the aim is to determine how can a specific future be realized this is generally focused on interesting future outcomes or failure points for decision alternatives each of these types of scenarios can be created in different ways for example a set of scenarios for a particular problem could be created in a largely qualitative manner through a participatory process with stakeholders with the aim of producing generalizable scenarios e g wada et al 2019 while a different set of scenarios for the same problem could be created through a largely quantitative process by varying the inputs to the system model of interest e g using an approach such as latin hypercube sampling lhs culley et al 2016 2019 hadka et al 2015 hall et al 2012 herman et al 2015 kasprzyk et al 2013 kwakkel 2017 kwakkel et al 2015 2016b mcphail et al 2018 quinn et al 2017 2018 singh et al 2015 trindade et al 2017 watson and kasprzyk 2017 weaver et al 2013 zeff et al 2014 each of these approaches can lead to vastly different scenarios being produced shepherd et al 2018 for example a participatory approach will generally result in a small number of scenarios in targeted regions of the uncertain inputs space while quantitative approaches e g lhs of scenarios would lead to a large number of scenarios with even coverage of the space recent studies have shown that as is the case for the use of different robustness metrics the use of different sets of scenarios can also result in different robustness values of decision alternatives mcphail et al 2020 quinn et al 2020 reis and shortridge 2020 adding further uncertainty to the way the robustness of decision alternatives is quantified in order to assist analysts and decision makers in performing appropriate robustness analyses mcphail et al 2018 and mcphail et al 2020 developed generalizable quantitative approaches to assessing the sensitivity of the absolute and relative robustness of decision alternatives e g designs policies to the selection of robustness metrics and scenarios respectively however there is still a lack of a holistic procedure that provides guidance to analysts on the best way to identify which of the available decision alternatives is likely to be the most robust consequently the overarching aim of this paper is to develop a generic guidance framework to help identify the robustness of decision alternatives this paper also introduces the rapid robustness analysis producing intelligent decisions software package implementing the proposed guidance framework in a consistent and user friendly manner that enables the most robust decision alternatives to be identified for a given problem the software package complements existing software packages in this robust decision making space including the exploratory modelling em workbench kwakkel 2017 and rhodium hadjimichael et al 2020 we illustrate the guidance framework and software package on a hypothetical lake pollution problem known as the lake problem as it is a simple and well represented case study in the literature carpenter et al 1999 eker and kwakkel 2018 hadka et al 2015 kwakkel 2017 lempert and collins 2007 quinn et al 2017 singh et al 2015 ward et al 2015 consequently the specific objectives of this paper are to 1 develop a generic guidance framework to help identify the most robust decision alternatives for a given decision context 2 describe a software package that enables the guidance framework to be implemented in a consistent and user friendly manner and 3 illustrate the application of the framework and software package on the lake problem the remainder of this paper is organized as follows section 2 introduces the guidance framework for analyzing the robustness of a set of decision alternatives including how to create a custom robustness metric and how to assess the impact of the selection of scenarios and choice of robustness metric section 3 introduces a software package that can be used to implement this guidance and quantitatively and visually assess the impact of the choice of scenarios and robustness metric on the robustness values and rankings of decision alternatives section 4 introduces the lake problem and provides a simple illustration of how the guidance and software package can be applied to an environmental model and conclusions are presented in section 5 2 guidance framework for identifying the most robust decision alternatives at the heart of the proposed framework for assisting with the identification of the robustness of decision alternatives is the calculation of different robustness metrics the calculation of these metrics requires scenarios decision alternatives i e plans policies solutions and one or more quantitative metrics e g reliability or vulnerability which can be used to determine the level of performance of each decision alternative in each individual scenario herman et al 2015 mcphail et al 2018 fig 1 shows the processes through which these three inputs are used to calculate the robustness of each decision alternative i e the system performance across all scenarios calculation of robustness consists of two main steps 1 the use of a system model to calculate each decision alternative s performance in each scenario followed by 2 the combination of these performance values in order to calculate a single robustness value while these steps are identical for each robustness metric different robustness metrics require the selection of different options at each of one of three transformations 1 performance value transformation 2 scenario subset selection and 3 aggregation of performance values mcphail et al 2018 fig 1 at the first transformation the options are whether to use the raw values of system performance or whether to alter these values using regret or satisficing transforms at the second transformation the choice is which subset of the available scenarios to use in the calculation of the robustness metric at the third transformation the options are whether to combine the transformed performance values over the selected scenarios using a measure of the level of performance such as the mean or a measure of variability in performance such as the standard deviation the proposed guidance framework for assisting with the identification of the most robust decision alternatives is given in fig 2 the framework is designed to be as generic as possible catering to the level of knowledge of the decision makers including the following situations where the most appropriate robustness metric for a particular problem is already fixed or pre selected section 2 1 where a range of robustness metrics are to be considered e g where decision makers are either interested in understanding multiple aspects of robustness via different robustness metrics or cannot decide on which robustness metric is most appropriate from some set of robustness metrics section 2 2 or where the most appropriate robustness metric is yet to be determined based on the different attributes of the decision context i e the properties of the problem such as system thresholds and the preferences of the decision maker s e g preferred levels of risk aversion section 2 3 the framework also caters to situations where the scenarios under which system performance is to be calculated are already selected and situations where the influence of different sets of scenarios on the robustness of decision alternatives is to be considered e g situations where one wishes to know the sensitivity of a particular decision outcome to the selection of scenarios for analysis it should be noted that the proposed framework assumes that the decision alternatives to be considered have already been selected and that the relevant performance metrics for these decision alternatives have been calculated 2 1 robustness metric is already pre selected the process of identifying the decision alternative that has the highest relative robustness commences with the candidate set of decision alternatives for which the relative robustness is to be calculated the first decision point in this process is whether the robustness metric to be used in the assessment has been pre selected fig 2 box 2 if an appropriate metric has already been selected the next decision point in determining the robustness of decision alternatives is whether the set of scenarios to be used to determine the performance of the decision alternatives under consideration is fixed pre selected or not fig 2 box 6 if the set of scenarios is pre selected the robustness of each decision alternative can be calculated by combining its performance over the selected scenarios with the aid of the selected robustness metric then the alternative with the highest robustness value can be selected fig 2 boxes 10 and 15 if it is not clear which scenarios should be used for the robustness calculation the sensitivity of the relative robustness values of the different decision alternatives can be determined for different user defined scenario sets using the approach of mcphail et al 2020 fig 2 box 13 visualizations of the relative ranking of the decision alternatives can be used to determine using human judgement whether the choice of candidate scenario set matters fig 2 box 14 as illustrated in mcphail et al 2020 if the choice of candidate scenarios does not matter because the visualizations indicate that the decision alternatives are ranked similarly regardless of which scenarios are selected then the decision alternative that is considered most robust can be easily selected fig 2 box 15 however if the choice of scenarios does affect the relative robustness of the decision alternatives of interest then depending on the degree of sensitivity of the relative robustness of the different decision alternatives to the selected scenario sets some degree of judgement will be required to determine which decision alternative is considered most robust or which decision alternatives have an acceptable level of robustness fig 2 box 16 or it might be concluded that it is not possible to identify which decision alternative is most robust note that in the situation where a robustness metric is known or pre selected it may still be useful to consider the pathways through fig 2 where the robustness metric is not known this would provide extra information about the system and the impact of the selected robustness metric on the robustness and rankings as described below 2 2 there is a range of robustness metrics under consideration if the robustness metric to be used is not known or pre selected the key decision point is whether there is a known or pre selected set of alternative robustness metrics to be considered in the analysis fig 2 box 3 if there is a pre selected set of robustness metrics for consideration the next decision point is whether there is a fixed pre selected set of scenarios or not fig 2 box 5 if there is a pre selected set of scenarios the stability of the relative robustness of the decision alternatives under consideration can be calculated for the selected robustness metrics over the selected scenarios using the approach of mcphail et al 2018 fig 2 box 12 visualizations of the relative ranking of the decision alternatives can be used to determine whether the choice of candidate robustness metrics matters fig 2 box 14 as illustrated in mcphail et al 2020 and further discussed in sections 3 and 4 if the choice of robustness metrics does not matter because the visualizations indicate that the decision alternatives are ranked similarly regardless of which robustness metric is used then the decision alternative that is considered most robust can be selected easily fig 2 box 15 however if the robustness metric does affect the relative robustness of the decision alternatives of interest then depending on the degree to which this has an effect some degree of judgement will be required to determine which alternative is most robust or which decision alternatives have an acceptable level of robustness and it is recommended that the process for identifying the most appropriate robustness metric for the decision context under consideration introduced in fig 3 and discussed below be applied and that the analysis be repeated for the selected robustness metric fig 2 box 16 if the set of scenarios to be used are not fixed or pre selected the sensitivity of the relative robustness values of the different decision alternatives to the different user defined scenario sets and robustness values can be determined using the approach of mcphail et al 2020 fig 2 box 11 again the visualizations as illustrated in mcphail et al 2020 and further discussed in sections 3 and 4 allow the decision maker to see whether the candidate sets of scenarios and candidate robustness metrics have a significant effect on relative robustness fig 2 box 14 if the selection of scenarios and robustness metrics has an insignificant effect on the rankings the most robust decision alternative can be selected easily fig 2 box 15 however if scenario and robustness metric selection have an effect on relative robustness then depending on the degree to which this is the case some degree of judgement will be required to determine which decision alternative is most robust or which decision alternatives have acceptable levels of robustness and it is recommended that the most appropriate robustness metric is used to help determine this fig 2 box 16 2 3 the robustness metric s are yet to be determined if the set of alternative robustness metrics to be considered in the analysis is yet to be determined fig 2 box 3 the most appropriate robustness metric to be used for each individual performance metric can be determined by selecting the most appropriate options at each of the three transformations in fig 1 with the aid of the guidance in fig 3 and the corresponding equations in table 1 fig 2 box 4 it should be noted that this guidance and corresponding equations can be used to derive many of the established robustness metrics but other pathways through the guidance frameworks may lead to novel robustness metrics the first step in this process is to determine whether there is a meaningful performance threshold in the problem under consideration for example in a water supply system the sustainable yield must be greater than demand and thus the required demand becomes a constraint for the problem in this case the question then becomes whether solutions can be assessed using a pass or fail criterion or whether the magnitude of the failure is important in the previous example a water supply system would be deemed to fail if demand was greater than the sustainable yield so all decision alternatives could be classified as passing or failing in each scenario alternatively a decision maker looking at a water supply system could choose to set a threshold as the point where supply is low enough to cause water restrictions in which case the magnitude of failure does matter since less water would mean greater water restrictions if there is no performance threshold then the question is whether the aim is to maximize performance or avoid making the wrong decision by avoiding making the wrong decision we are referring to some decision makers who may have a desire to avoid selecting decision alternatives if there is a potential that with hindsight the decision maker could be criticized for having made the wrong decision even if at the time of making the decision it appeared to be a reasonable option with the available information for example many publicly owned water authorities face intense public scrutiny and for that reason some decision makers may want to avoid making decisions e g large capital expenditure projects such as a desalination plant for water security that could be perceived to be wrong after the fact e g an unnecessary expenditure because climate change or population growth eventuates to be less than expected decision makers in this situation may prefer to choose a decision alternative that is not the best in any single scenario but is never far from the best decision alternative in extreme good or bad scenarios the next step in fig 3 is to determine whether it is most important to get an indication of the level of performance or the range or variability of performance across multiple plausible futures generally the former is of greatest importance but the latter may also be important as an additional robustness metric given that decision makers would generally prefer to know the precise outcome of a particular decision rather than a highly uncertain outcome nevertheless if the range of performance is considered important it would generally be considered as a secondary metric to be used in addition to a robustness metric that indicates the level of performance for example in a water supply system it would be most important for decision makers to have an indication of how much water each decision alternative will supply but as an additional metric the decision makers may opt to choose a decision alternative with a slightly lower performance if the range of performance values is smaller across the different scenarios since they would have greater confidence in the outcome of their decision regardless of which scenario is realized in this case decision makers could consider both robustness metrics in their decision making in the case where an indication of the level of performance is chosen as being most important this is based on the level of risk tolerance or risk aversion required for the problem or preferred by the decision maker often a high level of risk aversion is warranted when the consequences of failure are very high for example the design of a water supply system would require a high level of risk aversion in contrast the level of risk aversion associated with the design of a stormwater system for a road in a remote area would generally be considerably less alternatively the level of risk aversion may also be a matter of personal preference with some decision makers being more tolerant of risk than others or it may be a matter of regulation where government set some minimum level of risk aversion this scale of risk aversion and risk tolerance can be represented in the selection of an appropriate robustness metric by choosing a percentile between 0 and 100 with 0 reflecting the worst case scenario extreme risk aversion for each decision alternative i e 0 of scenarios have worse performance and 100 reflecting the best case scenario extreme risk tolerance it must be noted that unlike a probabilistic assessment of level of performance percentiles that are used for robustness metrics are reflective of relative not absolute risk for example the 50th percentile does not reflect the median level of performance that can be expected in future however it does represent a level of performance that is worse than the 90th percentile and therefore is more risk averse than selecting the 90th percentile once the most appropriate custom robustness metric has been determined based on the attributes of the decision context the properties of the problem and the preferences of the decision maker with the aid of the process in fig 3 the next decision point is whether the scenarios under which the performance of the decision alternatives under consideration should be evaluated are known or not fig 2 box 5 from here the same process is followed as if the robustness metric was known in advance as described above leading to a scenario analysis fig 2 box 13 if the scenarios are unknown and the selection of the most robust decision alternative or decision alternatives of acceptable levels of robustness if the scenarios are known fig 2 boxes 10 and 15 as with the selection of a performance metric in any problem including deterministic and probabilistic problems it is entirely possible that decision makers will not be able to agree on which metric to use i e in the case of selecting a robustness metric which transformations are most appropriate for creating a robustness metric decision makers may choose to use multiple metrics to consider multiple points of view such as using both reliability and vulnerability to measure performance in a probabilistic uncertainty problem 3 the rapid software package the rapid robustness analysis producing intelligent decisions python software package implements the generic guidance framework introduced in fig 2 in a user friendly and consistent manner including functionality to guide the user through the process of creating a custom robustness metric as described in fig 3 rapid is implemented in python which is being used increasingly for scientific modelling because it is a high level general purpose and open source programming language with an emphasis on code readability it also has a very large standard library and a significant repository of third party python packages the fact that the rapid package is written in python also makes it easier for it to interact with many other software packages including two packages for robust decision making analysis the exploratory modelling em workbench kwakkel 2017 and rhodium hadjimichael et al 2020 which are also written in python as the em workbench includes functionality for the generation of decision alternatives i e policy options solutions etc the generation of scenarios i e states of the world plausible futures and vulnerability analyses including scenario discovery feature scoring and sensitivity analyses the em workbench can be used for the creation of all of the inputs needed for the generic guidance framework fig 2 implemented by the rapid software package the gap in the em workbench that the rapid software package fills is to provide simple building blocks for robustness metrics that allow robustness metrics to be constructed in a consistent manner that corresponds to the guidance framework introduced in this paper this also conforms to software best practices such as the unix philosophy which emphasizes smaller more modular software packages rather than one large software package as shown in fig 4 the processes from the guidance framework are implemented across two sub packages metrics and analysis colored purple and green respectively in fig 4 the sub package metrics contains functions implementing each of the three transformations required for the calculation of robustness metrics fig 1 see table 2 for available options at each of the three transformations this enables user defined custom robustness metrics to be developed see table 2 for available options at each of the three transformations including those obtained by following the process outlined in fig 3 either by manually selecting the transformations and combining them using the custom r metric function or by interacting with the guidance helper function guidance to r which steps through the process in fig 3 a number of commonly used robustness metrics have also been pre programmed see table 3 for these metrics as well as the corresponding choices at each of the three transformations these robustness metrics can then be used to calculate the robustness values for given decision alternatives scenarios and performance metrics as highlighted in fig 1 the analysis sub package colored green in fig 4 contains the quantitative methods and visualizations for assessing the sensitivity of the relative robustness values of different decision alternatives to the choice of robustness metrics and or scenario sets for the assessment of the impact of scenario selection on robustness values the software package uses the approach outlined by mcphail et al 2020 that is the software package calculates the difference in robustness values when the robustness is calculated using two different sets of scenarios first for each decision alternative l i one can calculate robustness r using one set of scenarios s a then calculate the robustness again with a second set of scenarios s b and finally compare the relative difference between the two robustness values we use the average relative difference Œ¥ across all n decision alternatives Œ¥ i 1 n r l i s a r l i s b r l i s a r l i s b 2 n 100 similarly for the assessment of the impact that scenario selection has on the rankings of the decision alternatives we follow mcphail et al 2020 using kendall s tau b ranking correlation to determine the difference in rankings when robustness is calculated using two different sets of scenarios kendall s tau b ranking has a range between 1 and 1 inclusive where 1 indicates that all decision alternatives have opposite rankings 1 indicates that the rankings are exactly the same and 0 implies that there is no correlation between the rankings specifically kendall s tau b metric is used to compare two sets of robustness values one calculated using a set of scenarios s a and the other calculated using a different set of scenarios s b r l 1 s a r l 2 s a r l n s a r l 1 s b r l 2 s b r l n s b similarly kendall s tau b ranking can be used to assess the difference in rankings when robustness is calculated using two different robustness metrics rather than two different sets of scenarios as considered above as recommended by mcphail et al 2018 as a quantitative alternative to the comparison of robustness metrics using visual methods such as parallel axes plots giuliani and castelletti 2016 specifically kendall s tau b metric is used to compare two sets of robustness values one calculated using a robustness metric r 1 and the other calculated using a different robustness metric r 2 r 1 l 1 s r 1 l 2 s r 1 l n s r 2 l 1 s r 2 l 2 s r 2 l n s note that since we are comparing different robustness metrics they can be in different scales or units therefore the relative difference in robustness values cannot be calculated unlike when assessing the impact of scenario selections on robustness values where a single robustness metric is used and therefore the values can be compared directly the structure of the two sub packages mentioned above i e metrics and analysis is as follows metrics a sub package containing functions for each of the three robustness metric transformations common metrics from the literature functions to help build custom robustness metrics and a helper function which asks the user the questions from the guidance provided in section 2 this sub package is structured as o transforms a sub package split into the three transformations t1 t2 t3 as three separate modules the t1 t2 and t3 sub packages which implement the transformations listed in table 2 note that if the aim is to minimize the performance value e g if cost is the measure of performance the sign of the performance values is inverted in all t1 functions because this ensures that the value of all robustness metrics is maximized o common metrics a sub package that calculates the following 11 commonly used robustness metrics mcphail et al 2018 maximin maximax hurwicz s optimism pessimism rule laplace s principle of insufficient reason minimax regret percentile minimax regret mean variance undesirable deviations percentile based skew percentile based kurtosis and starr s domain criterion implementing the three transformations from the transforms sub package as listed in table 3 o custom metrics a module that includes a function custom r metric for creating a custom robustness metric composed of three transformations from the transforms sub package and also provides a helper function for stepping users through the flowchart in fig 3 to create a custom robustness metric that is most appropriate for the decision context under consideration the guidance to r function this helper function asks questions of the user and uses the responses to create the resulting custom robustness metric using the custom r metric function analysis a sub package that enables the influence of different sets of scenarios and robustness metrics on the robustness values and rankings to be determined the scenarios similarity and robustness similarity functions respectively this module also produces plots to visualize the influence that the scenarios and robustness metrics have including i the delta plot function for plotting the relative difference in robustness values i e the deltas caused by different scenario selections or robustness metrics and ii the tau plot function for plotting the ranking similarity i e the kendall s tau b correlation from different robustness metrics both functions explained in more detail above a number of examples using the software package are also contained within the package including a multi objective robust optimization of the lake problem also explored in section 4 a common hypothetical environmental modelling problem used in the environmental systems modelling literature 4 the lake problem 4 1 background the examples directory in the rapid package includes the lake problem as an example of common usage of the package the lake problem is a hypothetical stylized model that is well represented in the literature carpenter et al 1999 eker and kwakkel 2018 hadka et al 2015 kwakkel 2017 lempert and collins 2007 mcphail et al 2020 quinn et al 2017 singh et al 2015 ward et al 2015 and represents a city that must decide the amount of pollution that it releases into a lake there are four competing objectives 1 the average concentration of phosphorous in the lake 2 the frequency of pollution levels exceeding a critical threshold i e the reliability 3 the economic benefit i e economic utility of polluting the lake and 4 a penalty for if the change in level of pollution is too high from year to year i e a measure of inertia of the pollution to help achieve more realistic and appropriate solutions both deep and stochastic uncertainties are present for the natural inflows of pollution into the lake the natural removal and recycling rates of pollution in the lake and the discount rate for the economic benefits to illustrate the generic guidance framework on the lake problem we follow several different pathways through the framework fig 2 including the situations where 1 section 4 2 the robustness metric is unknown and there are no candidate robustness metrics under consideration the method for generating the scenarios is known 2 section 4 3 the robustness metric is unknown and there are no candidate robustness metrics under consideration there are multiple candidate sets of scenarios 3 section 4 4 the robustness metric is unknown however there are multiple candidate robustness metrics the method for generating the scenarios is known 4 2 no candidate robustness metrics but scenario generation method known following the guidance framework we consider a situation in which we aim to use an optimization process fig 4 box 18 to determine a set of robust decision alternatives in this situation we also assume that the robustness metric is unknown fig 4 box 2 and that there are no candidate robustness metrics fig 4 box 3 leading to box 4 in fig 4 here we deviate from the em workbench kwakkel 2017 example of the lake problem which used standard robustness metrics for each of the objectives in our example we create a custom robustness metric by following the guidelines in fig 3 note that the creation of these custom robustness metrics is illustrative of how to follow the guidance and uses many assumptions about decision maker preferences that are not present in previous formulations of the lake problem also note that we have created one robustness metric for each of the four lake problem performance metrics but this need not be the case first for the average concentration of the phosphorous in the lake we decide that there is no meaningful threshold note that some studies have created a threshold for this objective and that we are most interested in making the best decision which gives us the identity transform for t1 we are looking for an indication of the level of performance leading to the identity transform for t3 and are relatively risk averse so the 25th percentile is used for t2 also see summary in table 4 for the reliability we assume a situation where a requirement for the project is a minimum of 80 reliability i e a decision alternative performs satisfactorily in an individual scenario only if pollution remains below the critical threshold for 80 of the time and that this requirement should be met in as many scenarios as possible thus the t1 transformation is the satisficing transform and the t3 transformation is the mean it is also decided that the aim is to understand what percentage of all scenarios under consideration have acceptable performance and so all scenarios are selected for t2 for the economic utility it is assumed that a level of 0 75 is required the economic utility is dimensionless in this study but 0 75 represents some minimum economic benefit that must be achieved and that any level lower than this will have significant consequences therefore the satisficing regret transform is used since this can accommodate the threshold of 0 75 and penalizes decision alternatives in each scenario that fail to achieve this the level of performance i e the level of potential regret is most important and therefore the identity transform is used for t3 it is also assumed that the decision maker has a moderate level of risk aversion for this objective and t2 is the 50th percentile of performance i e regret the inertia is a measure of how much the decision alternative options vary from year to year it is preferred that there are no significant changes in the level of pollution from one year to the next we are not using a specific threshold for this although some other studies have and the objective of the decision maker is to make the best decision regarding the level of performance level of inertia therefore the identity transform is chosen for t1 and t3 again the level of risk aversion is moderate for this objective and thus the 50th percentile is chosen for t2 returning to the overarching guidance framework for robustness analysis figs 2 and 4 now that we have the robustness metrics fig 4 box 4 and the scenarios are known fig 4 box 6 we can calculate robustness using the selected scenarios and selected custom robustness metrics fig 4 box 10 to illustrate this with the rapid software package we build upon an example of the lake problem that is included in the em workbench kwakkel 2017 with the following methodology 1 using the em workbench we formulate the model e g uncertain parameters objectives etc 2 using the rapid package we create the custom robustness metrics defined above in table 4 3 using the em workbench we formulate an optimization problem with the formulated model from step 1 and custom robustness metrics from step 2 4 using the em workbench we run the optimization to determine the most robust decision alternatives note that this means we begin with random decision alternatives in fig 4 box1 but then the em workbench refines these decision alternatives using the feedback loop with box 18 for step 1 the lake problem was specified in the same manner as in the em workbench example i e the uncertain parameters options for the decision alternatives and the performance objectives were defined in the same way using the em workbench functionality for defining a model image 1 for step 2 the custom robustness metrics defined in table 4 were first specified using the rapid package and then put into the form required for the em workbench note that when defining these custom metrics it was possible to use any combination of the three robustness metric transformations from the guidance for decision makers fig 3 and defined in table 1 these metrics can be defined using code as shown or can also be created using the metrics guidance to r function this function asks the user the questions from the flow chart in fig 3 guiding them to the creation of the robustness metric best suited for the problem that can then be used in subsequent analyses as shown in fig 5 the output from the metrics guidance to r function is the same as the output from the metrics custom r metric function in the example code image 4 as per the em workbench example which uses many objective robust optimization moro once the model has been formulated and the robustness metrics have been defined the next step is to use the em workbench to create a set of scenarios formulate an optimization problem and then run that optimization problem to find optimally robust decision alternatives this corresponds to the loop formed by box 18 in fig 4 the results found from this process are shown in fig 6 note again that the robustness metric transformations from the rapid software package ensure that a higher robustness value is always better e g we seek to minimize vulnerability but the sign for the robustness metric for vulnerability is switched so that we are aiming to maximize the robustness value the pareto front fig 6 shows expected relationships between objectives for example better vulnerability also results in better reliability but a worse result for the economic utility the relationship between the inertia and the other three objectives is weaker image 5 in this example of following the guidance framework figs 2 and 4 we showed that with no known robustness metric or set of candidate robustness metrics we could create a set of custom robustness metrics that were best suited to the problem table 4 using the guidance for creating a custom robustness metric fig 3 to determine the appropriate robustness metric transformations from table 2 we then created these robustness metrics in a systematic manner using the rapid software package and used these newly created robustness metrics in conjunction with another software package the em workbench to run a robust optimization and develop a pareto front of optimal decision alternatives 4 3 no candidate robustness metrics and multiple candidate scenario sets again following the guidance framework we use the optimal decision alternatives from the previous section and assume a situation in which the robustness metric is unknown fig 4 box 2 and there are no candidate robustness metrics fig 4 box 3 leading to box 4 in fig 4 here we create custom robustness metrics as per section 4 2 leading to the robustness metrics in table 4 unlike in section 4 2 in this section we consider a situation where there are multiple candidate sets of scenarios fig 4 box 6 e g in order to increase the diversity of considered uncertainties xexakis et al 2020 this situation could occur where the decision makers have identified different sets of scenarios that could all be appropriate for the problem or the situation where different decision makers create different sets of scenarios for the problem we note that while we will refer to the decision alternatives from the previous section as optimal they were optimal for the scenarios and robustness metrics in the previous section and for the specific formulation of this optimization problem maier et al 2018 they may not be optimal in this proceeding section different sets of scenarios correspond to different sets of points within the space of uncertain model inputs mcphail et al 2020 because these points are inputs to the calculation of robustness see fig 1 different sets of scenarios can lead to differences in robustness as a simplified illustration of this we create five candidate sets of 20 scenarios where each set is sampled from the uncertain variable space using the em workbench package with latin hypercube sampling we then evaluate the optimal decision alternatives from section 4 2 in all 100 scenarios using the em workbench package and calculate the robustness for all 5 scenario sets and all decision alternatives using the custom robustness metrics created in section 4 2 using the rapid package fig 4 box 9 note that for simplicity we only focus on the vulnerability objective from here on the same analysis could be applied to each of the four objectives image 6 image 2 returning to the robustness analysis guidance framework this brings us to box 13 in fig 4 where we use the analysis module of the rapid package to evaluate the relative difference in robustness values and the kendall s tau b rank correlation for determining the ranking similarity as described in section 3 the analysis module also enables us to visualize the influence of the scenarios by creating heatmaps that show all combinations of candidate sets of scenarios see fig 7 a and b the diagonal of the heatmaps is each candidate scenario set compared to itself and therefore the relative difference is 0 indicated by purple in fig 7 a and the ranking correlation is 1 indicated by blue in fig 7 b as expected from fig 7 a we can see that for the other comparisons of the scenario sets the relative difference in robustness values is very high in general indicated by mostly orange squares 30 difference in robustness values however there are some cases e g scenario sets 1 and 5 and scenario sets 4 and 5 that are more similar than the rest indicated by the green note that despite a high difference in robustness values fig 7 b indicates that the rankings of the decision alternatives are very stable consistent with mcphail et al 2020 given that all five candidate sets of scenarios were sampled using latin hypercube sampling it is interesting that the relative difference in robustness is so high in fig 7 a if the robustness values were important for the decision making process it would be difficult to be sure of the actual robustness values because the values would depend on which set of scenarios is being considered leading to fig 4 box 16 there are many reasons why the relative difference could be high including dissimilarity in the coverage of the scenario space and discontinuities in performance space mcphail et al 2020 in this example it is likely to be the former of these reasons because the number of scenarios in each set is small running the same code as above but with a larger number of scenarios 100 scenarios per set rather than 20 scenarios per set in fig 7 a we produce the heatmap shown in fig 7 c with the larger number of scenarios the relative difference is significantly lower in general likely due to a more similar coverage of the scenario space indicated by the greater number of blue and green squares and the smaller number of orange squares in this case we move from box 14 to boxes 15 and 17 in fig 4 being able to accurately determine the robustness of the decision alternatives note that a greater number of scenarios will not always allow a decision maker to accurately determine the robustness of the decision alternatives as indicated by the comparisons of scenario sets 4 and 5 in fig 7 c in this case we move from box 14 to box 16 in which case we need to use human judgement to determine which decision alternatives are the most robust alternatively if we are simply interested in the rankings of the solutions see fig 7 b then we would be able to move from box 14 to boxes 15 and 17 without increasing the number of scenarios assuming that we judge the kendall s tau b values approximately in the range between 0 7 and 1 0 to be sufficiently high for our purposes in this second example of following the guidance framework figs 2 and 4 we showed that with multiple candidate sets of scenarios we could use the rapid software package to evaluate the influence these candidate sets of scenarios had on both the robustness values and rankings using the visualizations produced by the software package we were then able to determine that the relative robustness values of different decision alternatives was not substantially affected by the different scenario sets fig 7 b giving confidence to decision makers and enabling the most robust decision alternative to be identified 4 4 multiple candidate robustness metrics and a known set of scenarios in this situation we assume that the robustness metric is unknown fig 4 box 2 but that there are multiple candidate robustness metrics fig 4 box 5 and that the set of scenarios is known leading to box 8 in fig 4 note that if there were multiple candidate sets of scenarios the analysis would be a combination of the following method and the method in section 4 3 we create the candidate robustness metrics using the rapid software package retaining the original robustness metric for the vulnerability determined in section 4 2 table 4 using the metrics custom r metric module and four traditional robustness metrics as the other candidate metrics including the maximax laplace s principle of insufficient reason minimax regret and percentile based kurtosis robustness metrics all included in the metrics common metrics module as with the previous examples these metrics were calculated evaluated this time across a known set of 100 scenarios sampled using latin hypercube sampling and visualized using the rapid package see fig 7 d image 3 in the visualization of the similarity in rankings fig 7 d the diagonal shows full ranking similarity a value of 1 indicated by blue because that is where each robustness metric is being compared to itself most of the metrics also show high levels of ranking similarity with each other with the exception of the percentile based kurtosis metric which shows a slight negative correlation with all other metrics indicated by the slightly red squares this potentially leads us from box 14 to box 16 in fig 4 because it is unknown which ranking is the one that we should follow the rankings provided by the percentile based kurtosis or the rankings provided by the rest of the metrics again using our judgement we decide that the percentile based kurtosis does not reflect the needs of the decision makers as much as the other robustness metrics do because the t3 transformation does not reflect the need to get an indication of the level of performance as explained by fig 3 and by mcphail et al 2018 also since all of the other candidate solutions generally agree with the custom robustness metric it follows that we can rely on this custom metric to determine which decision alternative is most robust fig 4 box 16 in this final illustration of using the guidance framework figs 2 and 4 and rapid software package we showed that with multiple candidate robustness metrics we can use the software package to evaluate the influence these robustness metrics have on the rankings of the decision alternatives using the visualizations produced by the software package we were then able to determine whether or not the influence was sufficiently large to affect these rankings all three of the simple examples considered show that the rapid package is easy to use and can be used in conjunction with related software packages such as the em workbench they also show that the rapid package is a practical tool for systematically following the guidance framework in figs 2 and 4 the guidance for creating robustness metrics in fig 3 shown in section 4 2 assessing the influence of candidate sets of scenarios on the robustness values and rankings shown in section 4 3 and assessing the influence of candidate robustness metrics on the robustness rankings of decision alternatives shown in section 4 4 note that since this is a multi objective problem there is no single most robust decision alternative and decision makers can only narrow down the choice of decision alternatives to those that represent the best trade offs between the four objectives it is likely that the decision of a final decision alternative would require further analysis by other potentially more senior decision makers to determine which trade off represents the best strategic choice 5 summary and conclusions robustness is important in the long term planning of environmental systems however there is a variety of metrics that can be used to calculate the robustness of a set of decision alternatives and recent research has shown that the choice of metric can affect the ranking of decision alternatives similarly there is a variety of approaches to selecting or generating scenarios which are an input to the calculation of robustness and the chosen approach has also been shown to have an effect on the robustness values and rankings of decision alternatives despite the uncertainty associated with the selection of scenarios and robustness metrics when determining the rankings of decision alternatives under deep uncertainty no guidance exists for decision makers on which choices to make as a response to this need for guidance this paper proposes a generic guidance framework to assist decision makers in the identification of robust decision alternatives fig 2 this framework caters to a variety of situations where the scenarios and or robustness metrics are known or not known the framework includes guidance on how to create a custom robustness metric for the problem at hand fig 3 based on the attributes of the problem e g the presence of performance thresholds tipping points or the objectives of the problem as well as the preferences of the decision maker e g the level of risk aversion the output from the guidance for the creation of a custom robustness metric is three robustness metric transformations table 1 which form the robustness metric when combined fig 1 the overarching guidance framework also identifies situations where quantitative analyses can be used to determine the influence that the selection of scenarios and or the choice of robustness metric has on the rankings of decision alternatives this paper also introduces an open source software package the rapid robustness analysis producing intelligent decisions package to assist with the consistency and ease of use of implementing the guidance framework see fig 4 the software package includes a module for the creation of custom robustness metrics using a wide range of robustness metric transformations table 2 including a function that leads the user through the guidance of how to create the robustness metric most suited for the problem at hand fig 3 it also includes a variety of commonly used traditional robustness metrics from the literature table 3 the software package also contains a module for the calculation and visualization of the impact of the selection of scenarios and choice of robustness metric on robustness values and rankings to illustrate the implementation of the guidance framework and rapid software package we consider the lake problem a hypothetical lake pollution problem commonly used in literature we use the guidance in fig 3 to create custom robustness metrics for the lake problem based on hypothetical problem attributes and decision maker preferences table 4 in conjunction with the em workbench kwakkel 2017 we use these robustness metrics as objectives in a robust optimization to create a set of robust decision alternatives as an example of the utility of the guidance framework and software package we use these optimal decision alternatives to consider a situation where there are multiple sets of scenarios under consideration using the rapid software package we visualize the impact of these different sets of scenarios showing that the robustness values are affected fig 7 a but rankings of the decision alternatives are not fig 7 b providing confidence to decision makers that the most robust decision alternative has been identified we also show that when using a larger set of scenarios the impact of the set of scenarios on the robustness values is greatly decreased fig 7 c in another example to highlight the utility of the guidance framework and software package we consider a situation where there is a variety of candidate robustness metrics we use the framework and software package to visualize the impact of the choice of robustness metric fig 7 d showing that most of the metrics agree on the rankings of the decision alternatives again providing confidence to decision makers that the most robust solution has been identified this guidance framework and software package assist decision makers in the identification of robust decision alternatives it does so in a systematic way and the software package increases the consistency and ease of use of implementing the guidance the guidance framework and software package are generic and cater to a wide variety of circumstances where the robustness metrics and or scenarios may or may not be known greatly increasing the accessibility of robustness analyses and techniques to decision makers after identifying the most robust decision alternatives or those decision alternatives that represent the best trade offs across multiple objectives decision makers are able to then explore those decision alternatives in more detail with these selected decision alternatives decision makers can better understand what makes some decision alternatives more robust explain to other stakeholders what it is about these decisions that makes them robust based on the information obtained from the guidance framework software availability the lake model is widely available on github in multiple repositories including in the emaworkbench https github com quaquel emaworkbench the rapid robustness analysis producing intelligent decisions software package is available on github https github com cameronmcphail rapid and in the python package index pypi https pypi org project rapidrobustness archived at doi org 10 5281 zenodo 4171495 declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements thanks is given to sa water corporation australia who support the research of cameron mcphail through water research australia and thanks is also given to water research australia the authors would also like to thank andrea castelletti and matteo giuliani both from politecnico di milano for their important conceptual contributions to this research 
25811,skillful hydrological prediction is highly valuable for optimizing water resources operation and planning this study proposes a hybrid bayesian vine copula bvc model for daily and monthly water level prediction to achieve this goal we first build a conditional vine copula vc prediction framework by utilizing the flexible vine copula as the baseline technique then a bayesian based model averaging scheme was employed to derive better predictions by combining all of the candidate conditional vc models and meanwhile quantify the uncertainty associated with these vc structures the proposed bvc model was tested to predict the daily and monthly water levels at two stations in south china the results demonstrate that the hybrid bvc approach can yield more reliable forecasts than the conditional vc model and traditional deterministic models such as the adaptive neuro fuzzy inference system according to the evaluation metrics keywords probabilistic prediction bayesian vine copula model water levels forecast skill extremes 1 introduction hydrological prediction related to rivers and coastal waters is crucial for water resources management sustainable development of hydroelectricity flood mitigation and navigation in particular accurate water level modeling and prediction is essential for efficient operation of water infrastructures such as dams and levees for instance floods associated with water level in lakes or rivers overflowing their banks are a major concern to the protection of life and property in the flood zones while low water levels could also cause considerable impacts on water quality river navigation and aquatic ecosystem such as fish spawning and diversity of species khaliq et al 2009 enhanced capacity of forecasting water levels either high water levels or low water levels is therefore of high public interest it can be beneficial for developing an early warning system which would inform the public about the potential or incoming floods or low water level events and facilitate advance preparation particularly for vulnerable people close to medium or small drainage basins where the flood travel time is short physically based hydrological models have been extensively used to predict the hydrological variables including streamflow water levels etc they typically require a variety of meteorological data and underlying information to simulate the hydrological processes of a catchment liong and sivapragasam 2002 moradkhani et al 2004 wang et al 2017 yet in practice many field data are not accessible limiting the skill of physically based hydrological models to achieve more accurate real time and or site specific predictions in recent years data driven models e g machine learning models have emerged as a promising technique for hydrological and environmental prediction they are not dependent on a physically based representation of the hydrological system and may avoid certain input data e g meteorological data soil types land use types compared to physically based models while the data driven models are empirical they are likely to enhance the accuracy of predictions by offering additional information from the data adamowski and sun 2010 chang and chang 2006 khan and coulibaly 2006 liu et al 2014 to date a variety of data driven models have been used such as support vector regression svr artificial neural network ann and adaptive neuro fuzzy inference system anfis nourani et al 2014 these models demonstrate skillful predictability with less input parameters during the model calibration compared to physically based models moradkhani et al 2004 wang et al 2017 as a result for regions with scarce data or observation records data driven models point to a feasible alternative to physically based hydrological models tiwari and chatterjee 2010 since last decades the copula has been becoming a popular scheme in the simulation of multivariate data due to its great flexibility in modeling multivariate dependence between variables without any limitations on the margins feng et al 2017 nelsen 2005 salvadori and de michele 2004 valle and kaplan 2019 it has been widely applied in hydrology for frequency analysis of flood identification of drought return periods geostatistical interpolation rainfall simulation and drought prediction bardossy and li 2008 favre et al 2004 haff et al 2015 hao and singh 2013 2016 kao and govindaraju 2007 leng and hall 2019 li et al 2013a 2013b yang et al 2018 zhang and singh 2006 most of those applications focus on two dimensional copulas as their parametric forms are easy to access for instance gaussian clayton gumbel or frank copula models each of which indicates a certain type of dependence between the variables of interest in contrast the number and expressiveness of parametric copulas become more restricted at higher dimensional applications hao and aghakouchak 2013 a solution to this problem is the hierarchical pair copula construction pcc i e the vine copulas bedford and cooke 2002 vine copulas are able to decompose any high dimensional joint distribution into a hierarchy of bivariate copulas more recently vine copulas have shown promising ability in simulating hydrometeorological events li et al 2014 liu et al 2015a 2016 xu et al 2020 yu et al 2019 for instance klein et al 2016 developed a vine based method to combine forecasts from two physically based hydrological models and a data driven model at two hydrological sites in the moselle river pereira and veiga 2018 developed a vine copula based model which is able to generate the stochastic simulation of periodic streamflow scenarios while preserving features that are observed in historical streamflow data a year later they outlined a pure model for simulating periodic multivariate streamflow scenarios via the vine copula pereira and veiga 2019 in addition liu et al 2015a developed a multivariate probabilistic model for discharge forecasting with vine copula liu et al 2018 employed the vine copulas to examine concurrent events under different underlying conditions they investigated the compound floods in texas with the effect of the rising temperatures and the el ni√±o southern oscillation enso and found that the presence of el ni√±o and or rising temperatures could enhance the relevant flooding impacts model averaging techniques have been widely used in merging predictions from different statistical models they can be divided into two categories ajami et al 2007 the first category combines multiple model outputs by using deterministic weights such as simple arithmetic mean regression approach ann abrahart and see 2002 kim and barros 2001 and granger ramanathan averaging diks and vrugt 2010 gahrs et al 2003 in those model averaging schemes the weights are determined arbitrarily e g equal weights which are less likely to reflect the real forecast skill of candidate models moreover these schemes fail to account for the uncertainty associated with multiple models the other category derives ensemble predictions from candidate models by employing likelihood metrics as weights this approach is known as the bayesian model averaging bma the model weights of bma act as a probability measure of competing models likelihood of success while producing a credible evaluation about the predictive uncertainty duan et al 2007 bma has been successfully applied to combine results of different models and proven to yield more accurate forecasts than other multimodel schemes fernandez et al 2001 liu et al 2012 2015b raftery et al 2005 viallefont et al 2001 wintle et al 2003 this study aims to enhance the capability of vine copula as a prediction approach for forecasting water levels in rivers vine copulas will be used as a baseline model for water level prediction given its strengths in modeling the high dimensional dependence between input variables yet the application of vine based prediction approach is challenged partly due to the uncertainties associated with model structural discrepancies herein the model structure uncertainty refers to differences that come from a variety of vine types with diverse mathematical structures of vines e g different ordering of the nodes in the tree structures so far studies are limited in addressing the uncertainties arising from structures of vine based models although it is doable to select the best model among several possible competing vine based models according to statistical measures gruber and czado 2015 min and czado 2010 using one single best vine based model may exclude other plausible models which provide comparable overall forecast skills but perform better for certain events additionally the single model approach ignores uncertainty from different structures of the vine based models thus increasing the potential of prediction bias these limitations motivate us to seek solutions by incorporating a model ensemble technique with our vine prediction model this work proposes a joint bayesian based vine copula model i e the bvc model to yield improved prediction of water levels and quantify their associated uncertainties at the same time the proposed work will be implemented via three steps first we will build a joint dependence structure of water level and its predictors i e the contributing variables based on a variety of vine copula structures second we will derive the conditional formation of vine copulas as the initial prediction approach i e the vc model our final step is to quantify the predictive uncertainty arising from various conditional vc structures by integrating the bma scheme we will conduct a comparison among the proposed ensemble bvc model the vc model and the anfis to evaluate their forecast skills in section 2 we outline a brief description of the study area and data used in section 3 we describe the development of the proposed model the results are illustrated in section 4 with two applications of the proposed model in the final section we discuss the implications and limitations of our study 2 data used in this study we used daily and monthly water level data to test the model performance the data are from the heyuan and longchuan hydrological stations situated at the dongjiang river basin south china fig 1 the dongjiang river basin is affected by the monsoon climate the annual mean temperature of this watershed is around 20 22 c and the mean annual precipitation spans from 1500 to 2400 mm the dongjiang river is an important source of water supply for the pearl river delta one of the most developed and populated regions in china chen et al 2011 the water level data at heyuan and longchuan stations are available from 1989 to 2011 provided by hydrological department of guangdong in these cases the data are classified into training and evaluation data sets 80 of the observed water level data are applied for model calibration and the remaining data are used to evaluate the model forecast skills results using cross validation are also obtained in supplementary materials the predictors are antecedent water level records e g at time t 1 t 2 and t 3 and will serve as the conditioning variables of the vine based model to facilitate forecasting while other predictors can be used e g inflows water levels at other gauges precipitation evapotranspiration section 3 1 shows that water levels at the same gauge already provide good results at the time scales being predicted 3 model development in this section we provide details of each step to explain how the proposed approach is designed for water level prediction it consists of 1 selecting the potential predictors based on the autocorrelation analysis 2 building conditional prediction models by using the vine copula as the baseline technique i e the vc model and 3 integrating the conditional vc model with the multi model averaging technique to complete the proposed bvc model more detailed description is given below 3 1 predictor selection like many other data driven models we first need to determine the appropriate predictors for our prediction model herein one and multi step water level data ahead of the targeted water level are considered as the potential predictors autocorrelation between previous lagged water level series from lag 1 to 10 and the original water level series was evaluated for both daily and monthly data at two selected sites shown in fig 2 the autocorrelation analysis provides information to determine the appropriate predictors it can be seen that daily data of antecedent time series at t 1 t 2 and t 3 exhibit strong positive correlation 0 85 with current time t at both stations comparing to the daily data the correlation 0 5 between the monthly time series at t 1 t 2 and t 3 and the current time t is reduced but also relatively strong in terms of monthly data for this reason the lagged water level series i e antecedent daily and monthly time series at time t 1 t 2 and t 3 are selected as potential predictors i e the inputs for the proposed prediction model we will explore the effect of different numbers of predictors either more or less inputs on the overall forecast skill of the proposed bvc model in section 4 5 the following sections show the formation of the proposed bvc model for water level prediction 3 2 establishment of the conditional vine copula vc model we seek to use vine copula as the baseline approach and construct the proposed prediction model we start with a brief description about the vine copula originated from the copula theory copula offers a general framework to model the probabilistic dependence behavior from different variables sklar et al 1959 despite the flexibility in modeling multivariate dependence without any limitations on the margins its theoretical developments and applications are mostly limited to bivariate cases many efforts have attempted to overcome this limitation by incorporating bivariate copulas to form higher dimensional copulas among them vine copula proposed by joe 1997 offers a feasible way to model multivariate data the basic scheme of vine copula is to decompose an n dimensional density into n n 1 2 bivariate densities aas et al 2009 min and czado 2010 ren et al 2014 regular vines include two simple sub classes c and d vines as an illustration here we mainly use c vine to construct the conditional prediction model min and czado 2010 a similar way can be applied for d vine fig 3 shows the hierarchical graphical representation of c vine in the first tree the first root node specifies the dependence regarding one certain variable by employing bivariate copulas for each pair the other c vine trees are constructed in this manner indicating a star structure aas et al 2009 allen et al 2013 brechmann and schepsmeier 2013 as an example combining the three trees in fig 3a the density of a 4 d c vine can be written as 1 p 1234 p 1 p 2 p 3 p 4 c 12 c 13 c 14 c 23 1 c 24 1 c 34 12 where x 1 x 2 x 3 and x 4 indicate four different variables c 1 2 p 1 x 1 p 2 x 2 is simply denoted as c 12 c 23 1 c 24 1 c 34 12 represent the conditional copula densities the general form of n dimensional c vine density is given in the supplementary material eq s 1 for brevity the above vine copula is used to simulate the joint dependence between different variables we then derive the conditional form from the original vine copula which provides a way to establish the prediction model via the conditional distribution functions the detailed steps are given as follows step 1 to illustrate we use a 4 dimensional c vine and derive the conditional cumulative distribution function p u 4 u 1 u 2 u 3 this forms a vc prediction model for variable x 4 i e current water level series at time t given other predictors x 1 x 3 i e antecedent water level series at time t 1 t 2 and t 3 p u 4 u 1 u 2 u 3 can be computed by recursively applying conditional distribution function i e the h function eq s 2 in the supplementary material 2 p u 4 u 1 u 2 u 3 h h h u 3 u 1 Œ∏ 13 h u 2 u 1 Œ∏ 12 Œ∏ 23 1 h h u 4 u 1 Œ∏ 14 h u 2 u 1 Œ∏ 12 Œ∏ 24 1 Œ∏ 34 12 here u 1 u 2 u 3 and u 4 are the marginal cumulative distribution function of x 1 x 2 x 3 and x 4 respectively Œ∏ 12 Œ∏ 13 Œ∏ 14 Œ∏ 23 1 Œ∏ 24 1 and Œ∏ 34 12 denote the parameters of different conditional copulas in the 4 d c vine fig 3a respectively the marginal cumulative distribution function for each variable was fitted by a number of probability distributions e g normal lognormal weibull gamma and generalized gamma the chi square goodness of fit test was used to determine the most appropriate theoretical distributions bhat et al 2019 khedun et al 2014 pourghasemi et al 2017 step 2 given the conditional forms in eq 2 we can derive its inverse form to complete the vc prediction model for certain probabilities œÑ e g œÑ 0 05 0 5 0 95 we can obtain u 4 from p u 4 u 1 u 2 u 3 using u 4 c u 4 u 1 u 2 u 3 1 œÑ u 1 u 2 u 3 h 1 œÑ u 1 u 2 u 3 c u 4 u 1 u 2 u 3 1 is the inverse of the conditional copula function i e the œÑ quantile curve of the copula chen et al 2009 ren et al 2014 regarding the 4 d c vine the œÑ th conditional quantile function of x 4 q x 4 œÑ x 1 x 2 x 3 can be obtained by the recursive computation 3 x 4 q x 4 œÑ x 1 x 2 x 3 p 1 u 4 p 1 h 1 h 1 h 1 œÑ h h u 3 u 1 h u 2 u 1 h u 2 u 1 u 1 where x 4 is the predicted variables i e current water level series at time t and x 1 x 3 are the predictors i e antecedent water level series at time t 1 t 2 and t 3 step 3 next we produce a sample consisting of 200 uniformly distributed random numbers covering the interval 0 1 i e the œÑ values using monte carlo simulations then we can use eqs 2 and 3 i e a vc prediction model to generate 200 realizations of x 4 the median values of these realizations are treated as the forecasts or estimates in a similar way one can build the conditional vc model with a higher lower dimensional c vine model 3 3 formation of the bayesian vine copula bvc model 3 3 1 structural discrepancies in the conditional vc model and candidate models using the vine copula and its conditional form we can establish the vc model for predicting the water level however it is noted that various candidate orderings of different variables exist in the spanning trees of the vc model as shown in fig 3 for instance it is arbitrary to decide which variable can be the root of the tree 1 i e the unique node that connects to all other nodes different choices of variables for the root may result in different combinations of nodes and edges resulting in various model structures the forecasting performance of the conditional vc model may thus vary among different c vine model structures the inconsistency of vine copula structures triggers a model selection problem again we take the 4 d c vine as an illustration the forecasting model eq 2 can also be written in different forms as follows corresponding to the c vine structures b f in fig 3 4 p u 4 u 1 u 2 u 3 h h h u 3 u 1 Œ∏ 13 h u 2 u 1 Œ∏ 12 Œ∏ 23 1 h h u 4 u 1 Œ∏ 14 h u 3 u 1 Œ∏ 13 Œ∏ 34 1 Œ∏ 24 13 5 p u 4 u 1 u 2 u 3 h h h u 3 u 2 Œ∏ 23 h u 1 u 2 Œ∏ 21 Œ∏ 13 2 h h u 4 u 2 Œ∏ 24 h u 1 u 2 Œ∏ 21 Œ∏ 14 2 Œ∏ 34 12 6 p u 4 u 1 u 2 u 3 h h h u 3 u 2 Œ∏ 23 h u 1 u 2 Œ∏ 21 Œ∏ 13 2 h h u 4 u 2 Œ∏ 24 h u 3 u 2 Œ∏ 23 Œ∏ 34 2 Œ∏ 14 23 7 p u 4 u 1 u 2 u 3 h h h u 1 u 3 Œ∏ 31 h u 2 u 3 Œ∏ 32 Œ∏ 12 3 h h u 4 u 3 Œ∏ 34 h u 1 u 3 Œ∏ 31 Œ∏ 14 3 Œ∏ 24 13 8 p u 4 u 1 u 2 u 3 h h h u 1 u 3 Œ∏ 31 h u 2 u 3 Œ∏ 32 Œ∏ 12 3 h h u 4 u 3 Œ∏ 34 h u 2 u 3 Œ∏ 32 Œ∏ 24 3 Œ∏ 14 23 these different conditional formulas i e eqs 2 4 8 could be deemed as 6 different candidate vc models termed as vc1 vc6 given the predictors x 1 x 3 one can derive the predictand x 4 i e the current water level at time t from each of the above six conditional vc models the forecasting performance of each vc model might differ due to changes in model structure bivariate copula families and parameters applied for each pair of variables we used a r package i e vinecopula schepsmeier and brechmann 2015 to compute the akaike information criterion aic the smallest aic statistics determines the best bivariate copula families for the hierarchical vine copula tree structure when constructing the conditional vc model the potential copula families used in the study are the gaussian student t gumbel joe clayton and gumbel among the candidate vc models with different vine structure e g eqs 2 4 8 one can choose a single best model or make an average of the vc predictions generated from all vc candidate models as the ideal predictions however the rejection of other plausible candidate models with similar performance may lead to statistical bias and an underestimation of the predictive uncertainty inherent with the structural differences of the vc prediction models moreover the selection of best vc model highly depends on the evaluation measures which may vary as a result of changes in evaluation metrics therefore instead of using the single best model or a simple average we will use all the plausible models as candidate models for the purpose of bayesian based ensemble forecasts 3 3 2 bayesian based ensemble forecasts in the last step of the bvc construction we employ a multi model combination technique i e the bma which can merge forecasts from all candidates of the vc models with appropriate weights it enables to provide ensemble predictions by weighting candidate models based on their performance basically it will assign higher weights to models having better performance a brief description of the bma technique is given below raftery et al 2005 vrugt and robinson 2007 considering y as the ensemble forecast each candidate member i e the conditional vc prediction models presented above yields a forecast f f 1 f 2 f k assuming that we have k candidate models p k y f k d is the posterior distribution of y conditioning upon the forecast f k from the vc models and the training data d the posterior distribution of the bma forecasts can be written as 10 p y f 1 f 2 f k d k 1 k p f k d p k y f k d where p f k d is the posterior probability of the forecast of each candidate model it also reflects how well this candidate model f k fits the training data d duan et al 2007 let w k p f k d since w k is a probability value serving as the weights we should obtain k 1 k w k 1 the expectation maximization algorithm the algorithm given in table s1 will then be used to estimate these weights raftery et al 2005 the bma ensemble prediction is obtained by assigning higher weights to better candidate members and the spread of these predictions indicates the uncertainty associated with the vc model structures in this regard we combine the strengths of bma scheme with vc prediction models to establish an integrated bayesian vine copula model simply the bvc model the p k y f k d is estimated with the gamma distribution sloughter et al 2007 due to the highly skewed distribution of water level data in this study we compared the proposed bvc model to the vc model and the anfis the anfis first developed by jang et al 1997 is a hybrid machine learning model which combines the strengths of the conventional artificial neural network ann model and fuzzy system the learning algorithm for anfis is a combination of the gradient descent and least squares method anctil and tape 2004 anfis is capable of providing better forecasts compared to the conventional ann models and has gained the popularity in hydrological predictions badrzadeh et al 2013 moghaddamnia et al 2009 nourani et al 2011 partal and kisi 2007 more details about the anfis can be found in studies jang et al 1997 moghaddamnia et al 2009 3 4 performance measures we compared the forecast skill of our presented bvc model with the arithmetic mean of all candidate vc models and the anfis nourani et al 2014 three widely used metrics are applied to assess the forecast skills of different models that is the coefficient of determination r2 the root mean squared error rmse and the nash sutcliffe model efficiency coefficient nse bennett et al 2013 they offer evaluations from different perspectives including the degree of correlation between the observations and forecasts the discrepancy between the observations and forecasts and the relative magnitude of the residual variance compared to the observed data variance the formulas for these metrics are available in the supplementary material 4 results 4 1 comparison of overall forecast skills the performance of the dynamic bma scheme is shown with one event for each station the overall forecasting results during the whole validation period will be given in the following section one day ahead prediction is made at heyuan station on 31 july 2008 and at longchuan station on june 12 2007 fig 4 plots two examples of the bma predictive distribution which is a weighted sum of the pdf of six candidate vc models showing different distribution patterns the median value of the bma predictive distribution dictates the final forecast i e the bvc forecast it shows that observations fall in the 90 bma prediction interval and the bma is able to generate reasonable intervals and pdfs table 1 summarizes the respective weights of the six candidate models for the two examples in fig 4a and table 1 it shows that the vc5 vc6 and vc3 receive greater weights than other candidate members yet for the second case fig 4b the vc1 dominates the contributions of bayesian based ensemble forecast and other candidate vc models receive relatively smaller weights fig 5 illustrates the time series of observations and model forecasts of the proposed bvc model the arithmetic mean of vc models and anfis during the evaluation periods at heyuan station the results for longchuan station are given in fig s1 of the supplementary material there are evident differences in prediction performance among different models for the bvc model the total uncertainty is indicated by the shaded areas the 90 prediction uncertainty intervals of bvc model generally encompass the range of observations to complement the time series plots table 2 presents a quantitative comparison of different models with the performance metrics across these metrics it is clear to see that the bvc model marginally outperforms other models confirmed by the higher nse and r2 values and the lower rmse value for both stations the bayesian based model averaging strategies maximize the strengths of different candidate vc models to a certain degree thus delivering better overall performance according to the three metrics the performance of the arithmetic mean of six vc models during the validation period is superior to the deterministic model i e anfis in terms of the rmse r2 and nse also we show summary metrics regarding the forecast skills of each individual vc model at the two stations in table 2 the forecast skill of individual vc models varies between two stations and no individual vc model always provides the best prediction at these stations this inconsistency highlights the importance of the bayesian based ensemble predictions 4 2 forecast performance under different water level regimes we further provided the scatter plots of observations against the predicted water levels at the two stations for 1 day lead time forecasts with different models fig 6 those models can capture the majority of observations but the performance declines for more extreme water levels either high or low water levels indicated by the divergence from the 1 1 line a visual comparison of the scatter plots shows the predictions of bvc model are in a better agreement with the observations compared to other competing models this raises a question about how these models perform over different ranges of the observations during the evaluation period a simple way to consider how different approaches perform across different ranges of the observations is to divide the hydrograph into several parts e g low medium and high water levels we used 10 and 90 quantile thresholds to separate the historical observations into the low medium and high water level groups this enables us to examine how the proposed model and other competitors perform over different ranges of the hydrograph in terms of low water levels fig 7 a according to the predicted and observed flow duration curves fdcs all of the models either underestimate or overestimate the observations but the bvc model continues to perform the best given the smallest disparity with the observations in the medium range fig 7b the three models present similar results and all the forecasts are close to the observations except the anfis forecasts regarding the high water level fig 7c the bvc model keeps a better consistency with the observations this indicates the proposed bvc model outperforms the vc models arithmetic mean and the anfis for a particular sector of the hydrograph leading to a better overall performance across different metrics 4 3 forecast skill for longer lead times the results presented above focus on one step ahead prediction we also explored the model performance for longer lead times fig 8 presents summary metrics at 2 to 7 day lead time for water level prediction at the two stations it is clear that the performance of all models decreases with increasing lead times with respect to the three performance metrics this is due to less availability of information for longer lead time horizons and greater influence of predictors not included in the model the proposed bvc model tends to outperform the other methods indicated by the higher nse and r2 values and lower rmse values this in turn reveals that the bayesian techniques enable to combine the strengths of different candidate models and provide more reliable forecasts another point is that the performance with the arithmetic mean of vc models is not always better than the anfis according to the metrics during the validation period for instance for longer lead times at heyuan station the performance of anfis appears slightly superior to the arithmetic mean of vc models 4 4 forecast skill for monthly water levels in addition to the daily prediction monthly water level prediction for 1 month lead times was examined using the vc models the anfis and the bvc model fig 9 presents a comparison of the forecasts from these models during the evaluation period at the two stations as shown the bayesian based ensemble predictions the bvc model are in a better agreement with the observations than the forecasts produced by the arithmetic mean of six vine based models and the anfis the 90 uncertainty bounds generated from the bvc approach virtually capture all the observations also we show summary metrics regarding the forecast skills of individual models in table 3 for 1 3 month ahead water level prediction the results show that the bvc model generally provides better forecast skills than the other approaches the arithmetic mean outperforms the anfis the monthly forecasting thus also supports the advantage of the bvc model 4 5 effect of the number of predictors on the performance of the bvc model the vc models were established based on a 4 d vine copula with 3 lagged water levels as predictors i e antecedent water level time series at time t 1 t 2 and t 3 it remains unclear whether the addition of more variables as inputs i e establishing higher dimensional vc models will yield higher skills or not thus we considered more lagged water levels as input variables i e using the antecedent daily water levels from t 1 to t 4 4 predictors or from t 1 to t 5 5 predictors to forecast the water level at time t at longchuan station as an example again the bma scheme was used to combine the predictions from the candidate vc models in order to derive the ensemble predictions fig 10 a c shows that for the proposed bvc model the involvement of more time steps as predictors provides additional value in forecast skill to some extent this gain is accompanied by increased complexity in model structure and much more computational time resources in this particular case the forecast skill of the vc and bvc models is slightly lower for 4 antecedent predictors when compared to the results with 3 and 5 predictors 4 6 effect of the number of candidate vc models on the performance of the bvc model the above prediction based on the bvc model considers all vc models as candidates in other words the forecasts from all six vc models were used in the bvc model this makes us question whether it is necessary to include all these six vc models and how the number of candidate models may affect the performance of the ensemble model when using the multimodel averaging technique to explore these questions we again used the example of 1 day ahead water level forecast at longchuan station we tested the performance of the bvc models with different numbers of candidate vc models we selected different candidate members for instance three vc models vc1 vc3 were used as the candidate members to form for the 3 member bvc model and subsequently generate the ensemble forecasts as shown in fig 10d f the 5 member bvc model is already sufficient to provide satisfactory forecasts the use of additional candidate members i e the 6 member bvc model does not exceed the skill provided by the 5 member bvc model 5 discussion and conclusions to improve the forecasting performance of vine based models we proposed a hybrid bvc model the first component of the bvc model involves several conditional forecasting models based on different vine copula structures i e the vc models the bma approach was then used to combine the strengths of the forecasts from different vc models while accounting for the model uncertainty we compared the performance of the bvc model with the arithmetic mean of candidate vc models as well as the anfis model for daily and monthly water level prediction the results show that the bvc model which combines the strengths of different vc models generally outperforms other competing models both vine copula and bma approaches are easy to implement and the combination of these two techniques i e the bvc model provides a new alternative in hydrological forecasting despite its strength in water level prediction the presented bvc model has some limitations originally the vine copulas were used to model joint multivariate distributions here we explored its ability in establishing a statistical forecasting model i e the vc prediction model however such vc models involve a variety of vine copula types and mathematical structures thus increasing the prediction uncertainty the common assumption is that there is no best selection in vc model structures which motivates us to improve the vc model by using the model averaging scheme although the vc models perform well as compared to the traditional deterministic models e g the anfis the involvement of bayesian based averaging scheme i e the bvc model has significantly improved the overall forecast skill of the original vc models this is attributable to the strength of the bayesian based averaging scheme in combining various candidate models devineni et al 2008 doblas reyes et al 2000 relying on the complex structure and flexibility in modeling dependence between different variables the proposed bvc model achieves the best performance for the data used in the study one question that may be raised however whether an auto regression ar model with simpler structure may also yield quite well forecast skill the performance of the ar model strongly depends on the autocorrelation in the time series themselves e g the time series of water levels used here in this regard we included a simple auto regressive model as a comparison to be consistent antecedent time series at time t 1 t 2 and t 3 were also used as the predictors of the ar 3 model by taking the 1 3 month ahead water level prediction at longchuan station as an illustration table s1 in the supplementary material interestingly the ar 3 model also presents relatively well forecast skill for the one month ahead prediction during the evaluation period although the bvc approach with significant increment of the complexity in model structure remains keeping the superiority according to the performance metrics yet as for higher lead times i e 2 and 3 month ahead water level prediction the bvc model s strength is more apparent as compared to the ar 3 model the conventional auto regression methods may lack the absolute advantage in yielding accurate longer term predictions i e greater lags ahead in comparison with nascent copula based model moreover we also provided the performance metrices of different models during the training period table s2 we find the forecast skills of copula based models the bvc and vc models are slightly better than the traditional ar 3 and anfis models although the strength is not remarkable by comparing the performance metrices during the evaluation period this is expected because the higher performance in the training period could generally lead to better forecast skills in the evaluation period also the autocorrelation significantly declines in the monthly time series of water level at this station as the lag increasing fig 2d this could directly weaken forecast skill of traditional models heavily depending on the autocorrelation in time series we also explored the effect of the number of predictors and candidate members on the ensemble forecast skill we find the inclusion of more predictors and candidate members does not always guarantee additional gain in forecast skill of the bvc model in some cases the effect could be opposite that is more predictors or candidate members tend to worsen the forecast skill e g fig 10 despite skill improvements more candidates or predictors may significantly increase the computational time and burden we found that the daily water level prediction is more robust than the monthly prediction in the context of the three selected statistical metrics this can be attributed to the fact that daily water levels are more strongly correlated with the previous lagged water levels it is thus easier to model the short term water levels without other predictors for the monthly water levels the autocorrelation is apparently weaker than the daily values several existing studies with different data driven models have documented the effect of autocorrelation on the forecast skill kisi and cimen 2011 shortridge et al 2016 tiwari and adamowski 2013 in addition we found that the bvc models underestimated or overestimated the observations to some degree particularly for the extreme high water level values although it produced a better overall performance across different metrics this may be partially due to the fact that extreme high water level values are rare and the bvc model gives more weight to non extreme values another reason is that the theoretical marginal distributions e g gamma weibull distributions used are not so effective to represent the extreme behavior of the water levels which could affect the simulation of joint dependence structure of different variables with the vine copula 5 1 our results show the bvc model a potential improvement in the following study is to include other extreme value distributions such as generalized extreme value gev or generalized pareto distribution gpd to fit the marginal distribution of water levels this study used the conventional validation method i e partitioning the dataset into two sets of 80 for training and the remaining data for testing to evaluate the model performance which could be further improved with the cross validation method to give an example we applied the 4 fold cross validation method and evaluated 1 day ahead water level forecast at both longchuan station and heyuan station table s3 in the supplementary material we further compared the cross validation results with the conventional ones and our results reveal slight improvements regarding different evaluation metrics another limitation of this study is that we only used the previous lagged water levels as the predictors in the bvc model however the model is flexible to use other data as predictors if available for instance the regional climate variables e g precipitation and runoff play a critical role in extracting useful information for the river water level prediction in particular the daily water level strongly depends on the sudden occurrence of heavy rainfall moreover it has been widely documented that large scale oceanic and atmospheric climate signals such as enso have strong teleconnections with the hydrological variables in many regions chan and zhou 2005 ouyang et al 2014 such factors could provide valuable information used as the climate predictors for regional monthly or seasonal hydrological prediction this also points another possible improvement of the bvc model by involving more regional climate variables and large scale climate signals as potential predictors though further analysis of input variable selection may be beneficial galelli et al 2014 additionally the forecasting ability of the proposed approach at smaller timescales for example hourly forecasting is worth exploring the bvc approach was used for water level prediction as a demonstration in this study but we would like to highlight that the proposed model can be extended for forecasting other hydrometeorological variables such as river flows and precipitation and it has a great potential for broader applications in the hydroclimate field declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the first third and fourth author acknowledge the national natural science foundation of china 51809294 51779279 guangdong provincial department of science and technology 2019zt08g090 the national key r d program of china 2016yfc0402602 the water science and technology innovation project of guangdong province 2020 27 and the outstanding youth science foundation of nsfc 51822908 for the financial support we appreciate the constructive comments and suggestions from the editor associate editor and anonymous reviewers all data used this study are available for free on request from the author at the following e mail address liuzhiy25 mail sysu edu cn the code used in this study is available from https github com jeromelau11 bvc model appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105075 
25811,skillful hydrological prediction is highly valuable for optimizing water resources operation and planning this study proposes a hybrid bayesian vine copula bvc model for daily and monthly water level prediction to achieve this goal we first build a conditional vine copula vc prediction framework by utilizing the flexible vine copula as the baseline technique then a bayesian based model averaging scheme was employed to derive better predictions by combining all of the candidate conditional vc models and meanwhile quantify the uncertainty associated with these vc structures the proposed bvc model was tested to predict the daily and monthly water levels at two stations in south china the results demonstrate that the hybrid bvc approach can yield more reliable forecasts than the conditional vc model and traditional deterministic models such as the adaptive neuro fuzzy inference system according to the evaluation metrics keywords probabilistic prediction bayesian vine copula model water levels forecast skill extremes 1 introduction hydrological prediction related to rivers and coastal waters is crucial for water resources management sustainable development of hydroelectricity flood mitigation and navigation in particular accurate water level modeling and prediction is essential for efficient operation of water infrastructures such as dams and levees for instance floods associated with water level in lakes or rivers overflowing their banks are a major concern to the protection of life and property in the flood zones while low water levels could also cause considerable impacts on water quality river navigation and aquatic ecosystem such as fish spawning and diversity of species khaliq et al 2009 enhanced capacity of forecasting water levels either high water levels or low water levels is therefore of high public interest it can be beneficial for developing an early warning system which would inform the public about the potential or incoming floods or low water level events and facilitate advance preparation particularly for vulnerable people close to medium or small drainage basins where the flood travel time is short physically based hydrological models have been extensively used to predict the hydrological variables including streamflow water levels etc they typically require a variety of meteorological data and underlying information to simulate the hydrological processes of a catchment liong and sivapragasam 2002 moradkhani et al 2004 wang et al 2017 yet in practice many field data are not accessible limiting the skill of physically based hydrological models to achieve more accurate real time and or site specific predictions in recent years data driven models e g machine learning models have emerged as a promising technique for hydrological and environmental prediction they are not dependent on a physically based representation of the hydrological system and may avoid certain input data e g meteorological data soil types land use types compared to physically based models while the data driven models are empirical they are likely to enhance the accuracy of predictions by offering additional information from the data adamowski and sun 2010 chang and chang 2006 khan and coulibaly 2006 liu et al 2014 to date a variety of data driven models have been used such as support vector regression svr artificial neural network ann and adaptive neuro fuzzy inference system anfis nourani et al 2014 these models demonstrate skillful predictability with less input parameters during the model calibration compared to physically based models moradkhani et al 2004 wang et al 2017 as a result for regions with scarce data or observation records data driven models point to a feasible alternative to physically based hydrological models tiwari and chatterjee 2010 since last decades the copula has been becoming a popular scheme in the simulation of multivariate data due to its great flexibility in modeling multivariate dependence between variables without any limitations on the margins feng et al 2017 nelsen 2005 salvadori and de michele 2004 valle and kaplan 2019 it has been widely applied in hydrology for frequency analysis of flood identification of drought return periods geostatistical interpolation rainfall simulation and drought prediction bardossy and li 2008 favre et al 2004 haff et al 2015 hao and singh 2013 2016 kao and govindaraju 2007 leng and hall 2019 li et al 2013a 2013b yang et al 2018 zhang and singh 2006 most of those applications focus on two dimensional copulas as their parametric forms are easy to access for instance gaussian clayton gumbel or frank copula models each of which indicates a certain type of dependence between the variables of interest in contrast the number and expressiveness of parametric copulas become more restricted at higher dimensional applications hao and aghakouchak 2013 a solution to this problem is the hierarchical pair copula construction pcc i e the vine copulas bedford and cooke 2002 vine copulas are able to decompose any high dimensional joint distribution into a hierarchy of bivariate copulas more recently vine copulas have shown promising ability in simulating hydrometeorological events li et al 2014 liu et al 2015a 2016 xu et al 2020 yu et al 2019 for instance klein et al 2016 developed a vine based method to combine forecasts from two physically based hydrological models and a data driven model at two hydrological sites in the moselle river pereira and veiga 2018 developed a vine copula based model which is able to generate the stochastic simulation of periodic streamflow scenarios while preserving features that are observed in historical streamflow data a year later they outlined a pure model for simulating periodic multivariate streamflow scenarios via the vine copula pereira and veiga 2019 in addition liu et al 2015a developed a multivariate probabilistic model for discharge forecasting with vine copula liu et al 2018 employed the vine copulas to examine concurrent events under different underlying conditions they investigated the compound floods in texas with the effect of the rising temperatures and the el ni√±o southern oscillation enso and found that the presence of el ni√±o and or rising temperatures could enhance the relevant flooding impacts model averaging techniques have been widely used in merging predictions from different statistical models they can be divided into two categories ajami et al 2007 the first category combines multiple model outputs by using deterministic weights such as simple arithmetic mean regression approach ann abrahart and see 2002 kim and barros 2001 and granger ramanathan averaging diks and vrugt 2010 gahrs et al 2003 in those model averaging schemes the weights are determined arbitrarily e g equal weights which are less likely to reflect the real forecast skill of candidate models moreover these schemes fail to account for the uncertainty associated with multiple models the other category derives ensemble predictions from candidate models by employing likelihood metrics as weights this approach is known as the bayesian model averaging bma the model weights of bma act as a probability measure of competing models likelihood of success while producing a credible evaluation about the predictive uncertainty duan et al 2007 bma has been successfully applied to combine results of different models and proven to yield more accurate forecasts than other multimodel schemes fernandez et al 2001 liu et al 2012 2015b raftery et al 2005 viallefont et al 2001 wintle et al 2003 this study aims to enhance the capability of vine copula as a prediction approach for forecasting water levels in rivers vine copulas will be used as a baseline model for water level prediction given its strengths in modeling the high dimensional dependence between input variables yet the application of vine based prediction approach is challenged partly due to the uncertainties associated with model structural discrepancies herein the model structure uncertainty refers to differences that come from a variety of vine types with diverse mathematical structures of vines e g different ordering of the nodes in the tree structures so far studies are limited in addressing the uncertainties arising from structures of vine based models although it is doable to select the best model among several possible competing vine based models according to statistical measures gruber and czado 2015 min and czado 2010 using one single best vine based model may exclude other plausible models which provide comparable overall forecast skills but perform better for certain events additionally the single model approach ignores uncertainty from different structures of the vine based models thus increasing the potential of prediction bias these limitations motivate us to seek solutions by incorporating a model ensemble technique with our vine prediction model this work proposes a joint bayesian based vine copula model i e the bvc model to yield improved prediction of water levels and quantify their associated uncertainties at the same time the proposed work will be implemented via three steps first we will build a joint dependence structure of water level and its predictors i e the contributing variables based on a variety of vine copula structures second we will derive the conditional formation of vine copulas as the initial prediction approach i e the vc model our final step is to quantify the predictive uncertainty arising from various conditional vc structures by integrating the bma scheme we will conduct a comparison among the proposed ensemble bvc model the vc model and the anfis to evaluate their forecast skills in section 2 we outline a brief description of the study area and data used in section 3 we describe the development of the proposed model the results are illustrated in section 4 with two applications of the proposed model in the final section we discuss the implications and limitations of our study 2 data used in this study we used daily and monthly water level data to test the model performance the data are from the heyuan and longchuan hydrological stations situated at the dongjiang river basin south china fig 1 the dongjiang river basin is affected by the monsoon climate the annual mean temperature of this watershed is around 20 22 c and the mean annual precipitation spans from 1500 to 2400 mm the dongjiang river is an important source of water supply for the pearl river delta one of the most developed and populated regions in china chen et al 2011 the water level data at heyuan and longchuan stations are available from 1989 to 2011 provided by hydrological department of guangdong in these cases the data are classified into training and evaluation data sets 80 of the observed water level data are applied for model calibration and the remaining data are used to evaluate the model forecast skills results using cross validation are also obtained in supplementary materials the predictors are antecedent water level records e g at time t 1 t 2 and t 3 and will serve as the conditioning variables of the vine based model to facilitate forecasting while other predictors can be used e g inflows water levels at other gauges precipitation evapotranspiration section 3 1 shows that water levels at the same gauge already provide good results at the time scales being predicted 3 model development in this section we provide details of each step to explain how the proposed approach is designed for water level prediction it consists of 1 selecting the potential predictors based on the autocorrelation analysis 2 building conditional prediction models by using the vine copula as the baseline technique i e the vc model and 3 integrating the conditional vc model with the multi model averaging technique to complete the proposed bvc model more detailed description is given below 3 1 predictor selection like many other data driven models we first need to determine the appropriate predictors for our prediction model herein one and multi step water level data ahead of the targeted water level are considered as the potential predictors autocorrelation between previous lagged water level series from lag 1 to 10 and the original water level series was evaluated for both daily and monthly data at two selected sites shown in fig 2 the autocorrelation analysis provides information to determine the appropriate predictors it can be seen that daily data of antecedent time series at t 1 t 2 and t 3 exhibit strong positive correlation 0 85 with current time t at both stations comparing to the daily data the correlation 0 5 between the monthly time series at t 1 t 2 and t 3 and the current time t is reduced but also relatively strong in terms of monthly data for this reason the lagged water level series i e antecedent daily and monthly time series at time t 1 t 2 and t 3 are selected as potential predictors i e the inputs for the proposed prediction model we will explore the effect of different numbers of predictors either more or less inputs on the overall forecast skill of the proposed bvc model in section 4 5 the following sections show the formation of the proposed bvc model for water level prediction 3 2 establishment of the conditional vine copula vc model we seek to use vine copula as the baseline approach and construct the proposed prediction model we start with a brief description about the vine copula originated from the copula theory copula offers a general framework to model the probabilistic dependence behavior from different variables sklar et al 1959 despite the flexibility in modeling multivariate dependence without any limitations on the margins its theoretical developments and applications are mostly limited to bivariate cases many efforts have attempted to overcome this limitation by incorporating bivariate copulas to form higher dimensional copulas among them vine copula proposed by joe 1997 offers a feasible way to model multivariate data the basic scheme of vine copula is to decompose an n dimensional density into n n 1 2 bivariate densities aas et al 2009 min and czado 2010 ren et al 2014 regular vines include two simple sub classes c and d vines as an illustration here we mainly use c vine to construct the conditional prediction model min and czado 2010 a similar way can be applied for d vine fig 3 shows the hierarchical graphical representation of c vine in the first tree the first root node specifies the dependence regarding one certain variable by employing bivariate copulas for each pair the other c vine trees are constructed in this manner indicating a star structure aas et al 2009 allen et al 2013 brechmann and schepsmeier 2013 as an example combining the three trees in fig 3a the density of a 4 d c vine can be written as 1 p 1234 p 1 p 2 p 3 p 4 c 12 c 13 c 14 c 23 1 c 24 1 c 34 12 where x 1 x 2 x 3 and x 4 indicate four different variables c 1 2 p 1 x 1 p 2 x 2 is simply denoted as c 12 c 23 1 c 24 1 c 34 12 represent the conditional copula densities the general form of n dimensional c vine density is given in the supplementary material eq s 1 for brevity the above vine copula is used to simulate the joint dependence between different variables we then derive the conditional form from the original vine copula which provides a way to establish the prediction model via the conditional distribution functions the detailed steps are given as follows step 1 to illustrate we use a 4 dimensional c vine and derive the conditional cumulative distribution function p u 4 u 1 u 2 u 3 this forms a vc prediction model for variable x 4 i e current water level series at time t given other predictors x 1 x 3 i e antecedent water level series at time t 1 t 2 and t 3 p u 4 u 1 u 2 u 3 can be computed by recursively applying conditional distribution function i e the h function eq s 2 in the supplementary material 2 p u 4 u 1 u 2 u 3 h h h u 3 u 1 Œ∏ 13 h u 2 u 1 Œ∏ 12 Œ∏ 23 1 h h u 4 u 1 Œ∏ 14 h u 2 u 1 Œ∏ 12 Œ∏ 24 1 Œ∏ 34 12 here u 1 u 2 u 3 and u 4 are the marginal cumulative distribution function of x 1 x 2 x 3 and x 4 respectively Œ∏ 12 Œ∏ 13 Œ∏ 14 Œ∏ 23 1 Œ∏ 24 1 and Œ∏ 34 12 denote the parameters of different conditional copulas in the 4 d c vine fig 3a respectively the marginal cumulative distribution function for each variable was fitted by a number of probability distributions e g normal lognormal weibull gamma and generalized gamma the chi square goodness of fit test was used to determine the most appropriate theoretical distributions bhat et al 2019 khedun et al 2014 pourghasemi et al 2017 step 2 given the conditional forms in eq 2 we can derive its inverse form to complete the vc prediction model for certain probabilities œÑ e g œÑ 0 05 0 5 0 95 we can obtain u 4 from p u 4 u 1 u 2 u 3 using u 4 c u 4 u 1 u 2 u 3 1 œÑ u 1 u 2 u 3 h 1 œÑ u 1 u 2 u 3 c u 4 u 1 u 2 u 3 1 is the inverse of the conditional copula function i e the œÑ quantile curve of the copula chen et al 2009 ren et al 2014 regarding the 4 d c vine the œÑ th conditional quantile function of x 4 q x 4 œÑ x 1 x 2 x 3 can be obtained by the recursive computation 3 x 4 q x 4 œÑ x 1 x 2 x 3 p 1 u 4 p 1 h 1 h 1 h 1 œÑ h h u 3 u 1 h u 2 u 1 h u 2 u 1 u 1 where x 4 is the predicted variables i e current water level series at time t and x 1 x 3 are the predictors i e antecedent water level series at time t 1 t 2 and t 3 step 3 next we produce a sample consisting of 200 uniformly distributed random numbers covering the interval 0 1 i e the œÑ values using monte carlo simulations then we can use eqs 2 and 3 i e a vc prediction model to generate 200 realizations of x 4 the median values of these realizations are treated as the forecasts or estimates in a similar way one can build the conditional vc model with a higher lower dimensional c vine model 3 3 formation of the bayesian vine copula bvc model 3 3 1 structural discrepancies in the conditional vc model and candidate models using the vine copula and its conditional form we can establish the vc model for predicting the water level however it is noted that various candidate orderings of different variables exist in the spanning trees of the vc model as shown in fig 3 for instance it is arbitrary to decide which variable can be the root of the tree 1 i e the unique node that connects to all other nodes different choices of variables for the root may result in different combinations of nodes and edges resulting in various model structures the forecasting performance of the conditional vc model may thus vary among different c vine model structures the inconsistency of vine copula structures triggers a model selection problem again we take the 4 d c vine as an illustration the forecasting model eq 2 can also be written in different forms as follows corresponding to the c vine structures b f in fig 3 4 p u 4 u 1 u 2 u 3 h h h u 3 u 1 Œ∏ 13 h u 2 u 1 Œ∏ 12 Œ∏ 23 1 h h u 4 u 1 Œ∏ 14 h u 3 u 1 Œ∏ 13 Œ∏ 34 1 Œ∏ 24 13 5 p u 4 u 1 u 2 u 3 h h h u 3 u 2 Œ∏ 23 h u 1 u 2 Œ∏ 21 Œ∏ 13 2 h h u 4 u 2 Œ∏ 24 h u 1 u 2 Œ∏ 21 Œ∏ 14 2 Œ∏ 34 12 6 p u 4 u 1 u 2 u 3 h h h u 3 u 2 Œ∏ 23 h u 1 u 2 Œ∏ 21 Œ∏ 13 2 h h u 4 u 2 Œ∏ 24 h u 3 u 2 Œ∏ 23 Œ∏ 34 2 Œ∏ 14 23 7 p u 4 u 1 u 2 u 3 h h h u 1 u 3 Œ∏ 31 h u 2 u 3 Œ∏ 32 Œ∏ 12 3 h h u 4 u 3 Œ∏ 34 h u 1 u 3 Œ∏ 31 Œ∏ 14 3 Œ∏ 24 13 8 p u 4 u 1 u 2 u 3 h h h u 1 u 3 Œ∏ 31 h u 2 u 3 Œ∏ 32 Œ∏ 12 3 h h u 4 u 3 Œ∏ 34 h u 2 u 3 Œ∏ 32 Œ∏ 24 3 Œ∏ 14 23 these different conditional formulas i e eqs 2 4 8 could be deemed as 6 different candidate vc models termed as vc1 vc6 given the predictors x 1 x 3 one can derive the predictand x 4 i e the current water level at time t from each of the above six conditional vc models the forecasting performance of each vc model might differ due to changes in model structure bivariate copula families and parameters applied for each pair of variables we used a r package i e vinecopula schepsmeier and brechmann 2015 to compute the akaike information criterion aic the smallest aic statistics determines the best bivariate copula families for the hierarchical vine copula tree structure when constructing the conditional vc model the potential copula families used in the study are the gaussian student t gumbel joe clayton and gumbel among the candidate vc models with different vine structure e g eqs 2 4 8 one can choose a single best model or make an average of the vc predictions generated from all vc candidate models as the ideal predictions however the rejection of other plausible candidate models with similar performance may lead to statistical bias and an underestimation of the predictive uncertainty inherent with the structural differences of the vc prediction models moreover the selection of best vc model highly depends on the evaluation measures which may vary as a result of changes in evaluation metrics therefore instead of using the single best model or a simple average we will use all the plausible models as candidate models for the purpose of bayesian based ensemble forecasts 3 3 2 bayesian based ensemble forecasts in the last step of the bvc construction we employ a multi model combination technique i e the bma which can merge forecasts from all candidates of the vc models with appropriate weights it enables to provide ensemble predictions by weighting candidate models based on their performance basically it will assign higher weights to models having better performance a brief description of the bma technique is given below raftery et al 2005 vrugt and robinson 2007 considering y as the ensemble forecast each candidate member i e the conditional vc prediction models presented above yields a forecast f f 1 f 2 f k assuming that we have k candidate models p k y f k d is the posterior distribution of y conditioning upon the forecast f k from the vc models and the training data d the posterior distribution of the bma forecasts can be written as 10 p y f 1 f 2 f k d k 1 k p f k d p k y f k d where p f k d is the posterior probability of the forecast of each candidate model it also reflects how well this candidate model f k fits the training data d duan et al 2007 let w k p f k d since w k is a probability value serving as the weights we should obtain k 1 k w k 1 the expectation maximization algorithm the algorithm given in table s1 will then be used to estimate these weights raftery et al 2005 the bma ensemble prediction is obtained by assigning higher weights to better candidate members and the spread of these predictions indicates the uncertainty associated with the vc model structures in this regard we combine the strengths of bma scheme with vc prediction models to establish an integrated bayesian vine copula model simply the bvc model the p k y f k d is estimated with the gamma distribution sloughter et al 2007 due to the highly skewed distribution of water level data in this study we compared the proposed bvc model to the vc model and the anfis the anfis first developed by jang et al 1997 is a hybrid machine learning model which combines the strengths of the conventional artificial neural network ann model and fuzzy system the learning algorithm for anfis is a combination of the gradient descent and least squares method anctil and tape 2004 anfis is capable of providing better forecasts compared to the conventional ann models and has gained the popularity in hydrological predictions badrzadeh et al 2013 moghaddamnia et al 2009 nourani et al 2011 partal and kisi 2007 more details about the anfis can be found in studies jang et al 1997 moghaddamnia et al 2009 3 4 performance measures we compared the forecast skill of our presented bvc model with the arithmetic mean of all candidate vc models and the anfis nourani et al 2014 three widely used metrics are applied to assess the forecast skills of different models that is the coefficient of determination r2 the root mean squared error rmse and the nash sutcliffe model efficiency coefficient nse bennett et al 2013 they offer evaluations from different perspectives including the degree of correlation between the observations and forecasts the discrepancy between the observations and forecasts and the relative magnitude of the residual variance compared to the observed data variance the formulas for these metrics are available in the supplementary material 4 results 4 1 comparison of overall forecast skills the performance of the dynamic bma scheme is shown with one event for each station the overall forecasting results during the whole validation period will be given in the following section one day ahead prediction is made at heyuan station on 31 july 2008 and at longchuan station on june 12 2007 fig 4 plots two examples of the bma predictive distribution which is a weighted sum of the pdf of six candidate vc models showing different distribution patterns the median value of the bma predictive distribution dictates the final forecast i e the bvc forecast it shows that observations fall in the 90 bma prediction interval and the bma is able to generate reasonable intervals and pdfs table 1 summarizes the respective weights of the six candidate models for the two examples in fig 4a and table 1 it shows that the vc5 vc6 and vc3 receive greater weights than other candidate members yet for the second case fig 4b the vc1 dominates the contributions of bayesian based ensemble forecast and other candidate vc models receive relatively smaller weights fig 5 illustrates the time series of observations and model forecasts of the proposed bvc model the arithmetic mean of vc models and anfis during the evaluation periods at heyuan station the results for longchuan station are given in fig s1 of the supplementary material there are evident differences in prediction performance among different models for the bvc model the total uncertainty is indicated by the shaded areas the 90 prediction uncertainty intervals of bvc model generally encompass the range of observations to complement the time series plots table 2 presents a quantitative comparison of different models with the performance metrics across these metrics it is clear to see that the bvc model marginally outperforms other models confirmed by the higher nse and r2 values and the lower rmse value for both stations the bayesian based model averaging strategies maximize the strengths of different candidate vc models to a certain degree thus delivering better overall performance according to the three metrics the performance of the arithmetic mean of six vc models during the validation period is superior to the deterministic model i e anfis in terms of the rmse r2 and nse also we show summary metrics regarding the forecast skills of each individual vc model at the two stations in table 2 the forecast skill of individual vc models varies between two stations and no individual vc model always provides the best prediction at these stations this inconsistency highlights the importance of the bayesian based ensemble predictions 4 2 forecast performance under different water level regimes we further provided the scatter plots of observations against the predicted water levels at the two stations for 1 day lead time forecasts with different models fig 6 those models can capture the majority of observations but the performance declines for more extreme water levels either high or low water levels indicated by the divergence from the 1 1 line a visual comparison of the scatter plots shows the predictions of bvc model are in a better agreement with the observations compared to other competing models this raises a question about how these models perform over different ranges of the observations during the evaluation period a simple way to consider how different approaches perform across different ranges of the observations is to divide the hydrograph into several parts e g low medium and high water levels we used 10 and 90 quantile thresholds to separate the historical observations into the low medium and high water level groups this enables us to examine how the proposed model and other competitors perform over different ranges of the hydrograph in terms of low water levels fig 7 a according to the predicted and observed flow duration curves fdcs all of the models either underestimate or overestimate the observations but the bvc model continues to perform the best given the smallest disparity with the observations in the medium range fig 7b the three models present similar results and all the forecasts are close to the observations except the anfis forecasts regarding the high water level fig 7c the bvc model keeps a better consistency with the observations this indicates the proposed bvc model outperforms the vc models arithmetic mean and the anfis for a particular sector of the hydrograph leading to a better overall performance across different metrics 4 3 forecast skill for longer lead times the results presented above focus on one step ahead prediction we also explored the model performance for longer lead times fig 8 presents summary metrics at 2 to 7 day lead time for water level prediction at the two stations it is clear that the performance of all models decreases with increasing lead times with respect to the three performance metrics this is due to less availability of information for longer lead time horizons and greater influence of predictors not included in the model the proposed bvc model tends to outperform the other methods indicated by the higher nse and r2 values and lower rmse values this in turn reveals that the bayesian techniques enable to combine the strengths of different candidate models and provide more reliable forecasts another point is that the performance with the arithmetic mean of vc models is not always better than the anfis according to the metrics during the validation period for instance for longer lead times at heyuan station the performance of anfis appears slightly superior to the arithmetic mean of vc models 4 4 forecast skill for monthly water levels in addition to the daily prediction monthly water level prediction for 1 month lead times was examined using the vc models the anfis and the bvc model fig 9 presents a comparison of the forecasts from these models during the evaluation period at the two stations as shown the bayesian based ensemble predictions the bvc model are in a better agreement with the observations than the forecasts produced by the arithmetic mean of six vine based models and the anfis the 90 uncertainty bounds generated from the bvc approach virtually capture all the observations also we show summary metrics regarding the forecast skills of individual models in table 3 for 1 3 month ahead water level prediction the results show that the bvc model generally provides better forecast skills than the other approaches the arithmetic mean outperforms the anfis the monthly forecasting thus also supports the advantage of the bvc model 4 5 effect of the number of predictors on the performance of the bvc model the vc models were established based on a 4 d vine copula with 3 lagged water levels as predictors i e antecedent water level time series at time t 1 t 2 and t 3 it remains unclear whether the addition of more variables as inputs i e establishing higher dimensional vc models will yield higher skills or not thus we considered more lagged water levels as input variables i e using the antecedent daily water levels from t 1 to t 4 4 predictors or from t 1 to t 5 5 predictors to forecast the water level at time t at longchuan station as an example again the bma scheme was used to combine the predictions from the candidate vc models in order to derive the ensemble predictions fig 10 a c shows that for the proposed bvc model the involvement of more time steps as predictors provides additional value in forecast skill to some extent this gain is accompanied by increased complexity in model structure and much more computational time resources in this particular case the forecast skill of the vc and bvc models is slightly lower for 4 antecedent predictors when compared to the results with 3 and 5 predictors 4 6 effect of the number of candidate vc models on the performance of the bvc model the above prediction based on the bvc model considers all vc models as candidates in other words the forecasts from all six vc models were used in the bvc model this makes us question whether it is necessary to include all these six vc models and how the number of candidate models may affect the performance of the ensemble model when using the multimodel averaging technique to explore these questions we again used the example of 1 day ahead water level forecast at longchuan station we tested the performance of the bvc models with different numbers of candidate vc models we selected different candidate members for instance three vc models vc1 vc3 were used as the candidate members to form for the 3 member bvc model and subsequently generate the ensemble forecasts as shown in fig 10d f the 5 member bvc model is already sufficient to provide satisfactory forecasts the use of additional candidate members i e the 6 member bvc model does not exceed the skill provided by the 5 member bvc model 5 discussion and conclusions to improve the forecasting performance of vine based models we proposed a hybrid bvc model the first component of the bvc model involves several conditional forecasting models based on different vine copula structures i e the vc models the bma approach was then used to combine the strengths of the forecasts from different vc models while accounting for the model uncertainty we compared the performance of the bvc model with the arithmetic mean of candidate vc models as well as the anfis model for daily and monthly water level prediction the results show that the bvc model which combines the strengths of different vc models generally outperforms other competing models both vine copula and bma approaches are easy to implement and the combination of these two techniques i e the bvc model provides a new alternative in hydrological forecasting despite its strength in water level prediction the presented bvc model has some limitations originally the vine copulas were used to model joint multivariate distributions here we explored its ability in establishing a statistical forecasting model i e the vc prediction model however such vc models involve a variety of vine copula types and mathematical structures thus increasing the prediction uncertainty the common assumption is that there is no best selection in vc model structures which motivates us to improve the vc model by using the model averaging scheme although the vc models perform well as compared to the traditional deterministic models e g the anfis the involvement of bayesian based averaging scheme i e the bvc model has significantly improved the overall forecast skill of the original vc models this is attributable to the strength of the bayesian based averaging scheme in combining various candidate models devineni et al 2008 doblas reyes et al 2000 relying on the complex structure and flexibility in modeling dependence between different variables the proposed bvc model achieves the best performance for the data used in the study one question that may be raised however whether an auto regression ar model with simpler structure may also yield quite well forecast skill the performance of the ar model strongly depends on the autocorrelation in the time series themselves e g the time series of water levels used here in this regard we included a simple auto regressive model as a comparison to be consistent antecedent time series at time t 1 t 2 and t 3 were also used as the predictors of the ar 3 model by taking the 1 3 month ahead water level prediction at longchuan station as an illustration table s1 in the supplementary material interestingly the ar 3 model also presents relatively well forecast skill for the one month ahead prediction during the evaluation period although the bvc approach with significant increment of the complexity in model structure remains keeping the superiority according to the performance metrics yet as for higher lead times i e 2 and 3 month ahead water level prediction the bvc model s strength is more apparent as compared to the ar 3 model the conventional auto regression methods may lack the absolute advantage in yielding accurate longer term predictions i e greater lags ahead in comparison with nascent copula based model moreover we also provided the performance metrices of different models during the training period table s2 we find the forecast skills of copula based models the bvc and vc models are slightly better than the traditional ar 3 and anfis models although the strength is not remarkable by comparing the performance metrices during the evaluation period this is expected because the higher performance in the training period could generally lead to better forecast skills in the evaluation period also the autocorrelation significantly declines in the monthly time series of water level at this station as the lag increasing fig 2d this could directly weaken forecast skill of traditional models heavily depending on the autocorrelation in time series we also explored the effect of the number of predictors and candidate members on the ensemble forecast skill we find the inclusion of more predictors and candidate members does not always guarantee additional gain in forecast skill of the bvc model in some cases the effect could be opposite that is more predictors or candidate members tend to worsen the forecast skill e g fig 10 despite skill improvements more candidates or predictors may significantly increase the computational time and burden we found that the daily water level prediction is more robust than the monthly prediction in the context of the three selected statistical metrics this can be attributed to the fact that daily water levels are more strongly correlated with the previous lagged water levels it is thus easier to model the short term water levels without other predictors for the monthly water levels the autocorrelation is apparently weaker than the daily values several existing studies with different data driven models have documented the effect of autocorrelation on the forecast skill kisi and cimen 2011 shortridge et al 2016 tiwari and adamowski 2013 in addition we found that the bvc models underestimated or overestimated the observations to some degree particularly for the extreme high water level values although it produced a better overall performance across different metrics this may be partially due to the fact that extreme high water level values are rare and the bvc model gives more weight to non extreme values another reason is that the theoretical marginal distributions e g gamma weibull distributions used are not so effective to represent the extreme behavior of the water levels which could affect the simulation of joint dependence structure of different variables with the vine copula 5 1 our results show the bvc model a potential improvement in the following study is to include other extreme value distributions such as generalized extreme value gev or generalized pareto distribution gpd to fit the marginal distribution of water levels this study used the conventional validation method i e partitioning the dataset into two sets of 80 for training and the remaining data for testing to evaluate the model performance which could be further improved with the cross validation method to give an example we applied the 4 fold cross validation method and evaluated 1 day ahead water level forecast at both longchuan station and heyuan station table s3 in the supplementary material we further compared the cross validation results with the conventional ones and our results reveal slight improvements regarding different evaluation metrics another limitation of this study is that we only used the previous lagged water levels as the predictors in the bvc model however the model is flexible to use other data as predictors if available for instance the regional climate variables e g precipitation and runoff play a critical role in extracting useful information for the river water level prediction in particular the daily water level strongly depends on the sudden occurrence of heavy rainfall moreover it has been widely documented that large scale oceanic and atmospheric climate signals such as enso have strong teleconnections with the hydrological variables in many regions chan and zhou 2005 ouyang et al 2014 such factors could provide valuable information used as the climate predictors for regional monthly or seasonal hydrological prediction this also points another possible improvement of the bvc model by involving more regional climate variables and large scale climate signals as potential predictors though further analysis of input variable selection may be beneficial galelli et al 2014 additionally the forecasting ability of the proposed approach at smaller timescales for example hourly forecasting is worth exploring the bvc approach was used for water level prediction as a demonstration in this study but we would like to highlight that the proposed model can be extended for forecasting other hydrometeorological variables such as river flows and precipitation and it has a great potential for broader applications in the hydroclimate field declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the first third and fourth author acknowledge the national natural science foundation of china 51809294 51779279 guangdong provincial department of science and technology 2019zt08g090 the national key r d program of china 2016yfc0402602 the water science and technology innovation project of guangdong province 2020 27 and the outstanding youth science foundation of nsfc 51822908 for the financial support we appreciate the constructive comments and suggestions from the editor associate editor and anonymous reviewers all data used this study are available for free on request from the author at the following e mail address liuzhiy25 mail sysu edu cn the code used in this study is available from https github com jeromelau11 bvc model appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105075 
25812,the present work introduces a computational tool petp v2 0 0 which allows calculation and analysis of reference evapotranspiration through empirical formulas from thornthwaite hargreaves christiansen penman monteith penman modiÔ¨Åed turc jensen haise stephens stewart linacre papadakis blaney and criddle serruto priestley taylor makkink valiantzas garcia lopez turc annual coutagne annual and radiation in addition the software has tools for the analysis of eto selection of the best results for the study site and export of results among others which are explained in detail in this paper petp v2 0 0 has been developed using the basic programming language under the integrated development environment visual studio 2010 which allows to synthesize manual labor as it allows to carry out calculations in an automatic and accurate way under different calculation conditions the eto parameter is important in calculating water demand carry out hydrological studies environmental studies and others which is why this new computational tool is implemented graphical abstract image 1 keywords reference evapotranspiration water demand empirical formulas peru software 1 introduction the use of water resources is of first necessity whether for population agricultural use in the generation of electrical energy etc as far as the use in agriculture is concerned it is necessary to be aware of the amount of water that crops require during their vegetative period to supply it in an appropriate quantity and at the right time hassanzadeh et al 2014 ortega farias et al 2009 in this context a knowledge of the water need of the crop is a must evapotranspiration et understood as the demand for water that different crops and plantations have for optimal agricultural or forestry development is a parameter of interest in the climatic classification of different environments it allows the establishment of periods of deficit or excess of water it depends on numerous meteorological factors including solar radiation as the fundamental energy source for the development of the process the air temperature as a consequence of the previous one the relative humidity as a measure of the evaporative capacity of the environment and the wind speed that constantly removes water from the evaporating surface and sometimes transports heat to keep the process active but in addition it depends on the characteristics of the vegetation of the area its type density and state of growth as well as the soil its properties and its moisture content potential evapotranspiration pet refers to the maximum amount of water that a soil surface completely covered with vegetation is capable of transferring to the atmosphere the vegetation grows in optimal conditions and without limitations in the water supply depending only on the prevailing atmospheric conditions at the moment when the estimate is made thornthwaite 1948 the concept of pet is made more precise when using the reference crop evapotranspiration eto introduced in order to study the evapotranspiration demand of the atmosphere regardless of the type and development of the crop allen et al 1998 the eto estimates the effects of climatic conditions on the et presented by a standard plant type taken as a reference usually alfalfa or a grass which grows with all the necessary water and disease free mckenney and rosenberg 1993 burman and pochop 1994 once the eto corresponding to the specific or standard vegetation surface is known it is possible to define empirical coefficients to calculate the et corresponding to other covers doorenbos and pruitt 1977 allen et al 1998 these coefficients make it possible to measure the water demands of an agricultural or forest area and to properly program irrigation thus achieving that the actual or effective evapotranspiration to be equal to the maximum requirement of the vegetation obtaining the best yields and making efficient use of water with this an adequate management of the water and economic resources necessary for the construction of irrigation works and irrigation planning can be carried out hargreaves 1994 xu and singh 2001 droogers and allen 2002 temesgen et al 2005 l√≥pez moreno et al 2009 in an attempt to calculate the water transferred to the atmosphere in a given area numerous so called direct theoretical or empirical models have been proposed thus et can be known either through direct measurements of the water that enters an area and returns to the atmosphere or through an indirect estimate of that amount these models are diverse in their complexity precision and data requirements for their application evaluation of simple reference evapotranspiration eto methods has received considerable attention in developing countries where the weather data needed to estimate eto by the penman monteith fao 56 fao56 pm model are often incomplete and or not available many researchers rather than determining the most precise method try to know the degree of variation presented by the results produced by the different equations itenfisu et al 2003 temesgen et al 2005 cadro et al 2017 this type of study is of great importance since there is a fairly direct relationship between the precision of the methods and the number of variables and data required for their application data that are rarely available in their entirety for this reason they apply methods of different requirements and consequently of different precision and compare the results to establish the degree of difference between the results and take it into account when making decisions for instance lu et al 2005 contrasted three temperature based thornthwaite hamon and hargreaves samani and three radiation based turc makkink and priestley taylor pet methods in southeastern united states they concluded that the six methods were highly correlated however multivariate statistical tests showed that pet values were significantly different greater differences were found among the temperature based pet methods than radiation based methods alkaeed et al 2006 compared six different reference evapotranspiration eto methods thornthwaite hargreaves hamon the penman monteith fao56 pm and the two simplified versions the solar radiation and the net radiation based equations in the western region of fukuoka city japan they concluded that when considering the availability and reliability of the input data the use of all these methods are suggested as practical methods for estimating eto if the standard fao56 pm equation is not applicable due to the complexity of its input parameters however the fao56 pm as a standard method remains the most desirable for estimating eto if accuracy of data collection is considered to be the main concern the performance of 16 eto equations were evaluated against the penman monteith equation under sahelian conditions in senegal djaman et al 2015 some of them overestimated eto and others underestimated it however temperature based equations of romanenko 1961 and schendel 1967 and mass transfer equations of trabert 1896 and mahringer 1970 performed well bandyopadhyay et al 2012 developed a decision support system with 22 eto estimation methods which was applied in india eighteen eto estimation methods including the fao56 pm were found to be applicable in the study area in a study carried out by tabari et al 2013 eight pan evaporation based seven temperature based four radiation based and ten mass transfer based methods were evaluated against the penman monteith fao56 pm model in iran in addition two radiation based methods for estimating eto were derived using air temperature and solar radiation data based on the fao56 pm model as a reference cross comparison of the 31 tested methods showed that the five best methods as compared with the fao56 pm model were the two radiation based equations developed the temperature based blaney criddle and hargreaves m4 equations and the snyder pan evaporation based equation there is no golden rule concerning the optimal equations to estimate reference potential evapotranspiration under various climates because even in the same climatic type different studies have produced different results in relation to the performance of the empirical equations different evapotranspiration calculation methods are used depending on the complexity of the hydrological model therefore it is useful to compare different methods in order to select the one that better performs in specific situations thorp et al 2019 for the users to select the best eto estimation method for the available data and climatic condition to ease this task some software tools have been developed to compute evapotranspiration applying different methods for instance cropwat smith 1992 is a program developed by fao to calculate the etp by the fao56 pm method climate data entry requires a specific format climwat smith 1993 and only allows monthly etp calculation cropwat was succeeded by ref et allen 2000 which allows to calculate daily etp and associated variables computations are based on recommendations presented in asce manual 70 it is intended to supplement eto computation routines it applies 15 of the most common methods and equations that are currently used in the united states and europe empest kostinakis et al 2011 includes 13 different approaches to estimate the potential evapotranspiration the number of equations that will finally produce reference pet estimates depends on data availability a hydroinformatic system for estimating evapotranspiration has been designed and developed to provide an improved understanding of the spatial variation in evapotranspiration naoum and tsanis 2003 the reference evapotranspiration is calculated via the penman montheith approach as well as the class a pan evaporation records guo et al 2016 introduce an r package to estimate actual potential and reference et using 17 well known models results are presented as summary text and plots for further analysis the objective of this article is to introduce the petp v2 0 0 software that is aimed at researchers teachers and students for use in academic situations as well as for professional work in order to simplify the laborious calculation of water demand for systems irrigation by gravity sprinkling and localization in hydrological models water balance in basins planning and management of water resources environmental studies and others 2 materials and methods nowadays a large number of countries do not have lysimeters to measure evapotranspiration due to the high costs of implementation and operation that these imply in these cases eto estimation must be made using empirical methodologies that a large number of authors have proposed they are still in use today from the initial definitions the concept of evapotranspiration has always been linked to a mathematical expression that includes different climatological variables that intervene in the said process cleves et al 2016 farahani et al 2007 tabari et al 2013 in order to simplify the laborious calculation which includes the interpolation of values or solving extensive equations in carrying out evaporation estimation many tools have been designed naoum and tsanis 2003 adeloye et al 2012 dutta et al 2016 the petp v1 0 0 software was developed with few tools and had some deficiencies in data entry this software and the manual was developed in the frame of the water and energy research program of the faculty of agricultural sciences of the national university of san crist√≥bal de huamanga in ayacucho peru subsequently improvements were made until obtaining the petp v2 0 0 software project with which it was possible to systematize each of the 22 methodologies developed in the computational package this tool allows choosing the eto that best fits to a given study area using the following methods 2 1 thornthwaite method thornthwaite 1948 found that evapotranspiration was proportional to the mean temperature affected by an exponential coefficient to calculate reference evapotranspiration with this method the process is as follows almorox et al 2015 determination of the monthly thermal index i 1 i n t m 5 1 514 determination of the annual thermal index i 2 i n 1 12 i n determination of the exponent a which varies with the annual heat index 3 a 0 675 10 6 i 3 0 771 10 4 i 2 0 01792 i 0 49239 calculation the monthly evapotranspiration eto in mm per month with the following relation vill√≥n 2004 4 e t o 16 10 t m i a correction of the eto value according to the month considered and the number of hours of sunshine for the location in question 5 e t o e t o n 12 d 30 where eto corrected monthly reference evapotranspiration mm month eto uncorrected monthly reference evapotranspiration n average daily maximum duration of hours of strong insolation d number of days of the month tm average monthly temperature in c 2 2 hargreaves method 2 2 1 radiation based based on registered data on solar radiation v√°squez and v√°squez 2009 6 e t o 0 004 t m f r s where eto reference evapotranspiration mm month tmf average monthly temperature f rs average monthly solar radiation cal cm2 day based on extraterrestrial radiation allen et al 1998 7 e t o 0 0023 r a t m 17 8 t d 8 t m t m a x t m i n 2 9 t d t m a x t m i n where eto daily reference evapotranspiration mm day tm average daily temperature c ra extraterrestrial solar radiation mm day tmax maximum daily temperature c tmin minimum daily temperature c based on equivalent solar radiation data hoyos 2002 10 e t o 0 0075 r s m t m f f a where eto reference evapotranspiration mm month rsm equivalent solar radiation in mm of monthly evaporation tmf average monthly temperature f fa altitude correction factor 11 r s m 0 075 r m m s 12 s n n 100 13 r m m r a d where s percentage of hours of insolation n hours of average strong insolation of the place heliograph n hours of strong insolation of the place depending on the month and the latitude rmm extraterrestrial equivalent radiation in mm of monthly evaporation mm month ra extraterrestrial equivalent radiation in mm of daily evaporation mm day d number of days of the month that is analyzed 2 2 2 based on temperature 14 e t o m f t m f c h c e where eto reference evapotranspiration mm month mf monthly latitude factor tmf average monthly temperature f ch correction factor for the monthly average relative humidity ce correction factor for the height or elevation of the location v√°squez and v√°squez 2009 2 3 christiansen method 15 e t o k t r a c t c v c h c s c e where eto reference evapotranspiration mm day kt 0 324 dimensionless constant ra equivalent extraterrestrial radiation in mm of daily evaporation mm day ct correction factor for temperature cv correction factor for wind ch correction factor for humidity cs correction factor for hours of sunshine ce correction factor for height or elevation mar√≠n valencia 2010 christiansen 1968 describes in detail how to calculate the different parameters ct cv ch cs and ce 2 4 penman monteith method 16 e t o 0 408 Œ¥ r n g Œ≥ 900 t m 273 u 2 e s e a Œ¥ Œ≥ 1 0 34 u 2 where eto reference evapotranspiration mm day tm average monthly temperature c Œ¥ slope of the vapor pressure curve kpa Œ≥ psychometric constant kpa c u2 wind speed at 2 m height m s es saturation vapor pressure kpa ea actual vapor pressure kpa es ea vapor pressure deficit rn net radiation on the crop surface mj m2 day g soil heat flux mj m2 day allen et al 1998 2 5 modified penman method 17 e t o c w r n 1 w f u e s e a where eto reference evapotranspiration mm day c correction factor to compensate for the effects of the weather during the day and at night w weighting factor which considers the effect of radiation on eto at different temperatures and altitudes rn net radiation in equivalent evaporation mm day f u wind related function es saturation vapor pressure millibars ea actual vapor pressure millibars es ea vapor pressure deficit doorenbos and pruitt 1977 2 6 turc method 18 e t o f t m t m 15 r s 50 c where eto reference evapotranspiration in mm month f monthly correction factor its values are 0 37 for february and 0 4 for 30 and 31 day months tm average monthly temperature in c rs solar or short wave radiation in cal cm2 day c correction factor for arid areas which depends on the relative humidity of the month almorox et al 2015 2 7 jensen haise method 19 e t o r s 0 025 t m 0 08 where eto reference evapotranspiration mm day rs solar radiation mm day tm average daily temperature c goyal and ram√≠rez 2007 2 8 stephens stewart method 20 e t o 0 01476 t m 4 905 r s 59 59 0 055 t m where eto reference evapotranspiration mm day tm average monthly temperature c rs monthly solar radiation cal cm2 day goyal and gonz√°lez fuentes 2007 2 9 linacre method 21 e t o 500 t m 0 006 h 100 a 15 t m t p r 80 t m where eto reference evapotranspiration in mm day tm average monthly temperature c a latitude degrees tpr average dew point temperature c h height above sea level m guijarro pastor 1978 2 10 papadakis method 22 e t o 5 625 e o t i e o t i 2 where eto reference evapotranspiration mm month e ti vapor saturation tension for the average temperature of the maximums of the month considered mb e ti 2 vapor saturation tension for the minimum average temperature minus 2 c mb almorox et al 2015 2 11 blaney and criddle method 23 f p 0 46 t m 8 13 where f blaney criddle factor expressed in millimeters of water daily it has the same value for all days of the month considered p monthly rate of annual daytime hours tm average monthly temperature expressed in c fuentes 2003 2 12 radiation method 24 e t o w r s c where eto reference evapotranspiration in mm day rs solar or short wave radiation expressed in equivalent evaporation in mm day w weighting factor which depends on the temperature and latitude c adjustment factor which depends on the estimated values of humidity and wind fuentes 2003 2 13 serruto method 25 e t o 0 003 r a 2 5 0 16 t m 0 88 where eto reference evapotranspiration in mm day ra extraterrestrial radiation mm day tm average monthly temperature c flores et al 2015 2 14 priestley taylor method 26 e t o 1 26 Œª Œ¥ Œ¥ Œ≥ r n g where eto reference evapotranspiration in mm day Œ¥ slope of the vapor pressure curve kpa c Œ≥ psychometric constant kpa c rn net radiation on the crop surface mj m2 day g heat flux density in the soil mj m2 day it is considered 0 for a daily scale Œª latent heat of vaporization equal to 2 45 mj kg contreras silva 2015 2 15 makkink method 27 e t o Œ± r s Œ¥ Œ¥ Œ≥ Œ≤ where eto reference evapotranspiration in mm day Œ¥ slope of the vapor pressure curve kpa c Œ≥ psychometric constant kpa c rs solar or short wave radiation mm day in general Œ± takes the value of 0 61 and Œ≤ 0 12 shahidian et al 2007 2 16 valiantzas method 28 e t o 0 05 1 Œ± r s t m 9 5 0 188 t m 13 r s r a 0 194 1 0 00015 t m 45 2 h r 100 0 0165 r s u 2 0 7 0 0585 t m 17 u 2 0 75 1 0 00043 t m a x t m i n 2 2 h r 100 1 0 00043 t m a x t m i n 2 0 0001 z where eto reference evapotranspiration in mm day Œ± 0 23 dimensionless tm average air temperature at 2 m high c tmax maximum air temperature c tmin minimum air temperature c rs solar or shortwave radiation mj m2 day ra extraterrestrial radiation mj m2 day hr average relative humidity z elevation above sea level of the study area m u2 wind speed at 2 m height m s contreras silva 2015 valiantzas 2015 2 17 garc√≠a and l√≥pez method 29 e t o 1 21 10 n 1 0 01 h r 0 21 t m 2 3 30 n 7 45 t m 234 7 t m 31 h r h r 8 00 a m h r 2 00 p m 2 where eto reference evapotranspiration mm day tm average monthly air temperature c hr average daytime relative humidity the factor n dependent on the average daily temperature is determined by means of equation 30 mar√≠n valencia 2010 2 18 turc annual method turc s formula is used to calculate annual evapotranspiration in mm 32 e t o p m 0 9 p m l 2 33 l 300 25 t m 0 05 t m 2 where eto average annual real evapotranspiration mm year pm average annual precipitation mm tm annual average temperature in c s√°nchez 2010 2 19 coutagne annual method coutagne s formula is used to calculate the annual evapotranspiration in m 34 e t o p m x p m 2 35 x 1 0 8 0 14 t m where eto average annual real evapotranspiration m year pm average annual precipitation meters per year s√°nchez 2010 for the development of the computational tool the visual basic programming language was used under the integrated development environment visual studio 2010 which allows compiling applications for 32 bit and 64 bit operating systems the tool solves the equations of the previously described methods for which they use meteorological variables and interpolates according to the geographical location of the study area 3 results and discussion two application examples were realized in order to show the advantages of the petp v2 0 0 model and to visualize the calculation of reference evapotranspiration and the respective analyzes one of them carried out in peru and the other in ecuador the data base to run these examples will be available in the software after installation or downloaded from the link https bit ly 2yrtsun finally the results of the penman monteith method obtained with the cropwat v8 0 smith 1992 and the petp v2 0 0 models were compared 3 1 example of application in tambillo peru 3 1 1 data project name tambillo sprinkler irrigation system country per√∫ department ayacucho province huamanga district tambillo place tambillo latitude 13 08 50 s longitude 74 06 22 w altitude 3250 masl 3 1 2 data processing after entering meteorological data table 1 into petp v2 0 0 validation will be performed the software verifies the adequate data entry meteorological and location the methods are then activated and the message validation process completed successfully is displayed and the calculation is carried out if the calculation process is successfully completed a notification window is also displayed which indicates that the formulas have been processed and displays a description of each of them see fig 1 3 1 3 verification of results the results can be verified by entering the module for each of the methodologies processed the results of the thornthwaite hargreaves 01 and penman monteith methods are shown figs 2 7 show the tabular and graphic results of the mentioned methodologies 3 1 4 exporting results the export of results can be done in two ways 1st way with the tool export to excel located in the methodology module this tool allows to export the detailed table and the graph of the current methodology fig 8 2nd way with the tool export results located in the toolbar of the main interface this tool allows exporting detailed tables and graphs of all the methodologies applied this process may take a few seconds see fig 9 3 1 5 averaging analyzed methodologies use the tool summary of results located in the main interface of petp v2 0 0 see fig 10 fig 11 shows a summary table with all the methodologies processed in the last column with red text is the average of all of them with the graph tab the monthly variation of the eto mm day of all the methodologies processed can be obtained as well as the average of all of these with a solid line in black see fig 12 this summary table can also be exported to excel 3 1 6 choosing eto for the work area go to the eto selection tab by right clicking within the list of methodologies the options are displayed to select or deselect any method according to the user s criteria and based on the summary graph the methods that are appropriate to the work area must be marked with a check sign see fig 13 here are some selection criteria the trend that each of them has as well as the behavior pattern of the precipitation based on the criteria indicated by each author in the use of these formulas based on data measured in the field evaporation tanks lysimeters etc for neighboring areas and with characteristics similar to the place of study as can be seen in fig 13 a summary table has been created with the selected methodologies a new tab has also been added which contains the monthly graph of the selected methods and the chosen eto for the work area see fig 14 3 2 example of application in portoviejo ecuador 3 2 1 data project name portoviejo irrigation system country ecuador region regi√≥n costa litoral province manab√≠ place portoviejo latitude 01 02 26 s longitude 80 27 54 w altitude 46 0 masl 3 2 2 data entry select the project location option and fill in the fields as indicated in fig 15 then the meteorological data for the study area is entered table 2 only the available data will be entered it is not necessary to enter all the columns but incomplete columns should not be left as the program will understand that parameter as not entered as can be seen in fig 16 no solar radiation data has been entered since the station does not record this parameter the data can be copied from an excel file and pasted directly as an option 3 2 3 validation and processing once the data has been entered the validation and calculation proceed if it has been processed correctly the following message is displayed calculation process completed successfully 3 2 4 verification of results figs 17 20 show the results obtained for the turc and valiantzas methods 3 2 5 exporting results when entering the methodologies the export tool can be noted exportation of results can be done in excel format as shown in fig 21 another way to export is from the main menu this tool allows to export all the methodologies processed see fig 22 3 2 6 averaging analyzed methodologies when entering the view menu and selecting the summary of results tool the average of all the methodologies processed is obtained see figs 23 and 24 3 2 7 choosing eto for the work area when selecting the eto selection tab a submenu is displayed in which the methodologies that best suit the work area can be chosen see figs 25 and 26 a ranking system similar to the one proposed by aschonitis et al 2019 could also be considered in future versions of petp v2 0 0 to compare the methods compiled this could also identify the best eto model in order to evaluate their performance and run different scenarios 3 3 comparison between cropwat v8 0 and petp v2 0 0 models data from the tambillo peru station were entered into the cropwat v 8 0 model smith 1992 obtaining the results shown in fig 27 it is worth clarifying that this model calculates the eto with the penman monteith method values obtained do not differ significantly in all months of the year see table 3 the small differences are due to rounding of the decimals since the cropwat v 8 0 model rounds to the integer in the parameters wind speed and relative humidity and to one decimal in the parameters of temperature and insolation duration see fig 27 4 conclusions petp v2 0 0 is an application that allows the calculation and analysis of reference evapotranspiration based on its development is worth noting the following a computational tool called petp v2 0 0 has been developed which allows the calculation and analysis of reference evapotranspiration using the visual basic programming language under the visual studio 2010 integrated development environment the manual calculation of eto requires the development of each formula for the historical series and use of tables which must be interpolated petp v2 0 0 allows to simplify the laborious calculation obtaining immediate and precise results the software shows in its results the monthly averages of eto for an average year the petp v2 0 0 and cropwat 8 0 models were run under the same calculation parameters using the penman monteith method when the results obtained with the petp v2 0 0 and the cropwat 8 0 were compared these were very similar petp v2 0 0 has been tested with meteorological data from various regions of latin america showing satisfactory results two examples were presented in this paper the petp v2 0 0 is versatile and can be used by researchers teachers and or students both in academic matters and in professional work funding not applicable availability of data and material data will be made available code availability software will be made available author s contributions cesar gutierrez ninahuaman conceptualization methodology software writing original draft preparation roger gonzalez herrera visualization supervision writing reviewing and editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors wish to thank engineers leonidas arias baltazar and rub√©n meneses rojas professors from the national university of san crist√≥bal de huamanga professional training schools of agricultural engineering and agronomy respectively for the suggestions made in carrying out the work manual eto empirical formulas to dr hans p s√°nchez tueros his contributions helped in improving the petp v2 0 0 software 
25812,the present work introduces a computational tool petp v2 0 0 which allows calculation and analysis of reference evapotranspiration through empirical formulas from thornthwaite hargreaves christiansen penman monteith penman modiÔ¨Åed turc jensen haise stephens stewart linacre papadakis blaney and criddle serruto priestley taylor makkink valiantzas garcia lopez turc annual coutagne annual and radiation in addition the software has tools for the analysis of eto selection of the best results for the study site and export of results among others which are explained in detail in this paper petp v2 0 0 has been developed using the basic programming language under the integrated development environment visual studio 2010 which allows to synthesize manual labor as it allows to carry out calculations in an automatic and accurate way under different calculation conditions the eto parameter is important in calculating water demand carry out hydrological studies environmental studies and others which is why this new computational tool is implemented graphical abstract image 1 keywords reference evapotranspiration water demand empirical formulas peru software 1 introduction the use of water resources is of first necessity whether for population agricultural use in the generation of electrical energy etc as far as the use in agriculture is concerned it is necessary to be aware of the amount of water that crops require during their vegetative period to supply it in an appropriate quantity and at the right time hassanzadeh et al 2014 ortega farias et al 2009 in this context a knowledge of the water need of the crop is a must evapotranspiration et understood as the demand for water that different crops and plantations have for optimal agricultural or forestry development is a parameter of interest in the climatic classification of different environments it allows the establishment of periods of deficit or excess of water it depends on numerous meteorological factors including solar radiation as the fundamental energy source for the development of the process the air temperature as a consequence of the previous one the relative humidity as a measure of the evaporative capacity of the environment and the wind speed that constantly removes water from the evaporating surface and sometimes transports heat to keep the process active but in addition it depends on the characteristics of the vegetation of the area its type density and state of growth as well as the soil its properties and its moisture content potential evapotranspiration pet refers to the maximum amount of water that a soil surface completely covered with vegetation is capable of transferring to the atmosphere the vegetation grows in optimal conditions and without limitations in the water supply depending only on the prevailing atmospheric conditions at the moment when the estimate is made thornthwaite 1948 the concept of pet is made more precise when using the reference crop evapotranspiration eto introduced in order to study the evapotranspiration demand of the atmosphere regardless of the type and development of the crop allen et al 1998 the eto estimates the effects of climatic conditions on the et presented by a standard plant type taken as a reference usually alfalfa or a grass which grows with all the necessary water and disease free mckenney and rosenberg 1993 burman and pochop 1994 once the eto corresponding to the specific or standard vegetation surface is known it is possible to define empirical coefficients to calculate the et corresponding to other covers doorenbos and pruitt 1977 allen et al 1998 these coefficients make it possible to measure the water demands of an agricultural or forest area and to properly program irrigation thus achieving that the actual or effective evapotranspiration to be equal to the maximum requirement of the vegetation obtaining the best yields and making efficient use of water with this an adequate management of the water and economic resources necessary for the construction of irrigation works and irrigation planning can be carried out hargreaves 1994 xu and singh 2001 droogers and allen 2002 temesgen et al 2005 l√≥pez moreno et al 2009 in an attempt to calculate the water transferred to the atmosphere in a given area numerous so called direct theoretical or empirical models have been proposed thus et can be known either through direct measurements of the water that enters an area and returns to the atmosphere or through an indirect estimate of that amount these models are diverse in their complexity precision and data requirements for their application evaluation of simple reference evapotranspiration eto methods has received considerable attention in developing countries where the weather data needed to estimate eto by the penman monteith fao 56 fao56 pm model are often incomplete and or not available many researchers rather than determining the most precise method try to know the degree of variation presented by the results produced by the different equations itenfisu et al 2003 temesgen et al 2005 cadro et al 2017 this type of study is of great importance since there is a fairly direct relationship between the precision of the methods and the number of variables and data required for their application data that are rarely available in their entirety for this reason they apply methods of different requirements and consequently of different precision and compare the results to establish the degree of difference between the results and take it into account when making decisions for instance lu et al 2005 contrasted three temperature based thornthwaite hamon and hargreaves samani and three radiation based turc makkink and priestley taylor pet methods in southeastern united states they concluded that the six methods were highly correlated however multivariate statistical tests showed that pet values were significantly different greater differences were found among the temperature based pet methods than radiation based methods alkaeed et al 2006 compared six different reference evapotranspiration eto methods thornthwaite hargreaves hamon the penman monteith fao56 pm and the two simplified versions the solar radiation and the net radiation based equations in the western region of fukuoka city japan they concluded that when considering the availability and reliability of the input data the use of all these methods are suggested as practical methods for estimating eto if the standard fao56 pm equation is not applicable due to the complexity of its input parameters however the fao56 pm as a standard method remains the most desirable for estimating eto if accuracy of data collection is considered to be the main concern the performance of 16 eto equations were evaluated against the penman monteith equation under sahelian conditions in senegal djaman et al 2015 some of them overestimated eto and others underestimated it however temperature based equations of romanenko 1961 and schendel 1967 and mass transfer equations of trabert 1896 and mahringer 1970 performed well bandyopadhyay et al 2012 developed a decision support system with 22 eto estimation methods which was applied in india eighteen eto estimation methods including the fao56 pm were found to be applicable in the study area in a study carried out by tabari et al 2013 eight pan evaporation based seven temperature based four radiation based and ten mass transfer based methods were evaluated against the penman monteith fao56 pm model in iran in addition two radiation based methods for estimating eto were derived using air temperature and solar radiation data based on the fao56 pm model as a reference cross comparison of the 31 tested methods showed that the five best methods as compared with the fao56 pm model were the two radiation based equations developed the temperature based blaney criddle and hargreaves m4 equations and the snyder pan evaporation based equation there is no golden rule concerning the optimal equations to estimate reference potential evapotranspiration under various climates because even in the same climatic type different studies have produced different results in relation to the performance of the empirical equations different evapotranspiration calculation methods are used depending on the complexity of the hydrological model therefore it is useful to compare different methods in order to select the one that better performs in specific situations thorp et al 2019 for the users to select the best eto estimation method for the available data and climatic condition to ease this task some software tools have been developed to compute evapotranspiration applying different methods for instance cropwat smith 1992 is a program developed by fao to calculate the etp by the fao56 pm method climate data entry requires a specific format climwat smith 1993 and only allows monthly etp calculation cropwat was succeeded by ref et allen 2000 which allows to calculate daily etp and associated variables computations are based on recommendations presented in asce manual 70 it is intended to supplement eto computation routines it applies 15 of the most common methods and equations that are currently used in the united states and europe empest kostinakis et al 2011 includes 13 different approaches to estimate the potential evapotranspiration the number of equations that will finally produce reference pet estimates depends on data availability a hydroinformatic system for estimating evapotranspiration has been designed and developed to provide an improved understanding of the spatial variation in evapotranspiration naoum and tsanis 2003 the reference evapotranspiration is calculated via the penman montheith approach as well as the class a pan evaporation records guo et al 2016 introduce an r package to estimate actual potential and reference et using 17 well known models results are presented as summary text and plots for further analysis the objective of this article is to introduce the petp v2 0 0 software that is aimed at researchers teachers and students for use in academic situations as well as for professional work in order to simplify the laborious calculation of water demand for systems irrigation by gravity sprinkling and localization in hydrological models water balance in basins planning and management of water resources environmental studies and others 2 materials and methods nowadays a large number of countries do not have lysimeters to measure evapotranspiration due to the high costs of implementation and operation that these imply in these cases eto estimation must be made using empirical methodologies that a large number of authors have proposed they are still in use today from the initial definitions the concept of evapotranspiration has always been linked to a mathematical expression that includes different climatological variables that intervene in the said process cleves et al 2016 farahani et al 2007 tabari et al 2013 in order to simplify the laborious calculation which includes the interpolation of values or solving extensive equations in carrying out evaporation estimation many tools have been designed naoum and tsanis 2003 adeloye et al 2012 dutta et al 2016 the petp v1 0 0 software was developed with few tools and had some deficiencies in data entry this software and the manual was developed in the frame of the water and energy research program of the faculty of agricultural sciences of the national university of san crist√≥bal de huamanga in ayacucho peru subsequently improvements were made until obtaining the petp v2 0 0 software project with which it was possible to systematize each of the 22 methodologies developed in the computational package this tool allows choosing the eto that best fits to a given study area using the following methods 2 1 thornthwaite method thornthwaite 1948 found that evapotranspiration was proportional to the mean temperature affected by an exponential coefficient to calculate reference evapotranspiration with this method the process is as follows almorox et al 2015 determination of the monthly thermal index i 1 i n t m 5 1 514 determination of the annual thermal index i 2 i n 1 12 i n determination of the exponent a which varies with the annual heat index 3 a 0 675 10 6 i 3 0 771 10 4 i 2 0 01792 i 0 49239 calculation the monthly evapotranspiration eto in mm per month with the following relation vill√≥n 2004 4 e t o 16 10 t m i a correction of the eto value according to the month considered and the number of hours of sunshine for the location in question 5 e t o e t o n 12 d 30 where eto corrected monthly reference evapotranspiration mm month eto uncorrected monthly reference evapotranspiration n average daily maximum duration of hours of strong insolation d number of days of the month tm average monthly temperature in c 2 2 hargreaves method 2 2 1 radiation based based on registered data on solar radiation v√°squez and v√°squez 2009 6 e t o 0 004 t m f r s where eto reference evapotranspiration mm month tmf average monthly temperature f rs average monthly solar radiation cal cm2 day based on extraterrestrial radiation allen et al 1998 7 e t o 0 0023 r a t m 17 8 t d 8 t m t m a x t m i n 2 9 t d t m a x t m i n where eto daily reference evapotranspiration mm day tm average daily temperature c ra extraterrestrial solar radiation mm day tmax maximum daily temperature c tmin minimum daily temperature c based on equivalent solar radiation data hoyos 2002 10 e t o 0 0075 r s m t m f f a where eto reference evapotranspiration mm month rsm equivalent solar radiation in mm of monthly evaporation tmf average monthly temperature f fa altitude correction factor 11 r s m 0 075 r m m s 12 s n n 100 13 r m m r a d where s percentage of hours of insolation n hours of average strong insolation of the place heliograph n hours of strong insolation of the place depending on the month and the latitude rmm extraterrestrial equivalent radiation in mm of monthly evaporation mm month ra extraterrestrial equivalent radiation in mm of daily evaporation mm day d number of days of the month that is analyzed 2 2 2 based on temperature 14 e t o m f t m f c h c e where eto reference evapotranspiration mm month mf monthly latitude factor tmf average monthly temperature f ch correction factor for the monthly average relative humidity ce correction factor for the height or elevation of the location v√°squez and v√°squez 2009 2 3 christiansen method 15 e t o k t r a c t c v c h c s c e where eto reference evapotranspiration mm day kt 0 324 dimensionless constant ra equivalent extraterrestrial radiation in mm of daily evaporation mm day ct correction factor for temperature cv correction factor for wind ch correction factor for humidity cs correction factor for hours of sunshine ce correction factor for height or elevation mar√≠n valencia 2010 christiansen 1968 describes in detail how to calculate the different parameters ct cv ch cs and ce 2 4 penman monteith method 16 e t o 0 408 Œ¥ r n g Œ≥ 900 t m 273 u 2 e s e a Œ¥ Œ≥ 1 0 34 u 2 where eto reference evapotranspiration mm day tm average monthly temperature c Œ¥ slope of the vapor pressure curve kpa Œ≥ psychometric constant kpa c u2 wind speed at 2 m height m s es saturation vapor pressure kpa ea actual vapor pressure kpa es ea vapor pressure deficit rn net radiation on the crop surface mj m2 day g soil heat flux mj m2 day allen et al 1998 2 5 modified penman method 17 e t o c w r n 1 w f u e s e a where eto reference evapotranspiration mm day c correction factor to compensate for the effects of the weather during the day and at night w weighting factor which considers the effect of radiation on eto at different temperatures and altitudes rn net radiation in equivalent evaporation mm day f u wind related function es saturation vapor pressure millibars ea actual vapor pressure millibars es ea vapor pressure deficit doorenbos and pruitt 1977 2 6 turc method 18 e t o f t m t m 15 r s 50 c where eto reference evapotranspiration in mm month f monthly correction factor its values are 0 37 for february and 0 4 for 30 and 31 day months tm average monthly temperature in c rs solar or short wave radiation in cal cm2 day c correction factor for arid areas which depends on the relative humidity of the month almorox et al 2015 2 7 jensen haise method 19 e t o r s 0 025 t m 0 08 where eto reference evapotranspiration mm day rs solar radiation mm day tm average daily temperature c goyal and ram√≠rez 2007 2 8 stephens stewart method 20 e t o 0 01476 t m 4 905 r s 59 59 0 055 t m where eto reference evapotranspiration mm day tm average monthly temperature c rs monthly solar radiation cal cm2 day goyal and gonz√°lez fuentes 2007 2 9 linacre method 21 e t o 500 t m 0 006 h 100 a 15 t m t p r 80 t m where eto reference evapotranspiration in mm day tm average monthly temperature c a latitude degrees tpr average dew point temperature c h height above sea level m guijarro pastor 1978 2 10 papadakis method 22 e t o 5 625 e o t i e o t i 2 where eto reference evapotranspiration mm month e ti vapor saturation tension for the average temperature of the maximums of the month considered mb e ti 2 vapor saturation tension for the minimum average temperature minus 2 c mb almorox et al 2015 2 11 blaney and criddle method 23 f p 0 46 t m 8 13 where f blaney criddle factor expressed in millimeters of water daily it has the same value for all days of the month considered p monthly rate of annual daytime hours tm average monthly temperature expressed in c fuentes 2003 2 12 radiation method 24 e t o w r s c where eto reference evapotranspiration in mm day rs solar or short wave radiation expressed in equivalent evaporation in mm day w weighting factor which depends on the temperature and latitude c adjustment factor which depends on the estimated values of humidity and wind fuentes 2003 2 13 serruto method 25 e t o 0 003 r a 2 5 0 16 t m 0 88 where eto reference evapotranspiration in mm day ra extraterrestrial radiation mm day tm average monthly temperature c flores et al 2015 2 14 priestley taylor method 26 e t o 1 26 Œª Œ¥ Œ¥ Œ≥ r n g where eto reference evapotranspiration in mm day Œ¥ slope of the vapor pressure curve kpa c Œ≥ psychometric constant kpa c rn net radiation on the crop surface mj m2 day g heat flux density in the soil mj m2 day it is considered 0 for a daily scale Œª latent heat of vaporization equal to 2 45 mj kg contreras silva 2015 2 15 makkink method 27 e t o Œ± r s Œ¥ Œ¥ Œ≥ Œ≤ where eto reference evapotranspiration in mm day Œ¥ slope of the vapor pressure curve kpa c Œ≥ psychometric constant kpa c rs solar or short wave radiation mm day in general Œ± takes the value of 0 61 and Œ≤ 0 12 shahidian et al 2007 2 16 valiantzas method 28 e t o 0 05 1 Œ± r s t m 9 5 0 188 t m 13 r s r a 0 194 1 0 00015 t m 45 2 h r 100 0 0165 r s u 2 0 7 0 0585 t m 17 u 2 0 75 1 0 00043 t m a x t m i n 2 2 h r 100 1 0 00043 t m a x t m i n 2 0 0001 z where eto reference evapotranspiration in mm day Œ± 0 23 dimensionless tm average air temperature at 2 m high c tmax maximum air temperature c tmin minimum air temperature c rs solar or shortwave radiation mj m2 day ra extraterrestrial radiation mj m2 day hr average relative humidity z elevation above sea level of the study area m u2 wind speed at 2 m height m s contreras silva 2015 valiantzas 2015 2 17 garc√≠a and l√≥pez method 29 e t o 1 21 10 n 1 0 01 h r 0 21 t m 2 3 30 n 7 45 t m 234 7 t m 31 h r h r 8 00 a m h r 2 00 p m 2 where eto reference evapotranspiration mm day tm average monthly air temperature c hr average daytime relative humidity the factor n dependent on the average daily temperature is determined by means of equation 30 mar√≠n valencia 2010 2 18 turc annual method turc s formula is used to calculate annual evapotranspiration in mm 32 e t o p m 0 9 p m l 2 33 l 300 25 t m 0 05 t m 2 where eto average annual real evapotranspiration mm year pm average annual precipitation mm tm annual average temperature in c s√°nchez 2010 2 19 coutagne annual method coutagne s formula is used to calculate the annual evapotranspiration in m 34 e t o p m x p m 2 35 x 1 0 8 0 14 t m where eto average annual real evapotranspiration m year pm average annual precipitation meters per year s√°nchez 2010 for the development of the computational tool the visual basic programming language was used under the integrated development environment visual studio 2010 which allows compiling applications for 32 bit and 64 bit operating systems the tool solves the equations of the previously described methods for which they use meteorological variables and interpolates according to the geographical location of the study area 3 results and discussion two application examples were realized in order to show the advantages of the petp v2 0 0 model and to visualize the calculation of reference evapotranspiration and the respective analyzes one of them carried out in peru and the other in ecuador the data base to run these examples will be available in the software after installation or downloaded from the link https bit ly 2yrtsun finally the results of the penman monteith method obtained with the cropwat v8 0 smith 1992 and the petp v2 0 0 models were compared 3 1 example of application in tambillo peru 3 1 1 data project name tambillo sprinkler irrigation system country per√∫ department ayacucho province huamanga district tambillo place tambillo latitude 13 08 50 s longitude 74 06 22 w altitude 3250 masl 3 1 2 data processing after entering meteorological data table 1 into petp v2 0 0 validation will be performed the software verifies the adequate data entry meteorological and location the methods are then activated and the message validation process completed successfully is displayed and the calculation is carried out if the calculation process is successfully completed a notification window is also displayed which indicates that the formulas have been processed and displays a description of each of them see fig 1 3 1 3 verification of results the results can be verified by entering the module for each of the methodologies processed the results of the thornthwaite hargreaves 01 and penman monteith methods are shown figs 2 7 show the tabular and graphic results of the mentioned methodologies 3 1 4 exporting results the export of results can be done in two ways 1st way with the tool export to excel located in the methodology module this tool allows to export the detailed table and the graph of the current methodology fig 8 2nd way with the tool export results located in the toolbar of the main interface this tool allows exporting detailed tables and graphs of all the methodologies applied this process may take a few seconds see fig 9 3 1 5 averaging analyzed methodologies use the tool summary of results located in the main interface of petp v2 0 0 see fig 10 fig 11 shows a summary table with all the methodologies processed in the last column with red text is the average of all of them with the graph tab the monthly variation of the eto mm day of all the methodologies processed can be obtained as well as the average of all of these with a solid line in black see fig 12 this summary table can also be exported to excel 3 1 6 choosing eto for the work area go to the eto selection tab by right clicking within the list of methodologies the options are displayed to select or deselect any method according to the user s criteria and based on the summary graph the methods that are appropriate to the work area must be marked with a check sign see fig 13 here are some selection criteria the trend that each of them has as well as the behavior pattern of the precipitation based on the criteria indicated by each author in the use of these formulas based on data measured in the field evaporation tanks lysimeters etc for neighboring areas and with characteristics similar to the place of study as can be seen in fig 13 a summary table has been created with the selected methodologies a new tab has also been added which contains the monthly graph of the selected methods and the chosen eto for the work area see fig 14 3 2 example of application in portoviejo ecuador 3 2 1 data project name portoviejo irrigation system country ecuador region regi√≥n costa litoral province manab√≠ place portoviejo latitude 01 02 26 s longitude 80 27 54 w altitude 46 0 masl 3 2 2 data entry select the project location option and fill in the fields as indicated in fig 15 then the meteorological data for the study area is entered table 2 only the available data will be entered it is not necessary to enter all the columns but incomplete columns should not be left as the program will understand that parameter as not entered as can be seen in fig 16 no solar radiation data has been entered since the station does not record this parameter the data can be copied from an excel file and pasted directly as an option 3 2 3 validation and processing once the data has been entered the validation and calculation proceed if it has been processed correctly the following message is displayed calculation process completed successfully 3 2 4 verification of results figs 17 20 show the results obtained for the turc and valiantzas methods 3 2 5 exporting results when entering the methodologies the export tool can be noted exportation of results can be done in excel format as shown in fig 21 another way to export is from the main menu this tool allows to export all the methodologies processed see fig 22 3 2 6 averaging analyzed methodologies when entering the view menu and selecting the summary of results tool the average of all the methodologies processed is obtained see figs 23 and 24 3 2 7 choosing eto for the work area when selecting the eto selection tab a submenu is displayed in which the methodologies that best suit the work area can be chosen see figs 25 and 26 a ranking system similar to the one proposed by aschonitis et al 2019 could also be considered in future versions of petp v2 0 0 to compare the methods compiled this could also identify the best eto model in order to evaluate their performance and run different scenarios 3 3 comparison between cropwat v8 0 and petp v2 0 0 models data from the tambillo peru station were entered into the cropwat v 8 0 model smith 1992 obtaining the results shown in fig 27 it is worth clarifying that this model calculates the eto with the penman monteith method values obtained do not differ significantly in all months of the year see table 3 the small differences are due to rounding of the decimals since the cropwat v 8 0 model rounds to the integer in the parameters wind speed and relative humidity and to one decimal in the parameters of temperature and insolation duration see fig 27 4 conclusions petp v2 0 0 is an application that allows the calculation and analysis of reference evapotranspiration based on its development is worth noting the following a computational tool called petp v2 0 0 has been developed which allows the calculation and analysis of reference evapotranspiration using the visual basic programming language under the visual studio 2010 integrated development environment the manual calculation of eto requires the development of each formula for the historical series and use of tables which must be interpolated petp v2 0 0 allows to simplify the laborious calculation obtaining immediate and precise results the software shows in its results the monthly averages of eto for an average year the petp v2 0 0 and cropwat 8 0 models were run under the same calculation parameters using the penman monteith method when the results obtained with the petp v2 0 0 and the cropwat 8 0 were compared these were very similar petp v2 0 0 has been tested with meteorological data from various regions of latin america showing satisfactory results two examples were presented in this paper the petp v2 0 0 is versatile and can be used by researchers teachers and or students both in academic matters and in professional work funding not applicable availability of data and material data will be made available code availability software will be made available author s contributions cesar gutierrez ninahuaman conceptualization methodology software writing original draft preparation roger gonzalez herrera visualization supervision writing reviewing and editing declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgments the authors wish to thank engineers leonidas arias baltazar and rub√©n meneses rojas professors from the national university of san crist√≥bal de huamanga professional training schools of agricultural engineering and agronomy respectively for the suggestions made in carrying out the work manual eto empirical formulas to dr hans p s√°nchez tueros his contributions helped in improving the petp v2 0 0 software 
25813,process based crop models are popular tools to analyze and simulate the response of agricultural systems to weather agronomic or genetic factors they are often developed in modeling platforms to ensure their future extension and to couple different crop models with a soil model and a crop management event scheduler the intercomparison and improvement of crop simulation models is difficult due to the lack of efficient methods for exchanging biophysical processes between modeling platforms we developed crop2ml a modeling framework that enables the description and the assembly of crop model components independently of the formalism of modeling platforms and the exchange of components between platforms crop2ml is based on a declarative architecture of modular model representation to describe the biophysical processes and their transformation to model components that conform to crop modeling platforms here we present crop2ml framework and describe the mechanisms of import and export between crop2ml and modeling platforms keywords crop model crop2ml component based software model exchange and reuse 1 introduction the wide range of crop process based models pbm reflects the evolution of our knowledge of the soil plant atmosphere system and the rich historical development for more than five decades reviewed in jones et al 2017 muller and martre 2019 the high diversity of pbm is due to their multiple applications and the complexity of the system influenced by several factors e g weather soil crop management basso et al 2013 and genotypic factors wang et al 2019 most of the pbm are continuous models formalized using ordinary differential equations but are implemented as discrete time simulation models using finite difference equations they are commonly decomposed into simpler biophysical functions e g phenology morphogenesis resource acquisition pests and diseases impact often implemented by recurrent equations with control flows another common characteristic is that pbm simulate plant growth and development at the scale of the canopy or average plant level without spatial dependence with a daily or sub daily time step pbm are often implemented in modeling and simulation platforms at a higher level of abstraction to facilitate model development rizzoli et al 2008 these platforms offer not only scalable modular and robust modelling solutions but also the ability to analyze evaluate reuse and combine models the diversity of pbm led the crop modeling community to compare their performance and to improve them by aggregating modelers knowledge or by introducing improvements provided from diverse research groups under the umbrella of large international collaborative projects such as the agricultural model intercomparison and improvement project agmip rosenzweig et al 2013 studies conducted in the context of model intercomparison and improvement exercises e g asseng et al 2013 wang et al 2017 pointed out the large uncertainty of pbm simulations and have analyzed the sources of uncertainty or the processes involved these intercomparison results showed the potential and limits of pbm and highlighted the need to analyze models at the process level but also to exchange model components describing specific processes between simulation platforms e g donatelli et al 2014 wang et al 2017 the uncertainty of a pbm component may be related to its validity domain inputs parameters structure and the underlying scientific hypotheses walker et al 2003 epistemic uncertainty may arise from incomplete or lack of knowledge of these sources the uncertainty of pbm results from the aggregation of the uncertainty of each of its component refsgaard et al 2007 a framework that would allow the exchange of model components between different platforms would give crop modelers the ability to test alternative hypotheses in the same model thus helping to reduce epistemic uncertainty although most crop simulation platforms provide modular approaches and reuse techniques there is little exchange of pbm components between them despite theoretical and application interests pbm components often contain source code developed in different programming languages and are tightly coupled to the platforms therefore model components are not seamlessly reusable outside the modeling platforms in which they have been developed without recoding or wrapping them holzworth et al 2014 rizzoli et al 2008 re implementing a component in several platforms is a tedious and cumbersome task and requires a minimum knowledge of the different platforms the wrapping solution treats components as black boxes taking little or no advantage of the framework rizzoli et al 2008 or as white boxes but with a high level of complexity fernique and pradal 2018 pradal et al 2008 other reuse approaches in environmental modeling have been explored declarative modeling can provide portability and facilitate integration between independent uncoordinated models athanasiadis and villa 2013 however model specifications are seldom separate from implementation details model builders rely often directly on implementation that hides the scientific content of a model i e its algorithm and its structure moreover the publication of pbm components in scientific journals does not provide sufficient description associated with the modeled processes which is a fundamental criterion for reuse pradal et al 2013 this raises the problem of reproducibility and reliability of scientific results that are strongly linked to the platforms in which the models have been implemented and tested cohen boulakia et al 2017 hinsen 2016 visual domain specific languages such as simile muetzelfeldt and massheder 2003 or stella richmond 1985 provide a rich graphical interface to build models but become difficult to use for complex models and require many widgets to represent graphically nested control flows multiscale modelling and simulation frameworks marshall colon et al 2017 pradal et al 2015 propose model interface designs which enables communication of multi language components as black box components other declarative modelling languages are also used in the systems biology community who have developed declarative open standard such as sbml hucka et al 2010 cellml cuellar et al 2003 or neuroml le franc et al 2012 to describe biological models however crop modelers generally use procedural modelling rather than a mathematical formalism like differential or reaction equations as it is commonly done in system biology an alternative to the problem of pbm component reuse between pbm platforms is the use of a centralized framework that enables the development of pbm components regardless of the modeling platforms fig 1 we followed this approach and developed a modeling framework called crop2ml crop modelling meta language that separates the structure of a model component from its implementation given that the wrapping solution was excluded because of the lack of transparency and high maintenance cost and that crop2ml does not aim at replacing existing modeling platforms or at simulating components within large modeling solutions crop models we created a solution that generates components from a metalanguage for specific pbm platforms it provides a centralized pbm components repository to store model components in a standard format to facilitate their access and reuse this reuse approach is supported by the agricultural modeling exchange initiative amei which brings together some of the most widely used crop modelling and simulation platforms including the agricultural production systems simulator apsim holzworth et al 2018 the biophysical model applications bioma donatelli et al 2010 the decision support system for agrotechnology transfer dssat jones et al 2003 hoogenboom et al 2019 openalea pradal et al 2015 the renovation and coordination of agroecosystems modelling record bergez et al 2013 and the scientific impact assessment and modeling platform for advanced crop and ecosystem management simplace gaiser et al 2013 and other crop models such as stics brisson et al 2010 or siriusquality martre et al 2006 here we first present the main components of crop2ml framework then we describe the mechanisms of importing and exporting between crop2ml and pbm platforms we then discuss our approach and present some perspectives 2 crop2ml a centralized framework for crop model components development and sharing crop2ml is a framework for crop model component development exchange and reuse between pbm platforms it is designed following fair principles for research software lamprecht et al 2019 to provide simplicity model specifications are defined using a declarative language extensible markup language xml bray et al 2008 with generic concepts shared between pbm platforms and model algorithms are encoded using a minimal language transparency models are shared as documented components in a well defined format crop2ml format flexibility model units are composed with a shared abstract representation of model structure findability model specifications include rich metadata and are assigned a globally unique and persistent identifier for each released version reusability model components are transformed into pbm platform compliant code to support efficient interoperability reproducibility model components can be executed and tested regardless of the pbm platforms modularity three levels of modularity of models are defined single model units composite models and package package contains model units and composite as well as data it provides the flexibility to make different compositions based on these models we used the principles of lamprecht et al 2019 for assessing the fair ness of crop2ml framework supplementary data table c1 2 1 design and concepts of crop2ml model specification software modularity is one of the main criteria of reuse jones et al 2001 proposed key elements for modular model structure which is an essential first step to enhance collaborative modelling effort crop2ml follows and extends these principals in most pbm the system is decomposed into compartments such as plant parts or soil layers that interact for each compartment different processes are described and assembled in components to simulate the response of the compartment these processes can be subdivided into discrete explanatory independent biophysical sub processes which could be individually modeled modelunit or composed modelcomposite a modular model structure requires making an objective decomposition of the system to avoid coarse granularity models which limit reusability a modelunit should not encapsulate alternative assumptions and formalisms making it easier to test them in addition the management of input and output data such as data access logging and file generation must be managed separately from the implementation of model component these design principles foster the reuse of components which are intended to be integrated and simulated with a large variety of input data formats in different pbm platforms moreover to emphasis modularity the temporal integration loop must be removed from the model process implementation this makes it possible to reuse the same process with different modeling formalisms or simulation frameworks that manage temporal dynamics of the simulation differently e g different numerical integration techniques crop2ml provides a level of abstraction that enables a shared representation of model components between pbm platforms a modelunit is defined with the following descriptive elements fig 2 a a model description a list of inputs a list of outputs an initialization step of the state variables a link pointing to the source of the model algorithm a list of usual mathematical functions a set of unit tests with parameterization shared between modeling platforms a modelcomposite includes the same elements as a modelunit in addition it contains a list of models and the links between them fig 2b however if control structures are necessary to express the behavior of a modelcomposite the algorithm can be explicitly provided the crop2ml model specification is based on xml language xml is a widely used declarative metalanguage for describing or structuring data in a portable format with some descriptive elements xml format is used in several pbm platforms for template parametrization and model simulation configuration e g apsim bioma record simplace siriusquality this reinforces our choice on this format since the transformation between different xml documents or in any language is relatively straightforward allows using xml as a bridge between heterogeneous structures and it facilitates collaborative development moreover the use of xml and a formal description of model specifications and their associated metadata facilitate machine readability and model exchange in the following sections we describe the concepts of crop2ml model specifications 2 1 1 description the core description of a crop2ml model contains the name of the model an identifier that ensures the provenance of the model and a version number fig 3 the identifier of the model is specified to keep the property of the component since pbm are dynamic models the time step is an important factor that is specified to allow a multi temporal scale composition in addition other elements are described to provide rich metadata including author names and affiliations citable and findable references e g doi and a brief description of the model the description also includes usage licenses compatible with the model dependencies 2 1 2 inputs outputs in crop2ml a component takes parameter and variable values as inputs and produces variable values as outputs a variable is a quantity which is given by the context of the experiment input data or calculated by the model output data while the value of a parameter is an input that can be specified by the modeler within a defined interval variables and parameters are distinguished with input type attributes and are categorized with variable category and parameter category attributes respectively table 1 crop2ml currently supports four basic types integer double strings and logical it also supports two collection types lists and arrays which contain a sequence of elements of basic types they are explicitly specified in a datatype attribute similar to the varinfo type donatelli and rizzoli 2008 it also provides a common representation of date time the domain of validity of each variable is specified by min and max attributes a measurement unit can also be associated to the variables and parameters fig 4 gives an example of inputs and outputs specifications 2 1 3 initialization state variables of crop2ml modelunits and modelcomposites are initialized at the start of a simulation and are specified with an initialization element this element is optional and the default values of state variables are used if it is omitted initialization may also be a function that assigns initial values to state variables in this case the initialization element contains the path to the source code of the initialization function 2 1 4 algorithm algorithm elements link the model specifications with the model algorithm fig 5 a model algorithm describes the behavior of a component in terms of a sequence of inputs successive rules or actions conditions and a flow of instructions from inputs to outputs including mathematical expressions a model algorithm can be implemented in different programming languages however crop2ml proposes to encode the model algorithm in a shared language cyml midingoyi et al 2020 the cyml source code is the common representation for model algorithm shared by the supported languages and platforms see section 2 2 2 1 5 function a function is a utility routine that can be called from the model algorithm or from other functions it reduces the code length and improves the readability of the encoded algorithm if a model needs an external function this function must be declared in the model specification by referencing the path where the function is implemented a function can also be used for model adaptations such as temporal aggregation or integration unit conversion to link model components without changing their algorithms crop2ml provides a shared library of mathematical functions in different languages such as standard functions interpolation or upper and lower bound functions modelers can use these functions in their own algorithm implemented in the cyml language 2 1 6 parameter sets and test sets a crop2ml model specification includes one or more sets of model parameterizations used for different unit tests fig 6 a parameterization is a set of values assigned to an input parameter of a model it is described by a name and a description a unit test in crop2ml is described in the testsets element and allows comparing estimated and expected outputs values several unit tests can be specified they are described by their name their description and the name of parameters set associated to them each test provides a list of values assigned to each variable and the expected values of the model outputs a numerical precision could be associated with the output of the test to check its validity 2 1 7 model links model links are specified in a modelcomposite and depict how modelunits or modelcomposites are interconnected a modelcomposite is a port graph andrei and kirchner 2009 that defines a dataflow where nodes are modelunits and ports are inputs and outputs of the modelunits edges are oriented links connecting output ports of a source modelunit to the input ports of a target modelunit fig 7 three types of links must be specified internallink is the connection between an input of one sub model and the output of another sub model inputlink is the connection between an input port of a sub model and an input port of the composite model and outputlink is the connection between a modelunit or modelcomposite output port that can be either a modelunit or modelcomposite and a modelcomposite output port these connections show the hierarchical structure of a modelcomposite this modeling approach enhances reusability and has been used with success wyatt 1990 2 2 cyml the common modelling language of biophysical processes in crop models we defined a set of common features resulting from the intersection of the programming languages supported by pbm platforms to propose a shared modelling language a design choice was to define a subset of an existing language that can provide these common features we needed a widely used high level language with a low learning curve so that modelers with basic programming skills could efficiently use it the transformation of a language with dynamic typing can make code transformation into programming languages with static typing ambiguous therefore we choose cython a high level language that combines the expressive power of python language with explicit type declaration of c language behnel et al 2011 it is compiled directly in efficient c code which improves runtime speed and makes it possible to interact with c c and fortran source code however not all cython syntax can be directly transformed in all target languages for instance the yield statement and anonymous functions are not supported by fortran therefore we defined cyml cython meta language a sub set of cython to address the implementation of the model algorithm midingoyi et al 2020 we use cyml as a pivot language between various platform languages which can be mapped to their syntax and semantics the structure and syntax of cyml as well as its transformation system to various languages and platforms is detailed in midingoyi et al 2020 in brief cyml supports datatypes defined in the model specification and provides standard mathematical functions and operators in addition to local variable declaration and assignment statements control structures are used in the flow of instructions described by the encoded algorithms these include conditional statements if elif and else to check if a condition is satisfied before addressing part of an algorithm sequential statement for loop with an incremental index on a data collection and a repetitive statement while used to repeat part of an algorithm while a condition is satisfied these structures can be nested to support modular designs and the reuse of modelunits and functions cyml provides import mechanisms which assumes that imported modelunits or functions are referenced crop2ml framework provides a source to source transformation system cymlt which converts cyml source code into procedural fortran python c object oriented java c c python and scripting or functional r python languages midingoyi et al 2020 cymlt implementation relies on the transformation of the abstract syntax tree ast generated from the syntax analysis of the cyml code the ast is transformed to a self contained representation of the source code called abstract semantic graph which is independent of the source language cymlt proposes a unique approach to transform the abstract semantic graph into readable source code in many different languages the generated code is independent from the transformation system and can be run outside the crop2ml framework the transformation system integrates model documentation based on the model specification into generated code 2 3 crop2ml model package in the context of large projects and collaborative work it is useful to define some requirements or standards to facilitate common exchange crop2ml provides a logical standardized but flexible support to facilitate model sharing between modeling platforms through the definition of a directory structure fig 8 this template includes a folder that contains model description and associated algorithms a repository of source code for each language and modeling platforms it also includes a folder containing input data for a modelcomposite simulation and a folder containing the unit tests to save time and avoid omission of mandatory files or folders during package creation we created a cookiecutter roy 2017 template that automatically generates crop2ml package templates https crop2ml readthedocs io en latest user package html it increases model reusability by automatically generating a project that follows shared guidelines any modelunit or modelcomposite can be extracted as a stand alone model from an existing package tested reused or integrated in other modelcomposite or package the notion of package dependency increases the modularity of crop2ml and avoids model duplicity 2 4 crop2ml model lifecycle management crop2ml aims at collaborative model development that supports the entire model lifecycle including model creation editing verification validation transformation composition and documentation therefore we developed tools and services to support all the steps of a crop2ml model lifecycle 2 4 1 model analysis crop2ml models conform to a specific document type definition dtd that describes crop2ml concepts model analysis verifies if the model specifications are a well formed xml document validated by crop2ml dtd the analysis of a modelcomposite consists of checking model composability through port datatypes and units most xml editors can check the validity of an xml document against a dtd but the crop2ml software environment see section 3 2 ensures this 2 4 2 model validation crop2ml model components can be validated by executing unit tests it consists of using the parameter and variable values from the model specification to produce unit tests in different languages unit tests are generated in jupyter notebook format a document format for publishing source codes and reproducible computational workflows that could be executed in the appropriate kernel in crop2ml software environment this format is useful for code and documentation publishing and real time collaboration when running on a remote server kluyver et al 2016 unit tests may also be associated with a model publication 2 4 3 model transformation the success of crop2ml model reuse through a white box approach comes from its ability to generate model components that conform to platform requirements the transformation of a model component from a platform to another one goes through crop2ml model representation it relies on a system of transformation to and from crop2ml and the platforms for some pbm platforms meta information of model components are described inside their implementation as documentation for other platforms meta information are encoded in a textual or visual programming language cymlt generates from crop2ml model either appropriate documentation or variables and parameters specifications based on the artifacts of the target platforms in addition cymlt generates model component algorithms in various languages given a model component provided by a platform meta information are extracted by identifying crop2ml concepts inside the component to generate crop2ml model meta information moreover algorithms in cyml are produced to obtain a complete crop2ml model 2 4 4 model documentation sharing model knowledge requires detailed information on the model crop2ml generates model documentation from the model specification from the relationships between the modelunits of a modelcomposite the diagram flow of the modelcomposite is generated it may constitute part of the model documentation and gives a first description of the model component this allows groups of modelers to understand the model structure and evaluate the component 3 crop2ml software environment and tools 3 1 pycrop2ml a python library for crop2ml pycrop2ml is an open modular and extensible library developed in python that implements all the steps of crop2ml model lifecycle it is designed to support the current crop2ml model specifications but can easily be adapted to support future versions pycrop2ml can be integrated into other software projects as a plug in it allows verifying a crop2ml model this is ensured through a model parser based on the crop2ml dtd transforming a crop2ml modelunit to source code pycrop2ml integrates cymlt that generates model components that conform to pbm platform requirements transforming a cyml source code to various languages regardless of crop2ml model specifications any cyml source code can also be transformed into the target languages this source code can be used as auxiliary functions for crop2ml model development transforming source code to jupyter notebook format each modelunit source code generated can be translated as a cell of jupyter notebook as well as each unit test allowing its execution in crop2ml jupyterlab environment transforming a crop2ml modelcomposite a crop2ml modelcomposite provided as a directed graph can be transformed to source code as a sequential order of the submodels visualizing a modelcomposite pycrop2ml provides a function to visualize a modelcomposite with the links between modelunits fig 9 pycrop2ml is written in python and can be executed via a command line interface inputting either a crop2ml package or cyml source code as well as the target language or platform for transformation users with no knowledge of the python language can easily run pycrop2ml via the command line the pycrop2ml library incorporates three crop model components as model examples that can be used to test the different functionalities 3 2 cropmstudio a jupyterlab environment for crop2ml model life cycle management crop2ml model specifications can be created or edited using any xml editor however to fulfil our objective of collaborative model development accessible to modelers with no specific programming skills we developed a user friendly interface based on the pycrop2ml package to manage the lifecycle of crop2ml model components fig 10 since crop2ml models are transformed in different languages it is useful to execute the unit tests in a single environment our solution named cropmstudio uses the jupyterlab environment https jupyterlab readthedocs io an open source web application that allows working with code in different languages through different language backends kernels we installed python java c c r and fortran kernels to execute modelunit tests the current version of cropmstudio can be accessed through a web browser and run locally like a desktop application another motivation to use jupyterlab is to make publication results reproducible in a shared environment based on the capacity to produce interactive and readable code documents kluyver et al 2016 4 interoperability between various simulation platforms the interoperability between simulation platforms is based on two transformation processes import and export via crop2ml the import process consists of transforming any platform model component to crop2ml model the export process consists of transforming crop2ml models to any platform detailed descriptions of the import export mechanisms in five widely used platforms with different architectures bioma dssat record openalea simplace are provided in supplementary data appendix c table 2 summarizes the interoperability of model components between these platforms platforms are based on various programming languages which requires the definition of transformation rules between cyml and various languages including c bioma java simplace c record python openalea and fortran dssat in both directions we identified the levels of granularity of modeling processes that correspond to crop2ml concepts such as modelunit and modelcomposite in each platform we also considered how documentation or model specifications are described in these platforms the export process from crop2ml to platforms is automatically done in bioma openalea and simplace the modularity principle in bioma matches crop2ml which allows associating simple and composite bioma strategies with crop2ml modelunit and modelcomposite respectively moreover all the crop2ml elements are well translated into the varinfo type attributes donatelli and rizzoli 2008 and crop2ml model algorithms are transformed to a method of a strategy class that takes generated domain classes as inputs openalea relies on two families of approaches component based architecture and scientific workflows thus crop2ml exports modelunits as openalea components and modelcomposite as openalea workflows modelcomposite can thus be visualized and edited using visualea the visual programming environment in openalea widgets of modelunit are automatically generated based on the type of inputs that is mapped to an openalea interface simplace is based on the concept of software units called simcomponents as the smallest building blocks that map with modelunits modelcomposite are converted into a combination of simcomponents simcomponentgroup variables and parameters descriptions are automatically included in the simcomponents descriptive part in dssat and record the export process is many automatic but some aspects need to be done manually in dssat crop2ml transformation system generates a submodule in fortran 90 for each modelunit it also generates a sequence of submodules calls for composite models one issue that makes this transformation not completely automatic is that crop2ml does not manage the handling of input and output files therefore it requires to manually add the input and output methods into the generated submodules the concepts of atomic and coupled models in record are mapped with those of crop2ml thus atomic model classes are generated in c to correspond to modelunits however the configuration and simulation file vpz representing the modelcomposite is manually completed with further information such as the description of simulation result files the import process from simulation platforms to crop2ml is only partially automatic platform tools produce automatically the meta information in crop2ml format but algorithms are manually converted into the cyml language that leads to a semi automatic transformation a complete automatic transformation would require the implementation of source to source transformation from platforms language into cyml in bioma varinfo attributes are extracted from bioma strategies to produce crop2ml model meta information the process of automatically retrieving the estimate method to produce model algorithm in cyml is not implemented yet the description of component in openalea is very generic compared to crop2ml concepts although openalea is mainly built for functional structural plant modeling fspm application there is no plant domain specific description associated with inputs and outputs such as units categories of variables and parameters thus the generation of model description in crop2ml is partial it requires further description of components that can be provided in documentation or by extending openalea concepts like bioma the simcomponent specific descriptors in simplace allows generating modelunits meta information the process method algorithm is currently translated manually in cyml links between the different simcomponents unit stored in the simcomponentgroup composition are automatically exported to the crop2ml structure however there is a loss of information since when a modelunit is activated or ignored it is not transferred to the crop2ml structure in dssat unlike in the other platforms the description of physiological processes is provided as documentation in submodules and it is not fully complete with respect to crop2ml specifications inputs and outputs variables and their descriptions units can be clearly identified based on systematic platform guidelines dssat submodules contain specific platform variables such as control variables that need to be removed to produce cyml model algorithms in record as in dssat there is no explicit specification of a model the documentation of a model within its associated c class is used to generate partial modelunit meta information the parsing of the vpz file that contains the structure of composite models in record is used to generate a modelcomposite however it is not possible to represent retroaction loops in crop2ml as it is done in record with coupled models in order to illustrate crop2ml concepts and transformation results a phenology and an energy balance models are used phenology the timing of crop development is the heart of most crop growth models and is an essential component of most crop modeling platforms the energy balance model involves interconnected components that allows estimating canopy temperature evapotranspiration and heat transfer between the canopy and the air these processes are implemented as bioma standalone components manceau and martre 2018 of the wheat pbm siriusquality he et al 2012 martre et al 2006 the two components were converted into crop2ml packages and then automatically translated into different languages and model components that conform to different pbm platforms these packages are presented in appendixes a and b in table 3 we illustrate how to represent a parameter and an algorithm in a crop2ml modelunit and its translation with cymlt in record bioma and dssat the implementations of the model differ between the platforms for instance dssat defines a subroutine with all the variables as argument record defines a class method compute with the variables as attributes of the class and uses specific operator to manage temporal variables while bioma defines a class method calculatemodel that takes as argument data structures implementing each category of variables state rate auxiliary exogenous the aim of model transformation is to provide to the platforms alternative model components that could easily replace their corresponding components to analyze the effects of new hypotheses into their modeling solutions the sequence of modelunits that compose a crop2ml modelcomposite is formally modeled as a directed acyclic graph this means that there is no feedback loop or retroaction at a given time step instead they are usually represented by a cycle in the modelcomposite alternatively a state variable can be defined explicitly as two variables with respect to the current and the previous time thus a composite model may take as input a state variable at previous time and a state variable at current time as output making implicitly a loop with respect to time advance another way to represent feedback inside a time step is to associate an explicit algorithm to the modelcomposite that defines how to run it however this feature is not supported by two simulation platforms openalea and record 5 discussion the crop2ml framework enables a user to exchange and reuse biophysical components between various pbm platforms through shared declarative specifications the use of a minimal language to describe the model algorithm once and the transformation system facilitates reuse of models components modelunits and modelcomposite can be accessed and composed following a white box approach therefore the crop2ml approach greatly increases the ability of modelers to share their algorithms the protocol will allow modelers to borrow components easily and will facilitate their intercomparison and improvement in different pbm platforms 5 1 how does crop2ml address model reuse compared to other initiatives some initiatives addressed model reuse by providing multi scale and multi language integrative frameworks such as crops in silico marshall colon et al 2017 the open modeling foundation openmi buahin and horsburgh 2018 these frameworks can compose and simulate heterogeneous models provided by different frameworks through a communication interface the model components are often wrapped and are represented as black box components all state variables are not always exposed as model outputs which may limit their integration in an existing modeling solution therefore these frameworks enhance model reuse in their own environment but they do not address reusability with other pbm platforms many existing pbm platforms do not support the coupling of models written in multiple languages e g bioma apsim next generation donatelli and rizzoli 2008 proposed a design pattern for platform independent model components to enhance modularity and to facilitate model reuse in several pbm platforms via simple wrappers however this approach fixes the structure of the components the lack of specification or meta information makes the reuse of model components between platforms difficult even in component based systems explicit information about the component itself and its inputs and outputs types units and boundary conditions are required to ensure a syntactic composability and to meet the specificities of the platforms moreover the knowledge of the structure underlying the source code of a component is also required to systematically extract model information variables and algorithms for their transformation and integration in different platforms we thus argue that model component reuse is improved if it is supported by model specification crop2ml defines an abstract representation of model design shared by pbm platforms through some shared concepts enriching or extending those proposed by athanasiadis et al 2011 with other attributes and a formal and shared description of unit tests we included unit tests in crop2ml specifications to ensure model transformation validation and some imperative constructs for model dynamics several initiatives have used declarative modeling to describe model specifications and address model reuse issues the approach proposed by villa et al 2006 is similar to ours but it is limited to models where the dynamics of the modeled processes is represented by simple mathematical expressions without control structures which does not match crop modeling context hucka et al 2003 used mathml ausbrooks et al 2003 to express interactions between variables through mathematical formalisms well defined in the systems biology community this approach is similar to that of rizzoli et al 2008 and is useful when processes are governed by differential equations however in the pbm context simulation platforms use algorithms to describe processes rather than mathematical formalisms with differential equations moreover in pbm variables that drive the system are temporal series that change the behavior of the system at discrete time this does not require finding a general solution of recurrent equations used in crop models but rather estimating at each time step the state variables of the system automated model transformation is a core aspect of model driven development cuadrado and molina 2007 it uses model driven engineering mde principles based on metamodeling concepts crop2ml is in line with mde it defines structured concepts representing its metamodel with which all crop2ml models are conform and a model transformation to generate pbm platforms components model driven architecture brown 2004 is a framework of mde that provides several standard languages e g atl qvt etl henshin viatra and stratego for model transformation jouault and kurtev 2006 kurtev et al 2006 crop2ml is based on a transformation process through a set of refinement of models and code with some extensible rules defined as templates in python most mde approaches allow model to model or model to code transformation where a model represents the specification in our case however the use of transformation language standards was inappropriate in our context to unify transformation process towards many languages with different paradigms bucchiarone et al 2020 crop2ml produces code in a target language but also adapts the code to fit with pbm platform specificities to our knowledge model transformation languages in mde do not support code generation in multiple languages with extended features in the same environment 5 2 connecting crop2ml to pbm platforms given that crop2ml datatypes do not handle complex data structures other than arrays and lists some compromises or transformation should be made to the import export process on the platform side with respect to handling other data structures used in platforms as an example bioma provides the dictionary data type that is a set of keys associated with values to represent either input or output variables this data type is not handled by crop2ml and by most pbm platforms as an alternative dictionaries can be expressed in crop2ml as two list datatype variables that represent keys and values of the dictionary the simulation algorithm defining the feedback loop is explicitly described as control flow in some platforms e g bioma but this is not the case in other platforms e g record where the vpz file representing the simulation model file is handled by the simulation engine vle different simulation engines are based on different models of computation used by the platforms such as dataflow e g openalea devs simulation e g record control flow e g bioma dssat and simplace these models of computation are used to coordinate the execution of the model the current version of crop2ml framework does not take into account the specificities of simulation engines and addresses components which can be sequentially composed the crop2ml transformation system is designed to support the specificities of the target pbm platforms however the semantic of a crop2ml model is based on shared concepts to describe at a high level a biophysical process by a discrete time model there is no semantic reason to support the description of each instance of the concepts for example since we have not defined a convention to name process variables the integration of a crop2ml component into a pbm modeling solution requires adapting the name of its variables in the future we could annotate crop2ml models to add semantic information to make semantic links between any crop2ml model variables or parameters with those of model components of pbm platforms this will also allow a semantic composability of crop2ml models instead of a syntactic composability that analyzes whether the pair of variables to be linked are compatible however this would require the crop modeling community to agree on shared semantics and ontologies of crop model variables and parameter representations until now this has been a real challenge as the crop modeling community has not be too keen on adapting standards white et al 2013 in addition to facilitate the exchange and reuse of model components semantic descriptions of model variables and parameters would facilitate the linking of crop models to plant phenomics data neveu et al 2018 we were able to achieve fully automatic export of crop2ml model to several pbm platforms the import process into crop2ml is more mixed regarding the overall differences between pbm platforms it is much easier to start with concepts shared and reused by pbm platforms than to start from divergent views of model representations to achieve a particular result some pbm platforms need to extend their concepts for model specification or to provide a rich model documentation in order to produce complete crop2ml model specifications this reveals the need of a good level of abstraction to represent a model in various pbm platforms the higher the level of abstraction the further the description moves away from the platforms and the less easy it is to understand on the other hand if the level of abstraction is too low it is not always possible to represent all features of the models present in the platforms 5 3 future developments a common model repository infrastructure is essential for efficient model exchange glont et al 2018 lloyd et al 2008 currently crop2ml model components are stored in github repositories we aim to provide a crop2ml model repository to store models in a shared format to make them easily accessible and reusable by the plant and crop modeling community this repository should aim at hosting alternative biophysical processes it will help modelers to operate on multiple model components compare processes or evaluate the impact of the integration of alternative models of biophysical processes in crop models the success of the crop2ml repository requires that the community gives access to their models by feeding the repository which will be curated by the amei consortium to avoid error propagation crop2ml has some limitations which can be addressed in the next versions either by extending the model specifications with shared concepts or by adapting the target pbm platforms to crop2ml specification and language it is an ongoing long term activity to satisfy platform requirements and facilitate crop2ml model life cycle management to make crop2ml a standard for the plant and crop modeling community the transformation of a model component of a pbm platform into a crop2ml package requires rewriting the model algorithms in the cyml language this limit is currently being addressed by extending the cyml transpiler to a bidirectional transpiler thereby pbm platforms could provide model algorithms in the language they use and the extended cymlt will transform them in cyml and target languages used by other pbm platforms fig 11 this is a two step process first the model algorithms in the language of the source pbm platform will be parsed and an ast will be generated second the rules for transforming this ast into the cyml ast will be applied the second step will reuse the cyml transformation tool developed by midingoyi et al 2020 to produce model algorithms compatible with other languages and pbm platforms other future developments of crop2ml include enhance crop2ml model repositories with model annotation to link publications to models for reproducibility add unit checks and conversions in crop2ml to improve model validity define a methodology to link crop2ml with plant structure representation for multiscale viewing and analysis define and implement an ontology of crop model variable and parameters to allow better crop2ml model interpretation and improve transformation between pbm platforms and the integration of model component in complex modeling solutions extend crop2mlab prototype by including bidirectional transformation and the creation of a web interface on a remote server in order to give users the possibility to handle crop2ml model lifecycle without local installation 6 conclusion at the interface between modeling and software engineering this paper addresses plant and crop model component reuse by proposing the crop2ml framework despite all the differences between pbm platforms some common features can be identified that enabled model representation regardless of the platforms specificities crop2ml provides structured concepts to support the definition of modelunit and modelcomposite and allows their transformation to make them compatible with pbm platforms at implementation level therefore crop2ml defines a new unified crop model representation that considers the abstraction of pbm component features in several pbm platforms moreover crop2ml uses a domain specific language to describe biophysical processes and auxiliary functions to represent model dynamics based on a subset of the cython language which can then be automatically transformed into different target languages crop2ml proposes an open framework to manage all the steps of model lifecycle declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements c m was supported through a phd scholarship from the french national research agency under the investments for the future program referred as anr 16 conv 0004 and inrae divisions agroecosystem and num p m acknowledges the support of inrae division agroecosystem through the mod√©lisation du fonctionnemnet des peuplements cultiv√©s mfpc network the authors thank dr loic manceau inra umr lepse for discussions and its help to translate the wheat phenology bioma component of siriusquality in crop2ml appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105055 
25813,process based crop models are popular tools to analyze and simulate the response of agricultural systems to weather agronomic or genetic factors they are often developed in modeling platforms to ensure their future extension and to couple different crop models with a soil model and a crop management event scheduler the intercomparison and improvement of crop simulation models is difficult due to the lack of efficient methods for exchanging biophysical processes between modeling platforms we developed crop2ml a modeling framework that enables the description and the assembly of crop model components independently of the formalism of modeling platforms and the exchange of components between platforms crop2ml is based on a declarative architecture of modular model representation to describe the biophysical processes and their transformation to model components that conform to crop modeling platforms here we present crop2ml framework and describe the mechanisms of import and export between crop2ml and modeling platforms keywords crop model crop2ml component based software model exchange and reuse 1 introduction the wide range of crop process based models pbm reflects the evolution of our knowledge of the soil plant atmosphere system and the rich historical development for more than five decades reviewed in jones et al 2017 muller and martre 2019 the high diversity of pbm is due to their multiple applications and the complexity of the system influenced by several factors e g weather soil crop management basso et al 2013 and genotypic factors wang et al 2019 most of the pbm are continuous models formalized using ordinary differential equations but are implemented as discrete time simulation models using finite difference equations they are commonly decomposed into simpler biophysical functions e g phenology morphogenesis resource acquisition pests and diseases impact often implemented by recurrent equations with control flows another common characteristic is that pbm simulate plant growth and development at the scale of the canopy or average plant level without spatial dependence with a daily or sub daily time step pbm are often implemented in modeling and simulation platforms at a higher level of abstraction to facilitate model development rizzoli et al 2008 these platforms offer not only scalable modular and robust modelling solutions but also the ability to analyze evaluate reuse and combine models the diversity of pbm led the crop modeling community to compare their performance and to improve them by aggregating modelers knowledge or by introducing improvements provided from diverse research groups under the umbrella of large international collaborative projects such as the agricultural model intercomparison and improvement project agmip rosenzweig et al 2013 studies conducted in the context of model intercomparison and improvement exercises e g asseng et al 2013 wang et al 2017 pointed out the large uncertainty of pbm simulations and have analyzed the sources of uncertainty or the processes involved these intercomparison results showed the potential and limits of pbm and highlighted the need to analyze models at the process level but also to exchange model components describing specific processes between simulation platforms e g donatelli et al 2014 wang et al 2017 the uncertainty of a pbm component may be related to its validity domain inputs parameters structure and the underlying scientific hypotheses walker et al 2003 epistemic uncertainty may arise from incomplete or lack of knowledge of these sources the uncertainty of pbm results from the aggregation of the uncertainty of each of its component refsgaard et al 2007 a framework that would allow the exchange of model components between different platforms would give crop modelers the ability to test alternative hypotheses in the same model thus helping to reduce epistemic uncertainty although most crop simulation platforms provide modular approaches and reuse techniques there is little exchange of pbm components between them despite theoretical and application interests pbm components often contain source code developed in different programming languages and are tightly coupled to the platforms therefore model components are not seamlessly reusable outside the modeling platforms in which they have been developed without recoding or wrapping them holzworth et al 2014 rizzoli et al 2008 re implementing a component in several platforms is a tedious and cumbersome task and requires a minimum knowledge of the different platforms the wrapping solution treats components as black boxes taking little or no advantage of the framework rizzoli et al 2008 or as white boxes but with a high level of complexity fernique and pradal 2018 pradal et al 2008 other reuse approaches in environmental modeling have been explored declarative modeling can provide portability and facilitate integration between independent uncoordinated models athanasiadis and villa 2013 however model specifications are seldom separate from implementation details model builders rely often directly on implementation that hides the scientific content of a model i e its algorithm and its structure moreover the publication of pbm components in scientific journals does not provide sufficient description associated with the modeled processes which is a fundamental criterion for reuse pradal et al 2013 this raises the problem of reproducibility and reliability of scientific results that are strongly linked to the platforms in which the models have been implemented and tested cohen boulakia et al 2017 hinsen 2016 visual domain specific languages such as simile muetzelfeldt and massheder 2003 or stella richmond 1985 provide a rich graphical interface to build models but become difficult to use for complex models and require many widgets to represent graphically nested control flows multiscale modelling and simulation frameworks marshall colon et al 2017 pradal et al 2015 propose model interface designs which enables communication of multi language components as black box components other declarative modelling languages are also used in the systems biology community who have developed declarative open standard such as sbml hucka et al 2010 cellml cuellar et al 2003 or neuroml le franc et al 2012 to describe biological models however crop modelers generally use procedural modelling rather than a mathematical formalism like differential or reaction equations as it is commonly done in system biology an alternative to the problem of pbm component reuse between pbm platforms is the use of a centralized framework that enables the development of pbm components regardless of the modeling platforms fig 1 we followed this approach and developed a modeling framework called crop2ml crop modelling meta language that separates the structure of a model component from its implementation given that the wrapping solution was excluded because of the lack of transparency and high maintenance cost and that crop2ml does not aim at replacing existing modeling platforms or at simulating components within large modeling solutions crop models we created a solution that generates components from a metalanguage for specific pbm platforms it provides a centralized pbm components repository to store model components in a standard format to facilitate their access and reuse this reuse approach is supported by the agricultural modeling exchange initiative amei which brings together some of the most widely used crop modelling and simulation platforms including the agricultural production systems simulator apsim holzworth et al 2018 the biophysical model applications bioma donatelli et al 2010 the decision support system for agrotechnology transfer dssat jones et al 2003 hoogenboom et al 2019 openalea pradal et al 2015 the renovation and coordination of agroecosystems modelling record bergez et al 2013 and the scientific impact assessment and modeling platform for advanced crop and ecosystem management simplace gaiser et al 2013 and other crop models such as stics brisson et al 2010 or siriusquality martre et al 2006 here we first present the main components of crop2ml framework then we describe the mechanisms of importing and exporting between crop2ml and pbm platforms we then discuss our approach and present some perspectives 2 crop2ml a centralized framework for crop model components development and sharing crop2ml is a framework for crop model component development exchange and reuse between pbm platforms it is designed following fair principles for research software lamprecht et al 2019 to provide simplicity model specifications are defined using a declarative language extensible markup language xml bray et al 2008 with generic concepts shared between pbm platforms and model algorithms are encoded using a minimal language transparency models are shared as documented components in a well defined format crop2ml format flexibility model units are composed with a shared abstract representation of model structure findability model specifications include rich metadata and are assigned a globally unique and persistent identifier for each released version reusability model components are transformed into pbm platform compliant code to support efficient interoperability reproducibility model components can be executed and tested regardless of the pbm platforms modularity three levels of modularity of models are defined single model units composite models and package package contains model units and composite as well as data it provides the flexibility to make different compositions based on these models we used the principles of lamprecht et al 2019 for assessing the fair ness of crop2ml framework supplementary data table c1 2 1 design and concepts of crop2ml model specification software modularity is one of the main criteria of reuse jones et al 2001 proposed key elements for modular model structure which is an essential first step to enhance collaborative modelling effort crop2ml follows and extends these principals in most pbm the system is decomposed into compartments such as plant parts or soil layers that interact for each compartment different processes are described and assembled in components to simulate the response of the compartment these processes can be subdivided into discrete explanatory independent biophysical sub processes which could be individually modeled modelunit or composed modelcomposite a modular model structure requires making an objective decomposition of the system to avoid coarse granularity models which limit reusability a modelunit should not encapsulate alternative assumptions and formalisms making it easier to test them in addition the management of input and output data such as data access logging and file generation must be managed separately from the implementation of model component these design principles foster the reuse of components which are intended to be integrated and simulated with a large variety of input data formats in different pbm platforms moreover to emphasis modularity the temporal integration loop must be removed from the model process implementation this makes it possible to reuse the same process with different modeling formalisms or simulation frameworks that manage temporal dynamics of the simulation differently e g different numerical integration techniques crop2ml provides a level of abstraction that enables a shared representation of model components between pbm platforms a modelunit is defined with the following descriptive elements fig 2 a a model description a list of inputs a list of outputs an initialization step of the state variables a link pointing to the source of the model algorithm a list of usual mathematical functions a set of unit tests with parameterization shared between modeling platforms a modelcomposite includes the same elements as a modelunit in addition it contains a list of models and the links between them fig 2b however if control structures are necessary to express the behavior of a modelcomposite the algorithm can be explicitly provided the crop2ml model specification is based on xml language xml is a widely used declarative metalanguage for describing or structuring data in a portable format with some descriptive elements xml format is used in several pbm platforms for template parametrization and model simulation configuration e g apsim bioma record simplace siriusquality this reinforces our choice on this format since the transformation between different xml documents or in any language is relatively straightforward allows using xml as a bridge between heterogeneous structures and it facilitates collaborative development moreover the use of xml and a formal description of model specifications and their associated metadata facilitate machine readability and model exchange in the following sections we describe the concepts of crop2ml model specifications 2 1 1 description the core description of a crop2ml model contains the name of the model an identifier that ensures the provenance of the model and a version number fig 3 the identifier of the model is specified to keep the property of the component since pbm are dynamic models the time step is an important factor that is specified to allow a multi temporal scale composition in addition other elements are described to provide rich metadata including author names and affiliations citable and findable references e g doi and a brief description of the model the description also includes usage licenses compatible with the model dependencies 2 1 2 inputs outputs in crop2ml a component takes parameter and variable values as inputs and produces variable values as outputs a variable is a quantity which is given by the context of the experiment input data or calculated by the model output data while the value of a parameter is an input that can be specified by the modeler within a defined interval variables and parameters are distinguished with input type attributes and are categorized with variable category and parameter category attributes respectively table 1 crop2ml currently supports four basic types integer double strings and logical it also supports two collection types lists and arrays which contain a sequence of elements of basic types they are explicitly specified in a datatype attribute similar to the varinfo type donatelli and rizzoli 2008 it also provides a common representation of date time the domain of validity of each variable is specified by min and max attributes a measurement unit can also be associated to the variables and parameters fig 4 gives an example of inputs and outputs specifications 2 1 3 initialization state variables of crop2ml modelunits and modelcomposites are initialized at the start of a simulation and are specified with an initialization element this element is optional and the default values of state variables are used if it is omitted initialization may also be a function that assigns initial values to state variables in this case the initialization element contains the path to the source code of the initialization function 2 1 4 algorithm algorithm elements link the model specifications with the model algorithm fig 5 a model algorithm describes the behavior of a component in terms of a sequence of inputs successive rules or actions conditions and a flow of instructions from inputs to outputs including mathematical expressions a model algorithm can be implemented in different programming languages however crop2ml proposes to encode the model algorithm in a shared language cyml midingoyi et al 2020 the cyml source code is the common representation for model algorithm shared by the supported languages and platforms see section 2 2 2 1 5 function a function is a utility routine that can be called from the model algorithm or from other functions it reduces the code length and improves the readability of the encoded algorithm if a model needs an external function this function must be declared in the model specification by referencing the path where the function is implemented a function can also be used for model adaptations such as temporal aggregation or integration unit conversion to link model components without changing their algorithms crop2ml provides a shared library of mathematical functions in different languages such as standard functions interpolation or upper and lower bound functions modelers can use these functions in their own algorithm implemented in the cyml language 2 1 6 parameter sets and test sets a crop2ml model specification includes one or more sets of model parameterizations used for different unit tests fig 6 a parameterization is a set of values assigned to an input parameter of a model it is described by a name and a description a unit test in crop2ml is described in the testsets element and allows comparing estimated and expected outputs values several unit tests can be specified they are described by their name their description and the name of parameters set associated to them each test provides a list of values assigned to each variable and the expected values of the model outputs a numerical precision could be associated with the output of the test to check its validity 2 1 7 model links model links are specified in a modelcomposite and depict how modelunits or modelcomposites are interconnected a modelcomposite is a port graph andrei and kirchner 2009 that defines a dataflow where nodes are modelunits and ports are inputs and outputs of the modelunits edges are oriented links connecting output ports of a source modelunit to the input ports of a target modelunit fig 7 three types of links must be specified internallink is the connection between an input of one sub model and the output of another sub model inputlink is the connection between an input port of a sub model and an input port of the composite model and outputlink is the connection between a modelunit or modelcomposite output port that can be either a modelunit or modelcomposite and a modelcomposite output port these connections show the hierarchical structure of a modelcomposite this modeling approach enhances reusability and has been used with success wyatt 1990 2 2 cyml the common modelling language of biophysical processes in crop models we defined a set of common features resulting from the intersection of the programming languages supported by pbm platforms to propose a shared modelling language a design choice was to define a subset of an existing language that can provide these common features we needed a widely used high level language with a low learning curve so that modelers with basic programming skills could efficiently use it the transformation of a language with dynamic typing can make code transformation into programming languages with static typing ambiguous therefore we choose cython a high level language that combines the expressive power of python language with explicit type declaration of c language behnel et al 2011 it is compiled directly in efficient c code which improves runtime speed and makes it possible to interact with c c and fortran source code however not all cython syntax can be directly transformed in all target languages for instance the yield statement and anonymous functions are not supported by fortran therefore we defined cyml cython meta language a sub set of cython to address the implementation of the model algorithm midingoyi et al 2020 we use cyml as a pivot language between various platform languages which can be mapped to their syntax and semantics the structure and syntax of cyml as well as its transformation system to various languages and platforms is detailed in midingoyi et al 2020 in brief cyml supports datatypes defined in the model specification and provides standard mathematical functions and operators in addition to local variable declaration and assignment statements control structures are used in the flow of instructions described by the encoded algorithms these include conditional statements if elif and else to check if a condition is satisfied before addressing part of an algorithm sequential statement for loop with an incremental index on a data collection and a repetitive statement while used to repeat part of an algorithm while a condition is satisfied these structures can be nested to support modular designs and the reuse of modelunits and functions cyml provides import mechanisms which assumes that imported modelunits or functions are referenced crop2ml framework provides a source to source transformation system cymlt which converts cyml source code into procedural fortran python c object oriented java c c python and scripting or functional r python languages midingoyi et al 2020 cymlt implementation relies on the transformation of the abstract syntax tree ast generated from the syntax analysis of the cyml code the ast is transformed to a self contained representation of the source code called abstract semantic graph which is independent of the source language cymlt proposes a unique approach to transform the abstract semantic graph into readable source code in many different languages the generated code is independent from the transformation system and can be run outside the crop2ml framework the transformation system integrates model documentation based on the model specification into generated code 2 3 crop2ml model package in the context of large projects and collaborative work it is useful to define some requirements or standards to facilitate common exchange crop2ml provides a logical standardized but flexible support to facilitate model sharing between modeling platforms through the definition of a directory structure fig 8 this template includes a folder that contains model description and associated algorithms a repository of source code for each language and modeling platforms it also includes a folder containing input data for a modelcomposite simulation and a folder containing the unit tests to save time and avoid omission of mandatory files or folders during package creation we created a cookiecutter roy 2017 template that automatically generates crop2ml package templates https crop2ml readthedocs io en latest user package html it increases model reusability by automatically generating a project that follows shared guidelines any modelunit or modelcomposite can be extracted as a stand alone model from an existing package tested reused or integrated in other modelcomposite or package the notion of package dependency increases the modularity of crop2ml and avoids model duplicity 2 4 crop2ml model lifecycle management crop2ml aims at collaborative model development that supports the entire model lifecycle including model creation editing verification validation transformation composition and documentation therefore we developed tools and services to support all the steps of a crop2ml model lifecycle 2 4 1 model analysis crop2ml models conform to a specific document type definition dtd that describes crop2ml concepts model analysis verifies if the model specifications are a well formed xml document validated by crop2ml dtd the analysis of a modelcomposite consists of checking model composability through port datatypes and units most xml editors can check the validity of an xml document against a dtd but the crop2ml software environment see section 3 2 ensures this 2 4 2 model validation crop2ml model components can be validated by executing unit tests it consists of using the parameter and variable values from the model specification to produce unit tests in different languages unit tests are generated in jupyter notebook format a document format for publishing source codes and reproducible computational workflows that could be executed in the appropriate kernel in crop2ml software environment this format is useful for code and documentation publishing and real time collaboration when running on a remote server kluyver et al 2016 unit tests may also be associated with a model publication 2 4 3 model transformation the success of crop2ml model reuse through a white box approach comes from its ability to generate model components that conform to platform requirements the transformation of a model component from a platform to another one goes through crop2ml model representation it relies on a system of transformation to and from crop2ml and the platforms for some pbm platforms meta information of model components are described inside their implementation as documentation for other platforms meta information are encoded in a textual or visual programming language cymlt generates from crop2ml model either appropriate documentation or variables and parameters specifications based on the artifacts of the target platforms in addition cymlt generates model component algorithms in various languages given a model component provided by a platform meta information are extracted by identifying crop2ml concepts inside the component to generate crop2ml model meta information moreover algorithms in cyml are produced to obtain a complete crop2ml model 2 4 4 model documentation sharing model knowledge requires detailed information on the model crop2ml generates model documentation from the model specification from the relationships between the modelunits of a modelcomposite the diagram flow of the modelcomposite is generated it may constitute part of the model documentation and gives a first description of the model component this allows groups of modelers to understand the model structure and evaluate the component 3 crop2ml software environment and tools 3 1 pycrop2ml a python library for crop2ml pycrop2ml is an open modular and extensible library developed in python that implements all the steps of crop2ml model lifecycle it is designed to support the current crop2ml model specifications but can easily be adapted to support future versions pycrop2ml can be integrated into other software projects as a plug in it allows verifying a crop2ml model this is ensured through a model parser based on the crop2ml dtd transforming a crop2ml modelunit to source code pycrop2ml integrates cymlt that generates model components that conform to pbm platform requirements transforming a cyml source code to various languages regardless of crop2ml model specifications any cyml source code can also be transformed into the target languages this source code can be used as auxiliary functions for crop2ml model development transforming source code to jupyter notebook format each modelunit source code generated can be translated as a cell of jupyter notebook as well as each unit test allowing its execution in crop2ml jupyterlab environment transforming a crop2ml modelcomposite a crop2ml modelcomposite provided as a directed graph can be transformed to source code as a sequential order of the submodels visualizing a modelcomposite pycrop2ml provides a function to visualize a modelcomposite with the links between modelunits fig 9 pycrop2ml is written in python and can be executed via a command line interface inputting either a crop2ml package or cyml source code as well as the target language or platform for transformation users with no knowledge of the python language can easily run pycrop2ml via the command line the pycrop2ml library incorporates three crop model components as model examples that can be used to test the different functionalities 3 2 cropmstudio a jupyterlab environment for crop2ml model life cycle management crop2ml model specifications can be created or edited using any xml editor however to fulfil our objective of collaborative model development accessible to modelers with no specific programming skills we developed a user friendly interface based on the pycrop2ml package to manage the lifecycle of crop2ml model components fig 10 since crop2ml models are transformed in different languages it is useful to execute the unit tests in a single environment our solution named cropmstudio uses the jupyterlab environment https jupyterlab readthedocs io an open source web application that allows working with code in different languages through different language backends kernels we installed python java c c r and fortran kernels to execute modelunit tests the current version of cropmstudio can be accessed through a web browser and run locally like a desktop application another motivation to use jupyterlab is to make publication results reproducible in a shared environment based on the capacity to produce interactive and readable code documents kluyver et al 2016 4 interoperability between various simulation platforms the interoperability between simulation platforms is based on two transformation processes import and export via crop2ml the import process consists of transforming any platform model component to crop2ml model the export process consists of transforming crop2ml models to any platform detailed descriptions of the import export mechanisms in five widely used platforms with different architectures bioma dssat record openalea simplace are provided in supplementary data appendix c table 2 summarizes the interoperability of model components between these platforms platforms are based on various programming languages which requires the definition of transformation rules between cyml and various languages including c bioma java simplace c record python openalea and fortran dssat in both directions we identified the levels of granularity of modeling processes that correspond to crop2ml concepts such as modelunit and modelcomposite in each platform we also considered how documentation or model specifications are described in these platforms the export process from crop2ml to platforms is automatically done in bioma openalea and simplace the modularity principle in bioma matches crop2ml which allows associating simple and composite bioma strategies with crop2ml modelunit and modelcomposite respectively moreover all the crop2ml elements are well translated into the varinfo type attributes donatelli and rizzoli 2008 and crop2ml model algorithms are transformed to a method of a strategy class that takes generated domain classes as inputs openalea relies on two families of approaches component based architecture and scientific workflows thus crop2ml exports modelunits as openalea components and modelcomposite as openalea workflows modelcomposite can thus be visualized and edited using visualea the visual programming environment in openalea widgets of modelunit are automatically generated based on the type of inputs that is mapped to an openalea interface simplace is based on the concept of software units called simcomponents as the smallest building blocks that map with modelunits modelcomposite are converted into a combination of simcomponents simcomponentgroup variables and parameters descriptions are automatically included in the simcomponents descriptive part in dssat and record the export process is many automatic but some aspects need to be done manually in dssat crop2ml transformation system generates a submodule in fortran 90 for each modelunit it also generates a sequence of submodules calls for composite models one issue that makes this transformation not completely automatic is that crop2ml does not manage the handling of input and output files therefore it requires to manually add the input and output methods into the generated submodules the concepts of atomic and coupled models in record are mapped with those of crop2ml thus atomic model classes are generated in c to correspond to modelunits however the configuration and simulation file vpz representing the modelcomposite is manually completed with further information such as the description of simulation result files the import process from simulation platforms to crop2ml is only partially automatic platform tools produce automatically the meta information in crop2ml format but algorithms are manually converted into the cyml language that leads to a semi automatic transformation a complete automatic transformation would require the implementation of source to source transformation from platforms language into cyml in bioma varinfo attributes are extracted from bioma strategies to produce crop2ml model meta information the process of automatically retrieving the estimate method to produce model algorithm in cyml is not implemented yet the description of component in openalea is very generic compared to crop2ml concepts although openalea is mainly built for functional structural plant modeling fspm application there is no plant domain specific description associated with inputs and outputs such as units categories of variables and parameters thus the generation of model description in crop2ml is partial it requires further description of components that can be provided in documentation or by extending openalea concepts like bioma the simcomponent specific descriptors in simplace allows generating modelunits meta information the process method algorithm is currently translated manually in cyml links between the different simcomponents unit stored in the simcomponentgroup composition are automatically exported to the crop2ml structure however there is a loss of information since when a modelunit is activated or ignored it is not transferred to the crop2ml structure in dssat unlike in the other platforms the description of physiological processes is provided as documentation in submodules and it is not fully complete with respect to crop2ml specifications inputs and outputs variables and their descriptions units can be clearly identified based on systematic platform guidelines dssat submodules contain specific platform variables such as control variables that need to be removed to produce cyml model algorithms in record as in dssat there is no explicit specification of a model the documentation of a model within its associated c class is used to generate partial modelunit meta information the parsing of the vpz file that contains the structure of composite models in record is used to generate a modelcomposite however it is not possible to represent retroaction loops in crop2ml as it is done in record with coupled models in order to illustrate crop2ml concepts and transformation results a phenology and an energy balance models are used phenology the timing of crop development is the heart of most crop growth models and is an essential component of most crop modeling platforms the energy balance model involves interconnected components that allows estimating canopy temperature evapotranspiration and heat transfer between the canopy and the air these processes are implemented as bioma standalone components manceau and martre 2018 of the wheat pbm siriusquality he et al 2012 martre et al 2006 the two components were converted into crop2ml packages and then automatically translated into different languages and model components that conform to different pbm platforms these packages are presented in appendixes a and b in table 3 we illustrate how to represent a parameter and an algorithm in a crop2ml modelunit and its translation with cymlt in record bioma and dssat the implementations of the model differ between the platforms for instance dssat defines a subroutine with all the variables as argument record defines a class method compute with the variables as attributes of the class and uses specific operator to manage temporal variables while bioma defines a class method calculatemodel that takes as argument data structures implementing each category of variables state rate auxiliary exogenous the aim of model transformation is to provide to the platforms alternative model components that could easily replace their corresponding components to analyze the effects of new hypotheses into their modeling solutions the sequence of modelunits that compose a crop2ml modelcomposite is formally modeled as a directed acyclic graph this means that there is no feedback loop or retroaction at a given time step instead they are usually represented by a cycle in the modelcomposite alternatively a state variable can be defined explicitly as two variables with respect to the current and the previous time thus a composite model may take as input a state variable at previous time and a state variable at current time as output making implicitly a loop with respect to time advance another way to represent feedback inside a time step is to associate an explicit algorithm to the modelcomposite that defines how to run it however this feature is not supported by two simulation platforms openalea and record 5 discussion the crop2ml framework enables a user to exchange and reuse biophysical components between various pbm platforms through shared declarative specifications the use of a minimal language to describe the model algorithm once and the transformation system facilitates reuse of models components modelunits and modelcomposite can be accessed and composed following a white box approach therefore the crop2ml approach greatly increases the ability of modelers to share their algorithms the protocol will allow modelers to borrow components easily and will facilitate their intercomparison and improvement in different pbm platforms 5 1 how does crop2ml address model reuse compared to other initiatives some initiatives addressed model reuse by providing multi scale and multi language integrative frameworks such as crops in silico marshall colon et al 2017 the open modeling foundation openmi buahin and horsburgh 2018 these frameworks can compose and simulate heterogeneous models provided by different frameworks through a communication interface the model components are often wrapped and are represented as black box components all state variables are not always exposed as model outputs which may limit their integration in an existing modeling solution therefore these frameworks enhance model reuse in their own environment but they do not address reusability with other pbm platforms many existing pbm platforms do not support the coupling of models written in multiple languages e g bioma apsim next generation donatelli and rizzoli 2008 proposed a design pattern for platform independent model components to enhance modularity and to facilitate model reuse in several pbm platforms via simple wrappers however this approach fixes the structure of the components the lack of specification or meta information makes the reuse of model components between platforms difficult even in component based systems explicit information about the component itself and its inputs and outputs types units and boundary conditions are required to ensure a syntactic composability and to meet the specificities of the platforms moreover the knowledge of the structure underlying the source code of a component is also required to systematically extract model information variables and algorithms for their transformation and integration in different platforms we thus argue that model component reuse is improved if it is supported by model specification crop2ml defines an abstract representation of model design shared by pbm platforms through some shared concepts enriching or extending those proposed by athanasiadis et al 2011 with other attributes and a formal and shared description of unit tests we included unit tests in crop2ml specifications to ensure model transformation validation and some imperative constructs for model dynamics several initiatives have used declarative modeling to describe model specifications and address model reuse issues the approach proposed by villa et al 2006 is similar to ours but it is limited to models where the dynamics of the modeled processes is represented by simple mathematical expressions without control structures which does not match crop modeling context hucka et al 2003 used mathml ausbrooks et al 2003 to express interactions between variables through mathematical formalisms well defined in the systems biology community this approach is similar to that of rizzoli et al 2008 and is useful when processes are governed by differential equations however in the pbm context simulation platforms use algorithms to describe processes rather than mathematical formalisms with differential equations moreover in pbm variables that drive the system are temporal series that change the behavior of the system at discrete time this does not require finding a general solution of recurrent equations used in crop models but rather estimating at each time step the state variables of the system automated model transformation is a core aspect of model driven development cuadrado and molina 2007 it uses model driven engineering mde principles based on metamodeling concepts crop2ml is in line with mde it defines structured concepts representing its metamodel with which all crop2ml models are conform and a model transformation to generate pbm platforms components model driven architecture brown 2004 is a framework of mde that provides several standard languages e g atl qvt etl henshin viatra and stratego for model transformation jouault and kurtev 2006 kurtev et al 2006 crop2ml is based on a transformation process through a set of refinement of models and code with some extensible rules defined as templates in python most mde approaches allow model to model or model to code transformation where a model represents the specification in our case however the use of transformation language standards was inappropriate in our context to unify transformation process towards many languages with different paradigms bucchiarone et al 2020 crop2ml produces code in a target language but also adapts the code to fit with pbm platform specificities to our knowledge model transformation languages in mde do not support code generation in multiple languages with extended features in the same environment 5 2 connecting crop2ml to pbm platforms given that crop2ml datatypes do not handle complex data structures other than arrays and lists some compromises or transformation should be made to the import export process on the platform side with respect to handling other data structures used in platforms as an example bioma provides the dictionary data type that is a set of keys associated with values to represent either input or output variables this data type is not handled by crop2ml and by most pbm platforms as an alternative dictionaries can be expressed in crop2ml as two list datatype variables that represent keys and values of the dictionary the simulation algorithm defining the feedback loop is explicitly described as control flow in some platforms e g bioma but this is not the case in other platforms e g record where the vpz file representing the simulation model file is handled by the simulation engine vle different simulation engines are based on different models of computation used by the platforms such as dataflow e g openalea devs simulation e g record control flow e g bioma dssat and simplace these models of computation are used to coordinate the execution of the model the current version of crop2ml framework does not take into account the specificities of simulation engines and addresses components which can be sequentially composed the crop2ml transformation system is designed to support the specificities of the target pbm platforms however the semantic of a crop2ml model is based on shared concepts to describe at a high level a biophysical process by a discrete time model there is no semantic reason to support the description of each instance of the concepts for example since we have not defined a convention to name process variables the integration of a crop2ml component into a pbm modeling solution requires adapting the name of its variables in the future we could annotate crop2ml models to add semantic information to make semantic links between any crop2ml model variables or parameters with those of model components of pbm platforms this will also allow a semantic composability of crop2ml models instead of a syntactic composability that analyzes whether the pair of variables to be linked are compatible however this would require the crop modeling community to agree on shared semantics and ontologies of crop model variables and parameter representations until now this has been a real challenge as the crop modeling community has not be too keen on adapting standards white et al 2013 in addition to facilitate the exchange and reuse of model components semantic descriptions of model variables and parameters would facilitate the linking of crop models to plant phenomics data neveu et al 2018 we were able to achieve fully automatic export of crop2ml model to several pbm platforms the import process into crop2ml is more mixed regarding the overall differences between pbm platforms it is much easier to start with concepts shared and reused by pbm platforms than to start from divergent views of model representations to achieve a particular result some pbm platforms need to extend their concepts for model specification or to provide a rich model documentation in order to produce complete crop2ml model specifications this reveals the need of a good level of abstraction to represent a model in various pbm platforms the higher the level of abstraction the further the description moves away from the platforms and the less easy it is to understand on the other hand if the level of abstraction is too low it is not always possible to represent all features of the models present in the platforms 5 3 future developments a common model repository infrastructure is essential for efficient model exchange glont et al 2018 lloyd et al 2008 currently crop2ml model components are stored in github repositories we aim to provide a crop2ml model repository to store models in a shared format to make them easily accessible and reusable by the plant and crop modeling community this repository should aim at hosting alternative biophysical processes it will help modelers to operate on multiple model components compare processes or evaluate the impact of the integration of alternative models of biophysical processes in crop models the success of the crop2ml repository requires that the community gives access to their models by feeding the repository which will be curated by the amei consortium to avoid error propagation crop2ml has some limitations which can be addressed in the next versions either by extending the model specifications with shared concepts or by adapting the target pbm platforms to crop2ml specification and language it is an ongoing long term activity to satisfy platform requirements and facilitate crop2ml model life cycle management to make crop2ml a standard for the plant and crop modeling community the transformation of a model component of a pbm platform into a crop2ml package requires rewriting the model algorithms in the cyml language this limit is currently being addressed by extending the cyml transpiler to a bidirectional transpiler thereby pbm platforms could provide model algorithms in the language they use and the extended cymlt will transform them in cyml and target languages used by other pbm platforms fig 11 this is a two step process first the model algorithms in the language of the source pbm platform will be parsed and an ast will be generated second the rules for transforming this ast into the cyml ast will be applied the second step will reuse the cyml transformation tool developed by midingoyi et al 2020 to produce model algorithms compatible with other languages and pbm platforms other future developments of crop2ml include enhance crop2ml model repositories with model annotation to link publications to models for reproducibility add unit checks and conversions in crop2ml to improve model validity define a methodology to link crop2ml with plant structure representation for multiscale viewing and analysis define and implement an ontology of crop model variable and parameters to allow better crop2ml model interpretation and improve transformation between pbm platforms and the integration of model component in complex modeling solutions extend crop2mlab prototype by including bidirectional transformation and the creation of a web interface on a remote server in order to give users the possibility to handle crop2ml model lifecycle without local installation 6 conclusion at the interface between modeling and software engineering this paper addresses plant and crop model component reuse by proposing the crop2ml framework despite all the differences between pbm platforms some common features can be identified that enabled model representation regardless of the platforms specificities crop2ml provides structured concepts to support the definition of modelunit and modelcomposite and allows their transformation to make them compatible with pbm platforms at implementation level therefore crop2ml defines a new unified crop model representation that considers the abstraction of pbm component features in several pbm platforms moreover crop2ml uses a domain specific language to describe biophysical processes and auxiliary functions to represent model dynamics based on a subset of the cython language which can then be automatically transformed into different target languages crop2ml proposes an open framework to manage all the steps of model lifecycle declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements c m was supported through a phd scholarship from the french national research agency under the investments for the future program referred as anr 16 conv 0004 and inrae divisions agroecosystem and num p m acknowledges the support of inrae division agroecosystem through the mod√©lisation du fonctionnemnet des peuplements cultiv√©s mfpc network the authors thank dr loic manceau inra umr lepse for discussions and its help to translate the wheat phenology bioma component of siriusquality in crop2ml appendix a supplementary data the following is the supplementary data to this article multimedia component 1 multimedia component 1 appendix a supplementary data supplementary data to this article can be found online at https doi org 10 1016 j envsoft 2021 105055 
25814,ensemble based data assimilation eda has been effectively applied to estimate model parameters through inverse modeling in subsurface flow and transport problems to facilitate the management of eda workflow and lower the barriers for adopting eda based parameter estimation in subsurface science we develop a software framework linking the data assimilation research testbed dart with a massively parallel subsurface flow and transport code pflotran dart pflotran enables an iterative eda workflow based on the ensemble smoother for multiple data assimilation method es mda to improve estimation accuracy for nonlinear forward problems we verify the implementation of es mda in dart pflotran using two synthetic cases designed to estimate static permeability and dynamic exchange fluxes across the riverbed from continuous temperature measurements both cases yield accurate estimations of the parameters compared to their synthetic truth with a code base in python and fortran dart pflotran paves the way for large scale inverse modeling using the sequential es mda keywords data assimilation ensemble smoother dart pflotran inverse modeling subsurface flow and transport 1 introduction ensemble based data assimilation eda methods including ensemble kalman filter enkf and ensemble smoother have been extensively used to update model state vectors or estimate poorly known model parameters in various earth science domains evensen 1994 evensen 2003 evensen 2009 van leeuwen and evensen 1996 using a monte carlo based ensemble representation of the joint probability for model states or parameters eda enables the nonlinear evolution of system states through physics based process models in subsurface hydrology and petroleum engineering eda has been widely used for parameter estimation e g hydraulic conductivity or permeability or history matching using field observations such as hydraulic head soil moisture and tracer concentrations bailey and ba√π 2010 haugen et al 2006 chen et al 2013 gharamti et al 2015 zhu et al 2017 song et al 2019 iterative eda methods including ensemble smoother with multiple data assimilation es mda emerick reynoldset al 2013 emerick and reynolds 2013 and ensemble randomized maximum likelihood enrml chen and oliver 2012 chen and oliver 2013 chen et al 2013 have been developed to alleviate the accuracy deterioration caused by the nonlinear relationship in the evolution of model states or between model parameters and model states similar to the gauss newton or levenberg marquardt approaches for nonlinear optimization es mda has been applied to delineate distinct geological facies and to estimate permeability and exchange fluxes through a riverbed given its computational efficiency and estimation accuracy song et al 2019 chen et al 2021 although iterative es methods significantly reduce the number of forward simulation restarts required to conserve physical laws chen et al 2013 the implementation of es mda for large scale inverse modeling is not trivial due to the complex workflow in launching multi physics parallel forward simulations which is often required for managing the computational challenges chen et al 2013 song et al 2019 shuai et al 2019 therefore a user friendly software framework for performing eda with computationally intensive forward models and heterogeneous observational data will significantly increase scientific productivity there exist multiple community supported data assimilation tools including pest and the data assimilation research testbed dart pest developed by the u s geological survey for both parameter estimation and uncertainty analysis adopts an iterative ensemble smoother for solving the gauss levenberg marquardt algorithm in model calibration white et al 2020 dart developed by the national center for atmospheric research provides a variety of eda tools including different filtering techniques as well as various localization and inflation options anderson et al 2009 here we employ dart as the core assimilation engine due to its modular structure that allows integration with various forward simulators by customizing a model specific interface while keeping the data assimilation engine of dart and the forward simulator intact dart has been successfully linked with dozes of community codes for earth system research such as the weather research and forecasting model kurzrock et al 2019 the community atmosphere model raeder et al 2012 and the community land model fox et al 2018 to facilitate model data integration and consequently improve model accuracy the objective of this study is to develop a generic eda software framework for improving subsurface flow and transport models by linking dart with pflotran hammond et al 2014 an open source parallel subsurface flow and reactive transport model sequential es mda which performs es mda in a sequence of assimilation time windows is considered as a generic eda approach that is sufficiently flexible to be configured for performing traditional enkf and es mda one key feature of pflotran is its embedded ensemble simulation capability which greatly facilitates the implementation of eda workflow for subsurface permeability estimation as demonstrated in multiple applications chen et al 2013 song et al 2019 the framework will allow flexible data subsetting in space and time to reduce the data dimension by sequentially assimilating those data subsets using es mda we implemented the dart pflotran in a combination of python c shell and fortran scripting we also provide a jupyter notebook kluyver et al 2016 template as an alternative to python scripting for users to configure the data assimilation options set up forward simulation models and eventually execute the sequential es mda workflow using the c shell script the jupyter notebook not only provides a straightforward way of documentation using the markdown language but also serves as an interactive coding and visualization platform to verify the performance of the dart pflotran software framework we employ the proposed framework to conduct sequential es mda for two synthetic case studies that aim to estimate the exchange fluxes across a sediment water interface from continuous temperature measurements section 2 provides an overview of the sequential es mda workflow and the detailed design of the dart pflotran framework then in section 3 we verify the implementation of an inverse modeling framework using two synthetic test cases the estimated static and dynamic parameters are compared against the synthetic true values to assess implementation success a brief conclusion is drawn in section 4 2 methodology in this section we first describe the general workflow of sequential es mda then we introduce the detailed software design of the dart pflotran framework which includes enabling an ensemble smoother in dart the integrated workflow for performing sequential es mda in dart pflotran and utilities for coupling dart with pflotran 2 1 sequential es mda fig 1 illustrates the steps we take in sequential es mda to assimilate different subsets of data the observations contain n time steps t j 1 j n which are divided into m sub domains or assimilation windows i e t i with i 1 m each window t i contains one or multiple consecutive observations the workflow starts with a model spin up to ensure that the model has reached a reasonable initial state after the spin up es mda is sequentially performed on each t i following a prescribed order within each assimilation time window es mda assimilates all the observations taken within that time window which may include more than one observation step note that enkf is a special case of this general workflow when observations from one time step are assimilated for every t i i e m n without the multiple iterative data assimilation similarly es is another special case of this general workflow when all the observations are included in one single assimilation time window without iterations at the first assimilation window t 1 the prior ensemble of the model parameters are sampled from assumed prior distributions in the remaining windows t i the prior ensemble are generated based on the posterior ensemble at the preceding window t i 1 differently for static and dynamic parameters for static parameters the prior are directly adopted from their updated posterior ensemble at t i 1 for dynamic parameters if assuming continuity in time the prior ensemble can be sampled from distributions that preserve their mean values computed from the posterior ensemble at t i 1 with the same variance or lower and upper bounds used in the first assimilation time window otherwise the prior ensemble used in the first assimilation time window can be adopted in all the remaining time windows as well during each iteration of es mda ensemble forward simulations are performed to generate the prior ensemble of the state vectors within the assimilation time window model parameters are updated using the following equation emerick and reynolds 2013 1 m k l u m k l f c m d l f c d d l f Œ± l c d 1 d o b s Œ± l c d 1 2 z k d k l f k 1 n e and l 1 l where the superscripts u and f refer to updated and forecast respectively the subscripts k and l are the indices of the ensemble member and the iteration respectively n e is the ensemble size l is the total number of iterations in es mda m k l u and m k l f are the kth ensemble member of the updated i e posterior and forecast i e prior parameters respectively at the lth iteration d o b s is the observation data z k is the corresponding observation noise vector sampled from independent standard normal distributions for the kth ensemble member d k l f is the kth ensemble member of the predicted observation variables by the forward model driven by the prior ensemble of the parameters at the lth iteration c m d l f is the cross covariance matrix between the prior parameters and the predicted observation variables c d d l f is the auto covariance matrix of the predicted observation variables based on all the ensemble members d k l f c d is the auto covariance matrix of the observation errors and Œ± l is the inflation coefficient at the lth iteration satisfying l 1 l 1 Œ± l 1 once all the iterations within the assimilation time window are completed the posterior ensemble of the parameters are fed to the forward simulator to generate the posterior ensemble of the state vector at the end of the assimilation window t i which will then serve as the ensemble of the initial conditions for the next assimilation time window 2 2 design of the dart pflotran framework we developed a new software framework to perform the sequential es mda illustrated in fig 1 by linking dart and pflotran we first enabled an ensemble smoother capability in dart see section 2 3 to leverage all the assimilation options in its core data assimilation engine then we developed multiple utility functions see section 2 5 to modularize the execution of dart pflotran in four primary steps 1 configuring data assimilation specifics 2 pflotran forward model configuration and preparation 3 dart preparation and 4 performing sequential es mda scripts were developed in python and c shell to conduct the first three steps and the last step respectively a jupyter notebook is also provided to integrate the entire workflow which can be used as a tool to learn the framework and as a template for adapting to other applications 2 3 enabling ensemble smoother in dart designed for updating model states using the filter approach dart adopts a local least square optimization algorithm referred to as the anderson collins algorithm hereafter anderson 2003 anderson and collins 2007 the algorithm obtains the posterior state and parameter ensembles by adding up all individual ensemble increments which result from assimilating a single dimension of the multi dimensional observation at a given time step although the original dart does not directly allow the assimilation of observations from multiple time steps within a given time window the underlying anderson collins algorithm offers the flexibility to be readily extended for ensemble smoother based approaches it is noted that the anderson collins algorithm is theoretically equivalent to using eq 1 for non iterative updating i e setting l 1 in eq 1 when assimilating one observation dimension in each time window anderson and collins 2007 and is also practically equivalent to the es using eq 1 when several multi dimensional observations are assimilated within the window therefore we enabled the smoother option in dart by modifying how the code maps the observations with model simulated states in a given assimilation time step observed or modeled states from different time steps are treated as multiple dimensions of data from a single time step in doing so both filter and smoother options are now available in the assimilation engine of dart 2 4 integrated workflow for performing sequential es mda in dart pflotran with the smoother option enabled in dart we run dart pflotran following the integrated workflow shown in fig 2 step 1 data assimilation configuration the following information is required to configure a dart pflotran application 1 the path to relevant files folders such as the pflotran executable the application folder storing the pflotran files and the dart prior posterior ensemble and the dart pflotran framework folder storing the utility files needed to execute the workflow e g those described in section 2 5 2 information about the observations such as the variables to be assimilated the spatio temporal domains of the observations and the folder location of the observation file 3 information about the model parameters to be updated such as the list of parameter names and their corresponding prior distributions for generating the prior ensemble and 4 the data assimilation settings such as the ensemble size the assimilation window size the number of iterations along with the inflation coefficient for each iteration emerick and reynolds 2013 and any other data assimilation option supported by dart step 2 pflotran preparation at this step pflotran input files are prepared and model spin up is performed pflotran inputs are composed of a pflotran input deck which users have to provide to configure a pflotran model conforming to the conceptual model of a specific application and an hdf5 the hdf group file that contains the prior ensemble of the parameters generated from their prior distributions defined in step 1 once the pflotran inputs are ready model spin up will be performed for a selected period of time prior to the beginning of the first assimilation time window to ensure reasonable initial conditions for the forward simulations step 3 dart preparation to configure dart for the es mda task for a specific application we use utility tools see section 2 5 to automatically generate the following files based on the user input and preparation done in the previous two steps 1 the fortran namelist file that records a variety of dart configurations using the data assimilation setting specified in step 1 2 the updated dart variable library that includes new pflotran parameter and state names in step 2 and 3 a utility function file convert nc f90 to convert the observations at each time window from the netcdf file into a dart observation sequence file see section 2 5 finally this step will generate all the executables for running dart step 4 performing the sequential es mda the assimilation process performs es mda in each time window to update model parameters as illustrated in fig 3 until all observations are assimilated in each assimilation time window the es mda starts by updating the prior ensemble of model parameters in the hdf5 file which is then provided to launch ensemble pflotran simulations that produce the prior ensemble of simulated state vectors after the forward simulations the prior ensembles of parameters and model states are combined and saved into a netcdf file for each ensemble member then the data in the dart observation sequence file are assimilated by dart to generate the posterior ensemble of parameters which become the prior ensemble of parameters for the next iteration after all es mda iterations are completed within an assimilation time window the posterior ensemble of parameters are used to run pflotran simulations to generate the posterior state ensemble at the current window which are used as the initial conditions for the subsequent assimilation time window 2 5 utility functions to link dart and pflotran to facilitate the four step integrated workflow the following utility functions are developed to link dart and pflotran shown as blue lines and gray text in fig 3 prepare input nml py is used in step 3 to generate a fortran namelist file from user specified data assimilation configurations such as the number of ensemble members paths to prior and posterior files temporal range of observations a detailed description of the namelist file is available at dart s official website hoar 2017 list2dartqty py is used in step 3 to modify dart fortran files to register a list of pflotran parameter and state variable names in the dart variable library so that dart can extract the prior ensemble from the netcdf files and map the ensemble model state vectors with the data in the observation file prepare prior nc py is used to prepare an individual netcdf file for each prior ensemble member of pflotran parameters and state vectors as well as their spatial locations and time steps convert nc f90 is used to generate a dart observation sequence file for each assimilation window by extracting the associated data from the observation netcdf file to use this utility function users need to provide a standardized netcdf file in step 1 which includes the times when the observations were taken the spatial locations of observations values of each observation variable in a two dimensional matrix i e temporal and spatial dimensions and an additional two dimensional matrix for observation errors with one to one correspondence to all the observation values model mod f90 contains a set of fortran subroutines that allows dart to 1 define the spatial and temporal domains of observations at a data assimilation step and 2 compile the ensemble members of model simulated state variables at the same locations and times of the observations update pflotran inputs py is used to update the pflotran input files after an es mda iteration the realizations of pflotran parameters in the hdf5 file will be updated using the posterior ensemble of parameters the model simulation time window in pflotran input deck will be updated if the data assimilation proceeds to the next assimilation time window 3 verification of data assimilation implementation we verified the implementation of dart pflotran using two test cases that aim to estimate static and dynamic parameters respectively we attempted to estimate the permeability field as well as the dynamic exchange fluxes across the riverbed from temperature depth profiles monitored beneath the riverbed over time as illustrated in fig 4 a the groundwater temperatures below the riverbed at different depths are related to the dynamic exchange fluxes through one dimensional 1 d flow and heat transport processes simulated by pflotran 2a q k œÅ w g Œº w d h d l 2b q e t œÜ Œ∑ u 1 œÜ œÅ r c p t Œ∑ q h Œ∫ t where q is the groundwater exchange flux m s k is the soil permeability m2 œÅ w is water density kg m3 g is the gravitational acceleration m s2 Œº w is water viscosity kg ms d h is the difference between two hydraulic heads m d l is the flow path length between two points m œÜ is the porosity of soil matrix q e is source sink terms for energy transport j m3k Œ∑ is molar water density kmol m3 u is internal energy of the fluid j kg t is the groundwater temperature in kelvin k h is enthalpy j kg œÅ r is rock density kg m3 c p is specific heat capacity j kgk and Œ∫ is thermal conductivity j mks of the porous media for the flow process darcy s law is used to compute the exchange flux eq 2a which is coupled to the heat transport process in groundwater governed by the energy balance eq 2b in the first test case we assumed that the hydraulic heads at the top and bottom boundaries were measured continuously along with temperature therefore the exchange fluxes can be estimated using darcy s law if the permeability of the porous media is known in this case we implemented es mda to estimate the permeability of the soil column that does not change over time see fig 4 b in contrast in the second test case we assumed that no hydraulic heads were measured as a result the exchange flux within a time window has to be directly estimated from the temperature responses below the riverbed furthermore the exchange flux could vary over time driven by the stage fluctuations in the river which was reflected as the neumann type boundary condition for exchange flux based on the synthetic data thus we implemented the sequential es mda in the second test case to sequentially estimate the exchange fluxes in a set of predefined time windows see fig 4 c 3 1 generation of synthetic observation data in order to evaluate the accuracy of the estimated parameters in both test cases we generated synthetic observation data of hydraulic heads and temperature responses with known permeability and dynamic exchange fluxes which serve as the ground truth of the parameters estimated by dart pflotran for performance assessment pflotran was used to generate the temperature responses in a 65 cm soil column shown in fig 4 a the model domain was discretized into 1 cm vertical grid cells with a homogeneous soil permeability value across the entire depth the time varying hydraulic heads and temperature at the top and bottom boundaries which can be obtained from monitoring data in practice were used as the boundary conditions the pflotran simulation generated riverbed exchange flux at a 30 min resolution and groundwater temperature at the center of each grid cell at a 5 min resolution for 3 months we assumed that the temperature observations were available at the depths of 5 cm 15 cm and 25 cm in addition to the top and bottom of the boundaries the synthetic temperature observation data were obtained by adding observation errors generated from a gaussian distribution with a mean of 0 and a standard deviation of 0 05 3 0 0167 k mimicking an observation error of 0 05 k we used the same observation error during the data assimilation process 3 2 case 1 estimating static permeability using es mda in this test case the prior ensemble of log10 transformed permeability was generated by sampling 100 realizations from a log normal distribution with mean and standard deviation being 11 and 1 log 10 m 2 respectively we first performed a two day spin up data assimilation to constrain the initial temperature profile conformed to the point observations at the observation depths then the permeability ensemble was updated from the aforementioned prior ensemble by assimilating observations at the depths of 5 cm 15 cm and 25 cm over 50 time steps using the es mda in dart pflotran with a single assimilation time window we assessed the impact of the number of iterations i e l evensen 1994 evensen 2003 evensen 2009 on estimating the unknown permeability we adopted equally divided inflation coefficient Œ± l in eq 1 for each l such that Œ± l evensen 1994 evensen 2003 evensen 2009 for l evensen 1994 evensen 2003 evensen 2009 respectively in fig 5 the prior and updated posterior distributions of log10 transformed permeability resulting from different l are shown in violin plots and compared with the ground truth i e 10 41 log 10 m 2 represented by the red dashed line note that the posterior ensemble is obtained from the last iteration for a given l it can be observed that the mean of the posterior ensemble is improved and approaches the true permeability with the increasing number of iterations while the spread in the posterior ensemble shrinks significantly with more iterations two iterations appear to be adequate in this test case as there is negligible improvement in the estimation by increasing to three iterations the convergence of posterior permeability estimation to its true value verifies our implementation of dart pflotran in using es mda for a single assimilation time window by compiling data taken in different locations and at different times 3 3 case 2 estimating dynamic groundwater exchange fluxes using sequential es mda in this test case we sequentially estimated the hourly exchange fluxes over a month by assimilating the associated groundwater temperature observations within each assimilation time window i e hourly with 12 observed temperature data points at each depth the initial prior ensemble of the exchange flux was generated by sampling 100 realizations from a gaussian distribution with a mean of 0 m s and standard deviation of 0 5 m s positive and negative fluxes refer to the downwelling and upwelling fluxes respectively in each of the subsequent assimilation time windows we generated 100 realizations of the exchange flux for the prior ensemble by shifting its mean to the posterior mean resulting from the immediate preceding assimilation time window while maintaining 0 5 m s as the standard deviation which is a form of posterior inflation where the posterior spread is relaxed to the prior whitaker and hamill 2012 the data assimilation was started two days earlier than the targeted estimation time window as the spin up to minimize the impact of the initial conditions on the flux estimation accuracy we first tested the flux estimations using one iteration i e l 1 to verify the implementation of sequential es in dart pflotran fig 6 shows the ensembles of the hourly exchange fluxes and the simulated groundwater temperature before and after assimilating temperature responses as compared against the synthetic observations and ground truth in both the time series and scatter plots all pairs of ensemble mean of the estimated hourly flux vs its ground truth are tightly distributed around the 1 1 line with substantial reduction in the ensemble uncertainty which consequently improves the simulated temperature responses below the riverbed the results in fig 6 clearly demonstrate the effective dynamic parameter estimation using the sequential es approach implemented in dart pflotran we then compared the performance of the sequential es with the original enkf scheme in dart for estimating the dynamic exchange fluxes which does not honor different observed time steps in each assimilation window this is done by computing the mean absolute error mae against the ground truth in posterior flux and temperature estimations at each time window as plotted in fig 7 a showing the mae comparisons between es and enkf with their corresponding means over the entire estimation time window Œº m a e shown on the top of each subplot the results show that mae of estimated fluxes from the two approaches are comparable with most of the maes smaller than 0 2 m s and data pairs distributed nearly symmetrically around the black dotted 1 1 line there are more data pairs falling below the 1 1 line in the larger mae regime i e larger than 0 3 m s illustrating that enkf tends to produce more higher absolute errors which is also consistent with its higher average mae than that of es i e 0 141 m s vs 0 116 m s the more accurate estimations of exchange fluxes by es result in smaller maes in the simulated groundwater temperature across all depths more so at the depths of 15 and 25 cm as evidenced by more data pairs falling below the 1 1 line while the exchange fluxes by both es and enkf yield highly accurate predictions of groundwater temperature as illustrated by the small magnitude of their maximum and average maes es reduces the average temperature maes to approximately half compared to enkf such gain in estimation accuracy demonstrates the potential advantage of es in parameter estimation over the original enkf scheme in dart lastly we assessed the performance gain of multiple iterations in sequential es mda by comparing the maes of both exchange fluxes and simulated groundwater temperatures produced by l 3 and l 1 i e the es approach as shown in fig 7 b the results show universal reductions in maes when the number of iterations in es mda is increased from one to three although there is only a slight decrease in average mae of estimated exchange fluxes i e from 0 116 m s to 0 104 m s when increasing the iteration number from one to three there are substantial reductions in a number of large flux maes with the iterative data assimilation as represented by the data pairs far above the 1 1 line such improvement in flux estimation through iterations leads to significant reductions in the maes of the simulated groundwater temperature effectively eliminating all maes larger than 0 1 c in es the performance gain through iterative es in this test case not only verifies our implementation of the sequential es mda in dart pflotran it also demonstrates the necessity of taking the iterative es to improve data assimilation accuracy under nonlinearity while both cases show the performance improvement using iterative assimilation the performance gain in case 2 is slight and is not significant as case 1 whose estimated permeability converges to the ground truth after only two iterations fig 5 the less estimation improvement using multiple assimilation in case 2 is probably due to the sequential way of assimilation that uses hourly time window for performing es mda and therefore greatly reduces the nonlinear dynamics impact on the performance at each window hence it is expected that the iterative assimilation of dart pflotran would potentially facilitate applications with stronger nonlinearity impact e g large assimilation domain with more complex terrain 4 conclusion in this study we developed an open source software framework dart pflotran for conducting sequential es mda to estimate static and dynamic parameters for subsurface flow and transport models this new software framework links dart a community facility for data assimilation with pflotran a parallel simulation code for subsurface flow and reactive transport processes we enabled the ensemble smoother option in dart and developed multiple utility functions to establish communications between dart and pflotran for sequential es mda we verified the implementation of dart pflotran for both the static and dynamic parameter estimations using two synthetic cases which demonstrated that we have successfully extended dart beyond its traditional applications in atmospheric science for updating model state vectors we implemented the integrated workflow of performing es mda in both python and c shell scripts we also provided a user friendly interface using jupyter notebook the scripts and the jupyter notebook templates can be easily adapted to other applications to alleviate the burden of managing the complex data assimilation and parameter estimation workflow especially when sequential and iterative assimilation is necessary to reduce the adverse effects of nonlinearity on estimation accuracy with the added flexibility in subsetting observation data in space and time dart pflotran is poised for large scale applications with complex and highly heterogeneous terrain the workflow developed to link dart and pflotran can also be extended to link dart with other similar simulators such as the advanced terrestrial simulator the ats group and parflow kollet and maxwell 2006 which will greatly accelerate the integration of multi scale and multi type observations above and below ground with watershed models to improve the predictability of a wide variety of real systems software availability software name dart pflotran developer peishi jiang year first official release 2021 programming language python fortran c shell program size 134 mb availability gitlab pnnl gov sbrsfa dart pflotran declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank tim hoar from national center for atmospheric research for his insightful suggestions on improving the dart pflotran software this research was supported by the u s department of energy doe office of biological and environmental research ber as part of ber s subsurface biogeochemical research program sbr this contribution originates from the sbr scientific focus area sfa at the pacific northwest national laboratory pnnl and was supported by the partnership with the ideas watersheds this research used resources of the national energy research scientific computing center a doe office of science user facility supported by the office of science of the u s department of energy pnnl is operated for the doe by battelle memorial institute under contract de ac05 76rl01830 this paper describes objective technical results and analysis any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the u s department of energy or the united states government 
25814,ensemble based data assimilation eda has been effectively applied to estimate model parameters through inverse modeling in subsurface flow and transport problems to facilitate the management of eda workflow and lower the barriers for adopting eda based parameter estimation in subsurface science we develop a software framework linking the data assimilation research testbed dart with a massively parallel subsurface flow and transport code pflotran dart pflotran enables an iterative eda workflow based on the ensemble smoother for multiple data assimilation method es mda to improve estimation accuracy for nonlinear forward problems we verify the implementation of es mda in dart pflotran using two synthetic cases designed to estimate static permeability and dynamic exchange fluxes across the riverbed from continuous temperature measurements both cases yield accurate estimations of the parameters compared to their synthetic truth with a code base in python and fortran dart pflotran paves the way for large scale inverse modeling using the sequential es mda keywords data assimilation ensemble smoother dart pflotran inverse modeling subsurface flow and transport 1 introduction ensemble based data assimilation eda methods including ensemble kalman filter enkf and ensemble smoother have been extensively used to update model state vectors or estimate poorly known model parameters in various earth science domains evensen 1994 evensen 2003 evensen 2009 van leeuwen and evensen 1996 using a monte carlo based ensemble representation of the joint probability for model states or parameters eda enables the nonlinear evolution of system states through physics based process models in subsurface hydrology and petroleum engineering eda has been widely used for parameter estimation e g hydraulic conductivity or permeability or history matching using field observations such as hydraulic head soil moisture and tracer concentrations bailey and ba√π 2010 haugen et al 2006 chen et al 2013 gharamti et al 2015 zhu et al 2017 song et al 2019 iterative eda methods including ensemble smoother with multiple data assimilation es mda emerick reynoldset al 2013 emerick and reynolds 2013 and ensemble randomized maximum likelihood enrml chen and oliver 2012 chen and oliver 2013 chen et al 2013 have been developed to alleviate the accuracy deterioration caused by the nonlinear relationship in the evolution of model states or between model parameters and model states similar to the gauss newton or levenberg marquardt approaches for nonlinear optimization es mda has been applied to delineate distinct geological facies and to estimate permeability and exchange fluxes through a riverbed given its computational efficiency and estimation accuracy song et al 2019 chen et al 2021 although iterative es methods significantly reduce the number of forward simulation restarts required to conserve physical laws chen et al 2013 the implementation of es mda for large scale inverse modeling is not trivial due to the complex workflow in launching multi physics parallel forward simulations which is often required for managing the computational challenges chen et al 2013 song et al 2019 shuai et al 2019 therefore a user friendly software framework for performing eda with computationally intensive forward models and heterogeneous observational data will significantly increase scientific productivity there exist multiple community supported data assimilation tools including pest and the data assimilation research testbed dart pest developed by the u s geological survey for both parameter estimation and uncertainty analysis adopts an iterative ensemble smoother for solving the gauss levenberg marquardt algorithm in model calibration white et al 2020 dart developed by the national center for atmospheric research provides a variety of eda tools including different filtering techniques as well as various localization and inflation options anderson et al 2009 here we employ dart as the core assimilation engine due to its modular structure that allows integration with various forward simulators by customizing a model specific interface while keeping the data assimilation engine of dart and the forward simulator intact dart has been successfully linked with dozes of community codes for earth system research such as the weather research and forecasting model kurzrock et al 2019 the community atmosphere model raeder et al 2012 and the community land model fox et al 2018 to facilitate model data integration and consequently improve model accuracy the objective of this study is to develop a generic eda software framework for improving subsurface flow and transport models by linking dart with pflotran hammond et al 2014 an open source parallel subsurface flow and reactive transport model sequential es mda which performs es mda in a sequence of assimilation time windows is considered as a generic eda approach that is sufficiently flexible to be configured for performing traditional enkf and es mda one key feature of pflotran is its embedded ensemble simulation capability which greatly facilitates the implementation of eda workflow for subsurface permeability estimation as demonstrated in multiple applications chen et al 2013 song et al 2019 the framework will allow flexible data subsetting in space and time to reduce the data dimension by sequentially assimilating those data subsets using es mda we implemented the dart pflotran in a combination of python c shell and fortran scripting we also provide a jupyter notebook kluyver et al 2016 template as an alternative to python scripting for users to configure the data assimilation options set up forward simulation models and eventually execute the sequential es mda workflow using the c shell script the jupyter notebook not only provides a straightforward way of documentation using the markdown language but also serves as an interactive coding and visualization platform to verify the performance of the dart pflotran software framework we employ the proposed framework to conduct sequential es mda for two synthetic case studies that aim to estimate the exchange fluxes across a sediment water interface from continuous temperature measurements section 2 provides an overview of the sequential es mda workflow and the detailed design of the dart pflotran framework then in section 3 we verify the implementation of an inverse modeling framework using two synthetic test cases the estimated static and dynamic parameters are compared against the synthetic true values to assess implementation success a brief conclusion is drawn in section 4 2 methodology in this section we first describe the general workflow of sequential es mda then we introduce the detailed software design of the dart pflotran framework which includes enabling an ensemble smoother in dart the integrated workflow for performing sequential es mda in dart pflotran and utilities for coupling dart with pflotran 2 1 sequential es mda fig 1 illustrates the steps we take in sequential es mda to assimilate different subsets of data the observations contain n time steps t j 1 j n which are divided into m sub domains or assimilation windows i e t i with i 1 m each window t i contains one or multiple consecutive observations the workflow starts with a model spin up to ensure that the model has reached a reasonable initial state after the spin up es mda is sequentially performed on each t i following a prescribed order within each assimilation time window es mda assimilates all the observations taken within that time window which may include more than one observation step note that enkf is a special case of this general workflow when observations from one time step are assimilated for every t i i e m n without the multiple iterative data assimilation similarly es is another special case of this general workflow when all the observations are included in one single assimilation time window without iterations at the first assimilation window t 1 the prior ensemble of the model parameters are sampled from assumed prior distributions in the remaining windows t i the prior ensemble are generated based on the posterior ensemble at the preceding window t i 1 differently for static and dynamic parameters for static parameters the prior are directly adopted from their updated posterior ensemble at t i 1 for dynamic parameters if assuming continuity in time the prior ensemble can be sampled from distributions that preserve their mean values computed from the posterior ensemble at t i 1 with the same variance or lower and upper bounds used in the first assimilation time window otherwise the prior ensemble used in the first assimilation time window can be adopted in all the remaining time windows as well during each iteration of es mda ensemble forward simulations are performed to generate the prior ensemble of the state vectors within the assimilation time window model parameters are updated using the following equation emerick and reynolds 2013 1 m k l u m k l f c m d l f c d d l f Œ± l c d 1 d o b s Œ± l c d 1 2 z k d k l f k 1 n e and l 1 l where the superscripts u and f refer to updated and forecast respectively the subscripts k and l are the indices of the ensemble member and the iteration respectively n e is the ensemble size l is the total number of iterations in es mda m k l u and m k l f are the kth ensemble member of the updated i e posterior and forecast i e prior parameters respectively at the lth iteration d o b s is the observation data z k is the corresponding observation noise vector sampled from independent standard normal distributions for the kth ensemble member d k l f is the kth ensemble member of the predicted observation variables by the forward model driven by the prior ensemble of the parameters at the lth iteration c m d l f is the cross covariance matrix between the prior parameters and the predicted observation variables c d d l f is the auto covariance matrix of the predicted observation variables based on all the ensemble members d k l f c d is the auto covariance matrix of the observation errors and Œ± l is the inflation coefficient at the lth iteration satisfying l 1 l 1 Œ± l 1 once all the iterations within the assimilation time window are completed the posterior ensemble of the parameters are fed to the forward simulator to generate the posterior ensemble of the state vector at the end of the assimilation window t i which will then serve as the ensemble of the initial conditions for the next assimilation time window 2 2 design of the dart pflotran framework we developed a new software framework to perform the sequential es mda illustrated in fig 1 by linking dart and pflotran we first enabled an ensemble smoother capability in dart see section 2 3 to leverage all the assimilation options in its core data assimilation engine then we developed multiple utility functions see section 2 5 to modularize the execution of dart pflotran in four primary steps 1 configuring data assimilation specifics 2 pflotran forward model configuration and preparation 3 dart preparation and 4 performing sequential es mda scripts were developed in python and c shell to conduct the first three steps and the last step respectively a jupyter notebook is also provided to integrate the entire workflow which can be used as a tool to learn the framework and as a template for adapting to other applications 2 3 enabling ensemble smoother in dart designed for updating model states using the filter approach dart adopts a local least square optimization algorithm referred to as the anderson collins algorithm hereafter anderson 2003 anderson and collins 2007 the algorithm obtains the posterior state and parameter ensembles by adding up all individual ensemble increments which result from assimilating a single dimension of the multi dimensional observation at a given time step although the original dart does not directly allow the assimilation of observations from multiple time steps within a given time window the underlying anderson collins algorithm offers the flexibility to be readily extended for ensemble smoother based approaches it is noted that the anderson collins algorithm is theoretically equivalent to using eq 1 for non iterative updating i e setting l 1 in eq 1 when assimilating one observation dimension in each time window anderson and collins 2007 and is also practically equivalent to the es using eq 1 when several multi dimensional observations are assimilated within the window therefore we enabled the smoother option in dart by modifying how the code maps the observations with model simulated states in a given assimilation time step observed or modeled states from different time steps are treated as multiple dimensions of data from a single time step in doing so both filter and smoother options are now available in the assimilation engine of dart 2 4 integrated workflow for performing sequential es mda in dart pflotran with the smoother option enabled in dart we run dart pflotran following the integrated workflow shown in fig 2 step 1 data assimilation configuration the following information is required to configure a dart pflotran application 1 the path to relevant files folders such as the pflotran executable the application folder storing the pflotran files and the dart prior posterior ensemble and the dart pflotran framework folder storing the utility files needed to execute the workflow e g those described in section 2 5 2 information about the observations such as the variables to be assimilated the spatio temporal domains of the observations and the folder location of the observation file 3 information about the model parameters to be updated such as the list of parameter names and their corresponding prior distributions for generating the prior ensemble and 4 the data assimilation settings such as the ensemble size the assimilation window size the number of iterations along with the inflation coefficient for each iteration emerick and reynolds 2013 and any other data assimilation option supported by dart step 2 pflotran preparation at this step pflotran input files are prepared and model spin up is performed pflotran inputs are composed of a pflotran input deck which users have to provide to configure a pflotran model conforming to the conceptual model of a specific application and an hdf5 the hdf group file that contains the prior ensemble of the parameters generated from their prior distributions defined in step 1 once the pflotran inputs are ready model spin up will be performed for a selected period of time prior to the beginning of the first assimilation time window to ensure reasonable initial conditions for the forward simulations step 3 dart preparation to configure dart for the es mda task for a specific application we use utility tools see section 2 5 to automatically generate the following files based on the user input and preparation done in the previous two steps 1 the fortran namelist file that records a variety of dart configurations using the data assimilation setting specified in step 1 2 the updated dart variable library that includes new pflotran parameter and state names in step 2 and 3 a utility function file convert nc f90 to convert the observations at each time window from the netcdf file into a dart observation sequence file see section 2 5 finally this step will generate all the executables for running dart step 4 performing the sequential es mda the assimilation process performs es mda in each time window to update model parameters as illustrated in fig 3 until all observations are assimilated in each assimilation time window the es mda starts by updating the prior ensemble of model parameters in the hdf5 file which is then provided to launch ensemble pflotran simulations that produce the prior ensemble of simulated state vectors after the forward simulations the prior ensembles of parameters and model states are combined and saved into a netcdf file for each ensemble member then the data in the dart observation sequence file are assimilated by dart to generate the posterior ensemble of parameters which become the prior ensemble of parameters for the next iteration after all es mda iterations are completed within an assimilation time window the posterior ensemble of parameters are used to run pflotran simulations to generate the posterior state ensemble at the current window which are used as the initial conditions for the subsequent assimilation time window 2 5 utility functions to link dart and pflotran to facilitate the four step integrated workflow the following utility functions are developed to link dart and pflotran shown as blue lines and gray text in fig 3 prepare input nml py is used in step 3 to generate a fortran namelist file from user specified data assimilation configurations such as the number of ensemble members paths to prior and posterior files temporal range of observations a detailed description of the namelist file is available at dart s official website hoar 2017 list2dartqty py is used in step 3 to modify dart fortran files to register a list of pflotran parameter and state variable names in the dart variable library so that dart can extract the prior ensemble from the netcdf files and map the ensemble model state vectors with the data in the observation file prepare prior nc py is used to prepare an individual netcdf file for each prior ensemble member of pflotran parameters and state vectors as well as their spatial locations and time steps convert nc f90 is used to generate a dart observation sequence file for each assimilation window by extracting the associated data from the observation netcdf file to use this utility function users need to provide a standardized netcdf file in step 1 which includes the times when the observations were taken the spatial locations of observations values of each observation variable in a two dimensional matrix i e temporal and spatial dimensions and an additional two dimensional matrix for observation errors with one to one correspondence to all the observation values model mod f90 contains a set of fortran subroutines that allows dart to 1 define the spatial and temporal domains of observations at a data assimilation step and 2 compile the ensemble members of model simulated state variables at the same locations and times of the observations update pflotran inputs py is used to update the pflotran input files after an es mda iteration the realizations of pflotran parameters in the hdf5 file will be updated using the posterior ensemble of parameters the model simulation time window in pflotran input deck will be updated if the data assimilation proceeds to the next assimilation time window 3 verification of data assimilation implementation we verified the implementation of dart pflotran using two test cases that aim to estimate static and dynamic parameters respectively we attempted to estimate the permeability field as well as the dynamic exchange fluxes across the riverbed from temperature depth profiles monitored beneath the riverbed over time as illustrated in fig 4 a the groundwater temperatures below the riverbed at different depths are related to the dynamic exchange fluxes through one dimensional 1 d flow and heat transport processes simulated by pflotran 2a q k œÅ w g Œº w d h d l 2b q e t œÜ Œ∑ u 1 œÜ œÅ r c p t Œ∑ q h Œ∫ t where q is the groundwater exchange flux m s k is the soil permeability m2 œÅ w is water density kg m3 g is the gravitational acceleration m s2 Œº w is water viscosity kg ms d h is the difference between two hydraulic heads m d l is the flow path length between two points m œÜ is the porosity of soil matrix q e is source sink terms for energy transport j m3k Œ∑ is molar water density kmol m3 u is internal energy of the fluid j kg t is the groundwater temperature in kelvin k h is enthalpy j kg œÅ r is rock density kg m3 c p is specific heat capacity j kgk and Œ∫ is thermal conductivity j mks of the porous media for the flow process darcy s law is used to compute the exchange flux eq 2a which is coupled to the heat transport process in groundwater governed by the energy balance eq 2b in the first test case we assumed that the hydraulic heads at the top and bottom boundaries were measured continuously along with temperature therefore the exchange fluxes can be estimated using darcy s law if the permeability of the porous media is known in this case we implemented es mda to estimate the permeability of the soil column that does not change over time see fig 4 b in contrast in the second test case we assumed that no hydraulic heads were measured as a result the exchange flux within a time window has to be directly estimated from the temperature responses below the riverbed furthermore the exchange flux could vary over time driven by the stage fluctuations in the river which was reflected as the neumann type boundary condition for exchange flux based on the synthetic data thus we implemented the sequential es mda in the second test case to sequentially estimate the exchange fluxes in a set of predefined time windows see fig 4 c 3 1 generation of synthetic observation data in order to evaluate the accuracy of the estimated parameters in both test cases we generated synthetic observation data of hydraulic heads and temperature responses with known permeability and dynamic exchange fluxes which serve as the ground truth of the parameters estimated by dart pflotran for performance assessment pflotran was used to generate the temperature responses in a 65 cm soil column shown in fig 4 a the model domain was discretized into 1 cm vertical grid cells with a homogeneous soil permeability value across the entire depth the time varying hydraulic heads and temperature at the top and bottom boundaries which can be obtained from monitoring data in practice were used as the boundary conditions the pflotran simulation generated riverbed exchange flux at a 30 min resolution and groundwater temperature at the center of each grid cell at a 5 min resolution for 3 months we assumed that the temperature observations were available at the depths of 5 cm 15 cm and 25 cm in addition to the top and bottom of the boundaries the synthetic temperature observation data were obtained by adding observation errors generated from a gaussian distribution with a mean of 0 and a standard deviation of 0 05 3 0 0167 k mimicking an observation error of 0 05 k we used the same observation error during the data assimilation process 3 2 case 1 estimating static permeability using es mda in this test case the prior ensemble of log10 transformed permeability was generated by sampling 100 realizations from a log normal distribution with mean and standard deviation being 11 and 1 log 10 m 2 respectively we first performed a two day spin up data assimilation to constrain the initial temperature profile conformed to the point observations at the observation depths then the permeability ensemble was updated from the aforementioned prior ensemble by assimilating observations at the depths of 5 cm 15 cm and 25 cm over 50 time steps using the es mda in dart pflotran with a single assimilation time window we assessed the impact of the number of iterations i e l evensen 1994 evensen 2003 evensen 2009 on estimating the unknown permeability we adopted equally divided inflation coefficient Œ± l in eq 1 for each l such that Œ± l evensen 1994 evensen 2003 evensen 2009 for l evensen 1994 evensen 2003 evensen 2009 respectively in fig 5 the prior and updated posterior distributions of log10 transformed permeability resulting from different l are shown in violin plots and compared with the ground truth i e 10 41 log 10 m 2 represented by the red dashed line note that the posterior ensemble is obtained from the last iteration for a given l it can be observed that the mean of the posterior ensemble is improved and approaches the true permeability with the increasing number of iterations while the spread in the posterior ensemble shrinks significantly with more iterations two iterations appear to be adequate in this test case as there is negligible improvement in the estimation by increasing to three iterations the convergence of posterior permeability estimation to its true value verifies our implementation of dart pflotran in using es mda for a single assimilation time window by compiling data taken in different locations and at different times 3 3 case 2 estimating dynamic groundwater exchange fluxes using sequential es mda in this test case we sequentially estimated the hourly exchange fluxes over a month by assimilating the associated groundwater temperature observations within each assimilation time window i e hourly with 12 observed temperature data points at each depth the initial prior ensemble of the exchange flux was generated by sampling 100 realizations from a gaussian distribution with a mean of 0 m s and standard deviation of 0 5 m s positive and negative fluxes refer to the downwelling and upwelling fluxes respectively in each of the subsequent assimilation time windows we generated 100 realizations of the exchange flux for the prior ensemble by shifting its mean to the posterior mean resulting from the immediate preceding assimilation time window while maintaining 0 5 m s as the standard deviation which is a form of posterior inflation where the posterior spread is relaxed to the prior whitaker and hamill 2012 the data assimilation was started two days earlier than the targeted estimation time window as the spin up to minimize the impact of the initial conditions on the flux estimation accuracy we first tested the flux estimations using one iteration i e l 1 to verify the implementation of sequential es in dart pflotran fig 6 shows the ensembles of the hourly exchange fluxes and the simulated groundwater temperature before and after assimilating temperature responses as compared against the synthetic observations and ground truth in both the time series and scatter plots all pairs of ensemble mean of the estimated hourly flux vs its ground truth are tightly distributed around the 1 1 line with substantial reduction in the ensemble uncertainty which consequently improves the simulated temperature responses below the riverbed the results in fig 6 clearly demonstrate the effective dynamic parameter estimation using the sequential es approach implemented in dart pflotran we then compared the performance of the sequential es with the original enkf scheme in dart for estimating the dynamic exchange fluxes which does not honor different observed time steps in each assimilation window this is done by computing the mean absolute error mae against the ground truth in posterior flux and temperature estimations at each time window as plotted in fig 7 a showing the mae comparisons between es and enkf with their corresponding means over the entire estimation time window Œº m a e shown on the top of each subplot the results show that mae of estimated fluxes from the two approaches are comparable with most of the maes smaller than 0 2 m s and data pairs distributed nearly symmetrically around the black dotted 1 1 line there are more data pairs falling below the 1 1 line in the larger mae regime i e larger than 0 3 m s illustrating that enkf tends to produce more higher absolute errors which is also consistent with its higher average mae than that of es i e 0 141 m s vs 0 116 m s the more accurate estimations of exchange fluxes by es result in smaller maes in the simulated groundwater temperature across all depths more so at the depths of 15 and 25 cm as evidenced by more data pairs falling below the 1 1 line while the exchange fluxes by both es and enkf yield highly accurate predictions of groundwater temperature as illustrated by the small magnitude of their maximum and average maes es reduces the average temperature maes to approximately half compared to enkf such gain in estimation accuracy demonstrates the potential advantage of es in parameter estimation over the original enkf scheme in dart lastly we assessed the performance gain of multiple iterations in sequential es mda by comparing the maes of both exchange fluxes and simulated groundwater temperatures produced by l 3 and l 1 i e the es approach as shown in fig 7 b the results show universal reductions in maes when the number of iterations in es mda is increased from one to three although there is only a slight decrease in average mae of estimated exchange fluxes i e from 0 116 m s to 0 104 m s when increasing the iteration number from one to three there are substantial reductions in a number of large flux maes with the iterative data assimilation as represented by the data pairs far above the 1 1 line such improvement in flux estimation through iterations leads to significant reductions in the maes of the simulated groundwater temperature effectively eliminating all maes larger than 0 1 c in es the performance gain through iterative es in this test case not only verifies our implementation of the sequential es mda in dart pflotran it also demonstrates the necessity of taking the iterative es to improve data assimilation accuracy under nonlinearity while both cases show the performance improvement using iterative assimilation the performance gain in case 2 is slight and is not significant as case 1 whose estimated permeability converges to the ground truth after only two iterations fig 5 the less estimation improvement using multiple assimilation in case 2 is probably due to the sequential way of assimilation that uses hourly time window for performing es mda and therefore greatly reduces the nonlinear dynamics impact on the performance at each window hence it is expected that the iterative assimilation of dart pflotran would potentially facilitate applications with stronger nonlinearity impact e g large assimilation domain with more complex terrain 4 conclusion in this study we developed an open source software framework dart pflotran for conducting sequential es mda to estimate static and dynamic parameters for subsurface flow and transport models this new software framework links dart a community facility for data assimilation with pflotran a parallel simulation code for subsurface flow and reactive transport processes we enabled the ensemble smoother option in dart and developed multiple utility functions to establish communications between dart and pflotran for sequential es mda we verified the implementation of dart pflotran for both the static and dynamic parameter estimations using two synthetic cases which demonstrated that we have successfully extended dart beyond its traditional applications in atmospheric science for updating model state vectors we implemented the integrated workflow of performing es mda in both python and c shell scripts we also provided a user friendly interface using jupyter notebook the scripts and the jupyter notebook templates can be easily adapted to other applications to alleviate the burden of managing the complex data assimilation and parameter estimation workflow especially when sequential and iterative assimilation is necessary to reduce the adverse effects of nonlinearity on estimation accuracy with the added flexibility in subsetting observation data in space and time dart pflotran is poised for large scale applications with complex and highly heterogeneous terrain the workflow developed to link dart and pflotran can also be extended to link dart with other similar simulators such as the advanced terrestrial simulator the ats group and parflow kollet and maxwell 2006 which will greatly accelerate the integration of multi scale and multi type observations above and below ground with watershed models to improve the predictability of a wide variety of real systems software availability software name dart pflotran developer peishi jiang year first official release 2021 programming language python fortran c shell program size 134 mb availability gitlab pnnl gov sbrsfa dart pflotran declaration of competing interest the authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper acknowledgements the authors would like to thank tim hoar from national center for atmospheric research for his insightful suggestions on improving the dart pflotran software this research was supported by the u s department of energy doe office of biological and environmental research ber as part of ber s subsurface biogeochemical research program sbr this contribution originates from the sbr scientific focus area sfa at the pacific northwest national laboratory pnnl and was supported by the partnership with the ideas watersheds this research used resources of the national energy research scientific computing center a doe office of science user facility supported by the office of science of the u s department of energy pnnl is operated for the doe by battelle memorial institute under contract de ac05 76rl01830 this paper describes objective technical results and analysis any subjective views or opinions that might be expressed in the paper do not necessarily represent the views of the u s department of energy or the united states government 
